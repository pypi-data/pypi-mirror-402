# Development Style & Guidelines
- When git committing work, never use the Generated by Claude bylines in the commit message.
- Avoid comments that describe exactly what the code is doing, unless it is neccessary to explain why.
- Genenerally, comments should be why-focused. Class/Function docstrings are an exception.
- Avoid creating too much abstraction. Reuse or extend over reinvent.
- Avoid rolling out your own code when something exists in standard libraries or on pypi. Ask yourself has this been done before? Do not reinvent wheels.
- Explictly specify encoding type (typically utf-8) with file operations.
- If you change code, you should also update all dependencies, including tests and docs. This also includes README.md and CLAUDE.md
- When updating CLAUDE.md, keep in mind it should be concise. You will not touch the Development Guidelines (this) section.

# AutoRubric Development Reference

A Python library for evaluating text outputs against weighted criteria using LLM-as-a-judge.

**For detailed documentation, examples, and usage guides, see [README.md](README.md).**

## Package Structure

```
src/autorubric/
├── __init__.py              # Public exports
├── dataset.py               # DataItem, RubricDataset
├── eval.py                  # EvalRunner, EvalResult, evaluate()
├── llm.py                   # LLMConfig, LLMClient, ThinkingConfig
├── prompts.py               # Centralized prompt definitions
├── rate_limit.py            # Per-model rate limiting
├── rubric.py                # Core Rubric class
├── types.py                 # Criterion, LengthPenalty, ensemble types, etc.
├── utils.py                 # JSON parsing, length penalty utilities
├── graders/
│   ├── __init__.py          # Grader exports
│   ├── base.py              # Abstract Grader base class
│   └── criterion_grader.py  # Unified grader (single/ensemble/few-shot)
└── metrics/
    ├── __init__.py          # compute_metrics, result types
    ├── _compute.py          # Main compute_metrics implementation
    ├── _types.py            # MetricsResult, CriterionMetrics, etc.
    ├── _helpers.py          # Verdict extraction helpers
    └── distribution.py      # EMD, KS test, bias metrics
```

## Key Types (Quick Reference)

### Core Types (src/autorubric/types.py)

| Type | Purpose |
|------|---------|
| `Criterion` | Single evaluation criterion with weight, requirement, optional multi-choice options |
| `CriterionOption` | Multi-choice option with label, value (0-1), optional `na` flag |
| `CriterionVerdict` | Enum: `MET`, `UNMET`, `CANNOT_ASSESS` |
| `CriterionReport` | Criterion + verdict + reason |
| `EvaluationReport` | Full grading result with score, raw_score, report, token_usage, cost |
| `EnsembleEvaluationReport` | Adds judge_scores, mean_agreement, per-criterion votes |
| `LengthPenalty` | Config: free_budget, max_cap, penalty_at_cap, exponent, penalty_type |
| `TokenUsage` | prompt_tokens, completion_tokens, total_tokens, cache stats |

### Grader Types (src/autorubric/graders/)

| Type | Purpose |
|------|---------|
| `CriterionGrader` | Main grader - supports single LLM, ensemble, and few-shot modes |
| `JudgeSpec` | Ensemble judge config: llm_config, judge_id, weight |
| `FewShotConfig` | n_examples, balance_verdicts, include_reason, seed |

### Dataset Types (src/autorubric/dataset.py)

| Type | Purpose |
|------|---------|
| `DataItem` | submission, description, optional ground_truth verdicts, optional per-item rubric, optional reference_submission |
| `RubricDataset` | prompt, optional global rubric, items, name, optional reference_submission; methods: get_item_rubric, get_item_reference_submission, split_train_test, to/from_file |

### LLM Types (src/autorubric/llm.py)

| Type | Purpose |
|------|---------|
| `LLMConfig` | model, temperature, max_tokens, thinking, prompt_caching, max_parallel_requests |
| `LLMClient` | Async client with generate(), caching, rate limiting |
| `ThinkingConfig` | level (LOW/MEDIUM/HIGH) or budget_tokens |

### Eval Types (src/autorubric/eval.py)

| Type | Purpose |
|------|---------|
| `EvalConfig` | fail_fast, show_progress, experiment_name, resume |
| `EvalResult` | item_results, timing_stats, token_usage, cost; method: compute_metrics() |
| `ItemResult` | item_idx, item, report, duration_seconds, error |

### Metrics Types (src/autorubric/metrics/)

| Type | Purpose |
|------|---------|
| `MetricsResult` | accuracy, precision, recall, f1, kappa, correlations, bias, per_criterion |
| `CriterionMetrics` | Per-criterion binary metrics |
| `OrdinalCriterionMetrics` | weighted_kappa, adjacent_accuracy, correlations |
| `NominalCriterionMetrics` | kappa, per_option metrics |

## Architecture Notes

### Grading Flow
1. `Rubric.grade()` delegates to grader's `grade()` method
2. `CriterionGrader` treats single LLM as "ensemble of 1"
3. Makes concurrent LLM calls per criterion per judge via `asyncio.gather()`
4. Aggregates votes using strategy (majority/weighted/unanimous/any)
5. Returns `EnsembleEvaluationReport` (consistent interface)

### Score Calculation
```python
# Positive criteria: MET earns weight, UNMET earns 0
# Negative criteria: MET subtracts weight, UNMET contributes 0
weighted_sum = sum(verdict_value * criterion.weight for each criterion)
score = clamp(weighted_sum / total_positive_weight, 0, 1)  # if normalized
# Length penalty subtracted after base calculation
```

### Multi-Choice Criteria
- `scale_type`: "ordinal" (weighted kappa) or "nominal" (unweighted kappa)
- Options have explicit `value` (0-1) to avoid position bias
- `shuffle_options=True` (default) mitigates position bias
- NA options (`na: true`) excluded from scoring like CANNOT_ASSESS

### CANNOT_ASSESS Handling
Strategies: `SKIP` (adjust denominator), `ZERO`, `PARTIAL` (configurable), `FAIL` (worst case)

## Key Conventions

- All graders return `EnsembleEvaluationReport` for consistent interface
- `raw_score` always populated regardless of `normalize` setting
- Parse failures use conservative defaults (UNMET for positive, MET for negative weights)
- Filter `error is not None` results in training pipelines
- Rate limiting via `LLMConfig.max_parallel_requests` (per-provider semaphore)

## Public Exports

See `src/autorubric/__init__.py` for complete list.
