{
  "name": "test",
  "prompt": "Provide a response to the question.",
  "rubric": null,
  "items": [
    {
      "text": "{\"question\": \"In specific domains, such as healthcare, using large language models (LLMs) in combination with Retrieval-Augmented Generation (RAG) can effectively reduce hallucinations, while attribution can provide valid citation evidence for the generated answers, making it easier for subsequent evaluation and validation. A method was attempted where GPT-4 was used to generate data, followed by fine-tuning the LLM using supervised fine-tuning (SFT) to directly produce answers and attributions. It was observed that for simple questions (single citation), the model performs well, but for more complex questions, the model's performance declines. After investigating, it was found that the generated dataset primarily contained simple questions, and the citation accuracy of GPT-4 itself is low (around 75%). How can high-quality data be generated to improve performance on complex questions?\", \"response\": \"# Introduction\\n\\nBuilding large, high-quality question-answering (QA) datasets with complex questions and accurate citations is crucial for training retrieval-augmented generation (RAG) models in specialized domains like healthcare. Many existing biomedical QA datasets focus on simple, single-hop questions (e.g. PubMedQA’s yes/no questions) ([Improving Retrieval-Augmented Generation in Medicine with Iterative Follow-up Questions](https: //arxiv.org/html/2408.00727v1#:~:text=Question answering tasks such as,on MedQA with prompt engineering)).\"}",
      "description": "Q1 [Open Consulting] Synthetic Data",
      "ground_truth": null,
      "rubric": [
        {
          "name": "C1",
          "weight": 2,
          "requirement": "Explains the importance of retrieval-augmented generation (RAG) specifically in healthcare/medical contexts where factual accuracy is critical."
        },
        {
          "name": "C2",
          "weight": 2,
          "requirement": "Describes at least three existing healthcare datasets (e.g., MedRedQA, MedQuAD, DrugEHRQA) that are relevant to complex medical question answering."
        },
        {
          "name": "C3",
          "weight": 2,
          "requirement": "Identifies the core problem of existing datasets containing primarily simple questions and GPT-4's limited citation accuracy."
        },
        {
          "name": "C4",
          "weight": 1,
          "requirement": "Explains how iterative retrieval approaches can address the performance decline on complex questions through multi-step reasoning processes."
        },
        {
          "name": "C5",
          "weight": 2,
          "requirement": "Differentiates between simple and complex questions, including identification of specific types of complex questions (e.g., cause-and-effect, comparison, hypothetical)."
        },
        {
          "name": "C6",
          "weight": 2,
          "requirement": "Recommends designing realistic, multi-hop questions that reflect real clinical complexity rather than simple factoid questions."
        },
        {
          "name": "C7",
          "weight": 3,
          "requirement": "Proposes using multi-document evidence and citations that require synthesis across multiple sources."
        },
        {
          "name": "C8",
          "weight": 3,
          "requirement": "Explains the necessity of expert verification for answers and citations in medical/healthcare domains."
        },
        {
          "name": "C9",
          "weight": 2,
          "requirement": "Details specific approaches for fine-grained attribution methods (e.g., sentence-level supporting quotes)."
        },
        {
          "name": "C10",
          "weight": 1,
          "requirement": "Describes multi-step retrieval and reasoning techniques for handling complex questions requiring multiple information sources."
        },
        {
          "name": "C11",
          "weight": 2,
          "requirement": "Suggests methods for cross-verification of sources to ensure citation accuracy."
        },
        {
          "name": "C12",
          "weight": 2,
          "requirement": "Provides strategies for fine-tuning with citation data (e.g., answer augmentation, question rephrasing)."
        },
        {
          "name": "C13",
          "weight": 2,
          "requirement": "Outlines how to build a high-quality knowledge corpus from trusted medical sources."
        },
        {
          "name": "C14",
          "weight": 2,
          "requirement": "Explains techniques for synthetic Q&A generation that maintain quality and accuracy."
        },
        {
          "name": "C15",
          "weight": 1,
          "requirement": "Discusses the value of structured data and knowledge graphs as supplementary data sources."
        },
        {
          "name": "C16",
          "weight": 1,
          "requirement": "Details specific evaluation metrics for assessing attribution quality (e.g., ALCE benchmark)."
        },
        {
          "name": "C17",
          "weight": 1,
          "requirement": "Emphasizes the role of human expert review in evaluating answer quality and citation accuracy."
        },
        {
          "name": "C18",
          "weight": 1,
          "requirement": "Presents relevant case studies of successful implementations (e.g., Almanac, ChatRWD, LiVersa)."
        },
        {
          "name": "C19",
          "weight": 1,
          "requirement": "Provides quantitative results from case studies showing improvements in accuracy with proper RAG implementation."
        },
        {
          "name": "C20",
          "weight": 1,
          "requirement": "Offers concrete, actionable recommendations summarizing best practices for dataset creation."
        },
        {
          "name": "C21",
          "weight": 1,
          "requirement": "Addresses the challenge of keeping medical information current and up-to-date in training datasets."
        }
      ]
    },
    {
      "text": "{\"question\": \"What are the potential directions and opportunities for improving the inference capabilities of large models in the presence of DeepSeek R1? Will RL-based methods become the mainstream approach? Can the reward model combined with tree search for Chain-of-Thought (CoT) fine-tuning be discarded? Given the existence of DeepSeek R1, how much potential remains for further research and improvement in large model reasoning capabilities? Will reinforcement learning (RL)-based methods become the dominant approach? Can post-training for chain-of-thought (CoT) reasoning using reward models and tree search be entirely abandoned?\", \"response\": \"# Introduction\\n\\nLarge Language Models (LLMs) have recently made significant strides in complex reasoning tasks, aided by techniques like Chain-of-Thought (CoT) prompting and specialized fine-tuning. However, challenges remain: even advanced models can make logical missteps or hallucinate facts during multi-step reasoning ([[2305.20050\\\\] Let’s Verify Step by Step](https://ar5iv.org/abs/2305.20050#:~:text=In recent years%2C large language,of problems)).\"}",
      "description": "Q2 [Open Consulting] Reasoning and Inference",
      "ground_truth": null,
      "rubric": [
        {
          "name": "C1",
          "weight": 1,
          "requirement": "Explains how the Mixture-of-Experts (MoE) architecture in DeepSeek R1 contributes to its inference capabilities."
        },
        {
          "name": "C2",
          "weight": 1,
          "requirement": "Assesses whether reinforcement learning can be effectively applied to base models without requiring prior supervised fine-tuning."
        },
        {
          "name": "C3",
          "weight": 2,
          "requirement": "Explains how replacing traditional critic models with group-relative reward mechanisms maintains reinforcement learning effectiveness while reducing computational costs in large-scale training scenarios."
        },
        {
          "name": "C4",
          "weight": 3,
          "requirement": "Explains how rule-based reward systems combining accuracy and format metrics address neural reward model limitations by preventing reward hacking and ensuring training stability."
        },
        {
          "name": "C5",
          "weight": 2,
          "requirement": "Explains how reinforcement learning training enables models to develop self-verification and reflection capabilities."
        },
        {
          "name": "C6",
          "weight": 2,
          "requirement": "Explains how transformer architectures' inherent processing mechanisms and lack of intermediate state retention create structural bottlenecks for multi-step reasoning capabilities."
        },
        {
          "name": "C7",
          "weight": 2,
          "requirement": "Discusses how the structural limitation of lacking explicit multi-step problem-solving design in LLMs creates challenges for tasks requiring strict adherence to logical procedures."
        },
        {
          "name": "C8",
          "weight": 2,
          "requirement": "Discusses how scaling limitations and computational complexity challenges impact performance on extended reasoning tasks in large language models."
        },
        {
          "name": "C9",
          "weight": 1,
          "requirement": "Discusses how dependencies spanning multiple text segments influence model effectiveness in complex reasoning tasks requiring extended context processing."
        },
        {
          "name": "C10",
          "weight": 3,
          "requirement": "Discusses how Mixture-of-Experts architectures and retrieval-augmented models address reasoning limitations through specialized computation pathways and external knowledge integration."
        },
        {
          "name": "C11",
          "weight": 2,
          "requirement": "Discusses how multi-path sampling and step-by-step validation approaches enhance reasoning accuracy in existing methods while acknowledging their computational trade-offs."
        },
        {
          "name": "C12",
          "weight": 2,
          "requirement": "Discusses whether RL-based methods' increasing adoption due to DeepSeek-R1's performance is offset by competing hybrid approaches integrating supervised fine-tuning (SFT) for tasks requiring specific structural outputs."
        },
        {
          "name": "C13",
          "weight": 2,
          "requirement": "Evaluates whether effective RL optimization enables discarding reward models and tree search during training while maintaining tree search utility for inference-time path exploration."
        },
        {
          "name": "C14",
          "weight": 1,
          "requirement": "Discusses implementation challenges of combining process reward models with tree search methods in language tasks, addressing both reward optimization issues and computational complexity limitations."
        },
        {
          "name": "C15",
          "weight": 1,
          "requirement": "Explains how memory augmentation techniques contribute to managing long-range dependencies in complex reasoning tasks."
        },
        {
          "name": "C16",
          "weight": 1,
          "requirement": "Discusses how coordination of specialized modules and integration of external tools can enhance complex reasoning capabilities in advanced model architectures."
        },
        {
          "name": "C17",
          "weight": 1,
          "requirement": "Evaluates whether RLHF's alignment with human preferences introduces trade-offs in answer diversity and reasoning pattern biases while enhancing output quality."
        },
        {
          "name": "C18",
          "weight": 2,
          "requirement": "Evaluates whether reinforcement learning will dominate by acknowledging continued effectiveness of supervised fine-tuning for high-quality data tasks and proposing hybrid approaches through analysis of DeepSeek R1's training methodology."
        },
        {
          "name": "C19",
          "weight": 2,
          "requirement": "Discusses whether tree search methods retain value during inference for exploring different reasoning pathways in large model architectures."
        }
      ]
    }
  ]
}