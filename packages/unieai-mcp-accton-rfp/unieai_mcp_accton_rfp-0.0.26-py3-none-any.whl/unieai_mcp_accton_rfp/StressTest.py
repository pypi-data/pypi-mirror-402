import asyncio
import json
import logging
import os
import re
import tempfile
import time  # æ–°å¢ï¼šç”¨æ–¼è¨ˆæ™‚
from typing import Any, Dict, Tuple, List, Optional
from datetime import datetime
from dataclasses import dataclass, field  # æ–°å¢ï¼šç”¨æ–¼çµ±è¨ˆè³‡æ–™çµæ§‹

import requests
from dotenv import load_dotenv
from fastmcp import FastMCP
from openpyxl import load_workbook
from openpyxl.worksheet.worksheet import Worksheet

# LangChain (1.x API)
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, SystemMessage

# ==============================
# ğŸ› Environment & Logging
# ==============================

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("unieai-mcp-accton-rfp")

# é™åˆ¶åŒæ™‚é‹è¡Œçš„ LLM è«‹æ±‚æ•¸é‡ï¼Œé˜²æ­¢ API é€Ÿç‡é™åˆ¶
semaphore = asyncio.Semaphore(80) 

# --- å…¨åŸŸå¿«å– LLM å¯¦ä¾‹ ---
llm_cache: Dict[str, ChatOpenAI] = {}

# Appwrite ENV
APPWRITE_PROJECT_ID = "6901b22e0036150b66d3"
APPWRITE_API_KEY = "standard_b1462cfd2cd0b6e5b5f305a10799444e009b880adf74e4b578e96222b148da57e17d57957fe3ffba9c7bfa2f6443b66fbcb851b8fbae0b91dc908139ca1d8e54c1bcba9034449d579449fc2abcdb1d9fdca3cc67bdb15140d8f5df1193264bd070e0f738bc3b13fd94de0d4aee3e2075f6b2124b803470d82f9501e806d16ffd"
APPWRITE_ENDPOINT = "https://sgp.cloud.appwrite.io/v1"

BATCH_SIZE = 200   # æ¯æ¬¡ä¸¦è¡Œè™•ç†çš„ Excel è¡Œæ•¸
TIMEOUT = 600

# ==============================
# ğŸ“ˆ Stress Test Stats (å£“åŠ›æ¸¬è©¦çµ±è¨ˆé¡)
# ==============================

@dataclass
class StressTestStats:
    start_time: float = 0.0
    total_rows: int = 0
    completed_rows: int = 0
    error_count: int = 0
    latencies: List[float] = field(default_factory=list)

    def reset(self, total_rows: int):
        self.start_time = time.time()
        self.total_rows = total_rows
        self.completed_rows = 0
        self.error_count = 0
        self.latencies = []

    def add_latency(self, duration: float):
        self.latencies.append(duration)

    def report(self, model_name: str):
        end_time = time.time()
        total_duration = end_time - self.start_time
        avg_latency = sum(self.latencies) / len(self.latencies) if self.latencies else 0
        # æ¯ä¸€è¡ŒåŒ…å«å…©æ¬¡å‘¼å« (Ref + Res)
        total_calls = self.completed_rows * 2
        tps = total_calls / total_duration if total_duration > 0 else 0
        
        print("\n" + "â–ˆ" * 60)
        print(f"ğŸ“Š å£“åŠ›æ¸¬è©¦ç¸½çµå ±å‘Š | æ¨¡å‹: {model_name}")
        print("-" * 60)
        print(f"ğŸ”¹ ç¸½åŸ·è¡Œæ™‚é–“   : {total_duration:.2f} ç§’")
        print(f"ğŸ”¹ ç¸½è™•ç†è¡Œæ•¸   : {self.total_rows} è¡Œ")
        print(f"ğŸ”¹ æˆåŠŸå®Œæˆè¡Œæ•¸ : {self.completed_rows} è¡Œ")
        print(f"ğŸ”¹ ç¸½ API è«‹æ±‚æ•¸: {total_calls} æ¬¡")
        print(f"ğŸ”¹ å¤±æ•—è«‹æ±‚æ•¸   : {self.error_count} æ¬¡")
        print(f"ğŸ”¹ å¹³å‡å›æ‡‰å»¶é² : {avg_latency:.2f} ç§’")
        print(f"ğŸ”¹ ååé‡ (TPS) : {tps:.2f} requests/sec")
        print(f"ğŸ”¹ æˆåŠŸç‡       : {((total_calls - self.error_count) / (total_calls or 1)) * 100:.2f}%")
        print("â–ˆ" * 60 + "\n")

# å»ºç«‹å…¨åŸŸçµ±è¨ˆå¯¦ä¾‹
stats = StressTestStats()

# ==============================
# ğŸ§© Helper Functions
# ==============================

def _extract_result_json(text: str):
    try:
        match = re.search(r'\{[\s\S]*?\}', text)
        if not match: return {"Result": "Error"}
        json_str_cleaned = (
            match.group(0).strip()
            .replace('\u00A0', ' ').replace('\u200B', '')
            .replace('\n', '').replace('\r', '').replace('\t', '')
        )
        return json.loads(json_str_cleaned)
    except Exception:
        return {"Result": "Error"}

def _parse_appwrite_url(url: str) -> Tuple[Optional[str], Optional[str]]:
    pattern = r"/storage/buckets/([^/]+)/files/([^/]+)"
    m = re.search(pattern, url)
    return (m.group(1), m.group(2)) if m else (None, None)

def _generate_new_filename(original_name: str) -> str:
    base, ext = os.path.splitext(original_name)
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    return f"{base}_processed_{timestamp}{ext}"

# ==============================
# ğŸ¤– LLM Logic
# ==============================

async def _call_llm_raw(prompt: str, user_message: str, request_id: str, model_name: str):
    """è¿”å› LLM ç´”æ–‡å­—å…§å®¹ï¼Œä¸¦ç´€éŒ„å£“æ¸¬è€—æ™‚"""
    start_call = time.time()
    
    if model_name not in llm_cache:
        llm_cache[model_name] = ChatOpenAI(
            model=model_name,
            base_url="https://api.unieai.com/v1",
            api_key="sk-gNW9beBKfVh0rlIprds0pcMe9xit1xjW8U9bfytmeo1BBFQU",
            temperature=0.1,
            max_tokens=15000,
            top_p=1.0
        )
    
    current_llm = llm_cache[model_name]
    
    try:
        async with semaphore:
            response = await asyncio.wait_for(
                current_llm.ainvoke([
                    SystemMessage(content=prompt),
                    HumanMessage(content=user_message)
                ]),
                timeout=TIMEOUT
            )
            
            content = (response.content or "").strip()
            duration = time.time() - start_call
            stats.add_latency(duration)  # ç´€éŒ„å»¶é²
            logger.info(f"âœ… [ID:{request_id}] å®Œæˆ | è€—æ™‚: {duration:.2f}s")
            return content
            
    except Exception as e:
        stats.error_count += 1  # ç´€éŒ„éŒ¯èª¤
        logger.error(f"âŒ [ID:{request_id}] ç™¼ç”ŸéŒ¯èª¤: {e}")
        return f"LLM Error: {e}"

# ==============================
# ğŸ“˜ Prompt Builders (ä¿æŒä¸è®Š)
# ==============================

def _build_reference_prompt() -> str:
    return """ä½ æ˜¯ä¸€ä½åš´è¬¹çš„ç”¢å“ç¶“ç†åŠ©ç†ï¼Œå°ˆé–€è² è²¬å°‡å…§éƒ¨ç”¢å“è¦æ ¼ï¼ˆçŸ¥è­˜åº«ï¼‰èˆ‡å®¢æˆ¶çš„éœ€æ±‚å–®ï¼ˆRFPï¼‰é€²è¡Œæ¯”å°å’Œç¬¦åˆæ€§åˆ†æã€‚

    ä»»å‹™æŒ‡ç¤ºï¼š
    1.  ä½ å°‡æ”¶åˆ°å®¢æˆ¶çš„ç”¢å“éœ€æ±‚å–® (RFP) ä½œç‚ºè¼¸å…¥ã€‚
    2.  ä½ çš„çŸ¥è­˜åº«å·²åŒ…å«ä½ å…¬å¸ç”¢å“çš„å®Œæ•´èªªæ˜æ–‡ä»¶ã€‚
    3.  è«‹ä»”ç´°é–±è®€ RFP ä¸­çš„æ¯ä¸€æ¢å…·é«”éœ€æ±‚ï¼Œä¸¦åˆ©ç”¨ä½ çš„ç”¢å“çŸ¥è­˜åº«å…§å®¹é€²è¡Œåš´æ ¼æ¯”å°ã€‚

    æ¯”å°è¦å‰‡ï¼š
    1. Conform (å®Œå…¨ç¬¦åˆ)ï¼šå…¬å¸çš„ç”¢å“è¦æ ¼èƒ½å®Œæ•´ä¸”ç„¡æ¢ä»¶åœ°æ»¿è¶³ RFP ä¸­çš„è©²é …éœ€æ±‚ã€‚
    2. Half Conform (éƒ¨åˆ†ç¬¦åˆ)ï¼šå…¬å¸çš„ç”¢å“è¦æ ¼åªèƒ½æ»¿è¶³ RFP ä¸­è©²é …éœ€æ±‚çš„éƒ¨åˆ†å…§å®¹ï¼Œæˆ–è€…éœ€è¦é€éè®Šé€šã€é¡å¤–é…ç½®æˆ–æœªä¾†è¦åŠƒæ‰èƒ½æ»¿è¶³ã€‚
    3. Not Conform (ä¸ç¬¦åˆ)ï¼šå…¬å¸çš„ç”¢å“è¦æ ¼ç„¡æ³•æ»¿è¶³ RFP ä¸­çš„è©²é …éœ€æ±‚ã€‚

    è¼¸å‡ºæ ¼å¼è¦æ±‚ï¼ˆåš´ç¦ä½¿ç”¨æ˜Ÿè™Ÿ *ï¼‰ï¼š
    ä½ å¿…é ˆä»¥æ¢åˆ—å¼æ¸…æ™°åœ°è¼¸å‡ºåˆ†æçµæœï¼Œæ¯ä¸€æ¢çµæœå¿…é ˆåŒ…å«ï¼š
    1.  RFP ä¸­çš„åŸå§‹éœ€æ±‚æè¿° (ç°¡çŸ­æ‘˜éŒ„æˆ–ç·¨è™Ÿ)ã€‚
    2.  ç¬¦åˆç¨‹åº¦ : (åªèƒ½æ˜¯ï¼šConform, Half Conform, Not Conform ä¸‰è€…ä¹‹ä¸€)ã€‚
    3.  åƒè€ƒä¾æ“š : (èªªæ˜åšå‡ºåˆ¤æ–·çš„ä¾æ“šï¼Œéœ€æ˜ç¢ºå¼•ç”¨çŸ¥è­˜åº«ä¸­çš„ç›¸é—œç”¢å“èªªæ˜çš„é—œéµè³‡è¨Šæˆ–æ®µè½ï¼Œä¾‹å¦‚ï¼šçŸ¥è­˜åº«ä¸­ã€ŒåŠŸèƒ½Aã€çš„æè¿°æ”¯æŒæ­¤åˆ¤æ–·)ã€‚

    è«‹é‡å° RFP ä¸­çš„æ¯ä¸€æ¢ä¸»è¦éœ€æ±‚é€ä¸€é€²è¡Œåˆ†æã€‚
    æ³¨æ„ï¼šå…¨æ–‡ç¦æ­¢å‡ºç¾ä»»ä½•æ˜Ÿè™Ÿç¬¦è™Ÿã€‚
    ã€Œè«‹ä»¥ç´”æ–‡å­—æ ¼å¼è¼¸å‡ºï¼Œåš´ç¦ä½¿ç”¨ Markdown èªæ³•ï¼ˆå°¤å…¶æ˜¯ç²—é«”æ˜Ÿè™Ÿï¼‰ã€‚ã€""" # è«‹ä¿ç•™æ‚¨åŸæœ¬çš„å®Œæ•´ Prompt

def _build_result_prompt() -> str:
    return """è«‹ä¾æ“šä»¥ä¸‹ Reference æ–‡æœ¬ï¼Œåˆ¤æ–·å…¶ç¬¦åˆæ€§ï¼š
- Conformï¼šå®Œå…¨ç¬¦åˆ
- Half Conformï¼šéƒ¨åˆ†ç¬¦åˆ
- Not Conformï¼šä¸ç¬¦åˆ

è«‹åƒ…è¼¸å‡ºä»¥ä¸‹ JSON æ ¼å¼ï¼š
{
 "Result": "Conform / Half Conform / Not Conform"
}""" # è«‹ä¿ç•™æ‚¨åŸæœ¬çš„å®Œæ•´ Prompt

def _build_user_message(a: str, b: str, c: str, d: str) -> str:
    return f"{a}, {b}, {c}, {d}"

def chunk_list(data, size):
    for i in range(0, len(data), size):
        yield data[i:i + size]

# ==============================
# ğŸ“Š Excel Processing Core
# ==============================

async def _process_excel_logic(url: str, model_name: str) -> Dict[str, Any]:
    logger.info(f"ğŸŸ¢ é–‹å§‹è™•ç† Excelï¼š{url} (Model: {model_name})")

    # Step 1: Download / Load
    source_type = ""
    local_path = None
    appwrite_info = (None, None)

    if url.startswith("file:///"):
        local_path = url.replace("file:///", "")
        file_path = local_path
        source_type = "local"
    elif url.startswith("http"):
        resp = requests.get(url)
        resp.raise_for_status()
        with tempfile.NamedTemporaryFile(delete=False, suffix=".xlsx") as tmp:
            tmp.write(resp.content)
            file_path = tmp.name
        bucket_id, file_id = _parse_appwrite_url(url)
        source_type = "appwrite" if bucket_id else "remote_readonly"
        appwrite_info = (bucket_id, file_id)
    else:
        raise ValueError("âŒ ä¸æ”¯æ´æª”æ¡ˆä¾†æº")

    # Step 2: Open Excel
    wb = load_workbook(file_path)
    ws = wb.active
    header = {cell.value: idx for idx, cell in enumerate(ws[1], 1)}

    # Step 3: äºŒéšæ®µ LLM èˆ‡å£“æ¸¬ç›£æ§
    rows_for_llm = [row for row in ws.iter_rows(min_row=2, values_only=False) if any(c.value for c in row)]
    
    # é‡è¨­çµ±è¨ˆè³‡è¨Š
    stats.reset(len(rows_for_llm))
    request_counter = 0

    reference_prompt = _build_reference_prompt()
    result_prompt = _build_result_prompt()
    
    logger.info(f"ğŸš€ å£“æ¸¬å•Ÿå‹•ï¼šè™•ç† {stats.total_rows} è¡Œï¼Œé è¨ˆç™¼å‡º {stats.total_rows * 2} æ¬¡è«‹æ±‚ã€‚")

    for batch_rows in chunk_list(rows_for_llm, BATCH_SIZE):
        # --- Stage 1: Reference ---
        ref_tasks = []
        for row in batch_rows:
            r = row[0].row
            a, b, c, d = [row[header[k]-1].value or "" for k in ["itemA", "itemB", "itemC", "itemD"]]
            user_msg = _build_user_message(str(a), str(b), str(c), str(d))
            request_counter += 1
            ref_tasks.append(_call_llm_raw(reference_prompt, user_msg, f"R{r}-Ref", model_name))

        reference_results = await asyncio.gather(*ref_tasks)

        for row, ref_text in zip(batch_rows, reference_results):
            ws.cell(row[0].row, header["Reference"], ref_text)

        # --- Stage 2: Result ---
        result_tasks = []
        for row, ref_text in zip(batch_rows, reference_results):
            request_counter += 1
            result_tasks.append(_call_llm_raw(result_prompt, ref_text, f"R{row[0].row}-Res", model_name))

        raw_result_outputs = await asyncio.gather(*result_tasks)

        for row, raw_result in zip(batch_rows, raw_result_outputs):
            parsed = _extract_result_json(raw_result)
            ws.cell(row[0].row, header["Result"], parsed.get("Result", "Error"))

        stats.completed_rows += len(batch_rows)
        progress = (stats.completed_rows / stats.total_rows) * 100
        logger.info(f"ğŸ”¥ é€²åº¦: {progress:.1f}% | ç•¶å‰ API å‘¼å«ç¸½æ•¸: {request_counter}")

    # è¼¸å‡ºæœ€çµ‚å£“æ¸¬å ±å‘Š
    stats.report(model_name)

    # Step 4: Save & Write back (é€™éƒ¨åˆ†ä¿ç•™æ‚¨åŸæœ¬çš„é‚è¼¯)
    local_debug_dir = r"D:\TempExcelDebug"
    os.makedirs(local_debug_dir, exist_ok=True)
    local_debug_path = os.path.join(local_debug_dir, _generate_new_filename("debug_output.xlsx"))
    wb.save(local_debug_path)

    logger.info(f"ğŸ“ æœ¬æ©Ÿ debug æª”æ¡ˆå·²è¼¸å‡ºï¼š{local_debug_path}")
    

    # ... (å¾ŒçºŒçš„ Appwrite ä¸Šå‚³æˆ– local å¯«å›é‚è¼¯è«‹ä¿æŒä¸è®Š) ...
    return {"status": "success", "report": "è©³ç´°è³‡è¨Šè«‹è¦‹ Console"}

# ==============================
# ğŸš€ CLI Test
# ==============================

if __name__ == "__main__":
    load_dotenv() 
    test_url = "https://sgp.cloud.appwrite.io/v1/storage/buckets/6904374b00056677a970/files/694d38f500332bfd1ca9/view?project=6901b22e0036150b66d3&mode=admin"
    test_model = "Qwen3-30B-A3B-Instruct-2507-20260120-accton" #"Qwen3-30B-A3B-Instruct-2507-20251223-accton"

    logger.info(f"--- å£“æ¸¬æ­£å¼é–‹å§‹ {datetime.now()} ---")
    try:
        result = asyncio.run(_process_excel_logic(test_url, test_model))
        print(f"çµæœç‹€æ…‹: {result['status']}")
    except Exception as e:
        logger.error(f"âŒ åŸ·è¡Œå¤±æ•—: {e}")