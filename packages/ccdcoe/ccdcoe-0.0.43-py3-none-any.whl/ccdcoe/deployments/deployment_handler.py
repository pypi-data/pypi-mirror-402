import datetime
import inspect
import logging
import random
import re
from collections import defaultdict
from dataclasses import dataclass
from typing import Any, Tuple

import gitlab
from gitlab import Gitlab
from gitlab.v4.objects import Project, ProjectPipeline, ProjectPipelineSchedule
from tqdm import tqdm

from ccdcoe.deployments.deployment_config import Config
from ccdcoe.deployments.generic.constants import deploy_modes, gitlab_boolean
from ccdcoe.deployments.objects.pipeline_details import (
    PipelineDetails,
    PipelineScheduleDetails,
)
from ccdcoe.deployments.objects.pipeline_vars import PipelineVars
from ccdcoe.deployments.objects.tiers import (
    Tiers,
    Tier1,
    Tier0,
    Tier2,
    Tier3,
    Tier4,
    Tier5,
    Tier6,
    Tier7,
    Tier8,
    Tier9,
    FullTier1,
    FullTier4,
    FullTier2,
    FullTier3,
    FullTier5,
    FullTier6,
    FullTier7,
    FullTier8,
    FullTier9,
)
from ccdcoe.generic.utils import getenv_str
from ccdcoe.http_apis.providentia.providentia_api import ProvidentiaApi
from ccdcoe.loggers.console_logger import ConsoleLogger

logging.setLoggerClass(ConsoleLogger)

__UNIQUE_TIERS__ = {
    0: Tier0,
    1: Tier1,
    2: Tier2,
    3: Tier3,
    4: Tier4,
    5: Tier5,
    6: Tier6,
    7: Tier7,
    8: Tier8,
    9: Tier9,
}
__FULL_TIERS__ = {
    0: Tier0,
    1: FullTier1,
    2: FullTier2,
    3: FullTier3,
    4: FullTier4,
    5: FullTier5,
    6: FullTier6,
    7: FullTier7,
    8: FullTier8,
    9: FullTier9,
}

team_regex = re.compile(r"Team (\d+)")


@dataclass
class PipelineFilter:
    pipelines: list[ProjectPipeline]

    def filter_pipelines(
        self, team_number: int
    ) -> list[ProjectPipeline]:  # pragma: no cover
        filtered_pipelines = []
        for pipeline in self.pipelines:
            m = team_regex.search(pipeline.name)
            if m is not None:
                if int(m.groups()[0]) == team_number:
                    filtered_pipelines.append(pipeline)
        return filtered_pipelines

    def filter_pipelines_return_details(
        self, team_number: int
    ) -> list[PipelineDetails]:  # pragma: no cover
        filtered_pipelines = []
        for pipeline in self.pipelines:
            m = team_regex.search(pipeline.name)
            if m is not None:
                if int(m.groups()[0]) == team_number:
                    filtered_pipelines.append(
                        PipelineDetails(
                            pipeline.id,
                            pipeline.name,
                            pipeline.ref,
                            pipeline.status,
                            pipeline.web_url,
                            pipeline.updated_at,
                        )
                    )
        return filtered_pipelines

    def pipelines_return_details(self):
        filtered_pipelines = []
        for pipeline in self.pipelines:
            filtered_pipelines.append(
                PipelineDetails(
                    pipeline.id,
                    pipeline.name,
                    pipeline.ref,
                    pipeline.status,
                    pipeline.web_url,
                    pipeline.updated_at,
                )
            )
        return filtered_pipelines


class DeploymentHandler(object):

    def __init__(self):
        self.logger = logging.getLogger(__name__)

        self.config = Config

        self.providentia = ProvidentiaApi(
            baseurl=self.config.PROVIDENTIA_URL,
            api_path=self.config.PROVIDENTIA_VERSION,
            api_key=self.config.PROVIDENTIA_TOKEN,
        )

    def __repr__(self):
        return "<<DeploymentHandler>>"

    def get_gitlab_obj(self) -> Gitlab:
        return Gitlab(url=self.config.GITLAB_URL, private_token=self.config.PAT_TOKEN)

    def get_project_by_namespace(
        self, namespace: str, lazy: bool = True, **kwargs
    ) -> Project:

        self.logger.debug(
            f"Method '{inspect.currentframe().f_code.co_name}' called with arguments: {locals()}"
        )

        with self.get_gitlab_obj() as gl:
            project = gl.projects.get(namespace, lazy=lazy, **kwargs)

        return project

    def trigger_deployment_pipeline(
        self, reference: str = "main", variables: PipelineVars = None, **kwargs
    ) -> ProjectPipeline:  # pragma: no cover

        self.logger.debug(
            f"Method '{inspect.currentframe().f_code.co_name}' called with arguments: {locals()}"
        )

        if variables is not None:
            if not isinstance(variables, PipelineVars):
                raise TypeError(f"'variables' is not a PipelineVars object...")
        try:

            pipeline_project = self.get_project_by_namespace(
                self.config.PROJECT_NAMESPACE
            )
            variables_dict = {k: str(v) for k, v in variables.as_dict().items()}

            variables_dict["CI_CONFIG_PATH"] = getenv_str(
                "CI_CONFIG_PATH", ".gitlab-ci.yml"
            )

            return pipeline_project.trigger_pipeline(
                ref=reference,
                token=self.config.TRIGGER_TOKEN,
                variables=variables_dict,
                **kwargs,
            )
        except gitlab.exceptions.GitlabCreateError as e:
            self.logger.error(f"Pipeline could not be triggered -> {e}")
        except Exception as e:
            self.logger.error(f"Uncaught exception while triggering pipeline -> {e}")

    def custom_deployment(
        self, reference: str = "main", variables: dict[str, Any] = None, **kwargs
    ) -> ProjectPipeline:

        self.logger.debug(
            f"Method '{inspect.currentframe().f_code.co_name}' called with arguments: {locals()}"
        )

        if variables is not None:
            try:
                variables = PipelineVars(**variables)
            except ValueError:
                raise
            except Exception as e:
                self.logger.error(f"Uncaught exception parsing variables -> {e}")

        return self.trigger_deployment_pipeline(
            reference=reference, variables=variables, **kwargs
        )

    # noinspection PyArgumentList
    def get_tier(
        self,
        tier_level: int = 0,
        give_back_full: bool = False,
        retrieve_all: bool = False,
        show_bear_level: bool = False,
    ) -> Tiers | dict[int, dict[str, str]]:

        self.logger.debug(
            f"Method '{inspect.currentframe().f_code.co_name}' called with arguments: {locals()}"
        )

        if retrieve_all:
            if show_bear_level:
                return {k: v().show_bear_level() for (k, v) in __UNIQUE_TIERS__.items()}
            else:
                return {k: v().as_dict() for (k, v) in __UNIQUE_TIERS__.items()}
        else:
            if give_back_full:
                return __FULL_TIERS__[tier_level]()
            else:
                return __UNIQUE_TIERS__[tier_level]()

    def deploy_team(
        self,
        reference: str = "main",
        team_number: int = 28,
        tier_level: int = 0,
        start_tier_level: int = 0,
        deploy_full_tier: bool = False,
        deploy_mode: str = deploy_modes.REDEPLOY,
        skip_vulns: bool = False,
        snapshot: bool = True,
        snap_name: str = "CLEAN",
        skip_hosts: str = "",
        only_hosts: str = "",
        actor: str = "",
        large_tiers: str = "",
        standalone_tiers: str = "",
        ignore_deploy_order: bool = False,
        reverse_deploy_order: bool = False,
        docker_image_count: int = 1,
        nova_version: str = "PRODUCTION",
        core_level: int = 0,
        windows_tier: str = "",
        return_pipeline_object: bool = True,
    ) -> ProjectPipeline:

        self.logger.debug(
            f"Method '{inspect.currentframe().f_code.co_name}' called with arguments: {locals()}"
        )

        try:
            the_tier = self.get_tier(
                tier_level=tier_level, give_back_full=deploy_full_tier
            )

            if only_hosts:
                only_hosts_list = only_hosts.split(",")

                tier_assignments = self.get_tier_assignments_providentia()
                tiers = list(tier_assignments.keys())
                for tl in range(0, len(__UNIQUE_TIERS__)):
                    setattr(the_tier, f"REDEPLOY_TIER{tl}", gitlab_boolean.DISABLED)
                for i, tier in enumerate(tiers):
                    # Extract just the base tier name (e.g., "Tier3" from "Tier3A_CORE")
                    match = re.search(r"tier\d+", tier, re.IGNORECASE)
                    if match:
                        top_level_tier = match.group(0)
                    else:
                        # Fallback to original behavior if pattern doesn't match
                        top_level_tier = re.sub(r"[a-zA-Z_]+$", "", tier)
                    for host in tier_assignments[tier]:
                        if list(host.keys())[0] in only_hosts_list:
                            setattr(
                                the_tier,
                                f"REDEPLOY_{top_level_tier.upper()}",
                                gitlab_boolean.ENABLED,
                            )

            if start_tier_level != 0:
                for tl in range(0, start_tier_level):
                    setattr(the_tier, f"REDEPLOY_TIER{tl}", gitlab_boolean.DISABLED)

            tier_data = the_tier.as_dict()
            tier_data["CICD_TEAM"] = team_number
            tier_data["DEPLOY_MODE"] = deploy_mode
            tier_data["SKIP_VULNS"] = skip_vulns
            tier_data["SNAPSHOT"] = snapshot
            tier_data["SNAPSHOT_NAME"] = snap_name
            tier_data["SKIP_HOSTS"] = skip_hosts
            tier_data["ONLY_HOSTS"] = only_hosts
            tier_data["ACTOR"] = actor
            tier_data["LARGE_TIERS"] = large_tiers
            tier_data["STANDALONE_TIERS"] = standalone_tiers
            tier_data["IGNORE_DEPLOY_ORDER"] = ignore_deploy_order
            tier_data["REVERSE_DEPLOY_ORDER"] = reverse_deploy_order
            tier_data["DOCKER_IMAGE_COUNT"] = docker_image_count
            tier_data["NOVA_VERSION"] = nova_version
            tier_data["CORE_LEVEL"] = core_level
            tier_data["WINDOWS_TIER"] = windows_tier

            description = f"{deploy_mode.upper()} Team {team_number} - "

            if deploy_full_tier:
                if start_tier_level == 0:
                    description += f"FULL up to Tier {tier_level} - "
                else:
                    description += (
                        f"FULL from Tier {start_tier_level} to {tier_level} - "
                    )
            else:
                description += f"LIMITED to Tier {tier_level} - "

            if skip_vulns:
                description += f"SKIP_VULNS - "

            if only_hosts:
                description += f"LIMITED to hosts: {only_hosts} - "

            if skip_hosts:
                description += f"SKIP hosts: {skip_hosts} - "

            if actor:
                description += f"ACTOR: {actor} - "

            if nova_version == "STAGING":
                description += f"NOVA_VERSION: STAGING - "

            if len(description) >= 255:
                description = (
                    description[:220] + "-TRUNCATED"
                )  # accounting for 'deploy_mode Team xx' as well

            tier_data["DEPLOY_DESCRIPTION"] = description

            project_pipeline = self.custom_deployment(
                reference=reference, variables=tier_data
            )
            if project_pipeline is not None:
                msg = (
                    f"Project pipeline for team {team_number}({description}) deployed -> "
                    f"pipeline id {project_pipeline.id} status: {project_pipeline.status} ref: {project_pipeline.ref}"
                )
                self.logger.info(msg)
                if return_pipeline_object:
                    return project_pipeline
                else:
                    return msg
        except Exception as e:
            self.logger.error(f"Uncaught exception -> {e}")

    def deploy_standalone(
        self,
        reference: str = "main",
        deploy_mode: str = deploy_modes.REDEPLOY,
        skip_vulns: bool = False,
        snapshot: bool = True,
        snap_name: str = "CLEAN",
        only_hosts: str = "",
        return_pipeline_object: bool = True,
    ) -> ProjectPipeline:

        self.logger.debug(
            f"Method '{inspect.currentframe().f_code.co_name}' called with arguments: {locals()}"
        )

        try:
            the_tier = self.get_tier()

            tier_data = the_tier.as_dict()
            tier_data["CICD_TEAM"] = "SA"
            tier_data["DEPLOY_MODE"] = deploy_mode
            tier_data["SKIP_VULNS"] = skip_vulns
            tier_data["SNAPSHOT"] = snapshot
            tier_data["SNAPSHOT_NAME"] = snap_name
            tier_data["ONLY_HOSTS"] = only_hosts
            tier_data["STANDALONE_DEPLOYMENT"] = gitlab_boolean.ENABLED

            description = f"{deploy_mode.upper()} Standalone - "

            if skip_vulns:
                description += f"SKIP_VULNS - "

            if only_hosts:
                description += f"LIMITED to hosts: {only_hosts}"

            if len(description) >= 255:
                description = (
                    description[:220] + "-TRUNCATED"
                )  # accounting for 'deploy_mode Team xx' as well

            tier_data["DEPLOY_DESCRIPTION"] = description

            project_pipeline = self.custom_deployment(
                reference=reference, variables=tier_data
            )
            if project_pipeline is not None:
                msg = (
                    f"Project pipeline for standalone deployment({description}) deployed -> "
                    f"pipeline id {project_pipeline.id} status: {project_pipeline.status} ref: {project_pipeline.ref}"
                )
                self.logger.info(msg)
                if return_pipeline_object:
                    return project_pipeline
                else:
                    return msg
        except Exception as e:
            self.logger.error(f"Uncaught exception -> {e}")

    def get_last_deployment_pipeline(
        self,
        reference: str = "main",
        team_number: int = 28,
        update_delta_in_hours: int = 4,
        return_pipeline_details: bool = True,
    ) -> ProjectPipeline | list[PipelineDetails] | None:  # pragma: no cover
        """
        in order to limit the amount of records coming back; a time cap is used; so this command only fetches the
        results from pipelines that are updated the last 4 hours (default; can be controlled via the \
        'update_delta_in_hours' variable). If that yields more then 1 result; it will return the last entry.
        """
        self.logger.debug(
            f"Method '{inspect.currentframe().f_code.co_name}' called with arguments: {locals()}"
        )

        date_time = datetime.datetime.now()
        date_time_delta = datetime.timedelta(hours=update_delta_in_hours)

        iso_formatted_delta = (date_time - date_time_delta).isoformat()

        the_project = self.get_project_by_namespace(self.config.PROJECT_NAMESPACE)

        pipelines = the_project.pipelines.list(
            ref=reference,
            get_all=True,
            updated_after=iso_formatted_delta,
        )

        pf = PipelineFilter(pipelines=pipelines)

        if len(pipelines) == 0:
            return None

        if return_pipeline_details:
            return pf.filter_pipelines_return_details(team_number=team_number)
        else:
            return pipelines[0]

    def get_pipeline_by_id(
        self,
        pipeline_id: int | str,
    ) -> ProjectPipeline | None:  # pragma: no cover

        self.logger.debug(
            f"Method '{inspect.currentframe().f_code.co_name}' called with arguments: {locals()}"
        )

        try:
            the_project = self.get_project_by_namespace(self.config.PROJECT_NAMESPACE)

            pipeline = the_project.pipelines.get(pipeline_id)

            self.logger.debug(f"Fetched pipeline with id {pipeline_id}: {pipeline}")
            return pipeline

        except gitlab.exceptions.GitlabGetError as e:
            self.logger.error(
                f"Pipeline with id {pipeline_id} could not be fetched -> {e}"
            )
            return None

    def get_deployment_status(
        self,
        reference: str = "main",
        team_number: int | list[int] = 28,
        fetch_all: bool = False,
        pipeline_id: int | str = None,
    ) -> Tuple[list[str], list[str]]:  # pragma: no cover

        self.logger.debug(
            f"Method '{inspect.currentframe().f_code.co_name}' called with arguments: {locals()}"
        )

        header_list = ["ID", "Name", "Status", "Branch", "Updated", "Url"]
        entry_list = []
        if fetch_all:
            for i in tqdm(
                range(
                    self.config.DEPLOYMENT_RANGE_LOWER,
                    self.config.DEPLOYMENT_RANGE_UPPER + 1,
                ),
                desc="Fetching deployment status",
            ):
                details_obj = self.get_last_deployment_pipeline(
                    reference=reference, team_number=i, return_pipeline_details=True
                )
                if details_obj is not None:
                    entry_list.extend([x.get_entry_list() for x in details_obj])
        else:
            if pipeline_id is None:
                if isinstance(team_number, int):
                    team_number = [team_number]
                if len(team_number) == 1:
                    for each in team_number:
                        details_obj = self.get_last_deployment_pipeline(
                            reference=reference,
                            team_number=each,
                            return_pipeline_details=True,
                        )
                        if details_obj is not None:
                            entry_list.extend([x.get_entry_list() for x in details_obj])
                else:
                    for each in tqdm(
                        team_number,
                        desc="Fetching deployment status",
                    ):
                        details_obj = self.get_last_deployment_pipeline(
                            reference=reference,
                            team_number=each,
                            return_pipeline_details=True,
                        )
                        if details_obj is not None:
                            entry_list.extend([x.get_entry_list() for x in details_obj])
            else:
                pipeline_obj = self.get_pipeline_by_id(pipeline_id=pipeline_id)
                if pipeline_obj is not None:
                    details_obj = PipelineDetails(
                        pipeline_obj.id,
                        pipeline_obj.name,
                        pipeline_obj.ref,
                        pipeline_obj.status,
                        pipeline_obj.web_url,
                        pipeline_obj.updated_at,
                    )
                    entry_list.extend([details_obj.get_entry_list()])

        return header_list, entry_list

    def get_hosts_per_network_providentia(
        self,
    ) -> dict[dict, dict[str, Any] | list[dict[str, Any]]]:

        self.logger.info(f"Fetching all networks...")
        all_networks = self.providentia.environment_networks(
            self.config.PROJECT_VERSION
        )

        target_networks = [
            network
            for network in all_networks["result"]
            if network["actor"] not in ["gt", "for", "rt"] and network["id"] != "vpn_bt"
        ]

        self.logger.info(f"Found {len(target_networks)} networks...")

        host_per_network = defaultdict(dict)

        for network in target_networks:
            host_per_network[network["id"]] = {"network_details": network}

        host_per_network = dict(host_per_network)

        self.logger.info(f"Fetching all hosts...")

        host_inventory = self.providentia.environment_inventory(
            self.config.PROJECT_VERSION
        )

        all_hosts = [
            x
            for x in host_inventory["result"]
            if x["actor_id"] != "gt" or x["actor_id"] != "for"
        ]

        for host in all_hosts:
            try:
                if "hosts" not in host_per_network[host["connection_network"]]:
                    host_per_network[host["connection_network"]]["hosts"] = []

                host_per_network[host["connection_network"]]["hosts"].append(host)
            except KeyError:
                continue

        return dict(sorted(host_per_network.items()))

    def get_tier_assignments_providentia(self):

        tier_regex = re.compile(r"custom_tier.*")

        self.logger.info(f"Fetching inventory from providentia...")

        the_inventory = self.providentia.environment_inventory(
            self.config.PROJECT_VERSION
        )

        ret_dict = defaultdict(list)

        for host in the_inventory["result"]:
            tag_list = sorted(
                list(filter(tier_regex.match, host["tags"])), reverse=True
            )
            if len(tag_list) != 0:
                if len(tag_list) > 1:
                    self.logger.error(
                        f"Found multiple tags for {host['id']} -> {tag_list}; "
                        f"only processing the first (lowest tier!!) entry!!"
                    )
                # match on tier tag
                tier_level = tag_list[0].replace("custom_", "").title()
                if len(tier_level) > 7:
                    # tier with 2 sub-levels do some additional work....
                    top_tier_level = tier_level[:4]
                    sub_level = tier_level[5:7].upper()
                    second_sub_level = tier_level[8:].upper()
                    full_tier_level = (
                        top_tier_level + sub_level + "_" + second_sub_level
                    )
                elif len(tier_level) > 6:
                    # tier with sub-level do some additional work....
                    top_tier_level = tier_level[:4]
                    sub_level = tier_level[-2:].upper()
                    full_tier_level = top_tier_level + sub_level
                else:
                    top_tier_level = tier_level[:4]
                    sub_level = tier_level[-1]
                    full_tier_level = top_tier_level + sub_level
                if host["sequence_total"] is not None:
                    host_id = f"{host['id']}||{host['sequence_total']}"
                else:
                    host_id = host["id"]

                ret_dict[full_tier_level].append({host_id: {"actor": host["actor_id"]}})

        full_dict = dict(ret_dict)

        for key, value in full_dict.items():
            full_dict[key] = sorted(value, key=lambda d: list(d.keys())[0])

        # for key, value in full_dict.items():
        #     full_dict[key] = sorted(value)

        return dict(sorted(full_dict.items()))

    def generate_gitlab_ci(
        self,
        data: dict[str, list[dict[str, dict[str, Any]]]],
        skip_hosts: list[str] = None,
        only_hosts: list[str] = None,
        actor: list[str] = None,
        large_tiers: list[str] = None,
        standalone_tiers: list[str] = None,
        ignore_deploy_order: bool = False,
        reverse_deploy_order: bool = False,
        docker_image_count: int = 1,
        standalone_deployment: bool = False,
        core_level: int = 0,
        nova_version: str = "PRODUCTION",
        windows_tier: str = None,
    ) -> dict[str, list]:

        if skip_hosts is not None and only_hosts is not None:
            if any(skip_hosts) and any(only_hosts):
                self.logger.warning(
                    f"Warning: Both --skip_hosts and --only_hosts provided; --only_hosts takes precedence"
                )
                skip_hosts = []  # only_hosts takes precedence

        if skip_hosts is None:
            skip_hosts = []

        if only_hosts is None:
            only_hosts = []

        if actor is None:
            actor = []

        if large_tiers is None:
            large_tiers = []

        if standalone_tiers is None:
            standalone_tiers = []

        if windows_tier is None:
            windows_tier = ""
        else:
            windows_tier = str(windows_tier)

        self.logger.debug(
            f"Method '{inspect.currentframe().f_code.co_name}' called with arguments: {locals()}"
        )

        tiers = list(data.keys())
        top_level_tiers = list(set(re.sub(r"[a-zA-Z_]*$", "", tier) for tier in tiers))

        # Sort numerically based on the number in "TierX"
        top_level_tiers.sort(key=lambda x: int(re.search(r"\d+", x).group()))

        stages = []
        if core_level > 0:
            non_core_tiers = [
                tier
                for tier in top_level_tiers
                if int(re.sub(r"\D", "", tier)) > core_level
            ]
            if reverse_deploy_order:
                non_core_tiers.reverse()
                stages = non_core_tiers + ["CoreTiers"]
            else:
                stages = ["CoreTiers"] + non_core_tiers
        else:
            stages = top_level_tiers
            if reverse_deploy_order:
                stages.reverse()

        # Prepare GitLab CI/CD pipeline structure
        gitlab_ci = {"stages": stages}

        jobs = {}
        win_host_list_core = []
        windows_core_job_names = []

        for i, tier in enumerate(tiers):

            mode = getenv_str("DEPLOY_MODE", deploy_modes.REDEPLOY)
            top_level_tier = re.sub(r"[a-zA-Z_]*$", "", tier)
            top_level_tier_number = re.sub(r"\D", "", top_level_tier)

            if top_level_tier_number == windows_tier and "CORE" in tier.upper():
                win_sublevel = re.sub(r"\_(.*)", "", tier)
                job_name = win_sublevel.lower()
            else:
                job_name = f"{tier.lower()}"

            if top_level_tier_number == windows_tier and "CORE" in tier.upper():
                deploy_rule = [
                    {
                        "if": f'$REDEPLOY_{top_level_tier.upper()} == "true" && $DEPLOY_MODE != "redeploy" && $DEPLOY_MODE != "deploy"',
                        "when": "on_success",
                    }
                ]
            else:
                deploy_rule = [
                    {
                        "if": f'$REDEPLOY_{top_level_tier.upper()} == "true"',
                        "when": "on_success",
                    }
                ]
            host_list = []
            needs_fat_runner = False
            for host in data[tier]:
                host, host_actor = list(host.items())[0]
                host = host.strip()
                host_actor = host_actor["actor"].strip()

                if host_actor.upper() not in actor and any(actor):
                    continue
                if (
                    (any(only_hosts) and host in only_hosts)
                    or (
                        any(only_hosts)
                        and "||" in host
                        and host.split("||")[0] in only_hosts
                    )
                    or (not any(only_hosts) and host not in skip_hosts)
                ):
                    # Skip team suffix for standalone tiers
                    add_team_suffix = top_level_tier.upper() not in standalone_tiers

                    if add_team_suffix:
                        team_nr = "{0:02d}".format(int(getenv_str("CICD_TEAM", "28")))

                    if "||" in host:
                        needs_fat_runner = (
                            True  # set fat runner due to large host count
                        )
                        hostname, count_str = host.split("||")
                        count = int(count_str)

                        all_hosts = [f"hostname_{i:02}" for i in range(1, count + 1)]

                        grouped = [
                            all_hosts[i : i + self.config.DEPLOYMENT_SEQUENCE_STEP]
                            for i in range(
                                0, len(all_hosts), self.config.DEPLOYMENT_SEQUENCE_STEP
                            )
                        ]

                        for group in grouped:
                            numbers = [int(x.split("_")[-1]) for x in group]

                            tens_groups = defaultdict(list)
                            for n in numbers:
                                tens_groups[n // 10].append(n)

                            group_patterns = []
                            for tens, nums in sorted(tens_groups.items()):
                                nums.sort()
                                if len(nums) == 1:
                                    group_patterns.append(f"{nums[0]:02}")
                                else:
                                    start, end = nums[0], nums[-1]
                                    if start // 10 == end // 10:
                                        group_patterns.append(
                                            f"0[{start % 10}-{end % 10}]"
                                            if start < 10
                                            else f"{start//10}[{start%10}-{end%10}]"
                                        )
                                    else:
                                        group_patterns.append(
                                            "|".join(f"{n:02}" for n in nums)
                                        )

                            if len(group_patterns) == 1:
                                pattern = group_patterns[0]
                            else:
                                pattern = "(" + "|".join(group_patterns) + ")"

                            entry = f"{hostname}_{pattern}"
                            if add_team_suffix:
                                entry += f"_t{team_nr}"
                            host_list.append(entry)
                    else:
                        entry = host
                        if add_team_suffix:
                            entry += f"_t{team_nr}"
                        host_list.append(entry)

                    if top_level_tier_number == windows_tier and "CORE" in tier.upper():
                        win_host_list_core.append({"host": entry, "actor": host_actor})

            if top_level_tier.upper() in large_tiers or needs_fat_runner:
                job_tag = self.config.TAG_RUNNER_FAT
            else:
                job_tag = self.config.TAG_RUNNER_SLIM

            if docker_image_count >= 1:
                random_image_number = random.randint(1, docker_image_count)
                docker_image = f"{self.config.NEXUS_HOST}/{self.config.PROJECT_VERSION}-cicd-image-{random_image_number}-{nova_version.lower()}:latest"
            else:
                docker_image = self.config.EXECUTOR_DOCKER_IMAGE

            job_stage = (
                top_level_tier
                if core_level == 0 or int(top_level_tier_number) > core_level
                else "CoreTiers"
            )

            job_script = [
                f'echo "Deploying $HOST..."',
                f"bash /app/deploy.sh $HOST $SKIP_VULNS $DEPLOY_MODE $SNAPSHOT_NAME",
            ]

            jobs[job_name] = {
                "stage": job_stage,
                "image": docker_image,
                "tags": [job_tag],
                "before_script": [
                    "sudo bash /app/move_needed_files.sh",
                    'echo "$VAULT_PASS" >> /app/.vault_pass',
                ],
                "rules": deploy_rule,
                "script": job_script,
                "parallel": {"matrix": [{"HOST": host_list}]},
                "dependencies": [],  # no artifacts needed....
                "retry": {
                    "max": 2,
                    "when": [
                        "runner_system_failure",
                        "stuck_or_timeout_failure",
                        "script_failure",
                        "api_failure",
                    ],
                    "exit_codes": [1, 137],
                },
            }

            # Track windows CORE jobs
            if top_level_tier_number == windows_tier and "CORE" in tier.upper():
                windows_core_job_names.append(job_name)

            if ignore_deploy_order:
                continue
            else:
                if not reverse_deploy_order and i > 0:
                    my_job_index = list(jobs.keys()).index(job_name)
                    previous_job = list(jobs.keys())[my_job_index - 1]

                    current_job_is_core = job_stage == "CoreTiers"
                    previous_job_is_core = jobs[previous_job]["stage"] == "CoreTiers"

                    if (current_job_is_core and previous_job_is_core) or (
                        not current_job_is_core and previous_job_is_core
                    ):
                        jobs[job_name]["needs"] = [
                            {
                                "job": (
                                    "dummy_job" if my_job_index == 0 else previous_job
                                ),
                                "optional": True,
                            }
                        ]
                    elif (
                        not current_job_is_core
                        and not previous_job_is_core
                        and job_stage == jobs[previous_job]["stage"]
                    ):
                        jobs[job_name]["needs"] = [
                            {
                                "job": previous_job,
                                "optional": True,
                            }
                        ]

        # windows
        if windows_tier != "" and win_host_list_core == []:
            self.logger.warning(
                f"Tier {windows_tier} defined as Windows tier, but no Windows core hosts found, check your tier assignments"
            )

        win_core_actors = defaultdict(list)
        for entry in win_host_list_core:
            win_core_actors[entry["actor"]].append(entry["host"])
        for act in win_core_actors:
            self.logger.debug(
                f"Windows Core Hosts to be deployed for actor {act}: {win_core_actors[act]}"
            )

            job_name = ("tier" + str(windows_tier) + "_core").lower()
            jobs[job_name] = {}

            if f"TIER{windows_tier}" in large_tiers:
                job_tag = self.config.TAG_RUNNER_FAT

            else:
                job_tag = self.config.TAG_RUNNER_SLIM

            job_script = [
                f'echo "Deploying $HOST..."',
                f"bash /app/order.sh --skip-vulns $SKIP_VULNS $HOST",
            ]

            jobs[job_name] = {
                "stage": f"Tier{windows_tier}",
                "image": docker_image,
                "tags": [job_tag],
                "before_script": [
                    "sudo bash /app/move_needed_files.sh",
                    'echo "$VAULT_PASS" >> /app/.vault_pass',
                ],
                "rules": [
                    {
                        "if": f'$REDEPLOY_TIER{windows_tier} == "true" && ($DEPLOY_MODE == "redeploy" || $DEPLOY_MODE == "deploy")',
                        "when": "on_success",
                    }
                ],
                "script": job_script,
                "dependencies": [],  # no artifacts needed....
                "retry": {
                    "max": 2,
                    "when": [
                        "runner_system_failure",
                        "stuck_or_timeout_failure",
                        "script_failure",
                        "api_failure",
                    ],
                    "exit_codes": [1, 137],
                },
            }
            jobs[job_name]["parallel"] = {
                "matrix": [
                    {"HOST": " ".join(win_core_actors[act])} for act in win_core_actors
                ]
            }

        # Add needs for windows core job if core_level is set
        if (
            not ignore_deploy_order
            and not reverse_deploy_order
            and core_level > 0
            and windows_tier != ""
        ):
            win_job_name = ("tier" + str(windows_tier) + "_core").lower()
            if win_job_name in jobs:
                last_core_job = None
                for job_name, job_config in jobs.items():
                    if job_config["stage"] == "CoreTiers":
                        last_core_job = job_name

                if last_core_job:
                    jobs[win_job_name]["needs"] = [
                        {
                            "job": last_core_job,
                            "optional": True,
                        }
                    ]

        # Add needs for non-CORE windows jobs to depend on windows core job
        if (
            not ignore_deploy_order
            and not reverse_deploy_order
            and windows_tier != ""
            and len(windows_core_job_names) > 0
        ):
            win_job_name = ("tier" + str(windows_tier) + "_core").lower()
            if win_job_name in jobs:
                # Find the first non-CORE windows job in the same tier
                # Non-CORE jobs are those in the windows tier that are NOT in the windows_core_job_names list
                first_non_core_win_job = None

                for job_name, job_config in jobs.items():
                    if (
                        job_config["stage"] == f"Tier{windows_tier}"
                        and job_name != win_job_name
                        and job_name not in windows_core_job_names
                    ):
                        if first_non_core_win_job is None:
                            first_non_core_win_job = job_name
                            break

                if first_non_core_win_job:
                    # Add the windows core job as a dependency
                    if "needs" not in jobs[first_non_core_win_job]:
                        jobs[first_non_core_win_job]["needs"] = []

                    jobs[first_non_core_win_job]["needs"].append(
                        {
                            "job": win_job_name,
                            "optional": True,
                        }
                    )

        if not ignore_deploy_order and reverse_deploy_order:
            job_keys = list(jobs.keys())
            # Filter out the windows core job since it has different rules and may not exist
            win_core_job_name = (
                ("tier" + str(windows_tier) + "_core").lower()
                if windows_tier != ""
                else None
            )
            if win_core_job_name and win_core_job_name in job_keys:
                job_keys.remove(win_core_job_name)

            for idx, job_key in enumerate(job_keys):
                if idx < len(job_keys) - 1:  # Not the last job
                    jobs[job_key]["needs"] = [
                        {
                            "job": job_keys[idx + 1],
                            "optional": True,
                        }
                    ]

        if not ignore_deploy_order and not reverse_deploy_order and core_level > 0:
            last_core_job = None
            for job_name, job_config in jobs.items():
                if job_config["stage"] == "CoreTiers":
                    last_core_job = job_name

            if last_core_job:
                non_core_stages_first_jobs = {}
                for job_name, job_config in jobs.items():
                    stage = job_config["stage"]
                    if stage != "CoreTiers" and stage not in non_core_stages_first_jobs:
                        non_core_stages_first_jobs[stage] = job_name

                for stage, first_job in non_core_stages_first_jobs.items():
                    if "needs" not in jobs[first_job]:
                        jobs[first_job]["needs"] = [
                            {
                                "job": last_core_job,
                                "optional": True,
                            }
                        ]

        # Merge jobs into GitLab CI structure
        gitlab_ci.update(jobs)

        return gitlab_ci

    def get_gitlab_ci_from_tier_assignment(
        self,
        skip_hosts: list[str] = None,
        only_hosts: list[str] = None,
        actor: list[str] = None,
        large_tiers: list[str] = None,
        standalone_tiers: list[str] = None,
        ignore_deploy_order: bool = False,
        reverse_deploy_order: bool = False,
        docker_image_count: int = 1,
        standalone_deployment: bool = False,
        core_level: int = 0,
        nova_version: str = "PRODUCTION",
        windows_tier: str = None,
    ) -> dict[str, list[Any]]:

        if not standalone_deployment:
            tier_assignments = self.get_tier_assignments_providentia()
        else:
            tier_assignments = {"Tier0a": [{x: {"actor": "SA"}} for x in only_hosts]}
            standalone_tiers = ["TIER0"]
            large_tiers = ["TIER0"]

        gitlab_ci = self.generate_gitlab_ci(
            data=tier_assignments,
            skip_hosts=skip_hosts,
            only_hosts=only_hosts,
            actor=actor,
            large_tiers=large_tiers,
            standalone_tiers=standalone_tiers,
            ignore_deploy_order=ignore_deploy_order,
            reverse_deploy_order=reverse_deploy_order,
            docker_image_count=docker_image_count,
            standalone_deployment=standalone_deployment,
            core_level=core_level,
            nova_version=nova_version,
            windows_tier=windows_tier,
        )

        return gitlab_ci

    def get_pipeline_schedule(
        self, schedule_id: int = None, fetch_all: bool = False
    ) -> ProjectPipelineSchedule | list[ProjectPipelineSchedule]:
        pass

        the_project = self.get_project_by_namespace(self.config.PROJECT_NAMESPACE)

        if fetch_all:
            return the_project.pipelineschedules.list(get_all=True)
        else:
            return the_project.pipelineschedules.get(schedule_id)

    def get_pipeline_schedule_status(
        self, schedule_id: int = None, fetch_all: bool = False
    ) -> Tuple[list[str], list[str]]:
        self.logger.debug(
            f"Method '{inspect.currentframe().f_code.co_name}' called with arguments: {locals()}"
        )

        header_list = [
            "ID",
            "Description",
            "Cron",
            "TZ",
            "Active",
            "Branch",
            "Owner",
            "Next Run",
            "Last run result",
            "Variables",
        ]
        entry_list = []
        all_schedule_objs = []
        if fetch_all:
            all_schedules = self.get_pipeline_schedule(fetch_all=fetch_all)
            all_ids = [x.id for x in all_schedules]

            for each in all_ids:
                all_schedule_objs.append(self.get_pipeline_schedule(schedule_id=each))
        else:
            all_schedule_objs.append(
                self.get_pipeline_schedule(schedule_id=schedule_id)
            )
        all_schedule_objs = [
            PipelineScheduleDetails.from_pipelineschedule_attributes(x.attributes)
            for x in all_schedule_objs
        ]
        entry_list.extend([x.get_entry_list() for x in all_schedule_objs])

        return header_list, entry_list
