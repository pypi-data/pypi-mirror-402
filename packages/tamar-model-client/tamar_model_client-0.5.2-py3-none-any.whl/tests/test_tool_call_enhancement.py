#!/usr/bin/env python3
"""
Tool Call Enhancement æµ‹è¯•è„šæœ¬

æµ‹è¯• Tool Call åŠŸèƒ½çš„å¢å¼ºå®ç°ï¼ŒåŒ…æ‹¬ï¼š
1. ModelResponse çš„ tool_calls å’Œ finish_reason å­—æ®µ
2. ToolCallHelper å·¥å…·ç±»çš„ä¾¿åˆ©æ–¹æ³•
3. ResponseHandler çš„è‡ªåŠ¨æå–åŠŸèƒ½
"""

import asyncio
import json
import logging
import os
import sys
from unittest.mock import Mock

# é…ç½®æµ‹è¯•è„šæœ¬ä¸“ç”¨çš„æ—¥å¿—
test_logger = logging.getLogger('test_tool_call_enhancement')
test_logger.setLevel(logging.INFO)
test_logger.propagate = False

test_handler = logging.StreamHandler()
test_handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))
test_logger.addHandler(test_handler)

logger = test_logger

# å·¥å…·å‡½æ•°å®ç°
def get_weather(location: str) -> str:
    """è·å–æŒ‡å®šåŸå¸‚çš„å¤©æ°”ä¿¡æ¯

    Args:
        location: åŸå¸‚åç§°

    Returns:
        å¤©æ°”ä¿¡æ¯å­—ç¬¦ä¸²
    """
    # æ¨¡æ‹Ÿå¤©æ°”æ•°æ®
    weather_data = {
        "åŒ—äº¬": "åŒ—äº¬ä»Šå¤©æ™´å¤©ï¼Œæ¸©åº¦25Â°Cï¼Œå¾®é£",
        "ä¸Šæµ·": "ä¸Šæµ·ä»Šå¤©å¤šäº‘ï¼Œæ¸©åº¦28Â°Cï¼Œæ¹¿åº¦è¾ƒé«˜",
        "å¹¿å·": "å¹¿å·ä»Šå¤©é˜´å¤©ï¼Œæ¸©åº¦32Â°Cï¼Œæœ‰é›·é˜µé›¨",
        "æ·±åœ³": "æ·±åœ³ä»Šå¤©æ™´å¤©ï¼Œæ¸©åº¦30Â°Cï¼Œç©ºæ°”è´¨é‡è‰¯å¥½",
        "æ­å·": "æ­å·ä»Šå¤©å°é›¨ï¼Œæ¸©åº¦22Â°Cï¼Œå»ºè®®å¸¦ä¼",
        "æˆéƒ½": "æˆéƒ½ä»Šå¤©é˜´å¤©ï¼Œæ¸©åº¦26Â°Cï¼Œç©ºæ°”æ¹¿æ¶¦"
    }

    # é»˜è®¤å¤©æ°”ä¿¡æ¯
    return weather_data.get(location, f"{location}ä»Šå¤©å¤©æ°”æ™´æœ—ï¼Œæ¸©åº¦é€‚å®œ")

# è®¾ç½®æµ‹è¯•ç¯å¢ƒå˜é‡
os.environ['MODEL_MANAGER_SERVER_GRPC_USE_TLS'] = "false"
os.environ['MODEL_MANAGER_SERVER_ADDRESS'] = "localhost:50051"
os.environ['MODEL_MANAGER_SERVER_JWT_SECRET_KEY'] = "model-manager-server-jwt-key"

# å¯¼å…¥å®¢æˆ·ç«¯æ¨¡å—
try:
    from tamar_model_client import TamarModelClient, AsyncTamarModelClient
    from tamar_model_client.schemas import ModelRequest, UserContext
    from tamar_model_client.enums import ProviderType, InvokeType, Channel

    # ä¸ºäº†è°ƒè¯•ï¼Œä¸´æ—¶å¯ç”¨ SDK çš„æ—¥å¿—è¾“å‡º
    os.environ['TAMAR_MODEL_CLIENT_LOG_LEVEL'] = 'INFO'

except ImportError as e:
    logger.error(f"å¯¼å…¥æ¨¡å—å¤±è´¥: {e}")
    sys.exit(1)


def test_model_response_enhancement():
    """æµ‹è¯• ModelResponse å¢å¼ºåŠŸèƒ½"""
    print("\nğŸ“‹ æµ‹è¯• ModelResponse å¢å¼ºåŠŸèƒ½...")

    try:
        from tamar_model_client.schemas.outputs import ModelResponse

        # æµ‹è¯•æœ‰å·¥å…·è°ƒç”¨çš„æƒ…å†µ
        response_with_tools = ModelResponse(
            content="I need to call some tools.",
            tool_calls=[
                {
                    "id": "call_123",
                    "type": "function",
                    "function": {"name": "get_weather", "arguments": '{"location": "Beijing"}'}
                }
            ],
            finish_reason="tool_calls"
        )

        assert response_with_tools.has_tool_calls() is True
        assert len(response_with_tools.tool_calls) == 1
        print("   âœ… æœ‰å·¥å…·è°ƒç”¨çš„æƒ…å†µæµ‹è¯•é€šè¿‡")

        # æµ‹è¯•æ— å·¥å…·è°ƒç”¨çš„æƒ…å†µ
        response_without_tools = ModelResponse(
            content="Here is the answer.",
            finish_reason="stop"
        )

        assert response_without_tools.has_tool_calls() is False
        assert response_without_tools.tool_calls is None
        print("   âœ… æ— å·¥å…·è°ƒç”¨çš„æƒ…å†µæµ‹è¯•é€šè¿‡")

        # æµ‹è¯•ç©ºçš„å·¥å…·è°ƒç”¨åˆ—è¡¨
        response_empty_tools = ModelResponse(
            content="Here is the answer.",
            tool_calls=[],
            finish_reason="stop"
        )

        assert response_empty_tools.has_tool_calls() is False
        print("   âœ… ç©ºå·¥å…·è°ƒç”¨åˆ—è¡¨çš„æƒ…å†µæµ‹è¯•é€šè¿‡")

        print("âœ… ModelResponse å¢å¼ºåŠŸèƒ½æµ‹è¯•å…¨éƒ¨é€šè¿‡")

    except Exception as e:
        print(f"âŒ ModelResponse å¢å¼ºåŠŸèƒ½æµ‹è¯•å¤±è´¥: {str(e)}")


def test_tool_call_helper():
    """æµ‹è¯• ToolCallHelper å·¥å…·ç±»"""
    print("\nğŸ”§ æµ‹è¯• ToolCallHelper å·¥å…·ç±»...")

    try:
        from tamar_model_client import ToolCallHelper
        from tamar_model_client.schemas.outputs import ModelResponse

        # æµ‹è¯•åˆ›å»ºå‡½æ•°å·¥å…·
        tool = ToolCallHelper.create_function_tool(
            name="test_func",
            description="æµ‹è¯•å‡½æ•°",
            parameters={
                "type": "object",
                "properties": {"param1": {"type": "string"}},
                "required": ["param1"]
            }
        )

        assert tool["type"] == "function"
        assert tool["function"]["name"] == "test_func"
        assert tool["function"]["description"] == "æµ‹è¯•å‡½æ•°"
        assert "param1" in tool["function"]["parameters"]["properties"]
        print("   âœ… åˆ›å»ºå‡½æ•°å·¥å…·æµ‹è¯•é€šè¿‡")

        # æµ‹è¯•è§£æå‡½æ•°å‚æ•°
        tool_call = {
            "type": "function",
            "function": {
                "name": "test_func",
                "arguments": '{"location": "Beijing", "unit": "celsius"}'
            }
        }

        args = ToolCallHelper.parse_function_arguments(tool_call)
        assert args["location"] == "Beijing"
        assert args["unit"] == "celsius"
        print("   âœ… è§£æå‡½æ•°å‚æ•°æµ‹è¯•é€šè¿‡")

        # æµ‹è¯•åˆ›å»ºå·¥å…·å“åº”æ¶ˆæ¯
        response_msg = ToolCallHelper.create_tool_response_message(
            "call_123",
            "Tool execution result",
            "test_tool"
        )

        assert response_msg["role"] == "tool"
        assert response_msg["tool_call_id"] == "call_123"
        assert response_msg["content"] == "Tool execution result"
        assert response_msg["name"] == "test_tool"
        print("   âœ… åˆ›å»ºå·¥å…·å“åº”æ¶ˆæ¯æµ‹è¯•é€šè¿‡")

        # æµ‹è¯•æ„å»ºåŒ…å«å·¥å…·å“åº”çš„æ¶ˆæ¯åˆ—è¡¨
        original_messages = [
            {"role": "user", "content": "What's the weather?"}
        ]

        assistant_response = ModelResponse(
            content="I'll check the weather for you.",
            tool_calls=[
                {
                    "id": "call_weather",
                    "type": "function",
                    "function": {"name": "get_weather", "arguments": '{"location": "Beijing"}'}
                }
            ],
            finish_reason="tool_calls"
        )

        tool_responses = [
            {
                "role": "tool",
                "tool_call_id": "call_weather",
                "content": "Beijing: Sunny, 25Â°C"
            }
        ]

        new_messages = ToolCallHelper.build_messages_with_tool_response(
            original_messages, assistant_response, tool_responses
        )

        assert len(new_messages) == 3
        assert new_messages[0]["role"] == "user"
        assert new_messages[1]["role"] == "assistant"
        assert new_messages[1]["tool_calls"] == assistant_response.tool_calls
        assert new_messages[2]["role"] == "tool"
        assert new_messages[2]["tool_call_id"] == "call_weather"
        print("   âœ… æ„å»ºæ¶ˆæ¯åˆ—è¡¨æµ‹è¯•é€šè¿‡")

        print("âœ… ToolCallHelper å·¥å…·ç±»æµ‹è¯•å…¨éƒ¨é€šè¿‡")

    except Exception as e:
        print(f"âŒ ToolCallHelper å·¥å…·ç±»æµ‹è¯•å¤±è´¥: {str(e)}")


def test_response_handler_enhancement():
    """æµ‹è¯• ResponseHandler å¢å¼ºåŠŸèƒ½"""
    print("\nğŸ”„ æµ‹è¯• ResponseHandler å¢å¼ºåŠŸèƒ½...")

    try:
        from tamar_model_client.core.response_handler import ResponseHandler

        # æµ‹è¯• OpenAI æ ¼å¼çš„ tool calls æå–
        mock_grpc_response = Mock()
        mock_grpc_response.content = ""
        mock_grpc_response.usage = None
        mock_grpc_response.error = None
        mock_grpc_response.request_id = "req_123"
        mock_grpc_response.raw_response = json.dumps({
            "choices": [
                {
                    "message": {
                        "tool_calls": [
                            {
                                "id": "call_456",
                                "type": "function",
                                "function": {
                                    "name": "get_weather",
                                    "arguments": '{"location": "Shanghai"}'
                                }
                            }
                        ]
                    },
                    "finish_reason": "tool_calls"
                }
            ]
        })

        response = ResponseHandler.build_model_response(mock_grpc_response)

        assert response.has_tool_calls() is True
        assert response.finish_reason == "tool_calls"
        assert len(response.tool_calls) == 1
        assert response.tool_calls[0]["function"]["name"] == "get_weather"
        print("   âœ… OpenAI æ ¼å¼ tool calls æå–æµ‹è¯•é€šè¿‡")

        # æµ‹è¯• Google æ ¼å¼è½¬æ¢
        mock_google_response = Mock()
        mock_google_response.content = ""
        mock_google_response.usage = None
        mock_google_response.error = None
        mock_google_response.request_id = "req_456"
        mock_google_response.raw_response = json.dumps({
            "candidates": [
                {
                    "content": {
                        "parts": [
                            {
                                "functionCall": {
                                    "name": "get_weather",
                                    "args": {"location": "Guangzhou"}
                                }
                            }
                        ]
                    },
                    "finishReason": "STOP"
                }
            ]
        })

        google_response = ResponseHandler.build_model_response(mock_google_response)

        assert google_response.has_tool_calls() is True
        assert google_response.finish_reason == "tool_calls"  # è‡ªåŠ¨è½¬æ¢
        assert len(google_response.tool_calls) == 1

        tool_call = google_response.tool_calls[0]
        assert tool_call["type"] == "function"
        assert tool_call["function"]["name"] == "get_weather"
        assert "call_0_get_weather" in tool_call["id"]

        # éªŒè¯å‚æ•°è½¬æ¢
        args = json.loads(tool_call["function"]["arguments"])
        assert args["location"] == "Guangzhou"
        print("   âœ… Google æ ¼å¼è½¬æ¢æµ‹è¯•é€šè¿‡")

        print("âœ… ResponseHandler å¢å¼ºåŠŸèƒ½æµ‹è¯•å…¨éƒ¨é€šè¿‡")

    except Exception as e:
        print(f"âŒ ResponseHandler å¢å¼ºåŠŸèƒ½æµ‹è¯•å¤±è´¥: {str(e)}")


def test_openai_tool_call():
    """æµ‹è¯• OpenAI Tool Call åœºæ™¯"""
    print("\nğŸ”§ æµ‹è¯• OpenAI Tool Call...")

    try:
        client = TamarModelClient()

        request = ModelRequest(
            provider=ProviderType.AZURE,
            invoke_type=InvokeType.CHAT_COMPLETIONS,
            model="gpt-4o-mini",
            messages=[
                {"role": "user", "content": "åŒ—äº¬ä»Šå¤©å¤©æ°”å¦‚ä½•ï¼Ÿ"}
            ],
            tools=[
                {
                    "type": "function",
                    "name": "get_weather",
                    "description": "è·å–æŒ‡å®šåŸå¸‚çš„å¤©æ°”ä¿¡æ¯",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "location": {
                                "type": "string",
                                "description": "åŸå¸‚åç§°"
                            }
                        },
                        "required": ["location"]
                    },
                    "strict": None
                }
            ],
            tool_choice="auto",
            user_context=UserContext(
                user_id="test_user",
                org_id="test_org",
                client_type="tool_call_test"
            )
        )

        response = client.invoke(request)

        print(f"âœ… OpenAI Tool Call æµ‹è¯•æˆåŠŸ")
        print(f"   å“åº”ç±»å‹: {type(response)}")
        print(f"   æ˜¯å¦æœ‰ tool calls: {response.has_tool_calls()}")
        print(f"   finish_reason: {response.finish_reason}")

        if response.has_tool_calls():
            print(f"   tool_calls æ•°é‡: {len(response.tool_calls)}")
            for i, tool_call in enumerate(response.tool_calls):
                function_name = tool_call['function']['name']
                print(f"   å·¥å…· {i+1}: {function_name}")
                from tamar_model_client import ToolCallHelper
                args = ToolCallHelper.parse_function_arguments(tool_call)
                print(f"   å‚æ•°: {args}")

                # æ¼”ç¤ºå®é™…å·¥å…·å‡½æ•°è°ƒç”¨
                if function_name == "get_weather":
                    result = get_weather(args['location'])
                    print(f"   æ‰§è¡Œç»“æœ: {result}")

        print(f"   å“åº”å†…å®¹: {str(response.content)[:200]}...")

    except Exception as e:
        print(f"âŒ OpenAI Tool Call æµ‹è¯•å¤±è´¥: {str(e)}")


def test_google_tool_call():
    """æµ‹è¯• Google Tool Call åœºæ™¯"""
    print("\nğŸ”§ æµ‹è¯• Google Tool Call...")

    try:
        client = TamarModelClient()

        request = ModelRequest(
            provider=ProviderType.GOOGLE,
            invoke_type=InvokeType.GENERATION,
            model="tamar-google-gemini-flash-lite",
            contents=[
                {"role": "user", "parts": [{"text": "ä¸Šæµ·ä»Šå¤©å¤©æ°”å¦‚ä½•ï¼Ÿ"}]}
            ],
            config={
                "tools": [
                    {
                        "functionDeclarations": [
                            {
                                "name": "get_weather",
                                "description": "è·å–æŒ‡å®šåŸå¸‚çš„å¤©æ°”ä¿¡æ¯",
                                "parameters": {
                                    "type": "object",
                                    "properties": {
                                        "location": {
                                            "type": "string",
                                            "description": "åŸå¸‚åç§°"
                                        }
                                    },
                                    "required": ["location"]
                                }
                            }
                        ]
                    }
                ]
            },
            user_context=UserContext(
                user_id="test_user",
                org_id="test_org",
                client_type="google_tool_test"
            )
        )

        response = client.invoke(request)

        print(f"âœ… Google Tool Call æµ‹è¯•æˆåŠŸ")
        print(f"   å“åº”ç±»å‹: {type(response)}")
        print(f"   æ˜¯å¦æœ‰ tool calls: {response.has_tool_calls()}")
        print(f"   finish_reason: {response.finish_reason}")

        if response.has_tool_calls():
            print(f"   tool_calls æ•°é‡: {len(response.tool_calls)}")
            for i, tool_call in enumerate(response.tool_calls):
                function_name = tool_call['function']['name']
                print(f"   å·¥å…· {i+1}: {function_name}")
                from tamar_model_client import ToolCallHelper
                args = ToolCallHelper.parse_function_arguments(tool_call)
                print(f"   å‚æ•°: {args}")

                # æ¼”ç¤ºå®é™…å·¥å…·å‡½æ•°è°ƒç”¨
                if function_name == "get_weather":
                    result = get_weather(args['location'])
                    print(f"   æ‰§è¡Œç»“æœ: {result}")

        print(f"   å“åº”å†…å®¹: {str(response.content)[:200]}...")

    except Exception as e:
        print(f"âŒ Google Tool Call æµ‹è¯•å¤±è´¥: {str(e)}")


async def test_async_tool_call_workflow():
    """æµ‹è¯•å¼‚æ­¥å·¥å…·è°ƒç”¨å·¥ä½œæµç¨‹"""
    print("\nğŸ”„ æµ‹è¯•å¼‚æ­¥å·¥å…·è°ƒç”¨å·¥ä½œæµç¨‹...")

    try:
        from tamar_model_client import ToolCallHelper

        async with AsyncTamarModelClient() as client:
            # 1. å‘é€å¸¦å·¥å…·çš„è¯·æ±‚
            initial_messages = [
                {"role": "user", "content": "æ·±åœ³ä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ"}
            ]

            request = ModelRequest(
                provider=ProviderType.AZURE,
                invoke_type=InvokeType.CHAT_COMPLETIONS,
                model="gpt-4o-mini",
                messages=initial_messages,
                tools=[
                    {
                        "type": "function",
                        "name": "get_weather",
                        "description": "è·å–å¤©æ°”ä¿¡æ¯",
                        "parameters": {
                            "type": "object",
                            "properties": {
                                "location": {"type": "string", "description": "åŸå¸‚åç§°"}
                            },
                            "required": ["location"]
                        },
                        "strict": None
                    }
                ],
                tool_choice="auto",
                user_context=UserContext(
                    user_id="async_test_user",
                    org_id="test_org",
                    client_type="async_tool_test"
                )
            )

            # å‘é€åˆå§‹è¯·æ±‚
            response = await client.invoke(request)

            print(f"   æ­¥éª¤ 1: åˆå§‹è¯·æ±‚å®Œæˆ")
            print(f"   æ˜¯å¦éœ€è¦å·¥å…·è°ƒç”¨: {response.has_tool_calls()}")

            if response.has_tool_calls():
                # 2. æ‰§è¡Œå·¥å…·å‡½æ•°
                tool_responses = []
                for tool_call in response.tool_calls:
                    function_name = tool_call["function"]["name"]
                    args = ToolCallHelper.parse_function_arguments(tool_call)

                    # æ ¹æ®å‡½æ•°åè°ƒç”¨å¯¹åº”çš„å·¥å…·å‡½æ•°
                    if function_name == "get_weather":
                        weather_result = get_weather(args['location'])
                    else:
                        weather_result = f"æœªçŸ¥å‡½æ•°: {function_name}"

                    tool_response = ToolCallHelper.create_tool_response_message(
                        tool_call["id"],
                        weather_result,
                        function_name
                    )
                    tool_responses.append(tool_response)

                # 3. æ„å»ºåŒ…å«å·¥å…·å“åº”çš„æ–°æ¶ˆæ¯åˆ—è¡¨
                new_messages = ToolCallHelper.build_messages_with_tool_response(
                    initial_messages,
                    response,
                    tool_responses
                )

                # 4. å‘é€åŒ…å«å·¥å…·å“åº”çš„åç»­è¯·æ±‚
                follow_up_request = ModelRequest(
                    provider=ProviderType.AZURE,
                    invoke_type=InvokeType.CHAT_COMPLETIONS,
                    model="gpt-4o-mini",
                    messages=new_messages,
                    user_context=UserContext(
                        user_id="async_test_user",
                        org_id="test_org",
                        client_type="async_tool_test"
                    )
                )

                final_response = await client.invoke(follow_up_request)

                print(f"   æ­¥éª¤ 2: å·¥å…·è°ƒç”¨æ¨¡æ‹Ÿå®Œæˆ")
                print(f"   æ­¥éª¤ 3: æœ€ç»ˆå›å¤ç”Ÿæˆå®Œæˆ")
                print(f"   æœ€ç»ˆå›å¤: {final_response.content[:100]}...")

                print(f"âœ… å¼‚æ­¥å·¥å…·è°ƒç”¨å·¥ä½œæµç¨‹æµ‹è¯•æˆåŠŸ")
            else:
                print(f"   æ¨¡å‹æ²¡æœ‰è¯·æ±‚å·¥å…·è°ƒç”¨ï¼Œç›´æ¥å›å¤: {response.content}")

    except Exception as e:
        print(f"âŒ å¼‚æ­¥å·¥å…·è°ƒç”¨å·¥ä½œæµç¨‹æµ‹è¯•å¤±è´¥: {str(e)}")


async def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ Tool Call Enhancement åŠŸèƒ½æµ‹è¯•")
    print("=" * 50)

    try:
        # å•å…ƒæµ‹è¯•
        print("\nğŸ“‹ è¿è¡Œå•å…ƒæµ‹è¯•...")
        test_model_response_enhancement()
        test_tool_call_helper()
        test_response_handler_enhancement()
        print("\nâœ… æ‰€æœ‰å•å…ƒæµ‹è¯•é€šè¿‡")

        # çœŸå®åœºæ™¯æµ‹è¯•
        print("\nğŸŒ è¿è¡ŒçœŸå®åœºæ™¯æµ‹è¯•...")
        test_openai_tool_call()
        test_google_tool_call()
        await test_async_tool_call_workflow()

        print("\nâœ… æ‰€æœ‰æµ‹è¯•å®Œæˆ")

    except KeyboardInterrupt:
        print("\nâš ï¸ æµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•æ‰§è¡Œå‡ºé”™: {e}")
    finally:
        print("ğŸ æµ‹è¯•ç¨‹åºå·²é€€å‡º")


if __name__ == "__main__":
    asyncio.run(main())