#!/usr/bin/env python3
"""
ç®€åŒ–ç‰ˆçš„ Google/Azure åœºæ™¯æµ‹è¯•è„šæœ¬
åªä¿ç•™åŸºæœ¬è°ƒç”¨å’Œæ‰“å°åŠŸèƒ½
"""

import asyncio
import json
import logging
import os
import sys
import time
import threading
from concurrent.futures import ThreadPoolExecutor
from typing import List, Dict, Tuple

# é…ç½®æµ‹è¯•è„šæœ¬ä¸“ç”¨çš„æ—¥å¿—
# ä½¿ç”¨ç‰¹å®šçš„loggeråç§°ï¼Œé¿å…å½±å“å®¢æˆ·ç«¯æ—¥å¿—
test_logger = logging.getLogger('test_google_azure_final')
test_logger.setLevel(logging.INFO)
test_logger.propagate = False  # ä¸ä¼ æ’­åˆ°æ ¹logger

# åˆ›å»ºæµ‹è¯•è„šæœ¬ä¸“ç”¨çš„handler
test_handler = logging.StreamHandler()
test_handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))
test_logger.addHandler(test_handler)

logger = test_logger

os.environ['MODEL_MANAGER_SERVER_GRPC_USE_TLS'] = "false"
os.environ['MODEL_MANAGER_SERVER_ADDRESS'] = "localhost:50052"
os.environ['MODEL_MANAGER_SERVER_JWT_SECRET_KEY'] = "model-manager-server-jwt-key"

# å¯¼å…¥å®¢æˆ·ç«¯æ¨¡å—
try:
    from tamar_model_client import TamarModelClient, AsyncTamarModelClient
    from tamar_model_client.schemas import ModelRequest, UserContext
    from tamar_model_client.enums import ProviderType, InvokeType, Channel
    from google.genai import types

    # ä¸ºäº†è°ƒè¯•ï¼Œä¸´æ—¶å¯ç”¨ SDK çš„æ—¥å¿—è¾“å‡º
    # æ³¨æ„ï¼šè¿™ä¼šè¾“å‡º JSON æ ¼å¼çš„æ—¥å¿—
    import os

    os.environ['TAMAR_MODEL_CLIENT_LOG_LEVEL'] = 'INFO'

except ImportError as e:
    logger.error(f"å¯¼å…¥æ¨¡å—å¤±è´¥: {e}")
    sys.exit(1)


def test_google_ai_studio():
    """æµ‹è¯• Google AI Studio"""
    print("\nğŸ” æµ‹è¯• Google AI Studio...")

    try:
        client = TamarModelClient()

        request = ModelRequest(
            provider=ProviderType.GOOGLE,
            channel=Channel.AI_STUDIO,
            invoke_type=InvokeType.GENERATION,
            model="tamar-google-gemini-flash-lite",
            contents=[
                {"role": "user", "parts": [{"text": "Hello, how are you?"}]}
            ],
            user_context=UserContext(
                user_id="test_user",
                org_id="test_org",
                client_type="test_client"
            ),
            config={
                "temperature": 0.7,
                "maxOutputTokens": 100
            }
        )

        response = client.invoke(request)
        print(response)
        print(f"âœ… Google AI Studio æˆåŠŸ")
        print(f"   å“åº”ç±»å‹: {type(response)}")
        print(f"   å“åº”å†…å®¹: {str(response)[:200]}...")

    except Exception as e:
        print(f"âŒ Google AI Studio å¤±è´¥: {str(e)}")


def test_google_vertex_ai():
    """æµ‹è¯• Google Vertex AI"""
    print("\nğŸ” æµ‹è¯• Google Vertex AI...")

    try:
        client = TamarModelClient()

        request = ModelRequest(
            provider=ProviderType.GOOGLE,
            channel=Channel.VERTEXAI,
            invoke_type=InvokeType.GENERATION,
            model="tamar-google-gemini-flash-lite",
            contents=[
                {"role": "user", "parts": [{"text": "What is AI?"}]}
            ],
            user_context=UserContext(
                user_id="test_user",
                org_id="test_org",
                client_type="test_client"
            ),
            config={
                "temperature": 0.5
            }
        )

        response = client.invoke(request)
        print(f"âœ… Google Vertex AI æˆåŠŸ")
        print(f"   å“åº”ç±»å‹: {type(response)}")
        print(f"   å“åº”å†…å®¹: {str(response)[:200]}...")

    except Exception as e:
        print(f"âŒ Google Vertex AI å¤±è´¥: {str(e)}")


def test_azure_openai():
    """æµ‹è¯• Azure OpenAI"""
    print("\nâ˜ï¸  æµ‹è¯• Azure OpenAI...")

    try:
        client = TamarModelClient()

        request = ModelRequest(
            provider=ProviderType.AZURE,
            invoke_type=InvokeType.CHAT_COMPLETIONS,
            model="gpt-4o-mini",
            messages=[
                {"role": "user", "content": "Hello, how are you?"}
            ],
            user_context=UserContext(
                user_id="test_user",
                org_id="test_org",
                client_type="test_client"
            ),
        )

        response = client.invoke(request)
        print(f"âœ… Azure OpenAI æˆåŠŸ")
        print(f"   å“åº”å†…å®¹: {response.model_dump_json()}...")

    except Exception as e:
        print(f"âŒ Azure OpenAI å¤±è´¥: {str(e)}")


def test_google_genai_image_generation():
    """æµ‹è¯• Google GenAI å›¾åƒç”Ÿæˆ"""
    print("\nğŸ¨ æµ‹è¯• Google GenAI å›¾åƒç”Ÿæˆ...")

    try:
        client = TamarModelClient()

        request = ModelRequest(
            provider=ProviderType.GOOGLE,
            channel=Channel.AI_STUDIO,
            invoke_type=InvokeType.IMAGE_GENERATION_GENAI,
            model="imagen-3.0-generate-002",
            prompt="ä¸€åªå¯çˆ±çš„å°çŒ«å’ªåœ¨èŠ±å›­é‡Œç©è€ï¼Œé˜³å…‰é€è¿‡æ ‘å¶æ´’ä¸‹æ–‘é©³çš„å…‰å½±",
            user_context=UserContext(
                user_id="test_user",
                org_id="test_org",
                client_type="test_client"
            )
        )

        response = client.invoke(request, timeout=60000.0)
        print(f"âœ… Google GenAI å›¾åƒç”Ÿæˆè°ƒç”¨æˆåŠŸ")
        print(f"   å“åº”ç±»å‹: {type(response)}")
        
        # æ£€æŸ¥å›¾åƒæ•°æ®ï¼šraw_responseä¸­åº”è¯¥åŒ…å«image_bytes
        has_image_data = False
        if response.raw_response and isinstance(response.raw_response, list):
            for item in response.raw_response:
                if isinstance(item, dict) and 'image_bytes' in item and item['image_bytes']:
                    has_image_data = True
                    print(f"   å›¾åƒæ•°æ®é•¿åº¦: {len(item['image_bytes'])}")
                    break
        
        if has_image_data:
            print(f"   âœ… å›¾åƒç”ŸæˆæˆåŠŸï¼")
        elif response.content:
            print(f"   æ–‡æœ¬å†…å®¹é•¿åº¦: {len(str(response.content[:200]))}")
        else:
            print(f"   å“åº”å†…å®¹: {str(response)[:200]}...")

    except Exception as e:
        print(f"âŒ Google GenAI å›¾åƒç”Ÿæˆå¤±è´¥: {str(e)}")


def test_google_vertex_ai_image_generation():
    """æµ‹è¯• Google Vertex AI å›¾åƒç”Ÿæˆ (å¯¹æ¯”)"""
    print("\nğŸ¨ æµ‹è¯• Google Vertex AI å›¾åƒç”Ÿæˆ...")

    try:
        client = TamarModelClient()

        request = ModelRequest(
            provider=ProviderType.GOOGLE,
            channel=Channel.VERTEXAI,
            invoke_type=InvokeType.IMAGE_GENERATION,
            model="imagegeneration@006",
            prompt="ä¸€åº§é›„ä¼Ÿçš„é›ªå±±åœ¨é»„æ˜æ—¶åˆ†",
            user_context=UserContext(
                user_id="test_user",
                org_id="test_org",
                client_type="test_client"
            )
        )

        response = client.invoke(request, timeout=60000.0)
        print(f"âœ… Google Vertex AI å›¾åƒç”Ÿæˆè°ƒç”¨æˆåŠŸ")
        print(f"   å“åº”ç±»å‹: {type(response)}")
        
        # æ£€æŸ¥å›¾åƒæ•°æ®ï¼šraw_responseä¸­åº”è¯¥åŒ…å«image_bytes
        has_image_data = False
        if response.raw_response and isinstance(response.raw_response, list):
            for item in response.raw_response:
                if isinstance(item, dict) and 'image_bytes' in item and item['image_bytes']:
                    has_image_data = True
                    print(f"   å›¾åƒæ•°æ®é•¿åº¦: {len(item['image_bytes'])}")
                    break
        
        if has_image_data:
            print(f"   âœ… å›¾åƒç”ŸæˆæˆåŠŸï¼")
        elif response.content:
            print(f"   æ–‡æœ¬å†…å®¹é•¿åº¦: {len(str(response.content[:200]))}")
        else:
            print(f"   å“åº”å†…å®¹: {str(response)[:200]}...")

    except Exception as e:
        print(f"âŒ Google Vertex AI å›¾åƒç”Ÿæˆå¤±è´¥: {str(e)}")


async def test_google_streaming():
    """æµ‹è¯• Google æµå¼å“åº”"""
    print("\nğŸ“¡ æµ‹è¯• Google æµå¼å“åº”...")

    try:
        async with AsyncTamarModelClient() as client:
            request = ModelRequest(
                provider=ProviderType.GOOGLE,
                channel=Channel.AI_STUDIO,
                invoke_type=InvokeType.GENERATION,
                model="tamar-google-gemini-flash-lite",
                contents=[
                    {"role": "user", "parts": [{"text": "Count 1 to 5"}]}
                ],
                user_context=UserContext(
                    user_id="test_user",
                    org_id="test_org",
                    client_type="test_client"
                ),
                stream=True,
                config={
                    "temperature": 0.1,
                    "maxOutputTokens": 50
                }
            )

            response_gen = await client.invoke(request)
            print(f"âœ… Google æµå¼è°ƒç”¨æˆåŠŸ")
            print(f"   å“åº”ç±»å‹: {type(response_gen)}")

            chunk_count = 0
            async for chunk in response_gen:
                chunk_count += 1
                print(f"   æ•°æ®å— {chunk_count}: {type(chunk)} - {chunk.model_dump_json()}...")
                if chunk_count >= 3:  # åªæ˜¾ç¤ºå‰3ä¸ªæ•°æ®å—
                    break

    except Exception as e:
        print(f"âŒ Google æµå¼å“åº”å¤±è´¥: {str(e)}")


async def test_azure_streaming():
    """æµ‹è¯• Azure æµå¼å“åº”"""
    print("\nğŸ“¡ æµ‹è¯• Azure æµå¼å“åº”...")

    try:
        async with AsyncTamarModelClient() as client:
            request = ModelRequest(
                provider=ProviderType.AZURE,
                channel=Channel.OPENAI,
                invoke_type=InvokeType.CHAT_COMPLETIONS,
                model="gpt-4o-mini",
                messages=[
                    {"role": "user", "content": "Count 1 to 5"}
                ],
                user_context=UserContext(
                    user_id="test_user",
                    org_id="test_org",
                    client_type="test_client"
                ),
                stream=True  # æ·»åŠ æµå¼å‚æ•°
            )

            response_gen = await client.invoke(request)
            print(f"âœ… Azure æµå¼è°ƒç”¨æˆåŠŸ")
            print(f"   å“åº”ç±»å‹: {type(response_gen)}")

            chunk_count = 0
            async for chunk in response_gen:
                chunk_count += 1
                print(f"   æ•°æ®å— {chunk_count}: {type(chunk)} - {chunk.model_dump_json()}...")
                if chunk_count >= 3:  # åªæ˜¾ç¤ºå‰3ä¸ªæ•°æ®å—
                    break

    except Exception as e:
        print(f"âŒ Azure æµå¼å“åº”å¤±è´¥: {str(e)}")


async def test_google_genai_image_generation_async():
    """æµ‹è¯•å¼‚æ­¥ Google GenAI å›¾åƒç”Ÿæˆ"""
    print("\nğŸ¨ æµ‹è¯•å¼‚æ­¥ Google GenAI å›¾åƒç”Ÿæˆ...")

    try:
        async with AsyncTamarModelClient() as client:
            request = ModelRequest(
                provider=ProviderType.GOOGLE,
                channel=Channel.AI_STUDIO,
                invoke_type=InvokeType.IMAGE_GENERATION_GENAI,
                model="imagen-3.0-generate-002",
                prompt="ç°ä»£åŸå¸‚å¤œæ™¯ï¼Œéœ“è™¹ç¯é—ªçƒï¼Œç¹åçƒ­é—¹çš„è¡—é“",
                user_context=UserContext(
                    user_id="test_user_async",
                    org_id="test_org",
                    client_type="test_client_async"
                )
            )

            response = await client.invoke(request, timeout=60000.0)
            print(f"âœ… å¼‚æ­¥ Google GenAI å›¾åƒç”Ÿæˆè°ƒç”¨æˆåŠŸ")
            print(f"   å“åº”ç±»å‹: {type(response)}")

            # æ£€æŸ¥å›¾åƒæ•°æ®ï¼šraw_responseä¸­åº”è¯¥åŒ…å«image_bytes
            has_image_data = False
            if response.raw_response and isinstance(response.raw_response, list):
                for item in response.raw_response:
                    if isinstance(item, dict) and 'image_bytes' in item and item['image_bytes']:
                        has_image_data = True
                        print(f"   å›¾åƒæ•°æ®é•¿åº¦: {len(item['image_bytes'])}")
                        break
            
            if has_image_data:
                print(f"   âœ… å›¾åƒç”ŸæˆæˆåŠŸï¼")
            elif response.content:
                print(f"   æ–‡æœ¬å†…å®¹é•¿åº¦: {len(str(response.content[:200]))}")
                print(f"   âœ… å›¾åƒç”ŸæˆæˆåŠŸï¼")
            elif response.error:
                print(f"   é”™è¯¯: {response.error}")
            else:
                print(f"   å“åº”å†…å®¹: {str(response)[:200]}...")

    except Exception as e:
        print(f"âŒ å¼‚æ­¥ Google GenAI å›¾åƒç”Ÿæˆå¤±è´¥: {str(e)}")


async def test_google_vertex_ai_image_generation_async():
    """æµ‹è¯•å¼‚æ­¥ Google Vertex AI å›¾åƒç”Ÿæˆ (å¯¹æ¯”)"""
    print("\nğŸ¨ æµ‹è¯•å¼‚æ­¥ Google Vertex AI å›¾åƒç”Ÿæˆ...")

    try:
        async with AsyncTamarModelClient() as client:
            request = ModelRequest(
                provider=ProviderType.GOOGLE,
                channel=Channel.VERTEXAI,
                invoke_type=InvokeType.IMAGE_GENERATION,
                model="imagegeneration@006",
                prompt="å®é™çš„æ¹–æ³Šå€’æ˜ ç€å¤•é˜³ï¼Œå‘¨å›´ç¯ç»•ç€é’å±±ç»¿æ ‘",
                user_context=UserContext(
                    user_id="test_user_async",
                    org_id="test_org",
                    client_type="test_client_async"
                )
            )

            response = await client.invoke(request, timeout=60000.0)
            print(f"âœ… å¼‚æ­¥ Google Vertex AI å›¾åƒç”Ÿæˆè°ƒç”¨æˆåŠŸ")
            print(f"   å“åº”ç±»å‹: {type(response)}")

            # æ£€æŸ¥å›¾åƒæ•°æ®ï¼šraw_responseä¸­åº”è¯¥åŒ…å«image_bytes
            has_image_data = False
            if response.raw_response and isinstance(response.raw_response, list):
                for item in response.raw_response:
                    if isinstance(item, dict) and 'image_bytes' in item and item['image_bytes']:
                        has_image_data = True
                        print(f"   å›¾åƒæ•°æ®é•¿åº¦: {len(item['image_bytes'])}")
                        break
            
            if has_image_data:
                print(f"   âœ… å›¾åƒç”ŸæˆæˆåŠŸï¼")
            elif response.content:
                print(f"   æ–‡æœ¬å†…å®¹é•¿åº¦: {len(str(response.content))}")
                print(f"   âœ… å›¾åƒç”ŸæˆæˆåŠŸï¼")
            elif response.error:
                print(f"   é”™è¯¯: {response.error}")
            else:
                print(f"   å“åº”å†…å®¹: {str(response)[:200]}...")

    except Exception as e:
        print(f"âŒ å¼‚æ­¥ Google Vertex AI å›¾åƒç”Ÿæˆå¤±è´¥: {str(e)}")


def test_sync_batch_requests():
    """æµ‹è¯•åŒæ­¥æ‰¹é‡è¯·æ±‚"""
    print("\nğŸ“¦ æµ‹è¯•åŒæ­¥æ‰¹é‡è¯·æ±‚...")

    try:
        from tamar_model_client.schemas import BatchModelRequest, BatchModelRequestItem

        with TamarModelClient() as client:
            # æ„å»ºæ‰¹é‡è¯·æ±‚ï¼ŒåŒ…å« Google å’Œ Azure çš„å¤šä¸ªè¯·æ±‚
            batch_request = BatchModelRequest(
                user_context=UserContext(
                    user_id="test_user",
                    org_id="test_org",
                    client_type="test_client"
                ),
                items=[
                    # Google AI Studio è¯·æ±‚
                    BatchModelRequestItem(
                        provider=ProviderType.GOOGLE,
                        invoke_type=InvokeType.GENERATION,
                        model="tamar-google-gemini-flash-lite",
                        contents=[
                            {"role": "user", "parts": [{"text": "Hello from sync batch - Google AI Studio"}]}
                        ],
                        custom_id="sync-google-ai-studio-1",
                    ),
                    # Azure OpenAI è¯·æ±‚
                    BatchModelRequestItem(
                        provider=ProviderType.AZURE,
                        invoke_type=InvokeType.CHAT_COMPLETIONS,
                        model="gpt-4o-mini",
                        messages=[
                            {"role": "user", "content": "Hello from sync batch - Azure OpenAI"}
                        ],
                        custom_id="sync-azure-openai-1",
                    ),
                    # å†æ·»åŠ ä¸€ä¸ª Azure è¯·æ±‚
                    BatchModelRequestItem(
                        provider=ProviderType.AZURE,
                        invoke_type=InvokeType.CHAT_COMPLETIONS,
                        model="gpt-4o-mini",
                        messages=[
                            {"role": "user", "content": "What is 2+2?"}
                        ],
                        custom_id="sync-azure-openai-2",
                    )
                ]
            )

            # æ‰§è¡Œæ‰¹é‡è¯·æ±‚
            batch_response = client.invoke_batch(batch_request)

            print(f"âœ… åŒæ­¥æ‰¹é‡è¯·æ±‚æˆåŠŸ")
            print(f"   è¯·æ±‚æ•°é‡: {len(batch_request.items)}")
            print(f"   å“åº”æ•°é‡: {len(batch_response.responses)}")
            print(f"   æ‰¹é‡è¯·æ±‚ID: {batch_response.request_id}")

            # æ˜¾ç¤ºæ¯ä¸ªå“åº”çš„ç»“æœ
            for i, response in enumerate(batch_response.responses):
                print(f"\n   å“åº” {i + 1}:")
                print(f"   - custom_id: {response.custom_id}")
                print(f"   - å†…å®¹é•¿åº¦: {len(response.content) if response.content else 0}")
                print(f"   - æœ‰é”™è¯¯: {'æ˜¯' if response.error else 'å¦'}")
                if response.content:
                    print(f"   - å†…å®¹é¢„è§ˆ: {response.content[:100]}...")
                if response.error:
                    print(f"   - é”™è¯¯ä¿¡æ¯: {response.error}")
                if response.raw_response:
                    print(f"   - åŸä¿¡æ¯: {json.dumps(response.raw_response)}")

    except Exception as e:
        print(f"âŒ åŒæ­¥æ‰¹é‡è¯·æ±‚å¤±è´¥: {str(e)}")


async def test_batch_requests():
    """æµ‹è¯•å¼‚æ­¥æ‰¹é‡è¯·æ±‚"""
    print("\nğŸ“¦ æµ‹è¯•å¼‚æ­¥æ‰¹é‡è¯·æ±‚...")

    try:
        from tamar_model_client.schemas import BatchModelRequest, BatchModelRequestItem

        async with AsyncTamarModelClient() as client:
            # æ„å»ºæ‰¹é‡è¯·æ±‚ï¼ŒåŒ…å« Google å’Œ Azure çš„å¤šä¸ªè¯·æ±‚
            batch_request = BatchModelRequest(
                user_context=UserContext(
                    user_id="test_user",
                    org_id="test_org",
                    client_type="test_client"
                ),
                items=[
                    # Google AI Studio è¯·æ±‚
                    BatchModelRequestItem(
                        provider=ProviderType.GOOGLE,
                        invoke_type=InvokeType.GENERATION,
                        model="tamar-google-gemini-flash-lite",
                        contents=[
                            {"role": "user", "parts": [{"text": "Hello from Google AI Studio"}]}
                        ],
                        custom_id="google-ai-studio-1",
                    ),
                    # Google Vertex AI è¯·æ±‚
                    BatchModelRequestItem(
                        provider=ProviderType.GOOGLE,
                        channel=Channel.VERTEXAI,
                        invoke_type=InvokeType.GENERATION,
                        model="tamar-google-gemini-flash-lite",
                        contents=[
                            {"role": "user", "parts": [{"text": "Hello from Google Vertex AI"}]}
                        ],
                        custom_id="google-vertex-ai-1",
                    ),
                    # Google GenAI å›¾åƒç”Ÿæˆè¯·æ±‚
                    BatchModelRequestItem(
                        provider=ProviderType.GOOGLE,
                        channel=Channel.AI_STUDIO,
                        invoke_type=InvokeType.IMAGE_GENERATION_GENAI,
                        model="imagen-3.0-generate-002",
                        prompt="ä¸€æœµç¾ä¸½çš„ç«ç‘°èŠ±åœ¨é˜³å…‰ä¸‹ç»½æ”¾",
                        config=types.GenerateImagesConfig(
                            number_of_images=1,
                            aspect_ratio="1:1"
                        ),
                        custom_id="google-genai-image-1",
                    ),
                    # Azure OpenAI è¯·æ±‚
                    BatchModelRequestItem(
                        provider=ProviderType.AZURE,
                        invoke_type=InvokeType.CHAT_COMPLETIONS,
                        model="gpt-4o-mini",
                        messages=[
                            {"role": "user", "content": "Hello from Azure OpenAI"}
                        ],
                        custom_id="azure-openai-1",
                    ),
                    # å†æ·»åŠ ä¸€ä¸ª Azure è¯·æ±‚
                    BatchModelRequestItem(
                        provider=ProviderType.AZURE,
                        invoke_type=InvokeType.CHAT_COMPLETIONS,
                        model="gpt-4o-mini",
                        messages=[
                            {"role": "user", "content": "What is the capital of France?"}
                        ],
                        custom_id="azure-openai-2",
                    )
                ]
            )

            # æ‰§è¡Œæ‰¹é‡è¯·æ±‚
            batch_response = await client.invoke_batch(batch_request)

            print(f"âœ… æ‰¹é‡è¯·æ±‚æˆåŠŸ")
            print(f"   è¯·æ±‚æ•°é‡: {len(batch_request.items)}")
            print(f"   å“åº”æ•°é‡: {len(batch_response.responses)}")
            print(f"   æ‰¹é‡è¯·æ±‚ID: {batch_response.request_id}")

            # æ˜¾ç¤ºæ¯ä¸ªå“åº”çš„ç»“æœ
            for i, response in enumerate(batch_response.responses):
                print(f"\n   å“åº” {i + 1}:")
                print(f"   - custom_id: {response.custom_id}")
                print(f"   - å†…å®¹é•¿åº¦: {len(response.content) if response.content else 0}")
                print(f"   - æœ‰é”™è¯¯: {'æ˜¯' if response.error else 'å¦'}")
                if response.content:
                    print(f"   - å†…å®¹é¢„è§ˆ: {response.content[:100]}...")
                if response.error:
                    print(f"   - é”™è¯¯ä¿¡æ¯: {response.error}")
                if response.raw_response:
                    print(f"   - åŸä¿¡æ¯: {json.dumps(response.raw_response)[:100]}")

    except Exception as e:
        print(f"âŒ æ‰¹é‡è¯·æ±‚å¤±è´¥: {str(e)}")


async def test_image_generation_batch():
    """æµ‹è¯•å›¾åƒç”Ÿæˆæ‰¹é‡è¯·æ±‚ - åŒæ—¶æµ‹è¯• GenAIã€Vertex AI å›¾åƒç”Ÿæˆ"""
    print("\nğŸ–¼ï¸  æµ‹è¯•å›¾åƒç”Ÿæˆæ‰¹é‡è¯·æ±‚...")

    try:
        from tamar_model_client.schemas import BatchModelRequest, BatchModelRequestItem

        async with AsyncTamarModelClient() as client:
            # æ„å»ºå›¾åƒç”Ÿæˆæ‰¹é‡è¯·æ±‚
            batch_request = BatchModelRequest(
                user_context=UserContext(
                    user_id="test_image_batch",
                    org_id="test_org",
                    client_type="image_test_client"
                ),
                items=[
                    # Google GenAI å›¾åƒç”Ÿæˆè¯·æ±‚ 1
                    BatchModelRequestItem(
                        provider=ProviderType.GOOGLE,
                        channel=Channel.AI_STUDIO,
                        invoke_type=InvokeType.IMAGE_GENERATION_GENAI,
                        model="imagen-3.0-generate-002",
                        prompt="ä¸€åªå¯çˆ±çš„å°ç‹—åœ¨å…¬å›­é‡Œå¥”è·‘",
                        config=types.GenerateImagesConfig(
                            number_of_images=1,
                            aspect_ratio="1:1",
                        ),
                        custom_id="genai-dog-1",
                    ),
                    # Google GenAI å›¾åƒç”Ÿæˆè¯·æ±‚ 2
                    BatchModelRequestItem(
                        provider=ProviderType.GOOGLE,
                        channel=Channel.AI_STUDIO,
                        invoke_type=InvokeType.IMAGE_GENERATION_GENAI,
                        model="imagen-3.0-generate-002",
                        prompt="ç¾ä¸½çš„æ¨±èŠ±ç››å¼€åœ¨æ˜¥å¤©çš„å…¬å›­é‡Œ",
                        config=types.GenerateImagesConfig(
                            number_of_images=1,
                            aspect_ratio="16:9"
                        ),
                        custom_id="genai-sakura-1",
                    ),
                    # Google Vertex AI å›¾åƒç”Ÿæˆè¯·æ±‚ 1
                    BatchModelRequestItem(
                        provider=ProviderType.GOOGLE,
                        channel=Channel.VERTEXAI,
                        invoke_type=InvokeType.IMAGE_GENERATION,
                        model="imagen-3.0-generate-002",
                        prompt="å£®ä¸½çš„å±±å³¦åœ¨å¤•é˜³è¥¿ä¸‹æ—¶çš„æ™¯è‰²",
                        number_of_images=1,
                        aspect_ratio="16:9",
                        safety_filter_level="block_some",
                        custom_id="vertex-mountain-1",
                    ),
                    # Google Vertex AI å›¾åƒç”Ÿæˆè¯·æ±‚ 2
                    BatchModelRequestItem(
                        provider=ProviderType.GOOGLE,
                        channel=Channel.VERTEXAI,
                        invoke_type=InvokeType.IMAGE_GENERATION,
                        model="imagen-3.0-generate-002",
                        prompt="å®é™çš„æµ·æ»©ä¸Šæœ‰æ¤°å­æ ‘å’Œæµ·æµª",
                        number_of_images=1,
                        aspect_ratio="1:1",
                        safety_filter_level="block_some",
                        custom_id="vertex-beach-1",
                    )
                ]
            )

            # æ‰§è¡Œæ‰¹é‡å›¾åƒç”Ÿæˆè¯·æ±‚
            print(f"   å‘é€æ‰¹é‡å›¾åƒç”Ÿæˆè¯·æ±‚ (å…±{len(batch_request.items)}ä¸ª)...")
            batch_response = await client.invoke_batch(batch_request, timeout=60000.0)

            print(f"âœ… æ‰¹é‡å›¾åƒç”Ÿæˆè¯·æ±‚æˆåŠŸ")
            print(f"   è¯·æ±‚æ•°é‡: {len(batch_request.items)}")
            print(f"   å“åº”æ•°é‡: {len(batch_response.responses)}")
            print(f"   æ‰¹é‡è¯·æ±‚ID: {batch_response.request_id}")

            # è¯¦ç»†æ˜¾ç¤ºæ¯ä¸ªå›¾åƒç”Ÿæˆç»“æœ
            genai_success = 0
            vertex_success = 0
            total_errors = 0

            for i, response in enumerate(batch_response.responses):
                print(f"\n   å›¾åƒç”Ÿæˆ {i + 1}:")
                print(f"   - custom_id: {response.custom_id}")
                print(f"   - æœ‰é”™è¯¯: {'æ˜¯' if response.error else 'å¦'}")

                if response.error:
                    total_errors += 1
                    print(f"   - é”™è¯¯ä¿¡æ¯: {response.error}")
                else:
                    # æ£€æŸ¥å›¾åƒæ•°æ®ï¼šraw_responseä¸­åº”è¯¥åŒ…å«_image_bytes
                    has_image_data = False
                    if response.raw_response and isinstance(response.raw_response, list):
                        print(f"   - raw_responseç±»å‹: {type(response.raw_response)}, é•¿åº¦: {len(response.raw_response)}")
                        for idx, item in enumerate(response.raw_response):
                            print(f"   - item[{idx}]ç±»å‹: {type(item)}")
                            if isinstance(item, dict):
                                print(f"   - item[{idx}]é”®: {list(item.keys())}")
                                if 'image_bytes' in item:
                                    image_data = item['image_bytes']
                                    if image_data:
                                        has_image_data = True
                                        print(f"   - å›¾åƒæ•°æ®é•¿åº¦: {len(image_data)}")
                                        break
                                    else:
                                        print(f"   - image_byteså­—æ®µä¸ºç©º")
                                elif '_image_bytes' in item:
                                    image_data = item['_image_bytes']
                                    if image_data:
                                        has_image_data = True
                                        print(f"   - å›¾åƒæ•°æ®é•¿åº¦: {len(image_data)}")
                                        break
                                    else:
                                        print(f"   - _image_byteså­—æ®µä¸ºç©º")
                                else:
                                    print(f"   - æ²¡æœ‰æ‰¾åˆ°image_bytesæˆ–è€…_image_byteså­—æ®µ")
                    
                    if has_image_data:
                        print(f"   - âœ… å›¾åƒç”ŸæˆæˆåŠŸï¼")
                    elif response.content:
                        print(f"   - æ–‡æœ¬å†…å®¹é•¿åº¦: {len(str(response.content))}")
                        print(f"   - âœ… å›¾åƒç”ŸæˆæˆåŠŸï¼")
                    else:
                        print(f"   - å“åº”é¢„è§ˆ: {str(response)[:100]}...")
                        print(f"   - âš ï¸ å›¾åƒç”Ÿæˆå¯èƒ½æˆåŠŸä½†æ•°æ®æ ¼å¼å¼‚å¸¸")

                    # ç»Ÿè®¡ä¸åŒç±»å‹çš„æˆåŠŸæ•°ï¼ˆåªè¦æ²¡æœ‰errorå°±ç®—æˆåŠŸï¼‰
                    # å¦‚æœcustom_idå­˜åœ¨ï¼Œä½¿ç”¨å®ƒæ¥åˆ¤æ–­ç±»å‹
                    if response.custom_id:
                        if "genai" in response.custom_id:
                            genai_success += 1
                        elif "vertex" in response.custom_id:
                            vertex_success += 1
                    else:
                        # å¦‚æœcustom_idä¸ºNoneï¼Œæ ¹æ®å“åº”ç´¢å¼•åˆ¤æ–­ç±»å‹
                        # å‰2ä¸ªæ˜¯GenAIè¯·æ±‚ï¼Œå2ä¸ªæ˜¯Vertex AIè¯·æ±‚
                        if i < 2:  # GenAI è¯·æ±‚ (ç´¢å¼• 0, 1)
                            genai_success += 1
                            print(f"   - æ ¹æ®ç´¢å¼•åˆ¤æ–­ä¸ºGenAIè¯·æ±‚")
                        else:  # Vertex AI è¯·æ±‚ (ç´¢å¼• 2, 3)
                            vertex_success += 1
                            print(f"   - æ ¹æ®ç´¢å¼•åˆ¤æ–­ä¸ºVertex AIè¯·æ±‚")

            print(f"\nğŸ“Š å›¾åƒç”Ÿæˆæ‰¹é‡æµ‹è¯•ç»Ÿè®¡:")
            print(f"   - GenAI å›¾åƒç”ŸæˆæˆåŠŸ: {genai_success}/2")
            print(f"   - Vertex AI å›¾åƒç”ŸæˆæˆåŠŸ: {vertex_success}/2")
            print(f"   - æ€»é”™è¯¯æ•°: {total_errors}")

    except Exception as e:
        print(f"âŒ æ‰¹é‡å›¾åƒç”Ÿæˆè¯·æ±‚å¤±è´¥: {str(e)}")


def test_concurrent_requests(num_requests: int = 150):
    """æµ‹è¯•å¹¶å‘è¯·æ±‚
    
    Args:
        num_requests: è¦å‘é€çš„æ€»è¯·æ±‚æ•°ï¼Œé»˜è®¤150ä¸ª
    """
    print(f"\nğŸš€ æµ‹è¯•å¹¶å‘è¯·æ±‚ ({num_requests} ä¸ªè¯·æ±‚)...")

    # ç»Ÿè®¡å˜é‡
    total_requests = 0
    successful_requests = 0
    failed_requests = 0
    request_times: List[float] = []
    errors: Dict[str, int] = {}

    # çº¿ç¨‹å®‰å…¨çš„é”
    stats_lock = threading.Lock()

    def make_single_request(request_id: int) -> Tuple[bool, float, str]:
        """æ‰§è¡Œå•ä¸ªè¯·æ±‚å¹¶è¿”å›ç»“æœ
        
        Returns:
            (success, duration, error_msg)
        """
        start_time = time.time()
        try:
            # æ¯ä¸ªçº¿ç¨‹åˆ›å»ºè‡ªå·±çš„å®¢æˆ·ç«¯å®ä¾‹
            client = TamarModelClient()

            # Google Vertex AI
            request = ModelRequest(
                provider=ProviderType.GOOGLE,
                channel=Channel.VERTEXAI,
                invoke_type=InvokeType.GENERATION,
                model="tamar-google-gemini-flash-lite",
                contents="1+1ç­‰äºå‡ ï¼Ÿ",
                user_context=UserContext(
                    user_id=f"{os.environ.get('INSTANCE_ID', '0')}_{request_id:03d}",
                    org_id="test_org",
                    client_type="concurrent_test"
                ),
                config={"temperature": 0.1}
            )

            response = client.invoke(request, timeout=300000.0)
            duration = time.time() - start_time
            return (True, duration, "")

        except Exception as e:
            duration = time.time() - start_time
            error_msg = str(e)
            return (False, duration, error_msg)

    def worker(request_id: int):
        """å·¥ä½œçº¿ç¨‹å‡½æ•°"""
        nonlocal total_requests, successful_requests, failed_requests

        success, duration, error_msg = make_single_request(request_id)

        with stats_lock:
            total_requests += 1
            request_times.append(duration)

            if success:
                successful_requests += 1
            else:
                failed_requests += 1
                # ç»Ÿè®¡é”™è¯¯ç±»å‹
                error_type = error_msg.split(':')[0] if ':' in error_msg else error_msg[:50]
                errors[error_type] = errors.get(error_type, 0) + 1

            # æ¯20ä¸ªè¯·æ±‚è¾“å‡ºä¸€æ¬¡è¿›åº¦
            if total_requests % 20 == 0:
                print(
                    f"   è¿›åº¦: {total_requests}/{num_requests} (æˆåŠŸ: {successful_requests}, å¤±è´¥: {failed_requests})")

    # ä½¿ç”¨çº¿ç¨‹æ± æ‰§è¡Œå¹¶å‘è¯·æ±‚
    start_time = time.time()

    # ä½¿ç”¨çº¿ç¨‹æ± ï¼Œæœ€å¤š50ä¸ªå¹¶å‘çº¿ç¨‹
    with ThreadPoolExecutor(max_workers=50) as executor:
        # æäº¤æ‰€æœ‰ä»»åŠ¡
        futures = [executor.submit(worker, i) for i in range(num_requests)]

        # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
        for future in futures:
            future.result()

    total_duration = time.time() - start_time

    # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
    avg_request_time = sum(request_times) / len(request_times) if request_times else 0
    min_request_time = min(request_times) if request_times else 0
    max_request_time = max(request_times) if request_times else 0

    # è¾“å‡ºç»“æœ
    print(f"\nğŸ“Š å¹¶å‘æµ‹è¯•ç»“æœ:")
    print(f"   æ€»è¯·æ±‚æ•°: {total_requests}")
    print(f"   æˆåŠŸè¯·æ±‚: {successful_requests} ({successful_requests / total_requests * 100:.1f}%)")
    print(f"   å¤±è´¥è¯·æ±‚: {failed_requests} ({failed_requests / total_requests * 100:.1f}%)")
    print(f"   æ€»è€—æ—¶: {total_duration:.2f} ç§’")
    print(f"   å¹³å‡QPS: {total_requests / total_duration:.2f}")
    print(f"\n   è¯·æ±‚è€—æ—¶ç»Ÿè®¡:")
    print(f"   - å¹³å‡: {avg_request_time:.3f} ç§’")
    print(f"   - æœ€å°: {min_request_time:.3f} ç§’")
    print(f"   - æœ€å¤§: {max_request_time:.3f} ç§’")

    if errors:
        print(f"\n   é”™è¯¯ç»Ÿè®¡:")
        for error_type, count in sorted(errors.items(), key=lambda x: x[1], reverse=True):
            print(f"   - {error_type}: {count} æ¬¡")

    return {
        "total": total_requests,
        "successful": successful_requests,
        "failed": failed_requests,
        "duration": total_duration,
        "qps": total_requests / total_duration
    }


async def test_async_concurrent_requests(num_requests: int = 150):
    """æµ‹è¯•å¼‚æ­¥å¹¶å‘è¯·æ±‚
    
    Args:
        num_requests: è¦å‘é€çš„æ€»è¯·æ±‚æ•°ï¼Œé»˜è®¤150ä¸ª
    """
    print(f"\nğŸš€ æµ‹è¯•å¼‚æ­¥å¹¶å‘è¯·æ±‚ ({num_requests} ä¸ªè¯·æ±‚)...")

    # ç»Ÿè®¡å˜é‡
    total_requests = 0
    successful_requests = 0
    failed_requests = 0
    request_times: List[float] = []
    errors: Dict[str, int] = {}
    trace_id = "88888888888888888333333888888883333388888"

    # å¼‚æ­¥é”
    stats_lock = asyncio.Lock()

    async def make_single_async_request(client: AsyncTamarModelClient, request_id: int) -> Tuple[bool, float, str]:
        """æ‰§è¡Œå•ä¸ªå¼‚æ­¥è¯·æ±‚å¹¶è¿”å›ç»“æœ
        
        Returns:
            (success, duration, error_msg)
        """
        start_time = time.time()
        try:
            # æ ¹æ®è¯·æ±‚IDé€‰æ‹©ä¸åŒçš„providerï¼Œä»¥å¢åŠ æµ‹è¯•å¤šæ ·æ€§
            # Google Vertex AI
            request = ModelRequest(
                provider=ProviderType.GOOGLE,
                channel=Channel.VERTEXAI,
                invoke_type=InvokeType.GENERATION,
                model="tamar-google-gemini-flash-lite",
                contents="1+1ç­‰äºå‡ ï¼Ÿ",
                user_context=UserContext(
                    user_id=f"{os.environ.get('INSTANCE_ID', '0')}_{request_id:03d}",
                    org_id="test_org",
                    client_type="async_concurrent_test"
                ),
                config={"temperature": 0.1}
            )

            response = await client.invoke(request, timeout=300000.0, request_id=trace_id)
            duration = time.time() - start_time
            return (True, duration, "")

        except Exception as e:
            duration = time.time() - start_time
            error_msg = str(e)
            return (False, duration, error_msg)

    async def async_worker(client: AsyncTamarModelClient, request_id: int):
        """å¼‚æ­¥å·¥ä½œåç¨‹"""
        nonlocal total_requests, successful_requests, failed_requests

        success, duration, error_msg = await make_single_async_request(client, request_id)

        async with stats_lock:
            total_requests += 1
            request_times.append(duration)

            if success:
                successful_requests += 1
            else:
                failed_requests += 1
                # ç»Ÿè®¡é”™è¯¯ç±»å‹
                error_type = error_msg.split(':')[0] if ':' in error_msg else error_msg[:50]
                errors[error_type] = errors.get(error_type, 0) + 1

            # æ¯20ä¸ªè¯·æ±‚è¾“å‡ºä¸€æ¬¡è¿›åº¦
            if total_requests % 20 == 0:
                print(
                    f"   è¿›åº¦: {total_requests}/{num_requests} (æˆåŠŸ: {successful_requests}, å¤±è´¥: {failed_requests})")

    # ä½¿ç”¨å¼‚æ­¥å®¢æˆ·ç«¯æ‰§è¡Œå¹¶å‘è¯·æ±‚
    start_time = time.time()

    # åˆ›å»ºä¸€ä¸ªå…±äº«çš„å¼‚æ­¥å®¢æˆ·ç«¯
    async with AsyncTamarModelClient() as client:
        # åˆ›å»ºæ‰€æœ‰ä»»åŠ¡ï¼Œä½†é™åˆ¶å¹¶å‘æ•°
        semaphore = asyncio.Semaphore(50)  # é™åˆ¶æœ€å¤š50ä¸ªå¹¶å‘è¯·æ±‚

        async def limited_worker(request_id: int):
            async with semaphore:
                await async_worker(client, request_id)

        # åˆ›å»ºæ‰€æœ‰ä»»åŠ¡
        tasks = [limited_worker(i) for i in range(num_requests)]

        # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
        await asyncio.gather(*tasks)

    total_duration = time.time() - start_time

    # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
    avg_request_time = sum(request_times) / len(request_times) if request_times else 0
    min_request_time = min(request_times) if request_times else 0
    max_request_time = max(request_times) if request_times else 0

    # è¾“å‡ºç»“æœ
    print(f"\nğŸ“Š å¼‚æ­¥å¹¶å‘æµ‹è¯•ç»“æœ:")
    print(f"   æ€»è¯·æ±‚æ•°: {total_requests}")
    print(f"   æˆåŠŸè¯·æ±‚: {successful_requests} ({successful_requests / total_requests * 100:.1f}%)")
    print(f"   å¤±è´¥è¯·æ±‚: {failed_requests} ({failed_requests / total_requests * 100:.1f}%)")
    print(f"   æ€»è€—æ—¶: {total_duration:.2f} ç§’")
    print(f"   å¹³å‡QPS: {total_requests / total_duration:.2f}")
    print(f"\n   è¯·æ±‚è€—æ—¶ç»Ÿè®¡:")
    print(f"   - å¹³å‡: {avg_request_time:.3f} ç§’")
    print(f"   - æœ€å°: {min_request_time:.3f} ç§’")
    print(f"   - æœ€å¤§: {max_request_time:.3f} ç§’")

    if errors:
        print(f"\n   é”™è¯¯ç»Ÿè®¡:")
        for error_type, count in sorted(errors.items(), key=lambda x: x[1], reverse=True):
            print(f"   - {error_type}: {count} æ¬¡")

    return {
        "total": total_requests,
        "successful": successful_requests,
        "failed": failed_requests,
        "duration": total_duration,
        "qps": total_requests / total_duration
    }


async def test_async_batch_with_circuit_breaker_v2(num_requests: int = 10):
    """
    æµ‹è¯•ç†”æ–­å™¨åŠŸèƒ½ - ä½¿ç”¨å•ä¸ªè¯·æ±‚è€Œä¸æ˜¯æ‰¹é‡è¯·æ±‚
    
    é€šè¿‡å‘é€å¤šä¸ªå•ç‹¬çš„è¯·æ±‚æ¥è§¦å‘ç†”æ–­å™¨ï¼Œå› ä¸ºæ‰¹é‡è¯·æ±‚ä¸­çš„å•ä¸ªå¤±è´¥ä¸ä¼šè§¦å‘ç†”æ–­ã€‚
    
    Args:
        num_requests: è¦å‘é€çš„è¯·æ±‚æ•°ï¼Œé»˜è®¤10ä¸ª
    """
    print(f"\nğŸ”¥ æµ‹è¯•ç†”æ–­å™¨åŠŸèƒ½ - æ”¹è¿›ç‰ˆ ({num_requests} ä¸ªç‹¬ç«‹è¯·æ±‚)...")

    # ä¿å­˜åŸå§‹ç¯å¢ƒå˜é‡
    import os
    original_env = {}
    env_vars = ['MODEL_CLIENT_RESILIENT_ENABLED', 'MODEL_CLIENT_HTTP_FALLBACK_URL',
                'MODEL_CLIENT_CIRCUIT_BREAKER_THRESHOLD', 'MODEL_CLIENT_CIRCUIT_BREAKER_TIMEOUT']
    for var in env_vars:
        original_env[var] = os.environ.get(var)

    # è®¾ç½®ç¯å¢ƒå˜é‡ä»¥å¯ç”¨ç†”æ–­å™¨å’ŒHTTP fallback
    os.environ['MODEL_CLIENT_RESILIENT_ENABLED'] = 'true'
    os.environ['MODEL_CLIENT_HTTP_FALLBACK_URL'] = 'http://localhost:8000'
    os.environ['MODEL_CLIENT_CIRCUIT_BREAKER_THRESHOLD'] = '3'  # 3æ¬¡å¤±è´¥åè§¦å‘ç†”æ–­
    os.environ['MODEL_CLIENT_CIRCUIT_BREAKER_TIMEOUT'] = '30'  # ç†”æ–­å™¨30ç§’åæ¢å¤

    print(f"   ç¯å¢ƒå˜é‡è®¾ç½®:")
    print(f"   - MODEL_CLIENT_RESILIENT_ENABLED: {os.environ.get('MODEL_CLIENT_RESILIENT_ENABLED')}")
    print(f"   - MODEL_CLIENT_HTTP_FALLBACK_URL: {os.environ.get('MODEL_CLIENT_HTTP_FALLBACK_URL')}")
    print(f"   - ç†”æ–­é˜ˆå€¼: 3 æ¬¡å¤±è´¥")

    # ç»Ÿè®¡å˜é‡
    total_requests = 0
    successful_requests = 0
    failed_requests = 0
    circuit_breaker_opened = False
    http_fallback_used = 0
    request_times: List[float] = []
    errors: Dict[str, int] = {}

    try:
        # åˆ›å»ºä¸€ä¸ªå…±äº«çš„å¼‚æ­¥å®¢æˆ·ç«¯ï¼ˆå¯ç”¨ç†”æ–­å™¨ï¼‰
        async with AsyncTamarModelClient() as client:
            print(f"\n   ç†”æ–­å™¨é…ç½®:")
            print(f"   - å¯ç”¨çŠ¶æ€: {getattr(client, 'resilient_enabled', False)}")
            print(f"   - HTTP Fallback URL: {getattr(client, 'http_fallback_url', 'None')}")

            for i in range(num_requests):
                start_time = time.time()

                try:
                    # å‰4ä¸ªè¯·æ±‚ä½¿ç”¨é”™è¯¯çš„modelæ¥è§¦å‘å¤±è´¥
                    if i < 4:
                        request = ModelRequest(
                            provider=ProviderType.GOOGLE,
                            invoke_type=InvokeType.GENERATION,
                            model="invalid-model-to-trigger-error",  # æ— æ•ˆæ¨¡å‹
                            contents=f"æµ‹è¯•å¤±è´¥è¯·æ±‚ {i + 1}",
                            user_context=UserContext(
                                user_id=f"circuit_test_{i}",
                                org_id="test_org_circuit",
                                client_type="circuit_test"
                            )
                        )
                    else:
                        # åç»­è¯·æ±‚ä½¿ç”¨æ­£ç¡®çš„model
                        request = ModelRequest(
                            provider=ProviderType.GOOGLE,
                            invoke_type=InvokeType.GENERATION,
                            model="tamar-google-gemini-flash-lite",
                            contents=f"æµ‹è¯•è¯·æ±‚ {i + 1}: è®¡ç®— {i} + {i}",
                            user_context=UserContext(
                                user_id=f"circuit_test_{i}",
                                org_id="test_org_circuit",
                                client_type="circuit_test"
                            ),
                            config={"temperature": 0.1}
                        )

                    print(f"\n   ğŸ“¤ å‘é€è¯·æ±‚ {i + 1}/{num_requests}...")
                    response = await client.invoke(request, timeout=10000)

                    duration = time.time() - start_time
                    request_times.append(duration)
                    total_requests += 1
                    successful_requests += 1

                    print(f"   âœ… è¯·æ±‚ {i + 1} æˆåŠŸ - è€—æ—¶: {duration:.2f}ç§’")

                    # æ£€æŸ¥æ˜¯å¦é€šè¿‡HTTP fallback
                    if hasattr(client, 'resilient_enabled') and client.resilient_enabled:
                        try:
                            metrics = client.get_resilient_metrics()
                            if metrics and metrics['circuit_breaker']['state'] == 'open':
                                http_fallback_used += 1
                                print(f"      (é€šè¿‡HTTP fallback)")
                        except:
                            pass

                except Exception as e:
                    duration = time.time() - start_time
                    request_times.append(duration)
                    total_requests += 1
                    failed_requests += 1

                    error_type = type(e).__name__
                    errors[error_type] = errors.get(error_type, 0) + 1

                    print(f"   âŒ è¯·æ±‚ {i + 1} å¤±è´¥: {error_type} - {str(e)[:100]}")
                    print(f"      è€—æ—¶: {duration:.2f}ç§’")

                # æ£€æŸ¥ç†”æ–­å™¨çŠ¶æ€
                if hasattr(client, 'resilient_enabled') and client.resilient_enabled:
                    try:
                        metrics = client.get_resilient_metrics()
                        if metrics and 'circuit_breaker' in metrics:
                            state = metrics['circuit_breaker']['state']
                            failures = metrics['circuit_breaker']['failure_count']

                            if state == 'open' and not circuit_breaker_opened:
                                circuit_breaker_opened = True
                                print(f"   ğŸ”» ç†”æ–­å™¨å·²æ‰“å¼€ï¼å¤±è´¥æ¬¡æ•°: {failures}")

                            print(f"      ç†”æ–­å™¨: {state}, å¤±è´¥è®¡æ•°: {failures}")
                    except Exception as e:
                        print(f"      è·å–ç†”æ–­å™¨çŠ¶æ€å¤±è´¥: {e}")

                # è¯·æ±‚ä¹‹é—´çŸ­æš‚ç­‰å¾…
                await asyncio.sleep(0.2)

            # æœ€ç»ˆç»Ÿè®¡
            print(f"\nğŸ“Š ç†”æ–­å™¨æµ‹è¯•ç»“æœ:")
            print(f"   æ€»è¯·æ±‚æ•°: {total_requests}")
            print(f"   æˆåŠŸè¯·æ±‚: {successful_requests}")
            print(f"   å¤±è´¥è¯·æ±‚: {failed_requests}")

            print(f"\n   ğŸ”¥ ç†”æ–­å™¨ç»Ÿè®¡:")
            print(f"   - ç†”æ–­å™¨æ˜¯å¦è§¦å‘: {'æ˜¯' if circuit_breaker_opened else 'å¦'}")
            print(f"   - HTTP fallbackä½¿ç”¨æ¬¡æ•°: {http_fallback_used}")

            # è·å–æœ€ç»ˆçŠ¶æ€
            if hasattr(client, 'resilient_enabled') and client.resilient_enabled:
                try:
                    final_metrics = client.get_resilient_metrics()
                    if final_metrics and 'circuit_breaker' in final_metrics:
                        print(f"   - æœ€ç»ˆçŠ¶æ€: {final_metrics['circuit_breaker']['state']}")
                        print(f"   - æ€»å¤±è´¥æ¬¡æ•°: {final_metrics['circuit_breaker']['failure_count']}")
                except Exception as e:
                    print(f"   - è·å–æœ€ç»ˆçŠ¶æ€å¤±è´¥: {e}")

            if errors:
                print(f"\n   é”™è¯¯ç»Ÿè®¡:")
                for error_type, count in sorted(errors.items(), key=lambda x: x[1], reverse=True):
                    print(f"   - {error_type}: {count} æ¬¡")

    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()

    finally:
        # æ¢å¤åŸå§‹ç¯å¢ƒå˜é‡
        for var, value in original_env.items():
            if value is None:
                os.environ.pop(var, None)
            else:
                os.environ[var] = value


async def test_async_batch_with_circuit_breaker(batch_size: int = 10, num_batches: int = 5):
    """æµ‹è¯•å¼‚æ­¥æ‰¹é‡è¯·æ±‚ - è§¦å‘ç†”æ–­å™¨ä½¿ç”¨HTTP fallback
    
    è¿™ä¸ªæµ‹è¯•ä¼šå¤ç”¨ä¸€ä¸ªAsyncTamarModelClientï¼Œé€šè¿‡å‘é€å¤šä¸ªæ‰¹é‡è¯·æ±‚æ¥è§¦å‘ç†”æ–­å™¨ï¼Œ
    ä½¿å…¶è‡ªåŠ¨åˆ‡æ¢åˆ°HTTP fallbackæ¨¡å¼ã€‚
    
    Args:
        batch_size: æ¯ä¸ªæ‰¹é‡è¯·æ±‚åŒ…å«çš„è¯·æ±‚æ•°ï¼Œé»˜è®¤10ä¸ª
        num_batches: è¦å‘é€çš„æ‰¹é‡è¯·æ±‚æ•°ï¼Œé»˜è®¤5ä¸ª
    """
    print(f"\nğŸ”¥ æµ‹è¯•å¼‚æ­¥æ‰¹é‡è¯·æ±‚ - ç†”æ–­å™¨æ¨¡å¼ ({num_batches} ä¸ªæ‰¹é‡ï¼Œæ¯æ‰¹ {batch_size} ä¸ªè¯·æ±‚)...")

    # ä¿å­˜åŸå§‹ç¯å¢ƒå˜é‡
    import os
    original_env = {}
    env_vars = ['MODEL_CLIENT_RESILIENT_ENABLED', 'MODEL_CLIENT_HTTP_FALLBACK_URL',
                'MODEL_CLIENT_CIRCUIT_BREAKER_THRESHOLD', 'MODEL_CLIENT_CIRCUIT_BREAKER_TIMEOUT']
    for var in env_vars:
        original_env[var] = os.environ.get(var)

    # è®¾ç½®ç¯å¢ƒå˜é‡ä»¥å¯ç”¨ç†”æ–­å™¨å’ŒHTTP fallback
    os.environ['MODEL_CLIENT_RESILIENT_ENABLED'] = 'true'
    os.environ['MODEL_CLIENT_HTTP_FALLBACK_URL'] = 'http://localhost:8000'
    os.environ['MODEL_CLIENT_CIRCUIT_BREAKER_THRESHOLD'] = '3'  # 3æ¬¡å¤±è´¥åè§¦å‘ç†”æ–­
    os.environ['MODEL_CLIENT_CIRCUIT_BREAKER_TIMEOUT'] = '60'  # ç†”æ–­å™¨60ç§’åæ¢å¤

    # è°ƒè¯•ï¼šæ‰“å°ç¯å¢ƒå˜é‡ç¡®è®¤è®¾ç½®æˆåŠŸ
    print(f"   ç¯å¢ƒå˜é‡è®¾ç½®:")
    print(f"   - MODEL_CLIENT_RESILIENT_ENABLED: {os.environ.get('MODEL_CLIENT_RESILIENT_ENABLED')}")
    print(f"   - MODEL_CLIENT_HTTP_FALLBACK_URL: {os.environ.get('MODEL_CLIENT_HTTP_FALLBACK_URL')}")

    # ç»Ÿè®¡å˜é‡
    total_batches = 0
    successful_batches = 0
    failed_batches = 0
    circuit_breaker_opened = False
    http_fallback_used = 0
    batch_times: List[float] = []
    errors: Dict[str, int] = {}

    try:
        from tamar_model_client.schemas import BatchModelRequest, BatchModelRequestItem

        # åˆ›å»ºä¸€ä¸ªå…±äº«çš„å¼‚æ­¥å®¢æˆ·ç«¯ï¼ˆå¯ç”¨ç†”æ–­å™¨ï¼‰
        async with AsyncTamarModelClient() as client:
            print(f"   ç†”æ–­å™¨é…ç½®:")
            print(f"   - å¯ç”¨çŠ¶æ€: {getattr(client, 'resilient_enabled', False)}")
            print(f"   - HTTP Fallback URL: {getattr(client, 'http_fallback_url', 'None')}")
            if hasattr(client, 'resilient_enabled') and client.resilient_enabled:
                try:
                    metrics = client.get_resilient_metrics()
                    if metrics and 'circuit_breaker' in metrics:
                        print(f"   - ç†”æ–­é˜ˆå€¼: {metrics['circuit_breaker'].get('failure_threshold', 'Unknown')} æ¬¡å¤±è´¥")
                        print(f"   - ç†”æ–­æ¢å¤æ—¶é—´: {metrics['circuit_breaker'].get('recovery_timeout', 'Unknown')} ç§’")
                    else:
                        print(f"   - ç†”æ–­é˜ˆå€¼: {os.environ.get('MODEL_CLIENT_CIRCUIT_BREAKER_THRESHOLD', '5')} æ¬¡å¤±è´¥")
                        print(f"   - ç†”æ–­æ¢å¤æ—¶é—´: {os.environ.get('MODEL_CLIENT_CIRCUIT_BREAKER_TIMEOUT', '60')} ç§’")
                except:
                    print(f"   - ç†”æ–­é˜ˆå€¼: {os.environ.get('MODEL_CLIENT_CIRCUIT_BREAKER_THRESHOLD', '5')} æ¬¡å¤±è´¥")
                    print(f"   - ç†”æ–­æ¢å¤æ—¶é—´: {os.environ.get('MODEL_CLIENT_CIRCUIT_BREAKER_TIMEOUT', '60')} ç§’")
            else:
                print(f"   - ç†”æ–­å™¨æœªå¯ç”¨")

            for batch_num in range(num_batches):
                start_time = time.time()

                try:
                    # æ„å»ºæ‰¹é‡è¯·æ±‚
                    items = []
                    for i in range(batch_size):
                        request_idx = batch_num * batch_size + i

                        # æ··åˆä½¿ç”¨ä¸åŒçš„providerå’Œmodel
                        if request_idx % 4 == 0:
                            # Google Vertex AI
                            item = BatchModelRequestItem(
                                provider=ProviderType.GOOGLE,
                                channel=Channel.VERTEXAI,
                                invoke_type=InvokeType.GENERATION,
                                model="tamar-google-gemini-flash-lite",
                                contents=f"è®¡ç®— {request_idx} * 2 çš„ç»“æœ",
                                custom_id=f"batch-{batch_num}-google-vertex-{i}",
                                config={"temperature": 0.1}
                            )
                        elif request_idx % 4 == 1:
                            # Google AI Studio
                            item = BatchModelRequestItem(
                                provider=ProviderType.GOOGLE,
                                channel=Channel.AI_STUDIO,
                                invoke_type=InvokeType.GENERATION,
                                model="tamar-google-gemini-flash-lite",
                                contents=f"è§£é‡Šæ•°å­— {request_idx} çš„å«ä¹‰",
                                custom_id=f"batch-{batch_num}-google-studio-{i}",
                                config={"temperature": 0.2, "maxOutputTokens": 50}
                            )
                        elif request_idx % 4 == 2:
                            # Azure OpenAI
                            item = BatchModelRequestItem(
                                provider=ProviderType.AZURE,
                                invoke_type=InvokeType.CHAT_COMPLETIONS,
                                model="gpt-4o-mini",
                                messages=[{"role": "user", "content": f"æ•°å­— {request_idx} æ˜¯å¥‡æ•°è¿˜æ˜¯å¶æ•°ï¼Ÿ"}],
                                custom_id=f"batch-{batch_num}-azure-{i}",
                                config={"temperature": 0.1, "max_tokens": 30}
                            )
                        else:
                            # æ•…æ„ä½¿ç”¨é”™è¯¯çš„modelæ¥è§¦å‘å¤±è´¥ï¼ˆå¸®åŠ©è§¦å‘ç†”æ–­ï¼‰
                            if batch_num < 2:  # å‰ä¸¤ä¸ªæ‰¹æ¬¡ä½¿ç”¨é”™è¯¯model
                                item = BatchModelRequestItem(
                                    provider=ProviderType.GOOGLE,
                                    invoke_type=InvokeType.GENERATION,
                                    model="invalid-model-to-trigger-error",
                                    contents=f"æµ‹è¯•é”™è¯¯ {request_idx}",
                                    custom_id=f"batch-{batch_num}-error-{i}",
                                )
                            else:
                                # åç»­æ‰¹æ¬¡ä½¿ç”¨æ­£ç¡®çš„model
                                item = BatchModelRequestItem(
                                    provider=ProviderType.GOOGLE,
                                    invoke_type=InvokeType.GENERATION,
                                    model="tamar-google-gemini-flash-lite",
                                    contents=f"Hello from batch {batch_num}, item {i}",
                                    custom_id=f"batch-{batch_num}-recovery-{i}",
                                )

                        items.append(item)

                    batch_request = BatchModelRequest(
                        user_context=UserContext(
                            user_id=f"circuit_breaker_test_batch_{batch_num}",
                            org_id="test_org_circuit_breaker",
                            client_type="async_batch_circuit_test"
                        ),
                        items=items
                    )

                    # æ‰§è¡Œæ‰¹é‡è¯·æ±‚
                    print(f"\n   ğŸ“¦ å‘é€æ‰¹é‡è¯·æ±‚ {batch_num + 1}/{num_batches}...")
                    batch_response = await client.invoke_batch(
                        batch_request,
                        timeout=300000.0,
                        request_id=f"circuit_breaker_test_{batch_num}"
                    )

                    duration = time.time() - start_time
                    batch_times.append(duration)
                    total_batches += 1
                    successful_batches += 1

                    # ç»Ÿè®¡ç»“æœ
                    success_count = sum(1 for r in batch_response.responses if not r.error)
                    error_count = sum(1 for r in batch_response.responses if r.error)

                    print(f"   âœ… æ‰¹é‡è¯·æ±‚ {batch_num + 1} å®Œæˆ")
                    print(f"      - è€—æ—¶: {duration:.2f} ç§’")
                    print(f"      - æˆåŠŸ: {success_count}/{batch_size}")
                    print(f"      - å¤±è´¥: {error_count}/{batch_size}")

                    # æ£€æŸ¥ç†”æ–­å™¨çŠ¶æ€
                    if hasattr(client, 'resilient_enabled') and client.resilient_enabled:
                        try:
                            breaker_status = client.get_resilient_metrics()
                            if breaker_status and 'circuit_breaker' in breaker_status:
                                if breaker_status['circuit_breaker']['state'] == 'OPEN':
                                    if not circuit_breaker_opened:
                                        circuit_breaker_opened = True
                                        print(f"   ğŸ”» ç†”æ–­å™¨å·²æ‰“å¼€ï¼å°†ä½¿ç”¨HTTP fallback")
                                    http_fallback_used += 1

                                print(f"      - ç†”æ–­å™¨çŠ¶æ€: {breaker_status['circuit_breaker']['state']}")
                                print(f"      - å¤±è´¥è®¡æ•°: {breaker_status['circuit_breaker']['failure_count']}")
                        except Exception as e:
                            print(f"      - è·å–ç†”æ–­å™¨çŠ¶æ€å¤±è´¥: {e}")

                except Exception as e:
                    duration = time.time() - start_time
                    batch_times.append(duration)
                    total_batches += 1
                    failed_batches += 1

                    error_type = str(e).split(':')[0] if ':' in str(e) else str(e)[:50]
                    errors[error_type] = errors.get(error_type, 0) + 1

                    print(f"   âŒ æ‰¹é‡è¯·æ±‚ {batch_num + 1} å¤±è´¥: {error_type}")
                    print(f"      - è€—æ—¶: {duration:.2f} ç§’")

                # æ‰¹æ¬¡ä¹‹é—´çŸ­æš‚ç­‰å¾…
                if batch_num < num_batches - 1:
                    await asyncio.sleep(0.5)

            # æœ€ç»ˆç»Ÿè®¡
            print(f"\nğŸ“Š æ‰¹é‡è¯·æ±‚æµ‹è¯•ç»“æœ (ç†”æ–­å™¨æ¨¡å¼):")
            print(f"   æ€»æ‰¹æ¬¡æ•°: {total_batches}")
            print(f"   æˆåŠŸæ‰¹æ¬¡: {successful_batches} ({successful_batches / total_batches * 100:.1f}%)")
            print(f"   å¤±è´¥æ‰¹æ¬¡: {failed_batches} ({failed_batches / total_batches * 100:.1f}%)")

            if batch_times:
                avg_batch_time = sum(batch_times) / len(batch_times)
                print(f"\n   æ‰¹æ¬¡è€—æ—¶ç»Ÿè®¡:")
                print(f"   - å¹³å‡: {avg_batch_time:.3f} ç§’")
                print(f"   - æœ€å°: {min(batch_times):.3f} ç§’")
                print(f"   - æœ€å¤§: {max(batch_times):.3f} ç§’")

            print(f"\n   ğŸ”¥ ç†”æ–­å™¨ç»Ÿè®¡:")
            print(f"   - ç†”æ–­å™¨æ˜¯å¦è§¦å‘: {'æ˜¯' if circuit_breaker_opened else 'å¦'}")
            print(f"   - HTTP fallbackä½¿ç”¨æ¬¡æ•°: {http_fallback_used}")

            # è·å–æœ€ç»ˆçš„ç†”æ–­å™¨çŠ¶æ€
            if hasattr(client, 'resilient_enabled') and client.resilient_enabled:
                try:
                    final_metrics = client.get_resilient_metrics()
                    if final_metrics and 'circuit_breaker' in final_metrics:
                        print(f"   - æœ€ç»ˆçŠ¶æ€: {final_metrics['circuit_breaker']['state']}")
                        print(f"   - æ€»å¤±è´¥æ¬¡æ•°: {final_metrics['circuit_breaker']['failure_count']}")
                        print(f"   - å¤±è´¥é˜ˆå€¼: {final_metrics['circuit_breaker']['failure_threshold']}")
                        print(f"   - æ¢å¤è¶…æ—¶: {final_metrics['circuit_breaker']['recovery_timeout']}ç§’")
                    else:
                        print(f"   - æ— æ³•è·å–ç†”æ–­å™¨æŒ‡æ ‡")
                except Exception as e:
                    print(f"   - è·å–ç†”æ–­å™¨æŒ‡æ ‡å¤±è´¥: {e}")

            if errors:
                print(f"\n   é”™è¯¯ç»Ÿè®¡:")
                for error_type, count in sorted(errors.items(), key=lambda x: x[1], reverse=True):
                    print(f"   - {error_type}: {count} æ¬¡")

    except Exception as e:
        print(f"âŒ æ‰¹é‡æµ‹è¯•å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()

    finally:
        # æ¢å¤åŸå§‹ç¯å¢ƒå˜é‡
        for var, value in original_env.items():
            if value is None:
                os.environ.pop(var, None)
            else:
                os.environ[var] = value


async def test_async_concurrent_requests_independent_clients(num_requests: int = 150):
    """æµ‹è¯•å¼‚æ­¥å¹¶å‘è¯·æ±‚ - æ¯ä¸ªè¯·æ±‚ä½¿ç”¨ç‹¬ç«‹çš„AsyncTamarModelClient
    
    æ¯ä¸ªè¯·æ±‚éƒ½ä¼šåˆ›å»ºä¸€ä¸ªæ–°çš„AsyncTamarModelClientå®ä¾‹ï¼Œä¸å¤ç”¨è¿æ¥ï¼Œ
    è¿™ç§æ–¹å¼å¯ä»¥æµ‹è¯•å®¢æˆ·ç«¯çš„è¿æ¥ç®¡ç†å’Œèµ„æºæ¸…ç†èƒ½åŠ›ã€‚
    
    Args:
        num_requests: è¦å‘é€çš„æ€»è¯·æ±‚æ•°ï¼Œé»˜è®¤150ä¸ª
    """
    print(f"\nğŸš€ æµ‹è¯•å¼‚æ­¥å¹¶å‘è¯·æ±‚ - ç‹¬ç«‹å®¢æˆ·ç«¯æ¨¡å¼ ({num_requests} ä¸ªè¯·æ±‚)...")

    # ç»Ÿè®¡å˜é‡
    total_requests = 0
    successful_requests = 0
    failed_requests = 0
    request_times: List[float] = []
    errors: Dict[str, int] = {}
    trace_id = "9999999999999999933333999999993333399999"

    # å¼‚æ­¥é”
    stats_lock = asyncio.Lock()

    async def make_single_async_request_with_independent_client(request_id: int) -> Tuple[bool, float, str]:
        """ä½¿ç”¨ç‹¬ç«‹çš„AsyncTamarModelClientæ‰§è¡Œå•ä¸ªå¼‚æ­¥è¯·æ±‚
        
        Returns:
            (success, duration, error_msg)
        """
        start_time = time.time()
        try:
            # æ¯ä¸ªè¯·æ±‚åˆ›å»ºç‹¬ç«‹çš„å®¢æˆ·ç«¯å®ä¾‹
            async with AsyncTamarModelClient() as client:
                # æ ¹æ®è¯·æ±‚IDé€‰æ‹©ä¸åŒçš„providerå’Œmodelï¼Œå¢åŠ æµ‹è¯•å¤šæ ·æ€§
                if request_id % 3 == 0:
                    # Google Vertex AI
                    request = ModelRequest(
                        provider=ProviderType.GOOGLE,
                        channel=Channel.VERTEXAI,
                        invoke_type=InvokeType.GENERATION,
                        model="tamar-google-gemini-flash-lite",
                        contents=f"è¯·è®¡ç®— {request_id % 10} + {(request_id + 1) % 10} ç­‰äºå¤šå°‘ï¼Ÿ",
                        user_context=UserContext(
                            user_id=f"{os.environ.get('INSTANCE_ID', '0')}_independent_{request_id:03d}",
                            org_id="test_org_independent",
                            client_type="async_independent_test"
                        ),
                        config={"temperature": 0.1}
                    )
                elif request_id % 3 == 1:
                    # Google AI Studio
                    request = ModelRequest(
                        provider=ProviderType.GOOGLE,
                        channel=Channel.AI_STUDIO,
                        invoke_type=InvokeType.GENERATION,
                        model="tamar-google-gemini-flash-lite",
                        contents=f"ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿè¯·ç®€è¦å›ç­”ã€‚(è¯·æ±‚ID: {request_id})",
                        user_context=UserContext(
                            user_id=f"{os.environ.get('INSTANCE_ID', '0')}_independent_{request_id:03d}",
                            org_id="test_org_independent",
                            client_type="async_independent_test"
                        ),
                        config={"temperature": 0.3, "maxOutputTokens": 100}
                    )
                else:
                    # Azure OpenAI
                    request = ModelRequest(
                        provider=ProviderType.AZURE,
                        invoke_type=InvokeType.CHAT_COMPLETIONS,
                        model="gpt-4o-mini",
                        messages=[
                            {"role": "user", "content": f"è¯·ç®€å•è§£é‡Šä»€ä¹ˆæ˜¯äº‘è®¡ç®—ï¼Ÿ(è¯·æ±‚{request_id})"}
                        ],
                        user_context=UserContext(
                            user_id=f"{os.environ.get('INSTANCE_ID', '0')}_independent_{request_id:03d}",
                            org_id="test_org_independent",
                            client_type="async_independent_test"
                        ),
                        config={"temperature": 0.2, "max_tokens": 100}
                    )

                response = await client.invoke(request, timeout=300000.0, request_id=f"{trace_id}_{request_id}")
                duration = time.time() - start_time
                return (True, duration, "")

        except Exception as e:
            duration = time.time() - start_time
            error_msg = str(e)
            return (False, duration, error_msg)

    async def async_independent_worker(request_id: int):
        """ç‹¬ç«‹å¼‚æ­¥å·¥ä½œåç¨‹ - æ¯ä¸ªè¯·æ±‚ä½¿ç”¨ç‹¬ç«‹çš„å®¢æˆ·ç«¯"""
        nonlocal total_requests, successful_requests, failed_requests

        success, duration, error_msg = await make_single_async_request_with_independent_client(request_id)

        async with stats_lock:
            total_requests += 1
            request_times.append(duration)

            if success:
                successful_requests += 1
            else:
                failed_requests += 1
                # ç»Ÿè®¡é”™è¯¯ç±»å‹
                error_type = error_msg.split(':')[0] if ':' in error_msg else error_msg[:50]
                errors[error_type] = errors.get(error_type, 0) + 1

            # æ¯20ä¸ªè¯·æ±‚è¾“å‡ºä¸€æ¬¡è¿›åº¦
            if total_requests % 20 == 0:
                print(
                    f"   è¿›åº¦: {total_requests}/{num_requests} (æˆåŠŸ: {successful_requests}, å¤±è´¥: {failed_requests})")

    # ä½¿ç”¨ç‹¬ç«‹å®¢æˆ·ç«¯æ‰§è¡Œå¹¶å‘è¯·æ±‚
    start_time = time.time()

    # é™åˆ¶å¹¶å‘æ•°ï¼Œé¿å…åˆ›å»ºè¿‡å¤šè¿æ¥
    semaphore = asyncio.Semaphore(30)  # é™ä½å¹¶å‘æ•°ï¼Œå› ä¸ºæ¯ä¸ªè¯·æ±‚éƒ½è¦åˆ›å»ºæ–°è¿æ¥

    async def limited_independent_worker(request_id: int):
        async with semaphore:
            await async_independent_worker(request_id)

    # åˆ›å»ºæ‰€æœ‰ä»»åŠ¡
    tasks = [limited_independent_worker(i) for i in range(num_requests)]

    # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
    await asyncio.gather(*tasks)

    total_duration = time.time() - start_time

    # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
    avg_request_time = sum(request_times) / len(request_times) if request_times else 0
    min_request_time = min(request_times) if request_times else 0
    max_request_time = max(request_times) if request_times else 0

    # è¾“å‡ºç»“æœ
    print(f"\nğŸ“Š å¼‚æ­¥å¹¶å‘æµ‹è¯•ç»“æœ (ç‹¬ç«‹å®¢æˆ·ç«¯æ¨¡å¼):")
    print(f"   æ€»è¯·æ±‚æ•°: {total_requests}")
    print(f"   æˆåŠŸè¯·æ±‚: {successful_requests} ({successful_requests / total_requests * 100:.1f}%)")
    print(f"   å¤±è´¥è¯·æ±‚: {failed_requests} ({failed_requests / total_requests * 100:.1f}%)")
    print(f"   æ€»è€—æ—¶: {total_duration:.2f} ç§’")
    print(f"   å¹³å‡QPS: {total_requests / total_duration:.2f}")
    print(f"\n   è¯·æ±‚è€—æ—¶ç»Ÿè®¡:")
    print(f"   - å¹³å‡: {avg_request_time:.3f} ç§’")
    print(f"   - æœ€å°: {min_request_time:.3f} ç§’")
    print(f"   - æœ€å¤§: {max_request_time:.3f} ç§’")

    print(f"\n   ğŸ” æµ‹è¯•ç‰¹ç‚¹:")
    print(f"   - æ¯ä¸ªè¯·æ±‚ä½¿ç”¨ç‹¬ç«‹çš„AsyncTamarModelClientå®ä¾‹")
    print(f"   - ä¸å¤ç”¨è¿æ¥ï¼Œæµ‹è¯•è¿æ¥ç®¡ç†èƒ½åŠ›")
    print(f"   - é™åˆ¶å¹¶å‘æ•°ä¸º30ä¸ªï¼Œé¿å…è¿‡å¤šè¿æ¥")
    print(f"   - ä½¿ç”¨å¤šç§Provider (Google Vertex AI, AI Studio, Azure OpenAI)")

    if errors:
        print(f"\n   é”™è¯¯ç»Ÿè®¡:")
        for error_type, count in sorted(errors.items(), key=lambda x: x[1], reverse=True):
            print(f"   - {error_type}: {count} æ¬¡")

    return {
        "total": total_requests,
        "successful": successful_requests,
        "failed": failed_requests,
        "duration": total_duration,
        "qps": total_requests / total_duration
    }


async def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ ç®€åŒ–ç‰ˆ Google/Azure æµ‹è¯•")
    print("=" * 50)

    try:
        # åŒæ­¥æµ‹è¯•
        test_google_ai_studio()
        test_google_vertex_ai()
        test_azure_openai()

        # æ–°å¢ï¼šå›¾åƒç”Ÿæˆæµ‹è¯•
        test_google_genai_image_generation()
        test_google_vertex_ai_image_generation()

        # åŒæ­¥æ‰¹é‡æµ‹è¯•
        test_sync_batch_requests()

        # å¼‚æ­¥æµå¼æµ‹è¯•
        await asyncio.wait_for(test_google_streaming(), timeout=60.0)
        await asyncio.wait_for(test_azure_streaming(), timeout=60.0)

        # ï¼šå¼‚æ­¥å›¾åƒç”Ÿæˆæµ‹è¯•
        await asyncio.wait_for(test_google_genai_image_generation_async(), timeout=120.0)
        await asyncio.wait_for(test_google_vertex_ai_image_generation_async(), timeout=120.0)

        # å¼‚æ­¥æ‰¹é‡æµ‹è¯•
        await asyncio.wait_for(test_batch_requests(), timeout=120.0)

        # æ–°å¢ï¼šå›¾åƒç”Ÿæˆæ‰¹é‡æµ‹è¯•
        await asyncio.wait_for(test_image_generation_batch(), timeout=180.0)
        #
        # # åŒæ­¥å¹¶å‘æµ‹è¯•
        # test_concurrent_requests(2)  # æµ‹è¯•150ä¸ªå¹¶å‘è¯·æ±‚
        #
        # # å¼‚æ­¥å¹¶å‘æµ‹è¯•
        # await test_async_concurrent_requests(2)  # æµ‹è¯•50ä¸ªå¼‚æ­¥å¹¶å‘è¯·æ±‚ï¼ˆå¤ç”¨è¿æ¥ï¼‰

        print("\nâœ… æµ‹è¯•å®Œæˆ")

    except asyncio.TimeoutError:
        print("\nâ° æµ‹è¯•è¶…æ—¶")
    except KeyboardInterrupt:
        print("\nâš ï¸ ç”¨æˆ·ä¸­æ–­æµ‹è¯•")
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•æ‰§è¡Œå‡ºé”™: {e}")
    finally:
        # ç®€å•ä¼˜é›…çš„ä»»åŠ¡æ¸…ç†
        print("ğŸ“ æ¸…ç†å¼‚æ­¥ä»»åŠ¡...")
        try:
            # çŸ­æš‚ç­‰å¾…è®©æ­£åœ¨å®Œæˆçš„ä»»åŠ¡è‡ªç„¶ç»“æŸ
            await asyncio.sleep(0.5)

            # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰æœªå®Œæˆçš„ä»»åŠ¡
            current_task = asyncio.current_task()
            tasks = [task for task in asyncio.all_tasks()
                     if not task.done() and task != current_task]

            if tasks:
                print(f"   å‘ç° {len(tasks)} ä¸ªæœªå®Œæˆä»»åŠ¡ï¼Œç­‰å¾…è‡ªç„¶å®Œæˆ...")
                # ç®€å•ç­‰å¾…ï¼Œä¸å¼ºåˆ¶å–æ¶ˆ
                try:
                    await asyncio.wait_for(
                        asyncio.sleep(2.0),  # ç»™ä»»åŠ¡2ç§’æ—¶é—´è‡ªç„¶å®Œæˆ
                        timeout=2.0
                    )
                except asyncio.TimeoutError:
                    pass

            print("   ä»»åŠ¡æ¸…ç†å®Œæˆ")

        except Exception as e:
            print(f"   âš ï¸ ä»»åŠ¡æ¸…ç†æ—¶å‡ºç°å¼‚å¸¸: {e}")

        print("ğŸ”š ç¨‹åºå³å°†é€€å‡º")


if __name__ == "__main__":
    try:
        # ä¸´æ—¶é™ä½ asyncio æ—¥å¿—çº§åˆ«ï¼Œå‡å°‘ä»»åŠ¡å–æ¶ˆæ—¶çš„å™ªéŸ³
        asyncio_logger = logging.getLogger('asyncio')
        original_level = asyncio_logger.level
        asyncio_logger.setLevel(logging.ERROR)

        try:
            asyncio.run(main())
        finally:
            # æ¢å¤åŸå§‹æ—¥å¿—çº§åˆ«
            asyncio_logger.setLevel(original_level)

    except KeyboardInterrupt:
        print("\nâš ï¸ ç¨‹åºè¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ ç¨‹åºæ‰§è¡Œå‡ºé”™: {e}")
    finally:
        print("ğŸ ç¨‹åºå·²é€€å‡º")
