Metadata-Version: 2.4
Name: tamar-model-client
Version: 0.5.2
Summary: A Python SDK for interacting with the Model Manager gRPC service
Home-page: http://gitlab.tamaredge.top/project-tap/AgentOS/model-manager-client
Author: Oscar Ou
Author-email: oscar.ou@tamaredge.ai
Classifier: Programming Language :: Python :: 3.11
Classifier: Programming Language :: Python :: 3.12
Classifier: License :: OSI Approved :: MIT License
Classifier: Operating System :: OS Independent
Requires-Python: >=3.8
Description-Content-Type: text/markdown
Requires-Dist: grpcio~=1.67.1
Requires-Dist: grpcio-tools~=1.67.1
Requires-Dist: pydantic
Requires-Dist: PyJWT
Requires-Dist: nest_asyncio
Requires-Dist: openai==2.8.1
Requires-Dist: google-genai>=1.51.0
Requires-Dist: anthropic>=0.68.0
Requires-Dist: requests>=2.25.0
Requires-Dist: aiohttp>=3.7.0
Dynamic: author
Dynamic: author-email
Dynamic: classifier
Dynamic: description
Dynamic: description-content-type
Dynamic: home-page
Dynamic: requires-dist
Dynamic: requires-python
Dynamic: summary

# Tamar Model Client

**Tamar Model Client** æ˜¯ä¸€æ¬¾é«˜æ€§èƒ½çš„ Python SDKï¼Œé€šè¿‡ gRPC åè®®è¿æ¥ Model Manager æœåŠ¡ï¼Œä¸ºå¤šä¸ª AI æ¨¡å‹æœåŠ¡å•†æä¾›ç»Ÿä¸€çš„è°ƒç”¨æ¥å£ã€‚æ— è®ºæ‚¨ä½¿ç”¨ OpenAIã€Googleã€Azure è¿˜æ˜¯å…¶ä»– AI æœåŠ¡ï¼Œéƒ½å¯ä»¥é€šè¿‡ä¸€å¥— API è½»æ¾åˆ‡æ¢å’Œç®¡ç†ã€‚

## ğŸ¯ ä¸ºä»€ä¹ˆé€‰æ‹© Tamar Model Clientï¼Ÿ

### æ‚¨é‡åˆ°è¿‡è¿™äº›é—®é¢˜å—ï¼Ÿ

âŒ éœ€è¦åŒæ—¶é›†æˆ OpenAIã€Googleã€Azure ç­‰å¤šä¸ª AI æœåŠ¡ï¼Œæ¯ä¸ªéƒ½æœ‰ä¸åŒçš„ APIï¼Ÿ  
âŒ éš¾ä»¥ç»Ÿè®¡å’Œæ§åˆ¶ä¸åŒæœåŠ¡å•†çš„ä½¿ç”¨æˆæœ¬ï¼Ÿ  
âŒ åœ¨ä¸åŒ AI æä¾›å•†ä¹‹é—´åˆ‡æ¢éœ€è¦ä¿®æ”¹å¤§é‡ä»£ç ï¼Ÿ  
âŒ æ¯ä¸ªæœåŠ¡å•†éƒ½æœ‰è‡ªå·±çš„é”™è¯¯å¤„ç†å’Œé‡è¯•é€»è¾‘ï¼Ÿ  

### âœ… Tamar Model Client ä¸€ç«™å¼è§£å†³ï¼

ğŸ‰ **ä¸€ä¸ª SDKï¼Œè®¿é—®æ‰€æœ‰ AI æœåŠ¡**  
ğŸ“Š **ç»Ÿä¸€çš„ä½¿ç”¨é‡å’Œæˆæœ¬ç»Ÿè®¡**  
ğŸ”„ **æ— ç¼åˆ‡æ¢ï¼Œä¸€è¡Œä»£ç æå®š**  
ğŸ›¡ï¸ **ç”Ÿäº§çº§çš„é”™è¯¯å¤„ç†å’Œé‡è¯•æœºåˆ¶**

## âœ¨ æ ¸å¿ƒç‰¹æ€§

### ğŸ”Œ å¤šæœåŠ¡å•†æ”¯æŒ
- **OpenAI** (GPT-3.5/4, DALL-E)
- **Google** (Gemini - AI Studio & Vertex AI, Imagen å›¾åƒç”Ÿæˆ)
- **Azure OpenAI** (ä¼ä¸šçº§éƒ¨ç½²)
- **Anthropic** (Claude)
- **DeepSeek** (æ·±åº¦æ±‚ç´¢)
- **Perplexity** (æœç´¢å¢å¼ºç”Ÿæˆ)

### âš¡ çµæ´»çš„è°ƒç”¨æ–¹å¼
- ğŸ§© **åŒæ­¥/å¼‚æ­¥** åŒæ¨¡å¼å®¢æˆ·ç«¯
- ğŸ“¡ **æµå¼/éæµå¼** å“åº”æ”¯æŒ
- ğŸ“¦ **æ‰¹é‡è¯·æ±‚** å¹¶è¡Œå¤„ç†
- ğŸ”„ **è‡ªåŠ¨é‡è¯•** æŒ‡æ•°é€€é¿ç­–ç•¥

### ğŸ›¡ï¸ ç”Ÿäº§çº§ç‰¹æ€§
- ğŸ›¡ï¸ **ç†”æ–­é™çº§** æœåŠ¡æ•…éšœæ—¶è‡ªåŠ¨åˆ‡æ¢åˆ° HTTP
- ğŸš€ **å¿«é€Ÿé™çº§** å¤±è´¥ç«‹å³é™çº§ï¼Œæœ€å¤§åŒ–æˆåŠŸç‡
- ğŸ” **JWT è®¤è¯** å®‰å…¨å¯é 
- ğŸ“Š **ä½¿ç”¨é‡è¿½è¸ª** Token ç»Ÿè®¡ä¸æˆæœ¬è®¡ç®—
- ğŸ†” **è¯·æ±‚è¿½è¸ª** å”¯ä¸€ request_id å’Œ origin_request_id å…¨é“¾è·¯è¿½è¸ª
- âš ï¸ **å®Œå–„é”™è¯¯å¤„ç†** è¯¦ç»†é”™è¯¯ä¿¡æ¯å’Œå¼‚å¸¸å †æ ˆè¿½è¸ª
- âœ… **ç±»å‹å®‰å…¨** Pydantic v2 éªŒè¯
- ğŸ“¦ **æ‰¹é‡é™çº§** HTTP é™çº§æ”¯æŒæ‰¹é‡è¯·æ±‚
- ğŸ” **ç»“æ„åŒ–æ—¥å¿—** JSON æ ¼å¼æ—¥å¿—ä¾¿äºç›‘æ§åˆ†æ

### ğŸš€ é«˜æ€§èƒ½è®¾è®¡
- ğŸ”— **gRPC é€šä¿¡** HTTP/2 é•¿è¿æ¥
- â™»ï¸ **è¿æ¥å¤ç”¨** å‡å°‘æ¡æ‰‹å¼€é”€
- ğŸ¯ **æ™ºèƒ½è·¯ç”±** è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜é€šé“
- ğŸ“ˆ **æ€§èƒ½ç›‘æ§** å»¶è¿Ÿä¸ååé‡æŒ‡æ ‡

## ğŸ“‹ å®‰è£…

```bash
pip install tamar-model-client
```

### ç³»ç»Ÿè¦æ±‚

- Python â‰¥ 3.8
- æ”¯æŒ Windows / Linux / macOS
- ä¾èµ–é¡¹ä¼šè‡ªåŠ¨å®‰è£…ï¼ˆåŒ…æ‹¬ä»¥ä¸‹æ ¸å¿ƒåº“ï¼‰ï¼š
  - `grpcio>=1.67.1` - gRPC é€šä¿¡åè®®
  - `pydantic` - æ•°æ®éªŒè¯å’Œåºåˆ—åŒ–
  - `PyJWT` - JWT è®¤è¯
  - `requests>=2.25.0` - HTTP é™çº§åŠŸèƒ½ï¼ˆåŒæ­¥ï¼‰
  - `aiohttp>=3.7.0` - HTTP é™çº§åŠŸèƒ½ï¼ˆå¼‚æ­¥ï¼‰
  - `openai` - OpenAI æœåŠ¡å•†æ”¯æŒ
  - `google-genai` - Google AI æœåŠ¡å•†æ”¯æŒ

## ğŸ—ï¸ é¡¹ç›®æ¶æ„

```
tamar_model_client/
â”œâ”€â”€ ğŸ“ generated/              # gRPC ç”Ÿæˆçš„ä»£ç 
â”‚   â”œâ”€â”€ model_service.proto    # Protocol Buffer å®šä¹‰
â”‚   â””â”€â”€ *_pb2*.py             # ç”Ÿæˆçš„ Python ä»£ç 
â”œâ”€â”€ ğŸ“ schemas/                # Pydantic æ•°æ®æ¨¡å‹
â”‚   â”œâ”€â”€ inputs.py             # è¯·æ±‚æ¨¡å‹ï¼ˆModelRequest, UserContextï¼‰
â”‚   â””â”€â”€ outputs.py            # å“åº”æ¨¡å‹ï¼ˆModelResponse, Usageï¼‰
â”œâ”€â”€ ğŸ“ enums/                  # æšä¸¾å®šä¹‰
â”‚   â”œâ”€â”€ providers.py          # AI æœåŠ¡å•†ï¼ˆOpenAI, Google, Azure...ï¼‰
â”‚   â”œâ”€â”€ invoke.py             # è°ƒç”¨ç±»å‹ï¼ˆgeneration, images, image-generation-genai...ï¼‰
â”‚   â””â”€â”€ channel.py            # æœåŠ¡é€šé“ï¼ˆopenai, vertexai...ï¼‰
â”œâ”€â”€ ğŸ“ core/                   # æ ¸å¿ƒåŠŸèƒ½æ¨¡å—
â”‚   â”œâ”€â”€ base_client.py        # å®¢æˆ·ç«¯åŸºç±»ï¼ˆç†”æ–­ã€é™çº§ã€é…ç½®ï¼‰
â”‚   â”œâ”€â”€ http_fallback.py      # HTTP é™çº§åŠŸèƒ½ï¼ˆæ”¯æŒæ‰¹é‡è¯·æ±‚ï¼‰
â”‚   â”œâ”€â”€ request_builder.py    # è¯·æ±‚æ„å»ºå™¨
â”‚   â”œâ”€â”€ response_handler.py   # å“åº”å¤„ç†å™¨
â”‚   â”œâ”€â”€ logging_setup.py      # ç»“æ„åŒ–æ—¥å¿—é…ç½®
â”‚   â””â”€â”€ utils.py              # è¯·æ±‚IDç®¡ç†å’Œå·¥å…·å‡½æ•°
â”œâ”€â”€ ğŸ“„ sync_client.py          # åŒæ­¥å®¢æˆ·ç«¯ TamarModelClient
â”œâ”€â”€ ğŸ“„ async_client.py         # å¼‚æ­¥å®¢æˆ·ç«¯ AsyncTamarModelClient
â”œâ”€â”€ ğŸ“„ error_handler.py        # å¢å¼ºé”™è¯¯å¤„ç†å’Œé‡è¯•ç­–ç•¥
â”œâ”€â”€ ğŸ“„ circuit_breaker.py      # ç†”æ–­å™¨å®ç°
â”œâ”€â”€ ğŸ“„ exceptions.py           # å¼‚å¸¸å±‚çº§å®šä¹‰
â”œâ”€â”€ ğŸ“„ auth.py                 # JWT è®¤è¯ç®¡ç†
â”œâ”€â”€ ğŸ“„ json_formatter.py       # JSON æ—¥å¿—æ ¼å¼åŒ–å™¨
â””â”€â”€ ğŸ“„ utils.py                # å·¥å…·å‡½æ•°
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1ï¸âƒ£ å®¢æˆ·ç«¯åˆå§‹åŒ–

```python
from tamar_model_client import TamarModelClient, AsyncTamarModelClient

# æ–¹å¼ä¸€ï¼šä½¿ç”¨ç¯å¢ƒå˜é‡ï¼ˆæ¨èï¼‰
client = TamarModelClient()  # è‡ªåŠ¨è¯»å–ç¯å¢ƒå˜é‡é…ç½®

# æ–¹å¼äºŒï¼šä»£ç é…ç½®
client = TamarModelClient(
    server_address="localhost:50051",
    jwt_token="your-jwt-token"
)

# å¼‚æ­¥å®¢æˆ·ç«¯
async_client = AsyncTamarModelClient(
    server_address="localhost:50051",
    jwt_secret_key="your-jwt-secret-key"  # ä½¿ç”¨å¯†é’¥è‡ªåŠ¨ç”Ÿæˆ JWT
)
```

### 2ï¸âƒ£ åŸºç¡€ç¤ºä¾‹ - ä¸ AI å¯¹è¯

```python
from tamar_model_client import TamarModelClient
from tamar_model_client.schemas import ModelRequest, UserContext
from tamar_model_client.enums import ProviderType

# åˆ›å»ºå®¢æˆ·ç«¯
client = TamarModelClient()

# æ„å»ºè¯·æ±‚
request = ModelRequest(
    provider=ProviderType.OPENAI,
    model="gpt-3.5-turbo",
    messages=[
        {"role": "user", "content": "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±ã€‚"}
    ],
    user_context=UserContext(
        user_id="test_user",
        org_id="test_org",
        client_type="python-sdk"
    )
)

# å‘é€è¯·æ±‚
response = client.invoke(request)
print(f"AI å›å¤: {response.content}")
```


## ğŸ“š è¯¦ç»†ä½¿ç”¨ç¤ºä¾‹

### OpenAI è°ƒç”¨ç¤ºä¾‹

```python
from tamar_model_client import TamarModelClient
from tamar_model_client.schemas import ModelRequest, UserContext
from tamar_model_client.enums import ProviderType, InvokeType, Channel

# åˆ›å»ºåŒæ­¥å®¢æˆ·ç«¯
client = TamarModelClient()

# OpenAI è°ƒç”¨ç¤ºä¾‹
request_data = ModelRequest(
    provider=ProviderType.OPENAI,  # é€‰æ‹© OpenAI ä½œä¸ºæä¾›å•†
    channel=Channel.OPENAI,  # ä½¿ç”¨ OpenAI æ¸ é“
    invoke_type=InvokeType.CHAT_COMPLETIONS,  # ä½¿ç”¨ chat completions è°ƒç”¨ç±»å‹
    model="gpt-4",  # æŒ‡å®šå…·ä½“æ¨¡å‹
    messages=[
        {"role": "user", "content": "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±ã€‚"}
    ],
    user_context=UserContext(
        user_id="test_user",
        org_id="test_org",
        client_type="python-sdk"
    ),
    stream=False,  # éæµå¼è°ƒç”¨
    temperature=0.7,  # å¯é€‰å‚æ•°
    max_tokens=1000,  # å¯é€‰å‚æ•°
)

# å‘é€è¯·æ±‚å¹¶è·å–å“åº”
response = client.invoke(request_data)
if response.error:
    print(f"é”™è¯¯: {response.error}")
else:
    print(f"å“åº”: {response.content}")
    if response.usage:
        print(f"Token ä½¿ç”¨æƒ…å†µ: {response.usage}")
```

### Google è°ƒç”¨ç¤ºä¾‹ ï¼ˆAI Studio / Vertex AIï¼‰

```python
from tamar_model_client import TamarModelClient
from tamar_model_client.schemas import ModelRequest, UserContext
from tamar_model_client.enums import ProviderType, InvokeType, Channel

# åˆ›å»ºåŒæ­¥å®¢æˆ·ç«¯
client = TamarModelClient()

# Google AI Studio è°ƒç”¨ç¤ºä¾‹
request_data = ModelRequest(
    provider=ProviderType.GOOGLE,  # é€‰æ‹© Google ä½œä¸ºæä¾›å•†
    channel=Channel.AI_STUDIO,  # ä½¿ç”¨ AI Studio æ¸ é“
    invoke_type=InvokeType.GENERATION,  # ä½¿ç”¨ç”Ÿæˆè°ƒç”¨ç±»å‹
    model="gemini-pro",  # æŒ‡å®šå…·ä½“æ¨¡å‹
    contents=[
        {"role": "user", "parts": [{"text": "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±ã€‚"}]}
    ],
    user_context=UserContext(
        user_id="test_user",
        org_id="test_org",
        client_type="python-sdk"
    ),
    temperature=0.7,  # å¯é€‰å‚æ•°
)

# å‘é€è¯·æ±‚å¹¶è·å–å“åº”
response = client.invoke(request_data)
if response.error:
    print(f"é”™è¯¯: {response.error}")
else:
    print(f"å“åº”: {response.content}")
    if response.usage:
        print(f"Token ä½¿ç”¨æƒ…å†µ: {response.usage}")

# Google Vertex AI è°ƒç”¨ç¤ºä¾‹
vertex_request = ModelRequest(
    provider=ProviderType.GOOGLE,  # é€‰æ‹© Google ä½œä¸ºæä¾›å•†
    channel=Channel.VERTEXAI,  # ä½¿ç”¨ Vertex AI æ¸ é“
    invoke_type=InvokeType.GENERATION,  # ä½¿ç”¨ç”Ÿæˆè°ƒç”¨ç±»å‹
    model="gemini-pro",  # æŒ‡å®šå…·ä½“æ¨¡å‹
    contents=[
        {"role": "user", "parts": [{"text": "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±ã€‚"}]}
    ],
    user_context=UserContext(
        user_id="test_user",
        org_id="test_org",
        client_type="python-sdk"
    ),
    temperature=0.7,  # å¯é€‰å‚æ•°
)

# å‘é€è¯·æ±‚å¹¶è·å–å“åº”
vertex_response = client.invoke(vertex_request)
if vertex_response.error:
    print(f"é”™è¯¯: {vertex_response.error}")
else:
    print(f"å“åº”: {vertex_response.content}")
    if vertex_response.usage:
        print(f"Token ä½¿ç”¨æƒ…å†µ: {vertex_response.usage}")

# Google GenAI å›¾åƒç”Ÿæˆç¤ºä¾‹
from google.genai import types

genai_image_request = ModelRequest(
    provider=ProviderType.GOOGLE,  # é€‰æ‹© Google ä½œä¸ºæä¾›å•†
    channel=Channel.AI_STUDIO,  # ä½¿ç”¨ AI Studio æ¸ é“
    invoke_type=InvokeType.IMAGE_GENERATION_GENAI,  # ä½¿ç”¨ GenAI å›¾åƒç”Ÿæˆè°ƒç”¨ç±»å‹
    model="imagen-3.0-generate-001",  # æŒ‡å®šå›¾åƒç”Ÿæˆæ¨¡å‹
    prompt="ä¸€åªå¯çˆ±çš„å°çŒ«åœ¨èŠ±å›­é‡Œç©è€",  # å›¾åƒæè¿°æç¤ºè¯
    user_context=UserContext(
        user_id="test_user",
        org_id="test_org",
        client_type="python-sdk"
    ),
    # ä½¿ç”¨ Google GenAI ç±»å‹æ„å»ºé…ç½®
    config=types.GenerateImagesConfig(
        number_of_images=1,
        aspect_ratio="1:1",
        safety_filter_level="block_some"
    )
)

# å‘é€å›¾åƒç”Ÿæˆè¯·æ±‚å¹¶è·å–å“åº”
image_response = client.invoke(genai_image_request)
if image_response.error:
    print(f"é”™è¯¯: {image_response.error}")
else:
    print(f"å›¾åƒç”ŸæˆæˆåŠŸ: {image_response.content}")
    if image_response.usage:
        print(f"ä½¿ç”¨æƒ…å†µ: {image_response.usage}")
```

### Azure OpenAI è°ƒç”¨ç¤ºä¾‹

```python
from tamar_model_client import TamarModelClient
from tamar_model_client.schemas import ModelRequest, UserContext
from tamar_model_client.enums import ProviderType, InvokeType, Channel

# åˆ›å»ºåŒæ­¥å®¢æˆ·ç«¯
client = TamarModelClient()

# Azure OpenAI è°ƒç”¨ç¤ºä¾‹
request_data = ModelRequest(
    provider=ProviderType.AZURE,  # é€‰æ‹© Azure ä½œä¸ºæä¾›å•†
    channel=Channel.OPENAI,  # ä½¿ç”¨ OpenAI æ¸ é“
    invoke_type=InvokeType.CHAT_COMPLETIONS,  # ä½¿ç”¨ chat completions è°ƒç”¨ç±»å‹
    model="gpt-4o-mini",  # æŒ‡å®šå…·ä½“æ¨¡å‹
    messages=[
        {"role": "user", "content": "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±ã€‚"}
    ],
    user_context=UserContext(
        user_id="test_user",
        org_id="test_org",
        client_type="python-sdk"
    ),
    stream=False,  # éæµå¼è°ƒç”¨
    temperature=0.7,  # å¯é€‰å‚æ•°
    max_tokens=1000,  # å¯é€‰å‚æ•°
)

# å‘é€è¯·æ±‚å¹¶è·å–å“åº”
response = client.invoke(request_data)
if response.error:
    print(f"é”™è¯¯: {response.error}")
else:
    print(f"å“åº”: {response.content}")
    if response.usage:
        print(f"Token ä½¿ç”¨æƒ…å†µ: {response.usage}")
```

### å¼‚æ­¥è°ƒç”¨ç¤ºä¾‹

```python
import asyncio
from tamar_model_client import AsyncTamarModelClient
from tamar_model_client.schemas import ModelRequest, UserContext
from tamar_model_client.enums import ProviderType, InvokeType, Channel


async def main():
    # åˆ›å»ºå¼‚æ­¥å®¢æˆ·ç«¯
    client = AsyncTamarModelClient()

    # ç»„è£…è¯·æ±‚å‚æ•°
    request_data = ModelRequest(
        provider=ProviderType.OPENAI,
        channel=Channel.OPENAI,
        invoke_type=InvokeType.CHAT_COMPLETIONS,
        model="gpt-4o-mini",
        messages=[
            {"role": "user", "content": "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±ã€‚"}
        ],
        user_context=UserContext(
            user_id="test_user",
            org_id="test_org",
            client_type="python-sdk"
        ),
        stream=False,
        temperature=0.7,
        max_tokens=1000,
    )

    # å‘é€è¯·æ±‚å¹¶è·å–å“åº”
    async for r in await client.invoke(request_data):
        if r.error:
            print(f"é”™è¯¯: {r.error}")
        else:
            print(f"å“åº”: {r.content}")
            if r.usage:
                print(f"Token ä½¿ç”¨æƒ…å†µ: {r.usage}")


# è¿è¡Œå¼‚æ­¥ç¤ºä¾‹
asyncio.run(main())
```

### æµå¼è°ƒç”¨ç¤ºä¾‹

```python
import asyncio
from tamar_model_client import AsyncTamarModelClient
from tamar_model_client.schemas import ModelRequest, UserContext
from tamar_model_client.enums import ProviderType, InvokeType, Channel


async def stream_example():
    # åˆ›å»ºå¼‚æ­¥å®¢æˆ·ç«¯
    client = AsyncTamarModelClient()

    # ç»„è£…è¯·æ±‚å‚æ•°
    request_data = ModelRequest(
        provider=ProviderType.OPENAI,
        channel=Channel.OPENAI,
        invoke_type=InvokeType.CHAT_COMPLETIONS,
        model="gpt-4",
        messages=[
            {"role": "user", "content": "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±ã€‚"}
        ],
        user_context=UserContext(
            user_id="test_user",
            org_id="test_org",
            client_type="python-sdk"
        ),
        stream=True,  # å¯ç”¨æµå¼è¾“å‡º
        temperature=0.7,
    )

    # å‘é€è¯·æ±‚å¹¶è·å–æµå¼å“åº”
    async for response in client.invoke(request_data):
        if response.error:
            print(f"é”™è¯¯: {response.error}")
        else:
            print(f"å“åº”ç‰‡æ®µ: {response.content}", end="", flush=True)
            if response.usage:
                print(f"\nToken ä½¿ç”¨æƒ…å†µ: {response.usage}")


# è¿è¡Œæµå¼ç¤ºä¾‹
asyncio.run(stream_example())
```

### æ‰¹é‡è°ƒç”¨ç¤ºä¾‹

æ”¯æŒæ‰¹é‡å¤„ç†å¤šä¸ªæ¨¡å‹è¯·æ±‚ï¼š

```python
import asyncio
from tamar_model_client import AsyncTamarModelClient
from tamar_model_client.schemas import (
    BatchModelRequest, BatchModelRequestItem,
    UserContext
)
from tamar_model_client.enums import ProviderType, InvokeType, Channel


async def batch_example():
    # åˆ›å»ºå¼‚æ­¥å®¢æˆ·ç«¯
    client = AsyncTamarModelClient()

    # ç»„è£…æ‰¹é‡è¯·æ±‚å‚æ•°
    batch_request = BatchModelRequest(
        user_context=UserContext(
            user_id="test_user",
            org_id="test_org",
            client_type="python-sdk"
        ),
        items=[
            BatchModelRequestItem(
                provider=ProviderType.OPENAI,
                channel=Channel.OPENAI,
                invoke_type=InvokeType.CHAT_COMPLETIONS,
                model="gpt-4",
                messages=[
                    {"role": "user", "content": "ç¬¬ä¸€ä¸ªé—®é¢˜ï¼šä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ"}
                ],
                priority=1,
                custom_id="q1"
            ),
            BatchModelRequestItem(
                provider=ProviderType.GOOGLE,
                channel=Channel.AI_STUDIO,
                invoke_type=InvokeType.GENERATION,
                model="gemini-pro",
                contents=[
                    {"role": "user", "parts": [{"text": "ç¬¬äºŒä¸ªé—®é¢˜ï¼šä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ"}]}
                ],
                priority=2,
                custom_id="q2"
            )
        ]
    )

    # å‘é€æ‰¹é‡è¯·æ±‚å¹¶è·å–å“åº”
    response = await client.invoke_batch(batch_request)
    if response.responses:
        for resp in response.responses:
            print(f"\né—®é¢˜ {resp.custom_id} çš„å“åº”:")
            if resp.error:
                print(f"é”™è¯¯: {resp.error}")
            else:
                print(f"å†…å®¹: {resp.content}")
                if resp.usage:
                    print(f"Token ä½¿ç”¨æƒ…å†µ: {resp.usage}")


# è¿è¡Œæ‰¹é‡è°ƒç”¨ç¤ºä¾‹
asyncio.run(batch_example())
```

### å›¾åƒç”Ÿæˆè°ƒç”¨ç¤ºä¾‹

æ”¯æŒ OpenAI DALL-Eã€Google Vertex AI å’Œ Google GenAI å›¾åƒç”Ÿæˆï¼š

```python
from tamar_model_client import TamarModelClient
from tamar_model_client.schemas import ModelRequest, UserContext
from tamar_model_client.enums import ProviderType, InvokeType, Channel

client = TamarModelClient()

# OpenAI DALL-E å›¾åƒç”Ÿæˆ
openai_image_request = ModelRequest(
    provider=ProviderType.OPENAI,
    channel=Channel.OPENAI,
    invoke_type=InvokeType.IMAGE_GENERATION,
    model="dall-e-3",
    prompt="ä¸€åªç©¿ç€è¥¿è£…çš„çŒ«åœ¨åŠå…¬å®¤é‡Œå·¥ä½œ",
    user_context=UserContext(
        user_id="test_user",
        org_id="test_org",
        client_type="python-sdk"
    ),
    size="1024x1024",
    quality="hd",
    n=1
)

# Google Vertex AI å›¾åƒç”Ÿæˆ
vertex_image_request = ModelRequest(
    provider=ProviderType.GOOGLE,
    channel=Channel.VERTEXAI,
    invoke_type=InvokeType.IMAGE_GENERATION,
    model="imagegeneration@006",
    prompt="ä¸€åº§ç¾ä¸½çš„å±±å³°åœ¨æ—¥å‡ºæ—¶åˆ†",
    user_context=UserContext(
        user_id="test_user",
        org_id="test_org",
        client_type="python-sdk"
    ),
    number_of_images=1,
    aspect_ratio="1:1",
    safety_filter_level="block_some"
)

# Google GenAI å›¾åƒç”Ÿæˆï¼ˆæ–°å¢åŠŸèƒ½ï¼‰
genai_image_request = ModelRequest(
    provider=ProviderType.GOOGLE,
    channel=Channel.AI_STUDIO,
    invoke_type=InvokeType.IMAGE_GENERATION_GENAI,  # æ–°å¢çš„è°ƒç”¨ç±»å‹
    model="imagen-3.0-generate-001",
    prompt="ç§‘å¹»é£æ ¼çš„åŸå¸‚å¤œæ™¯ï¼Œéœ“è™¹ç¯é—ªçƒ",
    user_context=UserContext(
        user_id="test_user",
        org_id="test_org",
        client_type="python-sdk"
    ),
    config=types.GenerateImagesConfig(
        number_of_images=1,
        aspect_ratio="16:9"
    )
)

# å‘é€è¯·æ±‚
for request in [openai_image_request, vertex_image_request, genai_image_request]:
    response = client.invoke(request)
    if response.error:
        print(f"å›¾åƒç”Ÿæˆå¤±è´¥: {response.error}")
    else:
        print(f"å›¾åƒç”ŸæˆæˆåŠŸ: {response.content}")
```

### æ–‡ä»¶è¾“å…¥ç¤ºä¾‹

æ”¯æŒå¤„ç†å›¾åƒç­‰æ–‡ä»¶è¾“å…¥ï¼ˆéœ€ä½¿ç”¨æ”¯æŒå¤šæ¨¡æ€çš„æ¨¡å‹ï¼Œå¦‚ gemini-2.0-flashï¼‰ï¼š

```python
from tamar_model_client import TamarModelClient
from tamar_model_client.schemas import ModelRequest, UserContext
from tamar_model_client.enums import ProviderType
from google.genai.types import Part
model_request = ModelRequest(
    provider=ProviderType.GOOGLE,  # é€‰æ‹© Googleä½œä¸ºæä¾›å•†
    model="gemini-2.0-flash",
    contents=[
        "What is shown in this image?",
        Part.from_uri( # è¿™ä¸ªæ˜¯Googleé‚£è¾¹çš„å‚æ•°æ”¯æŒ
            file_uri="https://images.pexels.com/photos/248797/pexels-photo-248797.jpeg",
            mime_type="image/jpeg",
        ),
    ],
    user_context=UserContext(
        org_id="testllm",
        user_id="testllm",
        client_type="conversation-service"
    ),
)
client = TamarModelClient("localhost:50051")
response = client.invoke(
    model_request=model_request
)
```

### ğŸ”„ é”™è¯¯å¤„ç†æœ€ä½³å®è·µ

SDK æä¾›äº†å®Œå–„çš„å¼‚å¸¸ä½“ç³»ï¼Œä¾¿äºç²¾ç¡®å¤„ç†ä¸åŒç±»å‹çš„é”™è¯¯ï¼š

```python
from tamar_model_client import TamarModelClient
from tamar_model_client.exceptions import (
    TamarModelException,
    NetworkException,
    AuthenticationException,
    RateLimitException,
    ProviderException,
    TimeoutException
)

client = TamarModelClient()

try:
    response = client.invoke(request)
except TimeoutException as e:
    # å¤„ç†è¶…æ—¶é”™è¯¯
    logger.warning(f"è¯·æ±‚è¶…æ—¶: {e.message}, request_id: {e.request_id}")
    # å¯ä»¥é‡è¯•æˆ–ä½¿ç”¨æ›´å¿«çš„æ¨¡å‹
except RateLimitException as e:
    # å¤„ç†é™æµé”™è¯¯
    logger.error(f"è§¦å‘é™æµ: {e.message}")
    # ç­‰å¾…ä¸€æ®µæ—¶é—´åé‡è¯•
    time.sleep(60)
except AuthenticationException as e:
    # å¤„ç†è®¤è¯é”™è¯¯
    logger.error(f"è®¤è¯å¤±è´¥: {e.message}")
    # æ£€æŸ¥ JWT é…ç½®
except NetworkException as e:
    # å¤„ç†ç½‘ç»œé”™è¯¯ï¼ˆå·²è‡ªåŠ¨é‡è¯•åä»å¤±è´¥ï¼‰
    logger.error(f"ç½‘ç»œé”™è¯¯: {e.message}")
    # å¯èƒ½éœ€è¦æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–æœåŠ¡çŠ¶æ€
except ProviderException as e:
    # å¤„ç†æä¾›å•†ç‰¹å®šé”™è¯¯
    logger.error(f"æä¾›å•†é”™è¯¯: {e.message}")
    # æ ¹æ®é”™è¯¯ç è¿›è¡Œç‰¹å®šå¤„ç†
    if "insufficient_quota" in str(e):
        # åˆ‡æ¢åˆ°å…¶ä»–æä¾›å•†
        pass
except TamarModelException as e:
    # å¤„ç†å…¶ä»–æ‰€æœ‰æ¨¡å‹ç›¸å…³é”™è¯¯
    logger.error(f"æ¨¡å‹é”™è¯¯: {e.message}")
    logger.error(f"é”™è¯¯ä¸Šä¸‹æ–‡: {e.error_context}")
```

### ğŸ”€ å¤šæä¾›å•†æ— ç¼åˆ‡æ¢

è½»æ¾å®ç°æä¾›å•†ä¹‹é—´çš„åˆ‡æ¢å’Œé™çº§ï¼š

```python
from tamar_model_client import TamarModelClient
from tamar_model_client.schemas import ModelRequest, UserContext
from tamar_model_client.enums import ProviderType
from tamar_model_client.exceptions import ProviderException, RateLimitException

client = TamarModelClient()

# å®šä¹‰æä¾›å•†ä¼˜å…ˆçº§
providers = [
    (ProviderType.OPENAI, "gpt-4"),
    (ProviderType.GOOGLE, "gemini-pro"),
    (ProviderType.AZURE, "gpt-4o-mini")
]

user_context = UserContext(
    user_id="test_user",
    org_id="test_org",
    client_type="python-sdk"
)

# å°è¯•å¤šä¸ªæä¾›å•†ç›´åˆ°æˆåŠŸ
for provider, model in providers:
    try:
        request = ModelRequest(
            provider=provider,
            model=model,
            messages=[{"role": "user", "content": "Hello"}] if provider != ProviderType.GOOGLE else None,
            contents=[{"role": "user", "parts": [{"text": "Hello"}]}] if provider == ProviderType.GOOGLE else None,
            user_context=user_context
        )
        
        response = client.invoke(request)
        print(f"æˆåŠŸä½¿ç”¨ {provider.value} - {model}")
        print(f"å“åº”: {response.content}")
        break
        
    except (ProviderException, RateLimitException) as e:
        logger.warning(f"{provider.value} å¤±è´¥: {e.message}")
        continue
```

### ğŸ¯ è¯·æ±‚ä¸Šä¸‹æ–‡ç®¡ç†

ä½¿ç”¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨ç¡®ä¿èµ„æºæ­£ç¡®é‡Šæ”¾ï¼š

```python
from tamar_model_client import TamarModelClient, AsyncTamarModelClient
import asyncio

# åŒæ­¥å®¢æˆ·ç«¯ä¸Šä¸‹æ–‡ç®¡ç†å™¨
with TamarModelClient() as client:
    response = client.invoke(request)
    print(response.content)
# è‡ªåŠ¨è°ƒç”¨ client.close()

# å¼‚æ­¥å®¢æˆ·ç«¯ä¸Šä¸‹æ–‡ç®¡ç†å™¨
async def async_example():
    async with AsyncTamarModelClient() as client:
        response = await client.invoke(request)
        print(response.content)
    # è‡ªåŠ¨è°ƒç”¨ await client.close()

asyncio.run(async_example())
```

### â±ï¸ è¶…æ—¶æ§åˆ¶

é€šè¿‡ç¯å¢ƒå˜é‡æˆ–ä»£ç æ§åˆ¶è¯·æ±‚è¶…æ—¶ï¼š

```python
import os
from tamar_model_client import TamarModelClient

# æ–¹å¼ä¸€ï¼šç¯å¢ƒå˜é‡è®¾ç½®å…¨å±€è¶…æ—¶
os.environ['MODEL_MANAGER_SERVER_GRPC_TIMEOUT'] = '30'  # 30ç§’è¶…æ—¶

# æ–¹å¼äºŒï¼šåˆ›å»ºå®¢æˆ·ç«¯æ—¶è®¾ç½®
client = TamarModelClient(
    server_address="localhost:50051",
    timeout=30.0  # 30ç§’è¶…æ—¶
)

# å¤„ç†è¶…æ—¶
try:
    response = client.invoke(request)
except TimeoutException as e:
    logger.error(f"è¯·æ±‚è¶…æ—¶: {e.message}")
    # å¯ä»¥å°è¯•æ›´å°çš„æ¨¡å‹æˆ–å‡å°‘ max_tokens
```

### ğŸ“Š æ€§èƒ½ç›‘æ§ä¸æŒ‡æ ‡

è·å–è¯¦ç»†çš„æ€§èƒ½æŒ‡æ ‡å’Œä½¿ç”¨ç»Ÿè®¡ï¼š

```python
from tamar_model_client import TamarModelClient
import time

client = TamarModelClient()

# ç›‘æ§å•æ¬¡è¯·æ±‚æ€§èƒ½
start_time = time.time()
response = client.invoke(request)
latency = time.time() - start_time

print(f"è¯·æ±‚å»¶è¿Ÿ: {latency:.2f}ç§’")
print(f"Request ID: {response.request_id}")
if response.usage:
    print(f"è¾“å…¥ Tokens: {response.usage.prompt_tokens}")
    print(f"è¾“å‡º Tokens: {response.usage.completion_tokens}")
    print(f"æ€» Tokens: {response.usage.total_tokens}")
    print(f"é¢„ä¼°æˆæœ¬: ${response.usage.total_cost:.4f}")

# è·å–ç†”æ–­å™¨æŒ‡æ ‡
metrics = client.get_resilient_metrics()
if metrics:
    print(f"\nç†”æ–­å™¨çŠ¶æ€:")
    print(f"- çŠ¶æ€: {metrics['circuit_state']}")
    print(f"- å¤±è´¥æ¬¡æ•°: {metrics['failure_count']}")
    print(f"- ä¸Šæ¬¡å¤±è´¥: {metrics['last_failure_time']}")
    print(f"- HTTPé™çº§åœ°å€: {metrics['http_fallback_url']}")
```

### ğŸ”§ è‡ªå®šä¹‰é…ç½®ç¤ºä¾‹

çµæ´»çš„é…ç½®é€‰é¡¹æ»¡è¶³ä¸åŒåœºæ™¯éœ€æ±‚ï¼š

```python
from tamar_model_client import TamarModelClient

# å®Œæ•´é…ç½®ç¤ºä¾‹
client = TamarModelClient(
    # æœåŠ¡å™¨é…ç½®
    server_address="grpc.example.com:443",
    use_tls=True,
    default_authority="grpc.example.com",
    
    # è®¤è¯é…ç½®
    jwt_secret_key="your-secret-key",
    jwt_expiration=3600,  # 1å°æ—¶è¿‡æœŸ
    
    # é‡è¯•é…ç½®
    max_retries=5,
    retry_delay=1.0,
    
    # è¶…æ—¶é…ç½®
    timeout=60.0,
    
    # ç†”æ–­é™çº§é…ç½®
    resilient_enabled=True,
    http_fallback_url="https://backup.example.com",
    circuit_breaker_threshold=3,
    circuit_breaker_timeout=30
)
```

### ğŸ” å®‰å…¨æœ€ä½³å®è·µ

ç¡®ä¿ SDK ä½¿ç”¨çš„å®‰å…¨æ€§ï¼š

```python
import os
from tamar_model_client import TamarModelClient

# 1. ä½¿ç”¨ç¯å¢ƒå˜é‡å­˜å‚¨æ•æ„Ÿä¿¡æ¯
os.environ['MODEL_MANAGER_SERVER_JWT_SECRET_KEY'] = os.getenv('JWT_SECRET')

# 2. å¯ç”¨ TLS åŠ å¯†
client = TamarModelClient(
    server_address="grpc.example.com:443",
    use_tls=True
)

# 3. æœ€å°æƒé™åŸåˆ™ - åªè¯·æ±‚éœ€è¦çš„æ•°æ®
request = ModelRequest(
    provider=ProviderType.OPENAI,
    model="gpt-3.5-turbo",
    messages=[{"role": "user", "content": "åˆ†æè¿™æ®µæ–‡æœ¬"}],
    user_context=UserContext(
        user_id="limited_user",
        org_id="restricted_org",
        client_type="analysis-service"
    ),
    max_tokens=100  # é™åˆ¶è¾“å‡ºé•¿åº¦
)

# 4. å®¡è®¡æ—¥å¿—
response = client.invoke(request)
logger.info(f"AIè¯·æ±‚å®¡è®¡: user={request.user_context.user_id}, model={request.model}, request_id={response.request_id}")
```

### ğŸš€ å¹¶å‘è¯·æ±‚ä¼˜åŒ–

é«˜æ•ˆå¤„ç†å¤§é‡å¹¶å‘è¯·æ±‚ï¼š

```python
import asyncio
from tamar_model_client import AsyncTamarModelClient
from tamar_model_client.schemas import ModelRequest, UserContext

async def process_batch_async(questions: list[str]):
    """å¼‚æ­¥å¹¶å‘å¤„ç†å¤šä¸ªé—®é¢˜"""
    async with AsyncTamarModelClient() as client:
        tasks = []
        
        for i, question in enumerate(questions):
            request = ModelRequest(
                provider=ProviderType.OPENAI,
                model="gpt-3.5-turbo",
                messages=[{"role": "user", "content": question}],
                user_context=UserContext(
                    user_id="batch_user",
                    org_id="test_org",
                    client_type="batch-processor"
                )
            )
            
            # åˆ›å»ºå¼‚æ­¥ä»»åŠ¡
            task = asyncio.create_task(client.invoke(request))
            tasks.append((i, task))
        
        # å¹¶å‘æ‰§è¡Œæ‰€æœ‰è¯·æ±‚
        results = []
        for i, task in tasks:
            try:
                response = await task
                results.append((i, response.content))
            except Exception as e:
                results.append((i, f"Error: {str(e)}"))
        
        return results

# ä½¿ç”¨ç¤ºä¾‹
questions = [
    "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ",
    "è§£é‡Šæœºå™¨å­¦ä¹ çš„åŸç†",
    "æ·±åº¦å­¦ä¹ å’Œæœºå™¨å­¦ä¹ çš„åŒºåˆ«",
    "ä»€ä¹ˆæ˜¯ç¥ç»ç½‘ç»œï¼Ÿ"
]

results = asyncio.run(process_batch_async(questions))
for i, content in results:
    print(f"é—®é¢˜ {i+1} çš„å›ç­”: {content[:100]}...")
```

## ğŸ› ï¸ é«˜çº§åŠŸèƒ½

### ğŸ”¥ ä½¿ç”¨åœºæ™¯å’Œæœ€ä½³å®è·µ

#### ä½¿ç”¨åœºæ™¯
1. **å¤šæ¨¡å‹æ¯”è¾ƒ**ï¼šåŒæ—¶è°ƒç”¨å¤šä¸ªæœåŠ¡å•†çš„æ¨¡å‹ï¼Œæ¯”è¾ƒè¾“å‡ºè´¨é‡
2. **æˆæœ¬ä¼˜åŒ–**ï¼šæ ¹æ®ä»»åŠ¡ç±»å‹è‡ªåŠ¨é€‰æ‹©æ€§ä»·æ¯”æœ€é«˜çš„æ¨¡å‹
3. **é«˜å¯ç”¨æ¶æ„**ï¼šä¸»å¤‡æ¨¡å‹è‡ªåŠ¨åˆ‡æ¢ï¼Œç¡®ä¿æœåŠ¡ç¨³å®š
4. **ç»Ÿä¸€ç›‘æ§**ï¼šé›†ä¸­ç®¡ç†æ‰€æœ‰ AI æœåŠ¡çš„ä½¿ç”¨é‡å’Œæˆæœ¬

#### æœ€ä½³å®è·µ
1. **å®¢æˆ·ç«¯ç®¡ç†**
   ```python
   # âœ… æ¨èï¼šå•ä¾‹æ¨¡å¼ä½¿ç”¨
   client = TamarModelClient()
   # æ•´ä¸ªåº”ç”¨ç”Ÿå‘½å‘¨æœŸä½¿ç”¨åŒä¸€ä¸ªå®¢æˆ·ç«¯
   
   # âŒ é¿å…ï¼šé¢‘ç¹åˆ›å»ºå®¢æˆ·ç«¯
   for i in range(100):
       client = TamarModelClient()  # ä¸æ¨èï¼
   ```

2. **é”™è¯¯å¤„ç†**
   ```python
   try:
       response = client.invoke(request)
   except TamarModelException as e:
       logger.error(f"Model error: {e.message}, request_id: {e.request_id}")
       # å®æ–½é™çº§ç­–ç•¥æˆ–é‡è¯•
   ```

3. **æ€§èƒ½ä¼˜åŒ–**
   - ä½¿ç”¨æ‰¹é‡ API å¤„ç†å¤§é‡è¯·æ±‚
   - å¯ç”¨æµå¼å“åº”å‡å°‘é¦–å­—å»¶è¿Ÿ
   - åˆç†è®¾ç½® max_tokens é¿å…æµªè´¹

### ğŸ›¡ï¸ ç†”æ–­é™çº§åŠŸèƒ½ï¼ˆé«˜å¯ç”¨ä¿éšœï¼‰

SDK å†…ç½®äº†ç†”æ–­é™çº§æœºåˆ¶ï¼Œå½“ gRPC æœåŠ¡ä¸å¯ç”¨æ—¶è‡ªåŠ¨åˆ‡æ¢åˆ° HTTP æœåŠ¡ï¼Œç¡®ä¿ä¸šåŠ¡è¿ç»­æ€§ã€‚

#### å·¥ä½œåŸç†
1. **æ­£å¸¸çŠ¶æ€**ï¼šæ‰€æœ‰è¯·æ±‚é€šè¿‡é«˜æ€§èƒ½çš„ gRPC åè®®
2. **ç†”æ–­è§¦å‘**ï¼šå½“è¿ç»­å¤±è´¥è¾¾åˆ°é˜ˆå€¼æ—¶ï¼Œç†”æ–­å™¨æ‰“å¼€
3. **è‡ªåŠ¨é™çº§**ï¼šåˆ‡æ¢åˆ° HTTP åè®®ç»§ç»­æä¾›æœåŠ¡
4. **å®šæœŸæ¢å¤**ï¼šç†”æ–­å™¨ä¼šå®šæœŸå°è¯•æ¢å¤åˆ° gRPC

#### å¯ç”¨æ–¹å¼
```bash
# è®¾ç½®ç¯å¢ƒå˜é‡
export MODEL_CLIENT_RESILIENT_ENABLED=true
export MODEL_CLIENT_HTTP_FALLBACK_URL=http://localhost:8080
export MODEL_CLIENT_CIRCUIT_BREAKER_THRESHOLD=5
export MODEL_CLIENT_CIRCUIT_BREAKER_TIMEOUT=60
```

#### ä½¿ç”¨ç¤ºä¾‹
```python
from tamar_model_client import TamarModelClient

# å®¢æˆ·ç«¯ä¼šè‡ªåŠ¨å¤„ç†ç†”æ–­é™çº§ï¼Œå¯¹ä½¿ç”¨è€…é€æ˜
client = TamarModelClient()

# æ­£å¸¸ä½¿ç”¨ï¼Œæ— éœ€å…³å¿ƒåº•å±‚åè®®
response = client.invoke(request)

# è·å–ç†”æ–­å™¨çŠ¶æ€ï¼ˆå¯é€‰ï¼‰
metrics = client.get_resilient_metrics()
if metrics:
    print(f"ç†”æ–­å™¨çŠ¶æ€: {metrics['circuit_state']}")
    print(f"å¤±è´¥æ¬¡æ•°: {metrics['failure_count']}")
```

#### ç†”æ–­å™¨çŠ¶æ€
- **CLOSED**ï¼ˆå…³é—­ï¼‰ï¼šæ­£å¸¸å·¥ä½œçŠ¶æ€ï¼Œè¯·æ±‚æ­£å¸¸é€šè¿‡
- **OPEN**ï¼ˆæ‰“å¼€ï¼‰ï¼šç†”æ–­çŠ¶æ€ï¼Œæ‰€æœ‰è¯·æ±‚ç›´æ¥é™çº§åˆ° HTTP
- **HALF_OPEN**ï¼ˆåŠå¼€ï¼‰ï¼šæ¢å¤æµ‹è¯•çŠ¶æ€ï¼Œå…è®¸å°‘é‡è¯·æ±‚æµ‹è¯• gRPC æ˜¯å¦æ¢å¤

#### ç›‘æ§æŒ‡æ ‡
```python
# è·å–ç†”æ–­é™çº§æŒ‡æ ‡
metrics = client.get_resilient_metrics()
# è¿”å›ç¤ºä¾‹ï¼š
# {
#     "enabled": true,
#     "circuit_state": "closed",
#     "failure_count": 0,
#     "last_failure_time": null,
#     "http_fallback_url": "http://localhost:8080"
# }
```

### ğŸš€ å¿«é€Ÿé™çº§åŠŸèƒ½ï¼ˆç”¨æˆ·ä½“éªŒä¼˜åŒ–ï¼‰

åœ¨ä¼ ç»Ÿçš„ç†”æ–­é™çº§åŸºç¡€ä¸Šï¼ŒSDK æ–°å¢äº†å¿«é€Ÿé™çº§åŠŸèƒ½ï¼Œè¿›ä¸€æ­¥æå‡ç”¨æˆ·ä½“éªŒï¼š

#### ä¼ ç»Ÿé™çº§ vs å¿«é€Ÿé™çº§

**ä¼ ç»Ÿæ¨¡å¼**ï¼š
```
gRPCè¯·æ±‚ â†’ å¤±è´¥ â†’ é‡è¯•1 â†’ å¤±è´¥ â†’ é‡è¯•2 â†’ å¤±è´¥ â†’ ... â†’ é‡è¯•N â†’ å¤±è´¥ â†’ HTTPé™çº§
è€—æ—¶ï¼š(é‡è¯•æ¬¡æ•° Ã— é€€é¿æ—¶é—´) + é™çº§æ—¶é—´  // å¯èƒ½éœ€è¦åå‡ ç§’
```

**å¿«é€Ÿé™çº§æ¨¡å¼**ï¼š
```
gRPCè¯·æ±‚ â†’ å¤±è´¥ â†’ ç«‹å³HTTPé™çº§ (æˆ–é‡è¯•1æ¬¡åé™çº§)
è€—æ—¶ï¼šé™çº§æ—¶é—´  // é€šå¸¸1-2ç§’å†…å®Œæˆ
```

#### é™çº§ç­–ç•¥é…ç½®

- **ç«‹å³é™çº§é”™è¯¯**ï¼š`UNAVAILABLE`, `DEADLINE_EXCEEDED`, `CANCELLED` (ç½‘ç»œé—®é¢˜)
- **å»¶è¿Ÿé™çº§é”™è¯¯**ï¼šå…¶ä»–é”™è¯¯é‡è¯•æŒ‡å®šæ¬¡æ•°åé™çº§
- **æ°¸ä¸é™çº§é”™è¯¯**ï¼š`UNAUTHENTICATED`, `PERMISSION_DENIED`, `INVALID_ARGUMENT` (å®¢æˆ·ç«¯é—®é¢˜)

#### æ‰¹é‡è¯·æ±‚é™çº§æ”¯æŒ

å¿«é€Ÿé™çº§åŒæ—¶æ”¯æŒå•ä¸ªè¯·æ±‚å’Œæ‰¹é‡è¯·æ±‚ï¼š

```python
# å•ä¸ªè¯·æ±‚é™çº§
response = client.invoke(request)  # è‡ªåŠ¨é™çº§åˆ° /v1/invoke

# æ‰¹é‡è¯·æ±‚é™çº§  
batch_response = client.invoke_batch(batch_request)  # è‡ªåŠ¨é™çº§åˆ° /v1/batch-invoke
```

#### ä½¿ç”¨ç¤ºä¾‹

```python
from tamar_model_client import TamarModelClient

# å¯ç”¨å¿«é€Ÿé™çº§ï¼ˆé€šè¿‡ç¯å¢ƒå˜é‡ï¼‰
# MODEL_CLIENT_FAST_FALLBACK_ENABLED=true
# MODEL_CLIENT_FALLBACK_AFTER_RETRIES=1

client = TamarModelClient()

# æ­£å¸¸ä½¿ç”¨ï¼Œå¿«é€Ÿé™çº§å¯¹ç”¨æˆ·é€æ˜
response = client.invoke(request)
# å¦‚æœgRPCä¸å¯ç”¨ï¼Œä¼šåœ¨1-2ç§’å†…è‡ªåŠ¨åˆ‡æ¢åˆ°HTTPå¹¶è¿”å›ç»“æœ
```

#### é…ç½®é€‰é¡¹è¯¦è§£

```bash
# å¯ç”¨å¿«é€Ÿé™çº§ï¼ˆé»˜è®¤falseï¼Œå»ºè®®å¼€å¯ï¼‰
MODEL_CLIENT_FAST_FALLBACK_ENABLED=true

# éç«‹å³é™çº§çš„é”™è¯¯ï¼Œé‡è¯•å¤šå°‘æ¬¡åé™çº§ï¼ˆé»˜è®¤1æ¬¡ï¼‰
MODEL_CLIENT_FALLBACK_AFTER_RETRIES=1

# ç½‘ç»œé”™è¯¯ç«‹å³é™çº§ï¼ˆé»˜è®¤é…ç½®ï¼‰
MODEL_CLIENT_IMMEDIATE_FALLBACK_ERRORS=UNAVAILABLE,DEADLINE_EXCEEDED,CANCELLED

# è®¤è¯é”™è¯¯æ°¸ä¸é™çº§ï¼ˆé¿å…æ— æ•ˆé™çº§ï¼‰
MODEL_CLIENT_NEVER_FALLBACK_ERRORS=UNAUTHENTICATED,PERMISSION_DENIED,INVALID_ARGUMENT
```

### ğŸ” è¯·æ±‚è¿½è¸ªä¸ç›‘æ§

SDK æä¾›äº†å®Œå–„çš„è¯·æ±‚è¿½è¸ªåŠŸèƒ½ï¼Œä¾¿äºé—®é¢˜æ’æŸ¥å’Œæ€§èƒ½ç›‘æ§ï¼š

#### è¯·æ±‚ ID è¿½è¸ª

æ¯ä¸ªè¯·æ±‚éƒ½ä¼šè‡ªåŠ¨ç”Ÿæˆå”¯ä¸€çš„ `request_id`ï¼Œç”¨äºè¿½è¸ªå•æ¬¡è¯·æ±‚ï¼š

```python
from tamar_model_client import TamarModelClient
from tamar_model_client.core import generate_request_id, set_request_id

# è‡ªåŠ¨ç”Ÿæˆ request_id
response = client.invoke(request)
print(f"Request ID: {response.request_id}")

# æ‰‹åŠ¨è®¾ç½® request_id
custom_request_id = generate_request_id()
set_request_id(custom_request_id)
response = client.invoke(request)
```

#### åŸå§‹è¯·æ±‚ ID è¿½è¸ª

å¯¹äºéœ€è¦è·¨å¤šä¸ªæœåŠ¡è°ƒç”¨çš„åœºæ™¯ï¼Œå¯ä»¥ä½¿ç”¨ `origin_request_id` è¿›è¡Œå…¨é“¾è·¯è¿½è¸ªï¼š

```python
from tamar_model_client.core import set_origin_request_id

# è®¾ç½®åŸå§‹è¯·æ±‚ IDï¼ˆé€šå¸¸æ¥è‡ªä¸Šæ¸¸æœåŠ¡ï¼‰
set_origin_request_id("user-provided-id-123")

# æ‰€æœ‰åç»­è¯·æ±‚éƒ½ä¼šæºå¸¦è¿™ä¸ª origin_request_id
response = client.invoke(request)
```

#### ç»“æ„åŒ–æ—¥å¿—

å¯ç”¨ JSON æ—¥å¿—æ ¼å¼åï¼Œæ¯æ¡æ—¥å¿—éƒ½åŒ…å«å®Œæ•´çš„è¿½è¸ªä¿¡æ¯ï¼š

```json
{
  "timestamp": "2025-07-03T14:40:32.729313",
  "level": "INFO",
  "type": "request",
  "uri": "/invoke/openai/chat",
  "request_id": "448a64f4-3bb0-467c-af15-d4181d0ac499",
  "data": {
    "origin_request_id": "user-provided-id-123",
    "provider": "openai",
    "model": "gpt-4",
    "stream": false
  },
  "message": "ğŸš€ Invoke request started"
}
```

#### é”™è¯¯è¿½è¸ª

é”™è¯¯æ—¥å¿—åŒ…å«å¼‚å¸¸å †æ ˆå’Œå®Œæ•´ä¸Šä¸‹æ–‡ï¼š

```json
{
  "timestamp": "2025-07-03T14:40:35.123456",
  "level": "ERROR",
  "type": "response",
  "request_id": "448a64f4-3bb0-467c-af15-d4181d0ac499",
  "data": {
    "origin_request_id": "user-provided-id-123",
    "error_code": "DEADLINE_EXCEEDED",
    "error_message": "Request timeout after 30 seconds",
    "retry_count": 2,
    "fallback_attempted": true
  },
  "exception": {
    "type": "TimeoutException",
    "message": "Request timeout after 30 seconds",
    "traceback": ["Traceback (most recent call last):", "..."]
  }
}
```

### âš ï¸ æ³¨æ„äº‹é¡¹

1. **å‚æ•°è¯´æ˜**
   - **å¿…å¡«å‚æ•°**ï¼š`provider`, `model`, `user_context`
   - **å¯é€‰å‚æ•°**ï¼š`channel`, `invoke_type`ï¼ˆç³»ç»Ÿå¯è‡ªåŠ¨æ¨æ–­ï¼‰
   - **æµå¼æ§åˆ¶**ï¼šé€šè¿‡ `stream=True/False` å‚æ•°æ§åˆ¶

2. **è¿æ¥ç®¡ç†**
   - gRPC ä½¿ç”¨ HTTP/2 é•¿è¿æ¥ï¼Œå®¢æˆ·ç«¯åº”ä½œä¸ºå•ä¾‹ä½¿ç”¨
   - å¦‚éœ€å¤šå®ä¾‹ï¼ŒåŠ¡å¿…è°ƒç”¨ `client.close()` é‡Šæ”¾èµ„æº

3. **é”™è¯¯å¤„ç†**
   - æ‰€æœ‰é”™è¯¯åŒ…å« `request_id` å’Œ `origin_request_id` ç”¨äºå…¨é“¾è·¯é—®é¢˜è¿½è¸ª
   - ç½‘ç»œé”™è¯¯ä¼šè‡ªåŠ¨é‡è¯•ï¼ˆæŒ‡æ•°é€€é¿ï¼‰
   - æä¾›å•†é”™è¯¯ä¿ç•™åŸå§‹é”™è¯¯ä¿¡æ¯
   - æ”¯æŒå¼‚å¸¸å †æ ˆè¿½è¸ªï¼Œä¾¿äºé—®é¢˜æ’æŸ¥
   - ç»“æ„åŒ– JSON æ—¥å¿—æ ¼å¼ï¼Œä¾¿äºç›‘æ§ç³»ç»Ÿé›†æˆ

## âš™ï¸ ç¯å¢ƒå˜é‡é…ç½®ï¼ˆæ¨èï¼‰

å¯ä»¥é€šè¿‡ .env æ–‡ä»¶æˆ–ç³»ç»Ÿç¯å¢ƒå˜é‡ï¼Œè‡ªåŠ¨é…ç½®è¿æ¥ä¿¡æ¯

```bash
export MODEL_MANAGER_SERVER_ADDRESS="localhost:50051"
export MODEL_MANAGER_SERVER_JWT_TOKEN="your-jwt-secret"
export MODEL_MANAGER_SERVER_GRPC_USE_TLS="false"
export MODEL_MANAGER_SERVER_GRPC_DEFAULT_AUTHORITY="localhost"
export MODEL_MANAGER_SERVER_GRPC_MAX_RETRIES="5"
export MODEL_MANAGER_SERVER_GRPC_RETRY_DELAY="1.5"

# å¿«é€Ÿé™çº§é…ç½®ï¼ˆå¯é€‰ï¼Œä¼˜åŒ–ç”¨æˆ·ä½“éªŒï¼‰
export MODEL_CLIENT_FAST_FALLBACK_ENABLED="true"
export MODEL_CLIENT_HTTP_FALLBACK_URL="http://localhost:8080"
export MODEL_CLIENT_FALLBACK_AFTER_RETRIES="1"
```

æˆ–è€…æœ¬åœ° `.env` æ–‡ä»¶

```
# ========================
# ğŸ”Œ gRPC é€šä¿¡é…ç½®
# ========================

# gRPC æœåŠ¡ç«¯åœ°å€ï¼ˆå¿…å¡«ï¼‰
MODEL_MANAGER_SERVER_ADDRESS=localhost:50051

# æ˜¯å¦å¯ç”¨ TLS åŠ å¯†é€šé“ï¼ˆtrue/falseï¼Œé»˜è®¤ trueï¼‰
MODEL_MANAGER_SERVER_GRPC_USE_TLS=true

# æ˜¯å¦è·³è¿‡ TLS è¯ä¹¦éªŒè¯ï¼ˆtrue/falseï¼Œé»˜è®¤ falseï¼‰
# ç”¨äºè‡ªç­¾åè¯ä¹¦æˆ–å¼€å‘ç¯å¢ƒï¼Œä»ä½¿ç”¨åŠ å¯†ä½†ä¸éªŒè¯è¯ä¹¦
MODEL_MANAGER_SERVER_GRPC_SKIP_VERIFY=false

# å½“ä½¿ç”¨ TLS æ—¶æŒ‡å®š authorityï¼ˆåŸŸåå¿…é¡»å’Œè¯ä¹¦åŒ¹é…æ‰éœ€è¦ï¼‰
MODEL_MANAGER_SERVER_GRPC_DEFAULT_AUTHORITY=localhost


# ========================
# ğŸ” é‰´æƒé…ç½®ï¼ˆJWTï¼‰
# ========================

# JWT ç­¾åå¯†é’¥ï¼ˆç”¨äºç”Ÿæˆ Tokenï¼‰
MODEL_MANAGER_SERVER_JWT_SECRET_KEY=your_jwt_secret_key


# ========================
# ğŸ” é‡è¯•é…ç½®ï¼ˆå¯é€‰ï¼‰
# ========================

# æœ€å¤§é‡è¯•æ¬¡æ•°ï¼ˆé»˜è®¤ 3ï¼‰
MODEL_MANAGER_SERVER_GRPC_MAX_RETRIES=3

# åˆå§‹é‡è¯•å»¶è¿Ÿï¼ˆç§’ï¼Œé»˜è®¤ 1.0ï¼‰ï¼ŒæŒ‡æ•°é€€é¿
MODEL_MANAGER_SERVER_GRPC_RETRY_DELAY=1.0


# ========================
# ğŸ›¡ï¸ ç†”æ–­é™çº§é…ç½®ï¼ˆå¯é€‰ï¼‰
# ========================

# æ˜¯å¦å¯ç”¨ç†”æ–­é™çº§åŠŸèƒ½ï¼ˆé»˜è®¤ falseï¼‰
MODEL_CLIENT_RESILIENT_ENABLED=false

# HTTP é™çº§æœåŠ¡åœ°å€ï¼ˆå½“ gRPC ä¸å¯ç”¨æ—¶çš„å¤‡ç”¨åœ°å€ï¼‰
MODEL_CLIENT_HTTP_FALLBACK_URL=http://localhost:8080

# ç†”æ–­å™¨è§¦å‘é˜ˆå€¼ï¼ˆè¿ç»­å¤±è´¥å¤šå°‘æ¬¡åç†”æ–­ï¼Œé»˜è®¤ 5ï¼‰
MODEL_CLIENT_CIRCUIT_BREAKER_THRESHOLD=5

# ç†”æ–­å™¨æ¢å¤è¶…æ—¶ï¼ˆç§’ï¼Œç†”æ–­åå¤šä¹…å°è¯•æ¢å¤ï¼Œé»˜è®¤ 60ï¼‰
MODEL_CLIENT_CIRCUIT_BREAKER_TIMEOUT=60


# ========================
# ğŸš€ å¿«é€Ÿé™çº§é…ç½®ï¼ˆå¯é€‰ï¼Œä¼˜åŒ–ä½“éªŒï¼‰
# ========================

# æ˜¯å¦å¯ç”¨å¿«é€Ÿé™çº§åŠŸèƒ½ï¼ˆé»˜è®¤ falseï¼Œå»ºè®®å¼€å¯ï¼‰
# å¯ç”¨åï¼ŒgRPC è¯·æ±‚å¤±è´¥æ—¶ä¼šç«‹å³å°è¯• HTTP é™çº§ï¼Œè€Œä¸æ˜¯ç­‰å¾…æ‰€æœ‰é‡è¯•å®Œæˆ
MODEL_CLIENT_FAST_FALLBACK_ENABLED=true

# é™çº§å‰çš„æœ€å¤§ gRPC é‡è¯•æ¬¡æ•°ï¼ˆé»˜è®¤ 1ï¼‰
# å¯¹äºéç«‹å³é™çº§çš„é”™è¯¯ï¼Œé‡è¯•æŒ‡å®šæ¬¡æ•°åæ‰å°è¯•é™çº§
MODEL_CLIENT_FALLBACK_AFTER_RETRIES=1

# ç«‹å³é™çº§çš„é”™è¯¯ç±»å‹ï¼ˆé€—å·åˆ†éš”ï¼Œé»˜è®¤ç½‘ç»œç›¸å…³é”™è¯¯ï¼‰
# è¿™äº›é”™è¯¯ç±»å‹ä¼šåœ¨ç¬¬ä¸€æ¬¡å¤±è´¥åç«‹å³å°è¯•é™çº§
MODEL_CLIENT_IMMEDIATE_FALLBACK_ERRORS=UNAVAILABLE,DEADLINE_EXCEEDED,CANCELLED

# æ°¸ä¸é™çº§çš„é”™è¯¯ç±»å‹ï¼ˆé€—å·åˆ†éš”ï¼Œé»˜è®¤è®¤è¯ç›¸å…³é”™è¯¯ï¼‰
# è¿™äº›é”™è¯¯ç±»å‹ä¸ä¼šè§¦å‘é™çº§ï¼Œé€šå¸¸æ˜¯å®¢æˆ·ç«¯é—®é¢˜è€ŒéæœåŠ¡ä¸å¯ç”¨
MODEL_CLIENT_NEVER_FALLBACK_ERRORS=UNAUTHENTICATED,PERMISSION_DENIED,INVALID_ARGUMENT


# ========================
# ğŸ” æ—¥å¿—ä¸ç›‘æ§é…ç½®ï¼ˆå¯é€‰ï¼‰
# ========================

# å¯ç”¨ç»“æ„åŒ– JSON æ—¥å¿—æ ¼å¼ï¼ˆé»˜è®¤ falseï¼Œå»ºè®®å¼€å¯ï¼‰
# å¯ç”¨åæ—¥å¿—å°†ä»¥ JSON æ ¼å¼è¾“å‡ºï¼Œä¾¿äºç›‘æ§ç³»ç»Ÿé›†æˆ
MODEL_CLIENT_ENABLE_JSON_LOGGING=true

# æ—¥å¿—çº§åˆ«è®¾ç½®ï¼ˆDEBUG, INFO, WARNING, ERRORï¼Œé»˜è®¤ INFOï¼‰
MODEL_CLIENT_LOG_LEVEL=INFO
```

åŠ è½½åï¼Œåˆå§‹åŒ–æ—¶æ— éœ€ä¼ å‚ï¼š

```python
from tamar_model_client import TamarModelClient

client = TamarModelClient()  # å°†ä½¿ç”¨ç¯å¢ƒå˜é‡ä¸­çš„é…ç½®
```

## ğŸ”§ å¼€å‘æŒ‡å—

### ç¯å¢ƒè®¾ç½®

1. **å…‹éš†ä»“åº“**
```bash
git clone https://github.com/your-org/tamar-model-client.git
cd tamar-model-client
```

2. **åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ**
```bash
python -m venv .venv
source .venv/bin/activate  # Linux/macOS
# æˆ–
.venv\Scripts\activate  # Windows
```

3. **å®‰è£…å¼€å‘ä¾èµ–**
```bash
pip install -e .
pip install -r requirements-dev.txt  # å¦‚æœæœ‰å¼€å‘ä¾èµ–
```

### ä»£ç ç”Ÿæˆ

å¦‚æœéœ€è¦æ›´æ–° gRPC å®šä¹‰ï¼š
```bash
# ç”Ÿæˆ gRPC ä»£ç 
python make_grpc.py

# éªŒè¯ç”Ÿæˆçš„ä»£ç 
python -m pytest tests/
```

### å‘å¸ƒæµç¨‹

```bash
# 1. æ›´æ–°ç‰ˆæœ¬å· (setup.py)
# 2. æ„å»ºåŒ…
python setup.py sdist bdist_wheel

# 3. æ£€æŸ¥æ„å»º
twine check dist/*

# 4. ä¸Šä¼ åˆ° PyPI
twine upload dist/*
```

### è´¡çŒ®æŒ‡å—

1. Fork é¡¹ç›®
2. åˆ›å»ºåŠŸèƒ½åˆ†æ”¯ (`git checkout -b feature/amazing-feature`)
3. æäº¤æ›´æ”¹ (`git commit -m 'Add amazing feature'`)
4. æ¨é€åˆ°åˆ†æ”¯ (`git push origin feature/amazing-feature`)
5. åˆ›å»º Pull Request

## ğŸ“Š æ€§èƒ½æŒ‡æ ‡

- **å“åº”å»¶è¿Ÿ**: å¹³å‡ < 10msï¼ˆgRPC å¼€é”€ï¼‰
- **å¹¶å‘æ”¯æŒ**: 1000+ å¹¶å‘è¯·æ±‚
- **è¿æ¥å¤ç”¨**: HTTP/2 å¤šè·¯å¤ç”¨
- **è‡ªåŠ¨é‡è¯•**: æŒ‡æ•°é€€é¿ï¼Œæœ€å¤š 5 æ¬¡
- **é™çº§æ—¶é—´**: å¿«é€Ÿé™çº§ < 2 ç§’å†…å®Œæˆ
- **ç†”æ–­æ¢å¤**: è‡ªåŠ¨æ¢å¤æ£€æµ‹ï¼Œ60 ç§’å‘¨æœŸ

## ğŸ”§ æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

#### 1. gRPC è¿æ¥å¤±è´¥
```bash
# é”™è¯¯: failed to connect to all addresses
# è§£å†³æ–¹æ¡ˆ: æ£€æŸ¥æœåŠ¡åœ°å€å’Œç½‘ç»œè¿æ¥
export MODEL_MANAGER_SERVER_ADDRESS="correct-host:port"
```

#### 2. JWT è®¤è¯å¤±è´¥
```bash
# é”™è¯¯: UNAUTHENTICATED
# è§£å†³æ–¹æ¡ˆ: æ£€æŸ¥ JWT å¯†é’¥æˆ–ä»¤ç‰Œ
export MODEL_MANAGER_SERVER_JWT_SECRET_KEY="your-secret-key"
```

#### 3. HTTP é™çº§å¤±è´¥
```bash
# é”™è¯¯: HTTP fallback URL not configured
# è§£å†³æ–¹æ¡ˆ: é…ç½® HTTP é™çº§åœ°å€
export MODEL_CLIENT_HTTP_FALLBACK_URL="http://backup-server:8080"
```

#### 4. ä¾èµ–åŒ…ç¼ºå¤±
```bash
# é”™è¯¯: aiohttp library is not installed
# è§£å†³æ–¹æ¡ˆ: å®‰è£… HTTP å®¢æˆ·ç«¯ä¾èµ–
pip install aiohttp requests
```

### è°ƒè¯•æŠ€å·§

#### å¯ç”¨è¯¦ç»†æ—¥å¿—
```python
import logging
logging.basicConfig(level=logging.DEBUG)

# æˆ–ä½¿ç”¨ç¯å¢ƒå˜é‡
# MODEL_CLIENT_LOG_LEVEL=DEBUG
```

#### æ£€æŸ¥ç†”æ–­å™¨çŠ¶æ€
```python
client = TamarModelClient()
metrics = client.get_resilient_metrics()
print(f"Circuit state: {metrics.get('circuit_state')}")
print(f"Failure count: {metrics.get('failure_count')}")
```

#### è¿½è¸ªè¯·æ±‚æµç¨‹
```python
from tamar_model_client.core import set_origin_request_id
set_origin_request_id("debug-trace-001")

# åœ¨æ—¥å¿—ä¸­æœç´¢è¿™ä¸ª ID å¯ä»¥çœ‹åˆ°å®Œæ•´è¯·æ±‚æµç¨‹
response = client.invoke(request)
```

### æ€§èƒ½ä¼˜åŒ–å»ºè®®

1. **ä½¿ç”¨å•ä¾‹å®¢æˆ·ç«¯**ï¼šé¿å…é¢‘ç¹åˆ›å»ºå®¢æˆ·ç«¯å®ä¾‹
2. **å¯ç”¨å¿«é€Ÿé™çº§**ï¼šå‡å°‘ç”¨æˆ·æ„ŸçŸ¥çš„é”™è¯¯å»¶è¿Ÿ
3. **åˆç†è®¾ç½®è¶…æ—¶**ï¼šæ ¹æ®ä¸šåŠ¡éœ€æ±‚è°ƒæ•´è¶…æ—¶æ—¶é—´
4. **ç›‘æ§ç†”æ–­çŠ¶æ€**ï¼šåŠæ—¶å‘ç°æœåŠ¡é—®é¢˜
5. **ä½¿ç”¨æ‰¹é‡ API**ï¼šæé«˜æ‰¹é‡å¤„ç†æ•ˆç‡

## ğŸ¤ æ”¯æŒä¸è´¡çŒ®

### è·å–å¸®åŠ©

- ğŸ“– [API æ–‡æ¡£](https://docs.tamar-model-client.com)
- ğŸ› [æäº¤ Issue](https://github.com/your-org/tamar-model-client/issues)
- ğŸ’¬ [è®¨è®ºåŒº](https://github.com/your-org/tamar-model-client/discussions)
- ğŸ“ [æ›´æ–°æ—¥å¿—](CHANGELOG.md)

### ç›¸å…³é¡¹ç›®

- [Model Manager Server](https://github.com/your-org/model-manager) - åç«¯ gRPC æœåŠ¡
- [Model Manager Dashboard](https://github.com/your-org/model-manager-dashboard) - ç®¡ç†æ§åˆ¶å°

## ğŸ“œ è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨ MIT è®¸å¯è¯ - è¯¦è§ [LICENSE](LICENSE) æ–‡ä»¶

## ğŸ‘¥ å›¢é˜Ÿ

- **Oscar Ou** - é¡¹ç›®è´Ÿè´£äºº - [oscar.ou@tamaredge.ai](mailto:oscar.ou@tamaredge.ai)
- [è´¡çŒ®è€…åˆ—è¡¨](https://github.com/your-org/tamar-model-client/graphs/contributors)

---

<div align="center">
  <p>
    <b>â­ å¦‚æœè¿™ä¸ªé¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ©ï¼Œè¯·ç»™ä¸ª Star æ”¯æŒæˆ‘ä»¬ï¼â­</b>
  </p>
  <p>
    Made with â¤ï¸ by Tamar Edge Team
  </p>
</div> 
