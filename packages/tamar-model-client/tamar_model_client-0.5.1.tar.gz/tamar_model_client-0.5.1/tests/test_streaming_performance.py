#!/usr/bin/env python3
"""
æµå¼è¾“å‡ºæ€§èƒ½æµ‹è¯•è„šæœ¬

æµ‹è¯•åŒæ­¥å’Œå¼‚æ­¥å®¢æˆ·ç«¯çš„æµå¼è¾“å‡ºæ€§èƒ½ï¼Œé‡ç‚¹å…³æ³¨ï¼š
1. ç¬¬ä¸€ä¸ªtokençš„å»¶è¿Ÿæ—¶é—´
2. æ€»ä½“æµå¼è¾“å‡ºå®Œæˆæ—¶é—´
3. ç®€å•é—®å¥ vs å¤æ‚é—®å¥çš„æ€§èƒ½å·®å¼‚
4. åŒæ­¥ vs å¼‚æ­¥å®¢æˆ·ç«¯çš„æ€§èƒ½å¯¹æ¯”
"""

import asyncio
import logging
import os
import sys
import time
from typing import List, Dict, Union, Iterator, AsyncIterator

# é…ç½®æµ‹è¯•è„šæœ¬ä¸“ç”¨çš„æ—¥å¿—
test_logger = logging.getLogger('test_streaming_performance')
test_logger.setLevel(logging.INFO)
test_logger.propagate = False

# åˆ›å»ºæµ‹è¯•è„šæœ¬ä¸“ç”¨çš„handler
test_handler = logging.StreamHandler()
test_handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))
test_logger.addHandler(test_handler)

logger = test_logger

# è®¾ç½®ç¯å¢ƒå˜é‡
os.environ['MODEL_MANAGER_SERVER_GRPC_USE_TLS'] = "false"
os.environ['MODEL_MANAGER_SERVER_ADDRESS'] = "localhost:50051"
os.environ['MODEL_MANAGER_SERVER_JWT_SECRET_KEY'] = "model-manager-server-jwt-key"

# OpenAI API Key (å¦‚æœéœ€è¦æµ‹è¯•OpenAI SDK)
# os.environ['OPENAI_API_KEY'] = "your-openai-api-key-here"

# å¯¼å…¥å®¢æˆ·ç«¯æ¨¡å—
try:
    from tamar_model_client import TamarModelClient, AsyncTamarModelClient
    from tamar_model_client.schemas import ModelRequest, UserContext
    from tamar_model_client.enums import ProviderType, InvokeType, Channel

    # å¯ç”¨å®¢æˆ·ç«¯æ—¥å¿—ä»¥ä¾¿è°ƒè¯•
    os.environ['TAMAR_MODEL_CLIENT_LOG_LEVEL'] = 'INFO'

except ImportError as e:
    logger.error(f"å¯¼å…¥æ¨¡å—å¤±è´¥: {e}")
    sys.exit(1)

# å¯¼å…¥OpenAI SDK
try:
    import openai
    from openai import OpenAI, AsyncOpenAI
except ImportError as e:
    logger.warning(f"OpenAI SDKå¯¼å…¥å¤±è´¥: {e}")
    openai = None


class StreamingPerformanceMetrics:
    """æµå¼è¾“å‡ºæ€§èƒ½æŒ‡æ ‡æ”¶é›†å™¨"""
    
    def __init__(self, test_name: str):
        self.test_name = test_name
        self.start_time = None
        self.first_token_time = None
        self.end_time = None
        self.total_chunks = 0
        self.total_content_length = 0
        self.chunk_times = []
        self.error_occurred = False
        self.error_message = ""
        
    def start_test(self):
        """å¼€å§‹æµ‹è¯•è®¡æ—¶"""
        self.start_time = time.perf_counter()
        
    def record_first_token(self):
        """è®°å½•ç¬¬ä¸€ä¸ªtokenåˆ°è¾¾æ—¶é—´"""
        if self.first_token_time is None:
            self.first_token_time = time.perf_counter()
            
    def record_chunk(self, content: str):
        """è®°å½•æ¯ä¸ªæ•°æ®å—"""
        current_time = time.perf_counter()
        if content:
            self.total_chunks += 1
            self.total_content_length += len(content)
            if self.start_time:
                self.chunk_times.append(current_time - self.start_time)
            
    def end_test(self):
        """ç»“æŸæµ‹è¯•è®¡æ—¶"""
        self.end_time = time.perf_counter()
        
    def record_error(self, error_msg: str):
        """è®°å½•é”™è¯¯"""
        self.error_occurred = True
        self.error_message = error_msg
        
    def get_results(self) -> Dict:
        """è·å–æµ‹è¯•ç»“æœ"""
        if not self.start_time:
            return {
                "test_name": self.test_name,
                "success": False,
                "error_message": "æµ‹è¯•æœªå¼€å§‹",
                "total_duration": 0,
                "first_token_delay": None,
                "first_token_delay_ms": None,
                "total_chunks": 0,
                "total_content_length": 0,
                "avg_chunk_interval": 0,
                "tokens_per_second": 0,
                "chunks_per_second": 0
            }
            
        total_duration = (self.end_time or time.perf_counter()) - self.start_time
        first_token_delay = (self.first_token_time - self.start_time) if self.first_token_time else None
        
        return {
            "test_name": self.test_name,
            "success": not self.error_occurred,
            "error_message": self.error_message if self.error_occurred else None,
            "total_duration": total_duration,
            "first_token_delay": first_token_delay,
            "first_token_delay_ms": first_token_delay * 1000 if first_token_delay else None,
            "total_chunks": self.total_chunks,
            "total_content_length": self.total_content_length,
            "avg_chunk_interval": sum(self.chunk_times) / len(self.chunk_times) if self.chunk_times else 0,
            "tokens_per_second": self.total_content_length / total_duration if total_duration > 0 else 0,
            "chunks_per_second": self.total_chunks / total_duration if total_duration > 0 else 0
        }
        
    def print_results(self):
        """æ‰“å°æµ‹è¯•ç»“æœ"""
        results = self.get_results()
        
        print(f"\nğŸ“Š {results['test_name']} - æ€§èƒ½æµ‹è¯•ç»“æœ:")
        print("=" * 60)
        
        if results['success']:
            print(f"âœ… æµ‹è¯•çŠ¶æ€: æˆåŠŸ")
            print(f"ğŸ• æ€»è€—æ—¶: {results['total_duration']:.3f} ç§’")
            
            if results['first_token_delay_ms']:
                print(f"âš¡ é¦–Tokenå»¶è¿Ÿ: {results['first_token_delay_ms']:.1f} ms")
            else:
                print(f"âš¡ é¦–Tokenå»¶è¿Ÿ: æœªæ£€æµ‹åˆ°")
                
            print(f"ğŸ“¦ æ•°æ®å—æ•°é‡: {results['total_chunks']}")
            print(f"ğŸ“ å†…å®¹æ€»é•¿åº¦: {results['total_content_length']} å­—ç¬¦")
            
            if results['chunks_per_second'] > 0:
                print(f"ğŸš€ æ•°æ®å—é€Ÿåº¦: {results['chunks_per_second']:.2f} å—/ç§’")
            if results['tokens_per_second'] > 0:
                print(f"âŒ¨ï¸ å­—ç¬¦é€Ÿåº¦: {results['tokens_per_second']:.2f} å­—ç¬¦/ç§’")
                
        else:
            print(f"âŒ æµ‹è¯•çŠ¶æ€: å¤±è´¥")
            print(f"ğŸ’¥ é”™è¯¯ä¿¡æ¯: {results['error_message']}")
            
        print("=" * 60)


def test_sync_simple_streaming():
    """æµ‹è¯•åŒæ­¥å®¢æˆ·ç«¯ - ç®€å•é—®å¥æµå¼è¾“å‡º"""
    from google.genai import types
    metrics = StreamingPerformanceMetrics("åŒæ­¥å®¢æˆ·ç«¯ - ç®€å•é—®å¥")
    
    try:
        client = TamarModelClient()
        generate_content_config = types.GenerateContentConfig(
            thinking_config=types.ThinkingConfig(
                thinking_budget=0,
            )
        )
        # ç®€å•é—®å¥
        request = ModelRequest(
            provider=ProviderType.GOOGLE,
            #channel=Channel.AI_STUDIO,
            #invoke_type=InvokeType.GENERATION,
            model="tamar-google-gemini-flash",
            contents=[
                {"role": "user", "parts": [{"text": "1+1ç­‰äºå‡ ï¼Ÿ"}]}
            ],
            user_context=UserContext(
                user_id="test_sync_simple",
                org_id="test_org",
                client_type="sync_performance_test"
            ),
            stream=True,
            config = generate_content_config,
        )
        
        print(f"ğŸ” å¼€å§‹æµ‹è¯•: {metrics.test_name}")
        metrics.start_test()
        
        response_gen = client.invoke(request)
        
        # æ£€æŸ¥è¿”å›ç±»å‹æ˜¯å¦ä¸ºè¿­ä»£å™¨
        if not hasattr(response_gen, '__iter__'):
            raise Exception("å“åº”ä¸æ˜¯æµå¼è¿­ä»£å™¨")
            
        for chunk in response_gen:
            if chunk.content:
                if metrics.first_token_time is None:
                    metrics.record_first_token()
                    print(f"âš¡ æ”¶åˆ°é¦–ä¸ªæ•°æ®å—: '{chunk.content[:20]}...'")
                metrics.record_chunk(chunk.content)
                
        metrics.end_test()
        
    except Exception as e:
        metrics.record_error(str(e))
        metrics.end_test()
        logger.error(f"åŒæ­¥ç®€å•é—®å¥æµ‹è¯•å¤±è´¥: {e}")
        
    finally:
        if hasattr(client, 'close'):
            client.close()
            
    metrics.print_results()
    return metrics.get_results()


def test_sync_complex_streaming():
    """æµ‹è¯•åŒæ­¥å®¢æˆ·ç«¯ - å¤æ‚é—®å¥æµå¼è¾“å‡º"""
    from google.genai import types
    metrics = StreamingPerformanceMetrics("åŒæ­¥å®¢æˆ·ç«¯ - å¤æ‚é—®å¥")
    
    try:
        client = TamarModelClient()
        generate_content_config = types.GenerateContentConfig(
            thinking_config=types.ThinkingConfig(
                thinking_budget=0,
            )
        )
        
        # å¤æ‚é—®å¥
        request = ModelRequest(
            provider=ProviderType.GOOGLE,
            #channel=Channel.AI_STUDIO,
            #invoke_type=InvokeType.GENERATION,
            model="tamar-google-gemini-flash",
            contents=[
                {"role": "user", "parts": [{"text": "è¯·è¯¦ç»†è§£é‡Šäººå·¥æ™ºèƒ½çš„å‘å±•å†å²ï¼ŒåŒ…æ‹¬å…³é”®é‡Œç¨‹ç¢‘ã€é‡è¦äººç‰©å’ŒæŠ€æœ¯çªç ´ï¼Œå¹¶åˆ†æå…¶å¯¹ç°ä»£ç¤¾ä¼šçš„å½±å“ã€‚"}]}
            ],
            user_context=UserContext(
                user_id="test_sync_complex",
                org_id="test_org", 
                client_type="sync_performance_test"
            ),
            stream=True,
            config=generate_content_config
        )
        
        print(f"ğŸ” å¼€å§‹æµ‹è¯•: {metrics.test_name}")
        metrics.start_test()
        
        response_gen = client.invoke(request)
        
        # æ£€æŸ¥è¿”å›ç±»å‹æ˜¯å¦ä¸ºè¿­ä»£å™¨
        if not hasattr(response_gen, '__iter__'):
            raise Exception("å“åº”ä¸æ˜¯æµå¼è¿­ä»£å™¨")
            
        for chunk in response_gen:
            if chunk.content:
                if metrics.first_token_time is None:
                    metrics.record_first_token()
                    print(f"âš¡ æ”¶åˆ°é¦–ä¸ªæ•°æ®å—: '{chunk.content[:20]}...'")
                metrics.record_chunk(chunk.content)
                
        metrics.end_test()
        
    except Exception as e:
        metrics.record_error(str(e))
        metrics.end_test()
        logger.error(f"åŒæ­¥å¤æ‚é—®å¥æµ‹è¯•å¤±è´¥: {e}")
        
    finally:
        if hasattr(client, 'close'):
            client.close()
            
    metrics.print_results()
    return metrics.get_results()


async def test_async_simple_streaming():
    """æµ‹è¯•å¼‚æ­¥å®¢æˆ·ç«¯ - ç®€å•é—®å¥æµå¼è¾“å‡º"""
    metrics = StreamingPerformanceMetrics("å¼‚æ­¥å®¢æˆ·ç«¯ - ç®€å•é—®å¥")
    
    try:
        async with AsyncTamarModelClient() as client:
            # ç®€å•é—®å¥
            request = ModelRequest(
                provider=ProviderType.GOOGLE,
                #channel=Channel.AI_STUDIO,
                #invoke_type=InvokeType.GENERATION,
                model="tamar-google-gemini-flash",
                contents=[
                    {"role": "user", "parts": [{"text": "2+2ç­‰äºå‡ ï¼Ÿ"}]}
                ],
                user_context=UserContext(
                    user_id="test_async_simple",
                    org_id="test_org",
                    client_type="async_performance_test"
                ),
                stream=True,
                config={
                    "temperature": 0.1,
                    "maxOutputTokens": 50
                }
            )
            
            print(f"ğŸ” å¼€å§‹æµ‹è¯•: {metrics.test_name}")
            metrics.start_test()
            
            response_gen = await client.invoke(request)
            
            # æ£€æŸ¥è¿”å›ç±»å‹æ˜¯å¦ä¸ºå¼‚æ­¥è¿­ä»£å™¨
            if not hasattr(response_gen, '__aiter__'):
                raise Exception("å“åº”ä¸æ˜¯å¼‚æ­¥æµå¼è¿­ä»£å™¨")
                
            async for chunk in response_gen:
                if chunk.content:
                    if metrics.first_token_time is None:
                        metrics.record_first_token()
                        print(f"âš¡ æ”¶åˆ°é¦–ä¸ªæ•°æ®å—: '{chunk.content[:20]}...'")
                    metrics.record_chunk(chunk.content)
                    
            metrics.end_test()
            
    except Exception as e:
        metrics.record_error(str(e))
        metrics.end_test()
        logger.error(f"å¼‚æ­¥ç®€å•é—®å¥æµ‹è¯•å¤±è´¥: {e}")
        
    metrics.print_results()
    return metrics.get_results()


async def test_async_complex_streaming():
    """æµ‹è¯•å¼‚æ­¥å®¢æˆ·ç«¯ - å¤æ‚é—®å¥æµå¼è¾“å‡º"""
    metrics = StreamingPerformanceMetrics("å¼‚æ­¥å®¢æˆ·ç«¯ - å¤æ‚é—®å¥")
    
    try:
        async with AsyncTamarModelClient() as client:
            # å¤æ‚é—®å¥
            request = ModelRequest(
                provider=ProviderType.GOOGLE,
                # channel=Channel.AI_STUDIO,
                # invoke_type=InvokeType.GENERATION,
                model="tamar-google-gemini-flash",
                contents=[
                    {"role": "user", "parts": [{"text": "è¯·è¯¦ç»†ä»‹ç»åŒºå—é“¾æŠ€æœ¯çš„å·¥ä½œåŸç†ï¼ŒåŒ…æ‹¬å»ä¸­å¿ƒåŒ–ã€å…±è¯†æœºåˆ¶ã€åŠ å¯†ç®—æ³•ç­‰æ ¸å¿ƒæ¦‚å¿µï¼Œå¹¶åˆ†æå…¶åœ¨é‡‘èã€ä¾›åº”é“¾ç®¡ç†ç­‰é¢†åŸŸçš„åº”ç”¨å‰æ™¯ã€‚"}]}
                ],
                user_context=UserContext(
                    user_id="test_async_complex",
                    org_id="test_org",
                    client_type="async_performance_test"
                ),
                stream=True,
                config={
                    "temperature": 0.3,
                    "maxOutputTokens": 500
                }
            )
            
            print(f"ğŸ” å¼€å§‹æµ‹è¯•: {metrics.test_name}")
            metrics.start_test()
            
            response_gen = await client.invoke(request)
            
            # æ£€æŸ¥è¿”å›ç±»å‹æ˜¯å¦ä¸ºå¼‚æ­¥è¿­ä»£å™¨
            if not hasattr(response_gen, '__aiter__'):
                raise Exception("å“åº”ä¸æ˜¯å¼‚æ­¥æµå¼è¿­ä»£å™¨")
                
            async for chunk in response_gen:
                if chunk.content:
                    if metrics.first_token_time is None:
                        metrics.record_first_token()
                        print(f"âš¡ æ”¶åˆ°é¦–ä¸ªæ•°æ®å—: '{chunk.content[:20]}...'")
                    metrics.record_chunk(chunk.content)
                    
            metrics.end_test()
            
    except Exception as e:
        metrics.record_error(str(e))
        metrics.end_test()
        logger.error(f"å¼‚æ­¥å¤æ‚é—®å¥æµ‹è¯•å¤±è´¥: {e}")
        
    metrics.print_results()
    return metrics.get_results()


def test_sync_azure_simple_streaming():
    """æµ‹è¯•åŒæ­¥å®¢æˆ·ç«¯ - Azure OpenAI ç®€å•é—®å¥æµå¼è¾“å‡º"""
    metrics = StreamingPerformanceMetrics("åŒæ­¥å®¢æˆ·ç«¯ - Azure ç®€å•é—®å¥")
    
    try:
        client = TamarModelClient()
        
        # Azure OpenAI ç®€å•é—®å¥
        request = ModelRequest(
            provider=ProviderType.AZURE,
            invoke_type=InvokeType.CHAT_COMPLETIONS,
            model="gpt-4o-mini",
            messages=[
                {"role": "user", "content": "3+3ç­‰äºå‡ ï¼Ÿ"}
            ],
            user_context=UserContext(
                user_id="test_sync_azure_simple",
                org_id="test_org",
                client_type="sync_azure_performance_test"
            ),
            stream=True
        )
        
        print(f"ğŸ” å¼€å§‹æµ‹è¯•: {metrics.test_name}")
        metrics.start_test()
        
        response_gen = client.invoke(request)
        
        # æ£€æŸ¥è¿”å›ç±»å‹æ˜¯å¦ä¸ºè¿­ä»£å™¨
        if not hasattr(response_gen, '__iter__'):
            raise Exception("å“åº”ä¸æ˜¯æµå¼è¿­ä»£å™¨")
            
        for chunk in response_gen:
            if chunk.content:
                if metrics.first_token_time is None:
                    metrics.record_first_token()
                    print(f"âš¡ æ”¶åˆ°é¦–ä¸ªæ•°æ®å—: '{chunk.content[:20]}...'")
                metrics.record_chunk(chunk.content)
                
        metrics.end_test()
        
    except Exception as e:
        metrics.record_error(str(e))
        metrics.end_test()
        logger.error(f"åŒæ­¥Azureç®€å•é—®å¥æµ‹è¯•å¤±è´¥: {e}")
        
    finally:
        if hasattr(client, 'close'):
            client.close()
            
    metrics.print_results()
    return metrics.get_results()


async def test_async_azure_complex_streaming():
    """æµ‹è¯•å¼‚æ­¥å®¢æˆ·ç«¯ - Azure OpenAI å¤æ‚é—®å¥æµå¼è¾“å‡º"""
    metrics = StreamingPerformanceMetrics("å¼‚æ­¥å®¢æˆ·ç«¯ - Azure å¤æ‚é—®å¥")
    
    try:
        async with AsyncTamarModelClient() as client:
            # Azure OpenAI å¤æ‚é—®å¥
            request = ModelRequest(
                provider=ProviderType.AZURE,
                invoke_type=InvokeType.CHAT_COMPLETIONS,
                model="gpt-4o-mini",
                messages=[
                    {"role": "user", "content": "è¯·è¯¦ç»†è§£é‡Šäº‘è®¡ç®—çš„æ ¸å¿ƒæ¶æ„ï¼ŒåŒ…æ‹¬IaaSã€PaaSã€SaaSä¸‰ç§æœåŠ¡æ¨¡å¼çš„ç‰¹ç‚¹å’Œåº”ç”¨åœºæ™¯ï¼Œå¹¶åˆ†æå…¶å¯¹ä¼ä¸šæ•°å­—åŒ–è½¬å‹çš„ä»·å€¼ã€‚"}
                ],
                user_context=UserContext(
                    user_id="test_async_azure_complex", 
                    org_id="test_org",
                    client_type="async_azure_performance_test"
                ),
                stream=True
            )
            
            print(f"ğŸ” å¼€å§‹æµ‹è¯•: {metrics.test_name}")
            metrics.start_test()
            
            response_gen = await client.invoke(request)
            
            # æ£€æŸ¥è¿”å›ç±»å‹æ˜¯å¦ä¸ºå¼‚æ­¥è¿­ä»£å™¨
            if not hasattr(response_gen, '__aiter__'):
                raise Exception("å“åº”ä¸æ˜¯å¼‚æ­¥æµå¼è¿­ä»£å™¨")
                
            async for chunk in response_gen:
                if chunk.content:
                    if metrics.first_token_time is None:
                        metrics.record_first_token()
                        print(f"âš¡ æ”¶åˆ°é¦–ä¸ªæ•°æ®å—: '{chunk.content[:20]}...'")
                    metrics.record_chunk(chunk.content)
                    
            metrics.end_test()
            
    except Exception as e:
        metrics.record_error(str(e))
        metrics.end_test()
        logger.error(f"å¼‚æ­¥Azureå¤æ‚é—®å¥æµ‹è¯•å¤±è´¥: {e}")
        
    metrics.print_results()
    return metrics.get_results()


def test_sync_openai_chat_completions():
    """æµ‹è¯•åŒæ­¥OpenAI SDK - Chat Completions æµå¼è¾“å‡º"""
    metrics = StreamingPerformanceMetrics("åŒæ­¥OpenAI SDK - Chat")
    
    if not openai:
        metrics.record_error("OpenAI SDK æœªå®‰è£…")
        metrics.print_results()
        return metrics.get_results()
    
    try:
        # ä½¿ç”¨OpenAIå®˜æ–¹SDK
        client = OpenAI(
            base_url=os.environ.get("OPENAI_API_URL", "http://localhost:8000/v1"),
            api_key=os.environ.get("OPENAI_API_KEY", "call-model:user_id:data_scientist_agent")
        )
        
        print(f"ğŸ” å¼€å§‹æµ‹è¯•: {metrics.test_name}")
        metrics.start_test()
        
        # OpenAI Chat Completions æµå¼è¯·æ±‚
        stream = client.chat.completions.create(
            model="tamar-google-gemini-flash",
            messages=[
                {"role": "user", "content": "è¯·ç®€è¦ä»‹ç»äººå·¥æ™ºèƒ½åœ¨åŒ»ç–—é¢†åŸŸçš„åº”ç”¨ï¼ŒåŒ…æ‹¬è¯Šæ–­ã€æ²»ç–—å’Œè¯ç‰©ç ”å‘ç­‰æ–¹é¢ã€‚"}
            ],
            stream=True,
            max_tokens=300,
            temperature=0.4
        )
        
        # å¤„ç†æµå¼å“åº”
        for chunk in stream:
            try:
                # æ·»åŠ è°ƒè¯•ä¿¡æ¯
                if chunk is None:
                    print(f"ğŸ” è°ƒè¯•ï¼šæ”¶åˆ° None chunk")
                    continue
                    
                if not hasattr(chunk, 'choices'):
                    print(f"ğŸ” è°ƒè¯•ï¼šchunk æ²¡æœ‰ choices å±æ€§ï¼Œç±»å‹: {type(chunk)}")
                    continue
                    
                if not chunk.choices:
                    print(f"ğŸ” è°ƒè¯•ï¼šchunk.choices ä¸ºç©º")
                    continue
                    
                if len(chunk.choices) == 0:
                    print(f"ğŸ” è°ƒè¯•ï¼šchunk.choices é•¿åº¦ä¸º0")
                    continue
                    
                choice = chunk.choices[0]
                if not hasattr(choice, 'delta'):
                    print(f"ğŸ” è°ƒè¯•ï¼šchoice æ²¡æœ‰ delta å±æ€§ï¼Œç±»å‹: {type(choice)}")
                    continue
                    
                if not hasattr(choice.delta, 'content'):
                    print(f"ğŸ” è°ƒè¯•ï¼šchoice.delta æ²¡æœ‰ content å±æ€§ï¼Œç±»å‹: {type(choice.delta)}")
                    continue
                    
                if choice.delta.content:
                    content = choice.delta.content
                    if metrics.first_token_time is None:
                        metrics.record_first_token()
                        print(f"âš¡ æ”¶åˆ°é¦–ä¸ªæ•°æ®å—: '{content[:20]}...'")
                    metrics.record_chunk(content)
                else:
                    print(f"ğŸ” è°ƒè¯•ï¼šchoice.delta.content ä¸ºç©ºæˆ–None")
                    
            except Exception as chunk_error:
                print(f"ğŸ” è°ƒè¯•ï¼šå¤„ç†chunkæ—¶å‡ºé”™: {chunk_error}")
                print(f"ğŸ” è°ƒè¯•ï¼šchunkç±»å‹: {type(chunk)}")
                raise chunk_error
                
        metrics.end_test()
        
    except Exception as e:
        metrics.record_error(str(e))
        metrics.end_test()
        logger.error(f"åŒæ­¥OpenAI SDKæµ‹è¯•å¤±è´¥: {e}")
        
    metrics.print_results()
    return metrics.get_results()


async def test_async_openai_chat_completions():
    """æµ‹è¯•å¼‚æ­¥OpenAI SDK - Chat Completions æµå¼è¾“å‡º"""
    metrics = StreamingPerformanceMetrics("å¼‚æ­¥OpenAI SDK - Chat")
    
    if not openai:
        metrics.record_error("OpenAI SDK æœªå®‰è£…")
        metrics.print_results()
        return metrics.get_results()
    
    try:
        # ä½¿ç”¨OpenAIå®˜æ–¹å¼‚æ­¥SDK
        client = AsyncOpenAI(
            base_url=os.environ.get("OPENAI_API_URL", "http://localhost:8000/v1"),
            api_key=os.environ.get("OPENAI_API_KEY", "call-model:user_id:data_scientist_agent")
        )
        
        print(f"ğŸ” å¼€å§‹æµ‹è¯•: {metrics.test_name}")
        metrics.start_test()
        
        # OpenAI Chat Completions å¼‚æ­¥æµå¼è¯·æ±‚
        stream = await client.chat.completions.create(
            model="tamar-google-gemini-flash",
            messages=[
                {"role": "user", "content": "è¯¦ç»†è§£é‡Šæœºå™¨å­¦ä¹ çš„ä¸»è¦ç®—æ³•ç±»å‹ï¼ŒåŒ…æ‹¬ç›‘ç£å­¦ä¹ ã€æ— ç›‘ç£å­¦ä¹ å’Œå¼ºåŒ–å­¦ä¹ çš„ç‰¹ç‚¹ã€åº”ç”¨åœºæ™¯å’Œä¼˜ç¼ºç‚¹ã€‚"}
            ],
            stream=True,
            max_tokens=400,
            temperature=0.3
        )
        
        # å¤„ç†å¼‚æ­¥æµå¼å“åº”
        async for chunk in stream:
            if chunk and chunk.choices and len(chunk.choices) > 0 and chunk.choices[0].delta.content:
                content = chunk.choices[0].delta.content
                if metrics.first_token_time is None:
                    metrics.record_first_token()
                    print(f"âš¡ æ”¶åˆ°é¦–ä¸ªæ•°æ®å—: '{content[:20]}...'")
                metrics.record_chunk(content)
                
        metrics.end_test()
        
    except Exception as e:
        metrics.record_error(str(e))
        metrics.end_test()
        logger.error(f"å¼‚æ­¥OpenAI SDKæµ‹è¯•å¤±è´¥: {e}")
        
    finally:
        # å…³é—­å¼‚æ­¥å®¢æˆ·ç«¯
        if 'client' in locals():
            await client.close()
        
    metrics.print_results()
    return metrics.get_results()


def test_sync_openai_simple_streaming():
    """æµ‹è¯•åŒæ­¥OpenAI SDK - ç®€å•é—®å¥æµå¼è¾“å‡º"""
    metrics = StreamingPerformanceMetrics("åŒæ­¥OpenAI SDK - ç®€å•é—®å¥")
    
    if not openai:
        metrics.record_error("OpenAI SDK æœªå®‰è£…")
        metrics.print_results()
        return metrics.get_results()
    
    try:
        # ä½¿ç”¨OpenAIå®˜æ–¹SDK
        client = OpenAI(
            base_url=os.environ.get("OPENAI_API_URL", "http://localhost:8000/v1"),
            api_key=os.environ.get("OPENAI_API_KEY", "call-model:user_id:data_scientist_agent")
        )
        
        print(f"ğŸ” å¼€å§‹æµ‹è¯•: {metrics.test_name}")
        metrics.start_test()
        
        # OpenAI Chat Completions ç®€å•é—®å¥æµå¼è¯·æ±‚
        stream = client.chat.completions.create(
            model="tamar-google-gemini-flash",
            messages=[
                {"role": "user", "content": "5+5ç­‰äºå¤šå°‘ï¼Ÿ"}
            ],
            stream=True,
            max_tokens=50,
            temperature=0.1
        )
        
        # å¤„ç†æµå¼å“åº”
        for chunk in stream:
            if chunk and chunk.choices and len(chunk.choices) > 0 and chunk.choices[0].delta.content:
                content = chunk.choices[0].delta.content
                if metrics.first_token_time is None:
                    metrics.record_first_token()
                    print(f"âš¡ æ”¶åˆ°é¦–ä¸ªæ•°æ®å—: '{content[:20]}...'")
                metrics.record_chunk(content)
                
        metrics.end_test()
        
    except Exception as e:
        metrics.record_error(str(e))
        metrics.end_test()
        logger.error(f"åŒæ­¥OpenAI SDKç®€å•é—®å¥æµ‹è¯•å¤±è´¥: {e}")
        
    metrics.print_results()
    return metrics.get_results()


async def test_async_openai_simple_streaming():
    """æµ‹è¯•å¼‚æ­¥OpenAI SDK - ç®€å•é—®å¥æµå¼è¾“å‡º"""
    metrics = StreamingPerformanceMetrics("å¼‚æ­¥OpenAI SDK - ç®€å•é—®å¥")
    
    if not openai:
        metrics.record_error("OpenAI SDK æœªå®‰è£…")
        metrics.print_results()
        return metrics.get_results()
    
    try:
        # ä½¿ç”¨OpenAIå®˜æ–¹å¼‚æ­¥SDK
        client = AsyncOpenAI(
            base_url=os.environ.get("OPENAI_API_URL", "http://localhost:8000/v1"),
            api_key=os.environ.get("OPENAI_API_KEY", "call-model:user_id:data_scientist_agent")
        )
        
        print(f"ğŸ” å¼€å§‹æµ‹è¯•: {metrics.test_name}")
        metrics.start_test()
        
        # OpenAI Chat Completions ç®€å•é—®å¥å¼‚æ­¥æµå¼è¯·æ±‚
        stream = await client.chat.completions.create(
            model="tamar-google-gemini-flash",
            messages=[
                {"role": "user", "content": "10-3ç­‰äºå¤šå°‘ï¼Ÿ"}
            ],
            stream=True,
            max_tokens=50,
            temperature=0.1
        )
        
        # å¤„ç†å¼‚æ­¥æµå¼å“åº”
        async for chunk in stream:
            if chunk and chunk.choices and len(chunk.choices) > 0 and chunk.choices[0].delta.content:
                content = chunk.choices[0].delta.content
                if metrics.first_token_time is None:
                    metrics.record_first_token()
                    print(f"âš¡ æ”¶åˆ°é¦–ä¸ªæ•°æ®å—: '{content[:20]}...'")
                metrics.record_chunk(content)
                
        metrics.end_test()
        
    except Exception as e:
        metrics.record_error(str(e))
        metrics.end_test()
        logger.error(f"å¼‚æ­¥OpenAI SDKç®€å•é—®å¥æµ‹è¯•å¤±è´¥: {e}")
        
    finally:
        # å…³é—­å¼‚æ­¥å®¢æˆ·ç«¯
        if 'client' in locals():
            await client.close()
        
    metrics.print_results()
    return metrics.get_results()


class ImageGenerationMetrics:
    """å›¾åƒç”Ÿæˆæ€§èƒ½æŒ‡æ ‡æ”¶é›†å™¨"""
    
    def __init__(self, test_name: str):
        self.test_name = test_name
        self.start_time = None
        self.end_time = None
        self.image_generated = False
        self.image_size = 0
        self.error_occurred = False
        self.error_message = ""
        
    def start_test(self):
        """å¼€å§‹æµ‹è¯•è®¡æ—¶"""
        self.start_time = time.perf_counter()
        
    def record_image(self, image_data: str):
        """è®°å½•ç”Ÿæˆçš„å›¾åƒ"""
        if image_data:
            self.image_generated = True
            self.image_size = len(str(image_data))
            
    def end_test(self):
        """ç»“æŸæµ‹è¯•è®¡æ—¶"""
        self.end_time = time.perf_counter()
        
    def record_error(self, error_msg: str):
        """è®°å½•é”™è¯¯"""
        self.error_occurred = True
        self.error_message = error_msg
        
    def get_results(self) -> Dict:
        """è·å–æµ‹è¯•ç»“æœ"""
        if not self.start_time:
            return {
                "test_name": self.test_name,
                "success": False,
                "error_message": "æµ‹è¯•æœªå¼€å§‹",
                "total_duration": 0,
                "image_generated": False,
                "image_size": 0,
                "generation_speed": 0
            }
            
        total_duration = (self.end_time or time.perf_counter()) - self.start_time
        
        return {
            "test_name": self.test_name,
            "success": not self.error_occurred and self.image_generated,
            "error_message": self.error_message if self.error_occurred else None,
            "total_duration": total_duration,
            "image_generated": self.image_generated,
            "image_size": self.image_size,
            "generation_speed": self.image_size / total_duration if total_duration > 0 and self.image_size > 0 else 0
        }
        
    def print_results(self):
        """æ‰“å°æµ‹è¯•ç»“æœ"""
        results = self.get_results()
        
        print(f"\nğŸ–¼ï¸ {results['test_name']} - å›¾åƒç”Ÿæˆæµ‹è¯•ç»“æœ:")
        print("=" * 60)
        
        if results['success']:
            print(f"âœ… æµ‹è¯•çŠ¶æ€: æˆåŠŸ")
            print(f"ğŸ• æ€»è€—æ—¶: {results['total_duration']:.3f} ç§’")
            print(f"ğŸ–¼ï¸ å›¾åƒç”Ÿæˆ: {'æ˜¯' if results['image_generated'] else 'å¦'}")
            if results['image_size'] > 0:
                print(f"ğŸ“¦ å›¾åƒæ•°æ®å¤§å°: {results['image_size']} å­—ç¬¦")
                print(f"ğŸš€ ç”Ÿæˆé€Ÿåº¦: {results['generation_speed']:.2f} å­—ç¬¦/ç§’")
        else:
            print(f"âŒ æµ‹è¯•çŠ¶æ€: å¤±è´¥")
            print(f"ğŸ’¥ é”™è¯¯ä¿¡æ¯: {results['error_message']}")
            
        print("=" * 60)



async def test_async_google_vertex_image_generation():
    """æµ‹è¯•å¼‚æ­¥å®¢æˆ·ç«¯ - Google Vertex AI å›¾åƒç”Ÿæˆ"""
    metrics = ImageGenerationMetrics("å¼‚æ­¥å®¢æˆ·ç«¯ - Google Vertex å›¾åƒç”Ÿæˆ")
    
    try:
        async with AsyncTamarModelClient() as client:
            # Google Vertex AI å›¾åƒç”Ÿæˆè¯·æ±‚
            request = ModelRequest(
                provider=ProviderType.GOOGLE,
                channel=Channel.VERTEXAI,
                invoke_type=InvokeType.IMAGE_GENERATION,
                model="imagen-3.0-fast-generate-001",
                prompt="å£®ä¸½çš„é›ªå±±å³°é¡¶åœ¨å¤•é˜³ä½™æ™–ä¸‹å‘ˆç°é‡‘è‰²å…‰èŠ’ï¼Œå±±ä¸‹æ˜¯å®é™çš„æ¹–æ³Šå€’æ˜ ç€å±±æ™¯",
                user_context=UserContext(
                    user_id="test_async_vertex_image",
                    org_id="test_org",
                    client_type="async_vertex_image_performance_test"
                )
            )
            
            print(f"ğŸ” å¼€å§‹æµ‹è¯•: {metrics.test_name}")
            metrics.start_test()
            
            response = await client.invoke(request)
            
            # æ£€æŸ¥æ˜¯å¦ç”Ÿæˆäº†å›¾åƒ
            if response.error:
                raise Exception(f"å›¾åƒç”Ÿæˆå¤±è´¥: {response.error}")
            elif response.content:
                metrics.record_image(response.content)
                print(f"ğŸ–¼ï¸ å›¾åƒç”ŸæˆæˆåŠŸï¼Œæ•°æ®å¤§å°: {len(str(response.content))} å­—ç¬¦")
            elif hasattr(response, 'raw_response') and response.raw_response:
                # æ£€æŸ¥åŸå§‹å“åº”ä¸­æ˜¯å¦åŒ…å«å›¾åƒæ•°æ®
                image_data = None
                if isinstance(response.raw_response, list):
                    # æ£€æŸ¥ image_bytes (æ–°æ ¼å¼) æˆ– _image_bytes (æ—§æ ¼å¼)
                    for r in response.raw_response:
                        if isinstance(r, dict):
                            image_data = r.get("image_bytes") or r.get("_image_bytes")
                            if image_data:
                                print(f"ğŸ–¼ï¸ Vertex AI å›¾åƒç”ŸæˆæˆåŠŸï¼ŒBase64æ•°æ®å¤§å°: {len(str(image_data))} å­—ç¬¦")
                                break
                elif isinstance(response.raw_response, dict):
                    # å…¶ä»–æ ¼å¼ï¼šç›´æ¥ä½¿ç”¨æ•´ä¸ªå“åº”
                    image_data = str(response.raw_response)
                else:
                    image_data = str(response.raw_response)
                
                if image_data:
                    metrics.record_image(image_data)
                    print(f"ğŸ–¼ï¸ å›¾åƒç”ŸæˆæˆåŠŸï¼ˆæ¥è‡ªraw_responseï¼‰ï¼Œæ•°æ®å¤§å°: {len(str(image_data))} å­—ç¬¦")
                else:
                    raise Exception("raw_response ä¸­æœªæ‰¾åˆ°å›¾åƒæ•°æ®")
            else:
                # è¾“å‡ºå“åº”çš„è¯¦ç»†ä¿¡æ¯ç”¨äºè°ƒè¯•
                print(f"ğŸ” Vertex AI å“åº”è°ƒè¯•ä¿¡æ¯:")
                print(f"   content: {response.content}")
                print(f"   error: {response.error}")
                print(f"   raw_response ç±»å‹: {type(response.raw_response)}")
                print(f"   raw_response å†…å®¹: {getattr(response, 'raw_response', 'None')}")
                print(f"   usage: {getattr(response, 'usage', 'None')}")
                raise Exception("æœªæ”¶åˆ°ä»»ä½•å›¾åƒæ•°æ®")
                    
            metrics.end_test()
            
    except Exception as e:
        metrics.record_error(str(e))
        metrics.end_test()
        logger.error(f"å¼‚æ­¥Google Vertexå›¾åƒç”Ÿæˆæµ‹è¯•å¤±è´¥: {e}")
        
    metrics.print_results()
    return metrics.get_results()


def compare_results(results: List[Dict]):
    """å¯¹æ¯”ä¸åŒæµ‹è¯•çš„ç»“æœ"""
    print("\nğŸ”€ æ€§èƒ½å¯¹æ¯”åˆ†æ")
    print("=" * 80)
    
    # è¿‡æ»¤æ‰æ— æ•ˆçš„ç»“æœ
    valid_results = []
    for r in results:
        if isinstance(r, dict) and 'test_name' in r:
            valid_results.append(r)
        else:
            print(f"âš ï¸ å‘ç°æ— æ•ˆæµ‹è¯•ç»“æœ: {r}")
    
    if not valid_results:
        print("ğŸ˜ æ²¡æœ‰æœ‰æ•ˆçš„æµ‹è¯•ç»“æœå¯ä¾›å¯¹æ¯”")
        return
        
    # æŒ‰æµ‹è¯•ç±»å‹åˆ†ç±»
    streaming_tests = [r for r in valid_results if 'first_token_delay_ms' in r]
    image_tests = [r for r in valid_results if 'image_generated' in r]
    
    # æŒ‰æˆåŠŸ/å¤±è´¥åˆ†ç±»
    successful_streaming = [r for r in streaming_tests if r['success']]
    failed_streaming = [r for r in streaming_tests if not r['success']]
    successful_image = [r for r in image_tests if r['success']]
    failed_image = [r for r in image_tests if not r['success']]
    
    all_failed = failed_streaming + failed_image
    
    if all_failed:
        print(f"âŒ å¤±è´¥çš„æµ‹è¯• ({len(all_failed)}):")
        for test in all_failed:
            print(f"   - {test['test_name']}: {test['error_message']}")
        print()
    
    # æµå¼è¾“å‡ºæµ‹è¯•å¯¹æ¯”
    if successful_streaming:
        print(f"ğŸ“¡ æµå¼è¾“å‡ºæµ‹è¯•ç»“æœ ({len(successful_streaming)}):")
        print("=" * 70)
        
        # é¦–Tokenå»¶è¿Ÿå¯¹æ¯”
        print("âš¡ é¦–Tokenå»¶è¿Ÿå¯¹æ¯”:")
        print(f"{'æµ‹è¯•åç§°':<30} {'å»¶è¿Ÿ(ms)':<12} {'æ€»è€—æ—¶(s)':<12} {'æ•°æ®å—æ•°':<10}")
        print("-" * 70)
        
        for test in successful_streaming:
            delay_ms = f"{test['first_token_delay_ms']:.1f}" if test['first_token_delay_ms'] else "N/A"
            total_time = f"{test['total_duration']:.3f}"
            chunks = str(test['total_chunks'])
            
            print(f"{test['test_name']:<30} {delay_ms:<12} {total_time:<12} {chunks:<10}")
        
        print()
        
        # æ€§èƒ½æŒ‡æ ‡å¯¹æ¯”
        print("ğŸš€ æµå¼è¾“å‡ºæ€§èƒ½æŒ‡æ ‡:")
        print(f"{'æµ‹è¯•åç§°':<30} {'å­—ç¬¦/ç§’':<12} {'å—/ç§’':<12} {'å†…å®¹é•¿åº¦':<10}")
        print("-" * 70)
        
        for test in successful_streaming:
            chars_per_sec = f"{test['tokens_per_second']:.1f}"
            chunks_per_sec = f"{test['chunks_per_second']:.1f}"
            content_len = str(test['total_content_length'])
            
            print(f"{test['test_name']:<30} {chars_per_sec:<12} {chunks_per_sec:<12} {content_len:<10}")
        
        print()
    
    # å›¾åƒç”Ÿæˆæµ‹è¯•å¯¹æ¯”
    if successful_image:
        print(f"ğŸ–¼ï¸ å›¾åƒç”Ÿæˆæµ‹è¯•ç»“æœ ({len(successful_image)}):")
        print("=" * 70)
        print(f"{'æµ‹è¯•åç§°':<30} {'æ€»è€—æ—¶(s)':<12} {'å›¾åƒå¤§å°':<12} {'ç”Ÿæˆé€Ÿåº¦':<15}")
        print("-" * 70)
        
        for test in successful_image:
            total_time = f"{test['total_duration']:.3f}"
            image_size = f"{test['image_size']}" if test['image_size'] > 0 else "N/A"
            gen_speed = f"{test['generation_speed']:.1f} å­—ç¬¦/ç§’" if test['generation_speed'] > 0 else "N/A"
            
            print(f"{test['test_name']:<30} {total_time:<12} {image_size:<12} {gen_speed:<15}")
        
        print()
    
    # ç»¼åˆåˆ†æç»“è®º
    if successful_streaming or successful_image:
        print("ğŸ“ˆ åˆ†æç»“è®º:")
        
        # æµå¼è¾“å‡ºåˆ†æ
        if successful_streaming:
            # æ‰¾å‡ºæœ€å¿«çš„é¦–Token
            valid_first_token_tests = [t for t in successful_streaming if t['first_token_delay_ms']]
            if valid_first_token_tests:
                fastest_first_token = min(valid_first_token_tests, key=lambda x: x['first_token_delay_ms'])
                print(f"   ğŸ† é¦–Tokenæœ€å¿«: {fastest_first_token['test_name']} ({fastest_first_token['first_token_delay_ms']:.1f}ms)")
            
            # æ‰¾å‡ºæœ€å¿«çš„æ€»ä½“å®Œæˆæ—¶é—´
            fastest_total = min(successful_streaming, key=lambda x: x['total_duration'])
            print(f"   ğŸ† æµå¼è¾“å‡ºæœ€å¿«: {fastest_total['test_name']} ({fastest_total['total_duration']:.3f}s)")
            
            # åŒæ­¥vså¼‚æ­¥å¯¹æ¯”
            sync_streaming = [t for t in successful_streaming if 'åŒæ­¥' in t['test_name']]
            async_streaming = [t for t in successful_streaming if 'å¼‚æ­¥' in t['test_name']]
            
            if sync_streaming and async_streaming:
                sync_delays = [t['first_token_delay_ms'] for t in sync_streaming if t['first_token_delay_ms']]
                async_delays = [t['first_token_delay_ms'] for t in async_streaming if t['first_token_delay_ms']]
                
                if sync_delays and async_delays:
                    avg_sync_delay = sum(sync_delays) / len(sync_delays)
                    avg_async_delay = sum(async_delays) / len(async_delays)
                    
                    print(f"   ğŸ“Š åŒæ­¥å®¢æˆ·ç«¯å¹³å‡é¦–Tokenå»¶è¿Ÿ: {avg_sync_delay:.1f}ms")
                    print(f"   ğŸ“Š å¼‚æ­¥å®¢æˆ·ç«¯å¹³å‡é¦–Tokenå»¶è¿Ÿ: {avg_async_delay:.1f}ms")
                    
                    if avg_async_delay < avg_sync_delay:
                        print(f"   âœ¨ å¼‚æ­¥å®¢æˆ·ç«¯é¦–Tokenå»¶è¿Ÿå¹³å‡å¿« {avg_sync_delay - avg_async_delay:.1f}ms")
                    else:
                        print(f"   âœ¨ åŒæ­¥å®¢æˆ·ç«¯é¦–Tokenå»¶è¿Ÿå¹³å‡å¿« {avg_async_delay - avg_sync_delay:.1f}ms")
            
            # ç®€å•vså¤æ‚é—®å¥å¯¹æ¯”
            simple_streaming = [t for t in successful_streaming if 'ç®€å•' in t['test_name']]
            complex_streaming = [t for t in successful_streaming if 'å¤æ‚' in t['test_name']]
            
            if simple_streaming and complex_streaming:
                simple_delays = [t['first_token_delay_ms'] for t in simple_streaming if t['first_token_delay_ms']]
                complex_delays = [t['first_token_delay_ms'] for t in complex_streaming if t['first_token_delay_ms']]
                
                if simple_delays and complex_delays:
                    avg_simple_delay = sum(simple_delays) / len(simple_delays)
                    avg_complex_delay = sum(complex_delays) / len(complex_delays)
                    
                    print(f"   ğŸ“Š ç®€å•é—®å¥å¹³å‡é¦–Tokenå»¶è¿Ÿ: {avg_simple_delay:.1f}ms")
                    print(f"   ğŸ“Š å¤æ‚é—®å¥å¹³å‡é¦–Tokenå»¶è¿Ÿ: {avg_complex_delay:.1f}ms")
                    
                    if avg_complex_delay > avg_simple_delay:
                        print(f"   ğŸ’¡ å¤æ‚é—®å¥é¦–Tokenå»¶è¿Ÿå¹³å‡å¤š {avg_complex_delay - avg_simple_delay:.1f}ms")
                    else:
                        print(f"   ğŸ’¡ ç®€å•é—®å¥é¦–Tokenå»¶è¿Ÿå¹³å‡å¤š {avg_simple_delay - avg_complex_delay:.1f}ms")
        
        # å›¾åƒç”Ÿæˆåˆ†æ
        if successful_image:
            fastest_image = min(successful_image, key=lambda x: x['total_duration'])
            print(f"   ğŸ† å›¾åƒç”Ÿæˆæœ€å¿«: {fastest_image['test_name']} ({fastest_image['total_duration']:.3f}s)")
            
            # åŒæ­¥vså¼‚æ­¥å›¾åƒç”Ÿæˆå¯¹æ¯”
            sync_image = [t for t in successful_image if 'åŒæ­¥' in t['test_name']]
            async_image = [t for t in successful_image if 'å¼‚æ­¥' in t['test_name']]
            
            if sync_image and async_image:
                avg_sync_image_time = sum(t['total_duration'] for t in sync_image) / len(sync_image)
                avg_async_image_time = sum(t['total_duration'] for t in async_image) / len(async_image)
                
                print(f"   ğŸ“Š åŒæ­¥å®¢æˆ·ç«¯å¹³å‡å›¾åƒç”Ÿæˆæ—¶é—´: {avg_sync_image_time:.3f}s")
                print(f"   ğŸ“Š å¼‚æ­¥å®¢æˆ·ç«¯å¹³å‡å›¾åƒç”Ÿæˆæ—¶é—´: {avg_async_image_time:.3f}s")
                
                if avg_async_image_time < avg_sync_image_time:
                    print(f"   âœ¨ å¼‚æ­¥å®¢æˆ·ç«¯å›¾åƒç”Ÿæˆå¹³å‡å¿« {avg_sync_image_time - avg_async_image_time:.3f}s")
                else:
                    print(f"   âœ¨ åŒæ­¥å®¢æˆ·ç«¯å›¾åƒç”Ÿæˆå¹³å‡å¿« {avg_async_image_time - avg_sync_image_time:.3f}s")


async def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ æµå¼è¾“å‡ºæ€§èƒ½æµ‹è¯•")
    print("=" * 50)
    print("ç›®æ ‡ï¼šæµ‹è¯•ç¬¬ä¸€ä¸ªtokenå»¶è¿Ÿå’Œæ€»ä½“å®Œæˆæ—¶é—´")
    print("åœºæ™¯ï¼šç®€å•é—®å¥ vs å¤æ‚é—®å¥ï¼ŒåŒæ­¥ vs å¼‚æ­¥å®¢æˆ·ç«¯")
    print("=" * 50)
    
    all_results = []
    
    try:
        # 1. åŒæ­¥å®¢æˆ·ç«¯æµ‹è¯•
        print("\nğŸ”„ åŒæ­¥å®¢æˆ·ç«¯æµ‹è¯•")
        print("-" * 30)
        
        # åŒæ­¥ç®€å•é—®å¥
        result1 = test_sync_simple_streaming()
        all_results.append(result1)
        
        # ç­‰å¾…ä¸€ä¸‹å†è¿›è¡Œä¸‹ä¸€ä¸ªæµ‹è¯•
        await asyncio.sleep(1)

        # åŒæ­¥å¤æ‚é—®å¥
        result2 = test_sync_complex_streaming()
        all_results.append(result2)

        await asyncio.sleep(1)

        # åŒæ­¥Azureç®€å•é—®å¥
        result3 = test_sync_azure_simple_streaming()
        all_results.append(result3)

        await asyncio.sleep(1)

        # 2. å¼‚æ­¥å®¢æˆ·ç«¯æµ‹è¯•
        print("\nğŸ”„ å¼‚æ­¥å®¢æˆ·ç«¯æµ‹è¯•")
        print("-" * 30)

        # å¼‚æ­¥ç®€å•é—®å¥
        result4 = await test_async_simple_streaming()
        all_results.append(result4)

        await asyncio.sleep(1)

        # å¼‚æ­¥å¤æ‚é—®å¥
        result5 = await test_async_complex_streaming()
        all_results.append(result5)

        await asyncio.sleep(1)

        # å¼‚æ­¥Azureå¤æ‚é—®å¥
        result6 = await test_async_azure_complex_streaming()
        all_results.append(result6)
        
        # 3. OpenAI Chat Completions æµ‹è¯•
        print("\nğŸ”„ OpenAI Chat Completions æµ‹è¯•")
        print("-" * 30)

        # åŒæ­¥OpenAIç®€å•é—®å¥
        result7 = test_sync_openai_simple_streaming()
        all_results.append(result7)

        await asyncio.sleep(1)

        # å¼‚æ­¥OpenAIç®€å•é—®å¥
        result8 = await test_async_openai_simple_streaming()
        all_results.append(result8)

        await asyncio.sleep(1)

        # åŒæ­¥OpenAIå¤æ‚é—®å¥
        result9 = test_sync_openai_chat_completions()
        all_results.append(result9)

        await asyncio.sleep(1)

        # å¼‚æ­¥OpenAIå¤æ‚é—®å¥
        result10 = await test_async_openai_chat_completions()
        all_results.append(result10)

        await asyncio.sleep(1)
        
        # 4. Google å›¾åƒç”Ÿæˆæµ‹è¯•
        print("\nğŸ”„ Google å›¾åƒç”Ÿæˆæµ‹è¯•")
        print("-" * 30)
        
        # å¼‚æ­¥Google Vertexå›¾åƒç”Ÿæˆ
        result13 = await test_async_google_vertex_image_generation()
        all_results.append(result13)
        
        # 5. ç»“æœå¯¹æ¯”åˆ†æ
        compare_results(all_results)
        
        print("\nâœ… æ‰€æœ‰æµ‹è¯•å®Œæˆ")
        
    except Exception as e:
        logger.error(f"æµ‹è¯•æ‰§è¡Œå‡ºé”™: {e}")
        print(f"\nâŒ æµ‹è¯•æ‰§è¡Œå‡ºé”™: {e}")


if __name__ == "__main__":
    try:
        # è®¾ç½®asyncioæ—¥å¿—çº§åˆ«ï¼Œå‡å°‘å™ªéŸ³
        asyncio_logger = logging.getLogger('asyncio')
        original_level = asyncio_logger.level
        asyncio_logger.setLevel(logging.WARNING)
        
        try:
            asyncio.run(main())
        finally:
            asyncio_logger.setLevel(original_level)
            
    except KeyboardInterrupt:
        print("\nâš ï¸ æµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ ç¨‹åºæ‰§è¡Œå‡ºé”™: {e}")
    finally:
        print("ğŸ ç¨‹åºå·²é€€å‡º")