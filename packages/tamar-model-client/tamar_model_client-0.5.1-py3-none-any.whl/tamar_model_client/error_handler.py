"""
gRPC é”™è¯¯å¤„ç†å™¨

æä¾›ç»Ÿä¸€çš„é”™è¯¯å¤„ç†ã€æ¢å¤ç­–ç•¥å’Œé‡è¯•é€»è¾‘ã€‚
"""

import asyncio
import random
import grpc
import logging
from typing import Optional, Dict, Any, Callable, Union
from collections import defaultdict

from .core import get_protected_logger
from .exceptions import (
    ErrorContext, TamarModelException,
    NetworkException, ConnectionException, TimeoutException,
    AuthenticationException, TokenExpiredException, PermissionDeniedException,
    ValidationException, InvalidParameterException,
    RateLimitException, ProviderException,
    ERROR_CATEGORIES, RETRY_POLICY, ErrorStats
)

logger = get_protected_logger(__name__)


class GrpcErrorHandler:
    """ç»Ÿä¸€çš„ gRPC é”™è¯¯å¤„ç†å™¨"""

    def __init__(self, client_logger: Optional[logging.Logger] = None):
        self.logger = client_logger or logger
        self.error_stats = ErrorStats()

    def handle_error(self, error: Union[grpc.RpcError, Exception], context: dict) -> TamarModelException:
        """
        ç»Ÿä¸€é”™è¯¯å¤„ç†æµç¨‹ï¼š
        1. åˆ›å»ºé”™è¯¯ä¸Šä¸‹æ–‡
        2. è®°å½•é”™è¯¯æ—¥å¿—
        3. æ›´æ–°é”™è¯¯ç»Ÿè®¡
        4. å†³å®šé”™è¯¯ç±»å‹
        5. è¿”å›ç›¸åº”å¼‚å¸¸
        """
        error_context = ErrorContext(error, context)

        # è®°å½•è¯¦ç»†é”™è¯¯æ—¥å¿—
        # å°†error_contextçš„é‡è¦ä¿¡æ¯å¹³é“ºåˆ°æ—¥å¿—çš„dataå­—æ®µä¸­
        log_data = {
            "log_type": "info",
            "request_id": error_context.request_id,
            "data": {
                "error_code": error_context.error_code.name if error_context.error_code else 'UNKNOWN',
                "error_message": error_context.error_message,
                "provider": error_context.provider,
                "model": error_context.model,
                "method": error_context.method,
                "retry_count": error_context.retry_count,
                "category": error_context._get_error_category(),
                "is_retryable": error_context._is_retryable(),
                "suggested_action": error_context._get_suggested_action(),
                "debug_string": error_context.error_debug_string,
                "is_network_cancelled": error_context.is_network_cancelled() if error_context.error_code == grpc.StatusCode.CANCELLED else None
            }
        }

        # å¦‚æœä¸Šä¸‹æ–‡ä¸­æœ‰ durationï¼Œæ·»åŠ åˆ°æ—¥å¿—ä¸­
        if 'duration' in context:
            log_data['duration'] = context['duration']

        self.logger.error(
            f"âŒ gRPC Error occurred: {error_context.error_code.name if error_context.error_code else 'UNKNOWN'}",
            extra=log_data
        )

        # æ›´æ–°é”™è¯¯ç»Ÿè®¡
        if error_context.error_code:
            self.error_stats.record_error(error_context.error_code)

        # æ ¹æ®é”™è¯¯ç±»å‹è¿”å›ç›¸åº”å¼‚å¸¸
        return self._create_exception(error_context)

    def _create_exception(self, error_context: ErrorContext) -> TamarModelException:
        """æ ¹æ®é”™è¯¯ä¸Šä¸‹æ–‡åˆ›å»ºç›¸åº”çš„å¼‚å¸¸"""
        error_code = error_context.error_code

        if not error_code:
            return TamarModelException(error_context)

        # è®¤è¯ç›¸å…³é”™è¯¯
        if error_code in ERROR_CATEGORIES['AUTH']:
            if error_code == grpc.StatusCode.UNAUTHENTICATED:
                return TokenExpiredException(error_context)
            else:
                return PermissionDeniedException(error_context)

        # ç½‘ç»œç›¸å…³é”™è¯¯
        elif error_code in ERROR_CATEGORIES['NETWORK']:
            if error_code == grpc.StatusCode.DEADLINE_EXCEEDED:
                return TimeoutException(error_context)
            else:
                return ConnectionException(error_context)

        # éªŒè¯ç›¸å…³é”™è¯¯
        elif error_code in ERROR_CATEGORIES['VALIDATION']:
            return InvalidParameterException(error_context)

        # èµ„æºç›¸å…³é”™è¯¯
        elif error_code == grpc.StatusCode.RESOURCE_EXHAUSTED:
            return RateLimitException(error_context)

        # æœåŠ¡å•†ç›¸å…³é”™è¯¯
        elif error_code in ERROR_CATEGORIES['PROVIDER']:
            return ProviderException(error_context)

        # é»˜è®¤é”™è¯¯
        else:
            return TamarModelException(error_context)

    def get_error_stats(self) -> Dict[str, Any]:
        """è·å–é”™è¯¯ç»Ÿè®¡ä¿¡æ¯"""
        return self.error_stats.get_stats()

    def reset_stats(self):
        """é‡ç½®é”™è¯¯ç»Ÿè®¡"""
        self.error_stats.reset()


class ErrorRecoveryStrategy:
    """é”™è¯¯æ¢å¤ç­–ç•¥"""

    RECOVERY_ACTIONS = {
        'refresh_token': 'handle_token_refresh',
        'reconnect': 'handle_reconnect',
        'backoff': 'handle_backoff',
        'circuit_break': 'handle_circuit_break',
    }

    def __init__(self, client):
        self.client = client

    async def recover_from_error(self, error_context: ErrorContext):
        """æ ¹æ®é”™è¯¯ç±»å‹æ‰§è¡Œæ¢å¤åŠ¨ä½œ"""
        if not error_context.error_code:
            return

        policy = RETRY_POLICY.get(error_context.error_code, {})

        if action := policy.get('action'):
            if action in self.RECOVERY_ACTIONS:
                handler = getattr(self, self.RECOVERY_ACTIONS[action])
                await handler(error_context)

    async def handle_token_refresh(self, error_context: ErrorContext):
        """å¤„ç† Token åˆ·æ–°"""
        self.client.logger.info("ğŸ”„ Attempting to refresh JWT token")
        # è¿™é‡Œéœ€è¦å®¢æˆ·ç«¯å®ç° _refresh_jwt_token æ–¹æ³•
        if hasattr(self.client, '_refresh_jwt_token'):
            await self.client._refresh_jwt_token()

    async def handle_reconnect(self, error_context: ErrorContext):
        """å¤„ç†é‡è¿"""
        self.client.logger.info("ğŸ”„ Attempting to reconnect channel")
        # è¿™é‡Œéœ€è¦å®¢æˆ·ç«¯å®ç° _reconnect_channel æ–¹æ³•
        if hasattr(self.client, '_reconnect_channel'):
            await self.client._reconnect_channel()

    async def handle_backoff(self, error_context: ErrorContext):
        """å¤„ç†é€€é¿ç­‰å¾…"""
        wait_time = self._calculate_backoff(error_context.retry_count)
        await asyncio.sleep(wait_time)

    async def handle_circuit_break(self, error_context: ErrorContext):
        """å¤„ç†ç†”æ–­"""
        self.client.logger.warning("âš ï¸ Circuit breaker activated")
        # è¿™é‡Œå¯ä»¥å®ç°ç†”æ–­é€»è¾‘
        pass

    def _calculate_backoff(self, retry_count: int) -> float:
        """è®¡ç®—é€€é¿æ—¶é—´"""
        base_delay = 1.0
        max_delay = 60.0
        jitter_factor = 0.1

        delay = min(base_delay * (2 ** retry_count), max_delay)
        jitter = random.uniform(0, delay * jitter_factor)
        return delay + jitter


class EnhancedRetryHandler:
    """å¢å¼ºçš„é‡è¯•å¤„ç†å™¨"""

    def __init__(self, max_retries: int = 3, base_delay: float = 1.0):
        self.max_retries = max_retries
        self.base_delay = base_delay
        self.error_handler = GrpcErrorHandler()

    async def execute_with_retry(
            self,
            func: Callable,
            *args,
            context: Optional[Dict[str, Any]] = None,
            **kwargs
    ):
        """
        æ‰§è¡Œå‡½æ•°å¹¶å¤„ç†é‡è¯•
        
        Args:
            func: è¦æ‰§è¡Œçš„å‡½æ•°
            *args: å‡½æ•°å‚æ•°
            context: è¯·æ±‚ä¸Šä¸‹æ–‡ä¿¡æ¯
            **kwargs: å‡½æ•°å…³é”®å­—å‚æ•°
            
        Returns:
            å‡½æ•°æ‰§è¡Œç»“æœ
            
        Raises:
            TamarModelException: åŒ…è£…åçš„å¼‚å¸¸
        """
        # è®°å½•å¼€å§‹æ—¶é—´
        import time
        method_start_time = time.time()

        context = context or {}
        last_exception = None

        for attempt in range(self.max_retries + 1):
            try:
                context['retry_count'] = attempt
                return await func(*args, **kwargs)

            except (grpc.RpcError, grpc.aio.AioRpcError) as e:
                # åˆ›å»ºé”™è¯¯ä¸Šä¸‹æ–‡
                error_context = ErrorContext(e, context)
                current_duration = time.time() - method_start_time
                context['duration'] = current_duration

                # åˆ¤æ–­æ˜¯å¦å¯ä»¥é‡è¯•
                should_retry = self._should_retry(e, attempt)
                
                # æ£€æŸ¥æ˜¯å¦åº”è¯¥å°è¯•å¿«é€Ÿé™çº§ï¼ˆéœ€è¦ä»å¤–éƒ¨æ³¨å…¥clientå¼•ç”¨ï¼‰
                should_try_fallback = False
                if hasattr(self.error_handler, 'client') and hasattr(self.error_handler.client, '_should_try_fallback'):
                    should_try_fallback = self.error_handler.client._should_try_fallback(e.code(), attempt)
                
                if should_try_fallback:
                    # å°è¯•å¿«é€Ÿé™çº§åˆ°HTTP
                    logger.warning(
                        f"ğŸš€ Fast fallback triggered for {e.code().name} after {attempt + 1} attempts",
                        extra={
                            "log_type": "fast_fallback",
                            "request_id": error_context.request_id,
                            "data": {
                                "error_code": e.code().name,
                                "attempt": attempt,
                                "fallback_reason": "immediate" if hasattr(self.error_handler.client, 'immediate_fallback_errors') and e.code() in self.error_handler.client.immediate_fallback_errors else "after_retries"
                            }
                        }
                    )
                    
                    try:
                        # å°è¯•HTTPé™çº§ï¼ˆéœ€è¦ä»contextè·å–å¿…è¦å‚æ•°ï¼‰
                        if hasattr(self.error_handler, 'client'):
                            # æ£€æŸ¥æ˜¯å¦æ˜¯æ‰¹é‡è¯·æ±‚
                            if hasattr(self.error_handler.client, '_current_batch_request'):
                                batch_request = self.error_handler.client._current_batch_request
                                origin_request_id = getattr(self.error_handler.client, '_current_origin_request_id', None)
                                timeout = context.get('timeout')
                                request_id = context.get('request_id')
                                
                                # å°è¯•æ‰¹é‡HTTPé™çº§
                                result = await self.error_handler.client._invoke_batch_http_fallback(batch_request, timeout, request_id, origin_request_id)
                            elif hasattr(self.error_handler.client, '_current_model_request'):
                                model_request = self.error_handler.client._current_model_request
                                origin_request_id = getattr(self.error_handler.client, '_current_origin_request_id', None)
                                timeout = context.get('timeout')
                                request_id = context.get('request_id')
                                
                                # å°è¯•HTTPé™çº§
                                result = await self.error_handler.client._invoke_http_fallback(model_request, timeout, request_id, origin_request_id)
                            
                            logger.info(
                                f"âœ… Fast fallback to HTTP succeeded",
                                extra={
                                    "log_type": "fast_fallback_success",
                                    "request_id": request_id,
                                    "data": {
                                        "grpc_attempts": attempt + 1,
                                        "fallback_duration": time.time() - method_start_time
                                    }
                                }
                            )
                            
                            return result
                    except Exception as fallback_error:
                        # é™çº§å¤±è´¥ï¼Œè®°å½•æ—¥å¿—ä½†ç»§ç»­åŸæœ‰é‡è¯•é€»è¾‘
                        logger.warning(
                            f"âš ï¸ Fast fallback to HTTP failed: {str(fallback_error)}",
                            extra={
                                "log_type": "fast_fallback_failed",
                                "request_id": error_context.request_id,
                                "data": {
                                    "fallback_error": str(fallback_error),
                                    "will_continue_grpc_retry": should_retry and attempt < self.max_retries
                                }
                            }
                        )

                if not should_retry:
                    # ä¸å¯é‡è¯•æˆ–å·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°
                    # è®°å½•æœ€ç»ˆå¤±è´¥æ—¥å¿—
                    log_data = {
                        "log_type": "info",
                        "request_id": error_context.request_id,
                        "data": {
                            "error_code": error_context.error_code.name if error_context.error_code else 'UNKNOWN',
                            "error_message": error_context.error_message,
                            "retry_count": attempt,
                            "max_retries": self.max_retries,
                            "category": error_context._get_error_category(),
                            "is_retryable": False,
                            "method": error_context.method,
                            "final_failure": True
                        },
                        "duration": current_duration
                    }
                    error_detail = f" - {error_context.error_message}" if error_context.error_message else ""
                    logger.warning(
                        f"âš ï¸ Attempt {attempt + 1}/{self.max_retries + 1} failed: {e.code()}{error_detail} (no more retries)",
                        extra=log_data
                    )
                    last_exception = self.error_handler.handle_error(e, context)
                    break

                # å¯ä»¥é‡è¯•ï¼Œè®°å½•é‡è¯•æ—¥å¿—
                log_data = {
                    "log_type": "info",
                    "request_id": error_context.request_id,
                    "data": {
                        "error_code": error_context.error_code.name if error_context.error_code else 'UNKNOWN',
                        "error_message": error_context.error_message,
                        "retry_count": attempt,
                        "max_retries": self.max_retries,
                        "category": error_context._get_error_category(),
                        "is_retryable": True,
                        "method": error_context.method,
                        "will_retry": True,
                        "fallback_attempted": should_try_fallback
                    },
                    "duration": current_duration
                }
                error_detail = f" - {error_context.error_message}" if error_context.error_message else ""
                logger.warning(
                    f"ğŸ”„ Attempt {attempt + 1}/{self.max_retries + 1} failed: {e.code()}{error_detail} (will retry)",
                    extra=log_data
                )

                # æ‰§è¡Œé€€é¿ç­‰å¾…
                if attempt < self.max_retries:
                    delay = self._calculate_backoff(attempt)
                    await asyncio.sleep(delay)

                # ä¿å­˜å¼‚å¸¸ï¼Œä»¥å¤‡åç»­ä½¿ç”¨
                last_exception = e

            except Exception as e:
                # é gRPC é”™è¯¯ï¼Œç›´æ¥åŒ…è£…æŠ›å‡º
                context['retry_count'] = attempt
                error_context = ErrorContext(None, context)
                error_context.error_message = str(e)
                last_exception = TamarModelException(error_context)
                break

        # æŠ›å‡ºæœ€åçš„å¼‚å¸¸
        if last_exception:
            if isinstance(last_exception, TamarModelException):
                raise last_exception
            else:
                # å¯¹äºåŸå§‹çš„ gRPC å¼‚å¸¸ï¼Œéœ€è¦åŒ…è£…
                raise self.error_handler.handle_error(last_exception, context)
        else:
            raise TamarModelException("Unknown error occurred")

    def _should_retry(self, error: grpc.RpcError, attempt: int) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥é‡è¯•"""
        error_code = error.code()
        policy = RETRY_POLICY.get(error_code, {})

        # å…ˆæ£€æŸ¥é”™è¯¯çº§åˆ«çš„ max_attempts é…ç½®
        # max_attempts è¡¨ç¤ºæœ€å¤§é‡è¯•æ¬¡æ•°ï¼ˆä¸åŒ…æ‹¬åˆå§‹è¯·æ±‚ï¼‰
        error_max_attempts = policy.get('max_attempts', self.max_retries)
        if attempt >= error_max_attempts:
            return False

        # å†æ£€æŸ¥å…¨å±€çš„ max_retries
        if attempt >= self.max_retries:
            return False

        # æ£€æŸ¥åŸºæœ¬é‡è¯•ç­–ç•¥
        retryable = policy.get('retryable', False)
        if retryable == False:
            return False
        elif retryable == True:
            return True
        elif retryable == 'conditional':
            # æ¡ä»¶é‡è¯•ï¼Œéœ€è¦æ£€æŸ¥é”™è¯¯è¯¦æƒ…
            return self._check_conditional_retry(error)

        return False

    def _check_conditional_retry(self, error: grpc.RpcError) -> bool:
        """æ£€æŸ¥æ¡ä»¶é‡è¯•"""
        error_message = error.details().lower() if error.details() else ""

        # ä¸€äº›å¯é‡è¯•çš„å†…éƒ¨é”™è¯¯æ¨¡å¼
        retryable_patterns = [
            'temporary', 'timeout', 'unavailable',
            'connection', 'network', 'try again'
        ]

        for pattern in retryable_patterns:
            if pattern in error_message:
                return True

        return False

    def _calculate_backoff(self, attempt: int) -> float:
        """è®¡ç®—é€€é¿æ—¶é—´"""
        max_delay = 60.0
        jitter_factor = 0.1

        delay = min(self.base_delay * (2 ** attempt), max_delay)
        jitter = random.uniform(0, delay * jitter_factor)
        return delay + jitter
