#!/usr/bin/env python3
"""
ç®€åŒ–ç‰ˆçš„ Google/Azure åœºæ™¯æµ‹è¯•è„šæœ¬
åªä¿ç•™åŸºæœ¬è°ƒç”¨å’Œæ‰“å°åŠŸèƒ½
"""

import asyncio
import logging
import os
import sys

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

os.environ['MODEL_MANAGER_SERVER_GRPC_USE_TLS'] = "false"
os.environ['MODEL_MANAGER_SERVER_ADDRESS'] = "localhost:50051"
os.environ['MODEL_MANAGER_SERVER_JWT_SECRET_KEY'] = "model-manager-server-jwt-key"

# å¯¼å…¥å®¢æˆ·ç«¯æ¨¡å—
try:
    from tamar_model_client import TamarModelClient, AsyncTamarModelClient
    from tamar_model_client.schemas import ModelRequest, UserContext
    from tamar_model_client.enums import ProviderType, InvokeType, Channel
except ImportError as e:
    logger.error(f"å¯¼å…¥æ¨¡å—å¤±è´¥: {e}")
    sys.exit(1)


def test_google_ai_studio():
    """æµ‹è¯• Google AI Studio"""
    print("\nğŸ” æµ‹è¯• Google AI Studio...")

    try:
        client = TamarModelClient()

        request = ModelRequest(
            provider=ProviderType.GOOGLE,
            channel=Channel.AI_STUDIO,
            invoke_type=InvokeType.GENERATION,
            model="gemini-pro",
            contents=[
                {"role": "user", "parts": [{"text": "Hello, how are you?"}]}
            ],
            user_context=UserContext(
                user_id="test_user",
                org_id="test_org",
                client_type="test_client"
            ),
            config={
                "temperature": 0.7,
                "maxOutputTokens": 100
            }
        )

        response = client.invoke(request)
        print(f"âœ… Google AI Studio æˆåŠŸ")
        print(f"   å“åº”ç±»å‹: {type(response)}")
        print(f"   å“åº”å†…å®¹: {str(response)[:200]}...")

    except Exception as e:
        print(f"âŒ Google AI Studio å¤±è´¥: {str(e)}")


def test_google_vertex_ai():
    """æµ‹è¯• Google Vertex AI"""
    print("\nğŸ” æµ‹è¯• Google Vertex AI...")

    try:
        client = TamarModelClient()

        request = ModelRequest(
            provider=ProviderType.GOOGLE,
            channel=Channel.VERTEXAI,
            invoke_type=InvokeType.GENERATION,
            model="gemini-1.5-flash",
            contents=[
                {"role": "user", "parts": [{"text": "What is AI?"}]}
            ],
            user_context=UserContext(
                user_id="test_user",
                org_id="test_org",
                client_type="test_client"
            ),
            config={
                "temperature": 0.5
            }
        )

        response = client.invoke(request)
        print(f"âœ… Google Vertex AI æˆåŠŸ")
        print(f"   å“åº”ç±»å‹: {type(response)}")
        print(f"   å“åº”å†…å®¹: {str(response)[:200]}...")

    except Exception as e:
        print(f"âŒ Google Vertex AI å¤±è´¥: {str(e)}")


def test_azure_openai():
    """æµ‹è¯• Azure OpenAI"""
    print("\nâ˜ï¸  æµ‹è¯• Azure OpenAI...")

    try:
        client = TamarModelClient()

        request = ModelRequest(
            provider=ProviderType.AZURE,
            channel=Channel.OPENAI,
            invoke_type=InvokeType.CHAT_COMPLETIONS,
            model="gpt-4o-mini",
            messages=[
                {"role": "user", "content": "Hello, how are you?"}
            ],
            user_context=UserContext(
                user_id="test_user",
                org_id="test_org",
                client_type="test_client"
            ),
            temperature=0.7,
            max_tokens=100
        )

        response = client.invoke(request)
        print(f"âœ… Azure OpenAI æˆåŠŸ")
        print(f"   å“åº”ç±»å‹: {type(response)}")
        print(f"   å“åº”å†…å®¹: {str(response)[:200]}...")

    except Exception as e:
        print(f"âŒ Azure OpenAI å¤±è´¥: {str(e)}")


async def test_google_streaming():
    """æµ‹è¯• Google æµå¼å“åº”"""
    print("\nğŸ“¡ æµ‹è¯• Google æµå¼å“åº”...")

    try:
        client = AsyncTamarModelClient()

        request = ModelRequest(
            provider=ProviderType.GOOGLE,
            channel=Channel.AI_STUDIO,
            invoke_type=InvokeType.GENERATION,
            model="gemini-pro",
            contents=[
                {"role": "user", "parts": [{"text": "Count 1 to 5"}]}
            ],
            user_context=UserContext(
                user_id="test_user",
                org_id="test_org",
                client_type="test_client"
            ),
            stream=True,
            config={
                "temperature": 0.1,
                "maxOutputTokens": 50
            }
        )

        response_gen = await client.invoke(request)
        print(f"âœ… Google æµå¼è°ƒç”¨æˆåŠŸ")
        print(f"   å“åº”ç±»å‹: {type(response_gen)}")

        chunk_count = 0
        async for chunk in response_gen:
            chunk_count += 1
            print(f"   æ•°æ®å— {chunk_count}: {type(chunk)} - {str(chunk)[:100]}...")
            if chunk_count >= 3:  # åªæ˜¾ç¤ºå‰3ä¸ªæ•°æ®å—
                break

    except Exception as e:
        print(f"âŒ Google æµå¼å“åº”å¤±è´¥: {str(e)}")


async def test_azure_streaming():
    """æµ‹è¯• Azure æµå¼å“åº”"""
    print("\nğŸ“¡ æµ‹è¯• Azure æµå¼å“åº”...")

    try:
        client = AsyncTamarModelClient()

        request = ModelRequest(
            provider=ProviderType.AZURE,
            channel=Channel.OPENAI,
            invoke_type=InvokeType.CHAT_COMPLETIONS,
            model="gpt-4o-mini",
            messages=[
                {"role": "user", "content": "Count 1 to 5"}
            ],
            user_context=UserContext(
                user_id="test_user",
                org_id="test_org",
                client_type="test_client"
            ),
            stream=True,
            temperature=0.1,
            max_tokens=50
        )

        response_gen = await client.invoke(request)
        print(f"âœ… Azure æµå¼è°ƒç”¨æˆåŠŸ")
        print(f"   å“åº”ç±»å‹: {type(response_gen)}")

        chunk_count = 0
        async for chunk in response_gen:
            chunk_count += 1
            print(f"   æ•°æ®å— {chunk_count}: {type(chunk)} - {str(chunk)[:100]}...")
            if chunk_count >= 3:  # åªæ˜¾ç¤ºå‰3ä¸ªæ•°æ®å—
                break

    except Exception as e:
        print(f"âŒ Azure æµå¼å“åº”å¤±è´¥: {str(e)}")


async def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ ç®€åŒ–ç‰ˆ Google/Azure æµ‹è¯•")
    print("=" * 50)

    # åŒæ­¥æµ‹è¯•
    test_google_ai_studio()
    test_google_vertex_ai()
    test_azure_openai()

    # å¼‚æ­¥æµå¼æµ‹è¯•
    await test_google_streaming()
    await test_azure_streaming()

    print("\nâœ… æµ‹è¯•å®Œæˆ")


if __name__ == "__main__":
    asyncio.run(main())