import argparse
import pathlib

import diffmahnet
import equinox as eqx
import jax
import jax.numpy as jnp
from diffmahnet import datatools
from diffopt import kdescent

SAVE_DIR = pathlib.Path("./data/")
TRAIN_DATA_DIR = pathlib.Path(
    "/lcrc/project/halotools/diffmahpop_data/NM_12_NT_9_ISTART_0_IEND_576/"
)

NN_DEPTH = 2
NN_WIDTH = 50
FLOW_LAYERS = 8
SAMPLE_FRAC = 1.0
NUM_KERNELS = 20
NUM_FOURIER_KERNELS = 0
LEARNING_RATE = 1e-4


class KDescentLoss:
    """
    Custom loss function to fit flowjax model
    """

    def __init__(
        self,
        train_data,
        sample_size=None,
        randkey=None,
        num_kernels=20,
        num_fourier_kernels=0,
    ):
        randkey = jax.random.key(0) if randkey is None else randkey
        # t0 = 13.8
        # self.logt0 = np.log10(t0)
        self.logt0 = train_data.logt0
        self.sample_size = sample_size
        self.xscaler = train_data.x_scaler
        self.uscaler = train_data.u_scaler

        self.tgrids, self.log_mah = train_data.get_tgrid_and_log_mah(randkey)
        self.m_obs = train_data.m_obs
        self.t_obs = train_data.t_obs
        assert self.log_mah.ndim == self.tgrids.ndim == 2
        assert self.m_obs.ndim == self.t_obs.ndim == 1
        assert (
            self.log_mah.shape[0]
            == self.m_obs.shape[0]
            == self.t_obs.shape[0]
            == self.tgrids.shape[0]
        )
        self.condition = jnp.array([self.m_obs, self.t_obs]).T

        # Combine m and t with condition (m_obs, t_obs), since we always have
        # an equivalent sampling of the conditional variables
        # and this saves us from having to generate many separate
        # KCalc instances at different conditional value bins
        self.training_combined = (
            jnp.array(
                [
                    self.log_mah,
                    self.tgrids,
                    self.tile(self.m_obs),
                    self.tile(self.t_obs),
                ]
            )
            .reshape((4, -1))
            .T
        )
        self.kde = kdescent.KCalc(
            self.training_combined,
            num_kernels=num_kernels,
            num_fourier_kernels=num_fourier_kernels,
        )

    def tile(self, arr):
        return jnp.tile(arr[..., None], (1, self.tgrids.shape[1]))

    @eqx.filter_jit
    def __call__(self, diffmahflow, randkey):
        """Compute the loss using kdescent"""
        key0, key1, key2, key3 = jax.random.split(randkey, 4)
        if self.sample_size is None:
            tsamp = slice(None)
        else:
            tsamp = jax.random.choice(
                key0,
                self.training_combined.shape[0],
                (self.sample_size,),
                replace=False,
            )
        mah_params = diffmahflow.sample(
            self.condition[tsamp], randkey=key1, asparams=True
        )
        log_mah = diffmahnet.log_mah_kern(mah_params, self.tgrids[tsamp], self.logt0)
        model_combined = (
            jnp.array(
                [
                    log_mah,
                    self.tgrids[tsamp],
                    self.tile(self.m_obs[tsamp]),
                    self.tile(self.t_obs[tsamp]),
                ]
            )
            .reshape((4, -1))
            .T
        )

        if self.kde.num_fourier_kernels:
            counts_model, counts_truth = self.kde.compare_fourier_counts(
                key2, model_combined
            )
            ecf_model = counts_model / model_combined.shape[0]
            ecf_truth = counts_truth / self.training_combined.shape[0]
            loss = jnp.sum(jnp.abs(ecf_model - ecf_truth) ** 2)
        else:
            loss = 0.0

        counts_model, counts_truth = self.kde.compare_kde_counts(key3, model_combined)
        pdf_model = counts_model / model_combined.shape[0]
        pdf_truth = counts_truth / self.training_combined.shape[0]
        loss += jnp.sum((pdf_model - pdf_truth) ** 2)

        # Optionally divide by total number of kernels to get MSE loss
        # loss /= (self.kde.num_kernels + self.kde.num_fourier_kernels)
        jax.debug.print("loss = {loss}", loss=loss)

        return loss


parser = argparse.ArgumentParser(
    description="Train a DiffMahNet normalizing flow model."
)
parser.add_argument("SAVE_FILENAME", help="Filename to save the trained model.")
parser.add_argument(
    "--save-dir",
    type=str,
    default=SAVE_DIR,
    help="Directory to save the trained model.",
)
parser.add_argument(
    "--train-data-dir",
    type=str,
    default=TRAIN_DATA_DIR,
    help="Directory containing the training data.",
)
parser.add_argument(
    "--initial-model",
    type=str,
    default=None,
    help="Optional filename of an initial model to load.",
)
parser.add_argument("--sats", action="store_true")
parser.add_argument(
    "--nn-depth", type=int, default=NN_DEPTH, help="Depth of the hidden neural network."
)
parser.add_argument(
    "--nn-width", type=int, default=NN_WIDTH, help="Width of the hidden neural network."
)
parser.add_argument(
    "--flow-layers", type=int, default=FLOW_LAYERS, help="Number of flow layers."
)
parser.add_argument(
    "--include-test", action="store_true", help="Include test data in the training set."
)
parser.add_argument("--steps", type=int, default=100, help="Number of adam iterations.")
parser.add_argument(
    "--learning-rate",
    type=float,
    default=LEARNING_RATE,
    help="Initial adam learning rate.",
)
parser.add_argument(
    "--num-kernels", type=int, default=NUM_KERNELS, help="Number of kdescent kernels."
)
parser.add_argument(
    "--num-fourier-kernels",
    type=int,
    default=NUM_FOURIER_KERNELS,
    help="Number of kdescent fourier kernels.",
)
parser.add_argument(
    "--sample-frac",
    type=float,
    default=SAMPLE_FRAC,
    help="Fraction of training data to load.",
)
parser.add_argument(
    "--seed", type=int, default=0, help="Random seed for reproducibility."
)
parser.add_argument(
    "--plot-loss-curve",
    action="store_true",
    help="Plot the loss curve during training.",
)


if __name__ == "__main__":
    # Parse arguments
    args = parser.parse_args()
    save_dir = pathlib.Path(args.save_dir)
    save_dir.mkdir(parents=True, exist_ok=True)
    save_filename = args.SAVE_FILENAME
    train_data_dir = pathlib.Path(args.train_data_dir)
    is_cens = not args.sats
    nn_depth = args.nn_depth
    nn_width = args.nn_width
    flow_layers = args.flow_layers
    is_test = "both" if args.include_test else False
    initial_model = args.initial_model
    steps = args.steps
    sample_frac = args.sample_frac

    key = jax.random.key(args.seed)
    key1, key2 = jax.random.split(key)

    # Load training data and flow model
    train_data = datatools.DataHolder(
        train_data_dir,
        is_cens=is_cens,
        is_test=is_test,
        sample_frac=sample_frac,
        randkey=key1,
    )
    if initial_model is not None:
        initial_model = save_dir / initial_model
        flow = diffmahnet.DiffMahFlow.load(initial_model)
    else:
        flow = diffmahnet.DiffMahFlow(
            scaler=train_data.scaler,
            nn_depth=nn_depth,
            nn_width=nn_width,
            flow_layers=flow_layers,
        )
    print("Number of parameters =", flow.get_params().size)

    # Train the flow model
    if steps > 0:
        loss_func = KDescentLoss(
            train_data,
            num_kernels=args.num_kernels,
            num_fourier_kernels=args.num_fourier_kernels,
        )
        params, losses = flow.adam_fit(
            loss_func, randkey=key2, nsteps=steps, learning_rate=args.learning_rate
        )
        if args.plot_loss_curve:
            import matplotlib.pyplot as plt

            plt.semilogy(losses)
            plt.xlabel("Iteration")
            plt.ylabel("Loss")
            plot_filename = save_filename.removesuffix(".eqx") + ".png"
            plt.savefig(save_dir / plot_filename)
            plt.close()

    # Save the trained model
    flow.save(save_dir / save_filename)
