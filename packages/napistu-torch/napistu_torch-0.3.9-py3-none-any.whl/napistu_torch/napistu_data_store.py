"""
NapistuDataStore - Management system for Napistu data objects.

This module provides a store system for managing NapistuData, VertexTensor,
and pandas DataFrame objects related to a single SBML_dfs/NapistuGraph pair.

Classes
-------
NapistuDataStore
    Manage data objects related to a single SBML_dfs/NapistuGraph pair.
"""

import json
import logging
import shutil
from collections import defaultdict
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Union

import pandas as pd
from napistu.network.ng_core import NapistuGraph
from napistu.sbml_dfs_core import SBML_dfs

from napistu_torch.configs import DataConfig, TaskConfig
from napistu_torch.constants import (
    ARTIFACT_TYPES,
    NAPISTU_DATA,
    NAPISTU_DATA_STORE,
    NAPISTU_DATA_STORE_STRUCTURE,
    VALID_ARTIFACT_TYPES,
    VERTEX_TENSOR,
)
from napistu_torch.load.artifacts import (
    DEFAULT_ARTIFACT_REGISTRY,
    ArtifactDefinition,
    create_artifact,
    validate_artifact_registry,
)
from napistu_torch.load.constants import VALID_SPLITTING_STRATEGIES
from napistu_torch.ml.constants import DEVICE
from napistu_torch.napistu_data import NapistuData
from napistu_torch.utils.base_utils import ensure_path
from napistu_torch.vertex_tensor import VertexTensor

logger = logging.getLogger(__name__)


class NapistuDataStore:
    """
    Manage data objects related to a single SBML_dfs/NapistuGraph pair.

    Directory structure:
    store_dir/
    ├── registry.json           # Registry of all objects in this store
    ├── napistu_raw/            # (optional raw directory)
    │   ├── sbml_dfs.pkl        # (optional copy)
    │   └── napistu_graph.pkl   # (optional copy)
    ├── napistu_data/           # organizes NapistuData objects
    |   └── (NapistuData .pt files)
    ├── vertex_tensors/          # organizes VertexTensor objects
    │   └── (VertexTensor .pt files)
    └── pandas_dfs/             # organizes pandas DataFrames
        └── (DataFrame .parquet files)

    Each store manages objects for a single biological network.

    Public Methods
    --------------
    create(store_dir, sbml_dfs_path, napistu_graph_path, copy_to_store=False, overwrite=False)
        Create a new NapistuDataStore
    enable_artifact_creation(sbml_dfs_path, napistu_graph_path)
        Convert a read-only store to non-read-only by enabling artifact creation.
    ensure_artifacts(artifact_names, artifact_registry=DEFAULT_ARTIFACT_REGISTRY, overwrite=False)
        Ensure specified artifacts exist in the store, creating if missing.
    get_missing_artifacts(artifact_names)
        Check which artifacts are missing from the store.
    from_config(config)
        Create or load a NapistuDataStore from a DataConfig.
    from_huggingface(repo_id, store_dir, revision=None, token=None, sbml_dfs_path=None, napistu_graph_path=None)
        Download and create a NapistuDataStore from HuggingFace Hub.
        Optionally convert from read-only to non-read-only by providing paths.
    list_napistu_datas()
        List all NapistuData names in the store
    list_vertex_tensors()
        List all VertexTensor names in the store
    list_pandas_dfs()
        List all pandas DataFrame names in the store
    load_sbml_dfs()
        Load the SBML_dfs from disk
    load_napistu_data(name, map_location="cpu")
        Load a NapistuData object from the store
    load_napistu_graph()
        Load the NapistuGraph from disk
    load_vertex_tensor(name, map_location="cpu")
        Load a VertexTensor from the store
    load_pandas_df(name)
        Load a pandas DataFrame from the store
    publish_store_to_huggingface(repo_id, revision=None, overwrite=False, commit_message=None, token=None)
        Publish entire store to HuggingFace Hub as a read-only dataset
    save_napistu_data(napistu_data, name=None, overwrite=False)
        Save a NapistuData object to the store
    save_vertex_tensor(vertex_tensor, name=None, overwrite=False)
        Save a VertexTensor to the store
    save_pandas_df(dataframe, name=None, overwrite=False)
        Save a pandas DataFrame to the store
    summary()
        Get a summary of the store contents

    Private Methods
    ---------------
    _load_registry()
        Load the registry from disk
    _save_registry()
        Save the registry to disk
    """

    def __init__(self, store_dir: Union[str, Path]):
        """
        Initialize the NapistuDataStore from an existing registry.

        Parameters
        ----------
        store_dir : Union[str, Path]
            Root directory for this store. Must contain a registry.json file.

        Raises
        ------
        FileNotFoundError
            If the registry.json file does not exist

        Examples
        --------
        >>> # Load an existing store
        >>> store = NapistuDataStore('.store')
        """
        self.store_dir = ensure_path(store_dir)
        self.registry_path = self.store_dir / NAPISTU_DATA_STORE_STRUCTURE.REGISTRY_FILE

        if not self.registry_path.exists():
            raise FileNotFoundError(
                f"Registry not found at {self.registry_path}. "
                f"Use NapistuDataStore.create() to initialize a new store."
            )

        # Load registry
        self.registry = self._load_registry()

        # Set read_only attribute (default to False for backward compatibility)
        self.read_only = self.registry.get(NAPISTU_DATA_STORE.READ_ONLY, False)

        # set attributes based on values in the registry
        napistu_raw = self.registry[NAPISTU_DATA_STORE.NAPISTU_RAW]
        sbml_dfs_path_str = napistu_raw[NAPISTU_DATA_STORE.SBML_DFS]
        napistu_graph_path_str = napistu_raw[NAPISTU_DATA_STORE.NAPISTU_GRAPH]

        # Handle None paths (read_only stores)
        if self.read_only:
            self.sbml_dfs_path = None
        else:
            if sbml_dfs_path_str is None:
                raise FileNotFoundError(
                    "SBML_dfs file is not set but read_only is False"
                )
            self.sbml_dfs_path = _resolve_path(sbml_dfs_path_str, self.store_dir)

        if self.read_only:
            self.napistu_graph_path = None
        else:
            if napistu_graph_path_str is None:
                raise FileNotFoundError(
                    "NapistuGraph file is not set but read_only is False"
                )
            self.napistu_graph_path = _resolve_path(
                napistu_graph_path_str, self.store_dir
            )

    @classmethod
    def create(
        cls,
        store_dir: Union[str, Path],
        read_only: bool = False,
        sbml_dfs_path: Optional[Union[str, Path]] = None,
        napistu_graph_path: Optional[Union[str, Path]] = None,
        copy_to_store: bool = False,
        overwrite: bool = False,
    ) -> "NapistuDataStore":
        """
        Create a new NapistuDataStore.

        Parameters
        ----------
        store_dir : Union[str, Path]
            Root directory for this store
        read_only: bool, default=False
            If True, the store will be read-only which means that no new artifacts
            can be created. If True, sbml_dfs_path and napistu_graph_path will be ignored.
        sbml_dfs_path : Optional[Union[str, Path]], default=None
            Path to the SBML_dfs pickle file. Ignored if read_only is True.
        napistu_graph_path : Optional[Union[str, Path]], default=None
            Path to the NapistuGraph pickle file. Ignored if read_only is True.
        copy_to_store : bool, default=False
            If True, copy the files into the store directory and store relative paths.
            If False, store absolute paths to the original files.
            Ignored if read_only is True.
        overwrite : bool, default=False
            If True, remove existing store_dir if it exists before creating new store.
            If False, raise FileExistsError if store_dir already exists.

        Returns
        -------
        NapistuDataStore
            The newly created store

        Raises
        ------
        FileExistsError
            If a registry.json already exists at store_dir and overwrite=False
        FileNotFoundError
            If the specified napistu files don't exist (only when read_only=False)

        Examples
        --------
        >>> # Create a new store with external paths
        >>> store = NapistuDataStore.create(
        ...     store_dir='./stores/ecoli',
        ...     sbml_dfs_path='/data/ecoli_sbml_dfs.pkl',
        ...     napistu_graph_path='/data/ecoli_ng.pkl',
        ...     copy_to_store=False
        ... )
        """
        store_dir = ensure_path(store_dir)
        registry_path = store_dir / NAPISTU_DATA_STORE_STRUCTURE.REGISTRY_FILE

        # Handle overwrite logic
        if overwrite and store_dir.exists():
            logger.warning(f"Overwriting existing store at {store_dir}")
            shutil.rmtree(store_dir)

        # Skip path validation and copying when read_only is True
        if not read_only:
            sbml_dfs_path = (
                ensure_path(sbml_dfs_path) if sbml_dfs_path is not None else None
            )
            napistu_graph_path = (
                ensure_path(napistu_graph_path)
                if napistu_graph_path is not None
                else None
            )
            _validate_create_inputs(registry_path, sbml_dfs_path, napistu_graph_path)

        else:
            if sbml_dfs_path is not None:
                logger.warning(
                    f"sbml_dfs_path {sbml_dfs_path} will be ignored becasue read_only is True"
                )
            if napistu_graph_path is not None:
                logger.warning(
                    f"napistu_graph_path {napistu_graph_path} will be ignored becasue read_only is True"
                )

            # In read_only mode, paths are ignored
            sbml_dfs_path = None
            napistu_graph_path = None

        # create directories
        store_dir.mkdir(parents=True, exist_ok=True)
        napistu_data_dir = store_dir / NAPISTU_DATA_STORE_STRUCTURE.NAPISTU_DATA
        napistu_data_dir.mkdir(exist_ok=True)
        vertex_tensors_dir = store_dir / NAPISTU_DATA_STORE_STRUCTURE.VERTEX_TENSORS
        vertex_tensors_dir.mkdir(exist_ok=True)
        pandas_dfs_dir = store_dir / NAPISTU_DATA_STORE_STRUCTURE.PANDAS_DFS
        pandas_dfs_dir.mkdir(exist_ok=True)

        # Only create napistu_raw directory and copy files if not read_only
        if not read_only and copy_to_store:
            napistu_raw_dir = store_dir / NAPISTU_DATA_STORE_STRUCTURE.NAPISTU_RAW
            napistu_raw_dir.mkdir(exist_ok=True)

        # copy sbml_dfs and napistu_graph to store if requested (and not read_only)
        if not read_only and copy_to_store:
            # Copy files to store
            cached_sbml_path = napistu_raw_dir / sbml_dfs_path.name
            cached_ng_path = napistu_raw_dir / napistu_graph_path.name

            logger.info(f"Copying SBML_dfs from {sbml_dfs_path} to {cached_sbml_path}")
            logger.info(
                f"Copying NapistuGraph from {napistu_graph_path} to {cached_ng_path}"
            )

            shutil.copy2(sbml_dfs_path, cached_sbml_path)
            shutil.copy2(napistu_graph_path, cached_ng_path)

            # Store relative paths from store_dir
            sbml_relative = cached_sbml_path.relative_to(store_dir)
            ng_relative = cached_ng_path.relative_to(store_dir)

            napistu_entry = {
                NAPISTU_DATA_STORE.SBML_DFS: str(sbml_relative),
                NAPISTU_DATA_STORE.NAPISTU_GRAPH: str(ng_relative),
            }
        elif not read_only:
            # Store normalized absolute paths to original files
            napistu_entry = {
                NAPISTU_DATA_STORE.SBML_DFS: str(
                    _resolve_path(sbml_dfs_path, store_dir)
                ),
                NAPISTU_DATA_STORE.NAPISTU_GRAPH: str(
                    _resolve_path(napistu_graph_path, store_dir)
                ),
            }
        else:
            # read_only mode: store None for paths
            napistu_entry = {
                NAPISTU_DATA_STORE.SBML_DFS: None,
                NAPISTU_DATA_STORE.NAPISTU_GRAPH: None,
            }

        # Create initial registry
        registry = {
            NAPISTU_DATA_STORE.READ_ONLY: read_only,
            NAPISTU_DATA_STORE.NAPISTU_RAW: napistu_entry,
            NAPISTU_DATA_STORE.NAPISTU_DATA: {},
            NAPISTU_DATA_STORE.VERTEX_TENSORS: {},
            NAPISTU_DATA_STORE.PANDAS_DFS: {},
        }

        # Save registry
        logger.info(f"Saving registry to {registry_path}")
        with open(registry_path, "w") as f:
            json.dump(registry, f, indent=2)

        # Return new instance
        return cls(store_dir)

    def enable_artifact_creation(
        self,
        sbml_dfs_path: Union[str, Path],
        napistu_graph_path: Union[str, Path],
    ) -> None:
        """
        Convert a read-only store to non-read-only by enabling artifact creation.

        Updates the registry to set READ_ONLY to False and store paths to SBML_dfs
        and NapistuGraph files, allowing the store to create new artifacts.

        Parameters
        ----------
        sbml_dfs_path : Union[str, Path]
            Path to SBML_dfs pickle file
        napistu_graph_path : Union[str, Path]
            Path to NapistuGraph pickle file

        Raises
        ------
        ValueError
            If store is already non-read-only
        FileNotFoundError
            If the provided paths don't exist
        """
        # Validate that store is currently read-only
        if not self.read_only:
            logger.warning(
                "Store is already non-read-only. Cannot enable artifact creation on a store that already has artifact creation enabled."
            )
            return

        # Validate that files exist
        sbml_dfs_path = ensure_path(sbml_dfs_path)
        napistu_graph_path = ensure_path(napistu_graph_path)

        if not sbml_dfs_path.is_file():
            raise FileNotFoundError(f"SBML_dfs file not found: {sbml_dfs_path}")
        if not napistu_graph_path.is_file():
            raise FileNotFoundError(
                f"NapistuGraph file not found: {napistu_graph_path}"
            )

        # Update registry to convert from read-only to non-read-only
        logger.info(
            "Converting store from read-only to non-read-only by updating registry..."
        )

        # Resolve paths (handles both absolute and relative paths)
        resolved_sbml_path = _resolve_path(str(sbml_dfs_path), self.store_dir)
        resolved_ng_path = _resolve_path(str(napistu_graph_path), self.store_dir)

        # Update registry
        self.registry[NAPISTU_DATA_STORE.READ_ONLY] = False
        self.registry[NAPISTU_DATA_STORE.NAPISTU_RAW] = {
            NAPISTU_DATA_STORE.SBML_DFS: str(resolved_sbml_path),
            NAPISTU_DATA_STORE.NAPISTU_GRAPH: str(resolved_ng_path),
        }

        # Save updated registry
        self._save_registry()

        # Update instance attributes
        self.read_only = False
        self.sbml_dfs_path = resolved_sbml_path
        self.napistu_graph_path = resolved_ng_path

        logger.info("✓ Store converted to non-read-only")

    def ensure_artifacts(
        self,
        artifact_names: List[str],
        artifact_registry: Dict[str, ArtifactDefinition] = DEFAULT_ARTIFACT_REGISTRY,
        overwrite: bool = False,
    ) -> None:
        """
        Ensure specified artifacts exist in the store, creating if missing.

        This is the key method for efficient batch artifact creation.
        It loads raw data (sbml_dfs, ng) ONCE and creates all missing artifacts.

        Only checks registry for artifacts not already in store. This allows
        custom artifacts to exist in store without being in registry.

        Parameters
        ----------
        artifact_names : List[str]
            Names of artifacts to ensure exist
        artifact_registry : Dict[str, ArtifactDefinition], default=DEFAULT_ARTIFACT_REGISTRY
            Registry of artifact definitions
        overwrite : bool, default=False
            If True, recreate artifacts even if they exist

        Raises
        ------
        KeyError
            If artifact not in store and not in registry

        Examples
        --------
        >>> # Works with registry artifacts
        >>> store.ensure_artifacts(["unlabeled", "edge_prediction"])
        >>>
        >>> # Also works with custom artifacts already in store
        >>> custom_data = construct_custom_pyg_data(...)
        >>> store.save_napistu_data(custom_data, name="my_custom_artifact")
        >>> store.ensure_artifacts(["my_custom_artifact"])  # Just verifies existence
        """

        # validate artifact registry
        validate_artifact_registry(artifact_registry)

        # Determine which artifacts need creation
        if overwrite:
            to_create = artifact_names
        else:
            to_create = []
            for name in artifact_names:
                # Check if exists in store
                in_napistu_data = name in self.list_napistu_datas()
                in_vertex_tensors = name in self.list_vertex_tensors()
                in_pandas_dfs = name in self.list_pandas_dfs()

                if not (in_napistu_data or in_vertex_tensors or in_pandas_dfs):
                    to_create.append(name)

        if not to_create:
            logger.info("All requested artifacts already exist in store")
            return

        # Check if store is read_only (only if we need to create artifacts)
        if self.read_only:
            raise ValueError(
                "Cannot create artifacts: store is read_only. "
                "Read-only stores cannot create new artifacts that require SBML_dfs or NapistuGraph. "
                f"{len(to_create)} artifacts are unavailable: {to_create}"
            )

        if self.read_only:
            raise ValueError(
                "Cannot create artifacts: store is read_only. "
                "Read-only stores cannot create new artifacts that require SBML_dfs or NapistuGraph. "
                f"{len(to_create)} artifacts are unavailable: {to_create}"
            )

        logger.info(f"Need to create {len(to_create)} artifacts: {to_create}")

        # Validate all artifacts to create are in registry
        missing_from_registry = [
            name for name in to_create if name not in artifact_registry
        ]
        if missing_from_registry:
            available = sorted(artifact_registry.keys())
            raise KeyError(
                f"Cannot create artifacts not in registry: {missing_from_registry}. "
                f"Available in registry: {available}. "
                f"To use custom artifacts, save them to the store directly using "
                f"store.save_napistu_data() or store.save_vertex_tensor() or store.save_pandas_df()."
            )

        # Load raw data ONCE (expensive operation)
        logger.info("Loading SBML_dfs and NapistuGraph for artifact creation...")
        sbml_dfs = self.load_sbml_dfs()
        napistu_graph = self.load_napistu_graph()

        # Create all missing artifacts
        for name in to_create:
            logger.info(f"Creating artifact: {name}")

            artifact = create_artifact(
                name, sbml_dfs, napistu_graph, artifact_registry=artifact_registry
            )
            definition = artifact_registry[name]

            # Save based on type
            if definition.artifact_type == ARTIFACT_TYPES.NAPISTU_DATA:
                self.save_napistu_data(artifact, name=name, overwrite=overwrite)
            elif definition.artifact_type == ARTIFACT_TYPES.VERTEX_TENSOR:
                self.save_vertex_tensor(artifact, name=name, overwrite=overwrite)
            elif definition.artifact_type == ARTIFACT_TYPES.PANDAS_DFS:
                self.save_pandas_df(artifact, name=name, overwrite=overwrite)
            else:
                raise ValueError(f"Unknown artifact type: {definition.artifact_type}")

            # Free memory
            del artifact
            logger.info(f"Successfully created and saved artifact: {name}")

        # Free raw data
        del sbml_dfs, napistu_graph
        logger.info("Artifact creation complete")

    def get_missing_artifacts(
        self,
        artifact_names: List[str],
        artifact_registry: Dict[str, ArtifactDefinition] = DEFAULT_ARTIFACT_REGISTRY,
    ) -> List[str]:
        """
        Check which artifacts are missing from the store.

        Parameters
        ----------
        artifact_names : List[str]
            Names of artifacts to check
        artifact_registry : Dict[str, ArtifactDefinition], default=DEFAULT_ARTIFACT_REGISTRY
            Registry of artifact definitions

        Returns
        -------
        List[str]
            Names of artifacts that don't exist in store

        Raises
        ------
        KeyError
            If any artifact name is not in the registry

        Examples
        --------
        >>> missing = store.get_missing_artifacts([
        ...     "unlabeled",
        ...     "edge_prediction",
        ...     "custom_artifact"
        ... ])
        >>> print(missing)
        ['edge_prediction']
        """

        missing = []
        for name in artifact_names:
            if name not in artifact_registry:
                available = sorted(artifact_registry.keys())
                raise KeyError(
                    f"Unknown artifact: '{name}'. Available in registry: {available}"
                )

            definition = artifact_registry[name]
            exists = name in self.list_artifacts(definition.artifact_type)

            if not exists:
                missing.append(name)

        return missing

    @classmethod
    def from_config(
        cls,
        config: DataConfig,
        task_config: Optional[TaskConfig] = None,
        ensure_artifacts: bool = True,
    ) -> "NapistuDataStore":
        """
        Create or load a NapistuDataStore from a DataConfig.

        Flow:
        1. If store exists: load existing store
        2. If store doesn't exist:
           - If config.hf_repo_id is provided: load store from HuggingFace Hub
             * If sbml_dfs_path and napistu_graph_path are provided: convert to non-read-only
           - Otherwise: create new store locally (requires sbml_dfs_path and napistu_graph_path)
             * Creates regular store with provided paths
             * Copies to store if config.copy_to_store is True
        3. Ensure napistu_data_name, other_artifacts, and task artifacts exist (always, regardless of store creation)

        Parameters
        ----------
        config : DataConfig
            Configuration with store location, artifact paths, and requirements.
            If hf_repo_id is provided and store doesn't exist, the store will be loaded from HuggingFace Hub.
            Otherwise, both sbml_dfs_path and napistu_graph_path are required to create a new store.
        task_config : Optional[TaskConfig], default=None
            Optional task configuration. If provided, artifacts required by the task
            will be added to the required artifacts list.
        ensure_artifacts : bool, default=True
            Whether to ensure that napistu_data_name, other_artifacts, and task artifacts exist.
            Set to False when side-loading napistu_data directly (e.g., during testing).

        Returns
        -------
        NapistuDataStore
            Ready-to-use store with all required artifacts (if ensure_artifacts=True)

        Raises
        ------
        ValueError
            If store doesn't exist and neither hf_repo_id nor both paths are provided
            If hf_repo_id is provided but only one of sbml_dfs_path or napistu_graph_path is provided
        FileNotFoundError
            If sbml_dfs_path or napistu_graph_path don't exist when creating new store

        Examples
        --------
        >>> from napistu_torch.configs import DataConfig
        >>> from pathlib import Path
        >>>
        >>> # Create a regular store with paths
        >>> config = DataConfig(
        ...     store_dir=Path(".store"),
        ...     sbml_dfs_path=Path("/path/to/sbml_dfs.pkl"),
        ...     napistu_graph_path=Path("/path/to/napistu_graph.pkl"),
        ...     copy_to_store=True,
        ...     napistu_data_name="edge_prediction",
        ...     other_artifacts=["unlabeled"]
        ... )
        >>> store = NapistuDataStore.from_config(config)
        >>>
        >>> # Load store from HuggingFace Hub
        >>> hf_config = DataConfig(
        ...     store_dir=Path(".store"),
        ...     hf_repo_id="username/my-dataset",
        ...     hf_revision="v1.0",
        ...     napistu_data_name="edge_prediction"
        ... )
        >>> hf_store = NapistuDataStore.from_config(hf_config)
        >>>
        >>> # Load from HuggingFace and convert to non-read-only
        >>> hf_config_with_paths = DataConfig(
        ...     store_dir=Path(".store"),
        ...     hf_repo_id="username/my-dataset",
        ...     sbml_dfs_path=Path("/path/to/sbml_dfs.pkl"),
        ...     napistu_graph_path=Path("/path/to/napistu_graph.pkl"),
        ...     napistu_data_name="edge_prediction"
        ... )
        >>> hf_store = NapistuDataStore.from_config(hf_config_with_paths)
        """
        store_dir = config.store_dir
        registry_path = store_dir / NAPISTU_DATA_STORE_STRUCTURE.REGISTRY_FILE

        # Determine if we need to create or load the store
        if registry_path.is_file():
            # Store exists - load it
            logger.info(f"Loading existing store from {store_dir}")
            store = cls(store_dir)
        else:
            # Store doesn't exist
            # Check if we should load from HuggingFace Hub
            if config.hf_repo_id is not None:
                # Load store from HuggingFace Hub
                logger.info(
                    f"Loading store from HuggingFace Hub: {config.hf_repo_id} "
                    f"(revision: {config.hf_revision or 'main'})"
                )
                store = cls.from_huggingface(
                    repo_id=config.hf_repo_id,
                    store_dir=store_dir,
                    revision=config.hf_revision,
                    sbml_dfs_path=config.sbml_dfs_path,
                    napistu_graph_path=config.napistu_graph_path,
                )
            else:
                # Create new store locally - requires paths or hf_repo_id
                if config.sbml_dfs_path is None or config.napistu_graph_path is None:
                    raise ValueError(
                        "Cannot create a new store: both sbml_dfs_path and napistu_graph_path are required "
                        "when creating a local store. Alternatively, provide hf_repo_id to load from HuggingFace Hub."
                    )

                logger.info(f"Creating new store at {store_dir}")

                # Validate that paths exist
                if not config.sbml_dfs_path.is_file():
                    raise FileNotFoundError(
                        f"SBML_dfs file not found: {config.sbml_dfs_path}. "
                        "Please provide a valid path in config.sbml_dfs_path"
                    )
                if not config.napistu_graph_path.is_file():
                    raise FileNotFoundError(
                        f"NapistuGraph file not found: {config.napistu_graph_path}. "
                        "Please provide a valid path in config.napistu_graph_path"
                    )

                # Create store with paths
                store = cls.create(
                    store_dir=store_dir,
                    sbml_dfs_path=config.sbml_dfs_path,
                    napistu_graph_path=config.napistu_graph_path,
                    copy_to_store=config.copy_to_store,
                )

        # Conditionally ensure required artifacts exist
        if ensure_artifacts:
            required_artifacts = [config.napistu_data_name] + config.other_artifacts

            # Add additional artifacts required by the task
            if task_config is not None:
                from napistu_torch.configs import task_config_to_artifact_names

                task_artifacts = task_config_to_artifact_names(task_config)
                required_artifacts = list(set(required_artifacts) | set(task_artifacts))

            if required_artifacts:
                logger.info(f"Ensuring required artifacts exist: {required_artifacts}")
                store.ensure_artifacts(required_artifacts, overwrite=False)

        return store

    @classmethod
    def from_huggingface(
        cls,
        repo_id: str,
        store_dir: Union[str, Path],
        revision: Optional[str] = None,
        token: Optional[str] = None,
        sbml_dfs_path: Optional[Union[str, Path]] = None,
        napistu_graph_path: Optional[Union[str, Path]] = None,
    ) -> "NapistuDataStore":
        """
        Download and create a NapistuDataStore from HuggingFace Hub.

        This is a convenience method that uses HFDatasetLoader to download a store
        from HuggingFace Hub and create a local NapistuDataStore instance.

        If sbml_dfs_path and napistu_graph_path are provided, the store will be
        converted from read-only to non-read-only, allowing artifact creation.

        Parameters
        ----------
        repo_id : str
            HuggingFace repository in format "username/repo-name"
        store_dir : Union[str, Path]
            Local directory where the store will be created
        revision : Optional[str]
            Git revision (branch, tag, or commit hash). Defaults to "main"
        token : Optional[str]
            HuggingFace access token for private repositories
        sbml_dfs_path : Optional[Union[str, Path]]
            Path to SBML_dfs pickle file. If provided along with napistu_graph_path,
            converts the store from read-only to non-read-only.
        napistu_graph_path : Optional[Union[str, Path]]
            Path to NapistuGraph pickle file. If provided along with sbml_dfs_path,
            converts the store from read-only to non-read-only.

        Returns
        -------
        NapistuDataStore
            Loaded store ready to use

        Raises
        ------
        ValueError
            If only one of sbml_dfs_path or napistu_graph_path is provided
        FileNotFoundError
            If the provided paths don't exist

        Examples
        --------
        >>> from napistu_torch.napistu_data_store import NapistuDataStore
        >>> from pathlib import Path
        >>>
        >>> # Load read-only store from HuggingFace Hub
        >>> store = NapistuDataStore.from_huggingface(
        ...     repo_id="username/my-dataset",
        ...     store_dir=Path("./local_store")
        ... )
        >>>
        >>> # Load and convert to non-read-only store
        >>> store = NapistuDataStore.from_huggingface(
        ...     repo_id="username/my-dataset",
        ...     store_dir=Path("./local_store"),
        ...     sbml_dfs_path=Path("/data/sbml_dfs.pkl"),
        ...     napistu_graph_path=Path("/data/graph.pkl")
        ... )
        >>>
        >>> # Use the store
        >>> napistu_data = store.load_napistu_data("edge_prediction")
        """
        from napistu_torch.ml.hugging_face import HFDatasetLoader

        loader = HFDatasetLoader(
            repo_id=repo_id,
            store_dir=store_dir,
            revision=revision,
            token=token,
        )
        store_dir_path = loader.load_store()

        # Load the store first
        store = cls(store_dir_path)

        # Convert to non-read-only if paths are provided
        if sbml_dfs_path is not None or napistu_graph_path is not None:
            # Validate that both paths are provided
            if sbml_dfs_path is None or napistu_graph_path is None:
                raise ValueError(
                    "Both sbml_dfs_path and napistu_graph_path must be provided together "
                    "to convert the store from read-only to non-read-only. "
                    "Cannot provide only one."
                )
            store.enable_artifact_creation(sbml_dfs_path, napistu_graph_path)

        return store

    def list_artifacts(self, artifact_type: Optional[str] = None) -> list[str]:
        """
        List all artifact names in the store.

        Parameters
        ----------
        artifact_type : Optional[str], default=None
            Type of artifact to list. If not provided, all artifact types will be listed.

        Returns
        -------
        list[str]
            List of artifact names in the store
        """

        napistu_datas = self.list_napistu_datas()
        vertex_tensors = self.list_vertex_tensors()
        pandas_dfs = self.list_pandas_dfs()

        if artifact_type is None:
            # flag duplicates across 2 or more sets
            self._validate_no_duplicate_names(raise_error=False)
            return set(napistu_datas + vertex_tensors + pandas_dfs)
        else:
            if artifact_type == ARTIFACT_TYPES.NAPISTU_DATA:
                return napistu_datas
            elif artifact_type == ARTIFACT_TYPES.VERTEX_TENSOR:
                return vertex_tensors
            elif artifact_type == ARTIFACT_TYPES.PANDAS_DFS:
                return pandas_dfs
            else:
                raise ValueError(f"Invalid artifact type: {artifact_type}")

    def list_napistu_datas(self) -> list[str]:
        """
        List all NapistuData names in the store.

        Returns
        -------
        list[str]
        List of NapistuData names in the store
        """
        return list(self.registry[NAPISTU_DATA_STORE.NAPISTU_DATA].keys())

    def list_vertex_tensors(self) -> list[str]:
        """
        List all VertexTensor names in the store.

        Returns
        -------
        list[str]
            List of VertexTensor names in the store
        """
        return list(self.registry[NAPISTU_DATA_STORE.VERTEX_TENSORS].keys())

    def list_pandas_dfs(self) -> list[str]:
        """
        List all pandas DataFrame names in the store.

        Returns
        -------
        list[str]
            List of pandas DataFrame names in the store
        """
        return list(self.registry[NAPISTU_DATA_STORE.PANDAS_DFS].keys())

    def load_artifact(
        self, name: str, artifact_type: str
    ) -> Union[NapistuData, VertexTensor, pd.DataFrame]:
        """
        Load an artifact from the store.

        Parameters
        ----------
        name : str
            Name of the artifact to load
        artifact_type : str
            Type of the artifact to load

        Returns
        -------
        Union[NapistuData, VertexTensor, pd.DataFrame]
            The loaded artifact

        Raises
        ------
        ValueError
            If invalid artifact type
        """
        if artifact_type == ARTIFACT_TYPES.NAPISTU_DATA:
            return self.load_napistu_data(name)
        elif artifact_type == ARTIFACT_TYPES.VERTEX_TENSOR:
            return self.load_vertex_tensor(name)
        elif artifact_type == ARTIFACT_TYPES.PANDAS_DFS:
            return self.load_pandas_df(name)
        else:
            raise ValueError(f"Invalid artifact type: {artifact_type}")

    def load_napistu_data(
        self, name: str, map_location: str = DEVICE.CPU
    ) -> NapistuData:
        """
        Load a NapistuData object from the store.

        Parameters
        ----------
        name : str
            Name of the NapistuData to load
        map_location : str, default="cpu"
            Device to map tensors to

        Returns
        -------
        NapistuData
            The loaded NapistuData object

        Raises
        ------
        KeyError
            If name not found in registry
        FileNotFoundError
            If the .pt file doesn't exist
        """
        # Check if name exists in registry
        if name not in self.registry[NAPISTU_DATA_STORE.NAPISTU_DATA]:
            raise KeyError(
                f"NapistuData '{name}' not found in registry. "
                f"Available: {list(self.registry[NAPISTU_DATA_STORE.NAPISTU_DATA].keys())}"
            )

        # Get filename from registry
        entry = self.registry[NAPISTU_DATA_STORE.NAPISTU_DATA][name]
        filename = entry[NAPISTU_DATA_STORE.FILENAME]
        filepath = self.store_dir / NAPISTU_DATA_STORE_STRUCTURE.NAPISTU_DATA / filename

        # Load and return
        logger.info(f"Loading NapistuData from {filepath}")
        return NapistuData.load(filepath, map_location=map_location)

    def load_napistu_graph(self) -> NapistuGraph:
        """Load the NapistuGraph from disk."""
        if self.read_only:
            raise ValueError(
                "Cannot load NapistuGraph: store is read_only and "
                "no napistu_graph_path was provided."
            )
        if self.napistu_graph_path.is_file():
            return NapistuGraph.from_pickle(self.napistu_graph_path)
        else:
            raise FileNotFoundError(
                f"NapistuGraph file not found: {self.napistu_graph_path}"
            )

    def load_sbml_dfs(self) -> SBML_dfs:
        """Load the SBML_dfs from disk."""
        if self.read_only:
            raise ValueError(
                "Cannot load SBML_dfs: store is read_only and "
                "no sbml_dfs_path was provided."
            )
        if self.sbml_dfs_path.is_file():
            return SBML_dfs.from_pickle(self.sbml_dfs_path)
        else:
            raise FileNotFoundError(f"SBML_dfs file not found: {self.sbml_dfs_path}")

    def load_vertex_tensor(
        self, name: str, map_location: str = DEVICE.CPU
    ) -> VertexTensor:
        """
        Load a VertexTensor from the store.

        Parameters
        ----------
        name : str
            Name of the VertexTensor to load
        map_location : str, default=DEVICE.CPU
            Device to map tensors to

        Returns
        -------
        VertexTensor
            The loaded VertexTensor object

        Raises
        ------
        KeyError
            If name not found in registry
        FileNotFoundError
            If the .pt file doesn't exist
        """

        # Check if name exists in registry
        if name not in self.registry[NAPISTU_DATA_STORE.VERTEX_TENSORS]:
            raise KeyError(
                f"VertexTensor '{name}' not found in registry. "
                f"Available: {list(self.registry[NAPISTU_DATA_STORE.VERTEX_TENSORS].keys())}"
            )

        # Get filename from registry
        entry = self.registry[NAPISTU_DATA_STORE.VERTEX_TENSORS][name]
        filename = entry[NAPISTU_DATA_STORE.FILENAME]
        filepath = (
            self.store_dir / NAPISTU_DATA_STORE_STRUCTURE.VERTEX_TENSORS / filename
        )

        # Load and return
        logger.info(f"Loading VertexTensor from {filepath}")
        return VertexTensor.load(filepath, map_location=map_location)

    def load_pandas_df(self, name: str) -> pd.DataFrame:
        """
        Load a pandas DataFrame from the store.

        Parameters
        ----------
        name : str
            Name of the pandas DataFrame to load

        Returns
        -------
        pd.DataFrame
            The loaded pandas DataFrame

        Raises
        ------
        KeyError
            If name not found in registry
        FileNotFoundError
            If the .parquet file doesn't exist
        """
        # Check if name exists in registry
        if name not in self.registry[NAPISTU_DATA_STORE.PANDAS_DFS]:
            raise KeyError(
                f"pandas DataFrame '{name}' not found in registry. "
                f"Available: {list(self.registry[NAPISTU_DATA_STORE.PANDAS_DFS].keys())}"
            )

        # Get filename from registry
        entry = self.registry[NAPISTU_DATA_STORE.PANDAS_DFS][name]
        filename = entry[NAPISTU_DATA_STORE.FILENAME]
        filepath = self.store_dir / NAPISTU_DATA_STORE_STRUCTURE.PANDAS_DFS / filename

        # Load and return
        logger.info(f"Loading pandas DataFrame from {filepath}")
        return pd.read_parquet(filepath)

    def publish_store_to_huggingface(
        self,
        repo_id: str,
        revision: Optional[str] = None,
        overwrite: bool = False,
        commit_message: Optional[str] = None,
        token: Optional[str] = None,
        asset_name: Optional[str] = None,
        asset_version: Optional[str] = None,
        tag: Optional[str] = None,
        tag_message: Optional[str] = None,
    ) -> str:
        """
        Publish this entire store to HuggingFace Hub.

        This is a convenience method that uses HFDatasetPublisher to publish
        all artifacts from this store to HuggingFace Hub as a dataset repository.
        The published store will be read-only (sbml_dfs_path and napistu_graph_path
        set to None).

        Parameters
        ----------
        repo_id : str
            Repository ID in format "username/repo-name"
        revision : Optional[str]
            Git revision (branch, tag, or commit hash). Defaults to "main"
        overwrite : bool
            Explicitly confirm overwriting existing dataset (default: False)
        commit_message : Optional[str]
            Custom commit message (default: auto-generated)
        token : Optional[str]
            HuggingFace API token (default: uses `huggingface-cli login` token)
        asset_name : Optional[str]
            Name of the GCS asset used to create the store (for documentation)
        asset_version : Optional[str]
            Version of the GCS asset used to create the store (for documentation)
        tag : Optional[str]
            Tag name to create after all assets are uploaded (e.g., "v1.0")
        tag_message : Optional[str]
            Optional message for the tag

        Returns
        -------
        str
            URL to the published dataset on HuggingFace Hub

        Examples
        --------
        >>> store = NapistuDataStore("path/to/store")
        >>> url = store.publish_store_to_huggingface(
        ...     repo_id="username/my-dataset",
        ...     asset_name="human_consensus",
        ...     asset_version="v1.0"
        ... )
        """
        from napistu_torch.ml.hugging_face import HFDatasetPublisher

        publisher = HFDatasetPublisher(token=token)
        return publisher.publish_store(
            repo_id=repo_id,
            store=self,
            revision=revision,
            overwrite=overwrite,
            commit_message=commit_message,
            asset_name=asset_name,
            asset_version=asset_version,
            tag=tag,
            tag_message=tag_message,
        )

    def save_napistu_data(
        self,
        napistu_data: NapistuData,
        name: Optional[str] = None,
        overwrite: bool = False,
    ) -> None:
        """
        Save a NapistuData object to the store.

        Parameters
        ----------
        napistu_data : NapistuData
            The NapistuData object to save. The method will extract the
            splitting_strategy and labeling_manager from the object's attributes.
        name : str, optional
            Name to use for the registry entry and filename. If not provided,
            uses the napistu_data.name attribute.
        overwrite : bool, default=False
            If True, overwrite existing entry with same name
            If False, raise FileExistsError if name already exists

        Raises
        ------
        FileExistsError
            If name already exists in registry and overwrite=False
        ValueError
            If the splitting_strategy from the NapistuData object is invalid
        """
        # Use provided name or fall back to object's name
        if name is None:
            name = napistu_data.name
        splitting_strategy = napistu_data.splitting_strategy
        labeling_manager = getattr(napistu_data, NAPISTU_DATA.LABELING_MANAGER, None)

        # Check if name already exists
        if name in self.registry[NAPISTU_DATA_STORE.NAPISTU_DATA] and not overwrite:
            raise FileExistsError(
                f"NapistuData '{name}' already exists in registry. "
                f"Use overwrite=True to replace it."
            )
        if (
            splitting_strategy is not None
            and splitting_strategy not in VALID_SPLITTING_STRATEGIES
        ):
            raise ValueError(
                f"Invalid splitting strategy: {splitting_strategy}. Must be one of {VALID_SPLITTING_STRATEGIES}"
            )

        # Save the NapistuData object
        napistu_data_dir = self.store_dir / NAPISTU_DATA_STORE_STRUCTURE.NAPISTU_DATA
        filename = NAPISTU_DATA_STORE.PT_TEMPLATE.format(name=name)
        filepath = napistu_data_dir / filename

        logger.info(f"Saving NapistuData to {filepath}")
        napistu_data.save(filepath)

        # Create registry entry
        entry = {
            NAPISTU_DATA_STORE.FILENAME: filename,
            NAPISTU_DATA_STORE.CREATED: datetime.now().isoformat(),
            NAPISTU_DATA.NAME: name,
            NAPISTU_DATA.LABELING_MANAGER: (
                labeling_manager.to_dict() if labeling_manager is not None else None
            ),
            NAPISTU_DATA.SPLITTING_STRATEGY: splitting_strategy,
        }

        # Update registry
        self.registry[NAPISTU_DATA_STORE.NAPISTU_DATA][name] = entry
        self._save_registry()

    def save_vertex_tensor(
        self,
        vertex_tensor: VertexTensor,
        name: Optional[str] = None,
        overwrite: bool = False,
    ) -> None:
        """
        Save a VertexTensor to the store.

        Parameters
        ----------
        vertex_tensor : VertexTensor
            The VertexTensor object to save
        name : str, optional
            Name for storage (registry key and filename stem). If not provided,
            uses the vertex_tensor.name attribute.
        overwrite : bool, default=False
            If True, overwrite existing entry with same name
            If False, raise FileExistsError if name already exists

        Raises
        ------
        FileExistsError
            If name already exists in registry and overwrite=False
        """
        # Use provided name or fall back to object's name
        if name is None:
            name = vertex_tensor.name

        # Check if name already exists
        if name in self.registry[NAPISTU_DATA_STORE.VERTEX_TENSORS] and not overwrite:
            raise FileExistsError(
                f"VertexTensor '{name}' already exists in registry. "
                f"Use overwrite=True to replace it."
            )

        # Save the VertexTensor object
        vertex_tensors_dir = (
            self.store_dir / NAPISTU_DATA_STORE_STRUCTURE.VERTEX_TENSORS
        )
        vertex_tensors_dir.mkdir(exist_ok=True)
        filename = NAPISTU_DATA_STORE.PT_TEMPLATE.format(name=name)
        filepath = vertex_tensors_dir / filename

        logger.info(f"Saving VertexTensor to {filepath}")
        vertex_tensor.save(filepath)

        # Create registry entry
        entry = {
            NAPISTU_DATA_STORE.FILENAME: filename,
            NAPISTU_DATA_STORE.CREATED: datetime.now().isoformat(),
            VERTEX_TENSOR.NAME: vertex_tensor.name,
            VERTEX_TENSOR.DESCRIPTION: vertex_tensor.description,
        }

        # Update registry
        self.registry[NAPISTU_DATA_STORE.VERTEX_TENSORS][name] = entry
        self._save_registry()

    def save_pandas_df(
        self,
        df: pd.DataFrame,
        name: str,
        overwrite: bool = False,
    ) -> None:
        """
        Save a pandas DataFrame to the store.

        Parameters
        ----------
        dataframe : pd.DataFrame
            The pandas DataFrame to save
        name : str
            Name for storage (registry key and filename stem).
        overwrite : bool, default=False
            If True, overwrite existing entry with same name
            If False, raise FileExistsError if name already exists

        Raises
        ------
        FileExistsError
            If name already exists in registry and overwrite=False
        """
        # Check if name already exists
        if name in self.registry[NAPISTU_DATA_STORE.PANDAS_DFS] and not overwrite:
            raise FileExistsError(
                f"pandas DataFrame '{name}' already exists in registry. "
                f"Use overwrite=True to replace it."
            )

        # Save the pandas DataFrame
        pandas_dfs_dir = self.store_dir / NAPISTU_DATA_STORE_STRUCTURE.PANDAS_DFS
        pandas_dfs_dir.mkdir(exist_ok=True)
        filename = NAPISTU_DATA_STORE.PARQUET_TEMPLATE.format(name=name)
        filepath = pandas_dfs_dir / filename

        logger.info(f"Saving pandas DataFrame to {filepath}")
        df.to_parquet(filepath)

        # Create registry entry
        entry = {
            NAPISTU_DATA_STORE.FILENAME: filename,
            NAPISTU_DATA_STORE.CREATED: datetime.now().isoformat(),
            "shape": df.shape,
            "columns": list(df.columns),
        }

        # Update registry
        self.registry[NAPISTU_DATA_STORE.PANDAS_DFS][name] = entry
        self._save_registry()

    def summary(self) -> dict:
        """
        Get a summary of the store contents.

        Returns
        -------
        dict
            Dictionary containing summary information about the store
        """
        return {
            "store_dir": str(self.store_dir),
            "napistu_data_count": len(self.registry[NAPISTU_DATA_STORE.NAPISTU_DATA]),
            "vertex_tensors_count": len(
                self.registry[NAPISTU_DATA_STORE.VERTEX_TENSORS]
            ),
            "pandas_dfs_count": len(self.registry[NAPISTU_DATA_STORE.PANDAS_DFS]),
            "napistu_data_names": self.list_napistu_datas(),
            "vertex_tensor_names": self.list_vertex_tensors(),
            "pandas_df_names": self.list_pandas_dfs(),
            "last_modified": self.registry.get(NAPISTU_DATA_STORE.LAST_MODIFIED),
        }

    def validate(self) -> None:
        """
        Validate the store contents.
        """
        self._validate_no_duplicate_names(raise_error=False)

    def validate_artifact_name(
        self,
        name: str,
        artifact_registry: Dict[str, ArtifactDefinition] = DEFAULT_ARTIFACT_REGISTRY,
        required_type: Optional[str] = None,
    ) -> None:
        """
        Validate an artifact name by ensuring that it is either already in the store or available from the registry

        Parameters
        ----------
        name : str
            Name of artifact to validate
        artifact_registry : Dict[str, ArtifactDefinition], default=DEFAULT_ARTIFACT_REGISTRY
            Registry of artifact definitions
        required_type : Optional[str], default=None
            Type of artifact that is required. If not provided, any type is allowed.

        Raises
        ------
        KeyError
            If artifact is not in store and not in registry
        """

        if required_type is not None:
            if required_type not in VALID_ARTIFACT_TYPES:
                raise ValueError(
                    f"Invalid 'required_type' value ({required_type}). This must be one of: {VALID_ARTIFACT_TYPES}"
                )

            existing_w_valid_type = self.list_artifacts(required_type)
            if name in existing_w_valid_type:
                # existing name of the correct type
                return None

        # Check if already in store
        in_store = name in self.list_artifacts()
        if in_store:
            if required_type is not None:
                raise KeyError(
                    f"Artifact '{name}' already exists in store but is not of type {required_type}"
                )
            else:
                return None

        # Not in store - check if we can create it
        in_registry = name in artifact_registry

        if not in_registry:
            # Not in store and not in registry
            available_in_store = self.list_artifacts(required_type)
            available_in_registry = [
                name
                for name, defn in artifact_registry.items()
                if defn.artifact_type == required_type
            ]
            required_type_str = (
                required_type if required_type is not None else "any type"
            )

            raise ValueError(
                f"An artifact of type {required_type_str} named '{name}' was not found.\n"
                f"Available in store: {available_in_store}\n"
                f"Available from registry: {available_in_registry}\n"
                f"To add a custom artifact, save it to the store first using "
                f"store.save_napistu_data(), store.save_vertex_tensor(), or store.save_pandas_df()."
            )

        # In registry - check it's the right type
        artifact_def = artifact_registry[name]
        if required_type is not None and artifact_def.artifact_type != required_type:
            raise KeyError(
                f"Artifact '{name}' is a {artifact_def.artifact_type}, not a {required_type}"
            )

    def _load_registry(self) -> dict:
        """Load the registry from disk."""
        with open(self.registry_path, "r") as f:
            return json.load(f)

    def _save_registry(self) -> None:
        """Save the registry to disk."""
        self.registry[NAPISTU_DATA_STORE.LAST_MODIFIED] = datetime.now().isoformat()

        with open(self.registry_path, "w") as f:
            json.dump(self.registry, f, indent=2)

    def _validate_no_duplicate_names(self, raise_error: bool = True) -> None:
        """Check for duplicate names across artifact types and warn if found."""
        name_to_types = defaultdict(list)

        for name in self.list_napistu_datas():
            name_to_types[name].append(ARTIFACT_TYPES.NAPISTU_DATA)
        for name in self.list_vertex_tensors():
            name_to_types[name].append(ARTIFACT_TYPES.VERTEX_TENSOR)
        for name in self.list_pandas_dfs():
            name_to_types[name].append(ARTIFACT_TYPES.PANDAS_DFS)

        duplicates = {
            name: types for name, types in name_to_types.items() if len(types) > 1
        }

        if duplicates:
            msg = "Duplicate artifact names found:\n" + "\n".join(
                f"  - '{name}' in {types}" for name, types in sorted(duplicates.items())
            )

            if raise_error:
                raise ValueError(msg)
            else:
                logger.warning(msg)


# private functions


def _validate_create_inputs(
    registry_path: Path,
    sbml_dfs_path: Path,
    napistu_graph_path: Path,
) -> None:
    """
    Validate inputs for creating a new NapistuDataStore.

    Parameters
    ----------
    registry_path : Path
        Path where the registry file should be created
    sbml_dfs_path : Union[str, Path]
        Path to the SBML_dfs pickle file
    napistu_graph_path : Union[str, Path]
        Path to the NapistuGraph pickle file

    Raises
    ------
    FileExistsError
        If a registry already exists at registry_path
    FileNotFoundError
        If the specified napistu files don't exist
    """
    # Check if registry already exists
    if registry_path.exists():
        raise FileExistsError(
            f"Registry already exists at {registry_path}. "
            f"Use NapistuDataStore(store_dir) to load it."
        )

    if sbml_dfs_path is None:
        raise FileNotFoundError("sbml_dfs_path is required but was not provided")
    if napistu_graph_path is None:
        raise FileNotFoundError("napistu_graph_path is required but was not provided")

    if not sbml_dfs_path.is_file():
        raise FileNotFoundError(f"SBML_dfs file not found: {sbml_dfs_path}")
    if not napistu_graph_path.is_file():
        raise FileNotFoundError(f"NapistuGraph file not found: {napistu_graph_path}")


def _resolve_path(path_str: str, store_dir: Path) -> Path:
    """
    Resolve a path string to a normalized absolute Path.

    If the path starts with '/', it's treated as an absolute path.
    Otherwise, it's treated as relative to store_dir.
    All paths are normalized to resolve .. components and symbolic links.

    Parameters
    ----------
    path_str : str
        Path string from registry (either absolute or relative)
    store_dir : Path
        Store directory to resolve relative paths against

    Returns
    -------
    Path
        Resolved and normalized absolute path
    """
    path = Path(path_str)

    if path.is_absolute():
        return path.resolve()
    else:
        # paths are relative to store_dir
        return (store_dir / path).resolve()
