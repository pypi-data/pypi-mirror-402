"""
NapistuData - A PyTorch Geometric Data subclass for Napistu networks.

This module provides a PyTorch Geometric Data subclass with Napistu-specific
functionality including safe save/load methods and additional utilities.

Classes
-------
NapistuData
    A PyTorch Geometric Data subclass for Napistu biological networks.
"""

import copy
import logging
import re
from pathlib import Path
from typing import Any, Dict, List, Optional, Set, Tuple, Union

import pandas as pd
import torch
from napistu.network.constants import (
    NAPISTU_GRAPH,
    NAPISTU_GRAPH_EDGES,
    NAPISTU_GRAPH_VERTICES,
)
from napistu.network.ng_core import NapistuGraph
from napistu.utils import show
from torch_geometric.data import Data

from napistu_torch.constants import (
    NAPISTU_DATA,
    NAPISTU_DATA_DEFAULT_NAME,
    NAPISTU_DATA_SUMMARIES,
    NAPISTU_DATA_SUMMARY_TYPES,
    PYG,
    VALID_NAPISTU_DATA_SUMMARY_TYPES,
)
from napistu_torch.labels.apply import decode_labels
from napistu_torch.labels.labeling_manager import LabelingManager
from napistu_torch.load.constants import (
    EDGE_DEFAULT_TRANSFORMS,
    ENCODING_MANAGER,
    MERGE_RARE_STRATA_DEFS,
    VALID_SPLITTING_STRATEGIES,
    VERTEX_DEFAULT_TRANSFORMS,
)
from napistu_torch.load.encoders import DEFAULT_ENCODERS
from napistu_torch.load.encoding import (
    expand_deduplicated_features,
    fit_encoders,
    transform_dataframe,
)
from napistu_torch.load.encoding_manager import EncodingManager
from napistu_torch.ml.constants import DEVICE
from napistu_torch.utils.base_utils import ensure_path
from napistu_torch.utils.nd_utils import (
    add_optional_attr,
    compute_mask_hashes,
    format_summary,
)

logger = logging.getLogger(__name__)


class NapistuData(Data):
    """
    A PyTorch Geometric Data subclass for Napistu biological networks.

    This class extends PyG's Data class with Napistu-specific functionality
    including safe save/load methods and additional utilities for working
    with biological network data.

    Parameters
    ----------
    x : torch.Tensor
        Node feature matrix with shape [num_nodes, num_node_features]
    edge_index : torch.Tensor
        Graph connectivity in COO format with shape [2, num_edges]
    edge_attr : torch.Tensor
        Edge feature matrix with shape [num_edges, num_edge_features]
    name: str = NAPISTU_DATA_DEFAULT_NAME,
        Name of the NapistuData object. Used for summaries and for organizing objects in the NapistuDataStore.
    edge_weight : torch.Tensor, optional
        Edge weights tensor with shape [num_edges]
    y : torch.Tensor, optional
        Node labels tensor with shape [num_nodes] for supervised learning tasks
    vertex_feature_names : List[str], optional
        Names of vertex features for interpretability
    edge_feature_names : List[str], optional
        Names of edge features for interpretability
    vertex_feature_name_aliases : Dict[str, str], optional
        Mapping from vertex feature names to their canonical names (for deduplicated features)
    edge_feature_name_aliases : Dict[str, str], optional
        Mapping from edge feature names to their canonical names (for deduplicated features)
    ng_vertex_names : pd.Series, optional
        Minimal vertex names from the original NapistuGraph. Series aligned with
        the vertex tensor (x) - each element corresponds to a vertex in the same
        order as the tensor rows. Used for debugging and validation of tensor alignment.
    ng_edge_names : pd.DataFrame, optional
        Minimal edge names from the original NapistuGraph. DataFrame with 'from' and 'to'
        columns aligned with the edge tensor (edge_index, edge_attr) - each row corresponds
        to an edge in the same order as the tensor columns. Used for debugging and validation.
    splitting_strategy: Optional[str] = None,
        Strategy used to split the data into train/test/val sets. This occurs upstream but the approach is tracked as a reference here.
    labeling_manager: Optional[LabelingManager] = None,
        Labeling manager used to encode the labels. This is used to decode the labels back to the original values for validation purposes.
    **kwargs
        Additional attributes to store in the data object

    Public Methods
    --------------
    copy()
        Create a deep copy of the NapistuData object
    estimate_memory_footprint()
        Estimate memory footprint of the NapistuData object
    get_edge_feature_names()
        Get the names of edge features
    get_edge_indices(df, from_col, to_col)
        Get edge index tensor from a DataFrame with vertex names
    get_edge_names()
        Get the edge names from the original NapistuGraph
    get_edge_weights()
        Get edge weights as a 1D tensor
    get_feature_by_name(feature_name)
        Get a feature by name from the NapistuData object
    get_summary(summary_type="basic")
        Get a summary of the NapistuData object
    get_symmetrical_relation_indices()
        Get the indices of symmetric relation types
    get_vertex_feature_names()
        Get the names of vertex features
    get_vertex_indices(vertex_names)
        Get the indices of vertices by their names
    get_vertex_names()
        Get the vertex names from the original NapistuGraph
    has_edges(edge_indices)
        Check which edges in edge_indices are present in this NapistuData
    load(filepath, map_location="cpu")
        Load a NapistuData object from disk
    save(filepath)
        Save the NapistuData object to disk
    show_memory_footprint()
        Display memory footprint of the NapistuData object
    show_summary()
        Display a summary of the NapistuData object
    trim(keep_edge_attr=True, keep_labels=True, keep_masks=True, inplace=False)
        Trim the NapistuData object to keep only the specified attributes
    unencode_features(napistu_graph, attribute_type, attribute, encoding_manager=None)
        Unencode features from the NapistuData object

    Private Methods
    ---------------
    _validate_edge_encoding(napistu_graph, edge_attribute, encoding_manager=None)
        Validate the edge encoding of the NapistuData object
    _validate_labels(napistu_graph, labeling_manager)
        Validate the labels of the NapistuData object
    _validate_vertex_encoding(napistu_graph, vertex_attribute, encoding_manager=None)
        Validate the vertex encoding of the NapistuData object


    Examples
    --------
    >>> # Create a NapistuData object (x, edge_index, and edge_attr are required)
    >>> data = NapistuData(
    ...     x=torch.randn(100, 10),                    # Required: node features
    ...     edge_index=torch.randint(0, 100, (2, 200)), # Required: graph connectivity
    ...     edge_attr=torch.randn(200, 5),             # Required: edge features
    ...     y=torch.randint(0, 3, (100,)),             # Optional: node labels
    ...     vertex_feature_names=['feature_1', 'feature_2', ...],  # Optional
    ...     edge_feature_names=['weight', 'direction', ...],       # Optional
    ...     ng_vertex_names=vertex_names_series,        # Optional: minimal vertex names
    ...     ng_edge_names=edge_names_df,                # Optional: minimal edge names
    ... )
    >>>
    >>> # Save and load
    >>> data.save('my_network.pt')
    >>> loaded_data = NapistuData.load('my_network.pt')
    """

    def __init__(
        self,
        x: torch.Tensor,
        edge_index: torch.Tensor,
        edge_attr: torch.Tensor,
        name: str = NAPISTU_DATA_DEFAULT_NAME,
        edge_weight: Optional[torch.Tensor] = None,
        y: Optional[torch.Tensor] = None,
        vertex_feature_names: Optional[List[str]] = None,
        edge_feature_names: Optional[List[str]] = None,
        vertex_feature_name_aliases: Optional[Dict[str, str]] = None,
        edge_feature_name_aliases: Optional[Dict[str, str]] = None,
        ng_vertex_names: Optional[pd.Series] = None,
        ng_edge_names: Optional[pd.DataFrame] = None,
        splitting_strategy: Optional[str] = None,
        labeling_manager: Optional[LabelingManager] = None,
        relation_type: Optional[torch.Tensor] = None,
        relation_manager: Optional[LabelingManager] = None,
        **kwargs,
    ):
        # Validate required parameters
        _validate_required_nd_args(
            x=x, edge_index=edge_index, edge_attr=edge_attr, name=name
        )

        # Build parameters dict, only including non-None values
        params = {
            PYG.X: x,
            PYG.EDGE_INDEX: edge_index,
            PYG.EDGE_ATTR: edge_attr,
            PYG.EDGE_WEIGHT: edge_weight,
            NAPISTU_DATA.NAME: name,
        }

        # Add optional parameters if they are not None
        _apply_optional_nd_args(
            params=params,
            x=x,
            edge_attr=edge_attr,
            y=y,
            vertex_feature_names=vertex_feature_names,
            edge_feature_names=edge_feature_names,
            vertex_feature_name_aliases=vertex_feature_name_aliases,
            edge_feature_name_aliases=edge_feature_name_aliases,
            ng_vertex_names=ng_vertex_names,
            ng_edge_names=ng_edge_names,
            splitting_strategy=splitting_strategy,
            labeling_manager=labeling_manager,
            relation_type=relation_type,
            relation_manager=relation_manager,
        )

        # Add any non-None kwargs
        params.update({k: v for k, v in kwargs.items() if v is not None})

        super().__init__(**params)

    def copy(self) -> "NapistuData":
        """
        Create a deep copy of the NapistuData object.
        """
        return copy.deepcopy(self)

    def estimate_memory_footprint(self) -> Dict[str, Optional[int]]:
        """
        Estimate memory footprint of the NapistuData object.

        Calculates the memory usage (in bytes) for each major component
        of the data object, including node features, edge index, edge
        attributes, and training/validation/test masks.

        Returns
        -------
        Dict[str, Optional[int]]
            Dictionary containing memory usage in bytes for each component:
            - "node_features": Memory used by node features (x)
            - "edge_index": Memory used by edge index
            - "edge_attr": Memory used by edge attributes
            - "train_mask": Memory used by train mask (None if not present)
            - "val_mask": Memory used by validation mask (None if not present)
            - "test_mask": Memory used by test mask (None if not present)
            - "total": Total memory usage in bytes

        Examples
        --------
        >>> footprint = data.estimate_memory_footprint()
        >>> print(f"Total memory: {footprint['total'] / 1e9:.2f} GB")
        >>> print(f"Node features: {footprint['node_features'] / 1e9:.2f} GB")
        """
        memory_dict: Dict[str, Optional[int]] = {
            PYG.X: None,
            PYG.EDGE_INDEX: None,
            PYG.EDGE_ATTR: None,
            NAPISTU_DATA.TRAIN_MASK: None,
            NAPISTU_DATA.VAL_MASK: None,
            NAPISTU_DATA.TEST_MASK: None,
            "total": 0,
        }

        total_bytes = 0

        # Node features
        if hasattr(self, PYG.X) and self.x is not None:
            node_bytes = self.x.element_size() * self.x.nelement()
            memory_dict[PYG.X] = node_bytes
            total_bytes += node_bytes

        # Edge index
        if hasattr(self, PYG.EDGE_INDEX) and self.edge_index is not None:
            edge_index_bytes = (
                self.edge_index.element_size() * self.edge_index.nelement()
            )
            memory_dict[PYG.EDGE_INDEX] = edge_index_bytes
            total_bytes += edge_index_bytes

        # Edge attributes
        if hasattr(self, PYG.EDGE_ATTR) and self.edge_attr is not None:
            edge_attr_bytes = self.edge_attr.element_size() * self.edge_attr.nelement()
            memory_dict[PYG.EDGE_ATTR] = edge_attr_bytes
            total_bytes += edge_attr_bytes

        # Masks
        for mask_name in [
            NAPISTU_DATA.TRAIN_MASK,
            NAPISTU_DATA.VAL_MASK,
            NAPISTU_DATA.TEST_MASK,
        ]:
            if hasattr(self, mask_name):
                mask = getattr(self, mask_name)
                if mask is not None:
                    mask_bytes = mask.element_size() * mask.nelement()
                    memory_dict[mask_name] = mask_bytes
                    total_bytes += mask_bytes

        memory_dict["total"] = total_bytes
        return memory_dict

    def get_edge_feature_names(self) -> Optional[List[str]]:
        """
        Get the names of edge features.

        Returns
        -------
        Optional[List[str]]
            List of edge feature names, or None if not available
        """
        result = getattr(self, NAPISTU_DATA.EDGE_FEATURE_NAMES, None)
        if result is None:
            logger.warning(
                "Edge feature names not found in NapistuData. "
                f"Attribute '{NAPISTU_DATA.EDGE_FEATURE_NAMES}' is missing."
            )
        return result

    def get_edge_indices(
        self, df: pd.DataFrame, from_col: str, to_col: str
    ) -> torch.Tensor:
        """
        Get edge index tensor from a DataFrame with vertex names.

        Extracts vertex names from specified columns in a DataFrame, converts them
        to indices using get_vertex_indices, and returns a tensor of shape (2, num_edges)
        suitable for use as edge_index in PyTorch Geometric.

        Parameters
        ----------
        df : pd.DataFrame
            DataFrame containing edge information with vertex names.
        from_col : str
            Name of the column containing source vertex names.
        to_col : str
            Name of the column containing target vertex names.

        Returns
        -------
        torch.Tensor
            Tensor of shape (2, num_edges) with dtype torch.long, where:
            - Row 0 contains source vertex indices
            - Row 1 contains target vertex indices

        Raises
        ------
        KeyError
            If from_col or to_col are not in the DataFrame.
        ValueError
            If any vertex names in the columns are not found in NapistuData.
        """
        # Validate columns exist
        if from_col not in df.columns:
            raise KeyError(
                f"Column '{from_col}' not found in DataFrame. Available columns: {list(df.columns)}"
            )
        if to_col not in df.columns:
            raise KeyError(
                f"Column '{to_col}' not found in DataFrame. Available columns: {list(df.columns)}"
            )

        # Get indices for source and target vertices
        source_indices = self.get_vertex_indices(df[from_col])
        target_indices = self.get_vertex_indices(df[to_col])

        # Convert to tensor of shape (2, num_edges)
        edge_index = torch.tensor([source_indices, target_indices], dtype=torch.long)

        return edge_index

    def get_edge_names(self) -> Optional[pd.Index]:
        """
        Get the edge names as a pandas Index.

        Returns
        -------
        Optional[pd.Index]
            Pandas Index of edge names, or None if not available
        """
        result = getattr(self, NAPISTU_DATA.NG_EDGE_NAMES, None)
        if result is None:
            logger.warning(
                "Edge names not found in NapistuData. "
                f"Attribute '{NAPISTU_DATA.NG_EDGE_NAMES}' is missing."
            )
        return result

    def get_edge_weights(self) -> Optional[torch.Tensor]:
        """
        Get edge weights as a 1D tensor.

        This method provides access to the original edge weights stored in the
        edge_weight attribute, which is the standard PyG convention for scalar
        edge weights.

        Returns
        -------
        Optional[torch.Tensor]
            1D tensor of edge weights, or None if not available

        Examples
        --------
        >>> weights = data.get_edge_weights()
        >>> if weights is not None:
        ...     print(f"Edge weights shape: {weights.shape}")
        ...     print(f"Mean weight: {weights.mean():.3f}")
        """
        result = getattr(self, PYG.EDGE_WEIGHT, None)
        if result is None:
            logger.warning(
                "Edge weights not found in NapistuData. "
                f"Attribute '{PYG.EDGE_WEIGHT}' is missing."
            )
        return result

    def get_feature_by_name(self, feature_name: str) -> torch.Tensor:
        """
        Get a feature by name from the NapistuData object.

        Parameters
        ----------
        feature_name : str
            The name of the feature to get

        Returns
        -------
        torch.Tensor
            The feature tensor
        """
        vertex_feature_names = self.get_vertex_feature_names()
        if feature_name not in vertex_feature_names:
            raise ValueError(
                f"Feature name {feature_name} not found in NapistuData vertex feature names"
            )
        feature_idx = vertex_feature_names.index(feature_name)
        return self.x[:, feature_idx]

    def get_features_by_regex(
        self, regex: str, return_suffixes: bool = False
    ) -> Tuple[torch.Tensor, List[str]]:
        """
        Get features by regex from the NapistuData object.

        Parameters
        ----------
        regex: str
            The regex to search for in the vertex feature names
        return_suffixes: bool
            If True, return the substring following the regex as feature_names

        Returns:
        Tuple[torch.Tensor, List[str]]
            The features tensor and feature names

            features: torch.Tensor
                The features tensor
            feature_names: List[str]
                The feature names

        Examples
        --------
        >>> features, feature_names = napistu_data.get_features_by_regex("__source")
        >>> print(features.shape)
        >>> print(feature_names)
        (100, 5)
        ['source_1', 'source_2', 'source_3', 'source_4', 'source_5']
        """

        vertex_feature_names = self.get_vertex_feature_names()
        mask = [bool(re.search(regex, x)) for x in vertex_feature_names]
        if not any(mask):
            raise ValueError(f"No features found with regex {regex}")

        matching_feature_names = [
            name for name, match in zip(vertex_feature_names, mask) if match
        ]
        if return_suffixes:
            # Check if regex already has capturing groups - if so, throw an exception
            if "(" in regex and ")" in regex:
                raise ValueError(
                    f"Regex '{regex}' already contains capturing groups. When return_suffixes=True, the regex should not contain parentheses."
                )

            # No capturing groups, create one for the suffix
            matching_feature_names = [
                re.search(regex + r"(.*)", x).group(1) for x in matching_feature_names
            ]
            matching_feature_names = [x.lstrip("_") for x in matching_feature_names]

        indices = [i for i, m in enumerate(mask) if m]
        selected_features = self.x[:, indices]
        return selected_features, matching_feature_names

    def get_num_relations(self) -> Optional[int]:
        """
        Get the number of relations from relation_type tensor.

        Computes the number of unique relation types and validates that
        they are consecutive integers starting from 0 (0, 1, 2, ..., N-1).

        Returns
        -------
        int
            Number of unique relation types

        Raises
        ------
        ValueError
            If relation_type is missing or contains non-consecutive integers
        """
        relation_type = getattr(self, NAPISTU_DATA.RELATION_TYPE, None)
        if relation_type is None:
            raise ValueError(
                "Relation type not found in NapistuData. Attribute 'relation_type' is missing. "
                "This NapistuData object does not contain relation information. "
                "To use relation-aware heads (RotatE, TransE, DistMult), create NapistuData "
                "with relation_strata_type parameter or ensure relation_type is set."
            )

        # Get unique relation types
        unique_relations = torch.unique(relation_type)
        num_relations = len(unique_relations)

        # Validate that relation types are consecutive integers starting from 0
        expected_relations = torch.arange(
            num_relations, dtype=unique_relations.dtype, device=unique_relations.device
        )
        if not torch.equal(unique_relations, expected_relations):
            raise ValueError(
                f"Relation types must be consecutive integers starting from 0 (0, 1, 2, ..., N-1). "
                f"Found unique values: {unique_relations.tolist()}, expected: {expected_relations.tolist()}"
            )

        return num_relations

    def get_summary(
        self, summary_type: str = NAPISTU_DATA_SUMMARY_TYPES.BASIC
    ) -> Dict[str, Any]:
        """
        Get a summary of the NapistuData object.

        Parameters
        ----------
        summary_type : str, default="basic"
            Type of summary to return:
            - "basic": Core structural attributes only (num_nodes, num_edges, etc.)
            - "validation": Basic + feature metadata for compatibility validation
            (includes feature names, aliases, relation labels, mask hashes)
            - "detailed": Validation + boolean flags for attribute presence
            (for backward compatibility and debugging)
            - "all": All available information

        Returns
        -------
        Dict[str, Any]
            Dictionary containing summary information about the data object

        """
        if summary_type not in VALID_NAPISTU_DATA_SUMMARY_TYPES:
            raise ValueError(
                f"Invalid summary_type '{summary_type}'. Must be one of {VALID_NAPISTU_DATA_SUMMARY_TYPES}"
            )

        # BASIC: Core structural attributes
        summary_dict = {
            NAPISTU_DATA.NAME: self.name,
            PYG.NUM_NODES: self.num_nodes,
            PYG.NUM_EDGES: self.num_edges,
            PYG.NUM_NODE_FEATURES: self.num_node_features,
            PYG.NUM_EDGE_FEATURES: self.num_edge_features,
        }

        # Optional basic attributes
        add_optional_attr(
            summary_dict,
            NAPISTU_DATA.SPLITTING_STRATEGY,
            getattr(self, NAPISTU_DATA.SPLITTING_STRATEGY, None),
        )

        relation_type = getattr(self, NAPISTU_DATA.RELATION_TYPE, None)
        if relation_type is not None:
            summary_dict[NAPISTU_DATA_SUMMARIES.NUM_UNIQUE_RELATIONS] = int(
                relation_type.unique().numel()
            )

        train_mask = getattr(self, NAPISTU_DATA.TRAIN_MASK, None)
        if train_mask is not None:
            summary_dict[NAPISTU_DATA_SUMMARIES.NUM_TRAIN_EDGES] = int(train_mask.sum())

        val_mask = getattr(self, NAPISTU_DATA.VAL_MASK, None)
        if val_mask is not None:
            summary_dict[NAPISTU_DATA_SUMMARIES.NUM_VAL_EDGES] = int(val_mask.sum())

        test_mask = getattr(self, NAPISTU_DATA.TEST_MASK, None)
        if test_mask is not None:
            summary_dict[NAPISTU_DATA_SUMMARIES.NUM_TEST_EDGES] = int(test_mask.sum())

        # Return early for basic summary
        if summary_type == NAPISTU_DATA_SUMMARY_TYPES.BASIC:
            return summary_dict

        # summaries included for all types besides basic
        vertex_feature_names = getattr(self, NAPISTU_DATA.VERTEX_FEATURE_NAMES, None)
        add_optional_attr(
            summary_dict, NAPISTU_DATA.VERTEX_FEATURE_NAMES, vertex_feature_names
        )

        edge_feature_names = getattr(self, NAPISTU_DATA.EDGE_FEATURE_NAMES, None)
        add_optional_attr(
            summary_dict, NAPISTU_DATA.EDGE_FEATURE_NAMES, edge_feature_names
        )

        relation_manager = getattr(self, NAPISTU_DATA.RELATION_MANAGER, None)

        if summary_type in [
            NAPISTU_DATA_SUMMARY_TYPES.VALIDATION,
            NAPISTU_DATA_SUMMARY_TYPES.ALL,
        ]:

            add_optional_attr(
                summary_dict,
                NAPISTU_DATA.VERTEX_FEATURE_NAME_ALIASES,
                getattr(self, NAPISTU_DATA.VERTEX_FEATURE_NAME_ALIASES, None),
            )

            add_optional_attr(
                summary_dict,
                NAPISTU_DATA.EDGE_FEATURE_NAME_ALIASES,
                getattr(self, NAPISTU_DATA.EDGE_FEATURE_NAME_ALIASES, None),
            )

            if relation_manager is not None:
                # Convert label_names dict to list (sorted by index)
                label_names_dict = relation_manager.label_names
                if label_names_dict is not None:
                    # Sort by keys and extract values to create ordered list
                    relation_labels = [
                        label_names_dict[i] for i in sorted(label_names_dict.keys())
                    ]
                    summary_dict[NAPISTU_DATA.RELATION_TYPE_LABELS] = relation_labels

            # Mask hashes for split validation
            mask_hashes = compute_mask_hashes(
                train_mask=getattr(self, NAPISTU_DATA.TRAIN_MASK, None),
                val_mask=getattr(self, NAPISTU_DATA.VAL_MASK, None),
                test_mask=getattr(self, NAPISTU_DATA.TEST_MASK, None),
            )
            summary_dict.update(mask_hashes)

        if summary_type in [
            NAPISTU_DATA_SUMMARY_TYPES.DETAILED,
            NAPISTU_DATA_SUMMARY_TYPES.ALL,
        ]:

            summary_dict[NAPISTU_DATA_SUMMARIES.HAS_VERTEX_FEATURE_NAMES] = (
                vertex_feature_names is not None
            )
            summary_dict[NAPISTU_DATA_SUMMARIES.HAS_EDGE_FEATURE_NAMES] = (
                edge_feature_names is not None
            )
            summary_dict[NAPISTU_DATA_SUMMARIES.HAS_EDGE_WEIGHTS] = (
                getattr(self, PYG.EDGE_WEIGHT, None) is not None
            )
            summary_dict[NAPISTU_DATA_SUMMARIES.HAS_NG_VERTEX_NAMES] = (
                getattr(self, NAPISTU_DATA.NG_VERTEX_NAMES, None) is not None
            )
            summary_dict[NAPISTU_DATA_SUMMARIES.HAS_NG_EDGE_NAMES] = (
                getattr(self, NAPISTU_DATA.NG_EDGE_NAMES, None) is not None
            )
            summary_dict[NAPISTU_DATA_SUMMARIES.HAS_SPLITTING_STRATEGY] = (
                getattr(self, NAPISTU_DATA.SPLITTING_STRATEGY, None) is not None
            )
            summary_dict[NAPISTU_DATA_SUMMARIES.HAS_LABELING_MANAGER] = (
                getattr(self, NAPISTU_DATA.LABELING_MANAGER, None) is not None
            )
            summary_dict[NAPISTU_DATA_SUMMARIES.HAS_RELATION_MANAGER] = (
                relation_manager is not None
            )

        return summary_dict

    def get_symmetrical_relation_indices(
        self,
        treat_asymmetrically: Set[str] = {MERGE_RARE_STRATA_DEFS.OTHER_RELATION},
    ) -> List[int]:
        """
        Analyze relation type names to detect symmetric ones.

        Parses relation names in the format "{source_type} -> {target_type}" (spaces
        around the arrow are optional) and categorizes them based on whether
        source_type == target_type.

        Parameters
        ----------
        treat_asymmetrically : Set[str], optional
            Set of relation names to treat as asymmetric even if they don't match
            the standard pattern. Defaults to {MERGE_RARE_STRATA_DEFS.OTHER_RELATION}.

        Returns
        -------
        List[int]
            List of relation type indices that are symmetric

        Raises
        ------
        ValueError
            If relation_manager is missing or all relations are same type
        """

        relation_manager = getattr(self, NAPISTU_DATA.RELATION_MANAGER, None)
        if relation_manager is None:
            raise ValueError(
                "Cannot analyze relation symmetry - relation_manager is missing. "
                "This NapistuData object was not created with relation types."
            )

        # Get relation names from manager
        label_names = relation_manager.label_names  # Dict[int, str]
        if not label_names:
            raise ValueError("relation_manager has no label_names")

        # Analyze each relation name
        RELATION_NAME_PATTERN = re.compile(
            r"^([a-zA-Z_][a-zA-Z0-9_]*)\s*->\s*([a-zA-Z_][a-zA-Z0-9_]*)$"
        )

        symmetric_indices = []
        asymmetric_indices = []
        malformed_names = []

        for idx, name in label_names.items():
            # Treat special relation names as asymmetric
            if name in treat_asymmetrically:
                asymmetric_indices.append(idx)
                continue

            match = RELATION_NAME_PATTERN.match(name)

            if not match:
                malformed_names.append(name)
                continue

            source_type = match.group(1)
            target_type = match.group(2)

            if source_type == target_type:
                symmetric_indices.append(idx)
            else:
                asymmetric_indices.append(idx)

        # Validation
        if malformed_names:
            raise ValueError(
                f"Found {len(malformed_names)} malformed relation names. "
                f"Expected format: '{{source_type}} -> {{target_type}}' (spaces optional). "
                f"Malformed: {malformed_names}"
            )

        if not symmetric_indices:
            raise ValueError(
                f"All {len(label_names)} relations are asymmetric. "
                f"ConditionalRotateHead requires a mix. Use RotatE instead."
            )

        if not asymmetric_indices:
            raise ValueError(
                f"All {len(label_names)} relations are symmetric. "
                f"ConditionalRotateHead requires a mix. Use DotProduct/DistMult instead."
            )

        return symmetric_indices

    def get_vertex_feature_names(self) -> Optional[List[str]]:
        """
        Get the names of vertex features.

        Returns
        -------
        Optional[List[str]]
            List of vertex feature names, or None if not available
        """
        result = getattr(self, NAPISTU_DATA.VERTEX_FEATURE_NAMES, None)
        if result is None:
            logger.warning(
                "Vertex feature names not found in NapistuData. "
                f"Attribute '{NAPISTU_DATA.VERTEX_FEATURE_NAMES}' is missing."
            )
        return result

    def get_vertex_indices(
        self, vertex_names: Union[List[str], pd.Series]
    ) -> List[int]:
        """
        Get the indices of vertices by their names.

        Parameters
        ----------
        vertex_names : List[str] or pd.Series
            List or Series of vertex names to look up. If Series, uses the values.

        Returns
        -------
        List[int]
            List of integer indices corresponding to the vertex names.
            Indices are aligned with the vertex tensor (x) rows.

        Raises
        ------
        TypeError
            If vertex_names is not a list or pd.Series.
        ValueError
            If vertex names are not available in this NapistuData.
        ValueError
            If any vertex names are not found (results in -1 indices).
        """
        # Validate input type
        if not isinstance(vertex_names, (list, pd.Series)):
            raise TypeError(
                f"vertex_names must be a list or pd.Series, got {type(vertex_names)}"
            )

        # Get vertex names from NapistuData
        data_vertex_names = self.get_vertex_names()
        if data_vertex_names is None:
            raise ValueError(
                "Vertex names are not available in this NapistuData. "
                f"Attribute '{NAPISTU_DATA.NG_VERTEX_NAMES}' is missing."
            )

        # Convert input to list of names
        if isinstance(vertex_names, pd.Series):
            names_to_lookup = vertex_names.values.tolist()
        else:
            names_to_lookup = vertex_names

        # Get indices using pandas Index.get_indexer
        # This returns integer positions, or -1 for missing values
        indices = pd.Index(data_vertex_names).get_indexer(names_to_lookup)

        # Check for missing vertices (-1 indicates not found)
        missing_mask = indices == -1
        if missing_mask.any():
            missing_names = [
                name
                for name, is_missing in zip(names_to_lookup, missing_mask)
                if is_missing
            ]
            raise ValueError(
                f"Vertex names not found in NapistuData: {missing_names[:10]}"
                + (
                    f" (and {len(missing_names) - 10} more)"
                    if len(missing_names) > 10
                    else ""
                )
            )

        return indices.tolist()

    def get_vertex_names(self) -> Optional[pd.Index]:
        """
        Get the vertex names as a pandas Index.

        Returns
        -------
        Optional[pd.Index]
            Pandas Index of vertex names, or None if not available
        """
        result = getattr(self, NAPISTU_DATA.NG_VERTEX_NAMES, None)
        if result is None:
            logger.warning(
                "Vertex names not found in NapistuData. "
                f"Attribute '{NAPISTU_DATA.NG_VERTEX_NAMES}' is missing."
            )
        return result

    def has_edges(self, edge_indices: torch.Tensor) -> torch.Tensor:
        """
        Check which edges in edge_indices are present in this NapistuData.

        Uses efficient set-based lookup for fast checking of many edges.
        Suitable for looking up large numbers of edges (e.g., 30K+).

        Parameters
        ----------
        edge_indices : torch.Tensor
            Edge indices tensor of shape (2, num_edges) to check.
            Row 0 should contain source vertex indices.
            Row 1 should contain target vertex indices.

        Returns
        -------
        torch.Tensor
            Boolean tensor of shape (num_edges,) where True indicates
            the edge exists in this NapistuData.

        Examples
        --------
        >>> # Get edge indices from a DataFrame
        >>> query_edges = napistu_data.get_edge_indices(df, from_col='from', to_col='to')
        >>> # Check which edges exist
        >>> matches = napistu_data.has_edges(query_edges)
        >>> # Filter to only existing edges
        >>> existing_edges = query_edges[:, matches]
        """
        # Validate input shape
        if edge_indices.dim() != 2 or edge_indices.shape[0] != 2:
            raise ValueError(
                f"edge_indices must be a 2D tensor with shape (2, num_edges), "
                f"got shape {edge_indices.shape}"
            )

        # Convert existing edges to set of tuples for O(1) lookup
        existing_edges_set = set(tuple(edge.tolist()) for edge in self.edge_index.T)

        # Check each query edge
        query_t = edge_indices.T  # shape (num_query, 2)
        matches = [tuple(edge.tolist()) in existing_edges_set for edge in query_t]

        return torch.tensor(matches, dtype=torch.bool, device=edge_indices.device)

    @classmethod
    def load(
        cls, filepath: Union[str, Path], map_location: str = DEVICE.CPU
    ) -> "NapistuData":
        """
        Load a NapistuData object from disk.

        This method automatically uses weights_only=False to ensure compatibility
        with PyG Data objects, which contain custom classes that aren't allowed
        with the default weights_only=True setting in PyTorch 2.6+.

        Parameters
        ----------
        filepath : Union[str, Path]
            Path to the saved data object
        map_location : str, default='cpu'
            Device to map tensors to (e.g., 'cpu', 'cuda:0'). Defaults to 'cpu'
            for universal compatibility.

        Returns
        -------
        NapistuData
            The loaded NapistuData object

        Raises
        ------
        FileNotFoundError
            If the file doesn't exist
        RuntimeError
            If loading fails
        TypeError
            If the loaded object is not a NapistuData or Data object

        Examples
        --------
        >>> data = NapistuData.load('my_network.pt')  # Loads to CPU by default
        >>> data = NapistuData.load('my_network.pt', map_location='cuda:0')  # Load to GPU

        Notes
        -----
        This method uses weights_only=False by default because PyG Data objects
        contain custom classes that aren't allowed with weights_only=True.
        Only use this with trusted files, as it can result in arbitrary code execution.
        """
        filepath = ensure_path(filepath)

        if not filepath.exists():
            raise FileNotFoundError(f"File n?ot found: {filepath}")

        try:
            # Always use weights_only=False for PyG compatibility
            data = torch.load(filepath, weights_only=False, map_location=map_location)

            # Convert to NapistuData if it's a regular Data object
            if isinstance(data, NapistuData):
                return data
            else:
                raise TypeError(
                    f"Loaded object is not a NapistuData object, got {type(data)}. "
                    "This may indicate a corrupted file or incorrect file type."
                )

        except Exception as e:
            raise RuntimeError(
                f"Failed to load NapistuData object from {filepath}: {e}"
            ) from e

    def save(self, filepath: Union[str, Path]) -> None:
        """
        Save the NapistuData object to disk.

        This method provides a safe way to save NapistuData objects, ensuring
        compatibility with PyTorch's security features.

        Parameters
        ----------
        filepath : Union[str, Path]
            Path where to save the data object

        Examples
        --------
        >>> data.save('my_network.pt')
        """
        filepath = ensure_path(filepath)
        filepath.parent.mkdir(parents=True, exist_ok=True)

        torch.save(self, filepath)

    def show_memory_footprint(self) -> None:
        """
        Display memory footprint of the NapistuData object.

        Prints a formatted breakdown of memory usage for each component
        of the data object in gigabytes (GB), showing node features,
        edge index, edge attributes, and training/validation/test masks.

        Examples
        --------
        >>> data.show_memory_footprint()
        Node features: 0.05 GB
        Edge index: 0.01 GB
        Edge attributes: 0.20 GB
        train_mask: 0.00 GB
        val_mask: 0.00 GB
        test_mask: 0.00 GB

        Total data: 0.26 GB
        """
        memory_dict = self.estimate_memory_footprint()

        # Node features
        if memory_dict[PYG.X] is not None:
            print(f"Node features: {memory_dict[PYG.X] / 1e9:.2f} GB")

        # Edge index
        if memory_dict[PYG.EDGE_INDEX] is not None:
            print(f"Edge index: {memory_dict[PYG.EDGE_INDEX] / 1e9:.2f} GB")

        # Edge attributes
        if memory_dict[PYG.EDGE_ATTR] is not None:
            print(f"Edge attributes: {memory_dict[PYG.EDGE_ATTR] / 1e9:.2f} GB")

        # Masks
        for mask_name in [
            NAPISTU_DATA.TRAIN_MASK,
            NAPISTU_DATA.VAL_MASK,
            NAPISTU_DATA.TEST_MASK,
        ]:
            if memory_dict[mask_name] is not None:
                print(f"{mask_name}: {memory_dict[mask_name] / 1e9:.3f} GB")

        print(f"\nTotal data: {memory_dict['total'] / 1e9:.2f} GB")

    def show_summary(self) -> None:
        """
        Display a summary of the NapistuData object.
        """
        summary = self.get_summary(NAPISTU_DATA_SUMMARY_TYPES.DETAILED)
        summary_table = format_summary(summary)
        show(summary_table)

    def trim(
        self,
        keep_edge_attr: bool = True,
        keep_labels: bool = True,
        keep_masks: bool = True,
        keep_relation_type: bool = True,
        inplace: bool = False,
    ) -> "NapistuData":
        """
        Create a memory-optimized copy with only essential training attributes.

        This method creates a new NapistuData object with only the core attributes
        needed for training, stripping away all metadata and debugging information.

        **What's Always Kept:**
        - x (node features)
        - edge_index (graph structure)
        - edge_weight (if present)

        **What's Always Removed:**
        - ng_vertex_names, ng_edge_names (pandas objects)
        - vertex_feature_names, edge_feature_names (metadata)
        - name, splitting_strategy (metadata)

        **Conditionally Kept:**
        - labeling_manager: Kept if keep_labels=True (needed for label metadata)
        - relation_manager: Kept if keep_relation_type=True (needed for relation metadata)

        Parameters
        ----------
        keep_edge_attr : bool, default=True
            Whether to keep edge_attr. Set False if not using edge features
            (e.g., no edge encoder). **Major memory savings for large graphs.**
        keep_labels : bool, default=True
            Whether to keep y (node labels). Set False for unlabeled tasks.
        keep_masks : bool, default=True
            Whether to keep train_mask, val_mask, test_mask.
            Set False if using custom splitting.
        keep_relation_type : bool, default=True
            Whether to keep relation_type. Set False if not using relation-aware heads.
        inplace: bool, default=False
            Whether to modify the current object in place or return a new object.

        Returns
        -------
        NapistuData
            New trimmed NapistuData object with minimal attributes

        Examples
        --------
        >>> # Default - keep everything except metadata
        >>> trimmed = data.trim()
        >>>
        >>> # No edge features needed (biggest memory savings)
        >>> trimmed = data.trim(keep_edge_attr=False)
        >>>
        >>> # Unlabeled learning
        >>> trimmed = data.trim(keep_labels=False)
        >>>
        >>> # Check memory savings
        >>> print(f"Before: {data.estimate_memory():.2f} GB")
        >>> print(f"After: {trimmed.estimate_memory():.2f} GB")
        >>>
        >>> # Minimal for inference (no edge features, labels, or masks)
        >>> trimmed = data.trim(
        ...     keep_edge_attr=False,
        ...     keep_labels=False,
        ...     keep_masks=False
        ... )

        Notes
        -----
        **Memory Impact (10M edges example):**
        - Removing edge_attr: saves ~4 GB (100 features) to ~0.4 GB (10 features)
        - Removing pandas names: saves ~10-100 MB
        - Removing labels/masks: saves ~10-50 MB
        """

        new_attrs = {
            PYG.X: self.x,
            PYG.EDGE_INDEX: self.edge_index,
            # edge_attr is always included to satisfy NapistuData.__init__ requirements
            # If keep_edge_attr=False, use empty tensor (saves memory while keeping API compatibility)
            PYG.EDGE_ATTR: (
                self.edge_attr if keep_edge_attr else torch.empty((self.num_edges, 0))
            ),
            NAPISTU_DATA.NAME: NAPISTU_DATA_DEFAULT_NAME + "_trimmed",
        }

        # Add edge_weight if present
        if hasattr(self, PYG.EDGE_WEIGHT) and self.edge_weight is not None:
            new_attrs[PYG.EDGE_WEIGHT] = self.edge_weight

        # Add labels if requested
        if keep_labels and hasattr(self, PYG.Y) and self.y is not None:
            new_attrs[PYG.Y] = self.y
            # Also keep labeling_manager if labels are kept (needed for label metadata)
            if hasattr(self, NAPISTU_DATA.LABELING_MANAGER):
                labeling_manager = getattr(self, NAPISTU_DATA.LABELING_MANAGER)
                if labeling_manager is not None:
                    new_attrs[NAPISTU_DATA.LABELING_MANAGER] = labeling_manager

        # Add masks if requested
        if keep_masks:
            for mask_name in [
                NAPISTU_DATA.TRAIN_MASK,
                NAPISTU_DATA.VAL_MASK,
                NAPISTU_DATA.TEST_MASK,
            ]:
                if hasattr(self, mask_name):
                    mask = getattr(self, mask_name)
                    if mask is not None:
                        new_attrs[mask_name] = mask

        # Add relation_type if requested
        if keep_relation_type and hasattr(self, NAPISTU_DATA.RELATION_TYPE):
            new_attrs[NAPISTU_DATA.RELATION_TYPE] = getattr(
                self, NAPISTU_DATA.RELATION_TYPE
            )
            # Also keep relation_manager if relations are kept (needed for relation metadata)
            if hasattr(self, NAPISTU_DATA.RELATION_MANAGER):
                relation_manager = getattr(self, NAPISTU_DATA.RELATION_MANAGER)
                if relation_manager is not None:
                    new_attrs[NAPISTU_DATA.RELATION_MANAGER] = relation_manager

        if inplace:
            # Modify the object in place by clearing all attributes and setting new ones
            for key in list(self.keys()):
                delattr(self, key)
            for key, value in new_attrs.items():
                setattr(self, key, value)
            return None
        else:
            return NapistuData(**new_attrs)

    def unencode_features(
        self,
        napistu_graph: NapistuGraph,
        attribute_type: str,
        attribute: str,
        encoding_manager: Optional[EncodingManager] = None,
    ) -> pd.Series:
        """
        Unencode features from the NapistuData object back to the original values.

        This only categorical and passthrough encoding and is useful for validation purposes
        to ensure that encoded features are proprely aligned with their values in their original NapistuGraph.

        Parameters
        ----------
        napistu_graph : NapistuGraph
            The NapistuGraph object containing the original values
        attribute_type : str
            The type of attribute to unencode ("vertices" or "edges")
        attribute : str
            An attribute to unencode (e.g., "node_type" or "species_type")
        encoding_manager : Optional[EncodingManager]
            The encoding manager to use to unencode the features.
            If this is not provided then the default encoding managers will be used.

        Returns
        -------
        pd.DataFrame
            A DataFrame with the unencoded features
        """

        if attribute_type == NAPISTU_GRAPH.VERTICES:
            attribute_values = napistu_graph.get_vertex_series(attribute)
            encoded_features = self.x
            feature_names = self.vertex_feature_names
            feature_name_aliases = self.vertex_feature_name_aliases
        elif attribute_type == NAPISTU_GRAPH.EDGES:
            attribute_values = napistu_graph.get_edge_series(attribute)
            encoded_features = self.edge_attr
            feature_names = self.edge_feature_names
            feature_name_aliases = self.edge_feature_name_aliases
        else:
            raise ValueError(f"Invalid attribute type: {attribute_type}")

        if encoding_manager is None:
            if attribute_type == NAPISTU_GRAPH.VERTICES:
                encoding_manager = EncodingManager(
                    VERTEX_DEFAULT_TRANSFORMS, encoders=DEFAULT_ENCODERS
                )
            elif attribute_type == NAPISTU_GRAPH.EDGES:
                encoding_manager = EncodingManager(
                    EDGE_DEFAULT_TRANSFORMS, encoders=DEFAULT_ENCODERS
                )
        elif not isinstance(encoding_manager, EncodingManager):
            ValueError(
                f"Invalid value for `encoding_manager` it should be either None or an EncodingManager object but was given a {type(encoding_manager)}: {encoding_manager}"
            )

        # Filter encoding manager to only include the attribute of interest
        filtered_config = {}
        for transform_name, transform_config in encoding_manager.config_.items():
            if attribute in transform_config[ENCODING_MANAGER.COLUMNS]:
                # Create new config with only this attribute
                filtered_config[transform_name] = {
                    ENCODING_MANAGER.COLUMNS: [attribute],
                    ENCODING_MANAGER.TRANSFORMER: transform_config[
                        ENCODING_MANAGER.TRANSFORMER
                    ],
                }

        if not filtered_config:
            raise ValueError(f"Attribute '{attribute}' not found in encoding manager")

        # Create filtered encoding manager for just this attribute
        filtered_manager = EncodingManager(filtered_config)

        # Fit the encoder on the single-column DataFrame
        fitted_encoder = fit_encoders(attribute_values.to_frame(), filtered_manager)
        _, actual_transformer, _ = fitted_encoder.transformers_[0]

        # Get feature names for this specific attribute to find column indices
        _, fitted_feature_names, _ = transform_dataframe(
            attribute_values.to_frame(), fitted_encoder, deduplicate=False
        )

        # do we need columns which are only present as aliases?
        if any(name in feature_name_aliases for name in fitted_feature_names):
            # need to expand the encoding
            required_aliases = {
                k: v
                for k, v in feature_name_aliases.items()
                if k in fitted_feature_names
            }
            encoded_features, feature_names = expand_deduplicated_features(
                encoded_features, feature_names, required_aliases
            )

        # Find which columns in encoded_features correspond to this attribute
        col_indices = [feature_names.index(fname) for fname in fitted_feature_names]

        # Extract relevant columns from encoded features
        if isinstance(encoded_features, torch.Tensor):
            relevant_features = encoded_features[:, col_indices].cpu().numpy()
        else:
            relevant_features = encoded_features[:, col_indices]

        # Inverse transform using the actual transformer
        if actual_transformer == ENCODING_MANAGER.PASSTHROUGH:
            # For passthrough, just extract the column directly
            decoded_values = relevant_features.flatten()
        else:
            # For OneHotEncoder and other transformers with inverse_transform
            decoded = actual_transformer.inverse_transform(relevant_features)
            decoded_values = decoded.flatten()

        return pd.Series(decoded_values, name=attribute)

    def _validate_vertex_encoding(
        self,
        napistu_graph: NapistuGraph,
        vertex_attribute: str,
        encoding_manager: Optional[EncodingManager] = None,
    ) -> bool:
        """
        Validate consistency between encoded values and original NapistuGraph vertex values.

        This method compares the vertex values recovered from encoding
        in the NapistuData object with the original vertex values stored in
        the NapistuGraph object to ensure data consistency.

        Parameters
        ----------
        napistu_graph : NapistuGraph
            The NapistuGraph object containing the original categorical values
        categorical_vertex_attribute : str
            The name of the categorical vertex attribute to validate (e.g., 'node_type')

        Returns
        -------
        bool
            True if the encoding is consistent, False otherwise

        Raises
        ------
        ValueError
            If the categorical attribute is not found in the NapistuGraph,
            if vertex names don't match between NapistuData and NapistuGraph,
            or if there are encoding inconsistencies.

        Examples
        --------
        >>> # Validate node_type encoding consistency
        >>> is_consistent = napistu_data._validate_vertex_encoding(napistu_graph, 'node_type')
        >>> print(f"Encoding is consistent: {is_consistent}")
        True

        >>> # Validate a different categorical attribute
        >>> is_consistent = napistu_data._validate_vertex_encoding(napistu_graph, 'species_type')
        >>> print(f"Species type encoding is consistent: {is_consistent}")
        True
        """
        # Get the categorical values from NapistuGraph
        graph_values = napistu_graph.get_vertex_series(vertex_attribute)

        # Get the recovered values from encoding in NapistuData using unencode_features
        data_values = self.unencode_features(
            napistu_graph=napistu_graph,
            attribute_type=NAPISTU_GRAPH.VERTICES,
            attribute=vertex_attribute,
            encoding_manager=encoding_manager,
        )

        # Get vertex names for alignment
        if (
            not hasattr(self, NAPISTU_DATA.NG_VERTEX_NAMES)
            or getattr(self, NAPISTU_DATA.NG_VERTEX_NAMES) is None
        ):
            raise ValueError(
                f"Validation not available - the `{NAPISTU_DATA.NG_VERTEX_NAMES}` attribute is required for this method."
            )
        data_vertex_names = getattr(self, NAPISTU_DATA.NG_VERTEX_NAMES)

        # Align the graph values with the NapistuData vertex ordering
        # Create a DataFrame for easier merging
        graph_df = pd.DataFrame(
            {
                "graph_vertex_name": graph_values.index,
                "graph_vertex_value": graph_values.values,
            }
        )

        # Merge with NapistuData vertex names to get aligned graph values
        # Convert data_vertex_names Series to DataFrame for merging
        data_vertex_names_df = data_vertex_names.to_frame("graph_vertex_name")
        aligned_graph = data_vertex_names_df.merge(
            graph_df, on="graph_vertex_name", how="left"
        )
        graph_values_aligned = aligned_graph["graph_vertex_value"]

        # Debug: Check if we have any matches
        matches_found = aligned_graph["graph_vertex_value"].notna().sum()
        if matches_found == 0:
            raise ValueError(
                f"No matching vertex names found between NapistuData and NapistuGraph. "
                f"NapistuData vertex names: {data_vertex_names.tolist()[:5]}... "
                f"NapistuGraph vertex names: {graph_values.index.tolist()[:5]}..."
            )

        # Create masks for valid (non-null) values in both series
        graph_valid_mask = ~graph_values_aligned.isna()
        data_valid_mask = ~data_values.isna()

        # Check if the non-null masks are identical
        if not graph_valid_mask.equals(data_valid_mask):
            graph_null_count = (~graph_valid_mask).sum()
            data_null_count = (~data_valid_mask).sum()
            raise ValueError(
                f"Non-null masks don't match between graph and data values. "
                f"Graph values non-null count: {graph_valid_mask.sum()}, "
                f"Data values non-null count: {data_valid_mask.sum()}, "
                f"Graph values null count: {graph_null_count}, "
                f"Data values null count: {data_null_count}"
            )

        # Compare only valid values (since masks are identical, we can use either)
        graph_valid = graph_values_aligned[graph_valid_mask]
        data_valid = data_values[graph_valid_mask]

        # Check for exact matches
        matches = graph_valid == data_valid

        if not matches.all():
            # Find mismatches for detailed error reporting
            mismatches = ~matches
            mismatch_indices = matches.index[mismatches]

            mismatch_details = []
            for idx in mismatch_indices:
                graph_val = graph_valid[idx]
                data_val = data_valid[idx]
                vertex_name = data_vertex_names.iloc[
                    data_vertex_names.index.get_loc(idx)
                ]
                mismatch_details.append(
                    f"Vertex '{vertex_name}': graph='{graph_val}', data='{data_val}'"
                )

            raise ValueError(
                f"Encoding validation failed for {vertex_attribute}. "
                f"Found {mismatches.sum()} mismatches out of {len(matches)} valid comparisons:\n"
                + "\n".join(mismatch_details)
            )

        return True

    def _validate_edge_encoding(
        self,
        napistu_graph: NapistuGraph,
        edge_attribute: str,
        encoding_manager: Optional[EncodingManager] = None,
    ) -> bool:
        """
        Validate consistency between encoded values and original NapistuGraph edge values.

        This method compares the edge values recovered from encoding
        in the NapistuData object with the original edge values stored in
        the NapistuGraph object to ensure data consistency.

        Parameters
        ----------
        napistu_graph : NapistuGraph
            The NapistuGraph object containing the original edge values
        edge_attribute : str
            The name of the edge attribute to validate (e.g., 'r_irreversible')
        encoding_manager : Optional[EncodingManager]
            The encoding manager to use to unencode the features.
            If this is not provided then the default encoding managers will be used.

        Returns
        -------
        bool
            True if the encoding is consistent, False otherwise

        Raises
        ------
        ValueError
            If the edge attribute is not found in the NapistuGraph,
            if edge names don't match between NapistuData and NapistuGraph,
            or if there are encoding inconsistencies.

        Examples
        --------
        >>> # Validate r_irreversible encoding consistency
        >>> is_consistent = napistu_data._validate_edge_encoding(napistu_graph, 'r_irreversible')
        >>> print(f"Encoding is consistent: {is_consistent}")
        True

        >>> # Validate a different edge attribute
        >>> is_consistent = napistu_data._validate_edge_encoding(napistu_graph, 'weight')
        >>> print(f"Weight encoding is consistent: {is_consistent}")
        True
        """
        # Get the edge values from NapistuGraph
        graph_values = napistu_graph.get_edge_series(edge_attribute)

        # Get the recovered values from encoding in NapistuData using unencode_features
        data_values = self.unencode_features(
            napistu_graph=napistu_graph,
            attribute_type=NAPISTU_GRAPH.EDGES,
            attribute=edge_attribute,
            encoding_manager=encoding_manager,
        )

        # Get edge names for alignment
        if (
            not hasattr(self, NAPISTU_DATA.NG_EDGE_NAMES)
            or getattr(self, NAPISTU_DATA.NG_EDGE_NAMES) is None
        ):
            raise ValueError(
                f"Validation not available - the `{NAPISTU_DATA.NG_EDGE_NAMES}` attribute is required for this method."
            )
        data_edge_names = getattr(self, NAPISTU_DATA.NG_EDGE_NAMES)

        # Align the graph values with the NapistuData edge ordering
        # Convert the Series with MultiIndex to DataFrame
        graph_df = graph_values.to_frame("graph_edge_value").reset_index()

        # Merge with NapistuData edge names to get aligned graph values
        # Convert data_edge_names DataFrame for merging
        aligned_graph = data_edge_names.merge(
            graph_df, on=[NAPISTU_GRAPH_EDGES.FROM, NAPISTU_GRAPH_EDGES.TO], how="left"
        )
        graph_values_aligned = aligned_graph["graph_edge_value"]

        # Debug: Check if we have any matches
        matches_found = aligned_graph["graph_edge_value"].notna().sum()
        if matches_found == 0:
            raise ValueError(
                f"No matching edge names found between NapistuData and NapistuGraph. "
                f"NapistuData edge names: {data_edge_names.head().to_dict()}... "
                f"NapistuGraph edge names: {graph_values.index.tolist()[:5]}..."
            )

        # Create masks for valid (non-null) values in both series
        graph_valid_mask = ~graph_values_aligned.isna()
        data_valid_mask = ~data_values.isna()

        # Check if the non-null masks are identical
        if not graph_valid_mask.equals(data_valid_mask):
            graph_null_count = (~graph_valid_mask).sum()
            data_null_count = (~data_valid_mask).sum()
            raise ValueError(
                "Non-null masks don't match between graph and data values. "
                f"Graph values non-null count: {graph_valid_mask.sum()}, "
                f"Data values non-null count: {data_valid_mask.sum()}, "
                f"Graph values null count: {graph_null_count}, "
                f"Data values null count: {data_null_count}"
            )

        # Compare only valid values (since masks are identical, we can use either)
        graph_valid = graph_values_aligned[graph_valid_mask]
        data_valid = data_values[graph_valid_mask]

        # Check for exact matches
        matches = graph_valid == data_valid

        if not matches.all():
            # Find mismatches for detailed error reporting
            mismatches = ~matches
            mismatch_indices = matches.index[mismatches]

            mismatch_details = []
            for idx in mismatch_indices:
                graph_val = graph_valid[idx]
                data_val = data_valid[idx]
                edge_info = data_edge_names.iloc[data_edge_names.index.get_loc(idx)]
                mismatch_details.append(
                    f"Edge '{edge_info['from']} -> {edge_info['to']}': graph='{graph_val}', data='{data_val}'"
                )

            raise ValueError(
                f"Encoding validation failed for {edge_attribute}. "
                f"Found {mismatches.sum()} mismatches out of {len(matches)} valid comparisons:\n"
                + "\n".join(mismatch_details)
            )

        return True

    def _validate_labels(
        self,
        napistu_graph: NapistuGraph,
        labeling_manager: LabelingManager,
    ) -> bool:
        """
        Validate consistency between encoded labels and original NapistuGraph vertex labels.

        This method compares the labels recovered from encoding
        in the NapistuData object with the original labels stored in
        the NapistuGraph object to ensure data consistency.

        Parameters
        ----------
        napistu_graph : NapistuGraph
            The NapistuGraph object containing the original vertex labels
        labeling_manager: LabelingManager
            The labeling manager used to decode the encoded labels

        Returns
        -------
        bool
            True if the label encoding is consistent, False otherwise

        Raises
        ------
        ValueError
            If the NapistuData object doesn't have encoded labels (y attribute),
            if vertex names don't match between NapistuData and NapistuGraph,
            or if there are label encoding inconsistencies.

        Examples
        --------
        >>> # Validate label encoding consistency
        >>> is_consistent = napistu_data._validate_labels(napistu_graph, labeling_manager)
        >>> print(f"Label encoding is consistent: {is_consistent}")
        True
        """

        # Check if NapistuData has encoded labels
        if not hasattr(self, PYG.Y) or getattr(self, PYG.Y) is None:
            raise ValueError(
                "NapistuData object does not have encoded labels (y attribute)"
            )

        # Get the encoded labels and vertex names from NapistuData
        encoded_labels = getattr(self, PYG.Y)
        vertex_names = getattr(self, NAPISTU_DATA.NG_VERTEX_NAMES, None)

        if vertex_names is None:
            raise ValueError(
                f"Validation not available - the `{NAPISTU_DATA.NG_VERTEX_NAMES}` attribute is required for this method."
            )

        # Verify dimensions match
        if len(encoded_labels) != len(vertex_names):
            raise ValueError(
                f"Label count ({len(encoded_labels)}) should match vertex count ({len(vertex_names)})"
            )
        if len(encoded_labels) != self.num_nodes:
            raise ValueError(
                f"Label count ({len(encoded_labels)}) should match node count ({self.num_nodes})"
            )

        # Decode the labels using the utility function
        decoded_labels = pd.Series(decode_labels(encoded_labels, labeling_manager))

        # Get the corresponding labels from the NapistuGraph using merge
        vertex_df = napistu_graph.get_vertex_dataframe()
        vertex_names_df = pd.DataFrame({NAPISTU_GRAPH_VERTICES.NAME: vertex_names})

        # Merge vertex names with the vertex DataFrame to get labels
        merged_df = vertex_names_df.merge(
            vertex_df[[NAPISTU_GRAPH_VERTICES.NAME, labeling_manager.label_attribute]],
            on=NAPISTU_GRAPH_VERTICES.NAME,
            how="left",
        )
        graph_labels = merged_df[labeling_manager.label_attribute]

        # Create mask for valid (non-null) values in both decoded and graph labels
        decoded_valid_mask = decoded_labels.notna()
        graph_valid_mask = graph_labels.notna()
        valid_mask = decoded_valid_mask & graph_valid_mask

        # Compare only valid values
        decoded_valid = decoded_labels[valid_mask]
        graph_valid = graph_labels[valid_mask]

        if len(decoded_valid) != len(graph_valid):
            raise ValueError("Valid label counts should match")

        # Check for exact matches
        matches = decoded_valid == graph_valid

        if not matches.all():
            # Find mismatches for detailed error reporting
            mismatches = ~matches
            mismatch_indices = matches.index[mismatches]

            mismatch_details = []
            for idx in mismatch_indices:
                decoded_val = decoded_valid[idx]
                graph_val = graph_valid[idx]
                vertex_name = vertex_names.iloc[vertex_names.index.get_loc(idx)]
                mismatch_details.append(
                    f"Vertex '{vertex_name}': decoded='{decoded_val}', graph='{graph_val}'"
                )

            raise ValueError(
                f"Label encoding validation failed. "
                f"Found {mismatches.sum()} mismatches out of {len(matches)} valid comparisons:\n"
                + "\n".join(mismatch_details)
            )

        # Additional verification: check that we have some non-null labels
        non_null_decoded = decoded_labels[decoded_labels.notna()]
        non_null_graph = graph_labels[graph_labels.notna()]

        if len(non_null_decoded) == 0:
            raise ValueError("Should have some non-null decoded labels")
        if len(non_null_graph) == 0:
            raise ValueError("Should have some non-null graph labels")
        if len(non_null_decoded) != len(non_null_graph):
            raise ValueError("Non-null label counts should match")

        return True

    def __repr__(self) -> str:
        """String representation of the NapistuData object."""
        summary = self.get_summary(NAPISTU_DATA_SUMMARY_TYPES.DETAILED)
        summary_table = format_summary(summary)
        return summary_table.to_string(index=False)


def _apply_optional_nd_args(
    params: Dict[str, Any],
    x: torch.Tensor,
    edge_attr: torch.Tensor,
    y: Optional[torch.Tensor],
    vertex_feature_names: Optional[List[str]],
    edge_feature_names: Optional[List[str]],
    vertex_feature_name_aliases: Optional[Dict[str, str]],
    edge_feature_name_aliases: Optional[Dict[str, str]],
    ng_vertex_names: Optional[pd.Series],
    ng_edge_names: Optional[pd.DataFrame],
    splitting_strategy: Optional[str],
    labeling_manager: Optional[LabelingManager],
    relation_type: Optional[torch.Tensor],
    relation_manager: Optional[LabelingManager],
) -> None:
    """
    Apply and validate optional NapistuData arguments.

    Parameters
    ----------
    params : Dict[str, Any]
        Dictionary to update with validated optional parameters
    x : torch.Tensor
        Node feature matrix (for validation of vertex_feature_names length)
    edge_attr : torch.Tensor
        Edge feature matrix (for validation of edge_feature_names length)
    y : Optional[torch.Tensor]
        Node labels tensor
    vertex_feature_names : Optional[List[str]]
        Names of vertex features
    edge_feature_names : Optional[List[str]]
        Names of edge features
    vertex_feature_name_aliases : Optional[Dict[str, str]]
        Mapping from vertex feature names to their canonical names
    edge_feature_name_aliases : Optional[Dict[str, str]]
        Mapping from edge feature names to their canonical names
    ng_vertex_names : Optional[pd.Series]
        Minimal vertex names from the original NapistuGraph
    ng_edge_names : Optional[pd.DataFrame]
        Minimal edge names from the original NapistuGraph
    splitting_strategy : Optional[str]
        Strategy used to split the data into train/test/val sets
    labeling_manager : Optional[LabelingManager]
        Labeling manager used to encode the labels
    relation_type : Optional[torch.Tensor]
        Relation type tensor
    relation_manager : Optional[LabelingManager]
        Relation manager used to encode relation types

    Returns
    -------
    None
        Modifies params dict in place
    """
    if y is not None:
        if not isinstance(y, torch.Tensor):
            raise ValueError("if provided, y (node labels) must be a torch.Tensor")
        params[PYG.Y] = y

    if vertex_feature_names is not None:
        if not isinstance(vertex_feature_names, list):
            raise ValueError(
                "if provided, vertex_feature_names must be a list of strings"
            )
        if len(vertex_feature_names) != x.shape[1]:
            raise ValueError(
                "if provided, vertex_feature_names must be a list of strings with the same length as the number of columns in x"
            )
        params[NAPISTU_DATA.VERTEX_FEATURE_NAMES] = vertex_feature_names

    if vertex_feature_name_aliases is not None:
        if not isinstance(vertex_feature_name_aliases, dict):
            raise ValueError("if provided, vertex_feature_name_aliases must be a dict")
        params[NAPISTU_DATA.VERTEX_FEATURE_NAME_ALIASES] = vertex_feature_name_aliases

    if edge_feature_name_aliases is not None:
        if not isinstance(edge_feature_name_aliases, dict):
            raise ValueError("if provided, edge_feature_name_aliases must be a dict")
        params[NAPISTU_DATA.EDGE_FEATURE_NAME_ALIASES] = edge_feature_name_aliases

    if edge_feature_names is not None:
        if not isinstance(edge_feature_names, list):
            raise ValueError(
                "if provided, edge_feature_names must be a list of strings"
            )
        if len(edge_feature_names) != edge_attr.shape[1]:
            raise ValueError(
                "if provided, edge_feature_names must be a list of strings with the same length as the number of columns in edge_attr"
            )
        params[NAPISTU_DATA.EDGE_FEATURE_NAMES] = edge_feature_names

    if ng_vertex_names is not None:
        if not isinstance(ng_vertex_names, pd.Series):
            raise ValueError("if provided, ng_vertex_names must be a pd.Series")
        if ng_vertex_names.name != NAPISTU_GRAPH_VERTICES.NAME:
            raise ValueError(
                "if provided, ng_vertex_names must have a name attribute of 'name'"
            )
        params[NAPISTU_DATA.NG_VERTEX_NAMES] = ng_vertex_names

    if ng_edge_names is not None:
        if not isinstance(ng_edge_names, pd.DataFrame):
            raise ValueError("if provided, ng_edge_names must be a pd.DataFrame")
        if ng_edge_names.shape[1] != 2:
            raise ValueError("if provided, ng_edge_names must have 2 columns")
        EXPECTED_EDGE_NAMES = [NAPISTU_GRAPH_EDGES.FROM, NAPISTU_GRAPH_EDGES.TO]
        if not all(col in ng_edge_names.columns for col in EXPECTED_EDGE_NAMES):
            raise ValueError(
                f"if provided, ng_edge_names must have columns '{EXPECTED_EDGE_NAMES}'"
            )
        params[NAPISTU_DATA.NG_EDGE_NAMES] = ng_edge_names

    if splitting_strategy is not None:
        if not isinstance(splitting_strategy, str):
            raise ValueError("if provided, splitting_strategy must be a string")
        if splitting_strategy not in VALID_SPLITTING_STRATEGIES:
            raise ValueError(
                f"if provided, splitting_strategy must be one of {VALID_SPLITTING_STRATEGIES}"
            )
        params[NAPISTU_DATA.SPLITTING_STRATEGY] = splitting_strategy

    if labeling_manager is not None:
        if not isinstance(labeling_manager, LabelingManager):
            raise ValueError(
                "if provided, labeling_manager must be a LabelingManager object"
            )
        if y is None:
            logger.warning(
                "Labeling manager provided but no labels are present in the data. The labeling manager will be ignored."
            )
        else:
            params[NAPISTU_DATA.LABELING_MANAGER] = labeling_manager

    if relation_type is not None:
        if not isinstance(relation_type, torch.Tensor):
            raise ValueError("if provided, relation_type must be a torch.Tensor")
        params[NAPISTU_DATA.RELATION_TYPE] = relation_type

    if relation_manager is not None:
        if not isinstance(relation_manager, LabelingManager):
            raise ValueError(
                "if provided, relation_manager must be a LabelingManager object"
            )
        params[NAPISTU_DATA.RELATION_MANAGER] = relation_manager


def _validate_required_nd_args(
    x: torch.Tensor,
    edge_index: torch.Tensor,
    edge_attr: torch.Tensor,
    name: str,
) -> None:
    """
    Validate required NapistuData arguments for correct types.

    Parameters
    ----------
    x : torch.Tensor
        Node feature matrix
    edge_index : torch.Tensor
        Graph connectivity tensor
    edge_attr : torch.Tensor
        Edge feature matrix
    name : str
        Name of the NapistuData object

    Raises
    ------
    TypeError
        If any argument is not of the expected type
    ValueError
        If name is empty
    """
    if not isinstance(x, torch.Tensor):
        raise TypeError(f"x must be a torch.Tensor, got {type(x)}")
    if not isinstance(edge_index, torch.Tensor):
        raise TypeError(f"edge_index must be a torch.Tensor, got {type(edge_index)}")
    if not isinstance(edge_attr, torch.Tensor):
        raise TypeError(f"edge_attr must be a torch.Tensor, got {type(edge_attr)}")
    if not isinstance(name, str):
        raise TypeError(f"name must be a str, got {type(name)}")
    if not name:
        raise ValueError("name cannot be an empty string")
