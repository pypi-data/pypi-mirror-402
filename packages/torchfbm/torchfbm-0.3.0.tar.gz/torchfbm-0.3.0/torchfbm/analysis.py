"""Analysis utilities for Fractional Brownian Motion.

This module provides tools for analyzing and visualizing fBm/fGn processes,
including covariance matrix construction, autocorrelation plotting, and
spectral analysis.

Example:
    >>> from torchfbm.analysis import covariance_matrix, spectral_scaling_factor
    >>> cov = covariance_matrix(n=100, H=0.7)
    >>> freqs = torch.linspace(0.01, 0.5, 50)
    >>> scaling = spectral_scaling_factor(freqs, H=0.7)
"""

import torch
import torch.fft
import math


def covariance_matrix(
    n: int, H: float, device: str = "cpu", return_numpy: bool = False
) -> torch.Tensor:
    """Construct the exact autocovariance matrix for fractional Gaussian noise.

    Builds the symmetric Toeplitz covariance matrix $\\Sigma$ for fGn, where each
    entry $\\Sigma_{ij} = \\gamma(|i-j|)$ is determined by the autocovariance function.

    Based on Mandelbrot & Van Ness (1968).

    The autocovariance function for fGn is:

    $$\\gamma(k) = \\frac{1}{2}\\left(|k+1|^{2H} - 2|k|^{2H} + |k-1|^{2H}\\right)$$

    This matrix is positive semi-definite and can be Cholesky decomposed for
    exact fGn generation.

    Args:
        n: Size of the covariance matrix (n Ã— n).
        H: Hurst exponent in $(0, 1)$. Controls the correlation structure:
            - $H < 0.5$: Anti-persistent (negative correlations)
            - $H = 0.5$: Standard Brownian motion (independent increments)
            - $H > 0.5$: Persistent (positive correlations)
        device: Torch device for computation ('cpu' or 'cuda').
        return_numpy: If True, returns a NumPy array instead of torch.Tensor.

    Returns:
        Symmetric positive semi-definite Toeplitz matrix of shape $(n, n)$.

    Example:
        >>> cov = covariance_matrix(n=50, H=0.8)
        >>> assert cov.shape == (50, 50)
        >>> assert torch.allclose(cov, cov.T)  # Symmetric
    """
    from .generators import _autocovariance

    gamma = _autocovariance(H, n, torch.device(device), dtype=torch.float32)

    # Construct Toeplitz matrix from autocovariance
    idx = torch.arange(n, device=device)
    distance_matrix = torch.abs(idx.unsqueeze(0) - idx.unsqueeze(1))
    result = gamma[distance_matrix]
    return result.cpu().numpy() if return_numpy else result


def plot_acf(x: torch.Tensor, max_lag: int = 100, title: str = "Autocorrelation") -> None:
    """Plot the autocorrelation function of a time series.

    Computes and visualizes the ACF using FFT-based circular convolution for
    efficient $O(N \\log N)$ computation.

    The autocorrelation at lag $k$ is defined as:

    $$\\rho(k) = \\frac{\\text{Cov}(X_t, X_{t+k})}{\\text{Var}(X_t)}$$

    For fGn with Hurst exponent $H$, the theoretical ACF decays as:

    $$\\rho(k) \\sim H(2H-1)|k|^{2H-2} \\quad \\text{as } k \\to \\infty$$

    Note:
        Requires matplotlib to be installed separately.

    Args:
        x: Input time series tensor. Last dimension is treated as time axis.
        max_lag: Maximum lag to display in the plot.
        title: Title for the plot.

    Returns:
        None. Displays the plot using matplotlib.

    Example:
        >>> from torchfbm import fbm
        >>> path = fbm(n=1000, H=0.7, size=(1,))
        >>> plot_acf(path.squeeze(), max_lag=50, title="fBm ACF (H=0.7)")
    """
    try:
        import matplotlib.pyplot as plt
    except ImportError:
        print("Matplotlib not installed.")
        return

    # FFT-based ACF computation
    n = x.shape[-1]
    x_centered = x - x.mean()
    next_pow2 = 2 ** (2 * n - 1).bit_length()
    # Zero-pad to next power of 2 for efficiency and avoid circular correlation
    x_padded = torch.nn.functional.pad(x_centered, (0, next_pow2 - n))
    fft = torch.fft.fft(x_padded)
    
    power = fft * fft.conj()
    acf = torch.fft.ifft(power).real
    acf = acf[:max_lag]
    acf = acf / acf[0]
    acf_np = acf.detach().cpu().numpy()
    plt.figure(figsize=(10, 4))
    plt.bar(range(len(acf_np)), acf_np, width=0.5)
    plt.title(title)
    plt.xlabel("Lag")
    plt.ylabel("Correlation")
    plt.grid(True, alpha=0.3)
    plt.show()


def spectral_scaling_factor(
    f: torch.Tensor, H: float, return_numpy: bool = False
) -> torch.Tensor:
    """Compute the spectral scaling factor for fBm spectral synthesis.

    Returns the amplitude scaling $A(f)$ needed to synthesize fBm via spectral
    methods. Based on the $1/f^\\beta$ power spectral density law.

    Based on Flandrin (1989) and Mandelbrot & Van Ness (1968).

    The power spectral density of fBm scales as:

    $$S(f) \\propto \\frac{1}{|f|^\\beta} \\quad \\text{where } \\beta = 2H + 1$$

    The amplitude scaling is therefore:

    $$A(f) = \\frac{1}{|f|^{(H + 0.5)}}$$

    This is used in the spectral synthesis method where fBm is generated by
    filtering white noise with this frequency-dependent amplitude.

    Args:
        f: Frequency tensor. Can be any shape; scaling is applied element-wise.
        H: Hurst exponent in $(0, 1)$.
        return_numpy: If True, returns a NumPy array instead of torch.Tensor.

    Returns:
        Spectral scaling factors with same shape as input `f`.
        DC component ($f=0$) is set to zero.

    Example:
        >>> freqs = torch.linspace(0.01, 0.5, 100)
        >>> scaling = spectral_scaling_factor(freqs, H=0.7)
        >>> # Use for spectral synthesis: fft_coeffs = white_noise * scaling
    """
    beta = 2 * H + 1
    safe_f = torch.where(f == 0, torch.ones_like(f), f)
    scaling = 1.0 / (torch.abs(safe_f) ** (beta / 2.0))
    scaling[f == 0] = 0  # Zero DC component
    return scaling.cpu().numpy() if return_numpy else scaling
