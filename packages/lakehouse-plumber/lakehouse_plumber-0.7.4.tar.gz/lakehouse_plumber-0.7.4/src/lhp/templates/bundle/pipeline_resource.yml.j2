# Generated by LakehousePlumber - Bundle Resource for {{ pipeline_name }}

resources:
  pipelines:
    {{ pipeline_name }}_pipeline:
      name: {{ pipeline_name }}_pipeline
      
      # Catalog and Schema for the pipeline
{% if catalog and schema %}
      catalog: {{ catalog }}
      schema: {{ schema }}
{% else %}
      # Using databricks.yml variables (not defined in pipeline config)
      catalog: ${var.default_pipeline_catalog}
      schema: ${var.default_pipeline_schema}
{% endif %}
      
      serverless: {{ pipeline_config.serverless | lower }}
{%- if not pipeline_config.serverless and pipeline_config.get('clusters') %}

      # Compute clusters configuration
      clusters:
{% for cluster in pipeline_config.clusters %}
        - label: {{ cluster.label }}
          node_type_id: {{ cluster.node_type_id }}
{% if cluster.get('driver_node_type_id') %}
          driver_node_type_id: {{ cluster.driver_node_type_id }}
{% endif %}
{% if cluster.get('policy_id') %}
          policy_id: {{ cluster.policy_id }}
{% endif %}
{% if cluster.get('autoscale') %}
          autoscale:
            min_workers: {{ cluster.autoscale.min_workers }}
            max_workers: {{ cluster.autoscale.max_workers }}
{% if cluster.autoscale.get('mode') %}
            mode: {{ cluster.autoscale.mode }}
{% endif %}
{% endif %}
{% endfor %}
{%- endif %}

      libraries:
        - glob:
            include: ${workspace.file_path}/generated/${bundle.target}/{{ pipeline_name }}/**
      
      root_path: ${workspace.file_path}/generated/${bundle.target}/{{ pipeline_name }}
      
      configuration:
        bundle.sourcePath: ${workspace.file_path}/generated/${bundle.target}
{%- if pipeline_config.get('continuous') %}
      
      continuous: {{ pipeline_config.continuous | lower }}
{%- endif %}
{%- if pipeline_config.get('photon') %}
      
      photon: {{ pipeline_config.photon | lower }}
{%- endif %}
{%- if pipeline_config.get('edition') and not pipeline_config.serverless %}
      
      edition: {{ pipeline_config.edition }}
{%- endif %}
{%- if pipeline_config.get('channel') %}
      
      channel: {{ pipeline_config.channel }}
{%- endif %}
{%- if pipeline_config.get('notifications') %}
      
      notifications:
{% for notification in pipeline_config.notifications %}
        - email_recipients:
{% for email in notification.email_recipients %}
            - {{ email }}
{% endfor %}
          alerts:
{% for alert in notification.alerts %}
            - {{ alert }}
{% endfor %}
{% endfor %}
{%- endif %}
{%- if pipeline_config.get('tags') %}
      
      tags:
{% for key, value in pipeline_config.tags.items() %}
        {{ key }}: {{ value }}
{% endfor %}
{%- endif %}
{%- if pipeline_config.get('event_log') %}
      
      event_log:
        name: {{ pipeline_config.event_log.name }}
        schema: {{ pipeline_config.event_log.schema }}
        catalog: {{ pipeline_config.event_log.catalog }}
{%- endif %}

      # Additional pipeline configuration options 
      # Add to your pipeline_config-<env>.yaml as needed and pass the file path through the --pipeline-config flag:
      # You can use substitutions file to define the values for the pipeline configuration using the following syntax:
      # {variable_name}
 
      # Compute clusters configuration (alternative to serverless)
      # clusters:
      #   - label: default
      #     node_type_id: Standard_D16ds_v5
      #     driver_node_type_id: Standard_D32ds_v5
      #     policy_id: 1234ABCD1234ABCD
      #     autoscale:
      #       min_workers: 1
      #       max_workers: 5
      #       mode: ENHANCED
      
      # Enable continuous processing
      # continuous: false
      
   
      # Enable Photon engine only for classic computer not serverless
      # photon: true
      
      # DLT edition (CORE, PRO, ADVANCED)
      # edition: ADVANCED
      
      # Runtime channel (CURRENT, PREVIEW)
      # channel: CURRENT
      
      # Notification settings
      # notifications:
      #   - email_recipients:
      #       - user@databricks.com
      #     alerts:
      #       - on-update-success
      #       - on-update-failure
      #       - on-update-fatal-failure
      #       - on-flow-failure
      
      # Custom tags only for classic computer not serverless (serverless has its own tags through compute policy)
      # tags:
      #   tag1: val1
      
      # Event log configuration
      # event_log:
      #   name: pipeline_evenlog
      #   schema: _meta
      #   catalog: acmi_edw_dev

      # permissions:
      #   - service_principal_name: 2aa4ed8e-0a18-4072-97c6-9c074c8be40d
      #     level: CAN_MANAGE
      #   - user_name: user@example.com
      #     level: CAN_RUN
      #   - group_name: data-engineers 
      #     level: CAN_VIEW
      
      # # Pipeline-level run_as
      # run_as:
      #   service_principal_name: 2aa4ed8e-0a18-4072-97c6-9c074c8be40d OR "{SP_NAME}" (if using substitutions file)