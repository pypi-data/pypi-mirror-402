{% if mode == "cdc" %}
{# 
   CDC mode: Always create the streaming table first, then configure CDC flow
   CDC flows require their own dedicated table
#}
# Create the streaming table for CDC
dp.create_streaming_table(
    name="{{ full_table_name }}",
    comment="{{ comment }}"
    {%- if properties %},
    table_properties={{ properties | tojson }}
    {%- endif %}
    {%- if spark_conf %},
    spark_conf={{ spark_conf | tojson }}
    {%- endif %}
    {%- if partitions %},
    partition_cols={{ partitions | tojson }}
    {%- endif %}
    {%- if cluster_by %},
    cluster_by={{ cluster_by | tojson }}
    {%- endif %}
    {%- if table_path %},
    path="{{ table_path }}"
    {%- endif %}
    {%- if schema %},
    schema="""{{ schema }}"""
    {%- endif %}
    {%- if row_filter %},
    row_filter="{{ row_filter }}"
    {%- endif %}
    {%- if temporary %},
    temporary={{ temporary | string | title }}
    {%- endif %}
)

# CDC mode using auto_cdc
dp.create_auto_cdc_flow(
    target="{{ full_table_name }}",
    source="{{ source_view }}",
    keys={{ cdc_config['keys'] | tojson }},
    {% if cdc_config['sequence_by'] -%}
    {%- if cdc_config['sequence_by'] is string -%}
    sequence_by="{{ cdc_config['sequence_by'] }}",
    {%- else -%}
    sequence_by=struct({{ cdc_config['sequence_by'] | map('tojson') | join(', ') }}),
    {%- endif -%}
    {% endif %}
    stored_as_scd_type={{ cdc_config['scd_type'] | default(1) }},
    {% if cdc_config['scd_type'] == 2 and cdc_config.get('track_history_column_list') %}track_history_column_list={{ cdc_config['track_history_column_list'] | tojson }},
    {% elif cdc_config['scd_type'] == 2 and cdc_config.get('track_history_except_column_list') %}track_history_except_column_list={{ cdc_config['track_history_except_column_list'] | tojson }},
    {% endif %}
    {% if cdc_config['column_list'] %}column_list={{ cdc_config['column_list'] | tojson }},
    {% endif %}
    {% if cdc_config['except_column_list'] %}except_column_list={{ cdc_config['except_column_list'] | tojson }},
    {% endif %}
    {% if cdc_config['ignore_null_updates'] is not none %}ignore_null_updates={{ 'True' if cdc_config['ignore_null_updates'] else 'False' }},
    {% endif %}
    {% if cdc_config['apply_as_deletes'] %}apply_as_deletes="{{ cdc_config['apply_as_deletes'] }}",
    {% endif %}
    {% if cdc_config['apply_as_truncates'] %}apply_as_truncates="{{ cdc_config['apply_as_truncates'] }}"
    {% endif %}
)

{% elif mode == "snapshot_cdc" %}
{# 
   Snapshot CDC mode: Always create the streaming table first, then configure snapshot CDC flow
   Snapshot CDC flows require their own dedicated table
#}
{% if source_function_code %}
# Snapshot function embedded directly in generated code
{{ source_function_code }}

{% endif %}
# Create the streaming table for snapshot CDC
dp.create_streaming_table(
    name="{{ full_table_name }}",
    comment="{{ comment }}"
    {%- if properties %},
    table_properties={{ properties | tojson }}
    {%- endif %}
    {%- if spark_conf %},
    spark_conf={{ spark_conf | tojson }}
    {%- endif %}
    {%- if partitions %},
    partition_cols={{ partitions | tojson }}
    {%- endif %}
    {%- if cluster_by %},
    cluster_by={{ cluster_by | tojson }}
    {%- endif %}
    {%- if table_path %},
    path="{{ table_path }}"
    {%- endif %}
    {%- if schema %},
    schema="""{{ schema }}"""
    {%- endif %}
    {%- if row_filter %},
    row_filter="{{ row_filter }}"
    {%- endif %}
    {%- if temporary %},
    temporary={{ temporary | string | title }}
    {%- endif %}
)

# Snapshot CDC mode using create_auto_cdc_from_snapshot_flow
dp.create_auto_cdc_from_snapshot_flow(
    target="{{ full_table_name }}",
    {% if source_function_name %}source={{ source_function_name }},
    {% else %}source="{{ snapshot_cdc_config['source'] }}",
    {% endif %}
    keys={{ snapshot_cdc_config['keys'] | tojson }},
    stored_as_scd_type={{ snapshot_cdc_config['stored_as_scd_type'] | default(1) }}
    {%- if snapshot_cdc_config.get('track_history_column_list') %},
    track_history_column_list={{ snapshot_cdc_config['track_history_column_list'] | tojson }}
    {%- elif snapshot_cdc_config.get('track_history_except_column_list') %},
    track_history_except_column_list={{ snapshot_cdc_config['track_history_except_column_list'] | tojson }}
    {%- endif %}
)

{% else %}
{# 
   Standard streaming table: Create table first (if needed), then append flow(s)
#}
{% if create_table %}
# Create the streaming table
dp.create_streaming_table(
    name="{{ full_table_name }}",
    comment="{{ comment }}"
    {%- if properties %},
    table_properties={{ properties | tojson }}
    {%- endif %}
    {%- if spark_conf %},
    spark_conf={{ spark_conf | tojson }}
    {%- endif %}
    {%- if partitions %},
    partition_cols={{ partitions | tojson }}
    {%- endif %}
    {%- if cluster_by %},
    cluster_by={{ cluster_by | tojson }}
    {%- endif %}
    {%- if table_path %},
    path="{{ table_path }}"
    {%- endif %}
    {%- if schema %},
    schema="""{{ schema }}"""
    {%- endif %}
    {%- if row_filter %},
    row_filter="{{ row_filter }}"
    {%- endif %}
    {%- if temporary %},
    temporary={{ temporary | string | title }}
    {%- endif %}
)

{% endif %}

# Define append flow(s)
{% if action_metadata %}
{% for action_meta in action_metadata %}
@dp.append_flow(
    target="{{ full_table_name }}",
    name="{{ action_meta.flow_name }}",
    {% if action_meta.once %}once=True,{% endif %}
    comment="{{ action_meta.description }}"
)
{% if expect_all %}
@dp.expect_all({{ expect_all | tojson }})
{% endif %}
{% if expect_all_or_drop %}
@dp.expect_all_or_drop({{ expect_all_or_drop | tojson }})
{% endif %}
{% if expect_all_or_fail %}
@dp.expect_all_or_fail({{ expect_all_or_fail | tojson }})
{% endif %}
def {{ action_meta.flow_name }}():
    """{{ action_meta.description }}"""
    {% if action_meta.readMode == "batch" or (action_meta.readMode is none and readMode == "batch") %}
    # Batch mode
    df = spark.read.table("{{ action_meta.source_view }}")
    {% else %}
    # Streaming flow
    df = spark.readStream.table("{{ action_meta.source_view }}")
    {% endif %}
    {% if add_operational_metadata %}
    
    # Add operational metadata columns
    {% for col_name, expression in metadata_columns.items()|sort %}
    df = df.withColumn('{{ col_name }}', {{ expression }})
    {% endfor %}
    {% endif %}
    
    return df
{% endfor %}
{% elif source_views %}
{# Backward compatibility: use legacy source_views structure #}
{% for source_view in source_views %}
@dp.append_flow(
    target="{{ full_table_name }}",
    name="{{ flow_name }}{% if source_views|length > 1 %}_{{ loop.index }}{% endif %}",
    {% if once %}once=True,{% endif %}
    comment="{{ description }} from {{ source_view }}"
)
{% if expect_all %}
@dp.expect_all({{ expect_all | tojson }})
{% endif %}
{% if expect_all_or_drop %}
@dp.expect_all_or_drop({{ expect_all_or_drop | tojson }})
{% endif %}
{% if expect_all_or_fail %}
@dp.expect_all_or_fail({{ expect_all_or_fail | tojson }})
{% endif %}
def {{ flow_name }}{% if source_views|length > 1 %}_{{ loop.index }}{% endif %}():
    """{{ description }} from {{ source_view }}"""
    {% if readMode == "batch" %}
    # Batch mode
    df = spark.read.table("{{ source_view }}")
    {% else %}
    # Streaming flow
    df = spark.readStream.table("{{ source_view }}")
    {% endif %}
    {% if add_operational_metadata %}
    
    # Add operational metadata columns
    {% for col_name, expression in metadata_columns.items()|sort %}
    df = df.withColumn('{{ col_name }}', {{ expression }})
    {% endfor %}
    {% endif %}
    
    return df
{% endfor %}
{% else %}
# No source views provided for append flow
{% endif %}
{% endif %}