"""
Tests for BundleManager database extraction functionality.

Tests the new Python-based catalog/schema extraction logic including:
- Python file regex pattern extraction  
- Database info extraction from generated Python files
- Template integration with dynamic values from Python files
"""

import pytest
import tempfile
import shutil
from pathlib import Path

from lhp.bundle.manager import BundleManager


class TestBundleDatabaseExtraction:
    """Test suite for Python-based catalog/schema extraction functionality."""

    def setup_method(self):
        """Set up test environment for each test."""
        self.temp_dir = Path(tempfile.mkdtemp())
        self.project_root = self.temp_dir / "test_project"
        self.project_root.mkdir()
        self.manager = BundleManager(self.project_root)

    def teardown_method(self):
        """Clean up test environment after each test."""
        shutil.rmtree(self.temp_dir)

    # ============ PYTHON FILE REGEX EXTRACTION TESTS ============

    def test_extract_database_patterns_streaming_table(self):
        """Should extract database patterns from streaming table creation."""
        python_content = '''
# Create the streaming table
dp.create_streaming_table(
    name="acmi_edw_dev.edw_bronze.customer",
    comment="Streaming table: customer",
    table_properties={"PII": "true"})
'''
        
        patterns = self.manager._extract_database_patterns(python_content)
        assert patterns == ["acmi_edw_dev.edw_bronze"]

    def test_extract_database_patterns_materialized_view(self):
        """Should extract database patterns from materialized view creation."""
        python_content = '''
@dp.materialized_view(
    name="analytics_prod.gold.customer_summary",
    comment="Customer summary materialized view",
    table_properties={})
def customer_summary():
    return df
'''
        
        patterns = self.manager._extract_database_patterns(python_content)
        assert patterns == ["analytics_prod.gold"]

    def test_extract_database_patterns_multiple_tables(self):
        """Should extract multiple database patterns from same file."""
        python_content = '''
dp.create_streaming_table(
    name="dev_catalog.bronze.orders",
    comment="Orders table")

@dp.materialized_view(
    name="dev_catalog.gold.daily_summary",
    comment="Daily summary view")
def daily_summary():
    return df

dp.create_streaming_table(
    name="dev_catalog.bronze.customers",
    comment="Customers table")
'''
        
        patterns = self.manager._extract_database_patterns(python_content)
        assert set(patterns) == {"dev_catalog.bronze", "dev_catalog.gold"}

    def test_extract_database_from_python_files_integration(self):
        """Should extract database info from actual Python files end-to-end."""
        # Create mock output directory structure
        output_dir = self.temp_dir / "generated"
        pipeline_dir = output_dir / "test_pipeline"
        pipeline_dir.mkdir(parents=True)
        
        # Create Python file with table creation patterns
        python_file = pipeline_dir / "customer_bronze.py"
        python_content = '''
# Generated by LakehousePlumber
from pyspark import pipelines as dp

dp.create_streaming_table(
    name="acmi_edw_prod.edw_bronze.customer",
    comment="Customer bronze table")

@dp.materialized_view(
    name="acmi_edw_prod.edw_bronze.customer_summary",
    comment="Customer summary")
def customer_summary():
    return df
'''
        python_file.write_text(python_content)
        
        # Test the extraction
        result = self.manager._extract_database_from_python_files("test_pipeline", output_dir)
        
        assert result["catalog"] == "acmi_edw_prod"
        assert result["schema"] == "edw_bronze"

    def test_extract_database_from_python_files_no_directory(self):
        """Should return defaults when pipeline directory doesn't exist."""
        output_dir = self.temp_dir / "generated"
        
        result = self.manager._extract_database_from_python_files("nonexistent_pipeline", output_dir)
        
        assert result["catalog"] == "main"
        assert result["schema"] == "lhp_${bundle.target}"

    def test_parse_resolved_database_string(self):
        """Should parse already-resolved database strings correctly."""
        # Test normal case
        result = self.manager._parse_resolved_database_string("catalog_prod.schema_bronze")
        assert result["catalog"] == "catalog_prod"
        assert result["schema"] == "schema_bronze"
        
        # Test invalid cases
        result = self.manager._parse_resolved_database_string("no_dot_value")
        assert result["catalog"] == "main"
        assert result["schema"] == "lhp_${bundle.target}"
        
        result = self.manager._parse_resolved_database_string("")
        assert result["catalog"] == "main"
        assert result["schema"] == "lhp_${bundle.target}"

    def test_generate_resource_file_content_end_to_end(self):
        """Should generate bundle resource content from Python files end-to-end."""
        # Create mock output directory structure with Python files
        output_dir = self.temp_dir / "generated"
        pipeline_dir = output_dir / "bronze_load"
        pipeline_dir.mkdir(parents=True)
        
        # Create Python file that would be generated by LHP
        python_file = pipeline_dir / "customer_bronze.py"
        python_content = '''
# Generated by LakehousePlumber
from pyspark import pipelines as dp

dp.create_streaming_table(
    name="analytics_prod.bronze_layer.customer",
    comment="Customer bronze table",
    table_properties={"delta.autoOptimize.optimizeWrite": "true"})

@dp.materialized_view(
    name="analytics_prod.bronze_layer.customer_summary",
    comment="Customer summary")
def customer_summary():
    return df
'''
        python_file.write_text(python_content)
        
        # Test the complete workflow
        content = self.manager.generate_resource_file_content("bronze_load", output_dir, "dev")
        
        # Verify the bundle resource content uses variable references (new behavior)
        assert "catalog: ${var.default_pipeline_catalog}" in content
        assert "schema: ${var.default_pipeline_schema}" in content
        assert "bronze_load_pipeline:" in content
        assert "Generated by LakehousePlumber" in content

    def test_extract_database_patterns_no_matches(self):
        """Should return empty list when no database patterns found."""
        python_content = '''
# Regular Python code without DLT table creation
import pandas as pd

def process_data():
    return pd.DataFrame({"col1": [1, 2, 3]})
'''
        
        patterns = self.manager._extract_database_patterns(python_content)
        assert patterns == []

    def test_extract_database_patterns_mixed_content(self):
        """Should extract only valid patterns from mixed content."""
        python_content = '''
# Some DLT and some regular code
from pyspark import pipelines as dp

# This should be extracted
dp.create_streaming_table(
    name="valid_catalog.valid_schema.table1",
    comment="Valid table")

# Regular function - should be ignored
def regular_function():
    return "not a table"

# This should also be extracted  
@dp.materialized_view(
    name="another_catalog.another_schema.table2",
    comment="Another table")
def another_table():
    return df

# Invalid pattern - should be ignored
some_var = "not.a.table.pattern"
'''
        
        patterns = self.manager._extract_database_patterns(python_content)
        assert set(patterns) == {"valid_catalog.valid_schema", "another_catalog.another_schema"} 