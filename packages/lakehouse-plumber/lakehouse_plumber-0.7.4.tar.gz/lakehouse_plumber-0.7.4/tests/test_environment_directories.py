"""Test suite for environment-specific directory structure changes."""

import os
import tempfile
import shutil
import pytest
import yaml
from pathlib import Path
from click.testing import CliRunner
from unittest.mock import patch, Mock

from lhp.cli.main import cli
from lhp.bundle.manager import BundleManager
from lhp.core.state_manager import StateManager


class TestEnvironmentDirectories:
    """Test environment-based directory structure."""
    
    def setup_method(self):
        """Set up test environment."""
        self.temp_dir = Path(tempfile.mkdtemp())
        self.runner = CliRunner()
        
    def teardown_method(self):
        """Clean up test environment."""
        shutil.rmtree(self.temp_dir)
    

    # Test 2: Environment-Specific Resource Directories
    def test_bundle_resources_created_in_environment_directory(self):
        """Verify bundle resources created in resources/lhp/{env}/."""
        project_root = self.temp_dir / "test_project"
        project_root.mkdir()
        
        # Create databricks.yml to enable bundle support
        (project_root / "databricks.yml").write_text("bundle:\n  name: test_bundle")
        
        # Create generated files in environment directory
        generated_dir = project_root / "generated" / "dev"
        pipeline_dir = generated_dir / "test_pipeline"
        pipeline_dir.mkdir(parents=True)
        (pipeline_dir / "test.py").write_text("""
from pyspark import pipelines as dp

@dp.materialized_view(name="test_catalog.test_schema.test_table")
def test_table():
    pass
""")
        
        # Run bundle sync
        manager = BundleManager(project_root)
        manager.sync_resources_with_generated_files(generated_dir, "dev")
        
        # Verify resource file created in root-level directory (new behavior)
        resource_file = project_root / "resources" / "lhp" / "test_pipeline.pipeline.yml"
        assert resource_file.exists(), "Resource file should be created in root directory"
        
        # Verify content
        content = resource_file.read_text()
        assert "Generated by LakehousePlumber" in content
        assert "test_pipeline" in content
        # Note: Environment no longer in header (environment-agnostic files)
    
    # Test 3: Pipeline Template Environment Paths
    def test_pipeline_template_includes_environment_paths(self):
        """Verify pipeline.yml contains environment in paths."""
        project_root = self.temp_dir / "test_project"
        project_root.mkdir()
        
        # Create generated directory structure
        generated_dir = project_root / "generated" / "staging"
        pipeline_dir = generated_dir / "data_pipeline"
        pipeline_dir.mkdir(parents=True)
        (pipeline_dir / "transform.py").write_text("""
from pyspark import pipelines as dp

dp.create_streaming_table(name="staging_catalog.staging_schema.staging_table")
""")
        
        # Generate resource file
        manager = BundleManager(project_root)
        content = manager.generate_resource_file_content("data_pipeline", generated_dir, "staging")
        
        # Parse YAML and verify paths
        parsed = yaml.safe_load(content)
        pipeline_config = parsed["resources"]["pipelines"]["data_pipeline_pipeline"]
        
        # Check glob include path (now uses bundle.target variable)
        libraries = pipeline_config["libraries"]
        assert len(libraries) == 1
        glob_include = libraries[0]["glob"]["include"]
        assert glob_include == "${workspace.file_path}/generated/${bundle.target}/data_pipeline/**"
        
        # Check root_path (now uses bundle.target variable)
        root_path = pipeline_config["root_path"]
        assert root_path == "${workspace.file_path}/generated/${bundle.target}/data_pipeline"
        
        # Check bundle.sourcePath (now uses bundle.target variable)
        source_path = pipeline_config["configuration"]["bundle.sourcePath"]
        assert source_path == "${workspace.file_path}/generated/${bundle.target}"
    



    # Test 7: Bundle Sync with Environment
    def test_bundle_sync_uses_correct_environment_directory(self):
        """Verify BundleManager syncs to correct env directory."""
        project_root = self.temp_dir / "sync_test"
        project_root.mkdir()
        
        # Enable bundle support
        (project_root / "databricks.yml").write_text("bundle:\n  name: sync_test")
        
        # Create files in generated/dev/
        dev_dir = project_root / "generated" / "dev"
        pipeline1_dir = dev_dir / "pipeline1"
        pipeline1_dir.mkdir(parents=True)
        (pipeline1_dir / "flow1.py").write_text("""
from pyspark import pipelines as dp
dp.create_streaming_table(name="dev_catalog.dev_schema.table1")
""")
        
        pipeline2_dir = dev_dir / "pipeline2"
        pipeline2_dir.mkdir(parents=True)
        (pipeline2_dir / "flow2.py").write_text("""
from pyspark import pipelines as dp
@dp.materialized_view(name="dev_catalog.dev_schema.table2")
def table2():
    pass
""")
        
        # Create files in generated/prod/
        prod_dir = project_root / "generated" / "prod"
        pipeline3_dir = prod_dir / "pipeline3"
        pipeline3_dir.mkdir(parents=True)
        (pipeline3_dir / "flow3.py").write_text("""
from pyspark import pipelines as dp
dp.create_streaming_table(name="prod_catalog.prod_schema.table3")
""")
        
        # Run sync for dev
        manager = BundleManager(project_root)
        manager.sync_resources_with_generated_files(dev_dir, "dev")
        
        # Verify dev resources created at root level (new behavior)
        root_resources = project_root / "resources" / "lhp"
        assert (root_resources / "pipeline1.pipeline.yml").exists()
        assert (root_resources / "pipeline2.pipeline.yml").exists()
        # Note: pipeline3 doesn't exist yet since prod hasn't been generated
        
        # Run sync for prod
        manager.sync_resources_with_generated_files(prod_dir, "prod")
        
        # Verify prod sync behavior (new behavior: orphaned files deleted)
        # pipeline1 and pipeline2 don't exist in prod Python files, so they get deleted (Scenario 3)
        # Only pipeline3 exists in prod, so only it gets created
        assert (root_resources / "pipeline3.pipeline.yml").exists()  # New pipeline created
        assert not (root_resources / "pipeline1.pipeline.yml").exists()  # Deleted (orphaned)
        assert not (root_resources / "pipeline2.pipeline.yml").exists()  # Deleted (orphaned)
    
    # Test 8: State Management with New Paths
    def test_state_manager_tracks_environment_specific_paths(self):
        """Verify state file contains environment-specific paths."""
        project_root = self.temp_dir / "state_test"
        project_root.mkdir()
        
        # Create state manager
        state_manager = StateManager(project_root)
        
        # Track files for different environments
        dev_file = project_root / "generated" / "dev" / "pipeline1" / "flow1.py"
        dev_file.parent.mkdir(parents=True)
        dev_file.write_text("# dev content")
        
        staging_file = project_root / "generated" / "staging" / "pipeline1" / "flow1.py"
        staging_file.parent.mkdir(parents=True)
        staging_file.write_text("# staging content")
        
        # Track the files
        state_manager.track_generated_file(
            generated_path=dev_file,
            source_yaml=project_root / "pipelines" / "flow1.yaml",
            environment="dev",
            pipeline="pipeline1",
            flowgroup="flow1"
        )
        
        state_manager.track_generated_file(
            generated_path=staging_file,
            source_yaml=project_root / "pipelines" / "flow1.yaml",
            environment="staging",
            pipeline="pipeline1",
            flowgroup="flow1"
        )
        
        # Save and reload state
        state_manager.save()
        
        # Load state file and verify paths
        import json
        state_file = project_root / ".lhp_state.json"
        assert state_file.exists()
        
        with open(state_file, 'r') as f:
            state_data = json.load(f)
        
        # Check dev environment
        assert "dev" in state_data["environments"]
        dev_files = state_data["environments"]["dev"]
        assert "generated/dev/pipeline1/flow1.py" in dev_files
        
        # Check staging environment
        assert "staging" in state_data["environments"]
        staging_files = state_data["environments"]["staging"]
        assert "generated/staging/pipeline1/flow1.py" in staging_files
    
    # Test 9: Template Requires Env Parameter
    def test_template_requires_env_parameter(self):
        """Verify templates require env parameter (breaking change)."""
        project_root = self.temp_dir / "template_test"
        project_root.mkdir()
        
        generated_dir = project_root / "generated"
        generated_dir.mkdir()
        
        manager = BundleManager(project_root)
        
        # Test that env is required (should raise TypeError if missing)
        import pytest
        with pytest.raises(TypeError):
            content = manager.generate_resource_file_content("test_pipeline", generated_dir)
        
        # Test with valid env value works correctly
        content = manager.generate_resource_file_content("test_pipeline", generated_dir, "dev")
        
        # Parse YAML and verify it's valid
        parsed = yaml.safe_load(content)
        assert parsed is not None
        assert "resources" in parsed
        
        # Check header doesn't have (None)
        lines = content.split('\n')
        header = lines[0]
        assert "(None)" not in header
        assert "Generated by LakehousePlumber" in header
    
    # Test 10: Resource Directory Migration
    def test_clean_slate_no_migration_required(self):
        """Verify no automatic migration of old structure."""
        project_root = self.temp_dir / "migration_test"
        project_root.mkdir()
        
        # Create old structure
        old_generated = project_root / "generated" / "pipeline1"
        old_generated.mkdir(parents=True)
        (old_generated / "flow.py").write_text("# old generated file")
        
        old_resources = project_root / "resources" / "lhp"
        old_resources.mkdir(parents=True)
        old_resource_file = old_resources / "pipeline1.pipeline.yml"
        old_resource_file.write_text("""
# Generated by LakehousePlumber - Bundle Resource for pipeline1
resources:
  pipelines:
    pipeline1_pipeline:
      catalog: old_catalog
""")
        
        # Create new environment-specific files
        new_generated = project_root / "generated" / "dev" / "pipeline1"
        new_generated.mkdir(parents=True)
        (new_generated / "flow.py").write_text("# new generated file")
        
        # Run bundle sync
        manager = BundleManager(project_root)
        manager.sync_resources_with_generated_files(project_root / "generated" / "dev", "dev")
        
        # Verify old files remain untouched (no automatic migration)
        assert old_resource_file.exists(), "Old resource file should remain"
        assert "old_catalog" in old_resource_file.read_text(), "Old content should be unchanged"
        
        # Verify existing root-level file is preserved (Scenario 1a: DON'T TOUCH)
        # Since old_resource_file has LHP header, it should be preserved unchanged
        assert old_resource_file.exists(), "Root-level resource file should be preserved"
        assert "old_catalog" in old_resource_file.read_text(), "Existing file should remain unchanged"
        
        # Verify no automatic cleanup of old structure
        assert old_generated.exists(), "Old generated directory should remain"
        assert (old_generated / "flow.py").exists(), "Old generated file should remain"
