# Ministudio

**Model-Agnostic AI Video Framework**

*"Make AI video generation as consistent as CSS makes web styling"*

Python package for generating consistent AI videos across different providers.

## Installation

```bash
pip install ministudio
```

With provider support:
```bash
pip install ministudio[vertex-ai]  # Google Vertex AI
pip install ministudio[openai]     # OpenAI Sora
pip install ministudio[all]        # All providers
```

## Quick Start

### Basic Usage
```bash
# Mock provider (no API keys needed)
ministudio --provider mock --concept "Hello World" --action "orb waving"
```

### Real Providers
```bash
# Google Vertex AI
export GCP_PROJECT_ID="your-project-id"
ministudio --provider vertex-ai --concept "Nature" --action "forest growing"

# OpenAI Sora
export OPENAI_API_KEY="your-key"
ministudio --provider openai-sora --concept "Ocean" --action "waves crashing"
```

### Python API
```python
from ministudio import Ministudio

provider = Ministudio.create_provider("mock")
studio = Ministudio(provider=provider)

result = await studio.generate_concept_video(
    concept="Math",
    action="orb solving equations"
)
```

## Features

- **Model-Agnostic**: Swap AI providers without changing code
- **State Management**: Maintain visual consistency across generations
- **Configurable**: Customize all video generation parameters
- **Segmentation**: Generate long-form videos with state persistence
- **Multiple Providers**: Google Vertex AI, OpenAI Sora, Local models, Mock
- **Templates**: Pre-built styles for different use cases
- **API Server**: Self-hosted REST API
- **Docker**: Containerized deployment

## Usage

### Basic Generation
```python
from ministudio import Ministudio, VideoConfig

provider = Ministudio.create_provider("mock")
studio = Ministudio(provider=provider)

result = await studio.generate_concept_video(
    concept="Science",
    action="orb demonstrating physics"
)
```

### Configurable Generation
```python
from ministudio import VideoConfig

config = VideoConfig(
    duration_seconds=12,
    style_name="cinematic",
    mood="dramatic"
)

result = await studio.generate_concept_video(
    concept="Adventure",
    action="hero exploring cave",
    config=config
)
```

### Segmented Videos (Long-form)
```python
segments = [
    {"concept": "Intro", "action": "character enters scene"},
    {"concept": "Action", "action": "character finds treasure", "state_updates": {"character": {"holding": "treasure"}}},
    {"concept": "Climax", "action": "character escapes"}
]

results = await studio.generate_segmented_video(segments, config)
```

## Providers

| Provider | Setup | Status |
|----------|-------|--------|
| Mock | None | Ready |
| Google Vertex AI | `GCP_PROJECT_ID` env var | Ready |
| OpenAI Sora | `OPENAI_API_KEY` env var | Ready |
| Local | Model path | Ready |

## Styles & Templates

### Built-in Styles
- `ghibli`: Studio Ghibli aesthetic
- `cyberpunk`: Neon cyberpunk style
- `cinematic`: Hollywood filmmaking
- `realistic`: Photorealistic

### Templates
- `explainer`: Educational content
- `marketing`: Promotional videos
- `cinematic`: Cinematic scenes

## API Server

Run self-hosted API:
```bash
uvicorn ministudio.api:app --host 0.0.0.0 --port 8000
```

## Docker

```bash
docker build -t ministudio .
docker run -p 8000:8000 ministudio
```

## API Reference

### Ministudio Class
- `generate_concept_video(concept, action, config=None)`
- `generate_segmented_video(segments, base_config=None)`

### VideoConfig Class
- Configurable parameters: duration, aspect_ratio, style, provider, etc.

### Providers
- `Ministudio.create_provider(type, **kwargs)`

## Publishing to PyPI

To publish your package to PyPI:

1. **Install build tools:**
   ```bash
   pip install build twine
   ```

2. **Build the package:**
   ```bash
   python -m build
   ```

3. **Create PyPI account:**
   - Go to https://pypi.org/
   - Create account and verify email

4. **Upload to PyPI:**
   ```bash
   twine upload dist/*
   ```
   Enter your PyPI username and password when prompted.

5. **Test installation:**
   ```bash
   pip install your-package-name
   ```

For test releases, use TestPyPI:
```bash
twine upload --repository testpypi dist/*
pip install --index-url https://test.pypi.org/simple/ your-package-name
```

## Philosophy

Ministudio exists because AI video generation is powerful but inconsistent. We believe every developer should be able to create professional videos, AI should enhance creativity when making videos, open ecosystems beat walled gardens, consistency is programmable with proper state  management, and the best tools get out of the way.

We're building the standard framework for AI video generation - model-agnostic, stateful, and extensible.

## Contributing

We welcome contributions! This guide explains how to get started.

### Development Setup

```bash
git clone https://github.com/aynaash/ministudio.git
cd ministudio
pip install -e .[all]
```

### Code Quality

```bash
# Format
black ministudio/

# Lint  
ruff check ministudio/

# Test
pytest
```

### Adding Providers

Extend `BaseVideoProvider` and implement `generate_video()` method.

### Pull Requests

- Reference issues
- Clear descriptions
- Include tests
- Follow code style

Thank you for contributing to Ministudio! 

## License

MIT License

## Acknowledgments

Inspired by the open-source AI community's work on making AI accessible and consistent.

---

**Made with by the AI video generation community**

*Ready to make AI video generation consistent? Let's build the future together.*
