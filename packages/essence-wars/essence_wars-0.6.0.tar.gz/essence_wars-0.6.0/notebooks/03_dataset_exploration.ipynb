{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Exploration\n",
    "\n",
    "Explore pre-generated MCTS datasets for training agents.\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/christianwissmann85/ai-cardgame/blob/master/notebooks/03_dataset_exploration.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colab setup (uncomment if needed)\n",
    "# !curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh -s -- -y\n",
    "# import os; os.environ['PATH'] = f\"{os.environ['HOME']}/.cargo/bin:{os.environ['PATH']}\"\n",
    "# !pip install git+https://github.com/christianwissmann85/ai-cardgame.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: Change to repo root directory (required for data files)\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def find_repo_root():\n",
    "    path = Path.cwd()\n",
    "    while path != path.parent:\n",
    "        if (path / 'data' / 'cards').exists():\n",
    "            return path\n",
    "        path = path.parent\n",
    "    return None\n",
    "\n",
    "repo_root = find_repo_root()\n",
    "if repo_root:\n",
    "    os.chdir(repo_root)\n",
    "    print(f\"Working directory: {os.getcwd()}\")\n",
    "else:\n",
    "    print(\"Warning: Could not find repo root.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset\n",
    "\n",
    "Datasets are JSONL files (optionally gzipped) with one sample per line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from essence_wars.data import MCTSDataset\n",
    "\n",
    "# Option 1: Load from local file\n",
    "# dataset = MCTSDataset(\"data/datasets/mcts_1k_sims100.jsonl.gz\")\n",
    "\n",
    "# Option 2: Load with convenience function\n",
    "# dataset, loader = load_mcts_dataset(\"data/datasets/mcts_1k.jsonl.gz\", batch_size=64)\n",
    "\n",
    "# For this demo, we'll use the sample dataset included in the repo\n",
    "dataset_path = Path(\"data/datasets\")\n",
    "datasets = list(dataset_path.glob(\"*.jsonl.gz\")) + list(dataset_path.glob(\"*.jsonl\"))\n",
    "\n",
    "if datasets:\n",
    "    print(f\"Found datasets: {[d.name for d in datasets]}\")\n",
    "    dataset = MCTSDataset(str(datasets[0]), max_games=1000)  # Limit for demo\n",
    "else:\n",
    "    print(\"No local dataset found. Creating a small synthetic one for demo...\")\n",
    "    # Generate synthetic data for demo in the correct game-level format\n",
    "    import gzip\n",
    "    import json\n",
    "\n",
    "    from essence_wars._core import PyGame\n",
    "\n",
    "    demo_path = Path(\"/tmp/demo_dataset.jsonl.gz\")\n",
    "    with gzip.open(demo_path, 'wt') as f:\n",
    "        for game_id in range(100):\n",
    "            game = PyGame()\n",
    "            game.reset(seed=game_id)\n",
    "\n",
    "            moves = []\n",
    "            while not game.is_done():\n",
    "                player = game.current_player()\n",
    "                obs = list(map(float, game.observe()))\n",
    "                mask = list(map(float, game.action_mask()))\n",
    "                action = game.greedy_action()\n",
    "\n",
    "                # Create MCTS-style policy (greedy = 100% on chosen action)\n",
    "                mcts_policy = [0.0] * 256\n",
    "                mcts_policy[action] = 1.0\n",
    "\n",
    "                move = {\n",
    "                    \"player\": player,\n",
    "                    \"state_tensor\": obs,\n",
    "                    \"action_mask\": mask,\n",
    "                    \"action\": action,\n",
    "                    \"mcts_policy\": mcts_policy,\n",
    "                    \"mcts_value\": 0.0,  # Placeholder\n",
    "                }\n",
    "                moves.append(move)\n",
    "                game.step(action)\n",
    "\n",
    "            # Determine winner from game result\n",
    "            winner = 0 if game.get_reward(0) > 0 else (1 if game.get_reward(1) > 0 else -1)\n",
    "\n",
    "            game_record = {\n",
    "                \"game_id\": game_id,\n",
    "                \"deck1\": \"default\",\n",
    "                \"deck2\": \"default\",\n",
    "                \"winner\": winner,\n",
    "                \"moves\": moves,\n",
    "            }\n",
    "            f.write(json.dumps(game_record) + \"\\n\")\n",
    "\n",
    "    dataset = MCTSDataset(str(demo_path))\n",
    "\n",
    "print(f\"\\nDataset loaded: {len(dataset)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect a single sample\n",
    "sample = dataset[0]\n",
    "\n",
    "print(\"Sample keys:\")\n",
    "for key, value in sample.items():\n",
    "    if hasattr(value, 'shape'):\n",
    "        print(f\"  {key}: shape={value.shape}, dtype={value.dtype}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {type(value).__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed view of fields\n",
    "print(\"=== Observation (state tensor) ===\")\n",
    "print(f\"Shape: {sample['obs'].shape}\")\n",
    "print(f\"Range: [{sample['obs'].min():.3f}, {sample['obs'].max():.3f}]\")\n",
    "\n",
    "print(\"\\n=== Action Mask ===\")\n",
    "print(f\"Shape: {sample['mask'].shape}\")\n",
    "print(f\"Legal actions: {sample['mask'].sum().item():.0f}\")\n",
    "\n",
    "print(\"\\n=== Policy Target (from MCTS) ===\")\n",
    "print(f\"Shape: {sample['policy_target'].shape}\")\n",
    "print(f\"Sum: {sample['policy_target'].sum().item():.3f} (should be ~1.0)\")\n",
    "top_actions = sample['policy_target'].argsort(descending=True)[:5]\n",
    "print(f\"Top 5 actions: {top_actions.tolist()}\")\n",
    "\n",
    "print(\"\\n=== Value Target ===\")\n",
    "print(f\"Value: {sample['value_target'].item():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect statistics across dataset\n",
    "n_samples = min(1000, len(dataset))\n",
    "policy_entropies = []\n",
    "legal_action_counts = []\n",
    "top1_probs = []\n",
    "\n",
    "for i in range(n_samples):\n",
    "    s = dataset[i]\n",
    "    policy = s['policy_target'].numpy()\n",
    "    mask = s['mask'].numpy()\n",
    "\n",
    "    # Count legal actions\n",
    "    legal_action_counts.append(mask.sum())\n",
    "\n",
    "    # Top-1 probability\n",
    "    top1_probs.append(policy.max())\n",
    "\n",
    "    # Entropy of policy (only over legal actions)\n",
    "    legal_probs = policy[mask > 0.5]\n",
    "    if len(legal_probs) > 0 and legal_probs.sum() > 0:\n",
    "        legal_probs = legal_probs / legal_probs.sum()  # Renormalize\n",
    "        entropy = -np.sum(legal_probs * np.log(legal_probs + 1e-10))\n",
    "        policy_entropies.append(entropy)\n",
    "\n",
    "print(f\"Analyzed {n_samples} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(14, 4))\n",
    "\n",
    "# Legal actions distribution\n",
    "axes[0].hist(legal_action_counts, bins=30, edgecolor='black', alpha=0.7)\n",
    "axes[0].set_xlabel('Number of Legal Actions')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].set_title('Legal Actions per State')\n",
    "axes[0].axvline(np.mean(legal_action_counts), color='red', linestyle='--',\n",
    "                label=f'Mean: {np.mean(legal_action_counts):.1f}')\n",
    "axes[0].legend()\n",
    "\n",
    "# Policy entropy distribution\n",
    "axes[1].hist(policy_entropies, bins=30, edgecolor='black', alpha=0.7, color='green')\n",
    "axes[1].set_xlabel('Policy Entropy')\n",
    "axes[1].set_ylabel('Count')\n",
    "axes[1].set_title('MCTS Policy Entropy')\n",
    "axes[1].axvline(np.mean(policy_entropies), color='red', linestyle='--',\n",
    "                label=f'Mean: {np.mean(policy_entropies):.2f}')\n",
    "axes[1].legend()\n",
    "\n",
    "# Top-1 probability distribution\n",
    "axes[2].hist(top1_probs, bins=30, edgecolor='black', alpha=0.7, color='orange')\n",
    "axes[2].set_xlabel('Top-1 Probability')\n",
    "axes[2].set_ylabel('Count')\n",
    "axes[2].set_title('MCTS Confidence')\n",
    "axes[2].axvline(np.mean(top1_probs), color='red', linestyle='--',\n",
    "                label=f'Mean: {np.mean(top1_probs):.2f}')\n",
    "axes[2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Action Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What actions does MCTS prefer?\n",
    "action_counts = np.zeros(256)\n",
    "for i in range(n_samples):\n",
    "    s = dataset[i]\n",
    "    action = s['action'].item() if hasattr(s['action'], 'item') else s['action']\n",
    "    action_counts[action] += 1\n",
    "\n",
    "# Group by action type\n",
    "play_card = action_counts[:100].sum()\n",
    "attack = action_counts[100:150].sum()\n",
    "ability = action_counts[150:250].sum()\n",
    "end_turn = action_counts[255]\n",
    "\n",
    "categories = ['PlayCard\\n(0-99)', 'Attack\\n(100-149)', 'Ability\\n(150-249)', 'EndTurn\\n(255)']\n",
    "counts = [play_card, attack, ability, end_turn]\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar(categories, counts, color=['#2ecc71', '#e74c3c', '#9b59b6', '#3498db'])\n",
    "plt.ylabel('Count')\n",
    "plt.title('Action Type Distribution')\n",
    "for i, (cat, count) in enumerate(zip(categories, counts)):\n",
    "    plt.text(i, count + max(counts)*0.02, f'{count/sum(counts)*100:.1f}%',\n",
    "             ha='center', fontsize=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State Space Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect state features\n",
    "turns = []\n",
    "p1_lives = []\n",
    "p2_lives = []\n",
    "p1_essences = []\n",
    "\n",
    "for i in range(n_samples):\n",
    "    obs = dataset[i]['obs'].numpy()\n",
    "    turns.append(obs[0] * 30)  # Denormalize\n",
    "    p1_lives.append(obs[6] * 20)  # Player 1 life\n",
    "    p2_lives.append(obs[6+75] * 20)  # Player 2 life\n",
    "    p1_essences.append(obs[7] * 10)  # Player 1 essence\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# Turn distribution\n",
    "axes[0, 0].hist(turns, bins=30, edgecolor='black', alpha=0.7)\n",
    "axes[0, 0].set_xlabel('Turn Number')\n",
    "axes[0, 0].set_title('Turn Distribution')\n",
    "\n",
    "# Life totals\n",
    "axes[0, 1].hist(p1_lives, bins=20, alpha=0.7, label='Player 1')\n",
    "axes[0, 1].hist(p2_lives, bins=20, alpha=0.7, label='Player 2')\n",
    "axes[0, 1].set_xlabel('Life Total')\n",
    "axes[0, 1].set_title('Life Distribution')\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# Life difference\n",
    "life_diff = np.array(p1_lives) - np.array(p2_lives)\n",
    "axes[1, 0].hist(life_diff, bins=30, edgecolor='black', alpha=0.7, color='purple')\n",
    "axes[1, 0].set_xlabel('Life Difference (P1 - P2)')\n",
    "axes[1, 0].set_title('Life Advantage')\n",
    "axes[1, 0].axvline(0, color='red', linestyle='--')\n",
    "\n",
    "# Essence distribution\n",
    "axes[1, 1].hist(p1_essences, bins=10, edgecolor='black', alpha=0.7, color='gold')\n",
    "axes[1, 1].set_xlabel('Essence (Mana)')\n",
    "axes[1, 1].set_title('Essence Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using with PyTorch DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Create DataLoader\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    num_workers=0,  # Use 0 for notebooks\n",
    ")\n",
    "\n",
    "# Get a batch\n",
    "batch = next(iter(dataloader))\n",
    "\n",
    "print(\"Batch contents:\")\n",
    "for key, value in batch.items():\n",
    "    print(f\"  {key}: shape={value.shape}, dtype={value.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop example (pseudocode)\n",
    "print(\"Training loop example:\")\n",
    "print()\n",
    "print(\"for batch in dataloader:\")\n",
    "print(\"    obs = batch['obs'].to(device)\")\n",
    "print(\"    mask = batch['mask'].to(device)\")\n",
    "print(\"    policy_target = batch['policy_target'].to(device)\")\n",
    "print(\"    value_target = batch['value_target'].to(device)\")\n",
    "print(\"    \")\n",
    "print(\"    policy, value = model(obs, mask)\")\n",
    "print(\"    policy_loss = kl_div(policy, policy_target)\")\n",
    "print(\"    value_loss = mse(value, value_target)\")\n",
    "print(\"    loss = policy_loss + value_loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading from Hugging Face\n",
    "\n",
    "For larger datasets, load directly from Hugging Face:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example (requires huggingface_hub)\n",
    "# from essence_wars.data import load_from_huggingface\n",
    "#\n",
    "# # Load 100k game dataset\n",
    "# dataset = load_from_huggingface(\n",
    "#     \"christianwissmann85/essence-wars-mcts-100k\",\n",
    "#     split=\"train\"\n",
    "# )\n",
    "# print(f\"Loaded {len(dataset)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "- **04_behavioral_cloning.ipynb** - Train a neural network on this data\n",
    "- **05_alphazero_training.ipynb** - Fine-tune with self-play"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}