{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AlphaZero Self-Play Training\n",
    "\n",
    "Train agents through self-play, optionally starting from a BC checkpoint.\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/christianwissmann85/ai-cardgame/blob/master/notebooks/05_alphazero_training.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colab setup (uncomment if needed)\n",
    "# !curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh -s -- -y\n",
    "# import os; os.environ['PATH'] = f\"{os.environ['HOME']}/.cargo/bin:{os.environ['PATH']}\"\n",
    "# !pip install git+https://github.com/christianwissmann85/ai-cardgame.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: Change to repo root directory (required for data files)\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def find_repo_root():\n",
    "    path = Path.cwd()\n",
    "    while path != path.parent:\n",
    "        if (path / 'data' / 'cards').exists():\n",
    "            return path\n",
    "        path = path.parent\n",
    "    return None\n",
    "\n",
    "repo_root = find_repo_root()\n",
    "if repo_root:\n",
    "    os.chdir(repo_root)\n",
    "    print(f\"Working directory: {os.getcwd()}\")\n",
    "else:\n",
    "    print(\"Warning: Could not find repo root.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import deque\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Network Architecture\n",
    "\n",
    "Same architecture as BC - shared backbone with policy and value heads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, hidden_dim: int):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.ln1 = nn.LayerNorm(hidden_dim)\n",
    "        self.ln2 = nn.LayerNorm(hidden_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = self.ln1(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.ln2(x)\n",
    "        x = self.fc2(x)\n",
    "        return F.relu(x + residual)\n",
    "\n",
    "\n",
    "class AlphaZeroNetwork(nn.Module):\n",
    "    def __init__(self, obs_dim=326, action_dim=256, hidden_dim=256, num_blocks=4):\n",
    "        super().__init__()\n",
    "        self.input_proj = nn.Linear(obs_dim, hidden_dim)\n",
    "        self.blocks = nn.ModuleList([ResidualBlock(hidden_dim) for _ in range(num_blocks)])\n",
    "        self.policy_head = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, action_dim),\n",
    "        )\n",
    "        self.value_head = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim // 2, 1),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, obs, mask):\n",
    "        x = F.relu(self.input_proj(obs))\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        logits = self.policy_head(x)\n",
    "        logits = logits.masked_fill(~mask, float('-inf'))\n",
    "        value = self.value_head(x)\n",
    "        return logits, value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. MCTS Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class MCTSConfig:\n",
    "    num_simulations: int = 50\n",
    "    c_puct: float = 1.0\n",
    "    dirichlet_alpha: float = 0.3\n",
    "    dirichlet_epsilon: float = 0.25\n",
    "    temperature: float = 1.0\n",
    "\n",
    "\n",
    "class MCTSNode:\n",
    "    def __init__(self, prior: float):\n",
    "        self.prior = prior\n",
    "        self.visit_count = 0\n",
    "        self.value_sum = 0.0\n",
    "        self.children: dict[int, MCTSNode] = {}\n",
    "\n",
    "    @property\n",
    "    def value(self) -> float:\n",
    "        if self.visit_count == 0:\n",
    "            return 0.0\n",
    "        return self.value_sum / self.visit_count\n",
    "\n",
    "    def ucb_score(self, parent_visits: int, c_puct: float) -> float:\n",
    "        exploration = c_puct * self.prior * np.sqrt(parent_visits) / (1 + self.visit_count)\n",
    "        return self.value + exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCTS:\n",
    "    def __init__(self, network, config: MCTSConfig, device):\n",
    "        self.network = network\n",
    "        self.config = config\n",
    "        self.device = device\n",
    "\n",
    "    def search(self, game) -> tuple[np.ndarray, float]:\n",
    "        \"\"\"Run MCTS and return policy distribution and root value.\"\"\"\n",
    "        root = MCTSNode(prior=0.0)\n",
    "\n",
    "        # Expand root\n",
    "        obs = np.array(game.observe(), dtype=np.float32)\n",
    "        mask = np.array(game.action_mask(), dtype=np.float32)\n",
    "        policy, value = self._evaluate(obs, mask)\n",
    "\n",
    "        # Add Dirichlet noise at root for exploration\n",
    "        legal_actions = np.where(mask > 0.5)[0]\n",
    "        noise = np.random.dirichlet([self.config.dirichlet_alpha] * len(legal_actions))\n",
    "\n",
    "        for i, action in enumerate(legal_actions):\n",
    "            noisy_prior = (\n",
    "                (1 - self.config.dirichlet_epsilon) * policy[action] +\n",
    "                self.config.dirichlet_epsilon * noise[i]\n",
    "            )\n",
    "            root.children[action] = MCTSNode(prior=noisy_prior)\n",
    "\n",
    "        # Run simulations\n",
    "        for _ in range(self.config.num_simulations):\n",
    "            self._simulate(game.fork(), root)\n",
    "\n",
    "        # Build policy from visit counts\n",
    "        policy_out = np.zeros(256, dtype=np.float32)\n",
    "        for action, child in root.children.items():\n",
    "            policy_out[action] = child.visit_count\n",
    "\n",
    "        # Apply temperature\n",
    "        if self.config.temperature > 0:\n",
    "            policy_out = policy_out ** (1 / self.config.temperature)\n",
    "\n",
    "        policy_out = policy_out / policy_out.sum()\n",
    "\n",
    "        return policy_out, root.value\n",
    "\n",
    "    def _simulate(self, game, node: MCTSNode) -> float:\n",
    "        \"\"\"Run one simulation from node.\"\"\"\n",
    "        if game.is_done():\n",
    "            return game.get_reward(0)  # Return from P0 perspective\n",
    "\n",
    "        # Select action with highest UCB\n",
    "        best_action = None\n",
    "        best_score = -float('inf')\n",
    "\n",
    "        for action, child in node.children.items():\n",
    "            score = child.ucb_score(node.visit_count, self.config.c_puct)\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_action = action\n",
    "\n",
    "        if best_action is None:\n",
    "            return 0.0\n",
    "\n",
    "        child = node.children[best_action]\n",
    "        game.step(best_action)\n",
    "\n",
    "        # Expand if needed\n",
    "        if child.visit_count == 0 and not game.is_done():\n",
    "            obs = np.array(game.observe(), dtype=np.float32)\n",
    "            mask = np.array(game.action_mask(), dtype=np.float32)\n",
    "            policy, value = self._evaluate(obs, mask)\n",
    "\n",
    "            legal_actions = np.where(mask > 0.5)[0]\n",
    "            for action in legal_actions:\n",
    "                child.children[action] = MCTSNode(prior=policy[action])\n",
    "\n",
    "            # Value from current player's perspective\n",
    "            if game.current_player() == 0:\n",
    "                value = value\n",
    "            else:\n",
    "                value = -value\n",
    "        else:\n",
    "            value = -self._simulate(game, child)  # Negamax\n",
    "\n",
    "        # Backpropagate\n",
    "        child.visit_count += 1\n",
    "        child.value_sum += value\n",
    "\n",
    "        return -value\n",
    "\n",
    "    def _evaluate(self, obs: np.ndarray, mask: np.ndarray) -> tuple[np.ndarray, float]:\n",
    "        \"\"\"Neural network evaluation.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            obs_t = torch.from_numpy(obs).unsqueeze(0).to(self.device)\n",
    "            mask_t = torch.from_numpy(mask).bool().unsqueeze(0).to(self.device)\n",
    "            logits, value = self.network(obs_t, mask_t)\n",
    "            policy = F.softmax(logits, dim=-1).squeeze(0).cpu().numpy()\n",
    "            value = value.item()\n",
    "        return policy, value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Self-Play Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Experience:\n",
    "    obs: np.ndarray\n",
    "    mask: np.ndarray\n",
    "    policy: np.ndarray\n",
    "    value: float  # Filled in after game ends\n",
    "\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity: int = 100000):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def add(self, experiences: list[Experience]):\n",
    "        self.buffer.extend(experiences)\n",
    "\n",
    "    def sample(self, batch_size: int) -> dict:\n",
    "        samples = random.sample(list(self.buffer), min(batch_size, len(self.buffer)))\n",
    "        return {\n",
    "            'obs': torch.stack([torch.from_numpy(e.obs) for e in samples]),\n",
    "            'mask': torch.stack([torch.from_numpy(e.mask) for e in samples]),\n",
    "            'policy': torch.stack([torch.from_numpy(e.policy) for e in samples]),\n",
    "            'value': torch.tensor([e.value for e in samples]),\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_play_game(game, mcts: MCTS) -> list[Experience]:\n",
    "    \"\"\"Play one self-play game and collect experiences.\"\"\"\n",
    "    experiences = []\n",
    "    player_experiences = [[], []]  # Separate by player\n",
    "\n",
    "    while not game.is_done():\n",
    "        current_player = game.current_player()\n",
    "\n",
    "        obs = np.array(game.observe(), dtype=np.float32)\n",
    "        mask = np.array(game.action_mask(), dtype=np.float32)\n",
    "\n",
    "        # Run MCTS\n",
    "        policy, _ = mcts.search(game)\n",
    "\n",
    "        # Store experience (value filled in later)\n",
    "        exp = Experience(obs=obs, mask=mask, policy=policy, value=0.0)\n",
    "        player_experiences[current_player].append(exp)\n",
    "\n",
    "        # Sample action from policy\n",
    "        action = np.random.choice(256, p=policy)\n",
    "        game.step(action)\n",
    "\n",
    "    # Assign values based on game outcome\n",
    "    p0_reward = game.get_reward(0)\n",
    "\n",
    "    for exp in player_experiences[0]:\n",
    "        exp.value = p0_reward\n",
    "        experiences.append(exp)\n",
    "\n",
    "    for exp in player_experiences[1]:\n",
    "        exp.value = -p0_reward  # Opponent's value is negated\n",
    "        experiences.append(exp)\n",
    "\n",
    "    return experiences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Create network\n",
    "network = AlphaZeroNetwork().to(device)\n",
    "\n",
    "# Optional: Load BC checkpoint for warm start\n",
    "bc_checkpoint = Path(\"bc_model.pt\")\n",
    "if bc_checkpoint.exists():\n",
    "    print(f\"Found BC checkpoint: {bc_checkpoint}\")\n",
    "    try:\n",
    "        ckpt = torch.load(bc_checkpoint, map_location=device, weights_only=False)\n",
    "        network.load_state_dict(ckpt['model_state_dict'])\n",
    "        print(\"Successfully loaded BC checkpoint for warm start\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not load BC checkpoint (architecture mismatch?): {e}\")\n",
    "        print(\"Starting from scratch instead\")\n",
    "else:\n",
    "    print(\"No BC checkpoint found, starting from scratch\")\n",
    "\n",
    "optimizer = torch.optim.AdamW(network.parameters(), lr=1e-4)\n",
    "replay_buffer = ReplayBuffer(capacity=50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training config\n",
    "@dataclass\n",
    "class TrainConfig:\n",
    "    num_iterations: int = 10  # Outer loop iterations\n",
    "    games_per_iteration: int = 5  # Self-play games per iteration\n",
    "    train_steps_per_iteration: int = 50  # Gradient steps per iteration\n",
    "    batch_size: int = 64\n",
    "    mcts_simulations: int = 25  # Reduced for speed\n",
    "\n",
    "config = TrainConfig()\n",
    "mcts_config = MCTSConfig(num_simulations=config.mcts_simulations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from essence_wars._core import PyGame\n",
    "\n",
    "\n",
    "def train_step(network, optimizer, batch, device):\n",
    "    \"\"\"One training step.\"\"\"\n",
    "    obs = batch['obs'].to(device)\n",
    "    mask = batch['mask'].bool().to(device)\n",
    "    target_policy = batch['policy'].to(device)\n",
    "    target_value = batch['value'].to(device)\n",
    "\n",
    "    logits, value = network(obs, mask)\n",
    "\n",
    "    # Policy loss (cross-entropy with MCTS policy)\n",
    "    log_probs = F.log_softmax(logits, dim=-1)\n",
    "    policy_loss = -(target_policy * log_probs).sum(dim=-1).mean()\n",
    "\n",
    "    # Value loss\n",
    "    value_loss = F.mse_loss(value.squeeze(-1), target_value)\n",
    "\n",
    "    loss = policy_loss + value_loss\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(network.parameters(), 1.0)\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item(), policy_loss.item(), value_loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "history = {'loss': [], 'policy_loss': [], 'value_loss': [], 'buffer_size': []}\n",
    "\n",
    "for iteration in range(config.num_iterations):\n",
    "    print(f\"\\n=== Iteration {iteration + 1}/{config.num_iterations} ===\")\n",
    "\n",
    "    # Self-play\n",
    "    network.eval()\n",
    "    mcts = MCTS(network, mcts_config, device)\n",
    "\n",
    "    for g in tqdm(range(config.games_per_iteration), desc=\"Self-play\"):\n",
    "        game = PyGame()\n",
    "        game.reset(seed=iteration * 1000 + g)\n",
    "        experiences = self_play_game(game, mcts)\n",
    "        replay_buffer.add(experiences)\n",
    "\n",
    "    print(f\"Buffer size: {len(replay_buffer)}\")\n",
    "\n",
    "    # Training\n",
    "    if len(replay_buffer) < config.batch_size:\n",
    "        print(\"Not enough data, skipping training\")\n",
    "        continue\n",
    "\n",
    "    network.train()\n",
    "    losses, pol_losses, val_losses = [], [], []\n",
    "\n",
    "    for _ in tqdm(range(config.train_steps_per_iteration), desc=\"Training\"):\n",
    "        batch = replay_buffer.sample(config.batch_size)\n",
    "        loss, pol_loss, val_loss = train_step(network, optimizer, batch, device)\n",
    "        losses.append(loss)\n",
    "        pol_losses.append(pol_loss)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "    history['loss'].append(np.mean(losses))\n",
    "    history['policy_loss'].append(np.mean(pol_losses))\n",
    "    history['value_loss'].append(np.mean(val_losses))\n",
    "    history['buffer_size'].append(len(replay_buffer))\n",
    "\n",
    "    print(f\"Loss: {history['loss'][-1]:.4f} \"\n",
    "          f\"(policy: {history['policy_loss'][-1]:.4f}, \"\n",
    "          f\"value: {history['value_loss'][-1]:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from essence_wars.benchmark import EssenceWarsBenchmark\n",
    "\n",
    "\n",
    "# Create agent\n",
    "class AlphaZeroAgent:\n",
    "    def __init__(self, network, device, name=\"AlphaZero\"):\n",
    "        self.network = network\n",
    "        self.device = device\n",
    "        self._name = name\n",
    "\n",
    "    @property\n",
    "    def name(self):\n",
    "        return self._name\n",
    "\n",
    "    def select_action(self, obs, mask):\n",
    "        self.network.eval()\n",
    "        with torch.no_grad():\n",
    "            obs_t = torch.from_numpy(obs).float().unsqueeze(0).to(self.device)\n",
    "            mask_t = torch.from_numpy(mask).bool().unsqueeze(0).to(self.device)\n",
    "            logits, _ = self.network(obs_t, mask_t)\n",
    "            return logits.argmax(dim=-1).item()\n",
    "\n",
    "    def reset(self):\n",
    "        pass\n",
    "\n",
    "agent = AlphaZeroAgent(network, device)\n",
    "\n",
    "# Benchmark\n",
    "benchmark = EssenceWarsBenchmark(games_per_opponent=20, verbose=True)\n",
    "results = benchmark.evaluate(agent, baselines=[\"random\", \"greedy\"])\n",
    "print(\"\\n\" + results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = {\n",
    "    'network_state_dict': network.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'config': {\n",
    "        'hidden_dim': 256,\n",
    "        'num_blocks': 4,\n",
    "    },\n",
    "    'history': history,\n",
    "    'iteration': config.num_iterations,\n",
    "}\n",
    "\n",
    "torch.save(checkpoint, \"alphazero_model.pt\")\n",
    "print(\"Saved checkpoint to alphazero_model.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tips for Better Training\n",
    "\n",
    "1. **More iterations**: Run for 100-1000 iterations for stronger play\n",
    "2. **More simulations**: Use 100-400 MCTS simulations for better policies\n",
    "3. **Parallel self-play**: Use multiple processes for data generation\n",
    "4. **Temperature schedule**: Start high (1.0) and decay to 0.1 over training\n",
    "5. **Evaluation checkpoints**: Save and evaluate every N iterations\n",
    "\n",
    "For production training, see `python/scripts/train_alphazero.py`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}