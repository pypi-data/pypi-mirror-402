{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment Deep Dive\n",
    "\n",
    "This notebook covers the Essence Wars environment API in detail.\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/christianwissmann85/ai-cardgame/blob/master/notebooks/02_environment.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colab setup (uncomment if needed)\n",
    "# !curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh -s -- -y\n",
    "# import os; os.environ['PATH'] = f\"{os.environ['HOME']}/.cargo/bin:{os.environ['PATH']}\"\n",
    "# !pip install git+https://github.com/christianwissmann85/ai-cardgame.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: Change to repo root directory (required for data files)\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def find_repo_root():\n",
    "    path = Path.cwd()\n",
    "    while path != path.parent:\n",
    "        if (path / 'data' / 'cards').exists():\n",
    "            return path\n",
    "        path = path.parent\n",
    "    return None\n",
    "\n",
    "repo_root = find_repo_root()\n",
    "if repo_root:\n",
    "    os.chdir(repo_root)\n",
    "    print(f\"Working directory: {os.getcwd()}\")\n",
    "else:\n",
    "    print(\"Warning: Could not find repo root.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from essence_wars._core import STATE_TENSOR_SIZE, PyGame, PyParallelGames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyGame API Reference\n",
    "\n",
    "### Constructor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full constructor with all options\n",
    "game = PyGame(\n",
    "    deck1=\"vex_piercing\",         # Player 1's deck (Argentum)\n",
    "    deck2=\"alpha_frenzy\",         # Player 2's deck (Symbiote)\n",
    "    game_mode=\"attrition\"          # \"attrition\" or \"essence_duel\"\n",
    ")\n",
    "\n",
    "# Default constructor (uses default decks)\n",
    "game_default = PyGame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Core Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset with seed for reproducibility\n",
    "game.reset(seed=42)\n",
    "\n",
    "# Get observation (state tensor)\n",
    "obs = game.observe()  # Returns numpy array (326,)\n",
    "print(f\"Observation shape: {np.array(obs).shape}\")\n",
    "\n",
    "# Get legal action mask\n",
    "mask = game.action_mask()  # Returns numpy array (256,), 1.0=legal, 0.0=illegal\n",
    "print(f\"Legal actions: {int(np.sum(mask))}\")\n",
    "\n",
    "# Take an action\n",
    "action = 255  # EndTurn is always legal\n",
    "reward, done = game.step(action)\n",
    "print(f\"Reward: {reward}, Done: {done}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### State Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game.reset(seed=42)\n",
    "\n",
    "print(f\"Current player: {game.current_player()}\")\n",
    "print(f\"Turn number: {game.turn_number()}\")\n",
    "print(f\"Game over: {game.is_done()}\")\n",
    "print(f\"Player 0 reward: {game.get_reward(0)}\")\n",
    "print(f\"Player 1 reward: {game.get_reward(1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Built-in Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game.reset(seed=42)\n",
    "\n",
    "# Random agent - uniform selection from legal actions\n",
    "random_action = game.random_action()\n",
    "print(f\"Random action: {random_action}\")\n",
    "\n",
    "# Greedy agent - heuristic evaluation\n",
    "greedy_action = game.greedy_action()\n",
    "print(f\"Greedy action: {greedy_action}\")\n",
    "\n",
    "# MCTS agent - tree search (configurable simulations)\n",
    "mcts_action = game.mcts_action(simulations=100)\n",
    "print(f\"MCTS action (100 sims): {mcts_action}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### State Cloning (for MCTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game.reset(seed=42)\n",
    "\n",
    "# Fork creates an independent copy\n",
    "game_copy = game.fork()\n",
    "\n",
    "# Modify original\n",
    "game.step(game.greedy_action())\n",
    "\n",
    "# Copy is unaffected\n",
    "print(f\"Original turn: {game.turn_number()}\")\n",
    "print(f\"Copy turn: {game_copy.turn_number()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Static Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List available decks\n",
    "decks = PyGame.list_decks()\n",
    "print(f\"Available decks ({len(decks)}):\")\n",
    "for d in decks:\n",
    "    print(f\"  - {d}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorized Environments\n",
    "\n",
    "`PyParallelGames` runs multiple games in parallel for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 8 parallel environments\n",
    "num_envs = 8\n",
    "envs = PyParallelGames(\n",
    "    num_envs=num_envs,\n",
    "    deck1=\"vex_piercing\",\n",
    "    deck2=\"alpha_frenzy\",\n",
    ")\n",
    "\n",
    "# Reset all with different seeds\n",
    "seeds = list(range(100, 100 + num_envs))\n",
    "envs.reset(seeds)\n",
    "\n",
    "print(f\"Number of environments: {envs.num_envs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch observations\n",
    "obs_batch = envs.observe_batch()  # Shape: (num_envs, 326)\n",
    "print(f\"Observation batch shape: {np.array(obs_batch).shape}\")\n",
    "\n",
    "# Batch action masks\n",
    "mask_batch = envs.action_mask_batch()  # Shape: (num_envs, 256)\n",
    "print(f\"Action mask batch shape: {np.array(mask_batch).shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch step - take EndTurn in all envs\n",
    "actions = np.array([255] * num_envs, dtype=np.uint8)\n",
    "rewards, dones = envs.step_batch(actions)\n",
    "\n",
    "print(f\"Rewards: {np.array(rewards)}\")\n",
    "print(f\"Dones: {np.array(dones)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query state\n",
    "print(f\"Current players: {envs.current_players()}\")\n",
    "print(f\"Done flags: {envs.dones()}\")\n",
    "\n",
    "# Reset single environment\n",
    "envs.reset_single(idx=0, seed=999)\n",
    "\n",
    "# Reset all with base seed\n",
    "envs.reset_all(base_seed=1000)  # Seeds: 1000, 1001, ..., 1007"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Action Space Details\n",
    "\n",
    "### Action Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def describe_action_space():\n",
    "    \"\"\"Print action space documentation.\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"ACTION SPACE (256 discrete actions)\")\n",
    "    print(\"=\" * 60)\n",
    "    print()\n",
    "    print(\"PlayCard (indices 0-99):\")\n",
    "    print(\"  Formula: hand_index * 10 + target_slot\")\n",
    "    print(\"  - hand_index: 0-9 (card position in hand)\")\n",
    "    print(\"  - target_slot: 0-4 (creature slot) or 5-6 (support slot)\")\n",
    "    print(\"  - For spells: target can be creature/player\")\n",
    "    print()\n",
    "    print(\"Attack (indices 100-149):\")\n",
    "    print(\"  Formula: 100 + attacker_slot * 10 + target\")\n",
    "    print(\"  - attacker_slot: 0-4 (your creature slot)\")\n",
    "    print(\"  - target: 0-4 (enemy creature) or 5 (enemy face)\")\n",
    "    print()\n",
    "    print(\"UseAbility (indices 150-249):\")\n",
    "    print(\"  Formula: 150 + slot * 20 + ability_index * 10 + target\")\n",
    "    print(\"  - slot: 0-4 (creature with ability)\")\n",
    "    print(\"  - ability_index: 0-1 (which ability)\")\n",
    "    print(\"  - target: 0-9 (depends on ability)\")\n",
    "    print()\n",
    "    print(\"EndTurn (index 255):\")\n",
    "    print(\"  Always legal. Passes to opponent.\")\n",
    "    print()\n",
    "\n",
    "describe_action_space()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_action(idx: int) -> str:\n",
    "    \"\"\"Convert action index to human-readable description.\"\"\"\n",
    "    if idx == 255:\n",
    "        return \"EndTurn\"\n",
    "    elif idx < 100:\n",
    "        hand = idx // 10\n",
    "        target = idx % 10\n",
    "        return f\"PlayCard(hand[{hand}] -> slot {target})\"\n",
    "    elif idx < 150:\n",
    "        i = idx - 100\n",
    "        attacker = i // 10\n",
    "        target = i % 10\n",
    "        target_str = f\"creature[{target}]\" if target < 5 else \"face\"\n",
    "        return f\"Attack(slot[{attacker}] -> {target_str})\"\n",
    "    elif idx < 250:\n",
    "        i = idx - 150\n",
    "        slot = i // 20\n",
    "        ability = (i % 20) // 10\n",
    "        target = i % 10\n",
    "        return f\"UseAbility(slot[{slot}].ability[{ability}] -> {target})\"\n",
    "    else:\n",
    "        return f\"Reserved({idx})\"\n",
    "\n",
    "# Example decoding\n",
    "examples = [0, 5, 42, 100, 105, 115, 150, 170, 255]\n",
    "for idx in examples:\n",
    "    print(f\"{idx:3d} -> {decode_action(idx)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State Tensor Layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def describe_tensor_layout():\n",
    "    \"\"\"Print tensor layout documentation.\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"STATE TENSOR ({STATE_TENSOR_SIZE} floats)\")\n",
    "    print(\"=\" * 60)\n",
    "    print()\n",
    "\n",
    "    idx = 0\n",
    "\n",
    "    print(f\"[{idx:3d}-{idx+5:3d}] Global state (6 floats):\")\n",
    "    print(\"         [0] turn / 30\")\n",
    "    print(\"         [1] current_player (0 or 1)\")\n",
    "    print(\"         [2] game_over (0 or 1)\")\n",
    "    print(\"         [3] winner (-1, 0, or 1)\")\n",
    "    print(\"         [4-5] reserved\")\n",
    "    idx += 6\n",
    "\n",
    "    for p in [1, 2]:\n",
    "        print(f\"\\n[{idx:3d}-{idx+74:3d}] Player {p} state (75 floats):\")\n",
    "        print(f\"         [{idx}] life / 20\")\n",
    "        print(f\"         [{idx+1}] essence / 10\")\n",
    "        print(f\"         [{idx+2}] action_points / 3\")\n",
    "        print(f\"         [{idx+3}] deck_size / 30\")\n",
    "        print(f\"         [{idx+4}] hand_size / 10\")\n",
    "        print(f\"         [{idx+5}-{idx+14}] hand card IDs (10 slots)\")\n",
    "        print(f\"         [{idx+15}-{idx+64}] creatures (5 slots x 10 floats)\")\n",
    "        print(f\"         [{idx+65}-{idx+74}] supports (2 slots x 5 floats)\")\n",
    "        idx += 75\n",
    "\n",
    "    print(f\"\\n[{idx:3d}-{STATE_TENSOR_SIZE-1:3d}] Card embedding IDs ({STATE_TENSOR_SIZE - idx} floats)\")\n",
    "\n",
    "describe_tensor_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creature Slot Encoding (10 floats each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Creature slot encoding (10 floats):\")\n",
    "print(\"  [0] occupied (0 or 1)\")\n",
    "print(\"  [1] attack / 10\")\n",
    "print(\"  [2] health / 10\")\n",
    "print(\"  [3] max_health / 10\")\n",
    "print(\"  [4] can_attack (0 or 1)\")\n",
    "print(\"  [5] exhausted (0 or 1)\")\n",
    "print(\"  [6] silenced (0 or 1)\")\n",
    "print(\"  [7] has_rush (0 or 1)\")\n",
    "print(\"  [8] has_guard (0 or 1)\")\n",
    "print(\"  [9] keywords_bitfield / 65535\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Benchmark single env\n",
    "game = PyGame()\n",
    "game.reset(seed=42)\n",
    "\n",
    "n_steps = 10000\n",
    "start = time.perf_counter()\n",
    "for _ in range(n_steps):\n",
    "    game.observe()\n",
    "elapsed = time.perf_counter() - start\n",
    "print(f\"Observations: {n_steps / elapsed:.0f} per second\")\n",
    "\n",
    "start = time.perf_counter()\n",
    "for _ in range(n_steps):\n",
    "    game.action_mask()\n",
    "elapsed = time.perf_counter() - start\n",
    "print(f\"Action masks: {n_steps / elapsed:.0f} per second\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark vectorized env\n",
    "num_envs = 64\n",
    "envs = PyParallelGames(num_envs=num_envs)\n",
    "envs.reset_all(base_seed=0)\n",
    "\n",
    "n_batches = 1000\n",
    "start = time.perf_counter()\n",
    "for _ in range(n_batches):\n",
    "    envs.observe_batch()\n",
    "elapsed = time.perf_counter() - start\n",
    "print(f\"Batch observations ({num_envs} envs): {n_batches * num_envs / elapsed:.0f} per second\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "- **03_dataset_exploration.ipynb** - Explore MCTS training data\n",
    "- **04_behavioral_cloning.ipynb** - Train a neural network agent"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}