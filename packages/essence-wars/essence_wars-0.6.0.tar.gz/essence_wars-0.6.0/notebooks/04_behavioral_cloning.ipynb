{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Behavioral Cloning: Train Your First Agent\n",
    "\n",
    "Train a neural network to imitate MCTS policy using supervised learning.\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/christianwissmann85/ai-cardgame/blob/master/notebooks/04_behavioral_cloning.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colab setup (uncomment if needed)\n",
    "# !curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh -s -- -y\n",
    "# import os; os.environ['PATH'] = f\"{os.environ['HOME']}/.cargo/bin:{os.environ['PATH']}\"\n",
    "# !pip install git+https://github.com/christianwissmann85/ai-cardgame.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: Change to repo root directory (required for data files)\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def find_repo_root():\n",
    "    path = Path.cwd()\n",
    "    while path != path.parent:\n",
    "        if (path / 'data' / 'cards').exists():\n",
    "            return path\n",
    "        path = path.parent\n",
    "    return None\n",
    "\n",
    "repo_root = find_repo_root()\n",
    "if repo_root:\n",
    "    os.chdir(repo_root)\n",
    "    print(f\"Working directory: {os.getcwd()}\")\n",
    "else:\n",
    "    print(\"Warning: Could not find repo root.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from essence_wars.data import MCTSDataset\n",
    "\n",
    "# Load dataset (use your own path or download from HuggingFace)\n",
    "dataset_path = Path(\"data/datasets\")\n",
    "datasets = list(dataset_path.glob(\"*.jsonl.gz\")) + list(dataset_path.glob(\"*.jsonl\"))\n",
    "\n",
    "if datasets:\n",
    "    dataset = MCTSDataset(str(datasets[0]))\n",
    "    print(f\"Loaded {len(dataset)} samples from {datasets[0].name}\")\n",
    "else:\n",
    "    print(\"No dataset found. Generating demo data...\")\n",
    "    # Generate demo data in the correct game-level format\n",
    "    import gzip\n",
    "    import json\n",
    "\n",
    "    from essence_wars._core import PyGame\n",
    "\n",
    "    demo_path = Path(\"/tmp/demo_dataset.jsonl.gz\")\n",
    "    with gzip.open(demo_path, 'wt') as f:\n",
    "        for game_id in range(500):  # More games for training\n",
    "            game = PyGame()\n",
    "            game.reset(seed=game_id)\n",
    "\n",
    "            moves = []\n",
    "            while not game.is_done():\n",
    "                player = game.current_player()\n",
    "                obs = list(map(float, game.observe()))\n",
    "                mask = list(map(float, game.action_mask()))\n",
    "                action = game.greedy_action()\n",
    "\n",
    "                # Create MCTS-style policy (greedy = 100% on chosen action)\n",
    "                mcts_policy = [0.0] * 256\n",
    "                mcts_policy[action] = 1.0\n",
    "\n",
    "                move = {\n",
    "                    \"player\": player,\n",
    "                    \"state_tensor\": obs,\n",
    "                    \"action_mask\": mask,\n",
    "                    \"action\": action,\n",
    "                    \"mcts_policy\": mcts_policy,\n",
    "                    \"mcts_value\": 0.0,\n",
    "                }\n",
    "                moves.append(move)\n",
    "                game.step(action)\n",
    "\n",
    "            # Determine winner from game result\n",
    "            winner = 0 if game.get_reward(0) > 0 else (1 if game.get_reward(1) > 0 else -1)\n",
    "\n",
    "            game_record = {\n",
    "                \"game_id\": game_id,\n",
    "                \"deck1\": \"default\",\n",
    "                \"deck2\": \"default\",\n",
    "                \"winner\": winner,\n",
    "                \"moves\": moves,\n",
    "            }\n",
    "            f.write(json.dumps(game_record) + \"\\n\")\n",
    "\n",
    "    dataset = MCTSDataset(str(demo_path))\n",
    "    print(f\"Generated {len(dataset)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/val split\n",
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = random_split(\n",
    "    dataset, [train_size, val_size],\n",
    "    generator=torch.Generator().manual_seed(42)\n",
    ")\n",
    "\n",
    "print(f\"Train: {len(train_dataset)}, Val: {len(val_dataset)}\")\n",
    "\n",
    "# DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define the Model\n",
    "\n",
    "We use the same architecture as AlphaZero: shared backbone with policy and value heads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the official AlphaZeroNetwork for compatibility with checkpoints\n",
    "from essence_wars.agents.networks import AlphaZeroNetwork\n",
    "\n",
    "# You can also view the architecture:\n",
    "# AlphaZeroNetwork uses:\n",
    "# - Input projection (Linear -> ReLU)\n",
    "# - Residual tower (N residual blocks)\n",
    "# - Policy head (Linear -> ReLU -> Linear)\n",
    "# - Value head (Linear -> ReLU -> Linear -> Tanh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = AlphaZeroNetwork(\n",
    "    obs_dim=326,\n",
    "    action_dim=256,\n",
    "    hidden_dim=256,\n",
    "    num_blocks=4,\n",
    ").to(device)\n",
    "\n",
    "# Count parameters\n",
    "n_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Model parameters: {n_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(model, batch, device):\n",
    "    \"\"\"Compute BC loss: KL divergence for policy, MSE for value.\"\"\"\n",
    "    obs = batch['obs'].to(device)\n",
    "    mask = batch['mask'].to(device).bool()\n",
    "    policy_target = batch['policy_target'].to(device)\n",
    "    value_target = batch['value_target'].to(device)\n",
    "\n",
    "    # Forward pass\n",
    "    logits, value = model(obs, mask)\n",
    "\n",
    "    # Policy loss: KL divergence with proper masking\n",
    "    log_probs = F.log_softmax(logits, dim=-1)\n",
    "    mask_float = mask.float()\n",
    "\n",
    "    # Mask target and renormalize\n",
    "    masked_target = policy_target * mask_float\n",
    "    target_sum = masked_target.sum(dim=-1, keepdim=True).clamp(min=1e-8)\n",
    "    masked_target = masked_target / target_sum\n",
    "\n",
    "    # KL divergence over legal actions\n",
    "    policy_loss = -(masked_target * log_probs * mask_float).sum(dim=-1).mean()\n",
    "\n",
    "    # Value loss: MSE\n",
    "    value_loss = F.mse_loss(value.squeeze(-1), value_target)\n",
    "\n",
    "    # Total loss\n",
    "    total_loss = policy_loss + value_loss\n",
    "\n",
    "    # Metrics\n",
    "    with torch.no_grad():\n",
    "        pred_actions = logits.argmax(dim=-1)\n",
    "        target_actions = policy_target.argmax(dim=-1)\n",
    "        accuracy = (pred_actions == target_actions).float().mean()\n",
    "\n",
    "    return total_loss, policy_loss.item(), value_loss.item(), accuracy.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training config\n",
    "epochs = 10\n",
    "lr = 1e-3\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "\n",
    "# Training history\n",
    "history = {\n",
    "    'train_loss': [], 'val_loss': [],\n",
    "    'train_acc': [], 'val_acc': [],\n",
    "    'policy_loss': [], 'value_loss': [],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    # Training\n",
    "    model.train()\n",
    "    train_losses, train_accs = [], []\n",
    "    policy_losses, value_losses = [], []\n",
    "\n",
    "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\", leave=False):\n",
    "        optimizer.zero_grad()\n",
    "        loss, pol_loss, val_loss, acc = compute_loss(model, batch, device)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        train_losses.append(loss.item())\n",
    "        train_accs.append(acc)\n",
    "        policy_losses.append(pol_loss)\n",
    "        value_losses.append(val_loss)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_losses, val_accs = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            loss, _, _, acc = compute_loss(model, batch, device)\n",
    "            val_losses.append(loss.item())\n",
    "            val_accs.append(acc)\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    # Record history\n",
    "    history['train_loss'].append(np.mean(train_losses))\n",
    "    history['val_loss'].append(np.mean(val_losses))\n",
    "    history['train_acc'].append(np.mean(train_accs))\n",
    "    history['val_acc'].append(np.mean(val_accs))\n",
    "    history['policy_loss'].append(np.mean(policy_losses))\n",
    "    history['value_loss'].append(np.mean(value_losses))\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: \"\n",
    "          f\"train_loss={history['train_loss'][-1]:.4f}, \"\n",
    "          f\"val_loss={history['val_loss'][-1]:.4f}, \"\n",
    "          f\"train_acc={history['train_acc'][-1]:.1%}, \"\n",
    "          f\"val_acc={history['val_acc'][-1]:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Loss\n",
    "axes[0].plot(history['train_loss'], label='Train')\n",
    "axes[0].plot(history['val_loss'], label='Val')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Total Loss')\n",
    "axes[0].legend()\n",
    "\n",
    "# Accuracy\n",
    "axes[1].plot(history['train_acc'], label='Train')\n",
    "axes[1].plot(history['val_acc'], label='Val')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].set_title('Policy Accuracy')\n",
    "axes[1].legend()\n",
    "\n",
    "# Loss breakdown\n",
    "axes[2].plot(history['policy_loss'], label='Policy')\n",
    "axes[2].plot(history['value_loss'], label='Value')\n",
    "axes[2].set_xlabel('Epoch')\n",
    "axes[2].set_ylabel('Loss')\n",
    "axes[2].set_title('Loss Components')\n",
    "axes[2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluate Against Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from essence_wars.benchmark import EssenceWarsBenchmark, NeuralAgent\n",
    "\n",
    "\n",
    "# Create agent from trained model\n",
    "class TrainedAgent:\n",
    "    def __init__(self, model, device, name=\"BC-Agent\"):\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self._name = name\n",
    "        self.model.eval()\n",
    "\n",
    "    @property\n",
    "    def name(self):\n",
    "        return self._name\n",
    "\n",
    "    def select_action(self, obs, mask):\n",
    "        with torch.no_grad():\n",
    "            obs_t = torch.from_numpy(obs).float().unsqueeze(0).to(self.device)\n",
    "            mask_t = torch.from_numpy(mask).bool().unsqueeze(0).to(self.device)\n",
    "            logits, _ = self.model(obs_t, mask_t)\n",
    "            return logits.argmax(dim=-1).item()\n",
    "\n",
    "    def reset(self):\n",
    "        pass\n",
    "\n",
    "agent = TrainedAgent(model, device, name=\"BC-Trained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run benchmark (quick evaluation)\n",
    "benchmark = EssenceWarsBenchmark(games_per_opponent=20, verbose=True)\n",
    "results = benchmark.evaluate(agent, baselines=[\"random\", \"greedy\"])\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save checkpoint\n",
    "checkpoint = {\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'args': {\n",
    "        'hidden_dim': 256,\n",
    "        'num_blocks': 4,\n",
    "    },\n",
    "    'history': history,\n",
    "    'final_val_accuracy': history['val_acc'][-1],\n",
    "}\n",
    "\n",
    "save_path = Path(\"bc_model.pt\")\n",
    "torch.save(checkpoint, save_path)\n",
    "print(f\"Model saved to {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and verify\n",
    "loaded_agent = NeuralAgent.from_checkpoint(str(save_path), device=str(device))\n",
    "print(f\"Loaded agent: {loaded_agent.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Your BC model is a great starting point! To improve further:\n",
    "\n",
    "1. **More data**: Train on larger datasets (10k-100k games)\n",
    "2. **Fine-tuning**: Use AlphaZero self-play to improve beyond the MCTS teacher\n",
    "3. **Hyperparameters**: Try larger models, different learning rates\n",
    "\n",
    "Continue with **05_alphazero_training.ipynb** for self-play training!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}