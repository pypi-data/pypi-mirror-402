"""
SQLite database backend for workflow experiments.
"""

import os
import sqlite3
import threading

from ao.common.logger import logger
from ao.common.constants import DB_PATH


# Global lock among concurrent threads: Threads within a process share a single
# DB connection, so they cannot issue DB operations in parallel. Python releases
# the GIL during DB operations, so we use a global lock to ensure only one thread
# executes a DB operation at a time. Different processes use different connections
# and SQLite handles concurrency amongst them.
# NOTE: Alternatively, we can give each thread its own connection and avoid the
# global lock. This would improve scalability, which might be important for the
# server (e.g., 1000s of parallel production runs). However, we need to switch
# away from SQLite and make larger refactors for that anyways, so we currently
# stick with this strawman approach.
_db_lock = threading.RLock()
_shared_conn = None


def get_conn():
    """Get the shared SQLite connection"""
    global _shared_conn

    if _shared_conn is None:
        with _db_lock:
            # Double-check pattern to avoid race condition during initialization
            if _shared_conn is None:
                db_path = os.path.join(DB_PATH, "experiments.sqlite")
                # Ensure the directory exists with proper permissions
                os.makedirs(os.path.dirname(db_path), exist_ok=True)
                _shared_conn = sqlite3.connect(
                    db_path,
                    check_same_thread=False,
                    timeout=30.0,
                    detect_types=sqlite3.PARSE_DECLTYPES,
                )
                _shared_conn.row_factory = sqlite3.Row
                # Enable WAL mode for better concurrent access
                _shared_conn.execute("PRAGMA journal_mode=WAL")
                _shared_conn.execute("PRAGMA synchronous=NORMAL")
                _shared_conn.execute("PRAGMA busy_timeout=10000")  # 10 second timeout
                _init_db(_shared_conn)
                logger.debug(f"Initialized shared DB connection at {db_path}")

    return _shared_conn


def _init_db(conn):
    c = conn.cursor()

    # Note: Users are only managed in PostgreSQL for remote authentication
    # Local SQLite runs are single-user and don't need user management

    # Create experiments table
    c.execute(
        """
        CREATE TABLE IF NOT EXISTS experiments (
            session_id TEXT PRIMARY KEY,
            parent_session_id TEXT,
            graph_topology TEXT,
            color_preview TEXT,
            timestamp TIMESTAMP DEFAULT (datetime('now')),
            cwd TEXT,
            command TEXT,
            environment TEXT,
            version_date TEXT,
            name TEXT,
            success TEXT CHECK (success IN ('', 'Satisfactory', 'Failed')),
            notes TEXT,
            log TEXT,
            FOREIGN KEY (parent_session_id) REFERENCES experiments (session_id),
            UNIQUE (parent_session_id, name)
        )
    """
    )
    # Create llm_calls table
    c.execute(
        """
        CREATE TABLE IF NOT EXISTS llm_calls (
            session_id TEXT,
            node_id TEXT,
            input TEXT,
            input_hash TEXT,
            input_overwrite TEXT,
            output TEXT,
            color TEXT,
            label TEXT,
            api_type TEXT,
            timestamp TIMESTAMP DEFAULT (datetime('now')),
            PRIMARY KEY (session_id, node_id),
            FOREIGN KEY (session_id) REFERENCES experiments (session_id)
        )
    """
    )
    # Create attachments table (for caching file attachments like images)
    c.execute(
        """
        CREATE TABLE IF NOT EXISTS attachments (
            file_id TEXT PRIMARY KEY,
            content_hash TEXT,
            file_path TEXT
        )
    """
    )
    c.execute(
        """
        CREATE INDEX IF NOT EXISTS attachments_content_hash_idx ON attachments(content_hash)
    """
    )
    c.execute(
        """
        CREATE INDEX IF NOT EXISTS original_input_lookup ON llm_calls(session_id, input_hash)
    """
    )
    c.execute(
        """
        CREATE INDEX IF NOT EXISTS experiments_timestamp_idx ON experiments(timestamp DESC)
    """
    )

    # Create lessons table
    c.execute(
        """
        CREATE TABLE IF NOT EXISTS lessons (
            lesson_id TEXT PRIMARY KEY,
            from_session_id TEXT,
            from_node_id TEXT,
            lesson_text TEXT NOT NULL,
            created_at TIMESTAMP DEFAULT (datetime('now')),
            updated_at TIMESTAMP DEFAULT (datetime('now')),
            FOREIGN KEY (from_session_id) REFERENCES experiments (session_id)
        )
    """
    )

    # Create lessons_applied table
    c.execute(
        """
        CREATE TABLE IF NOT EXISTS lessons_applied (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            lesson_id TEXT NOT NULL,
            session_id TEXT NOT NULL,
            node_id TEXT,
            applied_at TIMESTAMP DEFAULT (datetime('now')),
            FOREIGN KEY (lesson_id) REFERENCES lessons (lesson_id),
            FOREIGN KEY (session_id) REFERENCES experiments (session_id),
            UNIQUE (lesson_id, session_id, node_id)
        )
    """
    )
    c.execute(
        """
        CREATE INDEX IF NOT EXISTS lessons_applied_lesson_idx ON lessons_applied(lesson_id)
    """
    )
    conn.commit()


def query_one(sql, params=()):
    with _db_lock:
        conn = get_conn()
        c = conn.cursor()
        c.execute(sql, params)
        return c.fetchone()


def query_all(sql, params=()):
    with _db_lock:
        conn = get_conn()
        c = conn.cursor()
        c.execute(sql, params)
        return c.fetchall()


def execute(sql, params=()):
    """Execute SQL with proper locking to prevent transaction conflicts"""
    with _db_lock:
        conn = get_conn()
        c = conn.cursor()
        c.execute(sql, params)
        conn.commit()
        return c.lastrowid


def clear_connections():
    """Clear cached SQLite connections to force reconnection."""
    global _shared_conn
    with _db_lock:
        if _shared_conn:
            try:
                _shared_conn.close()
            except Exception as e:
                logger.warning(f"Error closing SQLite connection: {e}")
            finally:
                _shared_conn = None
            logger.debug("Cleared SQLite connection cache")


def add_experiment_query(
    session_id,
    parent_session_id,
    name,
    default_graph,
    timestamp,
    cwd,
    command,
    env_json,
    default_success,
    default_note,
    default_log,
    user_id,  # Ignored in SQLite - kept for API compatibility
    version_date,
):
    """Execute SQLite-specific INSERT for experiments table"""
    execute(
        "INSERT OR REPLACE INTO experiments (session_id, parent_session_id, name, graph_topology, timestamp, cwd, command, environment, version_date, success, notes, log) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)",
        (
            session_id,
            parent_session_id,
            name,
            default_graph,
            timestamp,
            cwd,
            command,
            env_json,
            version_date,
            default_success,
            default_note,
            default_log,
        ),
    )


def set_input_overwrite_query(input_overwrite, session_id, node_id):
    """Execute SQLite-specific UPDATE for llm_calls input_overwrite"""
    execute(
        "UPDATE llm_calls SET input_overwrite=?, output=NULL WHERE session_id=? AND node_id=?",
        (input_overwrite, session_id, node_id),
    )


def set_output_overwrite_query(output_overwrite, session_id, node_id):
    """Execute SQLite-specific UPDATE for llm_calls output"""
    execute(
        "UPDATE llm_calls SET output=? WHERE session_id=? AND node_id=?",
        (output_overwrite, session_id, node_id),
    )


def delete_llm_calls_query(session_id):
    """Execute SQLite-specific DELETE for llm_calls"""
    execute("DELETE FROM llm_calls WHERE session_id=?", (session_id,))


def update_experiment_graph_topology_query(graph_json, session_id):
    """Execute SQLite-specific UPDATE for experiments graph_topology"""
    execute("UPDATE experiments SET graph_topology=? WHERE session_id=?", (graph_json, session_id))


def update_experiment_timestamp_query(timestamp, session_id):
    """Execute SQLite-specific UPDATE for experiments timestamp"""
    execute("UPDATE experiments SET timestamp=? WHERE session_id=?", (timestamp, session_id))


def update_experiment_name_query(run_name, session_id):
    """Execute SQLite-specific UPDATE for experiments name"""
    execute(
        "UPDATE experiments SET name=? WHERE session_id=?",
        (run_name, session_id),
    )


def update_experiment_result_query(result, session_id):
    """Execute SQLite-specific UPDATE for experiments success"""
    execute(
        "UPDATE experiments SET success=? WHERE session_id=?",
        (result, session_id),
    )


def update_experiment_notes_query(notes, session_id):
    """Execute SQLite-specific UPDATE for experiments notes"""
    execute(
        "UPDATE experiments SET notes=? WHERE session_id=?",
        (notes, session_id),
    )


def update_experiment_command_query(command, session_id):
    """Execute SQLite-specific UPDATE for experiments command"""
    execute(
        "UPDATE experiments SET command=? WHERE session_id=?",
        (command, session_id),
    )


def update_experiment_version_date_query(version_date, session_id):
    """Execute SQLite-specific UPDATE for experiments version_date"""
    execute(
        "UPDATE experiments SET version_date=? WHERE session_id=?",
        (version_date, session_id),
    )


def update_experiment_log_query(
    updated_log, updated_success, color_preview_json, graph_json, session_id
):
    """Execute SQLite-specific UPDATE for experiments log, success, color_preview, and graph_topology"""
    execute(
        "UPDATE experiments SET log=?, success=?, color_preview=?, graph_topology=? WHERE session_id=?",
        (updated_log, updated_success, color_preview_json, graph_json, session_id),
    )


# Attachment-related queries
def check_attachment_exists_query(file_id):
    """Check if attachment with given file_id exists."""
    return query_one("SELECT file_id FROM attachments WHERE file_id=?", (file_id,))


def get_attachment_by_content_hash_query(content_hash):
    """Get attachment file path by content hash."""
    return query_one("SELECT file_path FROM attachments WHERE content_hash=?", (content_hash,))


def insert_attachment_query(file_id, content_hash, file_path):
    """Insert new attachment record."""
    execute(
        "INSERT INTO attachments (file_id, content_hash, file_path) VALUES (?, ?, ?)",
        (file_id, content_hash, file_path),
    )


def get_attachment_file_path_query(file_id):
    """Get file path for attachment by file_id."""
    return query_one("SELECT file_path FROM attachments WHERE file_id=?", (file_id,))


# Subrun queries
def get_subrun_by_parent_and_name_query(parent_session_id, name):
    """Get subrun session_id by parent session and name."""
    return query_one(
        "SELECT session_id FROM experiments WHERE parent_session_id = ? AND name = ?",
        (parent_session_id, name),
    )


def get_parent_session_id_query(session_id):
    """Get parent session ID for a given session."""
    return query_one("SELECT parent_session_id FROM experiments WHERE session_id=?", (session_id,))


# LLM calls queries
def get_llm_call_by_session_and_hash_query(session_id, input_hash):
    """Get LLM call by session_id and input_hash."""
    return query_one(
        "SELECT node_id, input_overwrite, output FROM llm_calls WHERE session_id=? AND input_hash=?",
        (session_id, input_hash),
    )


def insert_llm_call_with_output_query(
    session_id, input_pickle, input_hash, node_id, api_type, output_pickle
):
    """Insert new LLM call record with output in a single operation (upsert)."""
    execute(
        """
        INSERT INTO llm_calls (session_id, input, input_hash, node_id, api_type, output)
        VALUES (?, ?, ?, ?, ?, ?)
        ON CONFLICT (session_id, node_id)
        DO UPDATE SET output = excluded.output
        """,
        (session_id, input_pickle, input_hash, node_id, api_type, output_pickle),
    )


# Experiment list and graph queries
def get_finished_runs_query():
    """Get all finished runs ordered by timestamp."""
    return query_all("SELECT session_id, timestamp FROM experiments ORDER BY timestamp DESC", ())


def get_all_experiments_sorted_query():
    """Get all experiments sorted by timestamp desc."""
    return query_all(
        "SELECT session_id, timestamp, color_preview, name, version_date, success, notes, log FROM experiments ORDER BY timestamp DESC",
        (),
    )


def get_all_experiments_sorted_by_user_query(user_id=None):
    """Get all experiments sorted by timestamp desc. SQLite ignores user_id filtering (single-user)."""
    # SQLite is single-user, so we always return all experiments regardless of user_id
    return query_all(
        "SELECT session_id, timestamp, color_preview, name, version_date, success, notes, log FROM experiments ORDER BY timestamp DESC",
        (),
    )


def get_experiment_graph_topology_query(session_id):
    """Get graph topology for an experiment."""
    return query_one("SELECT graph_topology FROM experiments WHERE session_id=?", (session_id,))


def get_experiment_color_preview_query(session_id):
    """Get color preview for an experiment."""
    return query_one("SELECT color_preview FROM experiments WHERE session_id=?", (session_id,))


def get_experiment_environment_query(parent_session_id):
    """Get experiment cwd, command, and environment."""
    return query_one(
        "SELECT cwd, command, environment FROM experiments WHERE session_id=?", (parent_session_id,)
    )


def update_experiment_color_preview_query(color_preview_json, session_id):
    """Update experiment color preview."""
    execute(
        "UPDATE experiments SET color_preview=? WHERE session_id=?",
        (color_preview_json, session_id),
    )


def get_experiment_exec_info_query(session_id):
    """Get experiment execution info (cwd, command, environment)."""
    return query_one(
        "SELECT cwd, command, environment FROM experiments WHERE session_id=?", (session_id,)
    )


# Database cleanup queries
def delete_all_experiments_query():
    """Delete all records from experiments table."""
    execute("DELETE FROM experiments")


def delete_all_llm_calls_query():
    """Delete all records from llm_calls table."""
    execute("DELETE FROM llm_calls")


def get_session_name_query(session_id):
    """Get session name by session_id."""
    return query_one("SELECT name FROM experiments WHERE session_id=?", (session_id,))


def get_llm_call_input_api_type_query(session_id, node_id):
    """Get input and api_type from llm_calls by session_id and node_id."""
    return query_one(
        "SELECT input, api_type FROM llm_calls WHERE session_id=? AND node_id=?",
        (session_id, node_id),
    )


def get_llm_call_output_api_type_query(session_id, node_id):
    """Get output and api_type from llm_calls by session_id and node_id."""
    return query_one(
        "SELECT output, api_type FROM llm_calls WHERE session_id=? AND node_id=?",
        (session_id, node_id),
    )


def get_experiment_log_success_graph_query(session_id):
    """Get log, success, and graph_topology from experiments by session_id."""
    return query_one(
        "SELECT log, success, graph_topology FROM experiments WHERE session_id=?",
        (session_id,),
    )


# User management functions - SQLite is single-user so these raise errors
def upsert_user(google_id, email, name, picture):
    """SQLite doesn't support user management - single user database."""
    raise Exception(
        "User management not supported in local SQLite database. Switch to remote mode for multi-user support."
    )


def get_user_by_id_query(user_id):
    """SQLite doesn't support user management - single user database."""
    raise Exception(
        "User management not supported in local SQLite database. Switch to remote mode for multi-user support."
    )


def get_next_run_index_query():
    """Get the next run index based on how many runs already exist."""
    row = query_one("SELECT COUNT(*) as count FROM experiments", ())
    if row:
        return row["count"] + 1
    return 1


# ============================================================
# Lessons queries
# ============================================================


def get_all_lessons_query():
    """Get all lessons with their extracted_from and applied_to information."""
    # Get all lessons
    lessons = query_all(
        """
        SELECT l.lesson_id, l.lesson_text, l.from_session_id, l.from_node_id,
               e.name as from_run_name
        FROM lessons l
        LEFT JOIN experiments e ON l.from_session_id = e.session_id
        ORDER BY l.created_at DESC
        """,
        (),
    )
    return lessons


def get_lessons_applied_query(lesson_id):
    """Get all sessions/nodes where a lesson was applied."""
    return query_all(
        """
        SELECT la.session_id, la.node_id, e.name as run_name
        FROM lessons_applied la
        LEFT JOIN experiments e ON la.session_id = e.session_id
        WHERE la.lesson_id = ?
        ORDER BY la.applied_at DESC
        """,
        (lesson_id,),
    )


def insert_lesson_query(lesson_id, lesson_text, from_session_id=None, from_node_id=None):
    """Insert a new lesson."""
    execute(
        """
        INSERT INTO lessons (lesson_id, lesson_text, from_session_id, from_node_id)
        VALUES (?, ?, ?, ?)
        """,
        (lesson_id, lesson_text, from_session_id, from_node_id),
    )


def update_lesson_query(lesson_id, lesson_text):
    """Update an existing lesson's text."""
    execute(
        """
        UPDATE lessons SET lesson_text = ?, updated_at = datetime('now')
        WHERE lesson_id = ?
        """,
        (lesson_text, lesson_id),
    )


def delete_lesson_query(lesson_id):
    """Delete a lesson and its applied records."""
    execute("DELETE FROM lessons_applied WHERE lesson_id = ?", (lesson_id,))
    execute("DELETE FROM lessons WHERE lesson_id = ?", (lesson_id,))


def add_lesson_applied_query(lesson_id, session_id, node_id=None):
    """Record that a lesson was applied to a session/node."""
    execute(
        """
        INSERT OR IGNORE INTO lessons_applied (lesson_id, session_id, node_id)
        VALUES (?, ?, ?)
        """,
        (lesson_id, session_id, node_id),
    )


def remove_lesson_applied_query(lesson_id, session_id, node_id=None):
    """Remove a lesson application record."""
    if node_id:
        execute(
            "DELETE FROM lessons_applied WHERE lesson_id = ? AND session_id = ? AND node_id = ?",
            (lesson_id, session_id, node_id),
        )
    else:
        execute(
            "DELETE FROM lessons_applied WHERE lesson_id = ? AND session_id = ? AND node_id IS NULL",
            (lesson_id, session_id),
        )
