from saber.filters.gaussian import gaussian_smoothing_3d
import torch.nn.functional as F
from saber.utils import io
from scipy import ndimage
import torch, gc, skimage
import numpy as np

def apply_classifier(image, masks, classifier, desired_class: int = None, min_mask_area: int = 100, batchsize: int = 32):
    """
    Apply a domain expert classifier to a segmentation mask and return the masks that match the desired class.
    """

    # Extract the 'segmentation' array from each SAM2 mask and convert booleans to uint8 (0, 1)
    sam2_masks = [mask_info['segmentation'].astype(np.uint8) for mask_info in masks]
    sam2_masks = np.array(sam2_masks)

    # Run predictions using your classifier and Determine predicted class for each mask
    with torch.no_grad():
        predictions = classifier.batch_predict(image[:,:,0], sam2_masks, batchsize)

    return convert_predictions_to_masks(predictions, masks, desired_class, min_mask_area) 

def convert_predictions_to_masks(predictions, masks, desired_class: int = None, min_mask_area: int = 100):

    if isinstance(masks, np.ndarray):
        masks = masks_to_list(masks)

     # Get predicted class for each mask
    predicted_classes = np.argmax(predictions, axis=1)  

    # Instance Segmentation 
    # If a desired class is specified, filter the masks that match the desired class
    if desired_class > 0 and desired_class is not None:

        # Get confidence for the desired class
        confidence_scores = predictions[:,desired_class]

        # Filter masks that match the desired class
        target_indices = [i for i, pred in enumerate(predicted_classes) if pred == desired_class]
        masks = [masks[i] for i in target_indices]
        confidence_scores = confidence_scores[target_indices]

        # Apply Consensus-Based Resolution to the Target Masks
        if len(masks) > 0:
            # Filter Out Small Masks and Apply Consensus-Based Resolution
            masks = _consensus_based_resolution(masks[0]['segmentation'].shape, masks, confidence_scores)
            
            # Sort Masks by Area
            masks = [mask for mask in masks if mask['area'] >= min_mask_area] 
            masks = sorted(masks, key=lambda x: x['area'], reverse=False)
            
    # Semantic Segmentation
    else:
        # Get the shape from the first mask
        if len(masks) > 0:
            nx, ny = masks[0]['segmentation'].shape
        else:
            return np.array([])  # Return empty array if no masks

        masks = _semantic_segmentation(masks, predictions)

    return masks

def _consensus_based_resolution(image_shape, masks, confidences):
    """
    Implements consensus-based approach by combining overlapping masks.
    """
    h, w = image_shape
    
    # Create confidence-weighted mask map
    confidence_map = np.zeros((h, w), dtype=np.float32)
    overlap_count = np.zeros((h, w), dtype=np.int32)
    
    # Accumulate all masks
    for mask_dict, conf in zip(masks, confidences):
        seg = mask_dict['segmentation']
        confidence_map += seg * conf
        overlap_count += seg
    
    # Create consensus regions (where any mask exists)
    consensus_mask = overlap_count > 0
    
    # Normalize confidence by overlap count
    with np.errstate(divide='ignore', invalid='ignore'):
        avg_confidence = np.divide(confidence_map, overlap_count)
        avg_confidence = np.nan_to_num(avg_confidence)
    
    # Label connected components in the consensus mask
    labeled_mask, num_components = ndimage.label(consensus_mask)
    
    # Create new mask dictionaries for each consensus region
    consensus_masks = []
    for label in range(1, num_components + 1):
        component_mask = labeled_mask == label
        
        # # Skip very small components
        # if np.sum(component_mask) < 100:  # Minimum area threshold
        #     continue
        
        # Calculate average confidence in this component
        component_confidence = np.mean(avg_confidence[component_mask])
        
        # Find bounding box
        coords = np.where(component_mask)
        y_min, y_max = np.min(coords[0]), np.max(coords[0])
        x_min, x_max = np.min(coords[1]), np.max(coords[1])
        
        # Create new mask dictionary
        new_mask = {
            'segmentation': component_mask,
            'area': int(np.sum(component_mask)),
            'bbox': [int(x_min), int(y_min), int(x_max - x_min), int(y_max - y_min)],
            'predicted_iou': float(component_confidence),  # Use confidence as IoU proxy
            'point_coords': [[int((x_min + x_max) / 2), int((y_min + y_max) / 2)]],
            'stability_score': float(component_confidence),  # Use confidence as stability score
            'crop_box': [int(x_min), int(y_min), int(x_max), int(y_max)]
        }
        
        consensus_masks.append(new_mask)
    
    return consensus_masks    

def _semantic_segmentation(masks, predictions):
    """
    Get array that returns the masks as semantic segmentation.
    Merges all masks belonging to each predicted class.
    """
    predicted_classes = np.argmax(predictions, axis=1)
    max_class = predictions.shape[1]
    
    # Initialize empty masks for each class (excluding class 0)
    output_masks = []
    for ii in range(1, max_class):  # Start from 1 to skip background class 0
        output_masks.append({
            'segmentation': np.zeros(masks[0]['segmentation'].shape, dtype=np.uint8),
            'area': 0,
            'label': ii
        })

    # Merge masks for each class
    for ii in range(len(masks)):
        predicted_class = predicted_classes[ii]
        if predicted_class > 0:  # Skip background class 0
            class_idx = predicted_class - 1  # Adjust index since we start from class 1
            
            # Merge segmentation masks using logical OR
            output_masks[class_idx]['segmentation'] = np.logical_or(
                output_masks[class_idx]['segmentation'], 
                masks[ii]['segmentation']
            ).astype(np.bool)
            
            # Accumulate area
            output_masks[class_idx]['area'] += masks[ii]['area']
    
    return output_masks

def masks_to_array(mask_list):
    """
    Convert list of masks to single label matrix
    """
    if not isinstance(mask_list, list):
        print('Returning None')
        return None

    # Convert Masks to Numpy Array 
    (nx, ny) = mask_list[0]['segmentation'].shape

    # Determine Appropriate Data Type
    if len(mask_list) == 0:
        return np.zeros((0, nx, ny), dtype=np.bool)
    elif len(mask_list) < 256:
        dtype = np.uint8
    elif len(mask_list) < 65536:
        dtype = np.uint16
    else:
        dtype = np.uint32
    masks = np.zeros([len(mask_list), nx, ny], dtype=dtype)

    # Populate the numpy array
    for j, mask in enumerate(mask_list):
        masks[j] = mask['segmentation'].astype(dtype) * (j + 1)
    
    return masks

def masks_to_list(masks):
    """
    Convert a 3D mask array to a list of masks.
    """

    # If already a list, return original masks
    if isinstance(masks, list):
        return masks

    # Convert masks to list of dictionaries
    masks_list = []
    vals = np.unique(masks)
    for val in vals:
        mask = masks == val
        masks_list.append({
            'segmentation': mask,
            'area': np.sum(mask > 0)})
    
    return masks_list

def segments_to_mask(video_segments, masks, mask_shape):
    """
    Convert SAM2 video segments to 3D mask array.
    """
    
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    first_obj = next(iter(next(iter(video_segments.values())).values()))
    sam_h, sam_w = first_obj[0].shape
    
    frames = sorted(video_segments.keys())
    temp = torch.zeros((len(frames), sam_h, sam_w), dtype=torch.int32, device=device)
    
    for i, f in enumerate(frames):
        for obj_id, obj_mask in video_segments[f].items():
            mask_t = torch.from_numpy(obj_mask[0]).to(device) if isinstance(obj_mask[0], np.ndarray) else obj_mask[0].to(device)
            temp[i][mask_t] = obj_id
    
    if sam_h != mask_shape[1] or sam_w != mask_shape[2]:
        temp = F.interpolate(temp.unsqueeze(1).float(), size=(mask_shape[1], mask_shape[2]), mode='nearest').squeeze(1).int()
    
    for i, f in enumerate(frames):
        masks[f] = temp[i].cpu().numpy()
    
    return masks

def fast_3d_gaussian_smoothing(volume, scale=0.075, deviceID = None):
    """
    Apply fast 3D Gaussian smoothing to a volume.
    
    This function takes either a binary volume or a labeled segmentation and applies
    efficient Gaussian smoothing with automatic sigma estimation based on volume.
    
    Args:
        volume: 3D numpy array (binary mask or segmentation with integer labels)
        sigma: Fixed sigma value to use (overrides automatic estimation if provided)
        scale: Scale factor for automatic sigma estimation (ignored if sigma is provided)
        
    Returns:
        Smoothed 3D volume with the same shape as input
    """
    
    # Check if CUDA is available
    device = io.get_available_devices(deviceID)
    
    # Check if input is 3D
    if volume.ndim != 3:
        raise ValueError(f"Expected 3D input, got {volume.ndim}D")
    
    # Check if input is binary or multi-label
    unique_values = np.unique(volume)
    
    # For segmentation with multiple labels
    result = np.zeros_like(volume, dtype=np.uint8)
    
    # Get unique labels (excluding 0 which is typically background)
    labels = unique_values
    if 0 in labels:
        labels = labels[labels != 0]
    
    # Apply Gaussian Smoothing to Each Label
    for label in labels:
        # Create binary mask for this label
        binary_mask = (volume == label)
        
        # Estimate appropriate sigma for this specific label
        label_sigma = _estimate_feature_size_3d(binary_mask, scale)
        
        # Apply 3D Gaussian filter
        smoothed = gaussian_smoothing_3d(binary_mask, label_sigma, device)
        
        # Threshold and add to result
        smoothed = (smoothed > 0.5)
        result[smoothed] = label

        # Explicitly delete intermediate results
        del binary_mask, smoothed
        # Force garbage collection
        gc.collect()
        if device.type == 'cuda':
            torch.cuda.empty_cache()
    
    return result

def _estimate_feature_size_3d(binary_volume, scale=0.075):
    """
    Estimate feature size for a 3D binary volume based on its volume.
    
    Args:
        binary_volume: 3D binary numpy array
        scale: Scale factor for sigma calculation
        
    Returns:
        Estimated sigma value
    """
    
    # Count total volume (number of True voxels)
    volume = np.sum(binary_volume)
    
    # For 3D: approximate as sphere
    # Volume of sphere = (4/3) * pi * r^3, so diameter = 2 * r = 2 * ((3V)/(4Ï€))^(1/3)
    approx_diameter = 2 * ((3 * volume) / (4 * np.pi))**(1/3)
    
    # Scale to get sigma
    sigma = scale * approx_diameter
    return sigma


# def merge_segmentation_masks(segmentation, min_volume_threshold=100):
#     """
#     Process a 3D segmentation array where different segments have unique labels.
#     Merge overlapping regions using OR operation, filter small objects, and relabel consecutively.
    
#     Args:
#         segmentation: 3D numpy array where different segments have unique label values
#         min_volume_threshold: Minimum volume (in voxels) for an object to be kept
        
#     Returns:
#         Filtered segmentation mask with consecutive labels, where overlapping regions are merged
#     """
#     if segmentation is None:
#         return None
    
#     # Get unique labels, excluding background (0)
#     unique_labels = np.unique(segmentation)
#     if 0 in unique_labels:
#         unique_labels = unique_labels[1:]
    
#     # Create a binary mask for all objects
#     combined_binary = np.zeros_like(segmentation, dtype=bool)
#     for label in unique_labels:
#         combined_binary = np.logical_or(combined_binary, segmentation == label)
    
#     # Label the connected components in the combined binary mask
#     labeled_mask, num_features = ndimage.label(combined_binary)
    
#     # Filter objects by volume and keep track of properties
#     region_properties = []
#     for label in range(1, num_features + 1):
#         # Create a binary mask for this label
#         mask = (labeled_mask == label)
        
#         # Calculate volume (number of voxels)
#         volume = np.sum(mask)
        
#         # Skip objects smaller than minimum volume threshold
#         if volume < min_volume_threshold:
#             continue
        
#         # Store properties
#         region_properties.append({
#             'label': label,
#             'volume': volume
#         })
    
#     # Sort regions by volume (largest first)
#     region_properties.sort(key=lambda x: x['volume'], reverse=True)
    
#     # Create new segmentation with consecutively labeled objects
#     new_segmentation = np.zeros_like(segmentation)
    
#     # Assign new consecutive labels
#     for new_label, prop in enumerate(region_properties, 1):
#         old_label = prop['label']
#         new_segmentation[labeled_mask == old_label] = new_label
    
#     return new_segmentation