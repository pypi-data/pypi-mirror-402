from __future__ import annotations
from saber import cli_context
import rich_click as click

def get_evaluator_class():
    from saber.classifier.trainer import ClassifierTrainer
    from saber.classifier.datasets import singleZarrDataset, augment
    from torch.utils.data import DataLoader
    from monai.transforms import Compose
    import torch

    class ClassifierEvaluator(ClassifierTrainer):
        """
        Classifier Evaluator class for evaluating a classifier model on a test set.
        """
        
        def __init__(self, dataset, model, device, beta=1.0, include_background=False):
            """
            Initialize the Classifier Evaluator.
            """
            
            dummy_loss_fn = lambda x, y: torch.tensor(0.0)
            
            # Call parent constructor with dummy optimizer/scheduler
            super().__init__(
                model=model,
                optimizer=None,
                scheduler=None,
                loss_fn=dummy_loss_fn,
                device=device,
                beta=beta,
                include_background=include_background
            )
            # Freeze model weights
            for param in self.model.parameters():
                param.requires_grad = False

            # Create Dataloader
            transforms = Compose([augment.get_validation_transforms()])
            dataset = singleZarrDataset.ZarrSegmentationDataset(dataset, mode='val', transform=transforms)
            self.dataloader = DataLoader(dataset, batch_size=32, shuffle=False)

        def evaluate(self, output_file = None):
            """
            Evaluate the model and optionally save results to a file.
            
            Args:
                output_file (str, optional): Path to save results. If None, only prints results.
                format (str): Output format - 'json', 'yaml', or 'csv'
                
            Returns:
                dict: Dictionary with evaluation metrics
            """        
            from tqdm import tqdm
            import torch, os, csv
            
            self.model.eval()
            all_preds = []
            all_labels = []

            with torch.no_grad():
                for batch_data in tqdm(self.dataloader, desc="Evaluating"):
                    loss, preds, labels = self.process_batch(batch_data, mode='val')
                    all_preds.extend(preds.detach().cpu().tolist())
                    all_labels.extend(labels.detach().cpu().tolist())

            # Use the inherited compute_metrics method
            precisions, recalls, f1s, fbetas = self.compute_metrics(all_preds, all_labels)
            
            # Calculate Macro Metrics
            macro_fbeta = sum(fbetas[i] for i in self.indices) / len(self.indices)
            macro_recall = sum(recalls[i] for i in self.indices) / len(self.indices)
            macro_precision = sum(precisions[i] for i in self.indices) / len(self.indices)
            macro_f1 = sum(f1s[i] for i in self.indices) / len(self.indices)

            # Create results dictionary
            results = {
                'beta': float(self.beta),
                'precision': float(macro_precision),
                'recall': float(macro_recall),
                'f1': float(macro_f1),
                'fbeta': float(macro_fbeta)
            }
            
            # Save results if output_file is provided
            if output_file:
                if os.path.dirname(output_file) != '' :
                    os.makedirs(os.path.dirname(output_file), exist_ok=True)
                
                with open(output_file, 'w', newline='') as f:
                    writer = csv.writer(f)
                    writer.writerow(['metric', 'value'])  # Header
                    for metric, value in results.items():
                        writer.writerow([metric, value])
                    
                print(f"Results saved to {output_file}")
            else:
                # Print results
                print(f"Evaluation Results:")
                print(f"F-beta Score (Î²={self.beta}): {macro_fbeta:.4f}")
                print(f"Precision: {macro_precision:.4f}")
                print(f"Recall: {macro_recall:.4f}")

            return results

    return ClassifierEvaluator

@click.command(context_settings=cli_context)
@click.option('--test', type=str, required=True, help='Path to the Test Zarr File')
@click.option('--model-config', type=str, required=True, help='Path to the Model Config')
@click.option('--model-weights', type=str, required=True, help='Path to the Model Weights')
@click.option('--beta', type=float, default=1.0, help='Beta for the F-Beta Score')
@click.option('--output', type=str, default=None, help='Path to save results')
def evaluate(test, model_config, model_weights, beta, output):
    """
    Evaluate a classifier model on a test set.
    """

    run_evaluate(test, model_config, model_weights, beta, output)

def run_evaluate(test, model_config, model_weights, beta, output):
    """
    Run Evaluation on an a Test Zarr file with a trained classifier.
    
    Args:
    """
    from saber.classifier.models import common
    from saber.utils import io
    import yaml, os, torch


    # Check if test file exists
    if not os.path.exists(test):
        raise FileNotFoundError(f"Test Zarr file {test} does not exist.")

    # Check if output file has a .csv extension
    if output is not None and output[-3:] != 'csv':
        raise ValueError(f"Output file must have a .csv extension. Received {output}.")

    # Load Model Config
    if os.path.exists(model_weights) and os.path.exists(model_config):
        with open(model_config, 'r') as f:
            config = yaml.safe_load(f)
    else:
        raise FileNotFoundError(f"Model config file {model_config} or model weights file {model_weights} does not exist.")            
            
    # Get Model Checkpoint
    model = common.get_classifier_model(
        'SAM2', 
        config['model']['num_classes'], 
        config['amg_params']['sam2_cfg']  )    
    model.load_state_dict(torch.load(model_weights, weights_only=True))

    # Create the Evaluator
    device = io.get_available_devices()
    model = model.to(device)

    EvaluatorClass = get_evaluator_class()
    evaluator = EvaluatorClass(test, model, device, beta)

    # Evaluate the Model
    evaluator.evaluate(output)


