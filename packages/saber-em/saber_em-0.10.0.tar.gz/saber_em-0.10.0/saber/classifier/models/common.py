from saber.classifier.models import ( SwinTransformer, ConvNeXt, SAM2 )
import torch.nn as nn
import os, torch

def get_classifier_model(backbone, num_classes, model_size, deviceID=0, **kwargs):
    model_map = {
        'ConvNeXt': ConvNeXt.ConvNeXtClassifier,
        'SwinTransformer': SwinTransformer.SwinTransformerClassifier,
        'SAM2': SAM2.SAM2Classifier,
    }
    if backbone not in model_map:
        raise ValueError(f"Unsupported backbone type: {backbone}")
    
    # cryoDinoV2 has a fixed backbone, so omit model_size
    if backbone == 'SAM2': # We Need to Pass the Device ID to SAM2 Image Encoder
        kwargs['deviceID'] = deviceID
        return  model_map[backbone](num_classes=num_classes, **kwargs)
    else:
        return  model_map[backbone](num_classes=num_classes, backbone_type=model_size, **kwargs)
    
def get_predictor(model_weights, model_config, deviceID: int = 0):
    """
    Initialize the Predictor with the specified model weights and config.

    Args:
        model_weights (str): Path to the model's weight file (.pth).
        model_config (str): Path to the model's config file (.yaml).
        deviceID (int): Device ID for the predictor.

    Returns:
        Predictor: The initialized predictor.
    """
    from saber.classifier.models.predictor import Predictor

    # If model_weights or model_config is missing, return None
    if model_weights is None or model_config is None:
        predictor = None
    # Check if the model_weights and model_config exist, return the predictor
    else:
        if not os.path.exists(model_weights):
            raise FileNotFoundError(f"Model weights file {model_weights} does not exist.")
        elif not os.path.exists(model_config):
            raise FileNotFoundError(f"Model config file {model_config} does not exist.")
        predictor = Predictor(model_config, model_weights, deviceID = deviceID)

    return predictor

def initialize_weights(model):
    """Initialize the weights of the classification and projection head for training stability."""
    # Initialize classifier layers
    for m in model.classifier.modules():
        if isinstance(m, nn.Linear):
            nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
            if m.bias is not None:
                nn.init.constant_(m.bias, 0)
        elif isinstance(m, nn.BatchNorm1d):
            nn.init.constant_(m.weight, 1)
            nn.init.constant_(m.bias, 0)
        elif isinstance(m, nn.LayerNorm):
            nn.init.constant_(m.weight, 1)
            nn.init.constant_(m.bias, 0)

    # Initialize projection layers (Only for SAM2 and DinoV2)
    if model.name == 'SAM2Classifer' or model.name == 'DinoV2Classifier':
        for m in model.projection.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)

def load_model_weights(model, model_weights, fine_tune=False):
    """
    Load model weights and freeze backbone if fine_tune is False.
    """
    # Extract state_dict from checkpoint dictionary
    print(f'Loading classifier weights from {model_weights}...')
    checkpoint = torch.load(model_weights, weights_only=True)
    if isinstance(checkpoint, dict) and "model" in checkpoint:
        state_dict = checkpoint["model"]
    else:
        state_dict = checkpoint  # Backwards compatibility
    model.load_state_dict(state_dict)
    
    # Freeze backbone (only train classifier head)
    if fine_tune:
        for param in model.backbone.model.parameters():
            param.requires_grad = False

    return model
    