# LLM Interceptor Configuration
# Copy this file to lli.toml and customize as needed
#
# Backward-compatible note: the legacy file name `cci.toml` is still supported.

[proxy]
# Proxy server binding address
host = "127.0.0.1"
# Proxy server port
port = 9090
# Skip SSL verification for upstream connections (not recommended for production)
ssl_insecure = false

[filter]
# URL patterns to capture (regex, case-insensitive)
# Default: common LLM API providers
include_patterns = [
    ".*api\\.anthropic\\.com.*",
    ".*api\\.openai\\.com.*",
    ".*generativelanguage\\.googleapis\\.com.*",
    ".*api\\.together\\.xyz.*",
    ".*api\\.groq\\.com.*",
    ".*api\\.mistral\\.ai.*",
    ".*api\\.cohere\\.ai.*",
    ".*api\\.deepseek\\.com.*",
]

# URL patterns to exclude (takes precedence over include)
exclude_patterns = [
    # ".*health.*",
    # ".*metrics.*",
]

[masking]
# Mask sensitive headers (Authorization, API keys)
mask_auth_headers = true

# List of header names to mask (case-insensitive)
sensitive_headers = [
    "authorization",
    "x-api-key",
    "api-key",
]

# List of body fields to mask (dot notation for nested fields)
# Example: ["credentials.api_key", "config.secret"]
sensitive_body_fields = []

# Pattern to replace masked values
mask_pattern = "***MASKED***"

[storage]
# Default output file name
output_file = "lli_trace.jsonl"

# Pretty-print JSON (useful for debugging, not recommended for production)
pretty_json = false

# Maximum file size in MB before rotation (0 = no rotation)
max_file_size_mb = 0

[logging]
# Log level: DEBUG, INFO, WARNING, ERROR
level = "INFO"

# Optional log file path (in addition to console)
# log_file = "/var/log/lli/lli.log"
