# ğŸ Colmena - AI Agent Orchestration Library

[![Rust](https://img.shields.io/badge/rust-1.70+-orange.svg)](https://www.rust-lang.org)
[![Python](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org)
[![License](https://img.shields.io/badge/license-MIT-green.svg)](LICENSE)
[![Status](https://img.shields.io/badge/status-alpha-red.svg)](https://github.com/your-org/colmena)

Una librerÃ­a **nativa** de Rust para la orquestaciÃ³n de agentes de IA, diseÃ±ada siguiendo principios de **Arquitectura Hexagonal** y expuesta a Python mediante PyO3. Proporciona una interfaz unificada para mÃºltiples proveedores de LLM con llamadas sÃ­ncronas y streaming.

## ğŸ¯ CaracterÃ­sticas

### ğŸ¤– MÃ³dulo LLM
- **ğŸ”Œ Multi-Proveedor**: Soporte nativo para OpenAI, Gemini y Anthropic
- **âš¡ Streaming**: Respuestas en tiempo real con chunks de texto
- **ğŸ Python Ready**: Bindings nativos compilados con PyO3 (no wrappers)
- **ğŸ§  Memoria Persistente**: SQLite y PostgreSQL para historial de conversaciones
- **ğŸ› ï¸ Tool Calling**: Soporte nativo para herramientas y agentes

### âš™ï¸ Motor DAG (DAG Engine)
- **ğŸ“Š EjecuciÃ³n de Grafos**: Motor para ejecutar flujos de trabajo complejos
- **ğŸ”— ComposiciÃ³n de Nodos**: Conecta LLMs, HTTP, Python y mÃ¡s
- **ğŸ Python Node**: Ejecuta cÃ³digo Python arbitrario con datos JSON
- **ğŸ¤– IntegraciÃ³n LLM**: Los LLMs pueden generar y ejecutar cÃ³digo Python
- **ğŸ“ ConfiguraciÃ³n JSON**: Define grafos con archivos JSON simple

### ğŸ—ï¸ Arquitectura
- **ğŸ—ï¸ Arquitectura Limpia**: ImplementaciÃ³n hexagonal para mÃ¡xima extensibilidad
- **ğŸ”§ ConfiguraciÃ³n Flexible**: API keys desde variables de entorno o valores directos
- **ğŸ›¡ï¸ Manejo de Errores**: GestiÃ³n robusta con tipos especÃ­ficos y recuperaciÃ³n
- **ğŸš€ Performance**: CÃ³digo nativo Rust, sin overhead de interpretaciÃ³n
- **ğŸ”’ Type Safety**: GarantÃ­as de tipos en tiempo de compilaciÃ³n

## âœ… Estado del Proyecto - FUNCIONAL

**MÃ³dulos Completamente Funcionales:**

### ğŸ¤– MÃ³dulo LLM Base
- âœ… **Arquitectura hexagonal completa** y probada
- âœ… **Soporte Multi-LLM**: OpenAI, Gemini, Anthropic funcionando
- âœ… **Llamadas sÃ­ncronas y streaming** implementadas
- âœ… **Bindings de Python nativos** compilados y probados
- âœ… **GestiÃ³n de configuraciÃ³n** flexible y robusta
- âœ… **Tests completos**: 8/8 tests pasando con Gemini
- âœ… **DocumentaciÃ³n tÃ©cnica** y ejemplos de uso

### âš™ï¸ Motor DAG (DAG Engine)
- âœ… **EjecuciÃ³n de grafos** dirigidos acÃ­clicos (DAGs)
- âœ… **Nodos disponibles**: Debug, Math, HTTP, LLM, Python, Trigger
- âœ… **Python Node**: Ejecuta cÃ³digo Python con integraciÃ³n LLM
- âœ… **Tool Calling**: Los LLMs pueden usar otros nodos como herramientas
- âœ… **Memoria persistente**: SQLite y PostgreSQL
- âœ… **Servidor HTTP**: API REST para ejecuciÃ³n de DAGs

## ğŸ“ Estructura del Proyecto

```
src/
â”œâ”€â”€ lib.rs                          # Entry point de la librerÃ­a
â”œâ”€â”€ llm/                           # ğŸ¤– MÃ³dulo LLM
â”‚   â”œâ”€â”€ domain/                    # ğŸ›ï¸ Capa de Dominio
â”‚   â”‚   â”œâ”€â”€ llm_provider.rs       # Enums de proveedores
â”‚   â”‚   â”œâ”€â”€ llm_config.rs         # Configuraciones
â”‚   â”‚   â”œâ”€â”€ llm_request.rs        # Requests
â”‚   â”‚   â”œâ”€â”€ llm_response.rs       # Responses
â”‚   â”‚   â”œâ”€â”€ llm_repository.rs     # Trait principal
â”‚   â”‚   â””â”€â”€ value_objects/        # Value Objects
â”‚   â”œâ”€â”€ application/               # ğŸ¯ Capa de AplicaciÃ³n
â”‚   â”‚   â”œâ”€â”€ llm_call_use_case.rs  # Caso de uso: llamada normal
â”‚   â”‚   â”œâ”€â”€ llm_stream_use_case.rs # Caso de uso: streaming
â”‚   â”‚   â””â”€â”€ agent_service.rs      # Servicio de agentes con tools
â”‚   â””â”€â”€ infrastructure/            # ğŸ”§ Capa de Infraestructura
â”‚       â”œâ”€â”€ openai_adapter.rs     # Adaptador OpenAI
â”‚       â”œâ”€â”€ gemini_adapter.rs     # Adaptador Gemini
â”‚       â”œâ”€â”€ anthropic_adapter.rs  # Adaptador Anthropic
â”‚       â”œâ”€â”€ llm_provider_factory.rs # Factory
â”‚       â””â”€â”€ persistence/          # Repositorios de memoria
â”‚           â”œâ”€â”€ sqlite_conversation_repository.rs
â”‚           â””â”€â”€ postgres_conversation_repository.rs
â”œâ”€â”€ dag_engine/                    # âš™ï¸ Motor DAG
â”‚   â”œâ”€â”€ main.rs                   # CLI para ejecutar DAGs
â”‚   â”œâ”€â”€ domain/                   # ğŸ›ï¸ Capa de Dominio
â”‚   â”‚   â”œâ”€â”€ node.rs              # Trait ExecutableNode
â”‚   â”‚   â””â”€â”€ dag.rs               # Estructura del grafo
â”‚   â”œâ”€â”€ application/              # ğŸ¯ Capa de AplicaciÃ³n
â”‚   â”‚   â”œâ”€â”€ dag_executor.rs      # Ejecutor de grafos
â”‚   â”‚   â””â”€â”€ ports/               # Puertos/interfaces
â”‚   â”œâ”€â”€ infrastructure/           # ğŸ”§ Capa de Infraestructura
â”‚   â”‚   â”œâ”€â”€ nodes/               # Implementaciones de nodos
â”‚   â”‚   â”‚   â”œâ”€â”€ debug.rs        # Nodos de depuraciÃ³n
â”‚   â”‚   â”‚   â”œâ”€â”€ math.rs         # Nodos matemÃ¡ticos
â”‚   â”‚   â”‚   â”œâ”€â”€ http.rs         # Nodo HTTP
â”‚   â”‚   â”‚   â”œâ”€â”€ llm.rs          # Nodo LLM
â”‚   â”‚   â”‚   â””â”€â”€ python_node.rs  # ğŸ Nodo Python (NUEVO)
â”‚   â”‚   â”œâ”€â”€ registry.rs          # Registro de nodos
â”‚   â”‚   â””â”€â”€ dag_tool_executor.rs # Herramientas para LLM
â”‚   â””â”€â”€ api/                     # ğŸŒ API REST
â”‚       â””â”€â”€ server.rs            # Servidor HTTP
â”œâ”€â”€ shared/                        # ğŸ¤ Funcionalidades compartidas
â”‚   â””â”€â”€ infrastructure/
â”‚       â”œâ”€â”€ config_resolver.rs    # ResoluciÃ³n de configuraciÃ³n
â”‚       â””â”€â”€ service_container.rs  # Contenedor de servicios
â””â”€â”€ python_bindings/              # ğŸ Bindings para Python
    â””â”€â”€ mod.rs                    # Wrappers PyO3
```

## ğŸ› ï¸ TecnologÃ­as

- **Rust**: Lenguaje principal, performance y seguridad
- **PyO3**: Bindings nativos para Python
- **Tokio**: Runtime asÃ­ncrono
- **Reqwest**: Cliente HTTP
- **Serde**: SerializaciÃ³n/deserializaciÃ³n
- **Arquitectura Hexagonal**: SeparaciÃ³n limpia de responsabilidades

## ğŸ“– DocumentaciÃ³n

### ğŸš€ Para Usuarios
- [ğŸ“¦ **GuÃ­a de InstalaciÃ³n**](docs/INSTALLATION_GUIDE.md) - InstalaciÃ³n paso a paso en cualquier sistema operativo
- [ğŸ **Ejemplos de Uso en Python**](docs/PYTHON_USAGE_EXAMPLES.md) - Casos de uso prÃ¡cticos y ejemplos completos
- [ğŸ”§ **GuÃ­a de SoluciÃ³n de Problemas**](docs/TROUBLESHOOTING.md) - Soluciones a problemas comunes

### ğŸ‘©â€ğŸ’» Para Desarrolladores
- [ğŸ“‹ **Documento de DiseÃ±o y Desarrollo (DDS)**](docs/dds/MODULO_LLM_DISEÃ‘O.md) - Arquitectura detallada del mÃ³dulo LLM
- [ğŸ—ï¸ **GuÃ­a de Arquitectura Hexagonal**](docs/dds/ARQUITECTURA_HEXAGONAL_GUIA.md) - Principios arquitectÃ³nicos aplicados
- [ğŸ‘©â€ğŸ’» **GuÃ­a del Desarrollador**](docs/DEVELOPER_GUIDE.md) - Contribuir, extender y entender el cÃ³digo
- [âš™ï¸ **CLAUDE.md**](CLAUDE.md) - GuÃ­a para desarrollo con Claude Code

## ğŸš€ InstalaciÃ³n y CompilaciÃ³n

### Prerrequisitos del Sistema

**En Linux (Ubuntu/Debian):**
```bash
# Instalar dependencias del sistema
sudo apt update
sudo apt install curl build-essential python3-dev python3-pip

# Instalar Rust
curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh
source ~/.bashrc
```

**En macOS:**
```bash
# Instalar Xcode command line tools
xcode-select --install

# Instalar Rust
curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh
source ~/.bashrc
```

**En Windows:**
1. Descarga e instala Rust desde [rustup.rs](https://rustup.rs/)
2. Instala Visual Studio Build Tools con C++ support
3. Instala Python 3.8+ desde [python.org](https://python.org)

### CompilaciÃ³n Paso a Paso

#### 1. Clonar y Preparar el Proyecto
```bash
# Clonar el repositorio
git clone https://github.com/tu-org/colmena.git
cd colmena

# Verificar que Rust estÃ¡ instalado correctamente
rustc --version
cargo --version
```

#### 2. Compilar la LibrerÃ­a Rust
```bash
# Verificar que el cÃ³digo compila
cargo check

# Ejecutar tests de Rust (opcional)
cargo test

# Compilar en modo release (opcional, para mejor performance)
cargo build --release
```

#### 3. Configurar Python y Maturin
```bash
# Crear entorno virtual de Python
python3 -m venv venv
source venv/bin/activate  # En Windows: venv\Scripts\activate

# Instalar maturin (herramienta para compilar extensiones Python en Rust)
pip install maturin

# Compilar e instalar la librerÃ­a Python
maturin develop --release

# Verificar la instalaciÃ³n
python -c "import colmena; print('âœ… Colmena instalado correctamente')"
```

### VerificaciÃ³n de la InstalaciÃ³n

Ejecuta este script para verificar que todo funciona:

```python
# test_installation.py
import colmena

# Verificar que el mÃ³dulo estÃ¡ disponible
print(f"âœ… MÃ³dulo colmena cargado desde: {colmena.__file__}")

# Verificar funcionalidad bÃ¡sica
llm = colmena.ColmenaLlm()
print(f"âœ… ColmenaLlm inicializado: {type(llm)}")

# Test con API key vÃ¡lida (reemplaza con tu key)
try:
    response = llm.call(
        messages=["Hola, Â¿cÃ³mo estÃ¡s?"],
        provider="gemini",
        api_key="TU_API_KEY_AQUI"
    )
    print(f"âœ… Llamada exitosa: {response[:50]}...")
except Exception as e:
    print(f"âš ï¸  Necesitas una API key vÃ¡lida: {e}")
```

### Variables de Entorno (Opcional)

Puedes configurar las API keys como variables de entorno:

```bash
# .env o en tu shell
export OPENAI_API_KEY="tu-openai-key"
export GEMINI_API_KEY="tu-gemini-key"
export ANTHROPIC_API_KEY="tu-anthropic-key"
```

### SoluciÃ³n de Problemas Comunes

**Error: "Microsoft Visual C++ 14.0 is required" (Windows)**
```bash
# Instalar Visual Studio Build Tools
# Descargar desde: https://visualstudio.microsoft.com/visual-cpp-build-tools/
```

**Error: "python3-dev not found" (Linux)**
```bash
sudo apt install python3-dev python3-pip
```

**Error: "maturin not found"**
```bash
pip install --upgrade pip
pip install maturin
```

**Error de compilaciÃ³n con PyO3**
```bash
# Verificar versiÃ³n de Python (debe ser 3.8+)
python --version

# Reinstalar con configuraciÃ³n especÃ­fica
pip uninstall maturin
pip install maturin
maturin develop --release
```

## ğŸ® Uso de la LibrerÃ­a

### Importar y Configurar

```python
import colmena

# Inicializar la librerÃ­a
llm = colmena.ColmenaLlm()
```

### Llamadas SÃ­ncronas

```python
# Llamada simple con Gemini
response = llm.call(
    messages=["Â¿QuÃ© es la arquitectura hexagonal?"],
    provider="gemini",
    model="gemini-1.5-flash",
    api_key="tu-gemini-api-key"
)
print(response)

# Llamada con OpenAI
response = llm.call(
    messages=["Explica quÃ© es Rust"],
    provider="openai",
    model="gpt-4",
    api_key="tu-openai-api-key",
    temperature=0.7,
    max_tokens=500
)
print(response)

# Llamada con Anthropic
response = llm.call(
    messages=["Â¿CÃ³mo funciona PyO3?"],
    provider="anthropic",
    model="claude-3-sonnet-20240229",
    api_key="tu-anthropic-api-key"
)
print(response)
```

### Llamadas con Streaming

```python
# Streaming con cualquier proveedor
chunks = llm.stream(
    messages=["Cuenta una historia corta"],
    provider="gemini",
    api_key="tu-api-key"
)

for chunk in chunks:
    print(chunk, end="", flush=True)
print()  # Nueva lÃ­nea al final
```

### Conversaciones con Contexto

```python
# Mantener contexto en mÃºltiples mensajes
messages = [
    "Hola, soy un desarrollador de Rust",
    "Â¿Puedes explicarme quÃ© es PyO3?",
    "Â¿Y cÃ³mo se compila una extensiÃ³n Python?"
]

response = llm.call(
    messages=messages,
    provider="gemini",
    api_key="tu-api-key"
)
print(response)
```

### ConfiguraciÃ³n Flexible

```python
# Usar variables de entorno (recomendado)
import os
os.environ['GEMINI_API_KEY'] = 'tu-api-key'

response = llm.call(
    messages=["Test con variable de entorno"],
    provider="gemini"
)

# ConfiguraciÃ³n manual con parÃ¡metros adicionales
response = llm.call(
    messages=["Respuesta creativa"],
    provider="openai",
    model="gpt-4",
    api_key="tu-openai-key",
    temperature=0.9,
    max_tokens=1000,
    top_p=0.95
)
```

### Manejo de Errores

```python
try:
    response = llm.call(
        messages=["Test"],
        provider="gemini",
        api_key="api-key-invalida"
    )
    print(response)
except colmena.LlmException as e:
    print(f"Error en la llamada LLM: {e}")
except Exception as e:
    print(f"Error inesperado: {e}")
```

## âš™ï¸ Uso del Motor DAG

El **DAG Engine** permite crear flujos de trabajo complejos conectando diferentes tipos de nodos en un grafo dirigido acÃ­clico (DAG).

### Ejecutar un DAG

```bash
# Ejecutar un grafo desde un archivo JSON
cargo run --bin dag_engine run tests/python_simple_graph.json

# Servir un grafo como API HTTP
cargo run --bin dag_engine serve tests/python_llm_graph.json --port 3000
```

### Tipos de Nodos Disponibles

| Tipo | DescripciÃ³n | Ejemplo de Uso |
|------|-------------|----------------|
| `mock_input` | Emite datos de configuraciÃ³n | Iniciar flujo con datos estÃ¡ticos |
| `log` | Imprime valores a consola | Debugging y visualizaciÃ³n |
| `add`, `subtract`, `multiply`, `divide` | Operaciones matemÃ¡ticas | CÃ¡lculos numÃ©ricos |
| `http_request` | Hace peticiones HTTP | Llamadas a APIs externas |
| `llm_call` | Ejecuta modelos LLM | GeneraciÃ³n de texto, anÃ¡lisis |
| `python_script` | Ejecuta cÃ³digo Python | LÃ³gica personalizada, transformaciones |
| `trigger_webhook` | Dispara webhooks | IntegraciÃ³n con sistemas externos |

### ğŸ Python Node

El **Python Node** ejecuta cÃ³digo Python arbitrario dentro del flujo del DAG, con integraciÃ³n completa con JSON.

#### CaracterÃ­sticas

- âœ… **EjecuciÃ³n Segura**: CÃ³digo Python ejecutado en thread aislado
- âœ… **IntegraciÃ³n JSON**: Inputs/outputs automÃ¡ticos desde/hacia JSON
- âœ… **Variables Inyectadas**: Los inputs del nodo se inyectan como variables Python
- âœ… **Soporte para Funciones**: Define y usa funciones dentro del script
- âœ… **Compatible con LLMs**: Procesa cÃ³digo generado por LLMs (limpia markdown)
- âœ… **LibrerÃ­as EstÃ¡ndar**: Acceso completo a la biblioteca estÃ¡ndar de Python

#### Ejemplo BÃ¡sico

```json
{
  "nodes": {
    "start": {
      "type": "mock_input",
      "config": {
        "x": 10,
        "y": 5
      }
    },
    "python_calc": {
      "type": "python_script",
      "config": {
        "code": "output = x * y + 2"
      }
    },
    "log_result": {
      "type": "log"
    }
  },
  "edges": [
    {"from": "start.x", "to": "python_calc.x"},
    {"from": "start.y", "to": "python_calc.y"},
    {"from": "python_calc.output", "to": "log_result.input"}
  ]
}
```

**Resultado**: `52` (10 Ã— 5 + 2)

#### Ejemplo con LLM

El Python Node puede ejecutar cÃ³digo generado dinÃ¡micamente por un LLM:

```json
{
  "nodes": {
    "start": {
      "type": "mock_input",
      "config": {
        "prompt": "Write a Python script that calculates the factorial of 5 and assigns it to 'output'"
      }
    },
    "llm_gen": {
      "type": "llm_call",
      "config": {
        "provider": "openai",
        "api_key": "${OPENAI_API_KEY}",
        "model": "gpt-4o"
      }
    },
    "python_exec": {
      "type": "python_script"
    },
    "log_result": {
      "type": "log"
    }
  },
  "edges": [
    {"from": "start.prompt", "to": "llm_gen.prompt"},
    {"from": "llm_gen.output.content", "to": "python_exec.code"},
    {"from": "python_exec.output", "to": "log_result.input"}
  ]
}
```

**Resultado**: `120` (factorial de 5)

#### Convenciones del Python Node

1. **Inputs**: Todas las entradas del nodo se inyectan como variables globales
2. **Output**: El script debe asignar el resultado a una variable llamada `output`
3. **Code Source**: El cÃ³digo puede venir de:
   - Config: `config.code` (estÃ¡tico)
   - Input: `inputs.code` (dinÃ¡mico, por ejemplo desde un LLM)
4. **Markdown Cleanup**: AutomÃ¡ticamente limpia bloques ```python ... ``` de cÃ³digo LLM
5. **LibrerÃ­as**: Se puede usar `import` para cualquier librerÃ­a estÃ¡ndar de Python

#### Ejemplo Avanzado con Funciones

```python
# Este cÃ³digo puede estar en config.code o ser generado por un LLM
def fibonacci(n):
    if n <= 1:
        return n
    return fibonacci(n-1) + fibonacci(n-2)

output = [fibonacci(i) for i in range(10)]
```

### Flujos Comunes

#### 1. LLM â†’ Python â†’ Resultado
LLM genera cÃ³digo â†’ Python ejecuta â†’ Log resultado

#### 2. HTTP â†’ Python â†’ LLM  
API externa â†’ Python transforma datos â†’ LLM analiza

#### 3. Input â†’ Math â†’ Python â†’ Output
Datos iniciales â†’ Operaciones â†’ LÃ³gica compleja â†’ Resultado



## ğŸ§ª Testing y VerificaciÃ³n

### Ejecutar Tests Completos

El proyecto incluye un script de testing completo:

```bash
# Activar entorno virtual
source venv/bin/activate

# Ejecutar tests de Gemini (requiere API key vÃ¡lida)
python test_gemini.py
```

### Tests Incluidos

1. **Health Check**: VerificaciÃ³n de conectividad
2. **Llamada Simple**: Test bÃ¡sico de funcionalidad
3. **Llamada con Contexto**: MÃºltiples mensajes
4. **ConversaciÃ³n**: InteracciÃ³n de ida y vuelta
5. **Streaming**: Respuestas en tiempo real
6. **Manejo de Errores**: API keys invÃ¡lidas y errores de red
7. **Test de Performance**: MediciÃ³n de tiempos de respuesta
8. **ConfiguraciÃ³n Personalizada**: ParÃ¡metros de temperatura y tokens

### Verificar CompilaciÃ³n Nativa

```python
# Verificar que usamos la librerÃ­a Rust compilada
python prove_rust_library.py
```

Este script demuestra que:
- Los mÃ©todos son nativos (compilados desde Rust)
- No hay cÃ³digo Python interpretado
- La librerÃ­a hace llamadas reales a APIs

## âš¡ Performance

### Ventajas de la ImplementaciÃ³n en Rust

- **ğŸš€ Velocidad Nativa**: Sin overhead de interpretaciÃ³n Python
- **ğŸ§  GestiÃ³n de Memoria**: Control preciso con ownership de Rust
- **ğŸ”’ Thread Safety**: GarantÃ­as de concurrencia sin data races
- **âš¡ HTTP Async**: Cliente HTTP nativo con tokio
- **ğŸ“¦ Zero-Copy**: Minimiza copias de datos entre Rust y Python

### Benchmarks (Aproximados)

| OperaciÃ³n | Tiempo (ms) | Notas |
|-----------|-------------|--------|
| InicializaciÃ³n | <1 | Una sola vez por proceso |
| Llamada Simple | 500-2000 | Depende del proveedor LLM |
| Streaming Chunk | <10 | Por chunk individual |
| Parsing JSON | <5 | Nativo con serde |

## ğŸ—ï¸ Arquitectura

Colmena sigue los principios de **Arquitectura Hexagonal** (Ports and Adapters):

### ğŸ›ï¸ Dominio (Core)
- **Entidades**: `LlmRequest`, `LlmResponse`, `LlmMessage`
- **Value Objects**: `LlmRequestId`, `LlmProvider`, `LlmConfig`
- **Puertos**: `LlmRepository` trait
- **LÃ³gica de Negocio**: Validaciones y reglas de dominio

### ğŸ¯ AplicaciÃ³n (Use Cases)
- **LlmCallUseCase**: Orquesta llamadas sÃ­ncronas
- **LlmStreamUseCase**: Maneja streaming
- **LlmHealthCheckUseCase**: Verifica salud de proveedores

### ğŸ”§ Infraestructura (Adapters)
- **OpenAiAdapter**: Implementa API de OpenAI
- **GeminiAdapter**: Implementa API de Gemini
- **AnthropicAdapter**: Implementa API de Anthropic
- **ConfigResolver**: Gestiona configuraciÃ³n
- **Python Bindings**: Expone funcionalidad a Python

## ğŸ¤ Contribuir

1. Fork el proyecto
2. Crea una rama feature (`git checkout -b feature/nueva-funcionalidad`)
3. Commit tus cambios (`git commit -am 'AÃ±adir nueva funcionalidad'`)
4. Push a la rama (`git push origin feature/nueva-funcionalidad`)
5. Crear Pull Request

### GuÃ­as de Desarrollo

- Seguir principios de arquitectura hexagonal
- Mantener separaciÃ³n clara entre capas
- Agregar tests para nueva funcionalidad
- Documentar APIs pÃºblicas
- Seguir convenciones de Rust

## ğŸ“œ Licencia

[Definir licencia]

## ğŸ™ Agradecimientos

- Arquitectura hexagonal inspirada en los principios de Alistair Cockburn
- PatrÃ³n Ports and Adapters
- Comunidad Rust y PyO3

---

**ğŸ Colmena** - *Orquestando el futuro de la IA, una llamada a la vez*