Metadata-Version: 2.4
Name: aitta_client
Version: 0.3.2
Summary: Python client for Aitta HPC ML inference platform
Author: CSC - IT Center for Science Oy
License: MIT License
        
        Copyright (c) <year> <copyright holders>
        
        Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the "Software"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:
        
        The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.
        
        THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
        
Classifier: Programming Language :: Python :: 3
Classifier: Operating System :: OS Independent
Classifier: License :: OSI Approved :: MIT License
Requires-Python: >=3.11
Description-Content-Type: text/markdown
Requires-Dist: requests~=2.28
Requires-Dist: pydantic~=2.5
Requires-Dist: PyJWT~=2.8
Provides-Extra: test
Requires-Dist: pytest; extra == "test"
Provides-Extra: examples
Requires-Dist: openai; extra == "examples"

<!--
SPDX-FileCopyrightText: 2024 CSC - IT Center for Science Oy

SPDX-License-Identifier: MIT
-->

# Python client for Aitta HPC ML inference platform

A Python client library for the Aitta ML inference platform for HPC systems.

> *__IMPORTANT__*: Note that both the API and the client library are still under heavy development and while
we try to keep changes mostly backwards-compatible, breaking changes may happen. Access to Aitta is
currently restricted to selected beta users.

## Main client API classes

### `AccessTokenSource`
Used by the client to get (and eventually refresh) access tokens

### `Client`
Implements all requests to the Aitta API servers on a low level and is used by all other classes.

Useful members:
- `get_model_list()`: Lists all models served by the API that can be accessed with the configured access credentials.

### `Model`
Represents a model and provides methods to perform inference.

Useful members:

- `load(model_id, client)`: Creates a Model instance given the models id, loading the relevant data from the Aitta API server.

### `Task`
Represents an active inference task and provides methods to query the current status and results.

Useful members:
- `max_progress`: The maximum number of progress increments of the task. Progress reporting is not supported by all models, in which case the return value is 1.
- `progress`: The current progress towards completion of the task. The increments are arbitrarily chosen by the model. Check max_progress for the maximum number of progress increments of the task.
If progress reporting is not supported by the model for this task the returned value is always 0 unless the task is successfully completed, in which case it is 1.
- `results`: The outputs of the model for inference request. Raises an IncompleteTaskError if the task was not yet 
  completed. Raises an InferenceError if the task was completed with a failure.

  
## Example usage

The below shows an example for usage of the Aitta API using the Python client library.

For accessing the Aitta API the client will need a way to obtain access tokens, which is
implemented in the form of an `AccessTokenSource`. For the time being, you can generate a static
model-specific token at the [web frontend](https://staging-aitta.2.rahtiapp.fi/) by opening
the model's page, switching to the "API Key" tab and pressing the "Generate API key" button.

With the token thus obtained, then have to create an instance of `StaticAccessTokenSource` for use
with the client library.

### Chat completion with the LumiOpen/Llama-Poro-2-70B-Instruct model
This example shows how to start a conversation with the model using OpenAIâ€™s chat completion feature. 

> *__NOTE__*: It is recommended to specify the `max_tokens` value to a high enough number in order to avoid responses 
> being cut out.

```
from aitta_client import Model, Client, StaticAccessTokenSource
import openai

# configure Client instance with API URL and access token
access_token = "token_for_given_model"
token_source = StaticAccessTokenSource(access_token)
aitta_client = Client("https://api-staging-aitta.2.rahtiapp.fi", token_source)

# load the LumiOpen/Llama-Poro-2-70B-Instruct model
model = Model.load("LumiOpen/Llama-Poro-2-70B-Instruct", aitta_client)
print(model.description)

# configure OpenAI client to use the Aitta OpenAI compatibility endpoints
client = openai.OpenAI(api_key=token_source.get_access_token(), base_url=model.openai_api_url)
# perform chat completion with the OpenAI client
chat_completion = client.chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "Write a novel about bear talking about how to combine HPC with QC."
        }
    ],
    model=model.id,
    max_tokens=8000,
    stream=False  # response streaming is currently not supported by Aitta
)

print(chat_completion.choices[0].message.content)

```
