"""
Pipeline for training and using a Flux1 model for simulation-based inference.

Examples:
    .. code-block:: python

        import grain
        import numpy as np
        import jax
        from jax import numpy as jnp
        from gensbi.recipes import Flux1Pipeline

        # Define your training and validation datasets.
        train_data = jax.random.rand((1024, 4)) # your training dataset
        val_data = jax.random.rand((128, 4)) # your validation dataset

        batch_size = 32

        train_dataset_grain = (
            grain.MapDataset.source(np.array(train_data)[...,None])
            .shuffle(42)
            .repeat()
            .to_iter_dataset()
            .batch(batch_size)
            # .mp_prefetch() # Uncomment if you want to use multiprocessing prefetching
        )

        val_dataset_grain = (
            grain.MapDataset.source(np.array(val_data)[...,None])
            .shuffle(42)
            .repeat()
            .to_iter_dataset()
            .batch(batch_size)
            # .mp_prefetch() # Uncomment if you want to use multiprocessing prefetching
        )

        # Define the model
        dim_obs = 2  # Dimension of the parameter space
        dim_cond = 2      # Dimension of the observation space
        pipeline = Flux1Pipeline(train_dataset_grain, val_dataset_grain, dim_obs, dim_cond)

        # Train the model
        rngs = jax.random.PRNGKey(0)
        pipeline.train(rngs)

        # Sample from the posterior
        x_o = jnp.array([0.5, -0.2])  # Example
        samples = pipeline.sample(rngs, x_o, nsamples=10000, step_size=0.01)

    .. note::

        If you plan on using multiprocessing prefetching, ensure that your script is wrapped in a `if __name__ == "__main__":` guard. See https://docs.python.org/3/library/multiprocessing.html

"""

import jax
import jax.numpy as jnp
from flax import nnx
import optax
from optax.contrib import reduce_on_plateau
from numpyro import distributions as dist
from tqdm.auto import tqdm
from functools import partial
import orbax.checkpoint as ocp

from gensbi.flow_matching.path import AffineProbPath
from gensbi.flow_matching.path.scheduler import CondOTScheduler
from gensbi.flow_matching.solver import ODESolver

from gensbi.diffusion.path import EDMPath
from gensbi.diffusion.path.scheduler import EDMScheduler, VEScheduler
from gensbi.diffusion.solver import SDESolver

from gensbi.models import (
    Flux1,
    Flux1Params,
    ConditionalCFMLoss,
    ConditionalWrapper,
    ConditionalDiffLoss,
)

from einops import repeat

from gensbi.utils.model_wrapping import _expand_dims

import os

import yaml

from gensbi.experimental.recipes.latent_conditional_pipeline import (
    ConditionalLatentFlowPipeline,
    ConditionalLatentDiffusionPipeline,
)


def parse_flux1_params(config_path: str):
    """
    Parse a Flux1 configuration file.

    Parameters
    ----------
    config_path : str
        Path to the configuration file.

    Returns
    -------
    config : dict
        Parsed configuration dictionary.

    """
    with open(config_path, "r") as f:
        config = yaml.safe_load(f)

    model_params = config.get("model", {})

    params_dict = dict(
        in_channels=model_params.get("in_channels", 1),
        vec_in_dim=model_params.get("vec_in_dim", None),
        context_in_dim=model_params.get("context_in_dim", 1),
        mlp_ratio=model_params.get("mlp_ratio", 4),
        num_heads=model_params.get("num_heads", 6),
        depth=model_params.get("depth", 8),
        depth_single_blocks=model_params.get("depth_single_blocks", 16),
        axes_dim=model_params.get("axes_dim", [6, 0]),
        qkv_bias=model_params.get("qkv_bias", True),
        theta=model_params.get("theta", -1),
        param_dtype=getattr(jnp, model_params.get("param_dtype", "float32")),
        id_embedding_strategy=tuple(model_params.get(
            "id_embedding_strategy", ("absolute", "absolute")
        )),
    )

    return params_dict


def parse_training_config(config_path: str):
    """
    Parse a training configuration file.

    Parameters
    ----------
    config_path : str
        Path to the configuration file.

    Returns
    -------
    config : dict
        Parsed configuration dictionary.

    """
    with open(config_path, "r") as f:
        config = yaml.safe_load(f)

    # Training parameters
    train_params = config.get("training", {})
    multistep = train_params.get("multistep", 1)
    experiment_id = train_params.get("experiment_id", 1)
    early_stopping = train_params.get("early_stopping", True)
    nsteps = train_params.get("nsteps", 30000) * multistep
    val_every = train_params.get("val_every", 100) * multistep

    # Optimizer parameters
    opt_params = config.get("optimizer", {})

    MAX_LR = opt_params.get("max_lr", 1e-3)
    MIN_LR = opt_params.get("min_lr", 0.0)
    MIN_SCALE = MIN_LR / MAX_LR if MAX_LR > 0 else 0.0

    warmup_steps = opt_params.get("warmup_steps", 500)

    ema_decay = opt_params.get("ema_decay", 0.999)

    decay_transition = opt_params.get("decay_transition", 0.85)

    training_config = {}
    # overwrite the defaults with the config file values
    training_config["nsteps"] = nsteps
    training_config["ema_decay"] = ema_decay
    training_config["decay_transition"] = decay_transition

    training_config["max_lr"] = MAX_LR
    training_config["min_lr"] = MIN_LR
    training_config["min_scale"] = MIN_SCALE
    training_config["val_every"] = val_every
    training_config["early_stopping"] = early_stopping
    training_config["experiment_id"] = experiment_id
    training_config["multistep"] = multistep
    training_config["warmup_steps"] = warmup_steps

    return training_config


class Flux1LatentFlowPipeline(ConditionalLatentFlowPipeline):
    def __init__(
        self,
        train_dataset,
        val_dataset,
        dim_obs: int,
        dim_cond: int,
        ch_obs=1,
        ch_cond=1,
        params=None,
        vae_obs=None,
        vae_cond=None,
        training_config=None,
    ):
        """
        Flow pipeline for training and using a Flux1 model for simulation-based inference.

        Parameters
        ----------
        train_dataset : grain dataset or iterator over batches
            Training dataset.
        val_dataset : grain dataset or iterator over batches
            Validation dataset.
        dim_obs : int
            Dimension of the parameter space.
        dim_cond : int
            Dimension of the observation space.
        ch_obs : int, optional
            Number of channels in the observation data. Default is 1.
        ch_cond : int, optional
            Number of channels in the conditional data. Default is 1.
        params : Flux1Params, optional
            Parameters for the Flux1 model. If None, default parameters are used.
        vae_obs : nnx.Module, optional
            VAE module for the observation input. If None, no encoding is applied.
        vae_cond : nnx.Module, optional
            VAE module for the conditional input. If None, no encoding is applied.
        training_config : dict, optional
            Configuration for training. If None, default configuration is used.

        """

        # if vae are provided, adjust dim_cond and dim_obs accordingly
        if vae_obs is not None:
            obs_latent_shape = vae_obs.latent_shape
            dim_obs = obs_latent_shape[1]
            ch_obs = obs_latent_shape[2]
        else:
            if params is not None:
                ch_obs = params.in_channels

        if vae_cond is not None:
            cond_latent_shape = vae_cond.latent_shape
            dim_cond = cond_latent_shape[1]
            ch_cond = cond_latent_shape[2]
        else:
            if params is not None:
                ch_cond = params.context_in_dim

        # FIXME: we need to set these values manually instead of init to allow _get_default_params to work properly
        # think of a better way to do it, avoiding circular dependencies
        self.vae_obs = vae_obs
        self.vae_cond = vae_cond

        self.dim_obs = dim_obs
        self.dim_cond = dim_cond

        self.ch_obs = ch_obs
        self.ch_cond = ch_cond

        if params is None:
            params = self._get_default_params()
        else:
            if vae_obs is not None:
                assert (
                    params.in_channels == self.ch_obs
                ), f"in_channels in params ({params.in_channels}) does not match VAE latent shape ({self.ch_obs})."
                assert (
                    params.dim_obs == dim_obs
                ), f"dim_obs in params ({params.dim_obs}) does not match the VAE latent dimension ({dim_obs})."
            if vae_cond is not None:
                assert (
                    params.context_in_dim == self.ch_cond
                ), f"context_in_dim in params ({params.context_in_dim}) does not match VAE latent shape ({self.ch_cond})."
                assert (
                    params.dim_cond == dim_cond
                ), f"dim_cond in params ({params.dim_cond}) does not match the VAE latent dimension ({dim_cond})."

        model = self._make_model(params)

        super().__init__(
            model=model,
            train_dataset=train_dataset,
            val_dataset=val_dataset,
            dim_obs=dim_obs,
            dim_cond=dim_cond,
            ch_obs=ch_obs,
            ch_cond=ch_cond,
            params=params,
            vae_obs=vae_obs,
            vae_cond=vae_cond,
            training_config=training_config,
        )
        self.ema_model = nnx.clone(self.model)

    # TODO: check how to implement the in channels and cond channels properly, we may need to modify something here
    @classmethod
    def init_pipeline_from_config(
        cls,
        train_dataset,
        val_dataset,
        dim_obs: int,
        dim_cond: int,
        config_path: str,
        checkpoint_dir: str,
    ):
        """
        Initialize the pipeline from a configuration file.

        Parameters
        ----------
        config_path : str
            Path to the configuration file.

        """
        with open(config_path, "r") as f:
            config = yaml.safe_load(f)

        # methodology
        strategy = config.get("strategy", {})
        method = strategy.get("method")
        model_type = strategy.get("model")

        assert (
            method == "flow"
        ), f"Method {method} not supported in Flux1LatentFlowPipeline."
        assert (
            model_type == "flux"
        ), f"Model type {model_type} not supported in Flux1LatentFlowPipeline."

        # Model parameters from config
        dim_joint = dim_obs + dim_cond

        params_dict = parse_flux1_params(config_path)

        if params_dict["theta"] == -1:
            params_dict["theta"] = 4 * dim_joint

        params = Flux1Params(
            rngs=nnx.Rngs(0),
            dim_obs=dim_obs,
            dim_cond=dim_cond,
            **params_dict,
        )

        # Training parameters
        training_config = cls.get_default_training_config()
        training_config["checkpoint_dir"] = checkpoint_dir

        training_config_ = parse_training_config(config_path)

        for key, value in training_config_.items():
            training_config[key] = value  # update with config file values

        pipeline = cls(
            train_dataset,
            val_dataset,
            dim_obs,
            dim_cond,
            ch_obs=params.in_channels,
            ch_cond=params.context_in_dim,
            params=params,
            training_config=training_config,
        )

        return pipeline

    def _make_model(self, params):
        """
        Create and return the Flux1 model to be trained.
        """
        model = Flux1(params)
        return model

    def _get_default_params(self):
        """
        Return default parameters for the Flux1 model.
        """
        params = Flux1Params(
            in_channels=self.ch_obs,
            vec_in_dim=None,
            context_in_dim=self.ch_cond,
            mlp_ratio=4,
            num_heads=6,
            depth=8,
            depth_single_blocks=16,
            axes_dim=[6,0],
            qkv_bias=True,
            dim_obs=self.dim_obs,
            dim_cond=self.dim_cond,
            theta=10 * (self.dim_obs + self.dim_cond),
            id_embedding_strategy=("absolute", "absolute"),
            rngs=nnx.Rngs(default=42),
            param_dtype=jnp.float32,
        )
        return params


class Flux1LatentDiffusionPipeline(ConditionalLatentDiffusionPipeline):
    def __init__(
        self,
        train_dataset,
        val_dataset,
        dim_obs: int,
        dim_cond: int,
        ch_obs=1,
        ch_cond=1,
        params=None,
        vae_obs=None,
        vae_cond=None,
        training_config=None,
    ):
        """
        Diffusion pipeline for training and using a Flux1 model for simulation-based inference.

        Parameters
        ----------
        train_dataset : grain dataset or iterator over batches
            Training dataset.
        val_dataset : grain dataset or iterator over batches
            Validation dataset.
        dim_obs : int
            Dimension of the parameter space.
        dim_cond : int
            Dimension of the observation space.
        ch_obs : int, optional
            Number of channels in the observation data. Default is 1.
        ch_cond : int, optional
            Number of channels in the conditional data. Default is 1.
        params : Flux1Params, optional
            Parameters for the Flux1 model. If None, default parameters are used.
        vae_obs : nnx.Module, optional
            VAE module for the observation input. If None, no encoding is applied.
        vae_cond : nnx.Module, optional
            VAE module for the conditional input. If None, no encoding is applied.
        training_config : dict, optional
            Configuration for training. If None, default configuration is used.

        """
        # if vae are provided, adjust dim_cond and dim_obs accordingly
        if vae_obs is not None:
            obs_latent_shape = vae_obs.latent_shape
            dim_obs = obs_latent_shape[1]
            ch_obs = obs_latent_shape[2]
        else:
            if params is not None:
                ch_obs = params.in_channels

        if vae_cond is not None:
            cond_latent_shape = vae_cond.latent_shape
            dim_cond = cond_latent_shape[1]
            ch_cond = cond_latent_shape[2]
        else:
            if params is not None:
                ch_cond = params.context_in_dim

        # FIXME: we need to set these values manually instead of init to allow _get_default_params to work properly
        # think of a better way to do it, avoiding circular dependencies
        self.vae_obs = vae_obs
        self.vae_cond = vae_cond

        self.dim_obs = dim_obs
        self.dim_cond = dim_cond

        self.ch_obs = ch_obs
        self.ch_cond = ch_cond

        if params is None:
            params = self._get_default_params()
        else:
            if vae_obs is not None:
                assert (
                    params.in_channels == self.ch_obs
                ), f"in_channels in params ({params.in_channels}) does not match VAE latent shape ({self.ch_obs})."
                assert (
                    params.dim_obs == dim_obs
                ), f"dim_obs in params ({params.dim_obs}) does not match the VAE latent dimension ({dim_obs})."
            if vae_cond is not None:
                assert (
                    params.context_in_dim == self.ch_cond
                ), f"context_in_dim in params ({params.context_in_dim}) does not match VAE latent shape ({self.ch_cond})."
                assert (
                    params.dim_cond == dim_cond
                ), f"dim_cond in params ({params.dim_cond}) does not match the VAE latent dimension ({dim_cond})."

        model = self._make_model(params)

        super().__init__(
            model=model,
            train_dataset=train_dataset,
            val_dataset=val_dataset,
            dim_obs=dim_obs,
            dim_cond=dim_cond,
            ch_obs=ch_obs,
            ch_cond=ch_cond,
            params=params,
            vae_obs=vae_obs,
            vae_cond=vae_cond,
            training_config=training_config,
        )
        self.ema_model = nnx.clone(self.model)

    # TODO: need to update this too
    @classmethod
    def init_pipeline_from_config(
        cls,
        train_dataset,
        val_dataset,
        dim_obs: int,
        dim_cond: int,
        config_path: str,
        checkpoint_dir: str,
    ):
        """
        Initialize the pipeline from a configuration file.

        Parameters
        ----------
        config_path : str
            Path to the configuration file.

        """
        with open(config_path, "r") as f:
            config = yaml.safe_load(f)

        # methodology
        strategy = config.get("strategy", {})
        method = strategy.get("method")
        model_type = strategy.get("model")

        assert (
            method == "diffusion"
        ), f"Method {method} not supported in Flux1LatentDiffusionPipeline."
        assert (
            model_type == "flux"
        ), f"Model type {model_type} not supported in Flux1LatentDiffusionPipeline."

        # Model parameters from config
        dim_joint = dim_obs + dim_cond

        params_dict = parse_flux1_params(config_path)

        if params_dict["theta"] == -1:
            params_dict["theta"] = 4 * dim_joint

        params = Flux1Params(
            rngs=nnx.Rngs(0),
            dim_obs=dim_obs,
            dim_cond=dim_cond,
            **params_dict,
        )

        # Training parameters
        training_config = cls.get_default_training_config()
        training_config["checkpoint_dir"] = checkpoint_dir

        training_config_ = parse_training_config(config_path)

        for key, value in training_config_.items():
            training_config[key] = value  # update with config file values

        pipeline = cls(
            train_dataset,
            val_dataset,
            dim_obs,
            dim_cond,
            ch_obs=params.in_channels,
            ch_cond=params.context_in_dim,
            params=params,
            training_config=training_config,
        )

        return pipeline

    def _make_model(self, params):
        """
        Create and return the Flux1 model to be trained.
        """
        model = Flux1(params)
        return model

    def _get_default_params(self):
        """
        Return default parameters for the Flux1 model.
        """
        params = Flux1Params(
            in_channels=self.ch_obs,
            vec_in_dim=None,
            context_in_dim=self.ch_cond,
            mlp_ratio=4,
            num_heads=6,
            depth=8,
            depth_single_blocks=16,
            axes_dim=[6,0],
            qkv_bias=True,
            dim_obs=self.dim_obs,
            dim_cond=self.dim_cond,
            theta=10 * (self.dim_obs + self.dim_cond),
            id_embedding_strategy=("absolute", "absolute"),
            rngs=nnx.Rngs(default=42),
            param_dtype=jnp.float32,
        )
        return params
