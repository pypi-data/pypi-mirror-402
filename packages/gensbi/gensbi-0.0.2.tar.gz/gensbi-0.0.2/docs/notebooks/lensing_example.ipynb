{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Gravitational Lensing Example: CNN Embedding + Flow Matching\n",
                "\n",
                "This notebook demonstrates a Simulation-Based Inference (SBI) workflow for a Strong Lensing task. \n",
                "\n",
                "## Conceptual Overview\n",
                "\n",
                "We aim to infer the parameters $\\theta$ of a lensing system (e.g., lens mass, shear) given an observed image $x$.\n",
                "\n",
                "**Strategy:**\n",
                "1.  **Compression (VAE)**: We use a **Variational Autoencoder (VAE)** to compress the high-dimensional conditioning data (32x32 images) into a lower-dimensional latent representation.\n",
                "2.  **Inference (Flow Matching)**: We condition our inference model (a **Flux1** Flow Matching model) on this latent representation.\n",
                "\n",
                "The encoder is trained end-to-end with the flow matching model to optimize the inference objective."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Configuration & Data Dimensions\n",
                "\n",
                "We use the configuration from `config_1a.yaml`.\n",
                "\n",
                "### Data Dimensions\n",
                "- **Observation ($\\\\theta$)**: The target of inference. It has **2 features** (parameters) and **1 channel**.\n",
                "- **Conditioning ($x$)**: Lensing images. **32x32 pixels** with **1 channel**.\n",
                "\n",
                "### Processing Pipeline\n",
                "The conditioning images go through several transformation steps before entering the inference model:\n",
                "\n",
                "1.  **VAE Encoder**: The 32x32x1 image is processed by the VAE encoder, which outputs a latent feature map of shape **8x8x16**.\n",
                "2.  **Patchification**: We apply standard Vision Transformer (ViT) patchification with 2x2 patches. \n",
                "    - Spatial dimension reduces by factor of 2: $8 \\to 4$.\n",
                "    - Channel dimension increases by factor of $2 \\times 2 = 4$: $16 \\to 64$.\n",
                "    - Resulting shape: **4x4x64**.\n",
                "3.  **Reshaping**: For the Transformer, we flatten the spatial dimensions.\n",
                "    - $4 \\times 4 = 16$ tokens.\n",
                "    - Each token has size **64**.\n",
                "    - Resulting array: **16x64**.\n",
                "\n",
                "The pipeline handles the initialization of condition IDs to represent the patched structure of the image."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Setup and Imports\n",
                "\n",
                "First, we set up the environment and import necessary libraries."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "\n",
                "if os.environ.get(\"JAX_PLATFORMS\") is None:\n",
                "    # os.environ[\"JAX_PLATFORMS\"] = \"cpu\"\n",
                "    os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \".90\"  # use 90% of GPU memory\n",
                "    os.environ[\"JAX_PLATFORMS\"] = \"cuda\"  # change to 'cpu' if no GPU is available\n",
                "\n",
                "import gensbi\n",
                "\n",
                "# base libraries\n",
                "import jax\n",
                "from jax import Array\n",
                "from jax import numpy as jnp\n",
                "import numpy as np\n",
                "from flax import nnx\n",
                "\n",
                "from tqdm import tqdm\n",
                "import gc\n",
                "\n",
                "# data loading\n",
                "import grain\n",
                "from datasets import load_dataset\n",
                "import yaml\n",
                "\n",
                "# plotting\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "# gensbi\n",
                "from gensbi.recipes import ConditionalFlowPipeline\n",
                "from gensbi.recipes.flux1 import parse_flux1_params, parse_training_config\n",
                "from gensbi.recipes.utils import patchify_2d\n",
                "\n",
                "from gensbi.experimental.models.autoencoders import AutoEncoder2D, AutoEncoderParams\n",
                "from gensbi.experimental.recipes.vae_pipeline import parse_autoencoder_params\n",
                "from gensbi.models import Flux1Params, Flux1\n",
                "\n",
                "from gensbi.utils.plotting import plot_marginals\n",
                "\n",
                "from gensbi.diagnostics import LC2ST, plot_lc2st\n",
                "from gensbi.diagnostics import run_sbc, sbc_rank_plot\n",
                "from gensbi.diagnostics import run_tarp, plot_tarp\n",
                "\n",
                "config_path = \"./config/config_1a.yaml\""
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Helper Functions and Classes"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def normalize(batch, mean, std):\n",
                "    mean = jnp.asarray(mean, dtype=batch.dtype)\n",
                "    std = jnp.asarray(std, dtype=batch.dtype)\n",
                "    return (batch - mean) / std\n",
                "\n",
                "\n",
                "def unnormalize(batch, mean, std):\n",
                "    mean = jnp.asarray(mean, dtype=batch.dtype)\n",
                "    std = jnp.asarray(std, dtype=batch.dtype)\n",
                "    return batch * std + mean\n",
                "\n",
                "\n",
                "class LensingModel(nnx.Module):\n",
                "    \"\"\"\n",
                "    A combined model that first encodes the conditioning data (images) using a VAE,\n",
                "    and then passes the latent embedding to the SBI model (Flux).\n",
                "    \"\"\"\n",
                "    def __init__(self, vae, sbi_model):\n",
                "        self.vae = vae\n",
                "        self.sbi_model = sbi_model\n",
                "\n",
                "    def __call__(\n",
                "        self,\n",
                "        t: Array,\n",
                "        obs: Array,\n",
                "        obs_ids: Array,\n",
                "        cond: Array,\n",
                "        cond_ids: Array,\n",
                "        conditioned: bool | Array = True,\n",
                "        guidance: Array | None = None,\n",
                "        encoder_key=None,\n",
                "    ):\n",
                "\n",
                "        # first we encode the conditioning data\n",
                "        cond_latent = self.vae.encode(cond, encoder_key)\n",
                "        # patchify the cond_latent for the transformer\n",
                "        cond_latent = patchify_2d(cond_latent)\n",
                "\n",
                "        # then we pass to the sbi model\n",
                "        return self.sbi_model(\n",
                "            t=t,\n",
                "            obs=obs,\n",
                "            obs_ids=obs_ids,\n",
                "            cond=cond_latent,\n",
                "            cond_ids=cond_ids,\n",
                "            conditioned=conditioned,\n",
                "            guidance=guidance,\n",
                "        )"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Data Loading\n",
                "\n",
                "We load the Lensing dataset."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "dim_obs = 2\n",
                "ch_obs = 1\n",
                "repo_name = \"aurelio-amerio/SBI-benchmarks\"\n",
                "task_name = \"lensing\"\n",
                "\n",
                "dataset = load_dataset(repo_name, task_name).with_format(\"numpy\")\n",
                "\n",
                "df_train = dataset[\"train\"]\n",
                "df_val = dataset[\"validation\"]\n",
                "df_test = dataset[\"test\"]\n",
                "\n",
                "xs_mean = jnp.array([-1.1874731e-05], dtype=jnp.bfloat16).reshape(1, 1, 1)\n",
                "thetas_mean = jnp.array([0.5996428, 0.15998043], dtype=jnp.bfloat16).reshape(1, 2)\n",
                "\n",
                "xs_std = jnp.array([1.0440514], dtype=jnp.bfloat16).reshape(1, 1, 1)\n",
                "thetas_std = jnp.array([0.2886958, 0.08657552], dtype=jnp.bfloat16).reshape(1, 2)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Model Initialization"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "params_dict = parse_autoencoder_params(config_path)\n",
                "\n",
                "ae_params = AutoEncoderParams(\n",
                "    rngs=nnx.Rngs(0),\n",
                "    **params_dict,\n",
                ")\n",
                "\n",
                "# define the vae model\n",
                "vae_model = AutoEncoder2D(ae_params)\n",
                "\n",
                "# for the sake of the NPE, we delete the decoder model as it is not needed\n",
                "vae_model.Decoder1D = None\n",
                "# run the garbage collector to free up memory\n",
                "gc.collect()\n",
                "\n",
                "# now we define the NPE pipeline\n",
                "# get the latent dimensions from the autoencoder\n",
                "latent_dim1 = vae_model.latent_shape[1]\n",
                "latent_dim2 = vae_model.latent_shape[2]\n",
                "\n",
                "# After 2x2 patchification, dimensions are halved\n",
                "dim_cond_latent = (latent_dim1 // 2) * (latent_dim2 // 2)\n",
                "# Channels are multiplied by 4 (2x2)\n",
                "ch_cond_latent = vae_model.latent_shape[3] * 4\n",
                "\n",
                "print(f\"Original Latent Shape: {vae_model.latent_shape}\")\n",
                "print(f\"Conditioning Transformer Input: {dim_cond_latent} tokens of size {ch_cond_latent}\")\n",
                "\n",
                "params_dict_flux = parse_flux1_params(config_path)\n",
                "assert (\n",
                "    params_dict_flux[\"context_in_dim\"] == ch_cond_latent\n",
                "), \"Context dimension mismatch, got {} expected {}\".format(\n",
                "    params_dict_flux[\"context_in_dim\"], ch_cond_latent\n",
                ")\n",
                "\n",
                "params_flux = Flux1Params(\n",
                "    rngs=nnx.Rngs(0),\n",
                "    dim_obs=dim_obs,\n",
                "    dim_cond=dim_cond_latent,\n",
                "    **params_dict_flux,\n",
                ")\n",
                "\n",
                "model_sbi = Flux1(params_flux)\n",
                "\n",
                "model = LensingModel(vae_model, model_sbi)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Pipeline Setup and Restoration"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "training_config = parse_training_config(config_path)\n",
                "\n",
                "with open(config_path, \"r\") as f:\n",
                "    config = yaml.safe_load(f)\n",
                "    batch_size = config[\"training\"][\"batch_size\"]\n",
                "    nsteps = config[\"training\"][\"nsteps\"]\n",
                "    multistep = config[\"training\"][\"multistep\"]\n",
                "    experiment = config[\"training\"][\"experiment_id\"]\n",
                "\n",
                "def split_data(batch):\n",
                "    obs = jnp.array(batch[\"thetas\"], dtype=jnp.bfloat16)\n",
                "    obs = normalize(obs, thetas_mean, thetas_std)\n",
                "    obs = obs.reshape(obs.shape[0], dim_obs, ch_obs)\n",
                "    cond = jnp.array(batch[\"xs\"], dtype=jnp.bfloat16)\n",
                "    cond = normalize(cond, xs_mean, xs_std)\n",
                "    cond = cond[..., None]\n",
                "    return obs, cond\n",
                "\n",
                "train_dataset_npe = (\n",
                "    grain.MapDataset.source(df_train).shuffle(42).repeat().to_iter_dataset()\n",
                ")\n",
                "\n",
                "performance_config = grain.experimental.pick_performance_config(\n",
                "    ds=train_dataset_npe,\n",
                "    ram_budget_mb=1024 * 8,\n",
                "    max_workers=None,\n",
                "    max_buffer_size=None,\n",
                ")\n",
                "\n",
                "train_dataset_npe = (\n",
                "    train_dataset_npe.batch(batch_size)\n",
                "    .map(split_data)\n",
                "    .mp_prefetch(performance_config.multiprocessing_options)\n",
                ")\n",
                "\n",
                "val_dataset_npe = (\n",
                "    grain.MapDataset.source(df_val)\n",
                "    .shuffle(42)\n",
                "    .repeat()\n",
                "    .to_iter_dataset()\n",
                "    .batch(256)\n",
                "    .map(split_data)\n",
                ")\n",
                "\n",
                "training_config[\"checkpoint_dir\"] = (\n",
                "    \"/lhome/ific/a/aamerio/data/github/GenSBI-examples/examples/sbi-benchmarks/lensing/npe_v1a/checkpoints\"\n",
                ")\n",
                "\n",
                "pipeline_latent = ConditionalFlowPipeline(\n",
                "    model,\n",
                "    train_dataset_npe,\n",
                "    val_dataset_npe,\n",
                "    dim_obs=dim_obs,\n",
                "    dim_cond=(\n",
                "        latent_dim1,\n",
                "        latent_dim2,\n",
                "    ),  # we are workin in the latent space of the vae\n",
                "    ch_obs=ch_obs,\n",
                "    ch_cond=ch_cond_latent,  # conditioning is now in the latent space\n",
                "    training_config=training_config,\n",
                "    id_embedding_strategy=(\"absolute\", \"rope2d\"),\n",
                ")\n",
                "\n",
                "print(\"Restoring model...\")\n",
                "pipeline_latent.restore_model()\n",
                "print(\"Done!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Inference and Visualization\n",
                "\n",
                "We generate samples and visualize the posterior for a test observation."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "x_o = df_test[\"xs\"][0][None, ...]\n",
                "x_o = normalize(jnp.array(x_o, dtype=jnp.bfloat16), xs_mean, xs_std)\n",
                "x_o = x_o[..., None]\n",
                "\n",
                "theta_true = df_test[\"thetas\"][0]  # already unnormalized\n",
                "\n",
                "print(\"Sampling 100,000 samples...\")\n",
                "samples = pipeline_latent.sample_batched(\n",
                "    nnx.Rngs(0).sample(),\n",
                "    x_o,\n",
                "    100_000,\n",
                "    chunk_size=10_000,\n",
                "    encoder_key=jax.random.PRNGKey(1234),\n",
                ")\n",
                "\n",
                "res = samples[:, 0, :, 0]  # shape (num_samples, 1, 2, 1) -> (num_samples, 2)\n",
                "# unnormalize the results for plotting\n",
                "res_unnorm = unnormalize(res, thetas_mean, thetas_std)\n",
                "\n",
                "plot_marginals(res_unnorm, true_param=theta_true, gridsize=30)\n",
                "plt.title(f\"Lensing Samples (Exp {experiment})\")\n",
                "# plt.savefig(f\"lensing_samples_conf{experiment}.png\", dpi=100, bbox_inches=\"tight\")\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Diagnostics\n",
                "\n",
                "We run several diagnostics to validate the quality of the posterior estimation."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### TARP (Test of Accuracy and Reliability of Posterior)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# # split in thetas and xs\n",
                "thetas_ = np.array(df_test[\"thetas\"])[:200]\n",
                "xs_ = np.array(df_test[\"xs\"])[:200]\n",
                "\n",
                "thetas_ = normalize(jnp.array(thetas_, dtype=jnp.bfloat16), thetas_mean, thetas_std)\n",
                "xs_ = normalize(jnp.array(xs_, dtype=jnp.bfloat16), xs_mean, xs_std)\n",
                "xs_ = xs_[..., None]\n",
                "\n",
                "num_posterior_samples = 1000\n",
                "\n",
                "print(\"Sampling for TARP...\")\n",
                "posterior_samples_ = pipeline_latent.sample_batched(\n",
                "    jax.random.PRNGKey(42),\n",
                "    xs_,\n",
                "    num_posterior_samples,\n",
                "    chunk_size=20,\n",
                "    encoder_key=jax.random.PRNGKey(1234),\n",
                ")\n",
                "\n",
                "thetas = thetas_.reshape(thetas_.shape[0], -1)\n",
                "xs = xs_.reshape(xs_.shape[0], -1)\n",
                "\n",
                "posterior_samples = posterior_samples_.reshape(\n",
                "    posterior_samples_.shape[0], posterior_samples_.shape[1], -1\n",
                ")\n",
                "\n",
                "ecp, alpha = run_tarp(\n",
                "    thetas,\n",
                "    posterior_samples,\n",
                "    references=None,  # will be calculated automatically.\n",
                ")\n",
                "\n",
                "plot_tarp(ecp, alpha)\n",
                "# plt.savefig(\n",
                "#     f\"lensing_tarp_v1a_conf{experiment}.png\", dpi=100, bbox_inches=\"tight\"\n",
                "# )  # uncomment to save the figure\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### SBC (Simulation-Based Calibration)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "ranks, dap_samples = run_sbc(thetas, xs, posterior_samples)\n",
                "\n",
                "f, ax = sbc_rank_plot(ranks, num_posterior_samples, plot_type=\"hist\", num_bins=20)\n",
                "# plt.savefig(\n",
                "#     f\"lensing_sbc_v1a_conf{experiment}.png\", dpi=100, bbox_inches=\"tight\"\n",
                "# )  # uncomment to save the figure\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### LC2ST (Local Classifier 2-Sample Test)\n",
                "This tests if the posterior samples are distinguishable from the true parameters."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "thetas_ = np.array(df_test[\"thetas\"])[:10_000]\n",
                "xs_ = np.array(df_test[\"xs\"])[:10_000]\n",
                "\n",
                "thetas_ = normalize(jnp.array(thetas_, dtype=jnp.bfloat16), thetas_mean, thetas_std)\n",
                "xs_ = normalize(jnp.array(xs_, dtype=jnp.bfloat16), xs_mean, xs_std)\n",
                "xs_ = xs_[..., None]\n",
                "\n",
                "num_posterior_samples = 1\n",
                "\n",
                "posterior_samples_ = pipeline_latent.sample(\n",
                "    jax.random.PRNGKey(42),\n",
                "    x_o=xs_,\n",
                "    nsamples=xs_.shape[0],\n",
                "    encoder_key=jax.random.PRNGKey(1234),\n",
                ")\n",
                "\n",
                "thetas = thetas_.reshape(thetas_.shape[0], -1)\n",
                "xs = xs_.reshape(xs_.shape[0], -1)\n",
                "posterior_samples = posterior_samples_.reshape(posterior_samples_.shape[0], -1)\n",
                "\n",
                "# Train the L-C2ST classifier.\n",
                "lc2st = LC2ST(\n",
                "    thetas=thetas[:-1],\n",
                "    xs=xs[:-1],\n",
                "    posterior_samples=posterior_samples[:-1],\n",
                "    classifier=\"mlp\",\n",
                "    num_ensemble=1,\n",
                ")\n",
                "\n",
                "_ = lc2st.train_under_null_hypothesis()\n",
                "_ = lc2st.train_on_observed_data()\n",
                "\n",
                "x_o = xs_[-1:]  # Take the last observation as observed data.\n",
                "theta_o = thetas_[-1:]  # True parameter for the observed data.\n",
                "\n",
                "post_samples_star = pipeline_latent.sample(\n",
                "    jax.random.PRNGKey(42), x_o, nsamples=10_000\n",
                ")\n",
                "\n",
                "x_o = x_o.reshape(1, -1)\n",
                "post_samples_star = np.array(\n",
                "    post_samples_star.reshape(post_samples_star.shape[0], -1)\n",
                ")\n",
                "\n",
                "fig, ax = plot_lc2st(\n",
                "    lc2st,\n",
                "    post_samples_star,\n",
                "    x_o,\n",
                ")\n",
                "# plt.savefig(\n",
                "#     f\"lensing_lc2st_v1a_conf{experiment}.png\", dpi=100, bbox_inches=\"tight\"\n",
                "# )  # uncomment to save the figure\n",
                "plt.show()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
