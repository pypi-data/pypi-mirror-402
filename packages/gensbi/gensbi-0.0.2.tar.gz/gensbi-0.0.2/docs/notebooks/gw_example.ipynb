{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Gravitational Waves Example: CNN Embedding + Flow Matching\n",
                "\n",
                "This notebook demonstrates an advanced Simulation-Based Inference (SBI) workflow for Gravitational Wave (GW) inference.\n",
                "\n",
                "## Conceptual Overview\n",
                "\n",
                "In standard NPE (Neural Posterior Estimation), we learn the posterior distribution $p(\\theta | x)$ directly. However, when $x$ is high-dimensional (like a GW time series), feeding it directly into a density estimator (like a Normalizing Flow) can be inefficient.\n",
                "\n",
                "**Strategy:**\n",
                "1.  **Compression (VAE)**: We utilize the encoder from a **Variational Autoencoder (VAE)** architecture to compress the high-dimensional conditional data $x$ (time series) into a lower-dimensional latent representation $z$.\n",
                "2.  **Inference (Flow Matching)**: We condition our inference model (a **Flux** Flow Matching model) on this latent representation $z$. The relationship is: $p(\\theta | x) \\approx p(\\theta | z = E(x))$, where $E$ is the VAE encoder.\n",
                "\n",
                "**Important Clarification:** Although we use the encoder from a VAE, **we are not actually training the entire VAE** (which would require training also the decoder). Training a full VAE (encoder + decoder) is only needed if one wants to perform sampling in the latent space of the observation (currently not yet supported). Here, the encoder is trained end-to-end with the flow matching model to optimize the inference objective.\n",
                "\n",
                "This two-step approach allows the inference model to work with compact, semantic representations of the data."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Configuration & Hyperparameters\n",
                "\n",
                "We use the configuration from `gw_config_6c.yaml`. Here is a breakdown of the key parameters:\n",
                "\n",
                "### Data Dimensions\n",
                "- **Observation ($\\theta$)**: 2 dimensions (Compact Binary Coalescence parameters).\n",
                "- **Conditioning ($x$)**: 8192 time steps (1D signal).\n",
                "- **Channels**: 2 input channels for the VAE (detectors), 1 channel for $\\theta$.\n",
                "\n",
                "### The Embedding Network (VAE)\n",
                "This is a 1D Convolutional Autoencoder.\n",
                "- **Resolution**: 8192\n",
                "- **Input Channels**: 2\n",
                "- **Base Channels (`ch`)**: 32\n",
                "- **Channel Multipliers (`ch_mult`)**: `[1, 2, 4, 8, 16, 16, 16, 16]`\n",
                "    - This defines the depth and downsampling. Each level increases the channel count and decreases the temporal resolution.\n",
                "- **Latent Channels (`z_channels`)**: 128. The final compressed representation has 128 channels.\n",
                "\n",
                "### The Inference Model (Flux)\n",
                "A Flow Matching model based on a Transformer backbone.\n",
                "- **Context Dimension (`context_in_dim`)**: 128. This MUST match the `z_channels` of the VAE, as the VAE's output is the Flux model's input context.\n",
                "- **Depth**: 8 transformer blocks.\n",
                "- **Heads**: 4 attention heads.\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Setup and Imports\n",
                "\n",
                "First, we set up the environment and import necessary libraries."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "\n",
                "# Set JAX to use CPU or GPU as appropriate\n",
                "if os.environ.get(\"JAX_PLATFORMS\") is None:\n",
                "    # Default to CPU for safety in this example, or set to 'cuda' if GPU is available\n",
                "    os.environ[\"JAX_PLATFORMS\"] = \"cuda\"\n",
                "    os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \".90\"\n",
                "\n",
                "import gc\n",
                "from datasets import load_dataset\n",
                "import grain.python as grain # grain is used for data loading\n",
                "import jax\n",
                "from jax import numpy as jnp\n",
                "from jax import Array\n",
                "import yaml\n",
                "import numpy as np\n",
                "from flax import nnx\n",
                "from tqdm.auto import tqdm\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "# GenSBI imports\n",
                "from gensbi.experimental.models.autoencoders import (\n",
                "    AutoEncoder1D,\n",
                "    AutoEncoderParams,\n",
                ")\n",
                "from gensbi.experimental.recipes.vae_pipeline import parse_autoencoder_params\n",
                "from gensbi.recipes.flux1 import parse_flux1_params, parse_training_config\n",
                "from gensbi.utils.plotting import plot_marginals\n",
                "from gensbi.models import Flux1Params, Flux1\n",
                "from gensbi.recipes import ConditionalFlowPipeline\n",
                "\n",
                "# Point to the config file\n",
                "config_path = \"./config/gw_config_6c_1.yaml\""
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Helper Functions and Classes\n",
                "\n",
                "We define normalization helpers and the wrapper class that connects the VAE and the SBI model."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def normalize(batch, mean, std):\n",
                "    mean = jnp.asarray(mean, dtype=batch.dtype)\n",
                "    std = jnp.asarray(std, dtype=batch.dtype)\n",
                "    return (batch - mean) / std\n",
                "\n",
                "def unnormalize(batch, mean, std):\n",
                "    mean = jnp.asarray(mean, dtype=batch.dtype)\n",
                "    std = jnp.asarray(std, dtype=batch.dtype)\n",
                "    return batch * std + mean\n",
                "\n",
                "class GWModel(nnx.Module):\n",
                "    \"\"\"\n",
                "    A combined model that first encodes the conditioning data (x) using a VAE,\n",
                "    and then passes the latent embedding to the SBI model (Flux).\n",
                "    \"\"\"\n",
                "    def __init__(self, vae, sbi_model):\n",
                "        self.vae = vae\n",
                "        self.sbi_model = sbi_model\n",
                "\n",
                "    def __call__(\n",
                "        self,\n",
                "        t: Array,\n",
                "        obs: Array,\n",
                "        obs_ids: Array,\n",
                "        cond: Array,\n",
                "        cond_ids: Array,\n",
                "        conditioned: bool | Array = True,\n",
                "        guidance: Array | None = None,\n",
                "        encoder_key=None,\n",
                "    ):\n",
                "        # 1. Encode the high-dimensional conditioning data `cond` (GW time series)\n",
                "        # The VAE encoder reduces it to a latent feature map `cond_latent`\n",
                "        cond_latent = self.vae.encode(cond, encoder_key)\n",
                "\n",
                "        # 2. Pass the parameters `obs` (theta) and the encoded condition to the Flow model\n",
                "        return self.sbi_model(\n",
                "            t=t,\n",
                "            obs=obs,\n",
                "            obs_ids=obs_ids,\n",
                "            cond=cond_latent, # The flow sees the compressed representation\n",
                "            cond_ids=cond_ids,\n",
                "            conditioned=conditioned,\n",
                "            guidance=guidance,\n",
                "        )"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Data Loading\n",
                "\n",
                "We load the Gravitational Waves dataset."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "repo_name = \"aurelio-amerio/SBI-benchmarks\"\n",
                "task_name = \"gravitational_waves\"\n",
                "\n",
                "# Load dataset with numpy format\n",
                "dataset = load_dataset(repo_name, task_name).with_format(\"numpy\")\n",
                "\n",
                "df_train = dataset[\"train\"]\n",
                "df_val = dataset[\"validation\"]\n",
                "df_test = dataset[\"test\"]\n",
                "\n",
                "# Define normalization statistics (pre-computed versions to save time vs computing on full dataset)\n",
                "xs_mean = jnp.array([[[0.00051776, -0.00040733]]], dtype=jnp.bfloat16)\n",
                "thetas_mean = jnp.array([[44.826576, 45.070328]], dtype=jnp.bfloat16)\n",
                "xs_std = jnp.array([[[60.80799, 59.33193]]], dtype=jnp.bfloat16)\n",
                "thetas_std = jnp.array([[20.189356, 20.16127]], dtype=jnp.bfloat16)\n",
                "\n",
                "dim_obs = 2\n",
                "ch_obs = 1\n",
                "# Note: dim_cond and ch_cond are for the raw data, but the model will work on VAE latents."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Model Initialization\n",
                "\n",
                "We initialize the VAE and the Flux model using the configuration."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# --- 1. Initialize VAE ---\n",
                "params_dict = parse_autoencoder_params(config_path)\n",
                "\n",
                "ae_params = AutoEncoderParams(\n",
                "    rngs=nnx.Rngs(0),\n",
                "    **params_dict,\n",
                ")\n",
                "\n",
                "vae_model = AutoEncoder1D(ae_params)\n",
                "\n",
                "# Optimization: Remove the decoder since we only need the encoder for NPE\n",
                "vae_model.Decoder1D = None\n",
                "gc.collect()\n",
                "\n",
                "# Get latent dimensions from the initialized VAE to configure the Flow model correctly\n",
                "dim_cond_latent = vae_model.latent_shape[1]\n",
                "z_ch = vae_model.latent_shape[2]\n",
                "print(f\"VAE Output Shape: (Length={dim_cond_latent}, Channels={z_ch})\")\n",
                "\n",
                "# --- 2. Initialize Flux (SBI Model) ---\n",
                "params_dict_flux = parse_flux1_params(config_path)\n",
                "\n",
                "# Ensure the Flux model expects the context dimension provided by the VAE\n",
                "params_flux = Flux1Params(\n",
                "    rngs=nnx.Rngs(0),\n",
                "    dim_obs=dim_obs,\n",
                "    dim_cond=dim_cond_latent, \n",
                "    **params_dict_flux,\n",
                ")\n",
                "\n",
                "model_sbi = Flux1(params_flux)\n",
                "\n",
                "# --- 3. Combine them ---\n",
                "model = GWModel(vae_model, model_sbi)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Pipeline and Restoration\n",
                "\n",
                "We set up the `ConditionalFlowPipeline`. Instead of training, we will restore a pre-trained checkpoint."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Pre-processing map for the dataset pipeline\n",
                "def split_data(batch):\n",
                "    obs = jnp.array(batch[\"thetas\"], dtype=jnp.bfloat16)\n",
                "    obs = normalize(obs, thetas_mean, thetas_std)\n",
                "    obs = obs.reshape(obs.shape[0], dim_obs, ch_obs)\n",
                "    cond = jnp.array(batch[\"xs\"], dtype=jnp.bfloat16)\n",
                "    cond = normalize(cond, xs_mean, xs_std)\n",
                "    return obs, cond\n",
                "\n",
                "# Training stats needed for pipeline init (even for inference)\n",
                "with open(config_path, \"r\") as f:\n",
                "    config = yaml.safe_load(f)\n",
                "    batch_size = config[\"training\"][\"batch_size\"]\n",
                "    training_config = parse_training_config(config_path)\n",
                "\n",
                "# Dummy dataset creation for pipeline initialization\n",
                "# (The pipeline uses this to infer shapes and types, even if we just restore)\n",
                "train_dataset_npe = (\n",
                "    grain.MapDataset.source(df_train)\n",
                "    .shuffle(42)\n",
                "    .repeat()\n",
                "    .to_iter_dataset()\n",
                "    .batch(batch_size)\n",
                "    .map(split_data)\n",
                ")\n",
                "val_dataset_npe = (\n",
                "    grain.MapDataset.source(df_val)\n",
                "    .shuffle(42)\n",
                "    .repeat()\n",
                "    .to_iter_dataset()\n",
                "    .batch(512)\n",
                "    .map(split_data)\n",
                ")\n",
                "\n",
                "# Set checkpoint directory\n",
                "training_config[\"checkpoint_dir\"] = (\n",
                "    \"/lhome/ific/a/aamerio/data/github/GenSBI-examples/examples/sbi-benchmarks/gravitational_waves/gw_npe_v6c/checkpoints\"\n",
                ")\n",
                "\n",
                "# the ConditionalFlowPipeline supports an arbitrary model (with the right interface), \n",
                "# so we can pass it our composite model\n",
                "pipeline_latent = ConditionalFlowPipeline(\n",
                "    model,\n",
                "    train_dataset_npe,\n",
                "    val_dataset_npe,\n",
                "    dim_obs=dim_obs,\n",
                "    dim_cond=dim_cond_latent,  # Latent length\n",
                "    ch_obs=ch_obs,\n",
                "    ch_cond=z_ch,  # Latent channels\n",
                "    training_config=training_config,\n",
                ")\n",
                "\n",
                "print(\"Restoring model from checkpoint...\")\n",
                "pipeline_latent.restore_model()\n",
                "print(\"Model restored!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Inference and Visualization\n",
                "\n",
                "We take a single observation from the test set, sample the posterior using our model, and visualize the results."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "0807bf9e",
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1. Select a test observation\n",
                "idx = 0\n",
                "x_o_raw = df_test[\"xs\"][idx][None, ...]\n",
                "theta_true = df_test[\"thetas\"][idx]\n",
                "\n",
                "# Normalize observation\n",
                "x_o = normalize(jnp.array(x_o_raw, dtype=jnp.bfloat16), xs_mean, xs_std)\n",
                "\n",
                "# 2. Sample from the posterior\n",
                "# We generate 100,000 samples to get a smooth distribution\n",
                "print(\"Sampling...\")\n",
                "samples = pipeline_latent.sample_batched(\n",
                "    nnx.Rngs(0).sample(),\n",
                "    x_o,\n",
                "    100_000,\n",
                "    chunk_size=10_000,\n",
                "    encoder_key=jax.random.PRNGKey(1234),\n",
                ")\n",
                "\n",
                "# Reshape samples: (num_samples, 1, 2, 1) -> (num_samples, 2)\n",
                "res = samples[:, 0, :, 0]\n",
                "\n",
                "# 3. Unnormalize to get back to physical units\n",
                "res_unnorm = unnormalize(res, thetas_mean, thetas_std)\n",
                "\n",
                "# Apply modulo 360 as these are periodic angular parameters\n",
                "res_unnorm = jnp.mod(res_unnorm, 360.0)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 4. Plot\n",
                "print(\"Plotting marginals...\")\n",
                "plot_marginals(\n",
                "    res_unnorm, \n",
                "    true_param=theta_true, \n",
                "    range=[(25, 75), (25, 75)], # Zoom in on the relevant region\n",
                "    gridsize=30\n",
                ")\n",
                "plt.show()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
