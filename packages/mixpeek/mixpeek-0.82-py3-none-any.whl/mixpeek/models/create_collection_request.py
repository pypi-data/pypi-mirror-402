# coding: utf-8

"""
    Mixpeek API

    This is the Mixpeek API, providing access to various endpoints for data processing and retrieval.

    The version of the OpenAPI document: 0.82
    Contact: info@mixpeek.com
    Generated by OpenAPI Generator (https://openapi-generator.tech)

    Do not edit the class manually.
"""  # noqa: E501


from __future__ import annotations
import pprint
import re  # noqa: F401
import json

from pydantic import BaseModel, ConfigDict, Field, StrictBool, StrictStr
from typing import Any, ClassVar, Dict, List, Optional
from mixpeek.models.alert_application_config_input import AlertApplicationConfigInput
from mixpeek.models.bucket_schema_input import BucketSchemaInput
from mixpeek.models.cluster_application_config import ClusterApplicationConfig
from mixpeek.models.shared_collection_features_extractors_models_feature_extractor_config_input import SharedCollectionFeaturesExtractorsModelsFeatureExtractorConfigInput
from mixpeek.models.source_config_input import SourceConfigInput
from mixpeek.models.taxonomy_application_config_input import TaxonomyApplicationConfigInput
from typing import Optional, Set
from typing_extensions import Self

class CreateCollectionRequest(BaseModel):
    """
    Request model for creating a new collection.  Collections process data from buckets or other collections using a single feature extractor.  KEY ARCHITECTURAL CHANGE: Each collection has EXACTLY ONE feature extractor. - Use field_passthrough to include additional source fields in output documents - Multiple extractors = multiple collections - This simplifies processing and makes output schema deterministic  CRITICAL: To use input_mappings: 1. Your source bucket MUST have a bucket_schema defined 2. The input_mappings reference fields from that bucket_schema 3. The system validates that mapped fields exist in the source schema  Example workflow: 1. Create bucket with schema: { \"properties\": { \"image\": {\"type\": \"image\"}, \"title\": {\"type\": \"string\"} } } 2. Upload objects conforming to that schema 3. Create collection with:    - input_mappings: { \"image\": \"image\" }    - field_passthrough: [{\"source_path\": \"title\"}] 4. Output documents will have both extractor outputs AND passthrough fields  Schema Computation: - output_schema is computed IMMEDIATELY when collection is created - output_schema = field_passthrough fields + extractor output fields - No waiting for documents to be processed!
    """ # noqa: E501
    collection_name: StrictStr = Field(description="Name of the collection to create")
    description: Optional[StrictStr] = Field(default=None, description="Description of the collection")
    source: SourceConfigInput = Field(description="Source configuration (bucket or collection) for this collection")
    input_schema: Optional[BucketSchemaInput] = Field(default=None, description="Input schema for the collection. If not provided, inferred from source bucket's bucket_schema or source collection's output_schema. REQUIRED for input_mappings to work - defines what fields can be mapped to feature extractors.")
    feature_extractor: SharedCollectionFeaturesExtractorsModelsFeatureExtractorConfigInput = Field(description="Single feature extractor for this collection. Use field_passthrough within the extractor config to include additional source fields. For multiple extractors, create multiple collections and use collection-to-collection pipelines.")
    enabled: Optional[StrictBool] = Field(default=True, description="Whether the collection is enabled")
    metadata: Optional[Dict[str, Any]] = Field(default=None, description="Additional metadata for the collection")
    taxonomy_applications: Optional[List[TaxonomyApplicationConfigInput]] = Field(default=None, description="Optional taxonomy applications to automatically enrich documents in this collection. Each taxonomy will classify/enrich documents based on configured retriever matches.")
    cluster_applications: Optional[List[ClusterApplicationConfig]] = Field(default=None, description="Optional cluster applications to automatically execute when batch processing completes. Each cluster enriches documents with cluster assignments (cluster_id, cluster_label, etc.).")
    alert_applications: Optional[List[AlertApplicationConfigInput]] = Field(default=None, description="Optional alert applications to automatically execute when documents are ingested. Each alert runs a retriever against new documents and sends notifications if matches are found. Supports both ON_INGEST (triggered per batch) and SCHEDULED (periodic) execution modes.")
    __properties: ClassVar[List[str]] = ["collection_name", "description", "source", "input_schema", "feature_extractor", "enabled", "metadata", "taxonomy_applications", "cluster_applications", "alert_applications"]

    model_config = ConfigDict(
        populate_by_name=True,
        validate_assignment=True,
        protected_namespaces=(),
    )


    def to_str(self) -> str:
        """Returns the string representation of the model using alias"""
        return pprint.pformat(self.model_dump(by_alias=True))

    def to_json(self) -> str:
        """Returns the JSON representation of the model using alias"""
        # TODO: pydantic v2: use .model_dump_json(by_alias=True, exclude_unset=True) instead
        return json.dumps(self.to_dict())

    @classmethod
    def from_json(cls, json_str: str) -> Optional[Self]:
        """Create an instance of CreateCollectionRequest from a JSON string"""
        return cls.from_dict(json.loads(json_str))

    def to_dict(self) -> Dict[str, Any]:
        """Return the dictionary representation of the model using alias.

        This has the following differences from calling pydantic's
        `self.model_dump(by_alias=True)`:

        * `None` is only added to the output dict for nullable fields that
          were set at model initialization. Other fields with value `None`
          are ignored.
        """
        excluded_fields: Set[str] = set([
        ])

        _dict = self.model_dump(
            by_alias=True,
            exclude=excluded_fields,
            exclude_none=True,
        )
        # override the default output from pydantic by calling `to_dict()` of source
        if self.source:
            _dict['source'] = self.source.to_dict()
        # override the default output from pydantic by calling `to_dict()` of input_schema
        if self.input_schema:
            _dict['input_schema'] = self.input_schema.to_dict()
        # override the default output from pydantic by calling `to_dict()` of feature_extractor
        if self.feature_extractor:
            _dict['feature_extractor'] = self.feature_extractor.to_dict()
        # override the default output from pydantic by calling `to_dict()` of each item in taxonomy_applications (list)
        _items = []
        if self.taxonomy_applications:
            for _item_taxonomy_applications in self.taxonomy_applications:
                if _item_taxonomy_applications:
                    _items.append(_item_taxonomy_applications.to_dict())
            _dict['taxonomy_applications'] = _items
        # override the default output from pydantic by calling `to_dict()` of each item in cluster_applications (list)
        _items = []
        if self.cluster_applications:
            for _item_cluster_applications in self.cluster_applications:
                if _item_cluster_applications:
                    _items.append(_item_cluster_applications.to_dict())
            _dict['cluster_applications'] = _items
        # override the default output from pydantic by calling `to_dict()` of each item in alert_applications (list)
        _items = []
        if self.alert_applications:
            for _item_alert_applications in self.alert_applications:
                if _item_alert_applications:
                    _items.append(_item_alert_applications.to_dict())
            _dict['alert_applications'] = _items
        return _dict

    @classmethod
    def from_dict(cls, obj: Optional[Dict[str, Any]]) -> Optional[Self]:
        """Create an instance of CreateCollectionRequest from a dict"""
        if obj is None:
            return None

        if not isinstance(obj, dict):
            return cls.model_validate(obj)

        _obj = cls.model_validate({
            "collection_name": obj.get("collection_name"),
            "description": obj.get("description"),
            "source": SourceConfigInput.from_dict(obj["source"]) if obj.get("source") is not None else None,
            "input_schema": BucketSchemaInput.from_dict(obj["input_schema"]) if obj.get("input_schema") is not None else None,
            "feature_extractor": SharedCollectionFeaturesExtractorsModelsFeatureExtractorConfigInput.from_dict(obj["feature_extractor"]) if obj.get("feature_extractor") is not None else None,
            "enabled": obj.get("enabled") if obj.get("enabled") is not None else True,
            "metadata": obj.get("metadata"),
            "taxonomy_applications": [TaxonomyApplicationConfigInput.from_dict(_item) for _item in obj["taxonomy_applications"]] if obj.get("taxonomy_applications") is not None else None,
            "cluster_applications": [ClusterApplicationConfig.from_dict(_item) for _item in obj["cluster_applications"]] if obj.get("cluster_applications") is not None else None,
            "alert_applications": [AlertApplicationConfigInput.from_dict(_item) for _item in obj["alert_applications"]] if obj.get("alert_applications") is not None else None
        })
        return _obj


