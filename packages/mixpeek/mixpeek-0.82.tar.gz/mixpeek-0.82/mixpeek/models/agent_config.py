# coding: utf-8

"""
    Mixpeek API

    This is the Mixpeek API, providing access to various endpoints for data processing and retrieval.

    The version of the OpenAPI document: 0.82
    Contact: info@mixpeek.com
    Generated by OpenAPI Generator (https://openapi-generator.tech)

    Do not edit the class manually.
"""  # noqa: E501


from __future__ import annotations
import pprint
import re  # noqa: F401
import json

from pydantic import BaseModel, ConfigDict, Field, StrictStr
from typing import Any, ClassVar, Dict, List, Optional, Union
from typing_extensions import Annotated
from typing import Optional, Set
from typing_extensions import Self

class AgentConfig(BaseModel):
    """
    Agent configuration for session.  This config is immutable after session creation (similar to RetrieverConfig). To change agent config, create a new session.  Attributes:     model: LLM model identifier. Supported models:         - gemini-2.0-flash (fastest, cheapest)         - gemini-2.5-pro (better quality)         - gpt-4o-mini, gpt-4o     temperature: Sampling temperature for LLM responses (0.0-2.0)         - 0.0-0.3: More deterministic, focused responses         - 0.5-0.7: Balanced creativity and coherence         - 0.8-2.0: More creative, varied responses     max_tokens: Maximum tokens per response (1-100000)     system_prompt: System prompt that defines agent behavior and persona     available_tools: List of tools the agent can call (see AvailableTool enum)  Example:     ```python     config = AgentConfig(         model=\"gemini-2.0-flash\",         temperature=0.7,         max_tokens=4096,         system_prompt=\"You are a helpful video search assistant.\",         available_tools=[             \"smart_search\",             \"execute_retriever\",             \"list_collections\"         ]     )     ```
    """ # noqa: E501
    model: Optional[StrictStr] = Field(default='gemini-2.0-flash', description="LLM model identifier. Options: 'gemini-2.0-flash' (fastest, cheapest), 'gemini-2.5-pro' (better quality), 'gpt-4o-mini', 'gpt-4o'")
    temperature: Optional[Union[Annotated[float, Field(le=2.0, strict=True, ge=0.0)], Annotated[int, Field(le=2, strict=True, ge=0)]]] = Field(default=0.7, description="Sampling temperature for LLM responses (0.0-2.0). Lower values (0.0-0.3) are more deterministic. Higher values (0.8-2.0) are more creative.")
    max_tokens: Optional[Annotated[int, Field(le=100000, strict=True, ge=1)]] = Field(default=4096, description="Maximum tokens per response")
    system_prompt: Optional[StrictStr] = Field(default='You are a helpful AI assistant with access to Mixpeek\'s data infrastructure.', description="System prompt that defines agent behavior and persona")
    available_tools: Optional[List[StrictStr]] = Field(default=None, description="List of tool names the agent can call. See AvailableTool enum for full list. Key tools: smart_search (natural language search), execute_retriever, execute_adhoc_retriever, list_collections, list_retrievers, list_buckets, create_upload, analyze_sample_with_pipeline, export_manifest, generate_manifest, detect_intent.")
    __properties: ClassVar[List[str]] = ["model", "temperature", "max_tokens", "system_prompt", "available_tools"]

    model_config = ConfigDict(
        populate_by_name=True,
        validate_assignment=True,
        protected_namespaces=(),
    )


    def to_str(self) -> str:
        """Returns the string representation of the model using alias"""
        return pprint.pformat(self.model_dump(by_alias=True))

    def to_json(self) -> str:
        """Returns the JSON representation of the model using alias"""
        # TODO: pydantic v2: use .model_dump_json(by_alias=True, exclude_unset=True) instead
        return json.dumps(self.to_dict())

    @classmethod
    def from_json(cls, json_str: str) -> Optional[Self]:
        """Create an instance of AgentConfig from a JSON string"""
        return cls.from_dict(json.loads(json_str))

    def to_dict(self) -> Dict[str, Any]:
        """Return the dictionary representation of the model using alias.

        This has the following differences from calling pydantic's
        `self.model_dump(by_alias=True)`:

        * `None` is only added to the output dict for nullable fields that
          were set at model initialization. Other fields with value `None`
          are ignored.
        """
        excluded_fields: Set[str] = set([
        ])

        _dict = self.model_dump(
            by_alias=True,
            exclude=excluded_fields,
            exclude_none=True,
        )
        return _dict

    @classmethod
    def from_dict(cls, obj: Optional[Dict[str, Any]]) -> Optional[Self]:
        """Create an instance of AgentConfig from a dict"""
        if obj is None:
            return None

        if not isinstance(obj, dict):
            return cls.model_validate(obj)

        _obj = cls.model_validate({
            "model": obj.get("model") if obj.get("model") is not None else 'gemini-2.0-flash',
            "temperature": obj.get("temperature") if obj.get("temperature") is not None else 0.7,
            "max_tokens": obj.get("max_tokens") if obj.get("max_tokens") is not None else 4096,
            "system_prompt": obj.get("system_prompt") if obj.get("system_prompt") is not None else 'You are a helpful AI assistant with access to Mixpeek\'s data infrastructure.',
            "available_tools": obj.get("available_tools")
        })
        return _obj


