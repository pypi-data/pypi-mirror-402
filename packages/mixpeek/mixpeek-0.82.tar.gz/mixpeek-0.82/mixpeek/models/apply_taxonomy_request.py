# coding: utf-8

"""
    Mixpeek API

    This is the Mixpeek API, providing access to various endpoints for data processing and retrieval.

    The version of the OpenAPI document: 0.82
    Contact: info@mixpeek.com
    Generated by OpenAPI Generator (https://openapi-generator.tech)

    Do not edit the class manually.
"""  # noqa: E501


from __future__ import annotations
import pprint
import re  # noqa: F401
import json

from pydantic import BaseModel, ConfigDict, Field, StrictStr
from typing import Any, ClassVar, Dict, List, Optional
from typing_extensions import Annotated
from typing import Optional, Set
from typing_extensions import Self

class ApplyTaxonomyRequest(BaseModel):
    """
    Request to apply a taxonomy to an existing collection.  This endpoint triggers retroactive taxonomy materialization on all documents in a collection using distributed Ray processing.  Use Cases:     - Apply taxonomy to documents that were ingested before the taxonomy was created     - Re-apply taxonomy after taxonomy configuration changes     - Backfill enrichment data for existing collections  Requirements:     - taxonomy_id: REQUIRED - Must be an existing, valid taxonomy     - The taxonomy must already be attached to the collection via taxonomy_applications     - Documents must exist in the collection
    """ # noqa: E501
    taxonomy_id: StrictStr = Field(description="ID of the taxonomy to apply. REQUIRED. Must be an existing taxonomy (tax_*). The taxonomy must already be in the collection's taxonomy_applications list.")
    scroll_filters: Optional[Dict[str, Any]] = Field(default=None, description="Optional Qdrant filters to limit which documents are enriched. NOT REQUIRED. If not provided, all documents in the collection will be enriched. Use to process specific subsets (e.g., documents missing enrichment).")
    batch_size: Optional[Annotated[int, Field(le=5000, strict=True, ge=100)]] = Field(default=1000, description="Number of documents to process in each parallel batch. NOT REQUIRED. Defaults to 1000. Larger batches = fewer Ray tasks but more memory per task. Smaller batches = more Ray tasks but lower memory per task.")
    parallelism: Optional[Annotated[int, Field(le=20, strict=True, ge=1)]] = Field(default=4, description="Number of parallel Ray workers to use for processing. NOT REQUIRED. Defaults to 4. Higher parallelism = faster processing but more cluster resources. Set based on available Ray cluster capacity.")
    __properties: ClassVar[List[str]] = ["taxonomy_id", "scroll_filters", "batch_size", "parallelism"]

    model_config = ConfigDict(
        populate_by_name=True,
        validate_assignment=True,
        protected_namespaces=(),
    )


    def to_str(self) -> str:
        """Returns the string representation of the model using alias"""
        return pprint.pformat(self.model_dump(by_alias=True))

    def to_json(self) -> str:
        """Returns the JSON representation of the model using alias"""
        # TODO: pydantic v2: use .model_dump_json(by_alias=True, exclude_unset=True) instead
        return json.dumps(self.to_dict())

    @classmethod
    def from_json(cls, json_str: str) -> Optional[Self]:
        """Create an instance of ApplyTaxonomyRequest from a JSON string"""
        return cls.from_dict(json.loads(json_str))

    def to_dict(self) -> Dict[str, Any]:
        """Return the dictionary representation of the model using alias.

        This has the following differences from calling pydantic's
        `self.model_dump(by_alias=True)`:

        * `None` is only added to the output dict for nullable fields that
          were set at model initialization. Other fields with value `None`
          are ignored.
        """
        excluded_fields: Set[str] = set([
        ])

        _dict = self.model_dump(
            by_alias=True,
            exclude=excluded_fields,
            exclude_none=True,
        )
        return _dict

    @classmethod
    def from_dict(cls, obj: Optional[Dict[str, Any]]) -> Optional[Self]:
        """Create an instance of ApplyTaxonomyRequest from a dict"""
        if obj is None:
            return None

        if not isinstance(obj, dict):
            return cls.model_validate(obj)

        _obj = cls.model_validate({
            "taxonomy_id": obj.get("taxonomy_id"),
            "scroll_filters": obj.get("scroll_filters"),
            "batch_size": obj.get("batch_size") if obj.get("batch_size") is not None else 1000,
            "parallelism": obj.get("parallelism") if obj.get("parallelism") is not None else 4
        })
        return _obj


