# coding: utf-8

"""
    Mixpeek API

    This is the Mixpeek API, providing access to various endpoints for data processing and retrieval.

    The version of the OpenAPI document: 1.3.0
    Contact: info@mixpeek.com
    Generated by OpenAPI Generator (https://openapi-generator.tech)

    Do not edit the class manually.
"""  # noqa: E501

import warnings
from pydantic import validate_call, Field, StrictFloat, StrictStr, StrictInt
from typing import Any, Dict, List, Optional, Tuple, Union
from typing_extensions import Annotated

from pydantic import Field, StrictStr
from typing import Optional
from typing_extensions import Annotated
from mixpeek.models.raw_inference_request import RawInferenceRequest
from mixpeek.models.raw_inference_response import RawInferenceResponse

from mixpeek.api_client import ApiClient, RequestSerialized
from mixpeek.api_response import ApiResponse
from mixpeek.rest import RESTResponseType


class InferenceApi:
    """NOTE: This class is auto generated by OpenAPI Generator
    Ref: https://openapi-generator.tech

    Do not edit the class manually.
    """

    def __init__(self, api_client=None) -> None:
        if api_client is None:
            api_client = ApiClient.get_default()
        self.api_client = api_client


    @validate_call
    def execute_raw_inference(
        self,
        raw_inference_request: RawInferenceRequest,
        authorization: Annotated[Optional[StrictStr], Field(description="REQUIRED: Bearer token authentication using your API key. Format: 'Bearer sk_xxxxxxxxxxxxx'. You can create API keys in the Mixpeek dashboard under Organization Settings.")] = None,
        x_namespace: Annotated[Optional[StrictStr], Field(description="REQUIRED: Namespace identifier for scoping this request. All resources (collections, buckets, taxonomies, etc.) are scoped to a namespace. You can provide either the namespace name or namespace ID. Format: ns_xxxxxxxxxxxxx (ID) or a custom name like 'my-namespace'")] = None,
        _request_timeout: Union[
            None,
            Annotated[StrictFloat, Field(gt=0)],
            Tuple[
                Annotated[StrictFloat, Field(gt=0)],
                Annotated[StrictFloat, Field(gt=0)]
            ]
        ] = None,
        _request_auth: Optional[Dict[StrictStr, Any]] = None,
        _content_type: Optional[StrictStr] = None,
        _headers: Optional[Dict[StrictStr, Any]] = None,
        _host_index: Annotated[StrictInt, Field(ge=0, le=0)] = 0,
    ) -> RawInferenceResponse:
        """Execute Raw Inference

        Execute raw inference with provider+model or custom plugin.  This endpoint provides direct access to inference services without the retriever framework overhead. Supports two modes:  1. **Provider + Model**: Use standard providers (openai, google, anthropic) 2. **Custom Plugin**: Use your custom inference plugins by inference_name  ## Supported Providers  - **openai**: GPT models, embeddings, Whisper transcription - **google**: Gemini models, Vertex multimodal embeddings (1408D) - **anthropic**: Claude models  ## Examples  ### Custom Plugin (by inference_name) ```json {     \"inference_name\": \"my_text_embedder_1_0_0\",     \"inputs\": {\"text\": \"hello world\"},     \"parameters\": {} } ```  ### Custom Plugin (by feature_uri) ```json {     \"feature_uri\": \"mixpeek://my_custom_embedder@1.0.0/embedding\",     \"inputs\": {\"text\": \"hello world\"},     \"parameters\": {} } ```  ### Builtin Embedder (by feature_uri) ```json {     \"feature_uri\": \"mixpeek://text_extractor@v1/multilingual_e5_large_instruct_v1\",     \"inputs\": {\"text\": \"hello world\"},     \"parameters\": {} } ```  ### Chat Completion ```json {     \"provider\": \"openai\",     \"model\": \"gpt-4o-mini\",     \"inputs\": {\"prompts\": [\"What is AI?\"]},     \"parameters\": {\"temperature\": 0.7, \"max_tokens\": 500} } ```  ### Text Embedding (OpenAI) ```json {     \"provider\": \"openai\",     \"model\": \"text-embedding-3-large\",     \"inputs\": {\"text\": \"machine learning\"},     \"parameters\": {} } ```  ### Text Embedding (Google Vertex Multimodal - 1408D) ```json {     \"provider\": \"google\",     \"model\": \"multimodalembedding\",     \"inputs\": {\"text\": \"machine learning\"},     \"parameters\": {} } ```  ### Image Embedding (Google Vertex Multimodal - 1408D) ```json {     \"provider\": \"google\",     \"model\": \"multimodalembedding\",     \"inputs\": {\"image_url\": \"https://example.com/image.jpg\"},     \"parameters\": {} } ```  ### Image Embedding from Base64 ```json {     \"provider\": \"google\",     \"model\": \"multimodalembedding\",     \"inputs\": {\"image_base64\": \"<base64-encoded-image>\"},     \"parameters\": {} } ```  ### Video Embedding (Google Vertex Multimodal - 1408D) ```json {     \"provider\": \"google\",     \"model\": \"multimodalembedding\",     \"inputs\": {\"video_url\": \"https://example.com/video.mp4\"},     \"parameters\": {} } ```  ### Video Embedding from Base64 ```json {     \"provider\": \"google\",     \"model\": \"multimodalembedding\",     \"inputs\": {\"video_base64\": \"<base64-encoded-video>\"},     \"parameters\": {} } ```  ### Audio Transcription ```json {     \"provider\": \"openai\",     \"model\": \"whisper-1\",     \"inputs\": {\"audio_url\": \"https://example.com/audio.mp3\"},     \"parameters\": {} } ```  ### Vision (Multimodal LLM) ```json {     \"provider\": \"openai\",     \"model\": \"gpt-4o\",     \"inputs\": {         \"prompts\": [\"Describe this image\"],         \"image_url\": \"https://example.com/image.jpg\"     },     \"parameters\": {\"temperature\": 0.5} } ```  Args:     request: FastAPI request object (populated by middleware)     payload: Raw inference request  Returns:     Inference response with results and metadata  Raises:     400 Bad Request: Invalid provider, model, or inputs     401 Unauthorized: Missing or invalid API key     429 Too Many Requests: Rate limit exceeded     500 Internal Server Error: Inference execution failed

        :param raw_inference_request: (required)
        :type raw_inference_request: RawInferenceRequest
        :param authorization: REQUIRED: Bearer token authentication using your API key. Format: 'Bearer sk_xxxxxxxxxxxxx'. You can create API keys in the Mixpeek dashboard under Organization Settings.
        :type authorization: str
        :param x_namespace: REQUIRED: Namespace identifier for scoping this request. All resources (collections, buckets, taxonomies, etc.) are scoped to a namespace. You can provide either the namespace name or namespace ID. Format: ns_xxxxxxxxxxxxx (ID) or a custom name like 'my-namespace'
        :type x_namespace: str
        :param _request_timeout: timeout setting for this request. If one
                                 number provided, it will be total request
                                 timeout. It can also be a pair (tuple) of
                                 (connection, read) timeouts.
        :type _request_timeout: int, tuple(int, int), optional
        :param _request_auth: set to override the auth_settings for an a single
                              request; this effectively ignores the
                              authentication in the spec for a single request.
        :type _request_auth: dict, optional
        :param _content_type: force content-type for the request.
        :type _content_type: str, Optional
        :param _headers: set to override the headers for a single
                         request; this effectively ignores the headers
                         in the spec for a single request.
        :type _headers: dict, optional
        :param _host_index: set to override the host_index for a single
                            request; this effectively ignores the host_index
                            in the spec for a single request.
        :type _host_index: int, optional
        :return: Returns the result object.
        """ # noqa: E501

        _param = self._execute_raw_inference_serialize(
            raw_inference_request=raw_inference_request,
            authorization=authorization,
            x_namespace=x_namespace,
            _request_auth=_request_auth,
            _content_type=_content_type,
            _headers=_headers,
            _host_index=_host_index
        )

        _response_types_map: Dict[str, Optional[str]] = {
            '200': "RawInferenceResponse",
            '400': "ErrorResponse",
            '401': "ErrorResponse",
            '403': "ErrorResponse",
            '404': "ErrorResponse",
            '500': "ErrorResponse",
            '422': "HTTPValidationError",
        }
        response_data = self.api_client.call_api(
            *_param,
            _request_timeout=_request_timeout
        )
        response_data.read()
        return self.api_client.response_deserialize(
            response_data=response_data,
            response_types_map=_response_types_map,
        ).data


    @validate_call
    def execute_raw_inference_with_http_info(
        self,
        raw_inference_request: RawInferenceRequest,
        authorization: Annotated[Optional[StrictStr], Field(description="REQUIRED: Bearer token authentication using your API key. Format: 'Bearer sk_xxxxxxxxxxxxx'. You can create API keys in the Mixpeek dashboard under Organization Settings.")] = None,
        x_namespace: Annotated[Optional[StrictStr], Field(description="REQUIRED: Namespace identifier for scoping this request. All resources (collections, buckets, taxonomies, etc.) are scoped to a namespace. You can provide either the namespace name or namespace ID. Format: ns_xxxxxxxxxxxxx (ID) or a custom name like 'my-namespace'")] = None,
        _request_timeout: Union[
            None,
            Annotated[StrictFloat, Field(gt=0)],
            Tuple[
                Annotated[StrictFloat, Field(gt=0)],
                Annotated[StrictFloat, Field(gt=0)]
            ]
        ] = None,
        _request_auth: Optional[Dict[StrictStr, Any]] = None,
        _content_type: Optional[StrictStr] = None,
        _headers: Optional[Dict[StrictStr, Any]] = None,
        _host_index: Annotated[StrictInt, Field(ge=0, le=0)] = 0,
    ) -> ApiResponse[RawInferenceResponse]:
        """Execute Raw Inference

        Execute raw inference with provider+model or custom plugin.  This endpoint provides direct access to inference services without the retriever framework overhead. Supports two modes:  1. **Provider + Model**: Use standard providers (openai, google, anthropic) 2. **Custom Plugin**: Use your custom inference plugins by inference_name  ## Supported Providers  - **openai**: GPT models, embeddings, Whisper transcription - **google**: Gemini models, Vertex multimodal embeddings (1408D) - **anthropic**: Claude models  ## Examples  ### Custom Plugin (by inference_name) ```json {     \"inference_name\": \"my_text_embedder_1_0_0\",     \"inputs\": {\"text\": \"hello world\"},     \"parameters\": {} } ```  ### Custom Plugin (by feature_uri) ```json {     \"feature_uri\": \"mixpeek://my_custom_embedder@1.0.0/embedding\",     \"inputs\": {\"text\": \"hello world\"},     \"parameters\": {} } ```  ### Builtin Embedder (by feature_uri) ```json {     \"feature_uri\": \"mixpeek://text_extractor@v1/multilingual_e5_large_instruct_v1\",     \"inputs\": {\"text\": \"hello world\"},     \"parameters\": {} } ```  ### Chat Completion ```json {     \"provider\": \"openai\",     \"model\": \"gpt-4o-mini\",     \"inputs\": {\"prompts\": [\"What is AI?\"]},     \"parameters\": {\"temperature\": 0.7, \"max_tokens\": 500} } ```  ### Text Embedding (OpenAI) ```json {     \"provider\": \"openai\",     \"model\": \"text-embedding-3-large\",     \"inputs\": {\"text\": \"machine learning\"},     \"parameters\": {} } ```  ### Text Embedding (Google Vertex Multimodal - 1408D) ```json {     \"provider\": \"google\",     \"model\": \"multimodalembedding\",     \"inputs\": {\"text\": \"machine learning\"},     \"parameters\": {} } ```  ### Image Embedding (Google Vertex Multimodal - 1408D) ```json {     \"provider\": \"google\",     \"model\": \"multimodalembedding\",     \"inputs\": {\"image_url\": \"https://example.com/image.jpg\"},     \"parameters\": {} } ```  ### Image Embedding from Base64 ```json {     \"provider\": \"google\",     \"model\": \"multimodalembedding\",     \"inputs\": {\"image_base64\": \"<base64-encoded-image>\"},     \"parameters\": {} } ```  ### Video Embedding (Google Vertex Multimodal - 1408D) ```json {     \"provider\": \"google\",     \"model\": \"multimodalembedding\",     \"inputs\": {\"video_url\": \"https://example.com/video.mp4\"},     \"parameters\": {} } ```  ### Video Embedding from Base64 ```json {     \"provider\": \"google\",     \"model\": \"multimodalembedding\",     \"inputs\": {\"video_base64\": \"<base64-encoded-video>\"},     \"parameters\": {} } ```  ### Audio Transcription ```json {     \"provider\": \"openai\",     \"model\": \"whisper-1\",     \"inputs\": {\"audio_url\": \"https://example.com/audio.mp3\"},     \"parameters\": {} } ```  ### Vision (Multimodal LLM) ```json {     \"provider\": \"openai\",     \"model\": \"gpt-4o\",     \"inputs\": {         \"prompts\": [\"Describe this image\"],         \"image_url\": \"https://example.com/image.jpg\"     },     \"parameters\": {\"temperature\": 0.5} } ```  Args:     request: FastAPI request object (populated by middleware)     payload: Raw inference request  Returns:     Inference response with results and metadata  Raises:     400 Bad Request: Invalid provider, model, or inputs     401 Unauthorized: Missing or invalid API key     429 Too Many Requests: Rate limit exceeded     500 Internal Server Error: Inference execution failed

        :param raw_inference_request: (required)
        :type raw_inference_request: RawInferenceRequest
        :param authorization: REQUIRED: Bearer token authentication using your API key. Format: 'Bearer sk_xxxxxxxxxxxxx'. You can create API keys in the Mixpeek dashboard under Organization Settings.
        :type authorization: str
        :param x_namespace: REQUIRED: Namespace identifier for scoping this request. All resources (collections, buckets, taxonomies, etc.) are scoped to a namespace. You can provide either the namespace name or namespace ID. Format: ns_xxxxxxxxxxxxx (ID) or a custom name like 'my-namespace'
        :type x_namespace: str
        :param _request_timeout: timeout setting for this request. If one
                                 number provided, it will be total request
                                 timeout. It can also be a pair (tuple) of
                                 (connection, read) timeouts.
        :type _request_timeout: int, tuple(int, int), optional
        :param _request_auth: set to override the auth_settings for an a single
                              request; this effectively ignores the
                              authentication in the spec for a single request.
        :type _request_auth: dict, optional
        :param _content_type: force content-type for the request.
        :type _content_type: str, Optional
        :param _headers: set to override the headers for a single
                         request; this effectively ignores the headers
                         in the spec for a single request.
        :type _headers: dict, optional
        :param _host_index: set to override the host_index for a single
                            request; this effectively ignores the host_index
                            in the spec for a single request.
        :type _host_index: int, optional
        :return: Returns the result object.
        """ # noqa: E501

        _param = self._execute_raw_inference_serialize(
            raw_inference_request=raw_inference_request,
            authorization=authorization,
            x_namespace=x_namespace,
            _request_auth=_request_auth,
            _content_type=_content_type,
            _headers=_headers,
            _host_index=_host_index
        )

        _response_types_map: Dict[str, Optional[str]] = {
            '200': "RawInferenceResponse",
            '400': "ErrorResponse",
            '401': "ErrorResponse",
            '403': "ErrorResponse",
            '404': "ErrorResponse",
            '500': "ErrorResponse",
            '422': "HTTPValidationError",
        }
        response_data = self.api_client.call_api(
            *_param,
            _request_timeout=_request_timeout
        )
        response_data.read()
        return self.api_client.response_deserialize(
            response_data=response_data,
            response_types_map=_response_types_map,
        )


    @validate_call
    def execute_raw_inference_without_preload_content(
        self,
        raw_inference_request: RawInferenceRequest,
        authorization: Annotated[Optional[StrictStr], Field(description="REQUIRED: Bearer token authentication using your API key. Format: 'Bearer sk_xxxxxxxxxxxxx'. You can create API keys in the Mixpeek dashboard under Organization Settings.")] = None,
        x_namespace: Annotated[Optional[StrictStr], Field(description="REQUIRED: Namespace identifier for scoping this request. All resources (collections, buckets, taxonomies, etc.) are scoped to a namespace. You can provide either the namespace name or namespace ID. Format: ns_xxxxxxxxxxxxx (ID) or a custom name like 'my-namespace'")] = None,
        _request_timeout: Union[
            None,
            Annotated[StrictFloat, Field(gt=0)],
            Tuple[
                Annotated[StrictFloat, Field(gt=0)],
                Annotated[StrictFloat, Field(gt=0)]
            ]
        ] = None,
        _request_auth: Optional[Dict[StrictStr, Any]] = None,
        _content_type: Optional[StrictStr] = None,
        _headers: Optional[Dict[StrictStr, Any]] = None,
        _host_index: Annotated[StrictInt, Field(ge=0, le=0)] = 0,
    ) -> RESTResponseType:
        """Execute Raw Inference

        Execute raw inference with provider+model or custom plugin.  This endpoint provides direct access to inference services without the retriever framework overhead. Supports two modes:  1. **Provider + Model**: Use standard providers (openai, google, anthropic) 2. **Custom Plugin**: Use your custom inference plugins by inference_name  ## Supported Providers  - **openai**: GPT models, embeddings, Whisper transcription - **google**: Gemini models, Vertex multimodal embeddings (1408D) - **anthropic**: Claude models  ## Examples  ### Custom Plugin (by inference_name) ```json {     \"inference_name\": \"my_text_embedder_1_0_0\",     \"inputs\": {\"text\": \"hello world\"},     \"parameters\": {} } ```  ### Custom Plugin (by feature_uri) ```json {     \"feature_uri\": \"mixpeek://my_custom_embedder@1.0.0/embedding\",     \"inputs\": {\"text\": \"hello world\"},     \"parameters\": {} } ```  ### Builtin Embedder (by feature_uri) ```json {     \"feature_uri\": \"mixpeek://text_extractor@v1/multilingual_e5_large_instruct_v1\",     \"inputs\": {\"text\": \"hello world\"},     \"parameters\": {} } ```  ### Chat Completion ```json {     \"provider\": \"openai\",     \"model\": \"gpt-4o-mini\",     \"inputs\": {\"prompts\": [\"What is AI?\"]},     \"parameters\": {\"temperature\": 0.7, \"max_tokens\": 500} } ```  ### Text Embedding (OpenAI) ```json {     \"provider\": \"openai\",     \"model\": \"text-embedding-3-large\",     \"inputs\": {\"text\": \"machine learning\"},     \"parameters\": {} } ```  ### Text Embedding (Google Vertex Multimodal - 1408D) ```json {     \"provider\": \"google\",     \"model\": \"multimodalembedding\",     \"inputs\": {\"text\": \"machine learning\"},     \"parameters\": {} } ```  ### Image Embedding (Google Vertex Multimodal - 1408D) ```json {     \"provider\": \"google\",     \"model\": \"multimodalembedding\",     \"inputs\": {\"image_url\": \"https://example.com/image.jpg\"},     \"parameters\": {} } ```  ### Image Embedding from Base64 ```json {     \"provider\": \"google\",     \"model\": \"multimodalembedding\",     \"inputs\": {\"image_base64\": \"<base64-encoded-image>\"},     \"parameters\": {} } ```  ### Video Embedding (Google Vertex Multimodal - 1408D) ```json {     \"provider\": \"google\",     \"model\": \"multimodalembedding\",     \"inputs\": {\"video_url\": \"https://example.com/video.mp4\"},     \"parameters\": {} } ```  ### Video Embedding from Base64 ```json {     \"provider\": \"google\",     \"model\": \"multimodalembedding\",     \"inputs\": {\"video_base64\": \"<base64-encoded-video>\"},     \"parameters\": {} } ```  ### Audio Transcription ```json {     \"provider\": \"openai\",     \"model\": \"whisper-1\",     \"inputs\": {\"audio_url\": \"https://example.com/audio.mp3\"},     \"parameters\": {} } ```  ### Vision (Multimodal LLM) ```json {     \"provider\": \"openai\",     \"model\": \"gpt-4o\",     \"inputs\": {         \"prompts\": [\"Describe this image\"],         \"image_url\": \"https://example.com/image.jpg\"     },     \"parameters\": {\"temperature\": 0.5} } ```  Args:     request: FastAPI request object (populated by middleware)     payload: Raw inference request  Returns:     Inference response with results and metadata  Raises:     400 Bad Request: Invalid provider, model, or inputs     401 Unauthorized: Missing or invalid API key     429 Too Many Requests: Rate limit exceeded     500 Internal Server Error: Inference execution failed

        :param raw_inference_request: (required)
        :type raw_inference_request: RawInferenceRequest
        :param authorization: REQUIRED: Bearer token authentication using your API key. Format: 'Bearer sk_xxxxxxxxxxxxx'. You can create API keys in the Mixpeek dashboard under Organization Settings.
        :type authorization: str
        :param x_namespace: REQUIRED: Namespace identifier for scoping this request. All resources (collections, buckets, taxonomies, etc.) are scoped to a namespace. You can provide either the namespace name or namespace ID. Format: ns_xxxxxxxxxxxxx (ID) or a custom name like 'my-namespace'
        :type x_namespace: str
        :param _request_timeout: timeout setting for this request. If one
                                 number provided, it will be total request
                                 timeout. It can also be a pair (tuple) of
                                 (connection, read) timeouts.
        :type _request_timeout: int, tuple(int, int), optional
        :param _request_auth: set to override the auth_settings for an a single
                              request; this effectively ignores the
                              authentication in the spec for a single request.
        :type _request_auth: dict, optional
        :param _content_type: force content-type for the request.
        :type _content_type: str, Optional
        :param _headers: set to override the headers for a single
                         request; this effectively ignores the headers
                         in the spec for a single request.
        :type _headers: dict, optional
        :param _host_index: set to override the host_index for a single
                            request; this effectively ignores the host_index
                            in the spec for a single request.
        :type _host_index: int, optional
        :return: Returns the result object.
        """ # noqa: E501

        _param = self._execute_raw_inference_serialize(
            raw_inference_request=raw_inference_request,
            authorization=authorization,
            x_namespace=x_namespace,
            _request_auth=_request_auth,
            _content_type=_content_type,
            _headers=_headers,
            _host_index=_host_index
        )

        _response_types_map: Dict[str, Optional[str]] = {
            '200': "RawInferenceResponse",
            '400': "ErrorResponse",
            '401': "ErrorResponse",
            '403': "ErrorResponse",
            '404': "ErrorResponse",
            '500': "ErrorResponse",
            '422': "HTTPValidationError",
        }
        response_data = self.api_client.call_api(
            *_param,
            _request_timeout=_request_timeout
        )
        return response_data.response


    def _execute_raw_inference_serialize(
        self,
        raw_inference_request,
        authorization,
        x_namespace,
        _request_auth,
        _content_type,
        _headers,
        _host_index,
    ) -> RequestSerialized:

        _host = None

        _collection_formats: Dict[str, str] = {
        }

        _path_params: Dict[str, str] = {}
        _query_params: List[Tuple[str, str]] = []
        _header_params: Dict[str, Optional[str]] = _headers or {}
        _form_params: List[Tuple[str, str]] = []
        _files: Dict[
            str, Union[str, bytes, List[str], List[bytes], List[Tuple[str, bytes]]]
        ] = {}
        _body_params: Optional[bytes] = None

        # process the path parameters
        # process the query parameters
        # process the header parameters
        if authorization is not None:
            _header_params['Authorization'] = authorization
        if x_namespace is not None:
            _header_params['X-Namespace'] = x_namespace
        # process the form parameters
        # process the body parameter
        if raw_inference_request is not None:
            _body_params = raw_inference_request


        # set the HTTP header `Accept`
        if 'Accept' not in _header_params:
            _header_params['Accept'] = self.api_client.select_header_accept(
                [
                    'application/json'
                ]
            )

        # set the HTTP header `Content-Type`
        if _content_type:
            _header_params['Content-Type'] = _content_type
        else:
            _default_content_type = (
                self.api_client.select_header_content_type(
                    [
                        'application/json'
                    ]
                )
            )
            if _default_content_type is not None:
                _header_params['Content-Type'] = _default_content_type

        # authentication setting
        _auth_settings: List[str] = [
        ]

        return self.api_client.param_serialize(
            method='POST',
            resource_path='/v1/inference',
            path_params=_path_params,
            query_params=_query_params,
            header_params=_header_params,
            body=_body_params,
            post_params=_form_params,
            files=_files,
            auth_settings=_auth_settings,
            collection_formats=_collection_formats,
            _host=_host,
            _request_auth=_request_auth
        )


