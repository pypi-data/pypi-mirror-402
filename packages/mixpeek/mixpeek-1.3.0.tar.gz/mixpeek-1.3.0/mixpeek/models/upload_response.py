# coding: utf-8

"""
    Mixpeek API

    This is the Mixpeek API, providing access to various endpoints for data processing and retrieval.

    The version of the OpenAPI document: 1.3.0
    Contact: info@mixpeek.com
    Generated by OpenAPI Generator (https://openapi-generator.tech)

    Do not edit the class manually.
"""  # noqa: E501


from __future__ import annotations
import pprint
import re  # noqa: F401
import json

from datetime import datetime
from pydantic import BaseModel, ConfigDict, Field, StrictBool, StrictInt, StrictStr
from typing import Any, ClassVar, Dict, List, Optional
from typing_extensions import Annotated
from mixpeek.models.task_status_enum import TaskStatusEnum
from typing import Optional, Set
from typing_extensions import Self

class UploadResponse(BaseModel):
    """
    Response containing presigned URL and upload tracking information.  This response includes everything needed to: 1. Upload your file to S3 using the presigned_url 2. Track the upload status using upload_id 3. Confirm the upload using the confirmation endpoint  The presigned_url is time-limited and specific to this upload. After uploading to S3, call POST /v1/buckets/{bucket_id}/uploads/{upload_id}/confirm.
    """ # noqa: E501
    upload_id: StrictStr = Field(description="Unique identifier for this upload. Auto-generated.   ⚠️  NEXT STEP: After uploading to S3, you MUST confirm:   POST /v1/uploads/{upload_id}/confirm   Other operations:   - Check status: GET /v1/uploads/{upload_id}   - Cancel upload: DELETE /v1/uploads/{upload_id}   Format: 'upl_' followed by 16 random characters.")
    bucket_id: StrictStr = Field(description="Target bucket ID where object will be created")
    filename: StrictStr = Field(description="Name of the file to upload")
    content_type: StrictStr = Field(description="MIME type enforced by the presigned URL")
    file_size_bytes: Optional[StrictInt] = Field(default=None, description="Expected file size in bytes if provided in request. Will be validated during confirmation.")
    presigned_url: Optional[Annotated[str, Field(min_length=1, strict=True, max_length=2083)]] = Field(default=None, description="Time-limited HTTPS URL for uploading directly to S3.   **Step 1 - Upload to S3:**   curl -X PUT '{presigned_url}' -H 'Content-Type: {content_type}' --upload-file {filename}   **Step 2 - REQUIRED: Confirm the upload:**   POST /v1/uploads/{upload_id}/confirm   (S3 has no callback - you MUST call confirm to finalize)   The URL includes authentication and expires after presigned_url_expiration seconds. S3 returns an ETag header on success - pass it to confirm for integrity validation. NOTE: This will be null if is_duplicate=true (duplicate found, no upload needed).")
    presigned_url_expiration: StrictInt = Field(description="How long the presigned URL is valid, in seconds")
    s3_key: StrictStr = Field(description="Full S3 object key where the file will be stored. Format: {internal_id}/{namespace_id}/api_buckets_uploads_create/{upload_id}/{filename}. Used internally for verification and object creation.")
    status: TaskStatusEnum = Field(description="Current upload status. After creation, always PENDING. Possible statuses: PENDING → IN_PROGRESS → PROCESSING → COMPLETED/FAILED/CANCELED")
    metadata: Optional[Dict[str, Any]] = Field(default=None, description="Custom metadata for tracking")
    create_object_on_confirm: StrictBool = Field(description="Whether bucket object will be auto-created on confirmation")
    object_metadata: Optional[Dict[str, Any]] = Field(default=None, description="Metadata for the bucket object (if create_object_on_confirm=true)")
    blob_property: Optional[StrictStr] = Field(default=None, description="Property name for the blob in bucket object")
    blob_type: Optional[StrictStr] = Field(default=None, description="Type of blob (IMAGE, VIDEO, etc.)")
    file_hash: Optional[StrictStr] = Field(default=None, description="SHA256 hash of the file content. Set during confirmation from S3 metadata or provided in request. Used for duplicate detection.")
    skip_duplicates: Optional[StrictBool] = Field(default=True, description="Whether duplicate detection was enabled for this upload")
    is_duplicate: Optional[StrictBool] = Field(default=False, description="Whether this upload was identified as a duplicate of an existing file. If true:   - duplicate_of_upload_id contains the original upload   - presigned_url will be null (no upload needed)   - You can use the original upload's S3 object. This saves bandwidth and storage costs.")
    duplicate_of_upload_id: Optional[StrictStr] = Field(default=None, description="If skip_duplicates=true and duplicate found, this is the original upload_id. The response will reference the existing upload instead of creating a new one.")
    skipped_unique_key: Optional[StrictBool] = Field(default=False, description="Whether this upload was skipped because the unique key already exists in the bucket. If true:   - existing_object_id contains the ID of the existing object   - presigned_url will be null (no upload needed)   - No S3 upload is required This saves bandwidth and prevents duplicate objects.")
    existing_object_id: Optional[StrictStr] = Field(default=None, description="If skipped_unique_key=true, this is the object_id of the existing object that has the same unique key values. The upload was skipped to prevent duplicates.")
    message: Optional[StrictStr] = Field(default=None, description="Human-readable message about the upload. Provided when is_duplicate=true or other special conditions. Example: 'File already exists with the same content hash. No upload needed - returning existing upload.'")
    created_at: datetime = Field(description="When this upload record was created (ISO 8601 format)")
    expires_at: datetime = Field(description="When the presigned URL expires (ISO 8601 format). After this time:   - The presigned URL cannot be used   - Upload status will be marked as FAILED if not completed   - The upload record will be auto-deleted 30 days later (MongoDB TTL)")
    completed_at: Optional[datetime] = Field(default=None, description="When the upload was completed and verified (ISO 8601 format)")
    verified_at: Optional[datetime] = Field(default=None, description="When S3 object existence was verified (ISO 8601 format)")
    etag: Optional[StrictStr] = Field(default=None, description="S3 ETag from the uploaded object (set during confirmation)")
    object_id: Optional[StrictStr] = Field(default=None, description="Created bucket object ID (if create_object_on_confirm was true)")
    task_id: Optional[StrictStr] = Field(default=None, description="Celery task ID for async confirmation (if processed asynchronously)")
    __properties: ClassVar[List[str]] = ["upload_id", "bucket_id", "filename", "content_type", "file_size_bytes", "presigned_url", "presigned_url_expiration", "s3_key", "status", "metadata", "create_object_on_confirm", "object_metadata", "blob_property", "blob_type", "file_hash", "skip_duplicates", "is_duplicate", "duplicate_of_upload_id", "skipped_unique_key", "existing_object_id", "message", "created_at", "expires_at", "completed_at", "verified_at", "etag", "object_id", "task_id"]

    model_config = ConfigDict(
        populate_by_name=True,
        validate_assignment=True,
        protected_namespaces=(),
    )


    def to_str(self) -> str:
        """Returns the string representation of the model using alias"""
        return pprint.pformat(self.model_dump(by_alias=True))

    def to_json(self) -> str:
        """Returns the JSON representation of the model using alias"""
        # TODO: pydantic v2: use .model_dump_json(by_alias=True, exclude_unset=True) instead
        return json.dumps(self.to_dict())

    @classmethod
    def from_json(cls, json_str: str) -> Optional[Self]:
        """Create an instance of UploadResponse from a JSON string"""
        return cls.from_dict(json.loads(json_str))

    def to_dict(self) -> Dict[str, Any]:
        """Return the dictionary representation of the model using alias.

        This has the following differences from calling pydantic's
        `self.model_dump(by_alias=True)`:

        * `None` is only added to the output dict for nullable fields that
          were set at model initialization. Other fields with value `None`
          are ignored.
        """
        excluded_fields: Set[str] = set([
        ])

        _dict = self.model_dump(
            by_alias=True,
            exclude=excluded_fields,
            exclude_none=True,
        )
        return _dict

    @classmethod
    def from_dict(cls, obj: Optional[Dict[str, Any]]) -> Optional[Self]:
        """Create an instance of UploadResponse from a dict"""
        if obj is None:
            return None

        if not isinstance(obj, dict):
            return cls.model_validate(obj)

        _obj = cls.model_validate({
            "upload_id": obj.get("upload_id"),
            "bucket_id": obj.get("bucket_id"),
            "filename": obj.get("filename"),
            "content_type": obj.get("content_type"),
            "file_size_bytes": obj.get("file_size_bytes"),
            "presigned_url": obj.get("presigned_url"),
            "presigned_url_expiration": obj.get("presigned_url_expiration"),
            "s3_key": obj.get("s3_key"),
            "status": obj.get("status"),
            "metadata": obj.get("metadata"),
            "create_object_on_confirm": obj.get("create_object_on_confirm"),
            "object_metadata": obj.get("object_metadata"),
            "blob_property": obj.get("blob_property"),
            "blob_type": obj.get("blob_type"),
            "file_hash": obj.get("file_hash"),
            "skip_duplicates": obj.get("skip_duplicates") if obj.get("skip_duplicates") is not None else True,
            "is_duplicate": obj.get("is_duplicate") if obj.get("is_duplicate") is not None else False,
            "duplicate_of_upload_id": obj.get("duplicate_of_upload_id"),
            "skipped_unique_key": obj.get("skipped_unique_key") if obj.get("skipped_unique_key") is not None else False,
            "existing_object_id": obj.get("existing_object_id"),
            "message": obj.get("message"),
            "created_at": obj.get("created_at"),
            "expires_at": obj.get("expires_at"),
            "completed_at": obj.get("completed_at"),
            "verified_at": obj.get("verified_at"),
            "etag": obj.get("etag"),
            "object_id": obj.get("object_id"),
            "task_id": obj.get("task_id")
        })
        return _obj


