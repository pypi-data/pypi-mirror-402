# coding: utf-8

"""
    Mixpeek API

    This is the Mixpeek API, providing access to various endpoints for data processing and retrieval.

    The version of the OpenAPI document: 1.3.0
    Contact: info@mixpeek.com
    Generated by OpenAPI Generator (https://openapi-generator.tech)

    Do not edit the class manually.
"""  # noqa: E501


from __future__ import annotations
import pprint
import re  # noqa: F401
import json

from pydantic import BaseModel, ConfigDict, Field, StrictBool, StrictFloat, StrictInt, StrictStr, field_validator
from typing import Any, ClassVar, Dict, List, Optional, Union
from mixpeek.models.generation_config import GenerationConfig
from mixpeek.models.response_shape1 import ResponseShape1
from mixpeek.models.split_method import SplitMethod
from typing import Optional, Set
from typing_extensions import Self

class MultimodalExtractorParams(BaseModel):
    """
    Parameters for the multimodal extractor.  The multimodal extractor processes video, audio, image, text, and GIF content in a unified embedding space. Videos/GIFs/Audio are decomposed into segments with transcription, visual analysis (video only), OCR, and embeddings. Images and text are embedded directly without decomposition.  **When to Use**:     - Video content libraries requiring searchable segments     - Audio content (podcasts, lectures, music) requiring transcription and search     - Media platforms with search across spoken and visual content     - Educational content with lecture videos and demonstrations     - Surveillance/security footage requiring event detection     - Social media platforms with user-generated video content     - Broadcasting/streaming services with large video catalogs     - Training video repositories with instructional content     - Marketing/advertising analytics for video campaigns  **When NOT to Use**:     - Static image collections → Use image_extractor instead     - Very short videos (<5 seconds) → Overhead not worth it     - Real-time live streams → Use specialized streaming extractors     - Extremely high-resolution videos (8K+) → Consider downsampling first  **Decomposition Methods**:      | Method | Use Case | Accuracy | Segments/Min | Best For |     |--------|----------|----------|--------------|----------|     | **TIME** | Fixed intervals | N/A | 60/interval_sec | General purpose, audio/video chunking |     | **SCENE** | Visual changes | 85-90% | Variable (2-20) | Movies, dynamic content (video only) |     | **SILENCE** | Audio pauses | 80-85% | Variable (5-30) | Lectures, presentations, audio/video |  **Feature Extraction Options**:     - Transcription: Speech-to-text using Whisper (95%+ accuracy)     - Multimodal Embeddings: Unified embeddings from Vertex AI (1408D) for video/image/gif/text     - Transcription Embeddings: Text embeddings from E5-Large (1024D)     - OCR: Text extraction from video frames using Gemini Vision     - Descriptions: AI-generated segment summaries using Gemini     - Thumbnails: Visual preview images for each segment  **Performance Characteristics**:     - Processing Speed: 0.5-2x realtime (depends on features enabled)     - Example: 10min video → 5-20 minutes processing time     - Transcription: ~200ms per second of audio     - Visual Embedding: ~50ms per segment     - OCR: ~300ms per segment     - Description: ~2s per segment (if enabled)  Requirements:     - video URL: REQUIRED (accessible video file)     - All feature parameters: OPTIONAL (defaults provided)
    """ # noqa: E501
    extractor_type: Optional[StrictStr] = Field(default='multimodal_extractor', description="Discriminator field for parameter type identification. Must be 'multimodal_extractor'.")
    split_method: Optional[SplitMethod] = Field(default=None, description="The PRIMARY control for video splitting strategy. This determines which splitting method is used.")
    description_prompt: Optional[StrictStr] = Field(default='Describe the video segment in detail.', description="The prompt to use for description generation.")
    time_split_interval: Optional[StrictInt] = Field(default=10, description="Interval in seconds for 'time' splitting. Used when split_method='time'.")
    silence_db_threshold: Optional[StrictInt] = Field(default=None, description="The decibel level below which audio is considered silent. Used when split_method='silence'. Recommended value: -40 (auto-applied if not specified). Lower values (e.g., -50) detect more silence, higher values (e.g., -30) detect less.")
    scene_detection_threshold: Optional[Union[StrictFloat, StrictInt]] = Field(default=None, description="The threshold for scene detection (0.0-1.0). Used when split_method='scene'. Recommended value: 0.5 (auto-applied if not specified). Lower values (e.g., 0.3) detect more scenes, higher values (e.g., 0.7) detect fewer scenes.")
    run_transcription: Optional[StrictBool] = Field(default=False, description="Whether to run transcription on video segments.")
    transcription_language: Optional[StrictStr] = Field(default='en', description="The language of the transcription. Used when run_transcription is True.")
    run_video_description: Optional[StrictBool] = Field(default=False, description="Whether to generate descriptions for video segments. OPTIMIZED: Defaults to False as descriptions add 1-2 minutes. Enable only when needed.")
    run_transcription_embedding: Optional[StrictBool] = Field(default=False, description="Whether to generate embeddings for transcriptions. Useful for semantic search over spoken content.")
    run_multimodal_embedding: Optional[StrictBool] = Field(default=True, description="Whether to generate multimodal embeddings for all content types (video/image/gif/text). Uses Google Vertex AI to create unified 1408D embeddings in a shared semantic space. Useful for cross-modal semantic search across all media types.")
    run_ocr: Optional[StrictBool] = Field(default=False, description="Whether to run OCR to extract text from video frames. OPTIMIZED: Defaults to False as OCR adds significant processing time. Enable only when text extraction from video is required.")
    sensitivity: Optional[StrictStr] = Field(default='low', description="The sensitivity of the scene detection.")
    enable_thumbnails: Optional[StrictBool] = Field(default=True, description="Whether to generate thumbnail images for video segments and images. Thumbnails provide visual previews for navigation and UI display. For videos: Extracts a frame from each segment. For images: Creates an optimized thumbnail version. ")
    use_cdn: Optional[StrictBool] = Field(default=False, description="Whether to use CloudFront CDN for thumbnail delivery. When True: Uploads to public bucket and returns CloudFront URLs. When False (default): Uploads to private bucket with presigned S3 URLs. Benefits of CDN: faster global delivery, permanent URLs, reduced bandwidth costs. Requires CLOUDFRONT_PUBLIC_DOMAIN to be configured in settings. Only applies when enable_thumbnails=True.")
    generation_config: Optional[GenerationConfig] = None
    response_shape: Optional[ResponseShape1] = None
    __properties: ClassVar[List[str]] = ["extractor_type", "split_method", "description_prompt", "time_split_interval", "silence_db_threshold", "scene_detection_threshold", "run_transcription", "transcription_language", "run_video_description", "run_transcription_embedding", "run_multimodal_embedding", "run_ocr", "sensitivity", "enable_thumbnails", "use_cdn", "generation_config", "response_shape"]

    @field_validator('extractor_type')
    def extractor_type_validate_enum(cls, value):
        """Validates the enum"""
        if value is None:
            return value

        if value not in set(['multimodal_extractor']):
            raise ValueError("must be one of enum values ('multimodal_extractor')")
        return value

    model_config = ConfigDict(
        populate_by_name=True,
        validate_assignment=True,
        protected_namespaces=(),
    )


    def to_str(self) -> str:
        """Returns the string representation of the model using alias"""
        return pprint.pformat(self.model_dump(by_alias=True))

    def to_json(self) -> str:
        """Returns the JSON representation of the model using alias"""
        # TODO: pydantic v2: use .model_dump_json(by_alias=True, exclude_unset=True) instead
        return json.dumps(self.to_dict())

    @classmethod
    def from_json(cls, json_str: str) -> Optional[Self]:
        """Create an instance of MultimodalExtractorParams from a JSON string"""
        return cls.from_dict(json.loads(json_str))

    def to_dict(self) -> Dict[str, Any]:
        """Return the dictionary representation of the model using alias.

        This has the following differences from calling pydantic's
        `self.model_dump(by_alias=True)`:

        * `None` is only added to the output dict for nullable fields that
          were set at model initialization. Other fields with value `None`
          are ignored.
        """
        excluded_fields: Set[str] = set([
        ])

        _dict = self.model_dump(
            by_alias=True,
            exclude=excluded_fields,
            exclude_none=True,
        )
        # override the default output from pydantic by calling `to_dict()` of generation_config
        if self.generation_config:
            _dict['generation_config'] = self.generation_config.to_dict()
        # override the default output from pydantic by calling `to_dict()` of response_shape
        if self.response_shape:
            _dict['response_shape'] = self.response_shape.to_dict()
        return _dict

    @classmethod
    def from_dict(cls, obj: Optional[Dict[str, Any]]) -> Optional[Self]:
        """Create an instance of MultimodalExtractorParams from a dict"""
        if obj is None:
            return None

        if not isinstance(obj, dict):
            return cls.model_validate(obj)

        _obj = cls.model_validate({
            "extractor_type": obj.get("extractor_type") if obj.get("extractor_type") is not None else 'multimodal_extractor',
            "split_method": obj.get("split_method"),
            "description_prompt": obj.get("description_prompt") if obj.get("description_prompt") is not None else 'Describe the video segment in detail.',
            "time_split_interval": obj.get("time_split_interval") if obj.get("time_split_interval") is not None else 10,
            "silence_db_threshold": obj.get("silence_db_threshold"),
            "scene_detection_threshold": obj.get("scene_detection_threshold"),
            "run_transcription": obj.get("run_transcription") if obj.get("run_transcription") is not None else False,
            "transcription_language": obj.get("transcription_language") if obj.get("transcription_language") is not None else 'en',
            "run_video_description": obj.get("run_video_description") if obj.get("run_video_description") is not None else False,
            "run_transcription_embedding": obj.get("run_transcription_embedding") if obj.get("run_transcription_embedding") is not None else False,
            "run_multimodal_embedding": obj.get("run_multimodal_embedding") if obj.get("run_multimodal_embedding") is not None else True,
            "run_ocr": obj.get("run_ocr") if obj.get("run_ocr") is not None else False,
            "sensitivity": obj.get("sensitivity") if obj.get("sensitivity") is not None else 'low',
            "enable_thumbnails": obj.get("enable_thumbnails") if obj.get("enable_thumbnails") is not None else True,
            "use_cdn": obj.get("use_cdn") if obj.get("use_cdn") is not None else False,
            "generation_config": GenerationConfig.from_dict(obj["generation_config"]) if obj.get("generation_config") is not None else None,
            "response_shape": ResponseShape1.from_dict(obj["response_shape"]) if obj.get("response_shape") is not None else None
        })
        return _obj


