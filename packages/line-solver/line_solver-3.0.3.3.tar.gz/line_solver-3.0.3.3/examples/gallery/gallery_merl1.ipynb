{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gallery Example: M/E₂/1 Queue (Erlang Service)\n",
    "\n",
    "This example demonstrates an M/E₂/1 queueing system:\n",
    "- **Arrivals**: Poisson process (Exponential inter-arrival times)\n",
    "- **Service**: Erlang-2 service times (less variable than exponential)\n",
    "- **Servers**: 1 server\n",
    "- **Capacity**: Infinite\n",
    "- **Scheduling**: FCFS\n",
    "\n",
    "The Erlang-2 service distribution has lower variance than exponential, representing more consistent service times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from line_solver import *\n",
    "import numpy as np\n",
    "GlobalConstants.set_verbose(VerboseLevel.STD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gallery_merl1():",
    "    \"\"\"Create M/E₂/1 queueing model\"\"\"",
    "    model = Network('M/E/1')",
    "    ",
    "    # Block 1: nodes",
    "    source = Source(model, 'mySource')",
    "    queue = Queue(model, 'myQueue', SchedStrategy.FCFS)",
    "    sink = Sink(model, 'mySink')",
    "    ",
    "    # Block 2: classes",
    "    oclass = OpenClass(model, 'myClass')",
    "    # Exponential arrivals with rate λ=1",
    "    source.set_arrival(oclass, Exp(1))",
    "    # Erlang-2 service with mean=0.5 and order=2",
    "    queue.set_service(oclass, Erlang.fit_mean_and_order(0.5, 2))",
    "    ",
    "    # Block 3: topology",
    "    P = model.init_routing_matrix()",
    "    P.add_route(oclass, source, queue, 1.0)",
    "    P.add_route(oclass, queue, sink, 1.0)",
    "    model.link(P)",
    "    ",
    "    return model, source, queue, sink, oclass",
    "",
    "# Create the model",
    "model, source, queue, sink, oclass = gallery_merl1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theoretical Analysis for M/E₂/1\n",
    "\n",
    "For M/E₂/1 with:\n",
    "- **Arrival rate**: λ = 1 (Exponential with mean=1)\n",
    "- **Service time**: Erlang-2 with mean=0.5, order=2\n",
    "- **Utilization**: ρ = λ × E[S] = 1 × 0.5 = 0.5\n",
    "\n",
    "The Erlang-2 service distribution characteristics:\n",
    "- **Mean**: 0.5\n",
    "- **Variance**: (0.5)²/2 = 0.125 (lower than exponential variance = 0.25)\n",
    "- **Coefficient of Variation**: C²ₛ = 1/2 = 0.5\n",
    "\n",
    "Lower service variability typically leads to better performance than M/M/1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve with multiple solvers\n",
    "print(\"\\n=== Solver Results ===\")\n",
    "\n",
    "# MVA Solver\n",
    "solver_mva = MVA(model)\n",
    "avg_table_mva = solver_mva.avg_table()\n",
    "print(\"\\nMVA Solver:\")\n",
    "print(avg_table_mva)\n",
    "\n",
    "# CTMC Solver\n",
    "solver_ctmc = CTMC(model, cutoff=15)\n",
    "avg_table_ctmc = solver_ctmc.avg_table()\n",
    "print(\"\\nCTMC Solver:\")\n",
    "print(avg_table_ctmc)\n",
    "\n",
    "# Fluid Solver\n",
    "solver_fluid = FLD(model)\n",
    "avg_table_fluid = solver_fluid.avg_table()\n",
    "print(\"\\nFluid Solver:\")\n",
    "print(avg_table_fluid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare M/E₂/1 with M/M/1\n",
    "print(\"\\n=== Comparison with M/M/1 ===\")\n",
    "\n",
    "# Create equivalent M/M/1 model\n",
    "def create_mm1_equivalent():\n",
    "    model_mm1 = Network('M/M/1-Equivalent')\n",
    "    source = Source(model_mm1, 'Source')\n",
    "    queue = Queue(model_mm1, 'Queue', SchedStrategy.FCFS)\n",
    "    sink = Sink(model_mm1, 'Sink')\n",
    "    \n",
    "    oclass = OpenClass(model_mm1, 'Class')\n",
    "    source.set_arrival(oclass, Exp(1))  # Same arrival rate\n",
    "    queue.set_service(oclass, Exp(2))   # Exponential with mean = 0.5\n",
    "    \n",
    "    P = model_mm1.init_routing_matrix()\n",
    "    P.add_route(oclass, source, queue, 1.0)\n",
    "    P.add_route(oclass, queue, sink, 1.0)\n",
    "    model_mm1.link(P)\n",
    "    \n",
    "    return model_mm1\n",
    "\n",
    "model_mm1 = create_mm1_equivalent()\n",
    "solver_merl = MVA(model)\n",
    "solver_mm1 = MVA(model_mm1)\n",
    "\n",
    "avg_table_merl = solver_merl.avg_table()\n",
    "avg_table_mm1 = solver_mm1.avg_table()\n",
    "\n",
    "print(\"M/E₂/1 Results:\")\n",
    "print(avg_table_merl)\n",
    "\n",
    "print(\"\\nM/M/1 Results:\")\n",
    "print(avg_table_mm1)\n",
    "\n",
    "# Extract key metrics for comparison\n",
    "merl_resp = float(avg_table_merl.iloc[1, 2])  # Queue response time\n",
    "merl_length = float(avg_table_merl.iloc[1, 3])  # Queue length\n",
    "\n",
    "mm1_resp = float(avg_table_mm1.iloc[1, 2])  # Queue response time\n",
    "mm1_length = float(avg_table_mm1.iloc[1, 3])  # Queue length\n",
    "\n",
    "print(f\"\\nPerformance Comparison:\")\n",
    "print(f\"Response Time: M/E₂/1={merl_resp:.4f}, M/M/1={mm1_resp:.4f}\")\n",
    "print(f\"Queue Length: M/E₂/1={merl_length:.4f}, M/M/1={mm1_length:.4f}\")\n",
    "improvement = ((mm1_resp - merl_resp) / mm1_resp * 100) if mm1_resp > merl_resp else ((merl_resp - mm1_resp) / mm1_resp * 100)\n",
    "print(f\"Improvement: {improvement:.1f}% {'better' if mm1_resp > merl_resp else 'worse'} response time with Erlang service\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze impact of Erlang order for service times\n",
    "print(\"\\n=== Impact of Service Time Variability ===\")\n",
    "\n",
    "def create_service_erlang_model(order):\n",
    "    \"\"\"Create M/Eₖ/1 model with specified Erlang order for service\"\"\"\n",
    "    model_erl = Network(f'M/E{order}/1')\n",
    "    source = Source(model_erl, 'Source')\n",
    "    queue = Queue(model_erl, 'Queue', SchedStrategy.FCFS)\n",
    "    sink = Sink(model_erl, 'Sink')\n",
    "    \n",
    "    oclass = OpenClass(model_erl, 'Class')\n",
    "    source.set_arrival(oclass, Exp(1))\n",
    "    queue.set_service(oclass, Erlang.fit_mean_and_order(0.5, order))\n",
    "    \n",
    "    P = model_erl.init_routing_matrix()\n",
    "    P.add_route(oclass, source, queue, 1.0)\n",
    "    P.add_route(oclass, queue, sink, 1.0)\n",
    "    model_erl.link(P)\n",
    "    \n",
    "    return model_erl\n",
    "\n",
    "# Test different Erlang orders for service\n",
    "orders = [1, 2, 3, 5, 10]  # Order 1 = Exponential\n",
    "\n",
    "print(\"Service Order | C²ₛ  | Response Time | Queue Length\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for order in orders:\n",
    "    model_order = create_service_erlang_model(order)\n",
    "    solver = MVA(model_order)\n",
    "    avg_table = solver.avg_table()\n",
    "    \n",
    "    resp_time = float(avg_table.iloc[1, 2])\n",
    "    queue_length = float(avg_table.iloc[1, 3])\n",
    "    cv_squared = 1.0 / order  # Coefficient of variation squared for Erlang\n",
    "    \n",
    "    print(f\"     {order:2d}       | {cv_squared:.3f} |     {resp_time:.4f}    |    {queue_length:.4f}\")\n",
    "\n",
    "print(\"\\nNote: Higher order (lower C²ₛ) leads to better performance due to reduced service variability.\")\n",
    "print(\"As order→∞, the system approaches M/D/1 (deterministic service).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with M/H₂/1 (hyperexponential service)\n",
    "print(\"\\n=== Comparison: Low vs High Service Variability ===\")\n",
    "\n",
    "def create_mhyp1_model():\n",
    "    \"\"\"Create M/H₂/1 model with hyperexponential service\"\"\"\n",
    "    model_hyp = Network('M/H2/1')\n",
    "    source = Source(model_hyp, 'Source')\n",
    "    queue = Queue(model_hyp, 'Queue', SchedStrategy.FCFS)\n",
    "    sink = Sink(model_hyp, 'Sink')\n",
    "    \n",
    "    oclass = OpenClass(model_hyp, 'Class')\n",
    "    source.set_arrival(oclass, Exp(1))\n",
    "    queue.set_service(oclass, HyperExp.fit_mean_and_scv(0.5, 4))  # High variability\n",
    "    \n",
    "    P = model_hyp.init_routing_matrix()\n",
    "    P.add_route(oclass, source, queue, 1.0)\n",
    "    P.add_route(oclass, queue, sink, 1.0)\n",
    "    model_hyp.link(P)\n",
    "    \n",
    "    return model_hyp\n",
    "\n",
    "model_hyp = create_mhyp1_model()\n",
    "model_erl = create_service_erlang_model(5)  # Low variability\n",
    "\n",
    "solver_hyp = MVA(model_hyp)\n",
    "solver_erl = MVA(model_erl)\n",
    "\n",
    "avg_table_hyp = solver_hyp.avg_table()\n",
    "avg_table_erl = solver_erl.avg_table()\n",
    "\n",
    "hyp_resp = float(avg_table_hyp.iloc[1, 2])\n",
    "erl_resp = float(avg_table_erl.iloc[1, 2])\n",
    "\n",
    "print(f\"Service Distribution | C²ₛ | Response Time\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"Erlang-5 (low var)   | 0.2 |    {erl_resp:.4f}\")\n",
    "print(f\"Exponential          | 1.0 |    {mm1_resp:.4f}\")\n",
    "print(f\"Hyperexp (high var)  | 4.0 |    {hyp_resp:.4f}\")\n",
    "\n",
    "print(f\"\\nVariability Impact: {(hyp_resp / erl_resp):.1f}x difference between high and low service variability\")\n",
    "print(\"This demonstrates the fundamental principle: variability degrades queueing performance.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
