import logging

import tables
from fastapi import APIRouter, HTTPException, UploadFile
from fastapi.params import Depends
from fastapi.responses import FileResponse, StreamingResponse
import os
from datetime import datetime
import json
import asyncio
from typing import Annotated, AsyncGenerator

from mytoolit.measurement import Storage
from starlette.responses import PlainTextResponse
from tables import NoSuchNodeError, Node, Table

from icoapi.models.globals import get_trident_client
from icoapi.models.models import Dataset, DiskCapacity, FileCloudDetails, FileListResponseModel, HDF5NodeInfo, \
    MeasurementFileDetails, \
    Metadata, MetadataPrefix, ParsedHDF5FileContent, ParsedMeasurement, \
    ParsedMetadata, Sensor, TridentBucketObject
from icoapi.models.trident import StorageClient
from icoapi.scripts.errors import HTTP_404_FILE_NOT_FOUND_EXCEPTION, HTTP_404_FILE_NOT_FOUND_SPEC
from icoapi.scripts.file_handling import get_disk_space_in_gb, get_drive_or_root_path, get_measurement_dir, \
    get_suffixed_filename, is_dangerous_filename
import pandas as pd

from icoapi.scripts.measurement import write_metadata

router = APIRouter(
    prefix="/files",
    tags=["File Handling"]
)

logger = logging.getLogger(__name__)

@router.get("")
async def list_files_and_capacity(
        measurement_dir: str = Depends(get_measurement_dir),
        storage: StorageClient = Depends(get_trident_client)
) -> FileListResponseModel:
    try:
        capacity = get_disk_space_in_gb(get_drive_or_root_path())
        files_info: list[MeasurementFileDetails] = []
        cloud_files: list[TridentBucketObject] = []
        if storage is not None:
            try:
                objects = storage.get_bucket_objects()
                cloud_files = [TridentBucketObject(**obj) for obj in objects]
            except HTTPException as e:
                logger.error(f"Error listing cloud files")
            except Exception as e:
                logger.error(f"General exception when comparing files to cloud: {e}")
        # Iterate over files in the directory
        for filename in os.listdir(measurement_dir):
            file_path = os.path.join(measurement_dir, filename)
            if os.path.isfile(file_path):
                # Get file creation time and size
                creation_time = datetime.fromtimestamp(os.path.getctime(file_path)).isoformat()
                file_size = os.path.getsize(file_path)
                cloud_details = FileCloudDetails(
                    is_uploaded=False,
                    upload_timestamp=None
                )
                if storage is not None:
                    matches = [file for file in cloud_files if filename in file.Key]
                    if matches:
                        cloud_details.is_uploaded = True
                        cloud_details.upload_timestamp = matches[0].LastModified

                details = MeasurementFileDetails(
                    name=filename,
                    size=file_size,
                    created=creation_time,
                    cloud=cloud_details
                )
                files_info.append(details)
        return FileListResponseModel(capacity, files_info, measurement_dir)
    except FileNotFoundError:
        raise HTTPException(status_code=404, detail="Directory not found")
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@router.get("/{name}")
async def download_file(name: str, measurement_dir: str = Depends(get_measurement_dir)):

    # Sanitization
    danger, cause = is_dangerous_filename(name)
    if danger:
        raise HTTPException(status_code=405, detail=f"Method not allowed: {cause}")

    full_path = os.path.join(measurement_dir, name)
    if os.path.isfile(full_path):
        return FileResponse(path=full_path, filename=name)
    else:
        raise HTTPException(status_code=404, detail="File not found")

@router.delete("/{name}")
async def delete_file(name: str, measurement_dir: str = Depends(get_measurement_dir)):

    # Sanitization
    danger, cause = is_dangerous_filename(name)
    if danger:
        raise HTTPException(status_code=405, detail=f"Method not allowed: {cause}")

    full_path = os.path.join(measurement_dir, name)
    if os.path.isfile(full_path):
        try:
            os.remove(full_path)
            return {"detail": f"File '{name}' deleted successfully"}
        except Exception as e:
            raise HTTPException(status_code=500, detail=f"Failed to delete file: {str(e)}")
    else:
        raise HTTPException(status_code=404, detail="File not found")


@router.get("/analyze/{name}", response_model=ParsedMeasurement)
async def get_analyzed_file(name: str, measurement_dir: str = Depends(get_measurement_dir)) -> StreamingResponse:

    danger, cause = is_dangerous_filename(name)
    if danger:
        raise HTTPException(status_code=405, detail=f"Method not allowed: {cause}")

    file_path = os.path.join(measurement_dir, name)
    if not os.path.isfile(file_path):
        raise HTTPException(status_code=404, detail="File not found")

    parsed_file_content = get_file_data(file_path)

    # Total number of rows for progress tracking
    total_rows = len(parsed_file_content.acceleration_df)

    # Streaming generator function
    # We approach this as a StreamingResponse because reading, parsing and sending the complete dataset
    # takes forever
    async def data_generator() -> AsyncGenerator[str, None]:
        # First: yield metadata
        sensors_raw = parsed_file_content.sensor_df.to_dict(orient="records")
        for sensor_raw in sensors_raw:
            if not "dimension" in sensor_raw:
                sensor_raw["dimension"] = ""
        sensors: list[Sensor] = [Sensor(**sensor) for sensor in sensors_raw]
        yield ParsedMetadata(
            acceleration=parsed_file_content.acceleration_meta,
            pictures=parsed_file_content.pictures,
            sensors=sensors
        ).model_dump_json() + "\n"

        # Then: yield measurement data
        batch_size = 1000
        parsed_rows = 0

        for start in range(0, total_rows, batch_size):
            end = min(start + batch_size, total_rows)
            batch = parsed_file_content.acceleration_df.iloc[start:end:10]
            batch_counter = batch["counter"].tolist()
            batch_timestamp = batch["timestamp"].tolist()
            datasets = batch.drop(columns=["counter", "timestamp"])

            batch_dict = ParsedMeasurement(
                name=name,
                counter=batch_counter,
                timestamp=batch_timestamp,
                datasets=[Dataset(name=column, data=batch[column].tolist()) for column in datasets.columns],
            )

            # Serialize the batch as JSON and yield it
            yield batch_dict.model_dump_json() + "\n"

            # Update progress
            parsed_rows += len(batch) * 10
            progress = parsed_rows / total_rows
            yield json.dumps({"progress": progress}) + "\n"

            # Simulate async behavior to avoid blocking
            await asyncio.sleep(0.01)

        # Final completion progress
        yield json.dumps({"progress": 1.0}) + "\n"

    return StreamingResponse(data_generator(), media_type="application/json")


@router.post("/analyze")
async def post_analyzed_file(file: UploadFile, measurement_dir: str = Depends(get_measurement_dir)) -> PlainTextResponse:

    filename = get_suffixed_filename(file.filename, measurement_dir)

    file_path = os.path.join(measurement_dir, filename)

    with open(file_path, "wb") as f:
        f.write(file.file.read())

    return PlainTextResponse(filename)


@router.get("/analyze/meta/{name}")
async def get_file_meta(name: str, measurement_dir: str = Depends(get_measurement_dir)) -> ParsedMetadata:
    data = get_file_data(os.path.join(measurement_dir, name))
    return ParsedMetadata(
        acceleration=data.acceleration_meta,
        pictures=data.pictures,
        sensors=[Sensor(**sensor) for sensor in data.sensor_df.to_dict(orient="records")]
    )


@router.post("/post_meta/{name}", responses={
    200: { "description": "Metadata successfully overwritten" },
    404: HTTP_404_FILE_NOT_FOUND_SPEC
})
async def overwrite_post_meta(name: str, metadata: Metadata, measurement_dir: str = Depends(get_measurement_dir)):
    file_path = os.path.join(measurement_dir, name)
    if not os.path.isfile(file_path):
        raise HTTP_404_FILE_NOT_FOUND_EXCEPTION

    # we have the file and the metadata object
    with Storage(file_path) as storage:
        try:
            node: Node = storage.hdf.get_node("/acceleration")
            del node.attrs["post_metadata"]
        except NoSuchNodeError:
            raise HTTPException(status_code=500, detail="Acceleration data not found in the file")
        write_metadata(MetadataPrefix.POST, metadata, storage)


@router.post("/pre_meta/{name}", responses={
    200: { "description": "Metadata successfully overwritten" },
    404: HTTP_404_FILE_NOT_FOUND_SPEC
})
async def overwrite_pre_meta(name: str, metadata: Metadata, measurement_dir: str = Depends(get_measurement_dir)):
    file_path = os.path.join(measurement_dir, name)
    if not os.path.isfile(file_path):
        raise HTTP_404_FILE_NOT_FOUND_EXCEPTION

    # we have the file and the metadata object
    with Storage(file_path) as storage:
        try:
            node: Node = storage.hdf.get_node("/acceleration")
            del node.attrs["pre_metadata"]
        except NoSuchNodeError:
            raise HTTPException(status_code=500, detail="Acceleration data not found in the file")
        write_metadata(MetadataPrefix.PRE, metadata, storage)


def get_node_names(hdf5_file_handle: tables.File) -> list[str]:
    nodes = hdf5_file_handle.list_nodes('/')
    return [node._v_pathname for node in nodes]


def get_picture_node_names(hdf5_file_handle: tables.File) -> list[str]:
    names = get_node_names(hdf5_file_handle)
    return [name for name in names if "pictures" in name]

def parse_json_if_possible(val):
    """
    If val is a str or bytes containing JSON, return the deserialized object.
    Otherwise, return val unchanged.
    """
    # Only attempt on str/bytes
    if isinstance(val, (bytes, bytearray)):
        try:
            text = val.decode('utf-8')
        except UnicodeDecodeError:
            return val
    elif isinstance(val, str):
        text = val
    else:
        return val

    # Try parsing
    try:
        return json.loads(text)
    except json.JSONDecodeError:
        return val


def node_to_dict(node):
    info = HDF5NodeInfo(
        name=node._v_name,
        path=node._v_pathname,
        type=node.__class__.__name__,
        attributes={}
    )

    for key in node._v_attrs._f_list(attrset='all'):
        raw = node._v_attrs[key]
        # first coerce numpyâ€types to Python
        if hasattr(raw, "tolist"):
            pyval = raw.tolist()
        elif hasattr(raw, "item"):
            pyval = raw.item()
        else:
            pyval = raw
        # then parse JSON if it is a JSON string
        info.attributes[key] = parse_json_if_possible(pyval)

    return info

def get_file_data(file_path: str,) -> ParsedHDF5FileContent:
    with tables.open_file(file_path, mode="r") as file_handle:

        picture_node_names = get_picture_node_names(file_handle)
        pictures: dict[str, list[str]] = {}
        for node_name in picture_node_names:
            node = file_handle.get_node(node_name)
            assert isinstance(node, tables.Array)
            pictures[node_name.removeprefix('/')] = [img.decode('utf-8') for img in node.read().tolist()]

        try:
            acceleration_data = file_handle.get_node("/acceleration")
            assert isinstance(acceleration_data, tables.Table)
            acceleration_df = pd.DataFrame.from_records(acceleration_data.read(), columns=acceleration_data.colnames)
            acceleration_meta = node_to_dict(acceleration_data)
        except NoSuchNodeError:
            raise HTTPException(status_code=500, detail="Acceleration data not found in the file")
        except AssertionError:
            raise HTTPException(status_code=500, detail="Acceleration data is not a table")

        try:
            for pics_key in pictures.keys():
                obj: dict[int, str] = {}
                for index, pic in enumerate(pictures[pics_key]):
                    obj[index] = pic
                if MetadataPrefix.PRE in pics_key:
                    stripped_key = pics_key.split(f"{MetadataPrefix.PRE}__")[1]
                    acceleration_meta.attributes["pre_metadata"]["parameters"][stripped_key] = obj
                elif MetadataPrefix.POST in pics_key:
                    stripped_key = pics_key.split(f"{MetadataPrefix.POST}__")[1]
                    acceleration_meta.attributes["post_metadata"]["parameters"][stripped_key] = obj
                else:
                    logger.error(f"Unknown picture key: {pics_key}")
        except KeyError:
            pass
        except IndexError:
            raise HTTPException(status_code=500, detail="Picture data is not prefixed.")

        sensor_df = pd.DataFrame()
        try:
            sensor_data = file_handle.get_node("/sensors")
            assert isinstance(sensor_data, tables.Table)
            sensor_df = pd.DataFrame.from_records(sensor_data.read(), columns=sensor_data.colnames)
        except NoSuchNodeError:
            # No sensor data available; pass
            pass
        except AssertionError:
            # sensor data available, but not in the right shape
            pass


    return ParsedHDF5FileContent(
        acceleration_df=acceleration_df,
        sensor_df=sensor_df,
        acceleration_meta=acceleration_meta,
        pictures=pictures
    )

def ensure_dataframe_with_columns(df, required_columns) -> pd.DataFrame:
    """
    Ensures the object is a DataFrame and contains the required columns.

    Parameters:
        df: The object to check.
        required_columns: A list or set of column names that must be present.

    Returns:
        The DataFrame if it meets the requirements.

    Raises:
        TypeError: If the object is not a DataFrame.
        ValueError: If required columns are missing.
    """
    # Ensure the object is a DataFrame
    if not isinstance(df, pd.DataFrame):
        raise TypeError(f"Expected a pandas DataFrame, but got {type(df).__name__}")

    # Check for required columns
    missing_columns = set(required_columns) - set(df.columns)
    if missing_columns:
        raise ValueError(f"Missing required columns: {', '.join(missing_columns)}")

    return df
