# API Analyzer Configuration File
# Uncomment and configure the provider you want to use

llm:
  # Provider: ollama, openai, claude, gemini
  provider: ollama
  
  # Ollama Configuration (local or remote)
  ollama:
    host: http://localhost:11434
    model: llama3.1
  
  # OpenAI Configuration
  # openai:
  #   api_key: your-openai-api-key
  #   model: gpt-4
  
  # Claude Configuration
  # claude:
  #   api_key: your-anthropic-api-key
  #   model: claude-3-5-sonnet-20241022
  
  # Gemini Configuration
  # gemini:
  #   api_key: your-google-api-key
  #   model: gemini-1.5-flash
