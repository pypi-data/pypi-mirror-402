import os
from pathlib import Path
from typing import List

import aiohttp
from dotenv import load_dotenv
from mcp.types import TextContent

# Load environment variables from .env file in project root
env_path = Path(__file__).parent.parent.parent / ".env"
load_dotenv(env_path)


async def get_all_pages(
    start: int | None = None, end: int | None = None
) -> List[TextContent]:
    """
    Get a simple list of all pages in the Logseq graph with essential metadata.

    Returns a clean listing optimized for LLM consumption with essential identifiers
    and timestamps for each page. By default shows all pages, but can be limited
    with start and end parameters.

    Args:
        start: Starting index (0-based, inclusive). If None, starts from beginning.
        end: Ending index (0-based, exclusive). If None, goes to end.
    """
    endpoint = os.getenv("LOGSEQ_API_ENDPOINT", "http://127.0.0.1:12315/api")
    token = os.getenv("LOGSEQ_API_TOKEN", "auth")

    headers = {"Authorization": f"Bearer {token}"}

    def format_timestamp(timestamp):
        """Convert timestamp to readable format"""
        if not timestamp:
            return "N/A"
        try:
            from datetime import datetime

            dt = datetime.fromtimestamp(timestamp / 1000)  # Convert from ms
            return dt.strftime("%Y-%m-%d %H:%M:%S")
        except (ValueError, TypeError, OverflowError):
            return str(timestamp)

    def format_page_entry(page):
        """Format a single page entry with essential metadata"""
        page_id = page.get("id", "N/A")
        uuid = page.get("uuid", "N/A")
        original_name = page.get("originalName", page.get("name", "Unknown"))
        created_at = format_timestamp(page.get("createdAt"))
        updated_at = format_timestamp(page.get("updatedAt"))

        # Determine page type
        page_type = "ğŸ“… Journal" if page.get("journal?", False) else "ğŸ“„ Page"

        return f"{page_type} **{original_name}** | ID: {page_id} | UUID: {uuid} | Created: {created_at} | Updated: {updated_at}"

    async with aiohttp.ClientSession() as session:
        try:
            # Get all pages
            payload = {"method": "logseq.Editor.getAllPages"}

            async with session.post(
                endpoint, json=payload, headers=headers
            ) as response:
                if response.status != 200:
                    return [
                        TextContent(
                            type="text",
                            text=f"âŒ Failed to fetch pages: HTTP {response.status}",
                        )
                    ]

                pages = await response.json()
                if not pages:
                    return [
                        TextContent(
                            type="text", text="âœ… No pages found in Logseq graph"
                        )
                    ]

            # Sort pages alphabetically by name
            def get_page_name(page):
                return page.get("originalName", page.get("name", "")).lower()

            # Sort using a different approach to avoid sort parameter
            page_names = [(get_page_name(page), page) for page in pages]
            page_names.sort()
            sorted_pages = [page for _, page in page_names]

            # Separate journal and regular pages
            journal_pages = [p for p in sorted_pages if p.get("journal?", False)]
            regular_pages = [p for p in sorted_pages if not p.get("journal?", False)]

            # Apply start/end limits if specified
            if start is not None or end is not None:
                # Apply limits to regular pages
                regular_pages = regular_pages[start:end]
                # Apply limits to journal pages
                journal_pages = journal_pages[start:end]

                # Build output with range information
                range_info = f" (showing indices {start if start is not None else 0}-{end if end is not None else 'end'})"
                output_lines = [
                    f"ğŸ“Š **LOGSEQ PAGES LISTING{range_info}**",
                    f"ğŸ“ˆ Total pages in graph: {len(pages)}",
                    f"ğŸ“„ Regular pages shown: {len(regular_pages)}",
                    f"ğŸ“… Journal pages shown: {len(journal_pages)}",
                    "",
                ]
            else:
                # Build simple output with clear distinction between Journal and Regular pages
                output_lines = [
                    "ğŸ“Š **LOGSEQ PAGES LISTING**",
                    f"ğŸ“ˆ Total pages: {len(pages)}",
                    f"ğŸ“… Journal pages: {len(journal_pages)}",
                    f"ğŸ“„ Regular pages: {len(regular_pages)}",
                    "",
                ]

            # Add regular pages section
            if regular_pages:
                output_lines.extend(["ğŸ“„ **REGULAR PAGES:**", ""])
                for page in regular_pages:
                    output_lines.append(
                        format_page_entry(page).replace("ğŸ“„ Page", "ğŸ“„")
                    )

            # Add journal pages section
            if journal_pages:
                output_lines.extend(["", "ğŸ“… **JOURNAL PAGES:**", ""])
                for page in journal_pages:
                    output_lines.append(
                        format_page_entry(page).replace("ğŸ“… Journal", "ğŸ“…")
                    )

            return [TextContent(type="text", text="\n".join(output_lines))]

        except Exception as e:
            return [TextContent(type="text", text=f"âŒ Error fetching pages: {str(e)}")]
