Metadata-Version: 2.4
Name: moda-ai
Version: 0.1.2
Summary: Moda SDK - LLM Observability with Automatic Conversation Threading
Project-URL: Repository, https://github.com/ModaLabs/moda-python
Project-URL: Documentation, https://docs.moda.so
Author-email: Moda Labs <support@moda.so>
License-Expression: Apache-2.0
Requires-Python: <4,>=3.10
Requires-Dist: aiohttp<4,>=3.11.11
Requires-Dist: colorama<0.5.0,>=0.4.6
Requires-Dist: cuid<0.5,>=0.4
Requires-Dist: deprecated<2,>=1.2.14
Requires-Dist: httpx<1,>=0.27.0
Requires-Dist: jinja2<4,>=3.1.5
Requires-Dist: moda-anthropic
Requires-Dist: moda-openai
Requires-Dist: opentelemetry-api<2,>=1.38.0
Requires-Dist: opentelemetry-exporter-otlp-proto-grpc<2,>=1.38.0
Requires-Dist: opentelemetry-exporter-otlp-proto-http<2,>=1.38.0
Requires-Dist: opentelemetry-instrumentation-logging>=0.59b0
Requires-Dist: opentelemetry-instrumentation-redis>=0.59b0
Requires-Dist: opentelemetry-instrumentation-requests>=0.59b0
Requires-Dist: opentelemetry-instrumentation-sqlalchemy>=0.59b0
Requires-Dist: opentelemetry-instrumentation-threading>=0.59b0
Requires-Dist: opentelemetry-instrumentation-urllib3>=0.59b0
Requires-Dist: opentelemetry-sdk<2,>=1.38.0
Requires-Dist: opentelemetry-semantic-conventions-ai<0.5.0,>=0.4.13
Requires-Dist: pydantic>=1
Requires-Dist: tenacity<10.0,>=8.2.3
Provides-Extra: datasets
Requires-Dist: pandas; extra == 'datasets'
Description-Content-Type: text/markdown

# moda-ai

Moda's Python SDK for LLM observability with automatic conversation threading. Built on OpenTelemetry.

## Installation

```bash
pip install moda-ai
```

## Quick Start

```python
import moda
from openai import OpenAI

moda.init("YOUR_MODA_API_KEY")

# Set conversation ID for your session (recommended)
moda.conversation_id = "session_" + session_id

client = OpenAI()
response = client.chat.completions.create(
    model="gpt-4",
    messages=[{"role": "user", "content": "Hello!"}]
)

moda.flush()
```

## Conversation Tracking

### Setting Conversation ID (Recommended)

For production use, explicitly set a conversation ID to group related LLM calls:

```python
# Property-style (recommended)
moda.conversation_id = "support_ticket_123"
client.chat.completions.create(...)
moda.conversation_id = None  # clear when done

# Or use the setter function
moda.set_conversation_id_value("support_ticket_123")
moda.set_conversation_id_value(None)  # clear

# Or use context manager (scoped)
with moda.set_conversation_id("support_ticket_123"):
    client.chat.completions.create(...)
```

### Setting User ID

Associate LLM calls with specific users:

```python
moda.user_id = "user_12345"
client.chat.completions.create(...)
moda.user_id = None  # clear
```

## Automatic Fallback

If you don't set a conversation ID, the SDK automatically computes one from the first user message and system prompt. This works for simple use cases but explicit IDs are recommended for production.

## Full Documentation

See the [main repository README](https://github.com/ModaLabs/moda-python) for complete documentation.
