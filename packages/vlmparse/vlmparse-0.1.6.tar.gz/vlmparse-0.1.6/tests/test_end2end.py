import os

import pytest

from vlmparse.registries import converter_config_registry


@pytest.mark.parametrize("model", ["gemini-2.5-flash-lite"])
def test_convert(file_path, model):
    config = converter_config_registry.get(model)
    client = config.get_client(return_documents_in_batch_mode=True, debug=True)
    docs = client.batch([file_path])
    assert len(docs) == 1
    doc = docs[0]
    assert len(doc.pages) == 2
    assert doc.pages[0].text is not None
    assert doc.pages[1].text is not None

    assert doc.pages[1].completion_tokens > 0
    assert doc.pages[1].prompt_tokens > 0


@pytest.mark.skipif(
    "RUN_DEPLOYMENT_VLLM" not in os.environ
    or os.environ["RUN_DEPLOYMENT_VLLM"] == "false"
    or "GPU_TEST_VLMPARSE" not in os.environ,
    reason="Skipping because RUN_DEPLOYMENT_VLLM is not set or is false or GPU_TEST is not set",
)
@pytest.mark.parametrize(
    "model",
    [
        "docling",
        "lightonocr",
        "dotsocr",
        "nanonets/Nanonets-OCR2-3B",
        "hunyuanocr",
        "olmocr-2-fp8",
        "paddleocrvl",
        "mineru25",
        "chandra",
        "deepseekocr",
        "granite-docling",
    ],
)
def test_converter_with_server_with_docker(file_path, model):
    """Test conversion with automatic Docker deployment (requires GPU due to vllm limitations)."""

    from vlmparse.converter_with_server import ConverterWithServer

    with ConverterWithServer(
        model=model,
        uri=None,
        gpus=os.environ["GPU_TEST_VLMPARSE"],
        with_vllm_server=True,
        concurrency=10,
        port=8059,
    ) as converter_with_server:
        converter_with_server.client.return_documents_in_batch_mode = True

        docs = converter_with_server.parse([str(file_path), str(file_path)], debug=True)

        # Assertions
        assert len(docs) == 1
        doc = docs[0]
        assert len(doc.pages) == 2
        assert doc.pages[0].text is not None
        assert doc.pages[1].text is not None
