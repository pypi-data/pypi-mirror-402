"""TPC-DS DataFrame query implementations.

This module provides DataFrame implementations of TPC-DS benchmark queries
for both Expression and Pandas families.

Each query is implemented following the official TPC-DS specification v2.0.0.
Queries are organized by complexity:
- Simple: Q3, Q7, Q19, Q25, Q42, Q43, Q52, Q53, Q55, Q63, Q65, Q68, Q73, Q79, Q89, Q96, Q98
- Moderate: Q1, Q2, Q4-Q6, Q8, Q10-Q13, Q15-Q22, Q24-Q40, Q44-Q50, Q54, Q56-Q62, Q64, Q66-Q72,
            Q74-Q78, Q80-Q88, Q90-Q95, Q97, Q99
- Complex: Q9, Q14, Q23, Q41, Q51, Q67, Q86

Copyright 2026 Joe Harris / BenchBox Project

TPC Benchmark™ DS (TPC-DS) - Copyright © Transaction Processing Performance Council

Licensed under the MIT License. See LICENSE file in the project root for details.
"""

from __future__ import annotations

from typing import Any

from benchbox.core.dataframe.context import DataFrameContext
from benchbox.core.dataframe.query import DataFrameQuery, QueryCategory

from .parameters import get_parameters
from .registry import register_query

# =============================================================================
# Simple Queries - 3-table joins with straightforward aggregation
# =============================================================================


def q3_expression_impl(ctx: DataFrameContext) -> Any:
    """TPC-DS Q3: Date/Item Brand Sales Analysis (Expression Family).

    Reports sales for items by brand for a specific month and manufacturer,
    grouped by year, brand ID, and brand name.

    Tables: date_dim, store_sales, item
    Pattern: 3-way join -> filter -> group by -> aggregate -> order by
    """
    params = get_parameters(3)
    month = params.get("month", 11)
    manufact_id = params.get("manufact_id", 128)

    date_dim = ctx.get_table("date_dim")
    store_sales = ctx.get_table("store_sales")
    item = ctx.get_table("item")
    col = ctx.col
    lit = ctx.lit

    result = (
        date_dim.join(store_sales, left_on="d_date_sk", right_on="ss_sold_date_sk")
        .join(item, left_on="ss_item_sk", right_on="i_item_sk")
        .filter((col("i_manufact_id") == lit(manufact_id)) & (col("d_moy") == lit(month)))
        .group_by("d_year", "i_brand", "i_brand_id")
        .agg(col("ss_ext_sales_price").sum().alias("sum_agg"))
        .sort(["d_year", "sum_agg", "i_brand_id"], descending=[False, True, False])
        .limit(100)
    )

    return result


def q3_pandas_impl(ctx: DataFrameContext) -> Any:
    """TPC-DS Q3: Date/Item Brand Sales Analysis (Pandas Family)."""
    params = get_parameters(3)
    month = params.get("month", 11)
    manufact_id = params.get("manufact_id", 128)

    date_dim = ctx.get_table("date_dim")
    store_sales = ctx.get_table("store_sales")
    item = ctx.get_table("item")

    # Join date_dim -> store_sales
    merged = date_dim.merge(store_sales, left_on="d_date_sk", right_on="ss_sold_date_sk")

    # Join with item
    merged = merged.merge(item, left_on="ss_item_sk", right_on="i_item_sk")

    # Filter
    filtered = merged[(merged["i_manufact_id"] == manufact_id) & (merged["d_moy"] == month)]

    # Group and aggregate
    result = (
        filtered.groupby(["d_year", "i_brand", "i_brand_id"], as_index=False)
        .agg(sum_agg=("ss_ext_sales_price", "sum"))
        .sort_values(["d_year", "sum_agg", "i_brand_id"], ascending=[True, False, True])
        .head(100)
    )

    return result


def q42_expression_impl(ctx: DataFrameContext) -> Any:
    """TPC-DS Q42: Date/Item Category Sales (Expression Family).

    Reports sales by item category for a specific month and year,
    for items managed by a specific manager.

    Tables: date_dim, store_sales, item
    Pattern: 3-way join -> filter -> group by -> aggregate -> order by
    """
    params = get_parameters(42)
    month = params.get("month", 11)
    year = params.get("year", 2000)

    date_dim = ctx.get_table("date_dim")
    store_sales = ctx.get_table("store_sales")
    item = ctx.get_table("item")
    col = ctx.col
    lit = ctx.lit

    result = (
        date_dim.join(store_sales, left_on="d_date_sk", right_on="ss_sold_date_sk")
        .join(item, left_on="ss_item_sk", right_on="i_item_sk")
        .filter((col("i_manager_id") == lit(1)) & (col("d_moy") == lit(month)) & (col("d_year") == lit(year)))
        .group_by("d_year", "i_category_id", "i_category")
        .agg(col("ss_ext_sales_price").sum().alias("sum_sales"))
        .sort(["sum_sales", "d_year", "i_category_id", "i_category"], descending=[True, False, False, False])
        .limit(100)
    )

    return result


def q42_pandas_impl(ctx: DataFrameContext) -> Any:
    """TPC-DS Q42: Date/Item Category Sales (Pandas Family)."""
    params = get_parameters(42)
    month = params.get("month", 11)
    year = params.get("year", 2000)

    date_dim = ctx.get_table("date_dim")
    store_sales = ctx.get_table("store_sales")
    item = ctx.get_table("item")

    # Join tables
    merged = date_dim.merge(store_sales, left_on="d_date_sk", right_on="ss_sold_date_sk")
    merged = merged.merge(item, left_on="ss_item_sk", right_on="i_item_sk")

    # Filter
    filtered = merged[(merged["i_manager_id"] == 1) & (merged["d_moy"] == month) & (merged["d_year"] == year)]

    # Group and aggregate
    result = (
        filtered.groupby(["d_year", "i_category_id", "i_category"], as_index=False)
        .agg(sum_sales=("ss_ext_sales_price", "sum"))
        .sort_values(["sum_sales", "d_year", "i_category_id", "i_category"], ascending=[False, True, True, True])
        .head(100)
    )

    return result


def q52_expression_impl(ctx: DataFrameContext) -> Any:
    """TPC-DS Q52: Date/Brand Extended Sales (Expression Family).

    Reports extended sales price by brand for a specific month and year,
    for items managed by a specific manager.

    Tables: date_dim, store_sales, item
    Pattern: 3-way join -> filter -> group by -> aggregate -> order by
    """
    params = get_parameters(52)
    month = params.get("month", 11)
    year = params.get("year", 2000)

    date_dim = ctx.get_table("date_dim")
    store_sales = ctx.get_table("store_sales")
    item = ctx.get_table("item")
    col = ctx.col
    lit = ctx.lit

    result = (
        date_dim.join(store_sales, left_on="d_date_sk", right_on="ss_sold_date_sk")
        .join(item, left_on="ss_item_sk", right_on="i_item_sk")
        .filter((col("i_manager_id") == lit(1)) & (col("d_moy") == lit(month)) & (col("d_year") == lit(year)))
        .group_by("d_year", "i_brand", "i_brand_id")
        .agg(col("ss_ext_sales_price").sum().alias("ext_price"))
        .sort(["d_year", "ext_price", "i_brand_id"], descending=[False, True, False])
        .limit(100)
    )

    return result


def q52_pandas_impl(ctx: DataFrameContext) -> Any:
    """TPC-DS Q52: Date/Brand Extended Sales (Pandas Family)."""
    params = get_parameters(52)
    month = params.get("month", 11)
    year = params.get("year", 2000)

    date_dim = ctx.get_table("date_dim")
    store_sales = ctx.get_table("store_sales")
    item = ctx.get_table("item")

    # Join tables
    merged = date_dim.merge(store_sales, left_on="d_date_sk", right_on="ss_sold_date_sk")
    merged = merged.merge(item, left_on="ss_item_sk", right_on="i_item_sk")

    # Filter
    filtered = merged[(merged["i_manager_id"] == 1) & (merged["d_moy"] == month) & (merged["d_year"] == year)]

    # Group and aggregate
    result = (
        filtered.groupby(["d_year", "i_brand", "i_brand_id"], as_index=False)
        .agg(ext_price=("ss_ext_sales_price", "sum"))
        .sort_values(["d_year", "ext_price", "i_brand_id"], ascending=[True, False, True])
        .head(100)
    )

    return result


def q55_expression_impl(ctx: DataFrameContext) -> Any:
    """TPC-DS Q55: Brand Manager Sales (Expression Family).

    Reports brand sales for items managed by a specific manager
    for a specific month and year.

    Tables: date_dim, store_sales, item
    Pattern: 3-way join -> filter -> group by -> aggregate -> order by
    """
    params = get_parameters(55)
    year = params.get("year", 1999)
    month = params.get("month", 11)
    manager_id = params.get("manager_id", 28)

    date_dim = ctx.get_table("date_dim")
    store_sales = ctx.get_table("store_sales")
    item = ctx.get_table("item")
    col = ctx.col
    lit = ctx.lit

    result = (
        date_dim.join(store_sales, left_on="d_date_sk", right_on="ss_sold_date_sk")
        .join(item, left_on="ss_item_sk", right_on="i_item_sk")
        .filter((col("i_manager_id") == lit(manager_id)) & (col("d_moy") == lit(month)) & (col("d_year") == lit(year)))
        .group_by("i_brand_id", "i_brand")
        .agg(col("ss_ext_sales_price").sum().alias("ext_price"))
        .sort(["ext_price", "i_brand_id"], descending=[True, False])
        .limit(100)
    )

    return result


def q55_pandas_impl(ctx: DataFrameContext) -> Any:
    """TPC-DS Q55: Brand Manager Sales (Pandas Family)."""
    params = get_parameters(55)
    year = params.get("year", 1999)
    month = params.get("month", 11)
    manager_id = params.get("manager_id", 28)

    date_dim = ctx.get_table("date_dim")
    store_sales = ctx.get_table("store_sales")
    item = ctx.get_table("item")

    # Join tables
    merged = date_dim.merge(store_sales, left_on="d_date_sk", right_on="ss_sold_date_sk")
    merged = merged.merge(item, left_on="ss_item_sk", right_on="i_item_sk")

    # Filter
    filtered = merged[(merged["i_manager_id"] == manager_id) & (merged["d_moy"] == month) & (merged["d_year"] == year)]

    # Group and aggregate
    result = (
        filtered.groupby(["i_brand_id", "i_brand"], as_index=False)
        .agg(ext_price=("ss_ext_sales_price", "sum"))
        .sort_values(["ext_price", "i_brand_id"], ascending=[False, True])
        .head(100)
    )

    return result


def q19_expression_impl(ctx: DataFrameContext) -> Any:
    """TPC-DS Q19: Store Sales Item/Customer Analysis (Expression Family).

    Reports sales by item brand for customers in specific states,
    filtered by manager ID, excluding specific zip codes.

    Tables: date_dim, store_sales, item, customer, customer_address, store
    Pattern: 6-way join -> filter -> group by -> aggregate -> order by
    """

    params = get_parameters(19)
    year = params.get("year", 1999)
    month = params.get("month", 11)
    manager_id = params.get("manager_id", 8)

    date_dim = ctx.get_table("date_dim")
    store_sales = ctx.get_table("store_sales")
    item = ctx.get_table("item")
    customer = ctx.get_table("customer")
    customer_address = ctx.get_table("customer_address")
    store = ctx.get_table("store")
    col = ctx.col
    lit = ctx.lit

    result = (
        date_dim.join(store_sales, left_on="d_date_sk", right_on="ss_sold_date_sk")
        .join(item, left_on="ss_item_sk", right_on="i_item_sk")
        .join(customer, left_on="ss_customer_sk", right_on="c_customer_sk")
        .join(customer_address, left_on="c_current_addr_sk", right_on="ca_address_sk")
        .join(store, left_on="ss_store_sk", right_on="s_store_sk")
        .filter(
            (col("i_manager_id") == lit(manager_id))
            & (col("d_moy") == lit(month))
            & (col("d_year") == lit(year))
            # Exclude matching zip codes (customer addr zip != store zip first 5 chars)
            & (col("ca_zip").cast_string().str.slice(0, 5) != col("s_zip").cast_string().str.slice(0, 5))
        )
        .group_by("i_brand_id", "i_brand", "i_manufact_id", "i_manufact")
        .agg(col("ss_ext_sales_price").sum().alias("ext_price"))
        .sort(
            ["ext_price", "i_brand", "i_brand_id", "i_manufact_id", "i_manufact"],
            descending=[True, False, False, False, False],
        )
        .limit(100)
    )

    return result


def q19_pandas_impl(ctx: DataFrameContext) -> Any:
    """TPC-DS Q19: Store Sales Item/Customer Analysis (Pandas Family)."""
    params = get_parameters(19)
    year = params.get("year", 1999)
    month = params.get("month", 11)
    manager_id = params.get("manager_id", 8)

    date_dim = ctx.get_table("date_dim")
    store_sales = ctx.get_table("store_sales")
    item = ctx.get_table("item")
    customer = ctx.get_table("customer")
    customer_address = ctx.get_table("customer_address")
    store = ctx.get_table("store")

    # Build the joins
    merged = date_dim.merge(store_sales, left_on="d_date_sk", right_on="ss_sold_date_sk")
    merged = merged.merge(item, left_on="ss_item_sk", right_on="i_item_sk")
    merged = merged.merge(customer, left_on="ss_customer_sk", right_on="c_customer_sk")
    merged = merged.merge(customer_address, left_on="c_current_addr_sk", right_on="ca_address_sk")
    merged = merged.merge(store, left_on="ss_store_sk", right_on="s_store_sk")

    # Filter - manager ID, month, year, and zip code mismatch
    # Note: Use .astype(str) before .str accessor as zip columns may not be string type
    filtered = merged[
        (merged["i_manager_id"] == manager_id)
        & (merged["d_moy"] == month)
        & (merged["d_year"] == year)
        & (merged["ca_zip"].astype(str).str[:5] != merged["s_zip"].astype(str).str[:5])
    ]

    # Group and aggregate
    result = (
        filtered.groupby(["i_brand_id", "i_brand", "i_manufact_id", "i_manufact"], as_index=False)
        .agg(ext_price=("ss_ext_sales_price", "sum"))
        .sort_values(
            ["ext_price", "i_brand", "i_brand_id", "i_manufact_id", "i_manufact"],
            ascending=[False, True, True, True, True],
        )
        .head(100)
    )

    return result


def q43_expression_impl(ctx: DataFrameContext) -> Any:
    """TPC-DS Q43: Store Sales Day Analysis (Expression Family).

    Reports store sales by day of week for a specific year and time zone.

    Tables: date_dim, store_sales, store
    Pattern: 3-way join -> filter -> group by -> aggregate -> order by
    """
    params = get_parameters(43)
    year = params.get("year", 2000)
    gmt_offset = params.get("gmt_offset", -5.0)

    date_dim = ctx.get_table("date_dim")
    store_sales = ctx.get_table("store_sales")
    store = ctx.get_table("store")
    col = ctx.col
    lit = ctx.lit

    result = (
        date_dim.join(store_sales, left_on="d_date_sk", right_on="ss_sold_date_sk")
        .join(store, left_on="ss_store_sk", right_on="s_store_sk")
        .filter((col("s_gmt_offset") == lit(gmt_offset)) & (col("d_year") == lit(year)))
        .group_by("s_store_name", "s_store_id", "d_day_name")
        .agg(col("ss_sales_price").sum().alias("total_sales"))
        .sort("s_store_name", "s_store_id", "d_day_name")
        .limit(100)
    )

    return result


def q43_pandas_impl(ctx: DataFrameContext) -> Any:
    """TPC-DS Q43: Store Sales Day Analysis (Pandas Family)."""
    params = get_parameters(43)
    year = params.get("year", 2000)
    gmt_offset = params.get("gmt_offset", -5.0)

    date_dim = ctx.get_table("date_dim")
    store_sales = ctx.get_table("store_sales")
    store = ctx.get_table("store")

    # Join tables
    merged = date_dim.merge(store_sales, left_on="d_date_sk", right_on="ss_sold_date_sk")
    merged = merged.merge(store, left_on="ss_store_sk", right_on="s_store_sk")

    # Filter
    filtered = merged[(merged["s_gmt_offset"] == gmt_offset) & (merged["d_year"] == year)]

    # Group and aggregate
    result = (
        filtered.groupby(["s_store_name", "s_store_id", "d_day_name"], as_index=False)
        .agg(total_sales=("ss_sales_price", "sum"))
        .sort_values(["s_store_name", "s_store_id", "d_day_name"])
        .head(100)
    )

    return result


def q96_expression_impl(ctx: DataFrameContext) -> Any:
    """TPC-DS Q96: Store Sales Time (Expression Family).

    Counts store sales for specific time periods.

    Tables: store_sales, household_demographics, time_dim, store
    Pattern: 4-way join -> filter -> count
    """
    time_dim = ctx.get_table("time_dim")
    store_sales = ctx.get_table("store_sales")
    store = ctx.get_table("store")
    household_demographics = ctx.get_table("household_demographics")
    col = ctx.col
    lit = ctx.lit

    result = (
        store_sales.join(time_dim, left_on="ss_sold_time_sk", right_on="t_time_sk")
        .join(store, left_on="ss_store_sk", right_on="s_store_sk")
        .join(household_demographics, left_on="ss_hdemo_sk", right_on="hd_demo_sk")
        .filter(
            (col("t_hour") == lit(8))
            & (col("t_minute") >= lit(30))
            & (col("hd_dep_count") == lit(0))
            & (col("s_number_employees") >= lit(200))
            & (col("s_number_employees") <= lit(295))
        )
        .select(col("ss_sold_time_sk").count().alias("count"))
    )

    return result


def q96_pandas_impl(ctx: DataFrameContext) -> Any:
    """TPC-DS Q96: Store Sales Time (Pandas Family)."""
    import pandas as pd

    time_dim = ctx.get_table("time_dim")
    store_sales = ctx.get_table("store_sales")
    store = ctx.get_table("store")
    household_demographics = ctx.get_table("household_demographics")

    # Join tables
    merged = store_sales.merge(time_dim, left_on="ss_sold_time_sk", right_on="t_time_sk")
    merged = merged.merge(store, left_on="ss_store_sk", right_on="s_store_sk")
    merged = merged.merge(household_demographics, left_on="ss_hdemo_sk", right_on="hd_demo_sk")

    # Filter
    filtered = merged[
        (merged["t_hour"] == 8)
        & (merged["t_minute"] >= 30)
        & (merged["hd_dep_count"] == 0)
        & (merged["s_number_employees"] >= 200)
        & (merged["s_number_employees"] <= 295)
    ]

    # Count
    count = len(filtered)
    result = pd.DataFrame({"count": [count]})

    return result


def q7_expression_impl(ctx: DataFrameContext) -> Any:
    """TPC-DS Q7: Promotion Analysis (Expression Family).

    Reports promotional impact on store sales for specific customer demographics.

    Tables: store_sales, customer_demographics, date_dim, item, promotion
    Pattern: 5-way join -> filter -> group by -> aggregate -> order by
    """
    params = get_parameters(7)
    year = params.get("year", 2000)

    store_sales = ctx.get_table("store_sales")
    customer_demographics = ctx.get_table("customer_demographics")
    date_dim = ctx.get_table("date_dim")
    item = ctx.get_table("item")
    promotion = ctx.get_table("promotion")
    col = ctx.col
    lit = ctx.lit

    result = (
        store_sales.join(customer_demographics, left_on="ss_cdemo_sk", right_on="cd_demo_sk")
        .join(date_dim, left_on="ss_sold_date_sk", right_on="d_date_sk")
        .join(item, left_on="ss_item_sk", right_on="i_item_sk")
        .join(promotion, left_on="ss_promo_sk", right_on="p_promo_sk")
        .filter(
            (col("cd_gender") == lit("M"))
            & (col("cd_marital_status") == lit("S"))
            & (col("cd_education_status") == lit("College"))
            & ((col("p_channel_email") == lit("N")) | (col("p_channel_event") == lit("N")))
            & (col("d_year") == lit(year))
        )
        .group_by("i_item_id")
        .agg(
            col("ss_quantity").mean().alias("agg1"),
            col("ss_list_price").mean().alias("agg2"),
            col("ss_coupon_amt").mean().alias("agg3"),
            col("ss_sales_price").mean().alias("agg4"),
        )
        .sort("i_item_id")
        .limit(100)
    )

    return result


def q7_pandas_impl(ctx: DataFrameContext) -> Any:
    """TPC-DS Q7: Promotion Analysis (Pandas Family)."""
    params = get_parameters(7)
    year = params.get("year", 2000)

    store_sales = ctx.get_table("store_sales")
    customer_demographics = ctx.get_table("customer_demographics")
    date_dim = ctx.get_table("date_dim")
    item = ctx.get_table("item")
    promotion = ctx.get_table("promotion")

    # Join tables
    merged = store_sales.merge(customer_demographics, left_on="ss_cdemo_sk", right_on="cd_demo_sk")
    merged = merged.merge(date_dim, left_on="ss_sold_date_sk", right_on="d_date_sk")
    merged = merged.merge(item, left_on="ss_item_sk", right_on="i_item_sk")
    merged = merged.merge(promotion, left_on="ss_promo_sk", right_on="p_promo_sk")

    # Filter
    filtered = merged[
        (merged["cd_gender"] == "M")
        & (merged["cd_marital_status"] == "S")
        & (merged["cd_education_status"] == "College")
        & ((merged["p_channel_email"] == "N") | (merged["p_channel_event"] == "N"))
        & (merged["d_year"] == year)
    ]

    # Group and aggregate
    result = (
        filtered.groupby(["i_item_id"], as_index=False)
        .agg(
            agg1=("ss_quantity", "mean"),
            agg2=("ss_list_price", "mean"),
            agg3=("ss_coupon_amt", "mean"),
            agg4=("ss_sales_price", "mean"),
        )
        .sort_values(["i_item_id"])
        .head(100)
    )

    return result


def q25_expression_impl(ctx: DataFrameContext) -> Any:
    """TPC-DS Q25: Store/Catalog Sales Item Analysis (Expression Family).

    Compares store and catalog sales for items in specific quarters.

    Tables: store_sales, store_returns, catalog_sales, date_dim, store, item
    Pattern: Multi-join -> filter -> group by -> aggregate -> order by
    """
    params = get_parameters(25)
    year = params.get("year", 2001)

    store_sales = ctx.get_table("store_sales")
    store_returns = ctx.get_table("store_returns")
    catalog_sales = ctx.get_table("catalog_sales")
    date_dim = ctx.get_table("date_dim")
    store = ctx.get_table("store")
    item = ctx.get_table("item")
    col = ctx.col
    lit = ctx.lit

    # Filter date_dim for sales and returns periods
    d1 = date_dim.filter((col("d_year") == lit(year)) & (col("d_qoy").is_in([1, 2, 3])))
    d2 = date_dim.filter((col("d_year") == lit(year + 1)) & (col("d_qoy").is_in([1, 2, 3])))
    d3 = date_dim.filter((col("d_year") == lit(year + 1)) & (col("d_qoy").is_in([1, 2, 3])))

    result = (
        store_sales.join(item, left_on="ss_item_sk", right_on="i_item_sk")
        .join(store, left_on="ss_store_sk", right_on="s_store_sk")
        .join(d1, left_on="ss_sold_date_sk", right_on="d_date_sk")
        .join(
            store_returns,
            left_on=["ss_customer_sk", "ss_item_sk", "ss_ticket_number"],
            right_on=["sr_customer_sk", "sr_item_sk", "sr_ticket_number"],
        )
        .join(d2.select("d_date_sk"), left_on="sr_returned_date_sk", right_on="d_date_sk")
        .join(
            catalog_sales,
            left_on=["ss_customer_sk", "ss_item_sk"],
            right_on=["cs_bill_customer_sk", "cs_item_sk"],
        )
        .join(d3.select("d_date_sk"), left_on="cs_sold_date_sk", right_on="d_date_sk")
        .group_by("i_item_id", "i_item_desc", "s_store_id", "s_store_name")
        .agg(
            col("ss_net_profit").sum().alias("store_sales_profit"),
            col("sr_net_loss").sum().alias("store_returns_loss"),
            col("cs_net_profit").sum().alias("catalog_sales_profit"),
        )
        .sort("i_item_id", "i_item_desc", "s_store_id", "s_store_name")
        .limit(100)
    )

    return result


def q25_pandas_impl(ctx: DataFrameContext) -> Any:
    """TPC-DS Q25: Store/Catalog Sales Item Analysis (Pandas Family)."""
    params = get_parameters(25)
    year = params.get("year", 2001)

    store_sales = ctx.get_table("store_sales")
    store_returns = ctx.get_table("store_returns")
    catalog_sales = ctx.get_table("catalog_sales")
    date_dim = ctx.get_table("date_dim")
    store = ctx.get_table("store")
    item = ctx.get_table("item")

    # Filter date_dim
    d1 = date_dim[(date_dim["d_year"] == year) & (date_dim["d_qoy"].isin([1, 2, 3]))]
    d2 = date_dim[(date_dim["d_year"] == year + 1) & (date_dim["d_qoy"].isin([1, 2, 3]))]
    d3 = date_dim[(date_dim["d_year"] == year + 1) & (date_dim["d_qoy"].isin([1, 2, 3]))]

    # Build joins
    merged = store_sales.merge(item, left_on="ss_item_sk", right_on="i_item_sk")
    merged = merged.merge(store, left_on="ss_store_sk", right_on="s_store_sk")
    merged = merged.merge(d1[["d_date_sk"]], left_on="ss_sold_date_sk", right_on="d_date_sk")
    merged = merged.merge(
        store_returns,
        left_on=["ss_customer_sk", "ss_item_sk", "ss_ticket_number"],
        right_on=["sr_customer_sk", "sr_item_sk", "sr_ticket_number"],
    )
    merged = merged.merge(d2[["d_date_sk"]], left_on="sr_returned_date_sk", right_on="d_date_sk")
    merged = merged.merge(
        catalog_sales,
        left_on=["ss_customer_sk", "ss_item_sk"],
        right_on=["cs_bill_customer_sk", "cs_item_sk"],
    )
    merged = merged.merge(d3[["d_date_sk"]], left_on="cs_sold_date_sk", right_on="d_date_sk")

    # Group and aggregate
    result = (
        merged.groupby(["i_item_id", "i_item_desc", "s_store_id", "s_store_name"], as_index=False)
        .agg(
            store_sales_profit=("ss_net_profit", "sum"),
            store_returns_loss=("sr_net_loss", "sum"),
            catalog_sales_profit=("cs_net_profit", "sum"),
        )
        .sort_values(["i_item_id", "i_item_desc", "s_store_id", "s_store_name"])
        .head(100)
    )

    return result


def q53_expression_impl(ctx: DataFrameContext) -> Any:
    """TPC-DS Q53: Store Manufacturer Sales (Expression Family).

    Reports store sales by manufacturer for specific months.

    Tables: store_sales, item, date_dim, store
    Pattern: 4-way join -> filter -> group by -> aggregate -> order by
    """
    params = get_parameters(53)
    manufacturer_ids = params.get("manufacturer_ids", [88, 33, 160, 129])

    store_sales = ctx.get_table("store_sales")
    item = ctx.get_table("item")
    date_dim = ctx.get_table("date_dim")
    store = ctx.get_table("store")
    col = ctx.col

    result = (
        store_sales.join(item, left_on="ss_item_sk", right_on="i_item_sk")
        .join(date_dim, left_on="ss_sold_date_sk", right_on="d_date_sk")
        .join(store, left_on="ss_store_sk", right_on="s_store_sk")
        .filter(col("i_manufact_id").is_in(manufacturer_ids) & col("d_month_seq").is_in(list(range(1200, 1212))))
        .group_by("i_manufact_id", "d_qoy")
        .agg(col("ss_sales_price").sum().alias("sum_sales"))
        .sort("i_manufact_id", "d_qoy")
        .limit(100)
    )

    return result


def q53_pandas_impl(ctx: DataFrameContext) -> Any:
    """TPC-DS Q53: Store Manufacturer Sales (Pandas Family)."""
    params = get_parameters(53)
    manufacturer_ids = params.get("manufacturer_ids", [88, 33, 160, 129])

    store_sales = ctx.get_table("store_sales")
    item = ctx.get_table("item")
    date_dim = ctx.get_table("date_dim")
    store = ctx.get_table("store")

    # Join tables
    merged = store_sales.merge(item, left_on="ss_item_sk", right_on="i_item_sk")
    merged = merged.merge(date_dim, left_on="ss_sold_date_sk", right_on="d_date_sk")
    merged = merged.merge(store, left_on="ss_store_sk", right_on="s_store_sk")

    # Filter
    filtered = merged[
        (merged["i_manufact_id"].isin(manufacturer_ids)) & (merged["d_month_seq"].isin(list(range(1200, 1212))))
    ]

    # Group and aggregate
    result = (
        filtered.groupby(["i_manufact_id", "d_qoy"], as_index=False)
        .agg(sum_sales=("ss_sales_price", "sum"))
        .sort_values(["i_manufact_id", "d_qoy"])
        .head(100)
    )

    return result


def q63_expression_impl(ctx: DataFrameContext) -> Any:
    """TPC-DS Q63: Store Manufacturer Profit (Expression Family).

    Reports store profit by manufacturer for specific months.

    Tables: store_sales, item, date_dim, store
    Pattern: 4-way join -> filter -> group by -> aggregate -> order by
    """
    params = get_parameters(63)
    manufacturer_ids = params.get("manufacturer_ids", [88, 33, 160, 129])

    store_sales = ctx.get_table("store_sales")
    item = ctx.get_table("item")
    date_dim = ctx.get_table("date_dim")
    store = ctx.get_table("store")
    col = ctx.col

    result = (
        store_sales.join(item, left_on="ss_item_sk", right_on="i_item_sk")
        .join(date_dim, left_on="ss_sold_date_sk", right_on="d_date_sk")
        .join(store, left_on="ss_store_sk", right_on="s_store_sk")
        .filter(col("i_manufact_id").is_in(manufacturer_ids) & (col("d_month_seq").is_in(list(range(1200, 1212)))))
        .group_by("i_manufact_id", "d_qoy")
        .agg(col("ss_net_profit").sum().alias("sum_profit"))
        .sort("i_manufact_id", "d_qoy")
        .limit(100)
    )

    return result


def q63_pandas_impl(ctx: DataFrameContext) -> Any:
    """TPC-DS Q63: Store Manufacturer Profit (Pandas Family)."""
    params = get_parameters(63)
    manufacturer_ids = params.get("manufacturer_ids", [88, 33, 160, 129])

    store_sales = ctx.get_table("store_sales")
    item = ctx.get_table("item")
    date_dim = ctx.get_table("date_dim")
    store = ctx.get_table("store")

    # Join tables
    merged = store_sales.merge(item, left_on="ss_item_sk", right_on="i_item_sk")
    merged = merged.merge(date_dim, left_on="ss_sold_date_sk", right_on="d_date_sk")
    merged = merged.merge(store, left_on="ss_store_sk", right_on="s_store_sk")

    # Filter
    filtered = merged[
        (merged["i_manufact_id"].isin(manufacturer_ids)) & (merged["d_month_seq"].isin(list(range(1200, 1212))))
    ]

    # Group and aggregate
    result = (
        filtered.groupby(["i_manufact_id", "d_qoy"], as_index=False)
        .agg(sum_profit=("ss_net_profit", "sum"))
        .sort_values(["i_manufact_id", "d_qoy"])
        .head(100)
    )

    return result


def q65_expression_impl(ctx: DataFrameContext) -> Any:
    """TPC-DS Q65: Store Sales Item Profit (Expression Family).

    Reports store sales item revenue analysis.

    Tables: store_sales, store, item, date_dim
    Pattern: 4-way join -> filter -> group by -> aggregate -> having -> order by
    """
    params = get_parameters(65)
    year = params.get("year", 2000)

    store_sales = ctx.get_table("store_sales")
    store = ctx.get_table("store")
    item = ctx.get_table("item")
    date_dim = ctx.get_table("date_dim")
    col = ctx.col
    lit = ctx.lit

    # Main sales aggregation
    sales_agg = (
        store_sales.join(store, left_on="ss_store_sk", right_on="s_store_sk")
        .join(item, left_on="ss_item_sk", right_on="i_item_sk")
        .join(date_dim, left_on="ss_sold_date_sk", right_on="d_date_sk")
        .filter((col("d_year") == lit(year)) & (col("d_dom").is_between(1, 15)))
        .group_by("s_store_name", "i_item_desc")
        .agg(col("ss_sales_price").sum().alias("revenue"))
        .sort(["revenue", "s_store_name", "i_item_desc"], descending=[True, False, False])
        .limit(100)
    )

    return sales_agg


def q65_pandas_impl(ctx: DataFrameContext) -> Any:
    """TPC-DS Q65: Store Sales Item Profit (Pandas Family)."""
    params = get_parameters(65)
    year = params.get("year", 2000)

    store_sales = ctx.get_table("store_sales")
    store = ctx.get_table("store")
    item = ctx.get_table("item")
    date_dim = ctx.get_table("date_dim")

    # Join tables
    merged = store_sales.merge(store, left_on="ss_store_sk", right_on="s_store_sk")
    merged = merged.merge(item, left_on="ss_item_sk", right_on="i_item_sk")
    merged = merged.merge(date_dim, left_on="ss_sold_date_sk", right_on="d_date_sk")

    # Filter
    filtered = merged[(merged["d_year"] == year) & (merged["d_dom"] >= 1) & (merged["d_dom"] <= 15)]

    # Group and aggregate
    result = (
        filtered.groupby(["s_store_name", "i_item_desc"], as_index=False)
        .agg(revenue=("ss_sales_price", "sum"))
        .sort_values(["revenue", "s_store_name", "i_item_desc"], ascending=[False, True, True])
        .head(100)
    )

    return result


def q68_expression_impl(ctx: DataFrameContext) -> Any:
    """TPC-DS Q68: Store Sales Customer Household (Expression Family).

    Reports customer household purchases by city.

    Tables: store_sales, date_dim, store, household_demographics, customer_address
    Pattern: 5-way join -> filter -> group by -> aggregate -> order by
    """
    params = get_parameters(68)
    year = params.get("year", 2001)
    cities = params.get("cities", ["Midway", "Fairview"])

    store_sales = ctx.get_table("store_sales")
    date_dim = ctx.get_table("date_dim")
    store = ctx.get_table("store")
    household_demographics = ctx.get_table("household_demographics")
    customer_address = ctx.get_table("customer_address")
    col = ctx.col
    lit = ctx.lit

    result = (
        store_sales.join(date_dim, left_on="ss_sold_date_sk", right_on="d_date_sk")
        .join(store, left_on="ss_store_sk", right_on="s_store_sk")
        .join(household_demographics, left_on="ss_hdemo_sk", right_on="hd_demo_sk")
        .join(customer_address, left_on="ss_addr_sk", right_on="ca_address_sk")
        .filter(
            (col("d_year") == lit(year))
            & (col("d_dom").is_between(1, 2))
            & col("ca_city").is_in(cities)
            & ((col("hd_dep_count") == lit(4)) | (col("hd_vehicle_count") == lit(3)))
        )
        .group_by("ss_ticket_number", "ss_customer_sk", "ss_addr_sk", "ca_city")
        .agg(
            col("ss_ext_sales_price").sum().alias("extended_price"),
            col("ss_ext_list_price").sum().alias("list_price"),
            col("ss_ext_tax").sum().alias("extended_tax"),
        )
        .sort("ss_ticket_number")
        .limit(100)
    )

    return result


def q68_pandas_impl(ctx: DataFrameContext) -> Any:
    """TPC-DS Q68: Store Sales Customer Household (Pandas Family)."""
    params = get_parameters(68)
    year = params.get("year", 2001)
    cities = params.get("cities", ["Midway", "Fairview"])

    store_sales = ctx.get_table("store_sales")
    date_dim = ctx.get_table("date_dim")
    store = ctx.get_table("store")
    household_demographics = ctx.get_table("household_demographics")
    customer_address = ctx.get_table("customer_address")

    # Join tables
    merged = store_sales.merge(date_dim, left_on="ss_sold_date_sk", right_on="d_date_sk")
    merged = merged.merge(store, left_on="ss_store_sk", right_on="s_store_sk")
    merged = merged.merge(household_demographics, left_on="ss_hdemo_sk", right_on="hd_demo_sk")
    merged = merged.merge(customer_address, left_on="ss_addr_sk", right_on="ca_address_sk")

    # Filter
    filtered = merged[
        (merged["d_year"] == year)
        & (merged["d_dom"] >= 1)
        & (merged["d_dom"] <= 2)
        & (merged["ca_city"].isin(cities))
        & ((merged["hd_dep_count"] == 4) | (merged["hd_vehicle_count"] == 3))
    ]

    # Group and aggregate
    result = (
        filtered.groupby(["ss_ticket_number", "ss_customer_sk", "ss_addr_sk", "ca_city"], as_index=False)
        .agg(
            extended_price=("ss_ext_sales_price", "sum"),
            list_price=("ss_ext_list_price", "sum"),
            extended_tax=("ss_ext_tax", "sum"),
        )
        .sort_values(["ss_ticket_number"])
        .head(100)
    )

    return result


def q73_expression_impl(ctx: DataFrameContext) -> Any:
    """TPC-DS Q73: Store Sales Household Vehicle (Expression Family).

    Reports customer household purchases by vehicle count.

    Tables: store_sales, date_dim, store, household_demographics, customer
    Pattern: 5-way join -> filter -> group by -> aggregate -> having -> order by
    """
    params = get_parameters(73)
    year = params.get("year", 1999)
    county = params.get("county", "Williamson County")

    store_sales = ctx.get_table("store_sales")
    date_dim = ctx.get_table("date_dim")
    store = ctx.get_table("store")
    household_demographics = ctx.get_table("household_demographics")
    customer = ctx.get_table("customer")
    col = ctx.col
    lit = ctx.lit

    result = (
        store_sales.join(date_dim, left_on="ss_sold_date_sk", right_on="d_date_sk")
        .join(store, left_on="ss_store_sk", right_on="s_store_sk")
        .join(household_demographics, left_on="ss_hdemo_sk", right_on="hd_demo_sk")
        .join(customer, left_on="ss_customer_sk", right_on="c_customer_sk")
        .filter(
            (col("d_year").is_in([year, year + 1, year + 2]))
            & (col("d_dom").is_between(1, 2))
            & (col("s_county") == lit(county))
            & ((col("hd_buy_potential") == lit(">10000")) | (col("hd_buy_potential") == lit("Unknown")))
            & (col("hd_vehicle_count") > lit(0))
        )
        .group_by("c_last_name", "c_first_name", "c_salutation", "c_preferred_cust_flag", "ss_ticket_number")
        .agg(col("ss_ticket_number").count().alias("cnt"))
        .filter((col("cnt") >= lit(1)) & (col("cnt") <= lit(5)))
        .sort("c_last_name", "c_first_name")
        .limit(100)
    )

    return result


def q73_pandas_impl(ctx: DataFrameContext) -> Any:
    """TPC-DS Q73: Store Sales Household Vehicle (Pandas Family)."""
    params = get_parameters(73)
    year = params.get("year", 1999)
    county = params.get("county", "Williamson County")

    store_sales = ctx.get_table("store_sales")
    date_dim = ctx.get_table("date_dim")
    store = ctx.get_table("store")
    household_demographics = ctx.get_table("household_demographics")
    customer = ctx.get_table("customer")

    # Join tables
    merged = store_sales.merge(date_dim, left_on="ss_sold_date_sk", right_on="d_date_sk")
    merged = merged.merge(store, left_on="ss_store_sk", right_on="s_store_sk")
    merged = merged.merge(household_demographics, left_on="ss_hdemo_sk", right_on="hd_demo_sk")
    merged = merged.merge(customer, left_on="ss_customer_sk", right_on="c_customer_sk")

    # Filter
    filtered = merged[
        (merged["d_year"].isin([year, year + 1, year + 2]))
        & (merged["d_dom"] >= 1)
        & (merged["d_dom"] <= 2)
        & (merged["s_county"] == county)
        & ((merged["hd_buy_potential"] == ">10000") | (merged["hd_buy_potential"] == "Unknown"))
        & (merged["hd_vehicle_count"] > 0)
    ]

    # Group and aggregate
    grouped = filtered.groupby(
        ["c_last_name", "c_first_name", "c_salutation", "c_preferred_cust_flag", "ss_ticket_number"], as_index=False
    ).agg(cnt=("ss_ticket_number", "count"))

    # Having clause
    result = (
        grouped[(grouped["cnt"] >= 1) & (grouped["cnt"] <= 5)].sort_values(["c_last_name", "c_first_name"]).head(100)
    )

    return result


def q79_expression_impl(ctx: DataFrameContext) -> Any:
    """TPC-DS Q79: Store Sales Customer/Store Profit (Expression Family).

    Reports customer profit by store for specific household demographics.

    Tables: store_sales, date_dim, store, household_demographics, customer
    Pattern: 5-way join -> filter -> group by -> aggregate -> order by
    """
    params = get_parameters(79)
    year = params.get("year", 2000)
    household_dep_count = params.get("household_dep_count", 6)

    store_sales = ctx.get_table("store_sales")
    date_dim = ctx.get_table("date_dim")
    store = ctx.get_table("store")
    household_demographics = ctx.get_table("household_demographics")
    customer = ctx.get_table("customer")
    col = ctx.col
    lit = ctx.lit

    result = (
        store_sales.join(date_dim, left_on="ss_sold_date_sk", right_on="d_date_sk")
        .join(store, left_on="ss_store_sk", right_on="s_store_sk")
        .join(household_demographics, left_on="ss_hdemo_sk", right_on="hd_demo_sk")
        .join(customer, left_on="ss_customer_sk", right_on="c_customer_sk")
        .filter(
            (col("d_year") == lit(year))
            & (col("d_dom").is_between(1, 2))
            & ((col("hd_dep_count") == lit(household_dep_count)) | (col("hd_vehicle_count") > lit(0)))
        )
        .group_by("c_last_name", "c_first_name", "ss_ticket_number", "s_city")
        .agg(
            col("ss_coupon_amt").sum().alias("amt"),
            col("ss_net_profit").sum().alias("profit"),
        )
        .sort(["c_last_name", "c_first_name", "s_city", "profit"], descending=[False, False, False, True])
        .limit(100)
    )

    return result


def q79_pandas_impl(ctx: DataFrameContext) -> Any:
    """TPC-DS Q79: Store Sales Customer/Store Profit (Pandas Family)."""
    params = get_parameters(79)
    year = params.get("year", 2000)
    household_dep_count = params.get("household_dep_count", 6)

    store_sales = ctx.get_table("store_sales")
    date_dim = ctx.get_table("date_dim")
    store = ctx.get_table("store")
    household_demographics = ctx.get_table("household_demographics")
    customer = ctx.get_table("customer")

    # Join tables
    merged = store_sales.merge(date_dim, left_on="ss_sold_date_sk", right_on="d_date_sk")
    merged = merged.merge(store, left_on="ss_store_sk", right_on="s_store_sk")
    merged = merged.merge(household_demographics, left_on="ss_hdemo_sk", right_on="hd_demo_sk")
    merged = merged.merge(customer, left_on="ss_customer_sk", right_on="c_customer_sk")

    # Filter
    filtered = merged[
        (merged["d_year"] == year)
        & (merged["d_dom"] >= 1)
        & (merged["d_dom"] <= 2)
        & ((merged["hd_dep_count"] == household_dep_count) | (merged["hd_vehicle_count"] > 0))
    ]

    # Group and aggregate
    result = (
        filtered.groupby(["c_last_name", "c_first_name", "ss_ticket_number", "s_city"], as_index=False)
        .agg(
            amt=("ss_coupon_amt", "sum"),
            profit=("ss_net_profit", "sum"),
        )
        .sort_values(["c_last_name", "c_first_name", "s_city", "profit"], ascending=[True, True, True, False])
        .head(100)
    )

    return result


def q89_expression_impl(ctx: DataFrameContext) -> Any:
    """TPC-DS Q89: Store Item Sales Profit (Expression Family).

    Reports monthly store sales average and deviation.

    Tables: store_sales, date_dim, store, item
    Pattern: 4-way join -> filter -> group by -> aggregate -> having -> order by
    """
    params = get_parameters(89)
    year = params.get("year", 1999)

    store_sales = ctx.get_table("store_sales")
    date_dim = ctx.get_table("date_dim")
    store = ctx.get_table("store")
    item = ctx.get_table("item")
    col = ctx.col
    lit = ctx.lit

    result = (
        store_sales.join(date_dim, left_on="ss_sold_date_sk", right_on="d_date_sk")
        .join(store, left_on="ss_store_sk", right_on="s_store_sk")
        .join(item, left_on="ss_item_sk", right_on="i_item_sk")
        .filter(
            (col("d_year") == lit(year))
            & (
                ((col("i_category") == lit("Books")) & (col("i_class") == lit("business")))
                | ((col("i_category") == lit("Electronics")) & (col("i_class") == lit("portable")))
            )
        )
        .group_by("i_category", "i_class", "i_brand", "s_store_name", "s_company_name", "d_moy")
        .agg(col("ss_sales_price").sum().alias("sum_sales"))
        .sort("sum_sales", "s_store_name", "i_category")
        .limit(100)
    )

    return result


def q89_pandas_impl(ctx: DataFrameContext) -> Any:
    """TPC-DS Q89: Store Item Sales Profit (Pandas Family)."""
    params = get_parameters(89)
    year = params.get("year", 1999)

    store_sales = ctx.get_table("store_sales")
    date_dim = ctx.get_table("date_dim")
    store = ctx.get_table("store")
    item = ctx.get_table("item")

    # Join tables
    merged = store_sales.merge(date_dim, left_on="ss_sold_date_sk", right_on="d_date_sk")
    merged = merged.merge(store, left_on="ss_store_sk", right_on="s_store_sk")
    merged = merged.merge(item, left_on="ss_item_sk", right_on="i_item_sk")

    # Filter
    filtered = merged[
        (merged["d_year"] == year)
        & (
            ((merged["i_category"] == "Books") & (merged["i_class"] == "business"))
            | ((merged["i_category"] == "Electronics") & (merged["i_class"] == "portable"))
        )
    ]

    # Group and aggregate
    result = (
        filtered.groupby(
            ["i_category", "i_class", "i_brand", "s_store_name", "s_company_name", "d_moy"], as_index=False
        )
        .agg(sum_sales=("ss_sales_price", "sum"))
        .sort_values(["sum_sales", "s_store_name", "i_category"])
        .head(100)
    )

    return result


def q98_expression_impl(ctx: DataFrameContext) -> Any:
    """TPC-DS Q98: Store Sales Item Band (Expression Family).

    Reports store sales by item category for specific categories.

    Tables: store_sales, item, date_dim
    Pattern: 3-way join -> filter -> group by -> aggregate -> order by
    """
    params = get_parameters(98)
    year = params.get("year", 1999)
    categories = params.get("categories", ["Sports", "Books", "Home"])

    store_sales = ctx.get_table("store_sales")
    item = ctx.get_table("item")
    date_dim = ctx.get_table("date_dim")
    col = ctx.col
    lit = ctx.lit

    result = (
        store_sales.join(item, left_on="ss_item_sk", right_on="i_item_sk")
        .join(date_dim, left_on="ss_sold_date_sk", right_on="d_date_sk")
        .filter((col("d_year") == lit(year)) & (col("d_moy") == lit(1)) & col("i_category").is_in(categories))
        .group_by("i_item_desc", "i_category", "i_class", "i_current_price")
        .agg(col("ss_ext_sales_price").sum().alias("itemrevenue"))
        .sort(["i_category", "i_class", "itemrevenue", "i_item_desc"], descending=[False, False, True, False])
        .limit(100)
    )

    return result


def q98_pandas_impl(ctx: DataFrameContext) -> Any:
    """TPC-DS Q98: Store Sales Item Band (Pandas Family)."""
    params = get_parameters(98)
    year = params.get("year", 1999)
    categories = params.get("categories", ["Sports", "Books", "Home"])

    store_sales = ctx.get_table("store_sales")
    item = ctx.get_table("item")
    date_dim = ctx.get_table("date_dim")

    # Join tables
    merged = store_sales.merge(item, left_on="ss_item_sk", right_on="i_item_sk")
    merged = merged.merge(date_dim, left_on="ss_sold_date_sk", right_on="d_date_sk")

    # Filter
    filtered = merged[(merged["d_year"] == year) & (merged["d_moy"] == 1) & (merged["i_category"].isin(categories))]

    # Group and aggregate
    result = (
        filtered.groupby(["i_item_desc", "i_category", "i_class", "i_current_price"], as_index=False)
        .agg(itemrevenue=("ss_ext_sales_price", "sum"))
        .sort_values(["i_category", "i_class", "itemrevenue", "i_item_desc"], ascending=[True, True, False, True])
        .head(100)
    )

    return result


# =============================================================================
# Moderate Queries - CTEs, subqueries, and more complex patterns
# =============================================================================


def q1_expression_impl(ctx: DataFrameContext) -> Any:
    """TPC-DS Q1: Customer Returns Analysis (Expression Family).

    Finds customers with above-average returns for their state.
    Uses CTE pattern for state-level aggregation.

    Tables: store_returns, date_dim, store, customer, customer_address
    Pattern: CTE -> join -> filter by aggregation -> order by
    """
    params = get_parameters(1)
    year = params.get("year", 2000)
    state = params.get("state", "TN")

    store_returns = ctx.get_table("store_returns")
    date_dim = ctx.get_table("date_dim")
    store = ctx.get_table("store")
    customer = ctx.get_table("customer")
    customer_address = ctx.get_table("customer_address")
    col = ctx.col
    lit = ctx.lit

    # CTE: Calculate customer returns by state
    # Note: After joins, right-side join keys are dropped. Use sr_customer_sk (preserved)
    # instead of c_customer_sk (dropped) in group_by
    customer_total = (
        store_returns.join(date_dim, left_on="sr_returned_date_sk", right_on="d_date_sk")
        .join(store, left_on="sr_store_sk", right_on="s_store_sk")
        .join(customer, left_on="sr_customer_sk", right_on="c_customer_sk")
        .join(customer_address, left_on="c_current_addr_sk", right_on="ca_address_sk")
        .filter((col("d_year") == lit(year)) & (col("ca_state") == lit(state)))
        .group_by("sr_customer_sk", "c_customer_id", "c_first_name", "c_last_name", "c_salutation")
        .agg(col("sr_return_amt").sum().alias("ctr_total_return"))
    )

    # Calculate state average
    state_avg = customer_total.select(col("ctr_total_return").mean().alias("avg_return"))

    # Main result: filter customers with above-average returns
    result = (
        customer_total.join(state_avg, how="cross")
        .filter(col("ctr_total_return") > col("avg_return") * lit(1.2))
        .select("c_customer_id", "c_salutation", "c_first_name", "c_last_name", "ctr_total_return")
        .sort("ctr_total_return", descending=True)
        .limit(100)
    )

    return result


def q1_pandas_impl(ctx: DataFrameContext) -> Any:
    """TPC-DS Q1: Customer Returns Analysis (Pandas Family)."""
    params = get_parameters(1)
    year = params.get("year", 2000)
    state = params.get("state", "TN")

    store_returns = ctx.get_table("store_returns")
    date_dim = ctx.get_table("date_dim")
    store = ctx.get_table("store")
    customer = ctx.get_table("customer")
    customer_address = ctx.get_table("customer_address")

    # Build joins
    merged = store_returns.merge(date_dim, left_on="sr_returned_date_sk", right_on="d_date_sk")
    merged = merged.merge(store, left_on="sr_store_sk", right_on="s_store_sk")
    merged = merged.merge(customer, left_on="sr_customer_sk", right_on="c_customer_sk")
    merged = merged.merge(customer_address, left_on="c_current_addr_sk", right_on="ca_address_sk")

    # Filter
    filtered = merged[(merged["d_year"] == year) & (merged["ca_state"] == state)]

    # Calculate customer totals
    customer_total = filtered.groupby(
        ["c_customer_sk", "c_customer_id", "c_first_name", "c_last_name", "c_salutation"], as_index=False
    ).agg(ctr_total_return=("sr_return_amt", "sum"))

    # Calculate state average and filter
    avg_return = customer_total["ctr_total_return"].mean()
    result = (
        customer_total[customer_total["ctr_total_return"] > avg_return * 1.2][
            ["c_customer_id", "c_salutation", "c_first_name", "c_last_name", "ctr_total_return"]
        ]
        .sort_values("ctr_total_return", ascending=False)
        .head(100)
    )

    return result


def q6_expression_impl(ctx: DataFrameContext) -> Any:
    """TPC-DS Q6: Customer State/Item Analysis (Expression Family).

    Finds items where more than 10% of customers are from a specific state.
    Uses subquery pattern.

    Tables: customer_address, customer, store_sales, date_dim, item
    Pattern: Subquery -> join -> filter -> group by -> having
    """
    params = get_parameters(6)
    year = params.get("year", 2001)
    month = params.get("month", 1)

    customer_address = ctx.get_table("customer_address")
    customer = ctx.get_table("customer")
    store_sales = ctx.get_table("store_sales")
    date_dim = ctx.get_table("date_dim")
    item = ctx.get_table("item")
    col = ctx.col
    lit = ctx.lit

    # Compute average item price as scalar first (aggregates not allowed in WHERE)
    avg_price = item.select(col("i_current_price").mean().alias("avg_price")).scalar(0, 0)
    price_threshold = 1.2 * avg_price

    result = (
        customer_address.join(customer, left_on="ca_address_sk", right_on="c_current_addr_sk")
        .join(store_sales, left_on="c_customer_sk", right_on="ss_customer_sk")
        .join(date_dim, left_on="ss_sold_date_sk", right_on="d_date_sk")
        .join(item, left_on="ss_item_sk", right_on="i_item_sk")
        .filter(
            (col("d_year") == lit(year))
            & (col("d_moy") == lit(month))
            & (col("i_current_price") > lit(price_threshold))
        )
        .group_by("i_item_id", "ca_state")
        .agg(col("ss_sold_date_sk").count().alias("cnt"))
        .filter(col("cnt") >= lit(10))
        .sort("cnt", "i_item_id")
        .limit(100)
    )

    return result


def q6_pandas_impl(ctx: DataFrameContext) -> Any:
    """TPC-DS Q6: Customer State/Item Analysis (Pandas Family)."""
    params = get_parameters(6)
    year = params.get("year", 2001)
    month = params.get("month", 1)

    customer_address = ctx.get_table("customer_address")
    customer = ctx.get_table("customer")
    store_sales = ctx.get_table("store_sales")
    date_dim = ctx.get_table("date_dim")
    item = ctx.get_table("item")

    # Build joins
    merged = customer_address.merge(customer, left_on="ca_address_sk", right_on="c_current_addr_sk")
    merged = merged.merge(store_sales, left_on="c_customer_sk", right_on="ss_customer_sk")
    merged = merged.merge(date_dim, left_on="ss_sold_date_sk", right_on="d_date_sk")
    merged = merged.merge(item, left_on="ss_item_sk", right_on="i_item_sk")

    # Filter by date and price threshold
    avg_price = item["i_current_price"].mean()
    filtered = merged[
        (merged["d_year"] == year) & (merged["d_moy"] == month) & (merged["i_current_price"] > 1.2 * avg_price)
    ]

    # Group and apply having clause
    grouped = filtered.groupby(["i_item_id", "ca_state"], as_index=False).agg(cnt=("ss_sold_date_sk", "count"))

    result = grouped[grouped["cnt"] >= 10].sort_values(["cnt", "i_item_id"]).head(100)

    return result


def q12_expression_impl(ctx: DataFrameContext) -> Any:
    """TPC-DS Q12: Web Sales Item Analysis (Expression Family).

    Reports web sales by item category for specific categories and date range.

    Tables: web_sales, item, date_dim
    Pattern: Join -> filter -> group by -> aggregate with revenue percent
    """
    params = get_parameters(12)
    year = params.get("year", 1999)
    month = params.get("month", 2)
    categories = params.get("item_categories", ["Sports", "Books", "Home"])

    web_sales = ctx.get_table("web_sales")
    item = ctx.get_table("item")
    date_dim = ctx.get_table("date_dim")
    col = ctx.col
    lit = ctx.lit

    result = (
        web_sales.join(item, left_on="ws_item_sk", right_on="i_item_sk")
        .join(date_dim, left_on="ws_sold_date_sk", right_on="d_date_sk")
        .filter(col("i_category").is_in(categories) & (col("d_year") == lit(year)) & (col("d_moy") == lit(month)))
        .group_by("i_item_id", "i_item_desc", "i_category", "i_class", "i_current_price")
        .agg(col("ws_ext_sales_price").sum().alias("itemrevenue"))
        .sort(["i_category", "i_class", "itemrevenue", "i_item_id"], descending=[False, False, True, False])
        .limit(100)
    )

    return result


def q12_pandas_impl(ctx: DataFrameContext) -> Any:
    """TPC-DS Q12: Web Sales Item Analysis (Pandas Family)."""
    params = get_parameters(12)
    year = params.get("year", 1999)
    month = params.get("month", 2)
    categories = params.get("item_categories", ["Sports", "Books", "Home"])

    web_sales = ctx.get_table("web_sales")
    item = ctx.get_table("item")
    date_dim = ctx.get_table("date_dim")

    # Join tables
    merged = web_sales.merge(item, left_on="ws_item_sk", right_on="i_item_sk")
    merged = merged.merge(date_dim, left_on="ws_sold_date_sk", right_on="d_date_sk")

    # Filter
    filtered = merged[(merged["i_category"].isin(categories)) & (merged["d_year"] == year) & (merged["d_moy"] == month)]

    # Group and aggregate
    result = (
        filtered.groupby(["i_item_id", "i_item_desc", "i_category", "i_class", "i_current_price"], as_index=False)
        .agg(itemrevenue=("ws_ext_sales_price", "sum"))
        .sort_values(["i_category", "i_class", "itemrevenue", "i_item_id"], ascending=[True, True, False, True])
        .head(100)
    )

    return result


def q15_expression_impl(ctx: DataFrameContext) -> Any:
    """TPC-DS Q15: Catalog Sales Analysis (Expression Family).

    Reports catalog sales for customers in specific zip code ranges.

    Tables: catalog_sales, customer, customer_address, date_dim
    Pattern: Multi-join -> filter -> group by -> aggregate
    """

    params = get_parameters(15)
    year = params.get("year", 2001)
    quarter = params.get("quarter", 2)
    zip_prefix = params.get("zip_prefix", "85")

    catalog_sales = ctx.get_table("catalog_sales")
    customer = ctx.get_table("customer")
    customer_address = ctx.get_table("customer_address")
    date_dim = ctx.get_table("date_dim")
    col = ctx.col
    lit = ctx.lit

    result = (
        catalog_sales.join(customer, left_on="cs_bill_customer_sk", right_on="c_customer_sk")
        .join(customer_address, left_on="c_current_addr_sk", right_on="ca_address_sk")
        .join(date_dim, left_on="cs_sold_date_sk", right_on="d_date_sk")
        .filter(
            (col("d_qoy") == lit(quarter))
            & (col("d_year") == lit(year))
            & (col("ca_zip").cast_string().str.slice(0, 2) == lit(zip_prefix))
        )
        .group_by("ca_zip")
        .agg(col("cs_sales_price").sum().alias("total_sales"))
        .sort("ca_zip")
        .limit(100)
    )

    return result


def q15_pandas_impl(ctx: DataFrameContext) -> Any:
    """TPC-DS Q15: Catalog Sales Analysis (Pandas Family)."""
    params = get_parameters(15)
    year = params.get("year", 2001)
    quarter = params.get("quarter", 2)
    zip_prefix = params.get("zip_prefix", "85")

    catalog_sales = ctx.get_table("catalog_sales")
    customer = ctx.get_table("customer")
    customer_address = ctx.get_table("customer_address")
    date_dim = ctx.get_table("date_dim")

    # Join tables
    merged = catalog_sales.merge(customer, left_on="cs_bill_customer_sk", right_on="c_customer_sk")
    merged = merged.merge(customer_address, left_on="c_current_addr_sk", right_on="ca_address_sk")
    merged = merged.merge(date_dim, left_on="cs_sold_date_sk", right_on="d_date_sk")

    # Filter
    # Note: Use .astype(str) before .str accessor as ca_zip may not be string type
    filtered = merged[
        (merged["d_qoy"] == quarter) & (merged["d_year"] == year) & (merged["ca_zip"].astype(str).str[:2] == zip_prefix)
    ]

    # Group and aggregate
    result = (
        filtered.groupby(["ca_zip"], as_index=False)
        .agg(total_sales=("cs_sales_price", "sum"))
        .sort_values(["ca_zip"])
        .head(100)
    )

    return result


def q20_expression_impl(ctx: DataFrameContext) -> Any:
    """TPC-DS Q20: Catalog Sales Item Analysis (Expression Family).

    Reports catalog sales by item category for specific categories.

    Tables: catalog_sales, item, date_dim
    Pattern: Join -> filter -> group by -> aggregate
    """
    params = get_parameters(20)
    year = params.get("year", 1999)
    month = params.get("month", 2)
    categories = params.get("item_categories", ["Sports", "Books", "Home"])

    catalog_sales = ctx.get_table("catalog_sales")
    item = ctx.get_table("item")
    date_dim = ctx.get_table("date_dim")
    col = ctx.col
    lit = ctx.lit

    result = (
        catalog_sales.join(item, left_on="cs_item_sk", right_on="i_item_sk")
        .join(date_dim, left_on="cs_sold_date_sk", right_on="d_date_sk")
        .filter(col("i_category").is_in(categories) & (col("d_year") == lit(year)) & (col("d_moy") == lit(month)))
        .group_by("i_item_id", "i_item_desc", "i_category", "i_class", "i_current_price")
        .agg(col("cs_ext_sales_price").sum().alias("itemrevenue"))
        .sort(["i_category", "i_class", "itemrevenue", "i_item_id"], descending=[False, False, True, False])
        .limit(100)
    )

    return result


def q20_pandas_impl(ctx: DataFrameContext) -> Any:
    """TPC-DS Q20: Catalog Sales Item Analysis (Pandas Family)."""
    params = get_parameters(20)
    year = params.get("year", 1999)
    month = params.get("month", 2)
    categories = params.get("item_categories", ["Sports", "Books", "Home"])

    catalog_sales = ctx.get_table("catalog_sales")
    item = ctx.get_table("item")
    date_dim = ctx.get_table("date_dim")

    # Join tables
    merged = catalog_sales.merge(item, left_on="cs_item_sk", right_on="i_item_sk")
    merged = merged.merge(date_dim, left_on="cs_sold_date_sk", right_on="d_date_sk")

    # Filter
    filtered = merged[(merged["i_category"].isin(categories)) & (merged["d_year"] == year) & (merged["d_moy"] == month)]

    # Group and aggregate
    result = (
        filtered.groupby(["i_item_id", "i_item_desc", "i_category", "i_class", "i_current_price"], as_index=False)
        .agg(itemrevenue=("cs_ext_sales_price", "sum"))
        .sort_values(["i_category", "i_class", "itemrevenue", "i_item_id"], ascending=[True, True, False, True])
        .head(100)
    )

    return result


def q26_expression_impl(ctx: DataFrameContext) -> Any:
    """TPC-DS Q26: Catalog Sales Promo Analysis (Expression Family).

    Reports catalog sales promotion effectiveness by customer demographics.

    Tables: catalog_sales, customer_demographics, date_dim, item, promotion
    Pattern: Multi-join -> filter -> group by -> aggregate
    """
    params = get_parameters(26)
    year = params.get("year", 2000)

    catalog_sales = ctx.get_table("catalog_sales")
    customer_demographics = ctx.get_table("customer_demographics")
    date_dim = ctx.get_table("date_dim")
    item = ctx.get_table("item")
    promotion = ctx.get_table("promotion")
    col = ctx.col
    lit = ctx.lit

    result = (
        catalog_sales.join(customer_demographics, left_on="cs_bill_cdemo_sk", right_on="cd_demo_sk")
        .join(date_dim, left_on="cs_sold_date_sk", right_on="d_date_sk")
        .join(item, left_on="cs_item_sk", right_on="i_item_sk")
        .join(promotion, left_on="cs_promo_sk", right_on="p_promo_sk")
        .filter(
            (col("cd_gender") == lit("M"))
            & (col("cd_marital_status") == lit("S"))
            & (col("cd_education_status") == lit("College"))
            & ((col("p_channel_email") == lit("N")) | (col("p_channel_event") == lit("N")))
            & (col("d_year") == lit(year))
        )
        .group_by("i_item_id")
        .agg(
            col("cs_quantity").mean().alias("agg1"),
            col("cs_list_price").mean().alias("agg2"),
            col("cs_coupon_amt").mean().alias("agg3"),
            col("cs_sales_price").mean().alias("agg4"),
        )
        .sort("i_item_id")
        .limit(100)
    )

    return result


def q26_pandas_impl(ctx: DataFrameContext) -> Any:
    """TPC-DS Q26: Catalog Sales Promo Analysis (Pandas Family)."""
    params = get_parameters(26)
    year = params.get("year", 2000)

    catalog_sales = ctx.get_table("catalog_sales")
    customer_demographics = ctx.get_table("customer_demographics")
    date_dim = ctx.get_table("date_dim")
    item = ctx.get_table("item")
    promotion = ctx.get_table("promotion")

    # Join tables
    merged = catalog_sales.merge(customer_demographics, left_on="cs_bill_cdemo_sk", right_on="cd_demo_sk")
    merged = merged.merge(date_dim, left_on="cs_sold_date_sk", right_on="d_date_sk")
    merged = merged.merge(item, left_on="cs_item_sk", right_on="i_item_sk")
    merged = merged.merge(promotion, left_on="cs_promo_sk", right_on="p_promo_sk")

    # Filter
    filtered = merged[
        (merged["cd_gender"] == "M")
        & (merged["cd_marital_status"] == "S")
        & (merged["cd_education_status"] == "College")
        & ((merged["p_channel_email"] == "N") | (merged["p_channel_event"] == "N"))
        & (merged["d_year"] == year)
    ]

    # Group and aggregate
    result = (
        filtered.groupby(["i_item_id"], as_index=False)
        .agg(
            agg1=("cs_quantity", "mean"),
            agg2=("cs_list_price", "mean"),
            agg3=("cs_coupon_amt", "mean"),
            agg4=("cs_sales_price", "mean"),
        )
        .sort_values(["i_item_id"])
        .head(100)
    )

    return result


def q32_expression_impl(ctx: DataFrameContext) -> Any:
    """TPC-DS Q32: Catalog Sales Excess Discount (Expression Family).

    Finds items sold with excess discount compared to average.

    Tables: catalog_sales, item, date_dim
    Pattern: Subquery -> filter by computed threshold
    """
    params = get_parameters(32)
    year = params.get("year", 2000)
    days = params.get("days", 90)

    catalog_sales = ctx.get_table("catalog_sales")
    item = ctx.get_table("item")
    date_dim = ctx.get_table("date_dim")
    col = ctx.col
    lit = ctx.lit

    # Calculate average discount
    avg_discount = (
        catalog_sales.join(date_dim, left_on="cs_sold_date_sk", right_on="d_date_sk")
        .filter((col("d_year") == lit(year)) & (col("d_dom") <= lit(days)))
        .select(col("cs_ext_discount_amt").mean().alias("avg_discount"))
    )

    result = (
        catalog_sales.join(item, left_on="cs_item_sk", right_on="i_item_sk")
        .join(date_dim, left_on="cs_sold_date_sk", right_on="d_date_sk")
        .join(avg_discount, how="cross")
        .filter(
            (col("i_manufact_id") == lit(977))
            & (col("d_year") == lit(year))
            & (col("d_dom") <= lit(days))
            & (col("cs_ext_discount_amt") > lit(1.3) * col("avg_discount"))
        )
        .select(col("cs_ext_discount_amt").sum().alias("excess_discount_amount"))
    )

    return result


def q32_pandas_impl(ctx: DataFrameContext) -> Any:
    """TPC-DS Q32: Catalog Sales Excess Discount (Pandas Family)."""
    import pandas as pd

    params = get_parameters(32)
    year = params.get("year", 2000)
    days = params.get("days", 90)

    catalog_sales = ctx.get_table("catalog_sales")
    item = ctx.get_table("item")
    date_dim = ctx.get_table("date_dim")

    # Join for average calculation
    merged_for_avg = catalog_sales.merge(date_dim, left_on="cs_sold_date_sk", right_on="d_date_sk")
    filtered_for_avg = merged_for_avg[(merged_for_avg["d_year"] == year) & (merged_for_avg["d_dom"] <= days)]
    avg_discount = filtered_for_avg["cs_ext_discount_amt"].mean()

    # Main query
    merged = catalog_sales.merge(item, left_on="cs_item_sk", right_on="i_item_sk")
    merged = merged.merge(date_dim, left_on="cs_sold_date_sk", right_on="d_date_sk")

    filtered = merged[
        (merged["i_manufact_id"] == 977)
        & (merged["d_year"] == year)
        & (merged["d_dom"] <= days)
        & (merged["cs_ext_discount_amt"] > 1.3 * avg_discount)
    ]

    excess_discount_amount = filtered["cs_ext_discount_amt"].sum()
    result = pd.DataFrame({"excess_discount_amount": [excess_discount_amount]})

    return result


def q82_expression_impl(ctx: DataFrameContext) -> Any:
    """TPC-DS Q82: Store Sales Inventory (Expression Family).

    Reports items in inventory with specific price and date constraints.

    Tables: store_sales, item, inventory, date_dim
    Pattern: Multi-join -> filter -> distinct
    """
    params = get_parameters(82)
    year = params.get("year", 2000)
    month_start = params.get("month_start", 1)
    month_end = params.get("month_end", 6)
    price_min = params.get("price_min", 62)
    price_max = params.get("price_max", 92)

    item = ctx.get_table("item")
    inventory = ctx.get_table("inventory")
    date_dim = ctx.get_table("date_dim")
    store_sales = ctx.get_table("store_sales")
    col = ctx.col
    lit = ctx.lit

    result = (
        item.join(inventory, left_on="i_item_sk", right_on="inv_item_sk")
        .join(date_dim, left_on="inv_date_sk", right_on="d_date_sk")
        .join(store_sales, left_on="i_item_sk", right_on="ss_item_sk")
        .filter(
            (col("i_current_price") >= lit(price_min))
            & (col("i_current_price") <= lit(price_max))
            & (col("d_year") == lit(year))
            & (col("d_moy") >= lit(month_start))
            & (col("d_moy") <= lit(month_end))
            & (col("inv_quantity_on_hand") >= lit(100))
            & (col("inv_quantity_on_hand") <= lit(500))
        )
        .select("i_item_id", "i_item_desc", "i_current_price")
        .unique()
        .sort("i_item_id")
        .limit(100)
    )

    return result


def q82_pandas_impl(ctx: DataFrameContext) -> Any:
    """TPC-DS Q82: Store Sales Inventory (Pandas Family)."""
    params = get_parameters(82)
    year = params.get("year", 2000)
    month_start = params.get("month_start", 1)
    month_end = params.get("month_end", 6)
    price_min = params.get("price_min", 62)
    price_max = params.get("price_max", 92)

    item = ctx.get_table("item")
    inventory = ctx.get_table("inventory")
    date_dim = ctx.get_table("date_dim")
    store_sales = ctx.get_table("store_sales")

    # Filter early to reduce data volume before joins
    item_filtered = item[(item["i_current_price"] >= price_min) & (item["i_current_price"] <= price_max)]

    inv_filtered = inventory[(inventory["inv_quantity_on_hand"] >= 100) & (inventory["inv_quantity_on_hand"] <= 500)]

    date_filtered = date_dim[
        (date_dim["d_year"] == year) & (date_dim["d_moy"] >= month_start) & (date_dim["d_moy"] <= month_end)
    ]

    # Get items that exist in store_sales (semi-join via unique item_sk set)
    ss_items = store_sales["ss_item_sk"].unique()

    # Join filtered tables
    merged = item_filtered.merge(inv_filtered, left_on="i_item_sk", right_on="inv_item_sk")
    merged = merged.merge(date_filtered, left_on="inv_date_sk", right_on="d_date_sk")

    # Apply semi-join filter for store_sales existence
    merged = merged[merged["i_item_sk"].isin(ss_items)]

    # Distinct and sort
    result = (
        merged[["i_item_id", "i_item_desc", "i_current_price"]].drop_duplicates().sort_values(["i_item_id"]).head(100)
    )

    return result


def q92_expression_impl(ctx: DataFrameContext) -> Any:
    """TPC-DS Q92: Web Sales Discount (Expression Family).

    Finds web items sold with excess discount.

    Tables: web_sales, item, date_dim
    Pattern: Subquery -> filter by threshold
    """
    params = get_parameters(92)
    year = params.get("year", 2000)
    days = params.get("days", 90)

    web_sales = ctx.get_table("web_sales")
    item = ctx.get_table("item")
    date_dim = ctx.get_table("date_dim")
    col = ctx.col
    lit = ctx.lit

    # Calculate average discount
    avg_discount = (
        web_sales.join(date_dim, left_on="ws_sold_date_sk", right_on="d_date_sk")
        .filter((col("d_year") == lit(year)) & (col("d_dom") <= lit(days)))
        .select(col("ws_ext_discount_amt").mean().alias("avg_discount"))
    )

    result = (
        web_sales.join(item, left_on="ws_item_sk", right_on="i_item_sk")
        .join(date_dim, left_on="ws_sold_date_sk", right_on="d_date_sk")
        .join(avg_discount, how="cross")
        .filter(
            (col("i_manufact_id") == lit(350))
            & (col("d_year") == lit(year))
            & (col("d_dom") <= lit(days))
            & (col("ws_ext_discount_amt") > lit(1.3) * col("avg_discount"))
        )
        .select(col("ws_ext_discount_amt").sum().alias("excess_discount_amount"))
    )

    return result


def q92_pandas_impl(ctx: DataFrameContext) -> Any:
    """TPC-DS Q92: Web Sales Discount (Pandas Family)."""
    import pandas as pd

    params = get_parameters(92)
    year = params.get("year", 2000)
    days = params.get("days", 90)

    web_sales = ctx.get_table("web_sales")
    item = ctx.get_table("item")
    date_dim = ctx.get_table("date_dim")

    # Join for average calculation
    merged_for_avg = web_sales.merge(date_dim, left_on="ws_sold_date_sk", right_on="d_date_sk")
    filtered_for_avg = merged_for_avg[(merged_for_avg["d_year"] == year) & (merged_for_avg["d_dom"] <= days)]
    avg_discount = filtered_for_avg["ws_ext_discount_amt"].mean()

    # Main query
    merged = web_sales.merge(item, left_on="ws_item_sk", right_on="i_item_sk")
    merged = merged.merge(date_dim, left_on="ws_sold_date_sk", right_on="d_date_sk")

    filtered = merged[
        (merged["i_manufact_id"] == 350)
        & (merged["d_year"] == year)
        & (merged["d_dom"] <= days)
        & (merged["ws_ext_discount_amt"] > 1.3 * avg_discount)
    ]

    excess_discount_amount = filtered["ws_ext_discount_amt"].sum()
    result = pd.DataFrame({"excess_discount_amount": [excess_discount_amount]})

    return result


# =============================================================================
# Complex Queries - Window functions, UNION, and advanced patterns
# =============================================================================


def q37_expression_impl(ctx: DataFrameContext) -> Any:
    """TPC-DS Q37: Item Inventory Analysis (Expression Family).

    Reports items in inventory within price and date constraints.

    Tables: item, inventory, date_dim, catalog_sales
    Pattern: Multi-join -> filter -> distinct
    """
    params = get_parameters(37)
    year = params.get("year", 2000)
    month_start = params.get("month_start", 1)
    month_end = params.get("month_end", 6)
    price_min = params.get("current_price_min", 68)
    price_max = params.get("current_price_max", 98)

    item = ctx.get_table("item")
    inventory = ctx.get_table("inventory")
    date_dim = ctx.get_table("date_dim")
    catalog_sales = ctx.get_table("catalog_sales")
    col = ctx.col
    lit = ctx.lit

    result = (
        item.join(inventory, left_on="i_item_sk", right_on="inv_item_sk")
        .join(date_dim, left_on="inv_date_sk", right_on="d_date_sk")
        .join(catalog_sales, left_on="i_item_sk", right_on="cs_item_sk")
        .filter(
            (col("i_current_price") >= lit(price_min))
            & (col("i_current_price") <= lit(price_max))
            & (col("inv_quantity_on_hand") >= lit(100))
            & (col("inv_quantity_on_hand") <= lit(500))
            & (col("d_year") == lit(year))
            & (col("d_moy") >= lit(month_start))
            & (col("d_moy") <= lit(month_end))
        )
        .select("i_item_id", "i_item_desc", "i_current_price")
        .unique()
        .sort("i_item_id")
        .limit(100)
    )

    return result


def q37_pandas_impl(ctx: DataFrameContext) -> Any:
    """TPC-DS Q37: Item Inventory Analysis (Pandas Family)."""
    params = get_parameters(37)
    year = params.get("year", 2000)
    month_start = params.get("month_start", 1)
    month_end = params.get("month_end", 6)
    price_min = params.get("current_price_min", 68)
    price_max = params.get("current_price_max", 98)

    item = ctx.get_table("item")
    inventory = ctx.get_table("inventory")
    date_dim = ctx.get_table("date_dim")
    catalog_sales = ctx.get_table("catalog_sales")

    # Filter early to reduce data volume before joins
    item_filtered = item[(item["i_current_price"] >= price_min) & (item["i_current_price"] <= price_max)]

    inv_filtered = inventory[(inventory["inv_quantity_on_hand"] >= 100) & (inventory["inv_quantity_on_hand"] <= 500)]

    date_filtered = date_dim[
        (date_dim["d_year"] == year) & (date_dim["d_moy"] >= month_start) & (date_dim["d_moy"] <= month_end)
    ]

    # Get items that exist in catalog_sales (semi-join via unique item_sk set)
    cs_items = catalog_sales["cs_item_sk"].unique()

    # Join filtered tables
    merged = item_filtered.merge(inv_filtered, left_on="i_item_sk", right_on="inv_item_sk")
    merged = merged.merge(date_filtered, left_on="inv_date_sk", right_on="d_date_sk")

    # Apply semi-join filter for catalog_sales existence
    merged = merged[merged["i_item_sk"].isin(cs_items)]

    # Distinct and sort
    result = (
        merged[["i_item_id", "i_item_desc", "i_current_price"]].drop_duplicates().sort_values(["i_item_id"]).head(100)
    )

    return result


def q46_expression_impl(ctx: DataFrameContext) -> Any:
    """TPC-DS Q46: Store Sales Household Analysis (Expression Family).

    Reports store sales by customer household across cities.

    Tables: store_sales, date_dim, store, household_demographics, customer_address, customer
    Pattern: Multi-join -> filter -> group by -> aggregate -> order by
    """
    params = get_parameters(46)
    year = params.get("year", 2001)
    cities = params.get("cities", ["Fairview", "Midway", "Fairview", "Fairview", "Fairview"])

    store_sales = ctx.get_table("store_sales")
    date_dim = ctx.get_table("date_dim")
    store = ctx.get_table("store")
    household_demographics = ctx.get_table("household_demographics")
    customer_address = ctx.get_table("customer_address")
    customer = ctx.get_table("customer")
    col = ctx.col
    lit = ctx.lit

    result = (
        store_sales.join(date_dim, left_on="ss_sold_date_sk", right_on="d_date_sk")
        .join(store, left_on="ss_store_sk", right_on="s_store_sk")
        .join(household_demographics, left_on="ss_hdemo_sk", right_on="hd_demo_sk")
        .join(customer_address, left_on="ss_addr_sk", right_on="ca_address_sk")
        .join(customer, left_on="ss_customer_sk", right_on="c_customer_sk")
        .filter(
            (col("d_year") == lit(year))
            & (col("d_dom").is_between(1, 2))
            & col("ca_city").is_in(cities)
            & ((col("hd_dep_count") == lit(5)) | (col("hd_vehicle_count") == lit(3)))
        )
        .group_by("c_last_name", "c_first_name", "ca_city", "ss_ticket_number", "ss_coupon_amt", "ss_net_profit")
        .agg(col("ss_ext_sales_price").sum().alias("bought_city"))
        .sort("c_last_name", "c_first_name", "ca_city", "ss_ticket_number")
        .limit(100)
    )

    return result


def q46_pandas_impl(ctx: DataFrameContext) -> Any:
    """TPC-DS Q46: Store Sales Household Analysis (Pandas Family)."""
    params = get_parameters(46)
    year = params.get("year", 2001)
    cities = params.get("cities", ["Fairview", "Midway", "Fairview", "Fairview", "Fairview"])

    store_sales = ctx.get_table("store_sales")
    date_dim = ctx.get_table("date_dim")
    store = ctx.get_table("store")
    household_demographics = ctx.get_table("household_demographics")
    customer_address = ctx.get_table("customer_address")
    customer = ctx.get_table("customer")

    # Join tables
    merged = store_sales.merge(date_dim, left_on="ss_sold_date_sk", right_on="d_date_sk")
    merged = merged.merge(store, left_on="ss_store_sk", right_on="s_store_sk")
    merged = merged.merge(household_demographics, left_on="ss_hdemo_sk", right_on="hd_demo_sk")
    merged = merged.merge(customer_address, left_on="ss_addr_sk", right_on="ca_address_sk")
    merged = merged.merge(customer, left_on="ss_customer_sk", right_on="c_customer_sk")

    # Filter
    filtered = merged[
        (merged["d_year"] == year)
        & (merged["d_dom"] >= 1)
        & (merged["d_dom"] <= 2)
        & (merged["ca_city"].isin(cities))
        & ((merged["hd_dep_count"] == 5) | (merged["hd_vehicle_count"] == 3))
    ]

    # Group and aggregate
    result = (
        filtered.groupby(
            ["c_last_name", "c_first_name", "ca_city", "ss_ticket_number", "ss_coupon_amt", "ss_net_profit"],
            as_index=False,
        )
        .agg(bought_city=("ss_ext_sales_price", "sum"))
        .sort_values(["c_last_name", "c_first_name", "ca_city", "ss_ticket_number"])
        .head(100)
    )

    return result


def q50_expression_impl(ctx: DataFrameContext) -> Any:
    """TPC-DS Q50: Store Sales Returns Analysis (Expression Family).

    Analyzes store sales return patterns.

    Tables: store_sales, store_returns, store, date_dim
    Pattern: Multi-join -> filter -> group by -> aggregate -> order by
    """
    params = get_parameters(50)
    year = params.get("year", 2001)

    store_sales = ctx.get_table("store_sales")
    store_returns = ctx.get_table("store_returns")
    store = ctx.get_table("store")
    date_dim = ctx.get_table("date_dim")
    col = ctx.col
    lit = ctx.lit

    result = (
        store_sales.join(
            store_returns,
            left_on=["ss_item_sk", "ss_customer_sk", "ss_ticket_number"],
            right_on=["sr_item_sk", "sr_customer_sk", "sr_ticket_number"],
        )
        .join(store, left_on="ss_store_sk", right_on="s_store_sk")
        .join(date_dim, left_on="sr_returned_date_sk", right_on="d_date_sk")
        .filter((col("d_year") == lit(year)) & (col("d_moy") == lit(8)))
        .group_by("s_store_name", "s_company_id", "s_street_number", "s_street_name", "s_street_type")
        .agg(
            col("sr_return_amt").sum().alias("return_amt"),
            col("sr_net_loss").sum().alias("net_loss"),
        )
        .sort(["s_store_name", "s_company_id", "return_amt"], descending=[False, False, True])
        .limit(100)
    )

    return result


def q50_pandas_impl(ctx: DataFrameContext) -> Any:
    """TPC-DS Q50: Store Sales Returns Analysis (Pandas Family)."""
    params = get_parameters(50)
    year = params.get("year", 2001)

    store_sales = ctx.get_table("store_sales")
    store_returns = ctx.get_table("store_returns")
    store = ctx.get_table("store")
    date_dim = ctx.get_table("date_dim")

    # Join tables
    merged = store_sales.merge(
        store_returns,
        left_on=["ss_item_sk", "ss_customer_sk", "ss_ticket_number"],
        right_on=["sr_item_sk", "sr_customer_sk", "sr_ticket_number"],
    )
    merged = merged.merge(store, left_on="ss_store_sk", right_on="s_store_sk")
    merged = merged.merge(date_dim, left_on="sr_returned_date_sk", right_on="d_date_sk")

    # Filter
    filtered = merged[(merged["d_year"] == year) & (merged["d_moy"] == 8)]

    # Group and aggregate
    result = (
        filtered.groupby(
            ["s_store_name", "s_company_id", "s_street_number", "s_street_name", "s_street_type"],
            as_index=False,
        )
        .agg(return_amt=("sr_return_amt", "sum"), net_loss=("sr_net_loss", "sum"))
        .sort_values(["s_store_name", "s_company_id", "return_amt"], ascending=[True, True, False])
        .head(100)
    )

    return result


def q72_expression_impl(ctx: DataFrameContext) -> Any:
    """TPC-DS Q72: Catalog Sales Inventory (Expression Family).

    Reports catalog sales where inventory was insufficient (inv_quantity < cs_quantity)
    during the same week as the sale, with ship date > sold_date + 5 days.
    Counts promotional vs non-promotional sales.

    Tables: catalog_sales, inventory, date_dim (x3), item, warehouse,
            customer_demographics, household_demographics, promotion
    Pattern: Complex multi-join with week_seq correlation -> filter -> group by -> aggregate

    CRITICAL: The inventory join MUST use week_seq as part of the join key to prevent
    cartesian explosion. Joining catalog_sales × inventory on just item_sk produces
    ~910M intermediate rows at SF1. By joining on (item_sk, week_seq), we reduce this
    to ~17.5M rows.
    """

    params = get_parameters(72)
    year = params.get("year", 1999)
    buy_potential = params.get("buy_potential", ">10000")
    marital_status = params.get("marital_status", "D")

    catalog_sales = ctx.get_table("catalog_sales")
    inventory = ctx.get_table("inventory")
    date_dim = ctx.get_table("date_dim")
    item = ctx.get_table("item")
    warehouse = ctx.get_table("warehouse")
    customer_demographics = ctx.get_table("customer_demographics")
    household_demographics = ctx.get_table("household_demographics")
    promotion = ctx.get_table("promotion")
    col = ctx.col
    lit = ctx.lit

    # Step 1: Prepare catalog_sales with sold_date week_seq (d1)
    # Filter by year early to reduce data volume
    cs_with_d1 = catalog_sales.join(
        date_dim.select(
            col("d_date_sk").alias("d1_date_sk"),
            col("d_week_seq").alias("cs_week_seq"),
            col("d_year").alias("d1_year"),
            col("d_date").alias("d1_date"),
        ),
        left_on="cs_sold_date_sk",
        right_on="d1_date_sk",
    ).filter(col("d1_year") == lit(year))

    # Step 2: Prepare inventory with inv_date week_seq (d2)
    inv_with_d2 = inventory.join(
        date_dim.select(
            col("d_date_sk").alias("d2_date_sk"),
            col("d_week_seq").alias("inv_week_seq"),
        ),
        left_on="inv_date_sk",
        right_on="d2_date_sk",
    )

    # Step 3: Join on BOTH item_sk AND week_seq to prevent cartesian explosion
    result = (
        cs_with_d1.join(
            inv_with_d2,
            left_on=["cs_item_sk", "cs_week_seq"],
            right_on=["inv_item_sk", "inv_week_seq"],
        )
        # Join d3 for ship date filter
        .join(
            date_dim.select(
                col("d_date_sk").alias("d3_date_sk"),
                col("d_date").alias("d3_date"),
            ),
            left_on="cs_ship_date_sk",
            right_on="d3_date_sk",
        )
        # Join dimension tables
        .join(warehouse, left_on="inv_warehouse_sk", right_on="w_warehouse_sk")
        .join(item, left_on="cs_item_sk", right_on="i_item_sk")
        .join(customer_demographics, left_on="cs_bill_cdemo_sk", right_on="cd_demo_sk")
        .join(household_demographics, left_on="cs_bill_hdemo_sk", right_on="hd_demo_sk")
        # Left join promotion to identify promo vs non-promo sales
        .join(promotion, left_on="cs_promo_sk", right_on="p_promo_sk", how="left")
        # Apply remaining filter conditions
        .filter(
            (col("inv_quantity_on_hand") < col("cs_quantity"))
            & (col("d3_date") > col("d1_date"))  # Ship date after sold date
            & (col("hd_buy_potential") == lit(buy_potential))
            & (col("cd_marital_status") == lit(marital_status))
        )
        .group_by("i_item_desc", "w_warehouse_name", "cs_week_seq")
        .agg(
            # Use p_promo_id to check for promotion match (p_promo_sk is dropped as join key)
            ctx.when(col("p_promo_id").is_null()).then(1).otherwise(0).sum().alias("no_promo"),
            ctx.when(col("p_promo_id").is_not_null()).then(1).otherwise(0).sum().alias("promo"),
            ctx.len().alias("total_cnt"),
        )
        .sort(["total_cnt", "i_item_desc", "w_warehouse_name", "cs_week_seq"], descending=[True, False, False, False])
        .limit(100)
    )

    return result


def q72_pandas_impl(ctx: DataFrameContext) -> Any:
    """TPC-DS Q72: Catalog Sales Inventory (Pandas Family).

    See q72_expression_impl for detailed documentation.
    CRITICAL: Must join on (item_sk, week_seq) to prevent cartesian explosion.
    """
    params = get_parameters(72)
    year = params.get("year", 1999)
    buy_potential = params.get("buy_potential", ">10000")
    marital_status = params.get("marital_status", "D")

    catalog_sales = ctx.get_table("catalog_sales")
    inventory = ctx.get_table("inventory")
    date_dim = ctx.get_table("date_dim")
    item = ctx.get_table("item")
    warehouse = ctx.get_table("warehouse")
    customer_demographics = ctx.get_table("customer_demographics")
    household_demographics = ctx.get_table("household_demographics")
    promotion = ctx.get_table("promotion")

    # Step 1: Prepare catalog_sales with sold_date week_seq, filter by year early
    d1 = date_dim[["d_date_sk", "d_week_seq", "d_year", "d_date"]].rename(
        columns={"d_date_sk": "d1_date_sk", "d_week_seq": "cs_week_seq", "d_year": "d1_year", "d_date": "d1_date"}
    )
    cs_with_d1 = catalog_sales.merge(d1, left_on="cs_sold_date_sk", right_on="d1_date_sk")
    cs_with_d1 = cs_with_d1[cs_with_d1["d1_year"] == year]

    # Step 2: Prepare inventory with inv_date week_seq
    d2 = date_dim[["d_date_sk", "d_week_seq"]].rename(columns={"d_date_sk": "d2_date_sk", "d_week_seq": "inv_week_seq"})
    inv_with_d2 = inventory.merge(d2, left_on="inv_date_sk", right_on="d2_date_sk")

    # Step 3: Join on BOTH item_sk AND week_seq to prevent cartesian explosion
    merged = cs_with_d1.merge(
        inv_with_d2,
        left_on=["cs_item_sk", "cs_week_seq"],
        right_on=["inv_item_sk", "inv_week_seq"],
    )

    # Join d3 for ship date filter
    d3 = date_dim[["d_date_sk", "d_date"]].rename(columns={"d_date_sk": "d3_date_sk", "d_date": "d3_date"})
    merged = merged.merge(d3, left_on="cs_ship_date_sk", right_on="d3_date_sk")

    # Join dimension tables
    merged = merged.merge(warehouse, left_on="inv_warehouse_sk", right_on="w_warehouse_sk")
    merged = merged.merge(item, left_on="cs_item_sk", right_on="i_item_sk")
    merged = merged.merge(customer_demographics, left_on="cs_bill_cdemo_sk", right_on="cd_demo_sk")
    merged = merged.merge(household_demographics, left_on="cs_bill_hdemo_sk", right_on="hd_demo_sk")
    merged = merged.merge(promotion, left_on="cs_promo_sk", right_on="p_promo_sk", how="left")

    # Filter
    filtered = merged[
        (merged["inv_quantity_on_hand"] < merged["cs_quantity"])
        & (merged["d3_date"] > merged["d1_date"])
        & (merged["hd_buy_potential"] == buy_potential)
        & (merged["cd_marital_status"] == marital_status)
    ]

    # Add promo indicators (use p_promo_id since p_promo_sk may be dropped as join key)
    filtered = filtered.copy()
    filtered["no_promo"] = filtered["p_promo_id"].isna().astype(int)
    filtered["promo"] = filtered["p_promo_id"].notna().astype(int)

    # Group and aggregate
    result = (
        filtered.groupby(["i_item_desc", "w_warehouse_name", "cs_week_seq"], as_index=False)
        .agg(no_promo=("no_promo", "sum"), promo=("promo", "sum"), total_cnt=("i_item_desc", "count"))
        .sort_values(
            ["total_cnt", "i_item_desc", "w_warehouse_name", "cs_week_seq"], ascending=[False, True, True, True]
        )
        .head(100)
    )

    return result


# =============================================================================
# Additional Simple Queries - Multi-join with CASE WHEN aggregations
# =============================================================================


def q62_expression_impl(ctx: DataFrameContext) -> Any:
    """TPC-DS Q62: Web Sales Delivery Analysis (Expression Family).

    Analyzes web sales delivery times by warehouse, ship mode, and web site.
    Uses CASE WHEN to bucket delivery times into ranges.

    Tables: web_sales, warehouse, ship_mode, web_site, date_dim
    Pattern: Multi-join -> filter by month_seq -> CASE WHEN aggregation -> group by -> order by
    """

    params = get_parameters(62)
    dms = params.get("dms", 1200)

    web_sales = ctx.get_table("web_sales")
    warehouse = ctx.get_table("warehouse")
    ship_mode = ctx.get_table("ship_mode")
    web_site = ctx.get_table("web_site")
    date_dim = ctx.get_table("date_dim")
    col = ctx.col

    # Join all tables
    result = (
        web_sales.join(date_dim, left_on="ws_ship_date_sk", right_on="d_date_sk")
        .join(warehouse, left_on="ws_warehouse_sk", right_on="w_warehouse_sk")
        .join(ship_mode, left_on="ws_ship_mode_sk", right_on="sm_ship_mode_sk")
        .join(web_site, left_on="ws_web_site_sk", right_on="web_site_sk")
        .filter(col("d_month_seq").is_between(dms, dms + 11))
        .with_columns(
            # Compute delivery days difference
            (col("ws_ship_date_sk") - col("ws_sold_date_sk")).alias("delivery_days")
        )
        .group_by(
            col("w_warehouse_name").str.slice(0, 20).alias("warehouse_name"),
            col("sm_type"),
            col("web_name"),
        )
        .agg(
            ctx.when(col("delivery_days") <= 30).then(1).otherwise(0).sum().alias("30_days"),
            ctx.when((col("delivery_days") > 30) & (col("delivery_days") <= 60))
            .then(1)
            .otherwise(0)
            .sum()
            .alias("31_60_days"),
            ctx.when((col("delivery_days") > 60) & (col("delivery_days") <= 90))
            .then(1)
            .otherwise(0)
            .sum()
            .alias("61_90_days"),
            ctx.when((col("delivery_days") > 90) & (col("delivery_days") <= 120))
            .then(1)
            .otherwise(0)
            .sum()
            .alias("91_120_days"),
            ctx.when(col("delivery_days") > 120).then(1).otherwise(0).sum().alias("gt_120_days"),
        )
        .sort("warehouse_name", "sm_type", "web_name")
        .limit(100)
    )

    return result


def q62_pandas_impl(ctx: DataFrameContext) -> Any:
    """TPC-DS Q62: Web Sales Delivery Analysis (Pandas Family)."""
    params = get_parameters(62)
    dms = params.get("dms", 1200)

    web_sales = ctx.get_table("web_sales")
    warehouse = ctx.get_table("warehouse")
    ship_mode = ctx.get_table("ship_mode")
    web_site = ctx.get_table("web_site")
    date_dim = ctx.get_table("date_dim")

    # Join all tables
    merged = web_sales.merge(date_dim, left_on="ws_ship_date_sk", right_on="d_date_sk")
    merged = merged.merge(warehouse, left_on="ws_warehouse_sk", right_on="w_warehouse_sk")
    merged = merged.merge(ship_mode, left_on="ws_ship_mode_sk", right_on="sm_ship_mode_sk")
    merged = merged.merge(web_site, left_on="ws_web_site_sk", right_on="web_site_sk")

    # Filter by month sequence
    filtered = merged[(merged["d_month_seq"] >= dms) & (merged["d_month_seq"] <= dms + 11)]

    # Compute delivery days
    filtered = filtered.copy()
    filtered["delivery_days"] = filtered["ws_ship_date_sk"] - filtered["ws_sold_date_sk"]
    filtered["warehouse_name"] = filtered["w_warehouse_name"].str[:20]

    # Create CASE WHEN columns
    filtered["30_days"] = (filtered["delivery_days"] <= 30).astype(int)
    filtered["31_60_days"] = ((filtered["delivery_days"] > 30) & (filtered["delivery_days"] <= 60)).astype(int)
    filtered["61_90_days"] = ((filtered["delivery_days"] > 60) & (filtered["delivery_days"] <= 90)).astype(int)
    filtered["91_120_days"] = ((filtered["delivery_days"] > 90) & (filtered["delivery_days"] <= 120)).astype(int)
    filtered["gt_120_days"] = (filtered["delivery_days"] > 120).astype(int)

    # Group and aggregate
    result = (
        filtered.groupby(["warehouse_name", "sm_type", "web_name"], as_index=False)
        .agg(
            {
                "30_days": "sum",
                "31_60_days": "sum",
                "61_90_days": "sum",
                "91_120_days": "sum",
                "gt_120_days": "sum",
            }
        )
        .sort_values(["warehouse_name", "sm_type", "web_name"])
        .head(100)
    )

    return result


def q99_expression_impl(ctx: DataFrameContext) -> Any:
    """TPC-DS Q99: Catalog Sales Delivery Analysis (Expression Family).

    Analyzes catalog sales delivery times by warehouse, ship mode, and call center.
    Uses CASE WHEN to bucket delivery times into ranges.

    Tables: catalog_sales, warehouse, ship_mode, call_center, date_dim
    Pattern: Multi-join -> filter by month_seq -> CASE WHEN aggregation -> group by -> order by
    """

    params = get_parameters(99)
    dms = params.get("dms", 1200)

    catalog_sales = ctx.get_table("catalog_sales")
    warehouse = ctx.get_table("warehouse")
    ship_mode = ctx.get_table("ship_mode")
    call_center = ctx.get_table("call_center")
    date_dim = ctx.get_table("date_dim")
    col = ctx.col

    # Join all tables
    result = (
        catalog_sales.join(date_dim, left_on="cs_ship_date_sk", right_on="d_date_sk")
        .join(warehouse, left_on="cs_warehouse_sk", right_on="w_warehouse_sk")
        .join(ship_mode, left_on="cs_ship_mode_sk", right_on="sm_ship_mode_sk")
        .join(call_center, left_on="cs_call_center_sk", right_on="cc_call_center_sk")
        .filter(col("d_month_seq").is_between(dms, dms + 11))
        .with_columns(
            # Compute delivery days difference
            (col("cs_ship_date_sk") - col("cs_sold_date_sk")).alias("delivery_days")
        )
        .group_by(
            col("w_warehouse_name").str.slice(0, 20).alias("warehouse_name"),
            col("sm_type"),
            col("cc_name"),
        )
        .agg(
            ctx.when(col("delivery_days") <= 30).then(1).otherwise(0).sum().alias("30_days"),
            ctx.when((col("delivery_days") > 30) & (col("delivery_days") <= 60))
            .then(1)
            .otherwise(0)
            .sum()
            .alias("31_60_days"),
            ctx.when((col("delivery_days") > 60) & (col("delivery_days") <= 90))
            .then(1)
            .otherwise(0)
            .sum()
            .alias("61_90_days"),
            ctx.when((col("delivery_days") > 90) & (col("delivery_days") <= 120))
            .then(1)
            .otherwise(0)
            .sum()
            .alias("91_120_days"),
            ctx.when(col("delivery_days") > 120).then(1).otherwise(0).sum().alias("gt_120_days"),
        )
        .sort("warehouse_name", "sm_type", "cc_name")
        .limit(100)
    )

    return result


def q99_pandas_impl(ctx: DataFrameContext) -> Any:
    """TPC-DS Q99: Catalog Sales Delivery Analysis (Pandas Family)."""
    params = get_parameters(99)
    dms = params.get("dms", 1200)

    catalog_sales = ctx.get_table("catalog_sales")
    warehouse = ctx.get_table("warehouse")
    ship_mode = ctx.get_table("ship_mode")
    call_center = ctx.get_table("call_center")
    date_dim = ctx.get_table("date_dim")

    # Join all tables
    merged = catalog_sales.merge(date_dim, left_on="cs_ship_date_sk", right_on="d_date_sk")
    merged = merged.merge(warehouse, left_on="cs_warehouse_sk", right_on="w_warehouse_sk")
    merged = merged.merge(ship_mode, left_on="cs_ship_mode_sk", right_on="sm_ship_mode_sk")
    merged = merged.merge(call_center, left_on="cs_call_center_sk", right_on="cc_call_center_sk")

    # Filter by month sequence
    filtered = merged[(merged["d_month_seq"] >= dms) & (merged["d_month_seq"] <= dms + 11)]

    # Compute delivery days
    filtered = filtered.copy()
    filtered["delivery_days"] = filtered["cs_ship_date_sk"] - filtered["cs_sold_date_sk"]
    filtered["warehouse_name"] = filtered["w_warehouse_name"].str[:20]

    # Create CASE WHEN columns
    filtered["30_days"] = (filtered["delivery_days"] <= 30).astype(int)
    filtered["31_60_days"] = ((filtered["delivery_days"] > 30) & (filtered["delivery_days"] <= 60)).astype(int)
    filtered["61_90_days"] = ((filtered["delivery_days"] > 60) & (filtered["delivery_days"] <= 90)).astype(int)
    filtered["91_120_days"] = ((filtered["delivery_days"] > 90) & (filtered["delivery_days"] <= 120)).astype(int)
    filtered["gt_120_days"] = (filtered["delivery_days"] > 120).astype(int)

    # Group and aggregate
    result = (
        filtered.groupby(["warehouse_name", "sm_type", "cc_name"], as_index=False)
        .agg(
            {
                "30_days": "sum",
                "31_60_days": "sum",
                "61_90_days": "sum",
                "91_120_days": "sum",
                "gt_120_days": "sum",
            }
        )
        .sort_values(["warehouse_name", "sm_type", "cc_name"])
        .head(100)
    )

    return result


def q13_expression_impl(ctx: DataFrameContext) -> Any:
    """TPC-DS Q13: Store Sales Demographics Analysis (Expression Family).

    Computes aggregate statistics for store sales filtered by demographics
    and geographic criteria. Returns a single row with AVG and SUM values.

    Tables: store_sales, store, customer_demographics, household_demographics, customer_address, date_dim
    Pattern: Multi-join -> complex OR filter -> aggregate (no GROUP BY)
    """
    params = get_parameters(13)
    year = params.get("year", 2001)

    store_sales = ctx.get_table("store_sales")
    store = ctx.get_table("store")
    customer_demographics = ctx.get_table("customer_demographics")
    household_demographics = ctx.get_table("household_demographics")
    customer_address = ctx.get_table("customer_address")
    date_dim = ctx.get_table("date_dim")
    col = ctx.col
    lit = ctx.lit

    # Join tables
    result = (
        store_sales.join(store, left_on="ss_store_sk", right_on="s_store_sk")
        .join(customer_demographics, left_on="ss_cdemo_sk", right_on="cd_demo_sk")
        .join(household_demographics, left_on="ss_hdemo_sk", right_on="hd_demo_sk")
        .join(customer_address, left_on="ss_addr_sk", right_on="ca_address_sk")
        .join(date_dim, left_on="ss_sold_date_sk", right_on="d_date_sk")
        .filter(
            (col("d_year") == lit(year))
            & (
                # Demographics condition 1
                (
                    (col("cd_marital_status") == lit("M"))
                    & (col("cd_education_status") == lit("Advanced Degree"))
                    & col("ss_sales_price").is_between(100.0, 150.0)
                    & (col("hd_dep_count") == lit(3))
                )
                |
                # Demographics condition 2
                (
                    (col("cd_marital_status") == lit("S"))
                    & (col("cd_education_status") == lit("College"))
                    & col("ss_sales_price").is_between(50.0, 100.0)
                    & (col("hd_dep_count") == lit(1))
                )
                |
                # Demographics condition 3
                (
                    (col("cd_marital_status") == lit("W"))
                    & (col("cd_education_status") == lit("2 yr Degree"))
                    & col("ss_sales_price").is_between(150.0, 200.0)
                    & (col("hd_dep_count") == lit(1))
                )
            )
            & (
                # Address condition 1
                (
                    (col("ca_country") == lit("United States"))
                    & col("ca_state").is_in(["TX", "OH", "TX"])
                    & col("ss_net_profit").is_between(100, 200)
                )
                |
                # Address condition 2
                (
                    (col("ca_country") == lit("United States"))
                    & col("ca_state").is_in(["OR", "NM", "KY"])
                    & col("ss_net_profit").is_between(150, 300)
                )
                |
                # Address condition 3
                (
                    (col("ca_country") == lit("United States"))
                    & col("ca_state").is_in(["VA", "TX", "MS"])
                    & col("ss_net_profit").is_between(50, 250)
                )
            )
        )
        .select(
            col("ss_quantity").mean().alias("avg_ss_quantity"),
            col("ss_ext_sales_price").mean().alias("avg_ss_ext_sales_price"),
            col("ss_ext_wholesale_cost").mean().alias("avg_ss_ext_wholesale_cost"),
            col("ss_ext_wholesale_cost").sum().alias("sum_ss_ext_wholesale_cost"),
        )
    )

    return result


def q13_pandas_impl(ctx: DataFrameContext) -> Any:
    """TPC-DS Q13: Store Sales Demographics Analysis (Pandas Family)."""
    import pandas as pd

    params = get_parameters(13)
    year = params.get("year", 2001)

    store_sales = ctx.get_table("store_sales")
    store = ctx.get_table("store")
    customer_demographics = ctx.get_table("customer_demographics")
    household_demographics = ctx.get_table("household_demographics")
    customer_address = ctx.get_table("customer_address")
    date_dim = ctx.get_table("date_dim")

    # Join tables
    merged = store_sales.merge(store, left_on="ss_store_sk", right_on="s_store_sk")
    merged = merged.merge(customer_demographics, left_on="ss_cdemo_sk", right_on="cd_demo_sk")
    merged = merged.merge(household_demographics, left_on="ss_hdemo_sk", right_on="hd_demo_sk")
    merged = merged.merge(customer_address, left_on="ss_addr_sk", right_on="ca_address_sk")
    merged = merged.merge(date_dim, left_on="ss_sold_date_sk", right_on="d_date_sk")

    # Demographics conditions
    demo_cond1 = (
        (merged["cd_marital_status"] == "M")
        & (merged["cd_education_status"] == "Advanced Degree")
        & (merged["ss_sales_price"] >= 100.0)
        & (merged["ss_sales_price"] <= 150.0)
        & (merged["hd_dep_count"] == 3)
    )
    demo_cond2 = (
        (merged["cd_marital_status"] == "S")
        & (merged["cd_education_status"] == "College")
        & (merged["ss_sales_price"] >= 50.0)
        & (merged["ss_sales_price"] <= 100.0)
        & (merged["hd_dep_count"] == 1)
    )
    demo_cond3 = (
        (merged["cd_marital_status"] == "W")
        & (merged["cd_education_status"] == "2 yr Degree")
        & (merged["ss_sales_price"] >= 150.0)
        & (merged["ss_sales_price"] <= 200.0)
        & (merged["hd_dep_count"] == 1)
    )

    # Address conditions
    addr_cond1 = (
        (merged["ca_country"] == "United States")
        & merged["ca_state"].isin(["TX", "OH", "TX"])
        & (merged["ss_net_profit"] >= 100)
        & (merged["ss_net_profit"] <= 200)
    )
    addr_cond2 = (
        (merged["ca_country"] == "United States")
        & merged["ca_state"].isin(["OR", "NM", "KY"])
        & (merged["ss_net_profit"] >= 150)
        & (merged["ss_net_profit"] <= 300)
    )
    addr_cond3 = (
        (merged["ca_country"] == "United States")
        & merged["ca_state"].isin(["VA", "TX", "MS"])
        & (merged["ss_net_profit"] >= 50)
        & (merged["ss_net_profit"] <= 250)
    )

    # Apply filters
    filtered = merged[
        (merged["d_year"] == year) & (demo_cond1 | demo_cond2 | demo_cond3) & (addr_cond1 | addr_cond2 | addr_cond3)
    ]

    # Aggregate (single row result)
    result = pd.DataFrame(
        {
            "avg_ss_quantity": [filtered["ss_quantity"].mean()],
            "avg_ss_ext_sales_price": [filtered["ss_ext_sales_price"].mean()],
            "avg_ss_ext_wholesale_cost": [filtered["ss_ext_wholesale_cost"].mean()],
            "sum_ss_ext_wholesale_cost": [filtered["ss_ext_wholesale_cost"].sum()],
        }
    )

    return result


def q48_expression_impl(ctx: DataFrameContext) -> Any:
    """TPC-DS Q48: Store Sales Quantity Demographics (Expression Family).

    Computes sum of store sales quantity filtered by demographics
    and geographic criteria. Returns a single value.

    Tables: store_sales, store, customer_demographics, customer_address, date_dim
    Pattern: Multi-join -> complex OR filter -> aggregate (no GROUP BY)
    """
    params = get_parameters(48)
    year = params.get("year", 2000)

    store_sales = ctx.get_table("store_sales")
    store = ctx.get_table("store")
    customer_demographics = ctx.get_table("customer_demographics")
    customer_address = ctx.get_table("customer_address")
    date_dim = ctx.get_table("date_dim")
    col = ctx.col
    lit = ctx.lit

    # Join tables
    result = (
        store_sales.join(store, left_on="ss_store_sk", right_on="s_store_sk")
        .join(customer_demographics, left_on="ss_cdemo_sk", right_on="cd_demo_sk")
        .join(customer_address, left_on="ss_addr_sk", right_on="ca_address_sk")
        .join(date_dim, left_on="ss_sold_date_sk", right_on="d_date_sk")
        .filter(
            (col("d_year") == lit(year))
            & (
                # Demographics condition 1
                (
                    (col("cd_marital_status") == lit("M"))
                    & (col("cd_education_status") == lit("Advanced Degree"))
                    & col("ss_sales_price").is_between(100.0, 150.0)
                )
                |
                # Demographics condition 2
                (
                    (col("cd_marital_status") == lit("S"))
                    & (col("cd_education_status") == lit("College"))
                    & col("ss_sales_price").is_between(50.0, 100.0)
                )
                |
                # Demographics condition 3
                (
                    (col("cd_marital_status") == lit("W"))
                    & (col("cd_education_status") == lit("2 yr Degree"))
                    & col("ss_sales_price").is_between(150.0, 200.0)
                )
            )
            & (
                # Address condition 1
                (
                    (col("ca_country") == lit("United States"))
                    & col("ca_state").is_in(["TX", "OH", "TX"])
                    & col("ss_net_profit").is_between(0, 2000)
                )
                |
                # Address condition 2
                (
                    (col("ca_country") == lit("United States"))
                    & col("ca_state").is_in(["OR", "NM", "KY"])
                    & col("ss_net_profit").is_between(150, 3000)
                )
                |
                # Address condition 3
                (
                    (col("ca_country") == lit("United States"))
                    & col("ca_state").is_in(["VA", "TX", "MS"])
                    & col("ss_net_profit").is_between(50, 25000)
                )
            )
        )
        .select(col("ss_quantity").sum().alias("sum_ss_quantity"))
    )

    return result


def q48_pandas_impl(ctx: DataFrameContext) -> Any:
    """TPC-DS Q48: Store Sales Quantity Demographics (Pandas Family)."""
    import pandas as pd

    params = get_parameters(48)
    year = params.get("year", 2000)

    store_sales = ctx.get_table("store_sales")
    store = ctx.get_table("store")
    customer_demographics = ctx.get_table("customer_demographics")
    customer_address = ctx.get_table("customer_address")
    date_dim = ctx.get_table("date_dim")

    # Join tables
    merged = store_sales.merge(store, left_on="ss_store_sk", right_on="s_store_sk")
    merged = merged.merge(customer_demographics, left_on="ss_cdemo_sk", right_on="cd_demo_sk")
    merged = merged.merge(customer_address, left_on="ss_addr_sk", right_on="ca_address_sk")
    merged = merged.merge(date_dim, left_on="ss_sold_date_sk", right_on="d_date_sk")

    # Demographics conditions
    demo_cond1 = (
        (merged["cd_marital_status"] == "M")
        & (merged["cd_education_status"] == "Advanced Degree")
        & (merged["ss_sales_price"] >= 100.0)
        & (merged["ss_sales_price"] <= 150.0)
    )
    demo_cond2 = (
        (merged["cd_marital_status"] == "S")
        & (merged["cd_education_status"] == "College")
        & (merged["ss_sales_price"] >= 50.0)
        & (merged["ss_sales_price"] <= 100.0)
    )
    demo_cond3 = (
        (merged["cd_marital_status"] == "W")
        & (merged["cd_education_status"] == "2 yr Degree")
        & (merged["ss_sales_price"] >= 150.0)
        & (merged["ss_sales_price"] <= 200.0)
    )

    # Address conditions
    addr_cond1 = (
        (merged["ca_country"] == "United States")
        & merged["ca_state"].isin(["TX", "OH", "TX"])
        & (merged["ss_net_profit"] >= 0)
        & (merged["ss_net_profit"] <= 2000)
    )
    addr_cond2 = (
        (merged["ca_country"] == "United States")
        & merged["ca_state"].isin(["OR", "NM", "KY"])
        & (merged["ss_net_profit"] >= 150)
        & (merged["ss_net_profit"] <= 3000)
    )
    addr_cond3 = (
        (merged["ca_country"] == "United States")
        & merged["ca_state"].isin(["VA", "TX", "MS"])
        & (merged["ss_net_profit"] >= 50)
        & (merged["ss_net_profit"] <= 25000)
    )

    # Apply filters
    filtered = merged[
        (merged["d_year"] == year) & (demo_cond1 | demo_cond2 | demo_cond3) & (addr_cond1 | addr_cond2 | addr_cond3)
    ]

    # Aggregate (single value result)
    result = pd.DataFrame({"sum_ss_quantity": [filtered["ss_quantity"].sum()]})

    return result


def q34_expression_impl(ctx: DataFrameContext) -> Any:
    """TPC-DS Q34: Store Sales County Household Analysis (Expression Family).

    Analyzes customer purchases by county and household demographics.
    Uses a subquery to filter tickets with counts in a specific range.

    Tables: store_sales, date_dim, store, household_demographics, customer
    Pattern: Subquery (group by ticket) -> filter by count -> join customer -> order by
    """
    params = get_parameters(34)
    year = params.get("year", 1998)
    counties = params.get(
        "counties",
        [
            "Williamson County",
            "Williamson County",
            "Williamson County",
            "Williamson County",
            "Williamson County",
            "Williamson County",
            "Williamson County",
            "Williamson County",
        ],
    )
    buy_potential_1 = params.get("buy_potential_1", "1001-5000")
    buy_potential_2 = params.get("buy_potential_2", "0-500")

    store_sales = ctx.get_table("store_sales")
    date_dim = ctx.get_table("date_dim")
    store = ctx.get_table("store")
    household_demographics = ctx.get_table("household_demographics")
    customer = ctx.get_table("customer")
    col = ctx.col
    lit = ctx.lit

    # Subquery: aggregate by ticket
    ticket_agg = (
        store_sales.join(date_dim, left_on="ss_sold_date_sk", right_on="d_date_sk")
        .join(store, left_on="ss_store_sk", right_on="s_store_sk")
        .join(household_demographics, left_on="ss_hdemo_sk", right_on="hd_demo_sk")
        .filter(
            (
                (col("d_dom") >= lit(1)) & (col("d_dom") <= lit(3))
                | (col("d_dom") >= lit(25)) & (col("d_dom") <= lit(28))
            )
            & ((col("hd_buy_potential") == lit(buy_potential_1)) | (col("hd_buy_potential") == lit(buy_potential_2)))
            & (col("hd_vehicle_count") > lit(0))
            & ((col("hd_dep_count") / col("hd_vehicle_count")) > lit(1.2))
            & (col("d_year").is_in([year, year + 1, year + 2]))
            & col("s_county").is_in(counties)
        )
        .group_by("ss_ticket_number", "ss_customer_sk")
        .agg(col("ss_ticket_number").count().alias("cnt"))
        .filter((col("cnt") >= lit(15)) & (col("cnt") <= lit(20)))
    )

    # Join with customer
    result = (
        ticket_agg.join(customer, left_on="ss_customer_sk", right_on="c_customer_sk")
        .select(
            col("c_last_name"),
            col("c_first_name"),
            col("c_salutation"),
            col("c_preferred_cust_flag"),
            col("ss_ticket_number"),
            col("cnt"),
        )
        .sort(
            ["c_last_name", "c_first_name", "c_salutation", "c_preferred_cust_flag", "ss_ticket_number"],
            descending=[False, False, False, True, False],
        )
    )

    return result


def q34_pandas_impl(ctx: DataFrameContext) -> Any:
    """TPC-DS Q34: Store Sales County Household Analysis (Pandas Family)."""
    params = get_parameters(34)
    year = params.get("year", 1998)
    counties = params.get(
        "counties",
        [
            "Williamson County",
            "Williamson County",
            "Williamson County",
            "Williamson County",
            "Williamson County",
            "Williamson County",
            "Williamson County",
            "Williamson County",
        ],
    )
    buy_potential_1 = params.get("buy_potential_1", "1001-5000")
    buy_potential_2 = params.get("buy_potential_2", "0-500")

    store_sales = ctx.get_table("store_sales")
    date_dim = ctx.get_table("date_dim")
    store = ctx.get_table("store")
    household_demographics = ctx.get_table("household_demographics")
    customer = ctx.get_table("customer")

    # Join tables for subquery
    merged = store_sales.merge(date_dim, left_on="ss_sold_date_sk", right_on="d_date_sk")
    merged = merged.merge(store, left_on="ss_store_sk", right_on="s_store_sk")
    merged = merged.merge(household_demographics, left_on="ss_hdemo_sk", right_on="hd_demo_sk")

    # Filter conditions
    dom_cond = ((merged["d_dom"] >= 1) & (merged["d_dom"] <= 3)) | ((merged["d_dom"] >= 25) & (merged["d_dom"] <= 28))
    bp_cond = (merged["hd_buy_potential"] == buy_potential_1) | (merged["hd_buy_potential"] == buy_potential_2)
    vehicle_cond = (merged["hd_vehicle_count"] > 0) & (merged["hd_dep_count"] / merged["hd_vehicle_count"] > 1.2)
    year_cond = merged["d_year"].isin([year, year + 1, year + 2])
    county_cond = merged["s_county"].isin(counties)

    filtered = merged[dom_cond & bp_cond & vehicle_cond & year_cond & county_cond]

    # Aggregate by ticket (use ctx.groupby_size for Dask compatibility)
    ticket_agg = ctx.groupby_size(filtered, ["ss_ticket_number", "ss_customer_sk"], name="cnt")

    # Filter by count range
    ticket_filtered = ticket_agg[(ticket_agg["cnt"] >= 15) & (ticket_agg["cnt"] <= 20)]

    # Join with customer
    result = ticket_filtered.merge(customer, left_on="ss_customer_sk", right_on="c_customer_sk")

    # Select and sort
    result = result[
        ["c_last_name", "c_first_name", "c_salutation", "c_preferred_cust_flag", "ss_ticket_number", "cnt"]
    ].sort_values(
        ["c_last_name", "c_first_name", "c_salutation", "c_preferred_cust_flag", "ss_ticket_number"],
        ascending=[True, True, True, False, True],
    )

    return result


def q45_expression_impl(ctx: DataFrameContext) -> Any:
    """TPC-DS Q45: Web Sales Customer Zip Analysis (Expression Family).

    Reports web sales by customer zip code for customers in specific zip codes
    or purchasing specific items.

    Tables: web_sales, customer, customer_address, date_dim, item
    Pattern: Multi-join -> OR filter (zip OR item_id) -> group by -> aggregate
    """

    params = get_parameters(45)
    year = params.get("year", 2001)
    qoy = params.get("qoy", 2)
    zip_codes = params.get("zip_codes", ["85669", "86197", "88274", "83405", "86475"])
    item_sks = params.get("item_sks", [2, 3, 5, 7, 11, 13, 17, 19, 23, 29])

    web_sales = ctx.get_table("web_sales")
    customer = ctx.get_table("customer")
    customer_address = ctx.get_table("customer_address")
    date_dim = ctx.get_table("date_dim")
    item = ctx.get_table("item")
    col = ctx.col
    lit = ctx.lit

    # Get item IDs for the specified item_sks - use platform-agnostic collect_column_as_list
    item_ids_list = (
        item.filter(col("i_item_sk").is_in(item_sks)).select("i_item_id").unique().collect_column_as_list("i_item_id")
    )

    # Join tables and filter
    result = (
        web_sales.join(customer, left_on="ws_bill_customer_sk", right_on="c_customer_sk")
        .join(customer_address, left_on="c_current_addr_sk", right_on="ca_address_sk")
        .join(item, left_on="ws_item_sk", right_on="i_item_sk")
        .join(date_dim, left_on="ws_sold_date_sk", right_on="d_date_sk")
        .filter(
            (col("d_qoy") == lit(qoy))
            & (col("d_year") == lit(year))
            & (col("ca_zip").cast_string().str.slice(0, 5).is_in(zip_codes) | col("i_item_id").is_in(item_ids_list))
        )
        .group_by(col("ca_zip"), col("ca_city"))
        .agg(col("ws_sales_price").sum().alias("sum_ws_sales_price"))
        .sort("ca_zip", "ca_city")
        .limit(100)
    )

    return result


def q45_pandas_impl(ctx: DataFrameContext) -> Any:
    """TPC-DS Q45: Web Sales Customer Zip Analysis (Pandas Family)."""
    params = get_parameters(45)
    year = params.get("year", 2001)
    qoy = params.get("qoy", 2)
    zip_codes = params.get("zip_codes", ["85669", "86197", "88274", "83405", "86475"])
    item_sks = params.get("item_sks", [2, 3, 5, 7, 11, 13, 17, 19, 23, 29])

    web_sales = ctx.get_table("web_sales")
    customer = ctx.get_table("customer")
    customer_address = ctx.get_table("customer_address")
    date_dim = ctx.get_table("date_dim")
    item = ctx.get_table("item")

    # Get item IDs for the specified item_sks
    item_ids = item[item["i_item_sk"].isin(item_sks)]["i_item_id"].unique().tolist()

    # Join tables
    merged = web_sales.merge(customer, left_on="ws_bill_customer_sk", right_on="c_customer_sk")
    merged = merged.merge(customer_address, left_on="c_current_addr_sk", right_on="ca_address_sk")
    merged = merged.merge(item, left_on="ws_item_sk", right_on="i_item_sk")
    merged = merged.merge(date_dim, left_on="ws_sold_date_sk", right_on="d_date_sk")

    # Filter
    filtered = merged[
        (merged["d_qoy"] == qoy)
        & (merged["d_year"] == year)
        & (merged["ca_zip"].str[:5].isin(zip_codes) | merged["i_item_id"].isin(item_ids))
    ]

    # Group and aggregate
    result = (
        filtered.groupby(["ca_zip", "ca_city"], as_index=False)
        .agg(sum_ws_sales_price=("ws_sales_price", "sum"))
        .sort_values(["ca_zip", "ca_city"])
        .head(100)
    )

    return result


def q90_expression_impl(ctx: DataFrameContext) -> Any:
    """TPC-DS Q90: Web Sales AM/PM Ratio Analysis (Expression Family).

    Computes the ratio of AM to PM web sales counts for specific
    time ranges and demographics.

    Tables: web_sales, household_demographics, time_dim, web_page
    Pattern: Two separate counts -> divide to get ratio
    """

    params = get_parameters(90)
    hour_am = params.get("hour_am", 8)
    hour_pm = params.get("hour_pm", 19)
    dep_count = params.get("dep_count", 6)
    char_count_min = params.get("char_count_min", 5000)
    char_count_max = params.get("char_count_max", 5200)

    web_sales = ctx.get_table("web_sales")
    household_demographics = ctx.get_table("household_demographics")
    time_dim = ctx.get_table("time_dim")
    web_page = ctx.get_table("web_page")
    col = ctx.col
    lit = ctx.lit

    # Base join for both AM and PM counts
    base = (
        web_sales.join(time_dim, left_on="ws_sold_time_sk", right_on="t_time_sk")
        .join(household_demographics, left_on="ws_ship_hdemo_sk", right_on="hd_demo_sk")
        .join(web_page, left_on="ws_web_page_sk", right_on="wp_web_page_sk")
        .filter(
            (col("hd_dep_count") == lit(dep_count)) & col("wp_char_count").is_between(char_count_min, char_count_max)
        )
    )

    # AM count (hour between hour_am and hour_am+1)
    am_count = base.filter(col("t_hour").is_between(hour_am, hour_am + 1)).select(ctx.count().alias("amc"))

    # PM count (hour between hour_pm and hour_pm+1)
    pm_count = base.filter(col("t_hour").is_between(hour_pm, hour_pm + 1)).select(ctx.count().alias("pmc"))

    # Compute ratio using platform-agnostic scalar extraction
    am_val = am_count.scalar(0, 0)
    pm_val = pm_count.scalar(0, 0)

    # Return as DataFrame with the ratio
    ratio = am_val / pm_val if pm_val and pm_val > 0 else None
    result = ctx.create_dataframe({"am_pm_ratio": [ratio]})

    return result


def q90_pandas_impl(ctx: DataFrameContext) -> Any:
    """TPC-DS Q90: Web Sales AM/PM Ratio Analysis (Pandas Family)."""
    import pandas as pd

    params = get_parameters(90)
    hour_am = params.get("hour_am", 8)
    hour_pm = params.get("hour_pm", 19)
    dep_count = params.get("dep_count", 6)
    char_count_min = params.get("char_count_min", 5000)
    char_count_max = params.get("char_count_max", 5200)

    web_sales = ctx.get_table("web_sales")
    household_demographics = ctx.get_table("household_demographics")
    time_dim = ctx.get_table("time_dim")
    web_page = ctx.get_table("web_page")

    # Join tables
    merged = web_sales.merge(time_dim, left_on="ws_sold_time_sk", right_on="t_time_sk")
    merged = merged.merge(household_demographics, left_on="ws_ship_hdemo_sk", right_on="hd_demo_sk")
    merged = merged.merge(web_page, left_on="ws_web_page_sk", right_on="wp_web_page_sk")

    # Base filter
    base = merged[
        (merged["hd_dep_count"] == dep_count)
        & (merged["wp_char_count"] >= char_count_min)
        & (merged["wp_char_count"] <= char_count_max)
    ]

    # AM count
    am_count = len(base[(base["t_hour"] >= hour_am) & (base["t_hour"] <= hour_am + 1)])

    # PM count
    pm_count = len(base[(base["t_hour"] >= hour_pm) & (base["t_hour"] <= hour_pm + 1)])

    # Compute ratio
    ratio = am_count / pm_count if pm_count > 0 else None

    return pd.DataFrame({"am_pm_ratio": [ratio]})


def q91_expression_impl(ctx: DataFrameContext) -> Any:
    """TPC-DS Q91: Call Center Returns Analysis (Expression Family).

    Reports catalog returns by call center with demographic filtering.

    Tables: call_center, catalog_returns, date_dim, customer, customer_address,
            customer_demographics, household_demographics
    Pattern: Multi-join -> demographics filter -> group by -> aggregate
    """
    params = get_parameters(91)
    year = params.get("year", 1998)
    month = params.get("month", 11)
    buy_potential = params.get("buy_potential", "Unknown")
    gmt_offset = params.get("gmt_offset", -7)

    call_center = ctx.get_table("call_center")
    catalog_returns = ctx.get_table("catalog_returns")
    date_dim = ctx.get_table("date_dim")
    customer = ctx.get_table("customer")
    customer_address = ctx.get_table("customer_address")
    customer_demographics = ctx.get_table("customer_demographics")
    household_demographics = ctx.get_table("household_demographics")
    col = ctx.col
    lit = ctx.lit

    result = (
        catalog_returns.join(call_center, left_on="cr_call_center_sk", right_on="cc_call_center_sk")
        .join(date_dim, left_on="cr_returned_date_sk", right_on="d_date_sk")
        .join(customer, left_on="cr_returning_customer_sk", right_on="c_customer_sk")
        .join(customer_demographics, left_on="c_current_cdemo_sk", right_on="cd_demo_sk")
        .join(household_demographics, left_on="c_current_hdemo_sk", right_on="hd_demo_sk")
        .join(customer_address, left_on="c_current_addr_sk", right_on="ca_address_sk")
        .filter(
            (col("d_year") == lit(year))
            & (col("d_moy") == lit(month))
            & (
                ((col("cd_marital_status") == lit("M")) & (col("cd_education_status") == lit("Unknown")))
                | ((col("cd_marital_status") == lit("W")) & (col("cd_education_status") == lit("Advanced Degree")))
            )
            & col("hd_buy_potential").str.starts_with(buy_potential)
            & (col("ca_gmt_offset") == lit(gmt_offset))
        )
        .group_by(
            "cc_call_center_id",
            "cc_name",
            "cc_manager",
            "cd_marital_status",
            "cd_education_status",
        )
        .agg(col("cr_net_loss").sum().alias("returns_loss"))
        .sort("returns_loss", descending=True)
    )

    return result


def q91_pandas_impl(ctx: DataFrameContext) -> Any:
    """TPC-DS Q91: Call Center Returns Analysis (Pandas Family)."""
    params = get_parameters(91)
    year = params.get("year", 1998)
    month = params.get("month", 11)
    buy_potential = params.get("buy_potential", "Unknown")
    gmt_offset = params.get("gmt_offset", -7)

    call_center = ctx.get_table("call_center")
    catalog_returns = ctx.get_table("catalog_returns")
    date_dim = ctx.get_table("date_dim")
    customer = ctx.get_table("customer")
    customer_address = ctx.get_table("customer_address")
    customer_demographics = ctx.get_table("customer_demographics")
    household_demographics = ctx.get_table("household_demographics")

    # Join tables
    merged = catalog_returns.merge(call_center, left_on="cr_call_center_sk", right_on="cc_call_center_sk")
    merged = merged.merge(date_dim, left_on="cr_returned_date_sk", right_on="d_date_sk")
    merged = merged.merge(customer, left_on="cr_returning_customer_sk", right_on="c_customer_sk")
    merged = merged.merge(customer_demographics, left_on="c_current_cdemo_sk", right_on="cd_demo_sk")
    merged = merged.merge(household_demographics, left_on="c_current_hdemo_sk", right_on="hd_demo_sk")
    merged = merged.merge(customer_address, left_on="c_current_addr_sk", right_on="ca_address_sk")

    # Demographics conditions
    demo_cond = ((merged["cd_marital_status"] == "M") & (merged["cd_education_status"] == "Unknown")) | (
        (merged["cd_marital_status"] == "W") & (merged["cd_education_status"] == "Advanced Degree")
    )

    # Filter
    filtered = merged[
        (merged["d_year"] == year)
        & (merged["d_moy"] == month)
        & demo_cond
        & merged["hd_buy_potential"].str.startswith(buy_potential)
        & (merged["ca_gmt_offset"] == gmt_offset)
    ]

    # Group and aggregate
    result = (
        filtered.groupby(
            ["cc_call_center_id", "cc_name", "cc_manager", "cd_marital_status", "cd_education_status"],
            as_index=False,
        )
        .agg(returns_loss=("cr_net_loss", "sum"))
        .sort_values("returns_loss", ascending=False)
    )

    return result


def q30_expression_impl(ctx: DataFrameContext) -> Any:
    """TPC-DS Q30: Web Returns Customer Analysis (Expression Family).

    Identifies customers whose web return amounts exceed 1.2x the average
    return amount for their state.

    Tables: web_returns, date_dim, customer_address, customer
    Pattern: CTE (aggregate by customer/state) -> correlated filter (> state avg) -> join customer
    """
    params = get_parameters(30)
    year = params.get("year", 2002)
    state = params.get("state", "GA")

    web_returns = ctx.get_table("web_returns")
    date_dim = ctx.get_table("date_dim")
    customer_address = ctx.get_table("customer_address")
    customer = ctx.get_table("customer")
    col = ctx.col
    lit = ctx.lit

    # CTE: customer_total_return - aggregate returns by customer and state
    ctr = (
        web_returns.join(date_dim, left_on="wr_returned_date_sk", right_on="d_date_sk")
        .join(customer_address, left_on="wr_returning_addr_sk", right_on="ca_address_sk")
        .filter(col("d_year") == lit(year))
        .group_by(
            col("wr_returning_customer_sk").alias("ctr_customer_sk"),
            col("ca_state").alias("ctr_state"),
        )
        .agg(col("wr_return_amt").sum().alias("ctr_total_return"))
    )

    # Calculate state averages
    state_avg = ctr.group_by("ctr_state").agg(col("ctr_total_return").mean().alias("state_avg"))

    # Join ctr with state averages and filter
    ctr_with_avg = ctr.join(state_avg, on="ctr_state")
    ctr_filtered = ctr_with_avg.filter(col("ctr_total_return") > col("state_avg") * 1.2)

    # Get customers in the target state
    ca_filtered = customer_address.filter(col("ca_state") == lit(state))

    # Join with customer
    result = (
        ctr_filtered.join(customer, left_on="ctr_customer_sk", right_on="c_customer_sk")
        .join(ca_filtered, left_on="c_current_addr_sk", right_on="ca_address_sk")
        .select(
            "c_customer_id",
            "c_salutation",
            "c_first_name",
            "c_last_name",
            "c_preferred_cust_flag",
            "c_birth_day",
            "c_birth_month",
            "c_birth_year",
            "c_birth_country",
            "c_login",
            "c_email_address",
            "c_last_review_date_sk",
            "ctr_total_return",
        )
        .sort(
            "c_customer_id",
            "c_salutation",
            "c_first_name",
            "c_last_name",
            "c_preferred_cust_flag",
            "c_birth_day",
            "c_birth_month",
            "c_birth_year",
            "c_birth_country",
            "c_login",
            "c_email_address",
            "c_last_review_date_sk",
            "ctr_total_return",
        )
        .limit(100)
    )

    return result


def q30_pandas_impl(ctx: DataFrameContext) -> Any:
    """TPC-DS Q30: Web Returns Customer Analysis (Pandas Family)."""
    params = get_parameters(30)
    year = params.get("year", 2002)
    state = params.get("state", "GA")

    web_returns = ctx.get_table("web_returns")
    date_dim = ctx.get_table("date_dim")
    customer_address = ctx.get_table("customer_address")
    customer = ctx.get_table("customer")

    # CTE: customer_total_return
    merged = web_returns.merge(date_dim, left_on="wr_returned_date_sk", right_on="d_date_sk")
    merged = merged.merge(customer_address, left_on="wr_returning_addr_sk", right_on="ca_address_sk")
    merged = merged[merged["d_year"] == year]

    ctr = merged.groupby(["wr_returning_customer_sk", "ca_state"], as_index=False).agg(
        ctr_total_return=("wr_return_amt", "sum")
    )
    ctr = ctr.rename(columns={"wr_returning_customer_sk": "ctr_customer_sk", "ca_state": "ctr_state"})

    # Calculate state averages
    state_avg = ctr.groupby("ctr_state", as_index=False).agg(state_avg=("ctr_total_return", "mean"))

    # Join and filter
    ctr_with_avg = ctr.merge(state_avg, on="ctr_state")
    ctr_filtered = ctr_with_avg[ctr_with_avg["ctr_total_return"] > ctr_with_avg["state_avg"] * 1.2]

    # Get customers in target state
    ca_filtered = customer_address[customer_address["ca_state"] == state]

    # Join with customer
    result = ctr_filtered.merge(customer, left_on="ctr_customer_sk", right_on="c_customer_sk")
    result = result.merge(ca_filtered, left_on="c_current_addr_sk", right_on="ca_address_sk")

    # Select and sort
    cols = [
        "c_customer_id",
        "c_salutation",
        "c_first_name",
        "c_last_name",
        "c_preferred_cust_flag",
        "c_birth_day",
        "c_birth_month",
        "c_birth_year",
        "c_birth_country",
        "c_login",
        "c_email_address",
        "c_last_review_date_sk",
        "ctr_total_return",
    ]
    result = result[cols].sort_values(cols).head(100)

    return result


def q81_expression_impl(ctx: DataFrameContext) -> Any:
    """TPC-DS Q81: Catalog Returns Customer Analysis (Expression Family).

    Identifies customers whose catalog return amounts exceed 1.2x the average
    return amount for their state.

    Tables: catalog_returns, date_dim, customer_address, customer
    Pattern: CTE (aggregate by customer/state) -> correlated filter (> state avg) -> join customer
    """
    params = get_parameters(81)
    year = params.get("year", 2000)
    state = params.get("state", "GA")

    catalog_returns = ctx.get_table("catalog_returns")
    date_dim = ctx.get_table("date_dim")
    customer_address = ctx.get_table("customer_address")
    customer = ctx.get_table("customer")
    col = ctx.col
    lit = ctx.lit

    # CTE: customer_total_return - aggregate returns by customer and state
    ctr = (
        catalog_returns.join(date_dim, left_on="cr_returned_date_sk", right_on="d_date_sk")
        .join(customer_address, left_on="cr_returning_addr_sk", right_on="ca_address_sk")
        .filter(col("d_year") == lit(year))
        .group_by(
            col("cr_returning_customer_sk").alias("ctr_customer_sk"),
            col("ca_state").alias("ctr_state"),
        )
        .agg(col("cr_return_amt_inc_tax").sum().alias("ctr_total_return"))
    )

    # Calculate state averages
    state_avg = ctr.group_by("ctr_state").agg(col("ctr_total_return").mean().alias("state_avg"))

    # Join ctr with state averages and filter
    ctr_with_avg = ctr.join(state_avg, on="ctr_state")
    ctr_filtered = ctr_with_avg.filter(col("ctr_total_return") > col("state_avg") * 1.2)

    # Get customers in the target state
    ca_filtered = customer_address.filter(col("ca_state") == lit(state))

    # Join with customer
    result = (
        ctr_filtered.join(customer, left_on="ctr_customer_sk", right_on="c_customer_sk")
        .join(ca_filtered, left_on="c_current_addr_sk", right_on="ca_address_sk")
        .select(
            "c_customer_id",
            "c_salutation",
            "c_first_name",
            "c_last_name",
            "ca_street_number",
            "ca_street_name",
            "ca_street_type",
            "ca_suite_number",
            "ca_city",
            "ca_county",
            col("ca_state").alias("ca_state_out"),
            "ca_zip",
            "ca_country",
            "ca_gmt_offset",
            "ca_location_type",
            "ctr_total_return",
        )
        .sort(
            "c_customer_id",
            "c_salutation",
            "c_first_name",
            "c_last_name",
            "ca_street_number",
            "ca_street_name",
            "ca_street_type",
            "ca_suite_number",
            "ca_city",
            "ca_county",
            "ca_state_out",
            "ca_zip",
            "ca_country",
            "ca_gmt_offset",
            "ca_location_type",
            "ctr_total_return",
        )
        .limit(100)
    )

    return result


def q81_pandas_impl(ctx: DataFrameContext) -> Any:
    """TPC-DS Q81: Catalog Returns Customer Analysis (Pandas Family)."""
    params = get_parameters(81)
    year = params.get("year", 2000)
    state = params.get("state", "GA")

    catalog_returns = ctx.get_table("catalog_returns")
    date_dim = ctx.get_table("date_dim")
    customer_address = ctx.get_table("customer_address")
    customer = ctx.get_table("customer")

    # CTE: customer_total_return
    merged = catalog_returns.merge(date_dim, left_on="cr_returned_date_sk", right_on="d_date_sk")
    merged = merged.merge(customer_address, left_on="cr_returning_addr_sk", right_on="ca_address_sk")
    merged = merged[merged["d_year"] == year]

    ctr = merged.groupby(["cr_returning_customer_sk", "ca_state"], as_index=False).agg(
        ctr_total_return=("cr_return_amt_inc_tax", "sum")
    )
    ctr = ctr.rename(columns={"cr_returning_customer_sk": "ctr_customer_sk", "ca_state": "ctr_state"})

    # Calculate state averages
    state_avg = ctr.groupby("ctr_state", as_index=False).agg(state_avg=("ctr_total_return", "mean"))

    # Join and filter
    ctr_with_avg = ctr.merge(state_avg, on="ctr_state")
    ctr_filtered = ctr_with_avg[ctr_with_avg["ctr_total_return"] > ctr_with_avg["state_avg"] * 1.2]

    # Get customers in target state
    ca_filtered = customer_address[customer_address["ca_state"] == state]

    # Join with customer
    result = ctr_filtered.merge(customer, left_on="ctr_customer_sk", right_on="c_customer_sk")
    result = result.merge(ca_filtered, left_on="c_current_addr_sk", right_on="ca_address_sk")

    # Select and sort
    cols = [
        "c_customer_id",
        "c_salutation",
        "c_first_name",
        "c_last_name",
        "ca_street_number",
        "ca_street_name",
        "ca_street_type",
        "ca_suite_number",
        "ca_city",
        "ca_county",
        "ca_state_y",
        "ca_zip",
        "ca_country",
        "ca_gmt_offset",
        "ca_location_type",
        "ctr_total_return",
    ]
    # Handle column name conflicts from merge
    available_cols = [c for c in cols if c in result.columns]
    result = result[available_cols].sort_values(available_cols).head(100)

    return result


# =============================================================================
# Q83: Cross-Channel Returns Analysis
# =============================================================================


def q83_expression_impl(ctx: DataFrameContext) -> Any:
    """TPC-DS Q83: Cross-Channel Returns Analysis (Expression Family).

    Compares return quantities across store, catalog, and web channels
    for items returned during specific date ranges.
    """

    params = get_parameters(83)
    return_dates = params.get("return_dates", ["2000-06-30", "2000-09-27", "2000-11-17"])

    col = ctx.col
    lit = ctx.lit

    # Get tables
    store_returns = ctx.get_table("store_returns")
    catalog_returns = ctx.get_table("catalog_returns")
    web_returns = ctx.get_table("web_returns")
    item = ctx.get_table("item")
    date_dim = ctx.get_table("date_dim")

    lit = ctx.lit

    # Get week sequences for the return dates
    # First, find the d_date rows for these specific dates
    date_filter = lit(False)
    for dt in return_dates:
        date_filter = date_filter | (col("d_date").cast_string().str.starts_with(dt))

    target_weeks = date_dim.filter(date_filter).select(col("d_week_seq")).unique()

    # Get all dates in those week sequences
    valid_dates = date_dim.join(target_weeks, on="d_week_seq").select(col("d_date_sk")).unique()

    # CTE 1: sr_items - Store Returns by item
    sr_items = (
        store_returns.join(item, left_on="sr_item_sk", right_on="i_item_sk")
        .join(valid_dates, left_on="sr_returned_date_sk", right_on="d_date_sk")
        .group_by(col("i_item_id").alias("item_id"))
        .agg(col("sr_return_quantity").sum().alias("sr_item_qty"))
    )

    # CTE 2: cr_items - Catalog Returns by item
    cr_items = (
        catalog_returns.join(item, left_on="cr_item_sk", right_on="i_item_sk")
        .join(valid_dates, left_on="cr_returned_date_sk", right_on="d_date_sk")
        .group_by(col("i_item_id").alias("item_id"))
        .agg(col("cr_return_quantity").sum().alias("cr_item_qty"))
    )

    # CTE 3: wr_items - Web Returns by item
    wr_items = (
        web_returns.join(item, left_on="wr_item_sk", right_on="i_item_sk")
        .join(valid_dates, left_on="wr_returned_date_sk", right_on="d_date_sk")
        .group_by(col("i_item_id").alias("item_id"))
        .agg(col("wr_return_quantity").sum().alias("wr_item_qty"))
    )

    # Join all three CTEs
    result = (
        sr_items.join(cr_items, on="item_id")
        .join(wr_items, on="item_id")
        .with_columns((col("sr_item_qty") + col("cr_item_qty") + col("wr_item_qty")).alias("total_qty"))
        .with_columns(
            ((col("sr_item_qty") / col("total_qty") / lit(3.0)) * lit(100.0)).alias("sr_dev"),
            ((col("cr_item_qty") / col("total_qty") / lit(3.0)) * lit(100.0)).alias("cr_dev"),
            ((col("wr_item_qty") / col("total_qty") / lit(3.0)) * lit(100.0)).alias("wr_dev"),
            (col("total_qty") / lit(3.0)).alias("average"),
        )
        .select(
            col("item_id"),
            col("sr_item_qty"),
            col("sr_dev"),
            col("cr_item_qty"),
            col("cr_dev"),
            col("wr_item_qty"),
            col("wr_dev"),
            col("average"),
        )
        .sort(["item_id", "sr_item_qty"])
        .head(100)
    )

    return result


def q83_pandas_impl(ctx: DataFrameContext) -> Any:
    """TPC-DS Q83: Cross-Channel Returns Analysis (Pandas Family)."""
    params = get_parameters(83)
    return_dates = params.get("return_dates", ["2000-06-30", "2000-09-27", "2000-11-17"])

    # Get tables
    store_returns = ctx.get_table("store_returns")
    catalog_returns = ctx.get_table("catalog_returns")
    web_returns = ctx.get_table("web_returns")
    item = ctx.get_table("item")
    date_dim = ctx.get_table("date_dim")

    # Get week sequences for the return dates
    date_dim["d_date_str"] = date_dim["d_date"].astype(str)
    target_weeks = date_dim[date_dim["d_date_str"].str[:10].isin(return_dates)]["d_week_seq"].unique()

    # Get all dates in those week sequences
    valid_date_sks = date_dim[date_dim["d_week_seq"].isin(target_weeks)]["d_date_sk"].unique()

    # CTE 1: sr_items - Store Returns by item
    sr_merged = store_returns.merge(item, left_on="sr_item_sk", right_on="i_item_sk")
    sr_merged = sr_merged[sr_merged["sr_returned_date_sk"].isin(valid_date_sks)]
    sr_items = sr_merged.groupby("i_item_id", as_index=False).agg(sr_item_qty=("sr_return_quantity", "sum"))
    sr_items = sr_items.rename(columns={"i_item_id": "item_id"})

    # CTE 2: cr_items - Catalog Returns by item
    cr_merged = catalog_returns.merge(item, left_on="cr_item_sk", right_on="i_item_sk")
    cr_merged = cr_merged[cr_merged["cr_returned_date_sk"].isin(valid_date_sks)]
    cr_items = cr_merged.groupby("i_item_id", as_index=False).agg(cr_item_qty=("cr_return_quantity", "sum"))
    cr_items = cr_items.rename(columns={"i_item_id": "item_id"})

    # CTE 3: wr_items - Web Returns by item
    wr_merged = web_returns.merge(item, left_on="wr_item_sk", right_on="i_item_sk")
    wr_merged = wr_merged[wr_merged["wr_returned_date_sk"].isin(valid_date_sks)]
    wr_items = wr_merged.groupby("i_item_id", as_index=False).agg(wr_item_qty=("wr_return_quantity", "sum"))
    wr_items = wr_items.rename(columns={"i_item_id": "item_id"})

    # Join all three CTEs
    result = sr_items.merge(cr_items, on="item_id").merge(wr_items, on="item_id")
    result["total_qty"] = result["sr_item_qty"] + result["cr_item_qty"] + result["wr_item_qty"]
    result["sr_dev"] = (result["sr_item_qty"] / result["total_qty"] / 3.0) * 100.0
    result["cr_dev"] = (result["cr_item_qty"] / result["total_qty"] / 3.0) * 100.0
    result["wr_dev"] = (result["wr_item_qty"] / result["total_qty"] / 3.0) * 100.0
    result["average"] = result["total_qty"] / 3.0

    # Select and sort
    result = result[["item_id", "sr_item_qty", "sr_dev", "cr_item_qty", "cr_dev", "wr_item_qty", "wr_dev", "average"]]
    result = result.sort_values(["item_id", "sr_item_qty"]).head(100)

    return result


# =============================================================================
# Q41: Item Dimension Analysis
# =============================================================================


def q41_expression_impl(ctx: DataFrameContext) -> Any:
    """TPC-DS Q41: Item Dimension Analysis (Expression Family).

    Finds distinct product names from items matching manufacturer and attribute criteria.
    This query only uses the item dimension table (no fact tables).
    """
    params = get_parameters(41)
    manufact_start = params.get("manufact_start", 738)

    col = ctx.col
    lit = ctx.lit

    item = ctx.get_table("item")

    # Filter i1: items within the manufact_id range
    i1 = item.filter((col("i_manufact_id") >= lit(manufact_start)) & (col("i_manufact_id") <= lit(manufact_start + 40)))

    # Build the complex filter for correlated subquery
    # The subquery counts items with matching i_manufact and specific attribute combinations
    # We implement this as a self-join pattern

    # Define the 8 OR conditions (simplified version - actual TPC-DS has parameterized colors/units/sizes)
    # For the DataFrame implementation, we use representative default values

    # Get all item data for the correlation
    all_items = item.select(
        col("i_manufact"),
        col("i_category"),
        col("i_color"),
        col("i_units"),
        col("i_size"),
    )

    # Build filter conditions matching the SQL pattern
    # Using OR of 8 conditions grouped into two sets of 4
    condition = (
        # Group 1: Women + various color/unit/size combos
        ((col("i_category") == lit("Women")) & col("i_color").is_in(["powder", "khaki"]))
        | ((col("i_category") == lit("Women")) & col("i_color").is_in(["brown", "honeydew"]))
        # Group 2: Men + various color/unit/size combos
        | ((col("i_category") == lit("Men")) & col("i_color").is_in(["floral", "deep"]))
        | ((col("i_category") == lit("Men")) & col("i_color").is_in(["light", "cornflower"]))
    )

    # Get manufacturers that have matching items
    matching_manufacts = all_items.filter(condition).select(col("i_manufact")).unique()

    # Join back to i1 to find items whose manufacturer has matching items
    result = (
        i1.join(matching_manufacts, on="i_manufact")
        .select(col("i_product_name"))
        .unique()
        .sort("i_product_name")
        .head(100)
    )

    return result


def q41_pandas_impl(ctx: DataFrameContext) -> Any:
    """TPC-DS Q41: Item Dimension Analysis (Pandas Family)."""
    params = get_parameters(41)
    manufact_start = params.get("manufact_start", 738)

    item = ctx.get_table("item")

    # Filter i1: items within the manufact_id range
    i1 = item[(item["i_manufact_id"] >= manufact_start) & (item["i_manufact_id"] <= manufact_start + 40)]

    # Build the complex filter for correlated subquery
    # The subquery counts items with matching i_manufact and specific attribute combinations

    # Build filter conditions matching the SQL pattern
    condition = (
        # Group 1: Women + various color combos
        ((item["i_category"] == "Women") & item["i_color"].isin(["powder", "khaki"]))
        | ((item["i_category"] == "Women") & item["i_color"].isin(["brown", "honeydew"]))
        # Group 2: Men + various color combos
        | ((item["i_category"] == "Men") & item["i_color"].isin(["floral", "deep"]))
        | ((item["i_category"] == "Men") & item["i_color"].isin(["light", "cornflower"]))
    )

    # Get manufacturers that have matching items
    matching_manufacts = item[condition]["i_manufact"].unique()

    # Filter i1 to items whose manufacturer has matching items
    result = i1[i1["i_manufact"].isin(matching_manufacts)][["i_product_name"]].drop_duplicates()
    result = result.sort_values("i_product_name").head(100)

    return result


# =============================================================================
# Q86: Web Sales ROLLUP with RANK
# =============================================================================


def q86_expression_impl(ctx: DataFrameContext) -> Any:
    """TPC-DS Q86: Web Sales ROLLUP with Rank (Expression Family).

    Computes sum of ws_net_paid with ROLLUP on (i_category, i_class),
    adds lochierarchy and rank_within_parent using RANK() OVER.
    """

    from benchbox.core.tpcds.dataframe_queries.rollup_helper import (
        expand_rollup_expression,
        lochierarchy_expression,
    )

    params = get_parameters(86)
    dms = params.get("dms", 1200)

    col = ctx.col
    lit = ctx.lit

    # Get tables
    web_sales = ctx.get_table("web_sales")
    date_dim = ctx.get_table("date_dim")
    item = ctx.get_table("item")

    # Join and filter
    base = (
        web_sales.join(date_dim, left_on="ws_sold_date_sk", right_on="d_date_sk")
        .join(item, left_on="ws_item_sk", right_on="i_item_sk")
        .filter(col("d_month_seq").is_between(dms, dms + 11))
    )

    # Aggregation expression for ROLLUP
    agg_exprs = [col("ws_net_paid").sum().alias("total_sum")]

    # Expand ROLLUP(i_category, i_class)
    rollup_result = expand_rollup_expression(
        base,
        group_cols=["i_category", "i_class"],
        agg_exprs=agg_exprs,
        ctx=ctx,
    )

    # Add lochierarchy = GROUPING(i_category) + GROUPING(i_class)
    lochierarchy_expr = lochierarchy_expression("grouping_id", 2, ctx=ctx)
    result = rollup_result.with_columns(lochierarchy_expr.alias("lochierarchy"))

    # Add rank_within_parent using RANK() OVER
    # PARTITION BY lochierarchy, CASE WHEN GROUPING(i_class)=0 THEN i_category END
    # ORDER BY total_sum DESC
    result = result.with_columns(
        ctx.when(col("grouping_id") & 1 == 0)  # GROUPING(i_class) = 0
        .then(col("i_category"))
        .otherwise(lit(None))
        .alias("partition_key")
    )

    # Compute rank within partition
    result = result.with_columns(
        col("total_sum")
        .rank(method="min", descending=True)
        .over(["lochierarchy", "partition_key"])
        .alias("rank_within_parent")
    )

    # Select and sort
    result = (
        result.select(
            col("total_sum"),
            col("i_category"),
            col("i_class"),
            col("lochierarchy"),
            col("rank_within_parent"),
        )
        .sort(
            [col("lochierarchy"), col("i_category"), col("rank_within_parent")],
            descending=[True, False, False],
            nulls_last=True,
        )
        .head(100)
    )

    return result


def q86_pandas_impl(ctx: DataFrameContext) -> Any:
    """TPC-DS Q86: Web Sales ROLLUP with Rank (Pandas Family)."""
    from benchbox.core.tpcds.dataframe_queries.rollup_helper import expand_rollup_pandas

    params = get_parameters(86)
    dms = params.get("dms", 1200)

    # Get tables
    web_sales = ctx.get_table("web_sales")
    date_dim = ctx.get_table("date_dim")
    item = ctx.get_table("item")

    # Join and filter
    base = web_sales.merge(date_dim, left_on="ws_sold_date_sk", right_on="d_date_sk")
    base = base.merge(item, left_on="ws_item_sk", right_on="i_item_sk")
    base = base[(base["d_month_seq"] >= dms) & (base["d_month_seq"] <= dms + 11)]

    # Expand ROLLUP(i_category, i_class)
    agg_dict = {"total_sum": ("ws_net_paid", "sum")}
    rollup_result = expand_rollup_pandas(
        base,
        group_cols=["i_category", "i_class"],
        agg_dict=agg_dict,
        ctx=ctx,
    )

    # Add lochierarchy
    rollup_result["lochierarchy"] = rollup_result["grouping_id"].apply(lambda x: bin(x).count("1"))

    # Add partition key for ranking
    rollup_result["partition_key"] = rollup_result.apply(
        lambda r: r["i_category"] if (r["grouping_id"] & 1) == 0 else None, axis=1
    )

    # Compute rank within partition
    rollup_result["rank_within_parent"] = rollup_result.groupby(["lochierarchy", "partition_key"], dropna=False)[
        "total_sum"
    ].rank(method="min", ascending=False)

    # Select and sort
    result = rollup_result[["total_sum", "i_category", "i_class", "lochierarchy", "rank_within_parent"]]
    result = result.sort_values(
        ["lochierarchy", "i_category", "rank_within_parent"],
        ascending=[False, True, True],
        na_position="last",
    ).head(100)

    return result


# =============================================================================
# Q36: Gross Margin ROLLUP with RANK
# =============================================================================


def q36_expression_impl(ctx: DataFrameContext) -> Any:
    """TPC-DS Q36: Gross Margin ROLLUP with Rank (Expression Family).

    Computes gross_margin = sum(ss_net_profit)/sum(ss_ext_sales_price)
    with ROLLUP on (i_category, i_class), filtered by state.
    """

    from benchbox.core.tpcds.dataframe_queries.rollup_helper import (
        expand_rollup_expression,
        lochierarchy_expression,
    )

    params = get_parameters(36)
    year = params.get("year", 2001)
    states = params.get("states", ["TN", "SD", "AL", "NC", "OK", "MS", "WI", "IN"])

    col = ctx.col
    lit = ctx.lit

    # Get tables
    store_sales = ctx.get_table("store_sales")
    date_dim = ctx.get_table("date_dim")
    item = ctx.get_table("item")
    store = ctx.get_table("store")

    # Join and filter
    base = (
        store_sales.join(date_dim, left_on="ss_sold_date_sk", right_on="d_date_sk")
        .join(item, left_on="ss_item_sk", right_on="i_item_sk")
        .join(store, left_on="ss_store_sk", right_on="s_store_sk")
        .filter((col("d_year") == lit(year)) & col("s_state").is_in(states))
    )

    # Aggregation expressions for ROLLUP
    agg_exprs = [
        col("ss_net_profit").sum().alias("sum_profit"),
        col("ss_ext_sales_price").sum().alias("sum_sales"),
    ]

    # Expand ROLLUP(i_category, i_class)
    rollup_result = expand_rollup_expression(
        base,
        group_cols=["i_category", "i_class"],
        agg_exprs=agg_exprs,
        ctx=ctx,
    )

    # Compute gross_margin
    result = rollup_result.with_columns((col("sum_profit") / col("sum_sales")).alias("gross_margin"))

    # Add lochierarchy
    lochierarchy_expr = lochierarchy_expression("grouping_id", 2, ctx=ctx)
    result = result.with_columns(lochierarchy_expr.alias("lochierarchy"))

    # Add partition key for ranking
    result = result.with_columns(
        ctx.when(col("grouping_id") & 1 == 0).then(col("i_category")).otherwise(lit(None)).alias("partition_key")
    )

    # Compute rank within partition (ASC for gross_margin)
    result = result.with_columns(
        col("gross_margin")
        .rank(method="min", descending=False)
        .over(["lochierarchy", "partition_key"])
        .alias("rank_within_parent")
    )

    # Select and sort
    result = (
        result.select(
            col("gross_margin"),
            col("i_category"),
            col("i_class"),
            col("lochierarchy"),
            col("rank_within_parent"),
        )
        .sort(
            [col("lochierarchy"), col("i_category"), col("rank_within_parent")],
            descending=[True, False, False],
            nulls_last=True,
        )
        .head(100)
    )

    return result


def q36_pandas_impl(ctx: DataFrameContext) -> Any:
    """TPC-DS Q36: Gross Margin ROLLUP with Rank (Pandas Family)."""
    from benchbox.core.tpcds.dataframe_queries.rollup_helper import expand_rollup_pandas

    params = get_parameters(36)
    year = params.get("year", 2001)
    states = params.get("states", ["TN", "SD", "AL", "NC", "OK", "MS", "WI", "IN"])

    # Get tables
    store_sales = ctx.get_table("store_sales")
    date_dim = ctx.get_table("date_dim")
    item = ctx.get_table("item")
    store = ctx.get_table("store")

    # Join and filter
    base = store_sales.merge(date_dim, left_on="ss_sold_date_sk", right_on="d_date_sk")
    base = base.merge(item, left_on="ss_item_sk", right_on="i_item_sk")
    base = base.merge(store, left_on="ss_store_sk", right_on="s_store_sk")
    base = base[(base["d_year"] == year) & base["s_state"].isin(states)]

    # Expand ROLLUP - need both sums for gross margin
    agg_dict = {
        "sum_profit": ("ss_net_profit", "sum"),
        "sum_sales": ("ss_ext_sales_price", "sum"),
    }
    rollup_result = expand_rollup_pandas(
        base,
        group_cols=["i_category", "i_class"],
        agg_dict=agg_dict,
        ctx=ctx,
    )

    # Compute gross_margin
    rollup_result["gross_margin"] = rollup_result["sum_profit"] / rollup_result["sum_sales"]

    # Add lochierarchy
    rollup_result["lochierarchy"] = rollup_result["grouping_id"].apply(lambda x: bin(x).count("1"))

    # Add partition key for ranking
    rollup_result["partition_key"] = rollup_result.apply(
        lambda r: r["i_category"] if (r["grouping_id"] & 1) == 0 else None, axis=1
    )

    # Compute rank within partition
    rollup_result["rank_within_parent"] = rollup_result.groupby(["lochierarchy", "partition_key"], dropna=False)[
        "gross_margin"
    ].rank(method="min", ascending=True)

    # Select and sort
    result = rollup_result[["gross_margin", "i_category", "i_class", "lochierarchy", "rank_within_parent"]]
    result = result.sort_values(
        ["lochierarchy", "i_category", "rank_within_parent"],
        ascending=[False, True, True],
        na_position="last",
    ).head(100)

    return result


# =============================================================================
# Q51: Cumulative Web/Store Sales
# =============================================================================


def q51_expression_impl(ctx: DataFrameContext) -> Any:
    """TPC-DS Q51: Cumulative Web/Store Sales (Expression Family).

    Computes cumulative sales for web and store channels per item/date,
    finds where web cumulative exceeds store cumulative.
    """

    params = get_parameters(51)
    dms = params.get("dms", 1200)

    col = ctx.col

    # Get tables
    web_sales = ctx.get_table("web_sales")
    store_sales = ctx.get_table("store_sales")
    date_dim = ctx.get_table("date_dim")

    # Filter date_dim for the date range
    dates = date_dim.filter(col("d_month_seq").is_between(dms, dms + 11))

    # Web CTE: cumulative sum per item/date
    web_base = (
        web_sales.filter(col("ws_item_sk").is_not_null())
        .join(dates, left_on="ws_sold_date_sk", right_on="d_date_sk")
        .group_by(["ws_item_sk", "d_date"])
        .agg(col("ws_sales_price").sum().alias("daily_sales"))
    )

    # Add cumulative sum
    web_v1 = web_base.with_columns(
        col("daily_sales").cum_sum().over(["ws_item_sk"], order_by="d_date").alias("cume_sales")
    ).select(
        col("ws_item_sk").alias("item_sk"),
        col("d_date"),
        col("cume_sales"),
    )

    # Store CTE: cumulative sum per item/date
    store_base = (
        store_sales.filter(col("ss_item_sk").is_not_null())
        .join(dates, left_on="ss_sold_date_sk", right_on="d_date_sk")
        .group_by(["ss_item_sk", "d_date"])
        .agg(col("ss_sales_price").sum().alias("daily_sales"))
    )

    store_v1 = store_base.with_columns(
        col("daily_sales").cum_sum().over(["ss_item_sk"], order_by="d_date").alias("cume_sales")
    ).select(
        col("ss_item_sk").alias("item_sk"),
        col("d_date"),
        col("cume_sales"),
    )

    # Full outer join - coalesce item_sk and d_date
    joined = web_v1.join(
        store_v1,
        on=["item_sk", "d_date"],
        how="full",
        suffix="_store",
    )

    # Coalesce columns and rename
    joined = joined.with_columns(
        ctx.coalesce(col("item_sk"), col("item_sk_store")).alias("item_sk_final"),
        ctx.coalesce(col("d_date"), col("d_date_store")).alias("d_date_final"),
        col("cume_sales").alias("web_sales"),
        col("cume_sales_store").alias("store_sales"),
    )

    # Compute cumulative max for both
    result = joined.with_columns(
        col("web_sales")
        .fill_null(0)
        .cum_max()
        .over(["item_sk_final"], order_by="d_date_final")
        .alias("web_cumulative"),
        col("store_sales")
        .fill_null(0)
        .cum_max()
        .over(["item_sk_final"], order_by="d_date_final")
        .alias("store_cumulative"),
    )

    # Filter where web > store
    result = (
        result.filter(col("web_cumulative") > col("store_cumulative"))
        .select(
            col("item_sk_final").alias("item_sk"),
            col("d_date_final").alias("d_date"),
            col("web_sales"),
            col("store_sales"),
            col("web_cumulative"),
            col("store_cumulative"),
        )
        .sort(["item_sk", "d_date"])
        .head(100)
    )

    return result


def q51_pandas_impl(ctx: DataFrameContext) -> Any:
    """TPC-DS Q51: Cumulative Web/Store Sales (Pandas Family)."""
    params = get_parameters(51)
    dms = params.get("dms", 1200)

    # Get tables
    web_sales = ctx.get_table("web_sales")
    store_sales = ctx.get_table("store_sales")
    date_dim = ctx.get_table("date_dim")

    # Filter dates
    dates = date_dim[(date_dim["d_month_seq"] >= dms) & (date_dim["d_month_seq"] <= dms + 11)]

    # Web CTE
    web_merged = web_sales[web_sales["ws_item_sk"].notna()].merge(
        dates[["d_date_sk", "d_date"]], left_on="ws_sold_date_sk", right_on="d_date_sk"
    )
    web_base = web_merged.groupby(["ws_item_sk", "d_date"], as_index=False).agg(daily_sales=("ws_sales_price", "sum"))
    web_base = web_base.sort_values(["ws_item_sk", "d_date"])
    web_base["cume_sales"] = web_base.groupby("ws_item_sk")["daily_sales"].cumsum()
    web_v1 = web_base[["ws_item_sk", "d_date", "cume_sales"]].rename(columns={"ws_item_sk": "item_sk"})

    # Store CTE
    store_merged = store_sales[store_sales["ss_item_sk"].notna()].merge(
        dates[["d_date_sk", "d_date"]], left_on="ss_sold_date_sk", right_on="d_date_sk"
    )
    store_base = store_merged.groupby(["ss_item_sk", "d_date"], as_index=False).agg(
        daily_sales=("ss_sales_price", "sum")
    )
    store_base = store_base.sort_values(["ss_item_sk", "d_date"])
    store_base["cume_sales"] = store_base.groupby("ss_item_sk")["daily_sales"].cumsum()
    store_v1 = store_base[["ss_item_sk", "d_date", "cume_sales"]].rename(columns={"ss_item_sk": "item_sk"})

    # Full outer join
    joined = web_v1.merge(store_v1, on=["item_sk", "d_date"], how="outer", suffixes=("_web", "_store"))
    joined["web_sales"] = joined["cume_sales_web"].fillna(0)
    joined["store_sales"] = joined["cume_sales_store"].fillna(0)

    # Compute cumulative max
    joined = joined.sort_values(["item_sk", "d_date"])
    joined["web_cumulative"] = joined.groupby("item_sk")["web_sales"].cummax()
    joined["store_cumulative"] = joined.groupby("item_sk")["store_sales"].cummax()

    # Filter where web > store
    result = joined[joined["web_cumulative"] > joined["store_cumulative"]]
    result = result[["item_sk", "d_date", "web_sales", "store_sales", "web_cumulative", "store_cumulative"]]
    result = result.sort_values(["item_sk", "d_date"]).head(100)

    return result


# =============================================================================
# Q47: Store Sales Rolling Average
# =============================================================================


def q47_expression_impl(ctx: DataFrameContext) -> Any:
    """TPC-DS Q47: Store Sales Rolling Average (Expression Family).

    Computes monthly sales with 3-month rolling average using window functions.
    Uses self-join pattern for previous/next month values.
    """
    params = get_parameters(47)
    year = params.get("year", 1999)

    col = ctx.col
    lit = ctx.lit

    # Get tables
    store_sales = ctx.get_table("store_sales")
    date_dim = ctx.get_table("date_dim")
    item = ctx.get_table("item")
    store = ctx.get_table("store")

    # Filter dates: current year +/- 1 month
    dates = date_dim.filter(
        (col("d_year") == lit(year))
        | ((col("d_year") == lit(year - 1)) & (col("d_moy") == 12))
        | ((col("d_year") == lit(year + 1)) & (col("d_moy") == 1))
    )

    # Base aggregation
    base = (
        store_sales.join(dates, left_on="ss_sold_date_sk", right_on="d_date_sk")
        .join(item, left_on="ss_item_sk", right_on="i_item_sk")
        .join(store, left_on="ss_store_sk", right_on="s_store_sk")
        .group_by(["i_category", "i_brand", "s_store_name", "s_company_name", "d_year", "d_moy"])
        .agg(col("ss_sales_price").sum().alias("sum_sales"))
    )

    # Add rank within partition for month ordering
    v1 = base.with_columns(
        col("d_year")
        .rank(method="ordinal")  # Just use as proxy for month order
        .over(["i_category", "i_brand", "s_store_name", "s_company_name"])
        .alias("rn")
    )

    # Add average monthly sales (within year) and rank for month order
    v1 = v1.with_columns(
        col("sum_sales")
        .mean()
        .over(["i_category", "i_brand", "s_store_name", "s_company_name", "d_year"])
        .alias("avg_monthly_sales"),
        # Use combined year*100+moy for proper ordering
        ((col("d_year") * 100) + col("d_moy"))
        .rank(method="ordinal")
        .over(["i_category", "i_brand", "s_store_name", "s_company_name"])
        .alias("rn"),
    )

    # Self-join for lag/lead (previous/next month)
    v1_lag = v1.select(
        col("i_category"),
        col("i_brand"),
        col("s_store_name"),
        col("s_company_name"),
        col("rn"),
        col("sum_sales").alias("psum"),
    )

    v1_lead = v1.select(
        col("i_category"),
        col("i_brand"),
        col("s_store_name"),
        col("s_company_name"),
        col("rn"),
        col("sum_sales").alias("nsum"),
    )

    # Join: v1.rn = v1_lag.rn + 1 and v1.rn = v1_lead.rn - 1
    result = (
        v1.join(
            v1_lag,
            on=["i_category", "i_brand", "s_store_name", "s_company_name"],
            suffix="_lag",
        )
        .filter(col("rn") == col("rn_lag") + 1)
        .join(
            v1_lead,
            on=["i_category", "i_brand", "s_store_name", "s_company_name"],
            suffix="_lead",
        )
        .filter(col("rn") == col("rn_lead") - 1)
    )

    # Filter for year and avg_monthly_sales > 0 and deviation > 0.1
    result = result.filter(
        (col("d_year") == lit(year))
        & (col("avg_monthly_sales") > 0)
        & ((col("sum_sales") - col("avg_monthly_sales")).abs() / col("avg_monthly_sales") > 0.1)
    )

    # Select and sort
    result = (
        result.select(
            col("i_category"),
            col("i_brand"),
            col("s_store_name"),
            col("s_company_name"),
            col("d_year"),
            col("d_moy"),
            col("avg_monthly_sales"),
            col("sum_sales"),
            col("psum"),
            col("nsum"),
        )
        .with_columns((col("sum_sales") - col("avg_monthly_sales")).alias("diff"))
        .sort(["diff", "avg_monthly_sales"])
        .head(100)
    )

    return result


def q47_pandas_impl(ctx: DataFrameContext) -> Any:
    """TPC-DS Q47: Store Sales Rolling Average (Pandas Family)."""

    params = get_parameters(47)
    year = params.get("year", 1999)

    # Get tables
    store_sales = ctx.get_table("store_sales")
    date_dim = ctx.get_table("date_dim")
    item = ctx.get_table("item")
    store = ctx.get_table("store")

    # Filter dates
    dates = date_dim[
        (date_dim["d_year"] == year)
        | ((date_dim["d_year"] == year - 1) & (date_dim["d_moy"] == 12))
        | ((date_dim["d_year"] == year + 1) & (date_dim["d_moy"] == 1))
    ]

    # Base aggregation
    base = store_sales.merge(dates, left_on="ss_sold_date_sk", right_on="d_date_sk")
    base = base.merge(item, left_on="ss_item_sk", right_on="i_item_sk")
    base = base.merge(store, left_on="ss_store_sk", right_on="s_store_sk")
    base = base.groupby(
        ["i_category", "i_brand", "s_store_name", "s_company_name", "d_year", "d_moy"], as_index=False
    ).agg(sum_sales=("ss_sales_price", "sum"))

    # Add year_month for ordering
    base["year_month"] = base["d_year"] * 100 + base["d_moy"]
    base = base.sort_values(["i_category", "i_brand", "s_store_name", "s_company_name", "year_month"])

    # Add rank
    base["rn"] = base.groupby(["i_category", "i_brand", "s_store_name", "s_company_name"]).cumcount() + 1

    # Add average monthly sales within year
    base["avg_monthly_sales"] = base.groupby(["i_category", "i_brand", "s_store_name", "s_company_name", "d_year"])[
        "sum_sales"
    ].transform("mean")

    # Self-join for lag and lead
    v1 = base.copy()
    v1_lag = v1[["i_category", "i_brand", "s_store_name", "s_company_name", "rn", "sum_sales"]].copy()
    v1_lag = v1_lag.rename(columns={"sum_sales": "psum", "rn": "rn_lag"})
    v1_lag["rn_lag"] = v1_lag["rn_lag"] + 1  # For v1.rn = v1_lag.rn + 1

    v1_lead = v1[["i_category", "i_brand", "s_store_name", "s_company_name", "rn", "sum_sales"]].copy()
    v1_lead = v1_lead.rename(columns={"sum_sales": "nsum", "rn": "rn_lead"})
    v1_lead["rn_lead"] = v1_lead["rn_lead"] - 1  # For v1.rn = v1_lead.rn - 1

    # Join
    result = v1.merge(
        v1_lag,
        left_on=["i_category", "i_brand", "s_store_name", "s_company_name", "rn"],
        right_on=["i_category", "i_brand", "s_store_name", "s_company_name", "rn_lag"],
    )
    result = result.merge(
        v1_lead,
        left_on=["i_category", "i_brand", "s_store_name", "s_company_name", "rn"],
        right_on=["i_category", "i_brand", "s_store_name", "s_company_name", "rn_lead"],
    )

    # Filter
    result = result[
        (result["d_year"] == year)
        & (result["avg_monthly_sales"] > 0)
        & (abs(result["sum_sales"] - result["avg_monthly_sales"]) / result["avg_monthly_sales"] > 0.1)
    ]

    # Select and sort
    result["diff"] = result["sum_sales"] - result["avg_monthly_sales"]
    result = result[
        [
            "i_category",
            "i_brand",
            "s_store_name",
            "s_company_name",
            "d_year",
            "d_moy",
            "avg_monthly_sales",
            "sum_sales",
            "psum",
            "nsum",
        ]
    ]
    result = result.sort_values(["diff", "avg_monthly_sales"]).head(100)

    return result


# =============================================================================
# Q57: Catalog Sales Rolling Average
# =============================================================================


def q57_expression_impl(ctx: DataFrameContext) -> Any:
    """TPC-DS Q57: Catalog Sales Rolling Average (Expression Family).

    Similar to Q47 but for catalog_sales with call_center dimension.
    """

    params = get_parameters(57)
    year = params.get("year", 1999)

    col = ctx.col
    lit = ctx.lit

    # Get tables
    catalog_sales = ctx.get_table("catalog_sales")
    date_dim = ctx.get_table("date_dim")
    item = ctx.get_table("item")
    call_center = ctx.get_table("call_center")

    # Filter dates
    dates = date_dim.filter(
        (col("d_year") == lit(year))
        | ((col("d_year") == lit(year - 1)) & (col("d_moy") == 12))
        | ((col("d_year") == lit(year + 1)) & (col("d_moy") == 1))
    )

    # Base aggregation
    base = (
        catalog_sales.join(dates, left_on="cs_sold_date_sk", right_on="d_date_sk")
        .join(item, left_on="cs_item_sk", right_on="i_item_sk")
        .join(call_center, left_on="cs_call_center_sk", right_on="cc_call_center_sk")
        .group_by(["i_category", "i_brand", "cc_name", "d_year", "d_moy"])
        .agg(col("cs_sales_price").sum().alias("sum_sales"))
    )

    # Add rank and average
    v1 = base.with_columns(
        col("sum_sales").mean().over(["i_category", "i_brand", "cc_name", "d_year"]).alias("avg_monthly_sales"),
        ((col("d_year") * 100) + col("d_moy"))
        .rank(method="ordinal")
        .over(["i_category", "i_brand", "cc_name"])
        .alias("rn"),
    )

    # Self-join for lag/lead
    v1_lag = v1.select(
        col("i_category"),
        col("i_brand"),
        col("cc_name"),
        col("rn"),
        col("sum_sales").alias("psum"),
    )

    v1_lead = v1.select(
        col("i_category"),
        col("i_brand"),
        col("cc_name"),
        col("rn"),
        col("sum_sales").alias("nsum"),
    )

    result = (
        v1.join(v1_lag, on=["i_category", "i_brand", "cc_name"], suffix="_lag")
        .filter(col("rn") == col("rn_lag") + 1)
        .join(v1_lead, on=["i_category", "i_brand", "cc_name"], suffix="_lead")
        .filter(col("rn") == col("rn_lead") - 1)
    )

    # Filter
    result = result.filter(
        (col("d_year") == lit(year))
        & (col("avg_monthly_sales") > 0)
        & ((col("sum_sales") - col("avg_monthly_sales")).abs() / col("avg_monthly_sales") > 0.1)
    )

    # Select and sort
    result = (
        result.select(
            col("i_category"),
            col("i_brand"),
            col("cc_name"),
            col("d_year"),
            col("d_moy"),
            col("avg_monthly_sales"),
            col("sum_sales"),
            col("psum"),
            col("nsum"),
        )
        .with_columns((col("sum_sales") - col("avg_monthly_sales")).alias("diff"))
        .sort(["diff", "avg_monthly_sales"])
        .head(100)
    )

    return result


def q57_pandas_impl(ctx: DataFrameContext) -> Any:
    """TPC-DS Q57: Catalog Sales Rolling Average (Pandas Family)."""

    params = get_parameters(57)
    year = params.get("year", 1999)

    # Get tables
    catalog_sales = ctx.get_table("catalog_sales")
    date_dim = ctx.get_table("date_dim")
    item = ctx.get_table("item")
    call_center = ctx.get_table("call_center")

    # Filter dates
    dates = date_dim[
        (date_dim["d_year"] == year)
        | ((date_dim["d_year"] == year - 1) & (date_dim["d_moy"] == 12))
        | ((date_dim["d_year"] == year + 1) & (date_dim["d_moy"] == 1))
    ]

    # Base aggregation
    base = catalog_sales.merge(dates, left_on="cs_sold_date_sk", right_on="d_date_sk")
    base = base.merge(item, left_on="cs_item_sk", right_on="i_item_sk")
    base = base.merge(call_center, left_on="cs_call_center_sk", right_on="cc_call_center_sk")
    base = base.groupby(["i_category", "i_brand", "cc_name", "d_year", "d_moy"], as_index=False).agg(
        sum_sales=("cs_sales_price", "sum")
    )

    # Add ordering and rank
    base["year_month"] = base["d_year"] * 100 + base["d_moy"]
    base = base.sort_values(["i_category", "i_brand", "cc_name", "year_month"])
    base["rn"] = base.groupby(["i_category", "i_brand", "cc_name"]).cumcount() + 1

    # Add average
    base["avg_monthly_sales"] = base.groupby(["i_category", "i_brand", "cc_name", "d_year"])["sum_sales"].transform(
        "mean"
    )

    # Self-join
    v1 = base.copy()
    v1_lag = v1[["i_category", "i_brand", "cc_name", "rn", "sum_sales"]].copy()
    v1_lag = v1_lag.rename(columns={"sum_sales": "psum", "rn": "rn_lag"})
    v1_lag["rn_lag"] = v1_lag["rn_lag"] + 1

    v1_lead = v1[["i_category", "i_brand", "cc_name", "rn", "sum_sales"]].copy()
    v1_lead = v1_lead.rename(columns={"sum_sales": "nsum", "rn": "rn_lead"})
    v1_lead["rn_lead"] = v1_lead["rn_lead"] - 1

    result = v1.merge(
        v1_lag,
        left_on=["i_category", "i_brand", "cc_name", "rn"],
        right_on=["i_category", "i_brand", "cc_name", "rn_lag"],
    )
    result = result.merge(
        v1_lead,
        left_on=["i_category", "i_brand", "cc_name", "rn"],
        right_on=["i_category", "i_brand", "cc_name", "rn_lead"],
    )

    # Filter
    result = result[
        (result["d_year"] == year)
        & (result["avg_monthly_sales"] > 0)
        & (abs(result["sum_sales"] - result["avg_monthly_sales"]) / result["avg_monthly_sales"] > 0.1)
    ]

    # Select and sort
    result["diff"] = result["sum_sales"] - result["avg_monthly_sales"]
    result = result[
        ["i_category", "i_brand", "cc_name", "d_year", "d_moy", "avg_monthly_sales", "sum_sales", "psum", "nsum"]
    ]
    result = result.sort_values(["diff", "avg_monthly_sales"]).head(100)

    return result


# =============================================================================
# Q67: Extensive ROLLUP with RANK
# =============================================================================


def q67_expression_impl(ctx: DataFrameContext) -> Any:
    """TPC-DS Q67: Extensive ROLLUP with Rank (Expression Family).

    8-column ROLLUP on item/date/store dimensions with RANK() OVER.
    """

    from benchbox.core.tpcds.dataframe_queries.rollup_helper import expand_rollup_expression

    params = get_parameters(67)
    dms = params.get("dms", 1200)

    col = ctx.col
    lit = ctx.lit

    # Get tables
    store_sales = ctx.get_table("store_sales")
    date_dim = ctx.get_table("date_dim")
    item = ctx.get_table("item")
    store = ctx.get_table("store")

    # Join and filter
    base = (
        store_sales.join(date_dim, left_on="ss_sold_date_sk", right_on="d_date_sk")
        .join(item, left_on="ss_item_sk", right_on="i_item_sk")
        .join(store, left_on="ss_store_sk", right_on="s_store_sk")
        .filter(col("d_month_seq").is_between(dms, dms + 11))
        .with_columns((ctx.coalesce(col("ss_sales_price") * col("ss_quantity"), lit(0))).alias("sales_amount"))
    )

    # 8-column ROLLUP
    group_cols = [
        "i_category",
        "i_class",
        "i_brand",
        "i_product_name",
        "d_year",
        "d_qoy",
        "d_moy",
        "s_store_id",
    ]

    agg_exprs = [col("sales_amount").sum().alias("sumsales")]

    rollup_result = expand_rollup_expression(base, group_cols, agg_exprs, ctx)

    # Add rank within i_category
    result = rollup_result.with_columns(
        col("sumsales").rank(method="min", descending=True).over(["i_category"]).alias("rk")
    )

    # Filter rk <= 100
    result = (
        result.filter(col("rk") <= 100)
        .select(
            col("i_category"),
            col("i_class"),
            col("i_brand"),
            col("i_product_name"),
            col("d_year"),
            col("d_qoy"),
            col("d_moy"),
            col("s_store_id"),
            col("sumsales"),
            col("rk"),
        )
        .sort(
            [
                "i_category",
                "i_class",
                "i_brand",
                "i_product_name",
                "d_year",
                "d_qoy",
                "d_moy",
                "s_store_id",
                "sumsales",
                "rk",
            ]
        )
        .head(100)
    )

    return result


def q67_pandas_impl(ctx: DataFrameContext) -> Any:
    """TPC-DS Q67: Extensive ROLLUP with Rank (Pandas Family)."""
    from benchbox.core.tpcds.dataframe_queries.rollup_helper import expand_rollup_pandas

    params = get_parameters(67)
    dms = params.get("dms", 1200)

    # Get tables
    store_sales = ctx.get_table("store_sales")
    date_dim = ctx.get_table("date_dim")
    item = ctx.get_table("item")
    store = ctx.get_table("store")

    # Join and filter
    base = store_sales.merge(date_dim, left_on="ss_sold_date_sk", right_on="d_date_sk")
    base = base.merge(item, left_on="ss_item_sk", right_on="i_item_sk")
    base = base.merge(store, left_on="ss_store_sk", right_on="s_store_sk")
    base = base[(base["d_month_seq"] >= dms) & (base["d_month_seq"] <= dms + 11)]
    base["sales_amount"] = (base["ss_sales_price"] * base["ss_quantity"]).fillna(0)

    # 8-column ROLLUP
    group_cols = [
        "i_category",
        "i_class",
        "i_brand",
        "i_product_name",
        "d_year",
        "d_qoy",
        "d_moy",
        "s_store_id",
    ]
    agg_dict = {"sumsales": ("sales_amount", "sum")}
    rollup_result = expand_rollup_pandas(base, group_cols, agg_dict, ctx)

    # Add rank within i_category
    rollup_result["rk"] = rollup_result.groupby("i_category", dropna=False)["sumsales"].rank(
        method="min", ascending=False
    )

    # Filter rk <= 100
    result = rollup_result[rollup_result["rk"] <= 100]
    result = result[
        [
            "i_category",
            "i_class",
            "i_brand",
            "i_product_name",
            "d_year",
            "d_qoy",
            "d_moy",
            "s_store_id",
            "sumsales",
            "rk",
        ]
    ]
    result = result.sort_values(
        [
            "i_category",
            "i_class",
            "i_brand",
            "i_product_name",
            "d_year",
            "d_qoy",
            "d_moy",
            "s_store_id",
            "sumsales",
            "rk",
        ]
    ).head(100)

    return result


# =============================================================================
# Q70: Store Sales State/County ROLLUP with Subquery Filter
# =============================================================================


def q70_expression_impl(ctx: DataFrameContext) -> Any:
    """TPC-DS Q70: Store Sales ROLLUP with Subquery Filter (Expression Family).

    ROLLUP on (s_state, s_county) with subquery filter for top 5 states.
    """

    from benchbox.core.tpcds.dataframe_queries.rollup_helper import (
        expand_rollup_expression,
        lochierarchy_expression,
    )

    params = get_parameters(70)
    dms = params.get("dms", 1200)

    col = ctx.col
    lit = ctx.lit

    # Get tables
    store_sales = ctx.get_table("store_sales")
    date_dim = ctx.get_table("date_dim")
    store = ctx.get_table("store")

    # Date filter
    dates = date_dim.filter(col("d_month_seq").is_between(dms, dms + 11))

    # First, find top 5 states by profit (subquery)
    state_profit = (
        store_sales.join(dates, left_on="ss_sold_date_sk", right_on="d_date_sk")
        .join(store, left_on="ss_store_sk", right_on="s_store_sk")
        .group_by("s_state")
        .agg(col("ss_net_profit").sum().alias("state_profit"))
        .with_columns(col("state_profit").rank(method="ordinal", descending=True).alias("ranking"))
        .filter(col("ranking") <= 5)
        .select(col("s_state"))
    )

    # Main query base - filter by top 5 states
    base = (
        store_sales.join(dates, left_on="ss_sold_date_sk", right_on="d_date_sk")
        .join(store, left_on="ss_store_sk", right_on="s_store_sk")
        .join(state_profit, on="s_state")  # Semi-join filter
    )

    # ROLLUP on (s_state, s_county)
    agg_exprs = [col("ss_net_profit").sum().alias("total_sum")]
    rollup_result = expand_rollup_expression(base, ["s_state", "s_county"], agg_exprs, ctx)

    # Add lochierarchy
    lochierarchy_expr = lochierarchy_expression("grouping_id", 2, ctx=ctx)
    result = rollup_result.with_columns(lochierarchy_expr.alias("lochierarchy"))

    # Add partition key for ranking
    result = result.with_columns(
        ctx.when(col("grouping_id") & 1 == 0).then(col("s_state")).otherwise(lit(None)).alias("partition_key")
    )

    # Compute rank within partition (DESC for total_sum)
    result = result.with_columns(
        col("total_sum")
        .rank(method="min", descending=True)
        .over(["lochierarchy", "partition_key"])
        .alias("rank_within_parent")
    )

    # Select and sort
    result = (
        result.select(
            col("total_sum"),
            col("s_state"),
            col("s_county"),
            col("lochierarchy"),
            col("rank_within_parent"),
        )
        .sort(
            [col("lochierarchy"), col("s_state"), col("rank_within_parent")],
            descending=[True, False, False],
            nulls_last=True,
        )
        .head(100)
    )

    return result


def q70_pandas_impl(ctx: DataFrameContext) -> Any:
    """TPC-DS Q70: Store Sales ROLLUP with Subquery Filter (Pandas Family)."""
    from benchbox.core.tpcds.dataframe_queries.rollup_helper import expand_rollup_pandas

    params = get_parameters(70)
    dms = params.get("dms", 1200)

    # Get tables
    store_sales = ctx.get_table("store_sales")
    date_dim = ctx.get_table("date_dim")
    store = ctx.get_table("store")

    # Date filter
    dates = date_dim[(date_dim["d_month_seq"] >= dms) & (date_dim["d_month_seq"] <= dms + 11)]

    # First, find top 5 states by profit (subquery)
    merged = store_sales.merge(dates[["d_date_sk"]], left_on="ss_sold_date_sk", right_on="d_date_sk")
    merged = merged.merge(store[["s_store_sk", "s_state", "s_county"]], left_on="ss_store_sk", right_on="s_store_sk")
    state_profit = merged.groupby("s_state", as_index=False).agg(state_profit=("ss_net_profit", "sum"))
    state_profit["ranking"] = state_profit["state_profit"].rank(method="first", ascending=False)
    top_states = state_profit[state_profit["ranking"] <= 5]["s_state"].tolist()

    # Main query - filter by top 5 states
    base = merged[merged["s_state"].isin(top_states)]

    # ROLLUP on (s_state, s_county)
    agg_dict = {"total_sum": ("ss_net_profit", "sum")}
    rollup_result = expand_rollup_pandas(base, ["s_state", "s_county"], agg_dict, ctx)

    # Add lochierarchy
    rollup_result["lochierarchy"] = rollup_result["grouping_id"].apply(lambda x: bin(x).count("1"))

    # Add partition key for ranking
    rollup_result["partition_key"] = rollup_result.apply(
        lambda r: r["s_state"] if (r["grouping_id"] & 1) == 0 else None, axis=1
    )

    # Compute rank within partition
    rollup_result["rank_within_parent"] = rollup_result.groupby(["lochierarchy", "partition_key"], dropna=False)[
        "total_sum"
    ].rank(method="min", ascending=False)

    # Select and sort
    result = rollup_result[["total_sum", "s_state", "s_county", "lochierarchy", "rank_within_parent"]]
    result = result.sort_values(
        ["lochierarchy", "s_state", "rank_within_parent"],
        ascending=[False, True, True],
        na_position="last",
    ).head(100)

    return result


# =============================================================================
# Q2: Web/Catalog Weekly Sales Year-over-Year
# =============================================================================


def q2_expression_impl(ctx: DataFrameContext) -> Any:
    """Q2: Web/Catalog weekly sales comparison year-over-year (Polars).

    Union web and catalog sales, aggregate by week with day-of-week pivot,
    compare weekly sales between current year and next year.
    """

    col = ctx.col

    # Parameters
    params = get_parameters(2)
    year = params.get("year", 1998)

    # Get tables
    web_sales = ctx.get_table("web_sales")
    catalog_sales = ctx.get_table("catalog_sales")
    date_dim = ctx.get_table("date_dim")

    # Union web and catalog sales (wscs CTE)
    ws = web_sales.select(
        [
            col("ws_sold_date_sk").alias("sold_date_sk"),
            col("ws_ext_sales_price").alias("sales_price"),
        ]
    )
    cs = catalog_sales.select(
        [
            col("cs_sold_date_sk").alias("sold_date_sk"),
            col("cs_ext_sales_price").alias("sales_price"),
        ]
    )
    wscs = ctx.concat([ws, cs])

    # Join with date_dim and aggregate by week with day-of-week pivoting (wswscs CTE)
    joined = wscs.join(
        date_dim,
        left_on="sold_date_sk",
        right_on="d_date_sk",
        how="inner",
    )

    wswscs = joined.group_by("d_week_seq").agg(
        [
            ctx.when(col("d_day_name") == "Sunday").then(col("sales_price")).otherwise(None).sum().alias("sun_sales"),
            ctx.when(col("d_day_name") == "Monday").then(col("sales_price")).otherwise(None).sum().alias("mon_sales"),
            ctx.when(col("d_day_name") == "Tuesday").then(col("sales_price")).otherwise(None).sum().alias("tue_sales"),
            ctx.when(col("d_day_name") == "Wednesday")
            .then(col("sales_price"))
            .otherwise(None)
            .sum()
            .alias("wed_sales"),
            ctx.when(col("d_day_name") == "Thursday").then(col("sales_price")).otherwise(None).sum().alias("thu_sales"),
            ctx.when(col("d_day_name") == "Friday").then(col("sales_price")).otherwise(None).sum().alias("fri_sales"),
            ctx.when(col("d_day_name") == "Saturday").then(col("sales_price")).otherwise(None).sum().alias("sat_sales"),
        ]
    )

    # Get year's weeks - join with date_dim to filter by year
    date_weeks = date_dim.filter(col("d_year") == year).select(["d_week_seq"]).unique()
    y1 = wswscs.join(date_weeks, on="d_week_seq", how="inner").select(
        [
            col("d_week_seq").alias("d_week_seq1"),
            col("sun_sales").alias("sun_sales1"),
            col("mon_sales").alias("mon_sales1"),
            col("tue_sales").alias("tue_sales1"),
            col("wed_sales").alias("wed_sales1"),
            col("thu_sales").alias("thu_sales1"),
            col("fri_sales").alias("fri_sales1"),
            col("sat_sales").alias("sat_sales1"),
        ]
    )

    # Get next year's weeks
    date_weeks_next = date_dim.filter(col("d_year") == year + 1).select(["d_week_seq"]).unique()
    y2 = wswscs.join(date_weeks_next, on="d_week_seq", how="inner").select(
        [
            col("d_week_seq").alias("d_week_seq2"),
            col("sun_sales").alias("sun_sales2"),
            col("mon_sales").alias("mon_sales2"),
            col("tue_sales").alias("tue_sales2"),
            col("wed_sales").alias("wed_sales2"),
            col("thu_sales").alias("thu_sales2"),
            col("fri_sales").alias("fri_sales2"),
            col("sat_sales").alias("sat_sales2"),
        ]
    )

    # Join where week_seq1 = week_seq2 - 53
    result = (
        y1.join(
            y2,
            left_on="d_week_seq1",
            right_on=col("d_week_seq2") - 53,
            how="inner",
        )
        .select(
            [
                col("d_week_seq1"),
                (col("sun_sales1") / col("sun_sales2")).round(2).alias("sun_ratio"),
                (col("mon_sales1") / col("mon_sales2")).round(2).alias("mon_ratio"),
                (col("tue_sales1") / col("tue_sales2")).round(2).alias("tue_ratio"),
                (col("wed_sales1") / col("wed_sales2")).round(2).alias("wed_ratio"),
                (col("thu_sales1") / col("thu_sales2")).round(2).alias("thu_ratio"),
                (col("fri_sales1") / col("fri_sales2")).round(2).alias("fri_ratio"),
                (col("sat_sales1") / col("sat_sales2")).round(2).alias("sat_ratio"),
            ]
        )
        .sort("d_week_seq1")
    )

    return result


def q2_pandas_impl(ctx: DataFrameContext) -> Any:
    """Q2: Web/Catalog weekly sales comparison year-over-year (Pandas)."""

    # Parameters
    params = get_parameters(2)
    year = params.get("year", 1998)

    # Get tables
    web_sales = ctx.get_table("web_sales")
    catalog_sales = ctx.get_table("catalog_sales")
    date_dim = ctx.get_table("date_dim")

    # Union web and catalog sales
    ws = web_sales[["ws_sold_date_sk", "ws_ext_sales_price"]].rename(
        columns={"ws_sold_date_sk": "sold_date_sk", "ws_ext_sales_price": "sales_price"}
    )
    cs = catalog_sales[["cs_sold_date_sk", "cs_ext_sales_price"]].rename(
        columns={"cs_sold_date_sk": "sold_date_sk", "cs_ext_sales_price": "sales_price"}
    )
    wscs = ctx.concat([ws, cs])

    # Join with date_dim
    joined = wscs.merge(date_dim, left_on="sold_date_sk", right_on="d_date_sk", how="inner")

    # Pivot by day of week and aggregate by week_seq
    # Use vectorized operations instead of .apply() for Dask compatibility
    days = ["Sunday", "Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday"]
    day_cols = ["sun_sales", "mon_sales", "tue_sales", "wed_sales", "thu_sales", "fri_sales", "sat_sales"]

    # Create conditional columns using vectorized where
    for day, day_col in zip(days, day_cols):
        # Use Dask-compatible conditional assignment
        mask = joined["d_day_name"] == day
        joined[day_col] = joined["sales_price"].where(mask, 0)

    agg_dict = dict.fromkeys(day_cols, "sum")
    wswscs = ctx.groupby_agg(joined, "d_week_seq", agg_dict, as_index=False)

    # Get weeks for each year (compute to set for Dask .isin() compatibility)
    year_weeks = date_dim[date_dim["d_year"] == year]["d_week_seq"]
    next_year_weeks = date_dim[date_dim["d_year"] == year + 1]["d_week_seq"]
    year_weeks_set = ctx.to_set(year_weeks)
    next_year_weeks_set = ctx.to_set(next_year_weeks)

    y1 = wswscs[wswscs["d_week_seq"].isin(year_weeks_set)].copy()
    y1.columns = ["d_week_seq1"] + [f"{c}1" for c in day_cols]

    y2 = wswscs[wswscs["d_week_seq"].isin(next_year_weeks_set)].copy()
    y2.columns = ["d_week_seq2"] + [f"{c}2" for c in day_cols]

    # Join where week_seq1 = week_seq2 - 53
    y2["join_key"] = y2["d_week_seq2"] - 53
    result = y1.merge(y2, left_on="d_week_seq1", right_on="join_key", how="inner")

    # Calculate ratios
    for day_col in day_cols:
        result[f"{day_col[:3]}_ratio"] = (result[f"{day_col}1"] / result[f"{day_col}2"]).round(2)

    result = result[
        ["d_week_seq1", "sun_ratio", "mon_ratio", "tue_ratio", "wed_ratio", "thu_ratio", "fri_ratio", "sat_ratio"]
    ]
    result = result.sort_values("d_week_seq1")

    return result


# =============================================================================
# Q31: Store/Web Quarterly Sales Growth by County
# =============================================================================


def q31_expression_impl(ctx: DataFrameContext) -> Any:
    """Q31: Store/Web quarterly sales growth by county (Polars).

    Compare Q1->Q2 and Q2->Q3 sales growth between store and web channels,
    filtering for counties where web growth exceeds store growth.
    """
    col = ctx.col
    lit = ctx.lit

    # Parameters
    params = get_parameters(31)
    year = params.get("year", 2000)

    # Get tables
    store_sales = ctx.get_table("store_sales")
    web_sales = ctx.get_table("web_sales")
    date_dim = ctx.get_table("date_dim")
    customer_address = ctx.get_table("customer_address")

    # Build store sales CTE (ss)
    ss = (
        store_sales.join(date_dim, left_on="ss_sold_date_sk", right_on="d_date_sk", how="inner")
        .join(customer_address, left_on="ss_addr_sk", right_on="ca_address_sk", how="inner")
        .group_by(["ca_county", "d_qoy", "d_year"])
        .agg(col("ss_ext_sales_price").sum().alias("store_sales"))
    )

    # Build web sales CTE (ws)
    ws = (
        web_sales.join(date_dim, left_on="ws_sold_date_sk", right_on="d_date_sk", how="inner")
        .join(customer_address, left_on="ws_bill_addr_sk", right_on="ca_address_sk", how="inner")
        .group_by(["ca_county", "d_qoy", "d_year"])
        .agg(col("ws_ext_sales_price").sum().alias("web_sales"))
    )

    # Filter for the year and Q1, Q2, Q3
    ss1 = ss.filter((col("d_qoy") == 1) & (col("d_year") == year))
    ss2 = ss.filter((col("d_qoy") == 2) & (col("d_year") == year))
    ss3 = ss.filter((col("d_qoy") == 3) & (col("d_year") == year))

    ws1 = ws.filter((col("d_qoy") == 1) & (col("d_year") == year))
    ws2 = ws.filter((col("d_qoy") == 2) & (col("d_year") == year))
    ws3 = ws.filter((col("d_qoy") == 3) & (col("d_year") == year))

    # Join all quarters by county
    result = (
        ss1.select([col("ca_county"), col("store_sales").alias("ss1_sales")])
        .join(
            ss2.select([col("ca_county"), col("store_sales").alias("ss2_sales")]),
            on="ca_county",
            how="inner",
        )
        .join(
            ss3.select([col("ca_county"), col("store_sales").alias("ss3_sales")]),
            on="ca_county",
            how="inner",
        )
        .join(
            ws1.select([col("ca_county"), col("web_sales").alias("ws1_sales")]),
            on="ca_county",
            how="inner",
        )
        .join(
            ws2.select([col("ca_county"), col("web_sales").alias("ws2_sales")]),
            on="ca_county",
            how="inner",
        )
        .join(
            ws3.select([col("ca_county"), col("web_sales").alias("ws3_sales")]),
            on="ca_county",
            how="inner",
        )
    )

    # Calculate growth ratios and filter
    result = result.with_columns(
        [
            lit(year).alias("d_year"),
            (col("ws2_sales") / col("ws1_sales")).alias("web_q1_q2_increase"),
            (col("ss2_sales") / col("ss1_sales")).alias("store_q1_q2_increase"),
            (col("ws3_sales") / col("ws2_sales")).alias("web_q2_q3_increase"),
            (col("ss3_sales") / col("ss2_sales")).alias("store_q2_q3_increase"),
        ]
    )

    # Filter where web growth > store growth for both periods
    result = result.filter(
        (
            ctx.when(col("ws1_sales") > 0).then(col("ws2_sales") / col("ws1_sales")).otherwise(None)
            > ctx.when(col("ss1_sales") > 0).then(col("ss2_sales") / col("ss1_sales")).otherwise(None)
        )
        & (
            ctx.when(col("ws2_sales") > 0).then(col("ws3_sales") / col("ws2_sales")).otherwise(None)
            > ctx.when(col("ss2_sales") > 0).then(col("ss3_sales") / col("ss2_sales")).otherwise(None)
        )
    )

    result = result.select(
        [
            col("ca_county"),
            col("d_year"),
            col("web_q1_q2_increase"),
            col("store_q1_q2_increase"),
            col("web_q2_q3_increase"),
            col("store_q2_q3_increase"),
        ]
    ).sort("ca_county")

    return result


def q31_pandas_impl(ctx: DataFrameContext) -> Any:
    """Q31: Store/Web quarterly sales growth by county (Pandas)."""
    # Parameters
    params = get_parameters(31)
    year = params.get("year", 2000)

    # Get tables
    store_sales = ctx.get_table("store_sales")
    web_sales = ctx.get_table("web_sales")
    date_dim = ctx.get_table("date_dim")
    customer_address = ctx.get_table("customer_address")

    # Build store sales
    ss = (
        store_sales.merge(date_dim, left_on="ss_sold_date_sk", right_on="d_date_sk", how="inner")
        .merge(customer_address, left_on="ss_addr_sk", right_on="ca_address_sk", how="inner")
        .groupby(["ca_county", "d_qoy", "d_year"], as_index=False)
        .agg(store_sales=("ss_ext_sales_price", "sum"))
    )

    # Build web sales
    ws = (
        web_sales.merge(date_dim, left_on="ws_sold_date_sk", right_on="d_date_sk", how="inner")
        .merge(customer_address, left_on="ws_bill_addr_sk", right_on="ca_address_sk", how="inner")
        .groupby(["ca_county", "d_qoy", "d_year"], as_index=False)
        .agg(web_sales=("ws_ext_sales_price", "sum"))
    )

    # Filter for each quarter
    ss1 = ss[(ss["d_qoy"] == 1) & (ss["d_year"] == year)][["ca_county", "store_sales"]].rename(
        columns={"store_sales": "ss1_sales"}
    )
    ss2 = ss[(ss["d_qoy"] == 2) & (ss["d_year"] == year)][["ca_county", "store_sales"]].rename(
        columns={"store_sales": "ss2_sales"}
    )
    ss3 = ss[(ss["d_qoy"] == 3) & (ss["d_year"] == year)][["ca_county", "store_sales"]].rename(
        columns={"store_sales": "ss3_sales"}
    )

    ws1 = ws[(ws["d_qoy"] == 1) & (ws["d_year"] == year)][["ca_county", "web_sales"]].rename(
        columns={"web_sales": "ws1_sales"}
    )
    ws2 = ws[(ws["d_qoy"] == 2) & (ws["d_year"] == year)][["ca_county", "web_sales"]].rename(
        columns={"web_sales": "ws2_sales"}
    )
    ws3 = ws[(ws["d_qoy"] == 3) & (ws["d_year"] == year)][["ca_county", "web_sales"]].rename(
        columns={"web_sales": "ws3_sales"}
    )

    # Join all quarters
    result = (
        ss1.merge(ss2, on="ca_county", how="inner")
        .merge(ss3, on="ca_county", how="inner")
        .merge(ws1, on="ca_county", how="inner")
        .merge(ws2, on="ca_county", how="inner")
        .merge(ws3, on="ca_county", how="inner")
    )

    # Calculate growth ratios
    result["d_year"] = year
    result["web_q1_q2_increase"] = result["ws2_sales"] / result["ws1_sales"]
    result["store_q1_q2_increase"] = result["ss2_sales"] / result["ss1_sales"]
    result["web_q2_q3_increase"] = result["ws3_sales"] / result["ws2_sales"]
    result["store_q2_q3_increase"] = result["ss3_sales"] / result["ss2_sales"]

    # Filter: web growth > store growth for both periods
    def safe_ratio(num, denom):
        return num / denom if denom > 0 else None

    result = result[
        result.apply(
            lambda r: (safe_ratio(r["ws2_sales"], r["ws1_sales"]) or 0)
            > (safe_ratio(r["ss2_sales"], r["ss1_sales"]) or 0)
            and (safe_ratio(r["ws3_sales"], r["ws2_sales"]) or 0) > (safe_ratio(r["ss3_sales"], r["ss2_sales"]) or 0),
            axis=1,
        )
    ]

    result = result[
        [
            "ca_county",
            "d_year",
            "web_q1_q2_increase",
            "store_q1_q2_increase",
            "web_q2_q3_increase",
            "store_q2_q3_increase",
        ]
    ].sort_values("ca_county")

    return result


# =============================================================================
# Q33: Three-Channel Sales by Manufacturer (Category Filter)
# =============================================================================


def q33_expression_impl(ctx: DataFrameContext) -> Any:
    """Q33: Three-channel sales by manufacturer with category filter (Polars).

    Union store, catalog, and web sales filtered by item category and GMT offset,
    aggregate by manufacturer.
    """

    col = ctx.col

    # Parameters
    params = get_parameters(33)
    year = params.get("year", 1998)
    month = params.get("month", 1)
    gmt_offset = params.get("gmt_offset", -5)
    category = params.get("category", "Books")

    # Get tables
    store_sales = ctx.get_table("store_sales")
    catalog_sales = ctx.get_table("catalog_sales")
    web_sales = ctx.get_table("web_sales")
    date_dim = ctx.get_table("date_dim")
    customer_address = ctx.get_table("customer_address")
    item = ctx.get_table("item")

    # Get manufacturer IDs for the category
    mfg_ids = item.filter(col("i_category") == category).select("i_manufact_id").unique()

    # Filter date
    date_filter = date_dim.filter((col("d_year") == year) & (col("d_moy") == month))

    # Filter address by GMT offset
    addr_filter = customer_address.filter(col("ca_gmt_offset") == gmt_offset)

    # Store sales CTE
    ss = (
        store_sales.join(item, left_on="ss_item_sk", right_on="i_item_sk", how="inner")
        .join(date_filter, left_on="ss_sold_date_sk", right_on="d_date_sk", how="inner")
        .join(addr_filter, left_on="ss_addr_sk", right_on="ca_address_sk", how="inner")
        .join(mfg_ids, on="i_manufact_id", how="semi")
        .group_by("i_manufact_id")
        .agg(col("ss_ext_sales_price").sum().alias("total_sales"))
    )

    # Catalog sales CTE
    cs = (
        catalog_sales.join(item, left_on="cs_item_sk", right_on="i_item_sk", how="inner")
        .join(date_filter, left_on="cs_sold_date_sk", right_on="d_date_sk", how="inner")
        .join(addr_filter, left_on="cs_bill_addr_sk", right_on="ca_address_sk", how="inner")
        .join(mfg_ids, on="i_manufact_id", how="semi")
        .group_by("i_manufact_id")
        .agg(col("cs_ext_sales_price").sum().alias("total_sales"))
    )

    # Web sales CTE
    ws = (
        web_sales.join(item, left_on="ws_item_sk", right_on="i_item_sk", how="inner")
        .join(date_filter, left_on="ws_sold_date_sk", right_on="d_date_sk", how="inner")
        .join(addr_filter, left_on="ws_bill_addr_sk", right_on="ca_address_sk", how="inner")
        .join(mfg_ids, on="i_manufact_id", how="semi")
        .group_by("i_manufact_id")
        .agg(col("ws_ext_sales_price").sum().alias("total_sales"))
    )

    # Union all channels
    combined = ctx.concat([ss, cs, ws])

    # Final aggregation
    result = (
        combined.group_by("i_manufact_id")
        .agg(col("total_sales").sum().alias("total_sales"))
        .sort("total_sales")
        .head(100)
    )

    return result


def q33_pandas_impl(ctx: DataFrameContext) -> Any:
    """Q33: Three-channel sales by manufacturer with category filter (Pandas)."""

    # Parameters
    params = get_parameters(33)
    year = params.get("year", 1998)
    month = params.get("month", 1)
    gmt_offset = params.get("gmt_offset", -5)
    category = params.get("category", "Books")

    # Get tables
    store_sales = ctx.get_table("store_sales")
    catalog_sales = ctx.get_table("catalog_sales")
    web_sales = ctx.get_table("web_sales")
    date_dim = ctx.get_table("date_dim")
    customer_address = ctx.get_table("customer_address")
    item = ctx.get_table("item")

    # Get manufacturer IDs for the category
    mfg_ids = item[item["i_category"] == category]["i_manufact_id"].unique()

    # Filter date
    date_filter = date_dim[(date_dim["d_year"] == year) & (date_dim["d_moy"] == month)]

    # Filter address by GMT offset
    addr_filter = customer_address[customer_address["ca_gmt_offset"] == gmt_offset]

    # Store sales
    ss = (
        store_sales.merge(
            item[item["i_manufact_id"].isin(mfg_ids)], left_on="ss_item_sk", right_on="i_item_sk", how="inner"
        )
        .merge(date_filter, left_on="ss_sold_date_sk", right_on="d_date_sk", how="inner")
        .merge(addr_filter, left_on="ss_addr_sk", right_on="ca_address_sk", how="inner")
        .groupby("i_manufact_id", as_index=False)
        .agg(total_sales=("ss_ext_sales_price", "sum"))
    )

    # Catalog sales
    cs = (
        catalog_sales.merge(
            item[item["i_manufact_id"].isin(mfg_ids)], left_on="cs_item_sk", right_on="i_item_sk", how="inner"
        )
        .merge(date_filter, left_on="cs_sold_date_sk", right_on="d_date_sk", how="inner")
        .merge(addr_filter, left_on="cs_bill_addr_sk", right_on="ca_address_sk", how="inner")
        .groupby("i_manufact_id", as_index=False)
        .agg(total_sales=("cs_ext_sales_price", "sum"))
    )

    # Web sales
    ws = (
        web_sales.merge(
            item[item["i_manufact_id"].isin(mfg_ids)], left_on="ws_item_sk", right_on="i_item_sk", how="inner"
        )
        .merge(date_filter, left_on="ws_sold_date_sk", right_on="d_date_sk", how="inner")
        .merge(addr_filter, left_on="ws_bill_addr_sk", right_on="ca_address_sk", how="inner")
        .groupby("i_manufact_id", as_index=False)
        .agg(total_sales=("ws_ext_sales_price", "sum"))
    )

    # Union and aggregate
    combined = ctx.concat([ss, cs, ws])
    result = (
        combined.groupby("i_manufact_id", as_index=False)
        .agg(total_sales=("total_sales", "sum"))
        .sort_values("total_sales")
        .head(100)
    )

    return result


# =============================================================================
# Q56: Three-Channel Sales by Item (Color Filter)
# =============================================================================


def q56_expression_impl(ctx: DataFrameContext) -> Any:
    """Q56: Three-channel sales by item with color filter (Polars).

    Union store, catalog, and web sales filtered by item colors and GMT offset,
    aggregate by item_id.
    """

    col = ctx.col

    # Parameters
    params = get_parameters(56)
    year = params.get("year", 1998)
    month = params.get("month", 1)
    gmt_offset = params.get("gmt_offset", -5)
    colors = params.get("colors", ["pale", "chiffon", "thistle"])

    # Get tables
    store_sales = ctx.get_table("store_sales")
    catalog_sales = ctx.get_table("catalog_sales")
    web_sales = ctx.get_table("web_sales")
    date_dim = ctx.get_table("date_dim")
    customer_address = ctx.get_table("customer_address")
    item = ctx.get_table("item")

    # Get item IDs for the colors
    item_ids = item.filter(col("i_color").is_in(colors)).select("i_item_id").unique()

    # Filter date
    date_filter = date_dim.filter((col("d_year") == year) & (col("d_moy") == month))

    # Filter address by GMT offset
    addr_filter = customer_address.filter(col("ca_gmt_offset") == gmt_offset)

    # Store sales CTE
    ss = (
        store_sales.join(item, left_on="ss_item_sk", right_on="i_item_sk", how="inner")
        .join(date_filter, left_on="ss_sold_date_sk", right_on="d_date_sk", how="inner")
        .join(addr_filter, left_on="ss_addr_sk", right_on="ca_address_sk", how="inner")
        .join(item_ids, on="i_item_id", how="semi")
        .group_by("i_item_id")
        .agg(col("ss_ext_sales_price").sum().alias("total_sales"))
    )

    # Catalog sales CTE
    cs = (
        catalog_sales.join(item, left_on="cs_item_sk", right_on="i_item_sk", how="inner")
        .join(date_filter, left_on="cs_sold_date_sk", right_on="d_date_sk", how="inner")
        .join(addr_filter, left_on="cs_bill_addr_sk", right_on="ca_address_sk", how="inner")
        .join(item_ids, on="i_item_id", how="semi")
        .group_by("i_item_id")
        .agg(col("cs_ext_sales_price").sum().alias("total_sales"))
    )

    # Web sales CTE
    ws = (
        web_sales.join(item, left_on="ws_item_sk", right_on="i_item_sk", how="inner")
        .join(date_filter, left_on="ws_sold_date_sk", right_on="d_date_sk", how="inner")
        .join(addr_filter, left_on="ws_bill_addr_sk", right_on="ca_address_sk", how="inner")
        .join(item_ids, on="i_item_id", how="semi")
        .group_by("i_item_id")
        .agg(col("ws_ext_sales_price").sum().alias("total_sales"))
    )

    # Union all channels
    combined = ctx.concat([ss, cs, ws])

    # Final aggregation
    result = (
        combined.group_by("i_item_id")
        .agg(col("total_sales").sum().alias("total_sales"))
        .sort(["total_sales", "i_item_id"])
        .head(100)
    )

    return result


def q56_pandas_impl(ctx: DataFrameContext) -> Any:
    """Q56: Three-channel sales by item with color filter (Pandas)."""

    # Parameters
    params = get_parameters(56)
    year = params.get("year", 1998)
    month = params.get("month", 1)
    gmt_offset = params.get("gmt_offset", -5)
    colors = params.get("colors", ["pale", "chiffon", "thistle"])

    # Get tables
    store_sales = ctx.get_table("store_sales")
    catalog_sales = ctx.get_table("catalog_sales")
    web_sales = ctx.get_table("web_sales")
    date_dim = ctx.get_table("date_dim")
    customer_address = ctx.get_table("customer_address")
    item = ctx.get_table("item")

    # Get item IDs for the colors
    item_ids = item[item["i_color"].isin(colors)]["i_item_id"].unique()

    # Filter date
    date_filter = date_dim[(date_dim["d_year"] == year) & (date_dim["d_moy"] == month)]

    # Filter address
    addr_filter = customer_address[customer_address["ca_gmt_offset"] == gmt_offset]

    # Store sales
    ss = (
        store_sales.merge(
            item[item["i_item_id"].isin(item_ids)], left_on="ss_item_sk", right_on="i_item_sk", how="inner"
        )
        .merge(date_filter, left_on="ss_sold_date_sk", right_on="d_date_sk", how="inner")
        .merge(addr_filter, left_on="ss_addr_sk", right_on="ca_address_sk", how="inner")
        .groupby("i_item_id", as_index=False)
        .agg(total_sales=("ss_ext_sales_price", "sum"))
    )

    # Catalog sales
    cs = (
        catalog_sales.merge(
            item[item["i_item_id"].isin(item_ids)], left_on="cs_item_sk", right_on="i_item_sk", how="inner"
        )
        .merge(date_filter, left_on="cs_sold_date_sk", right_on="d_date_sk", how="inner")
        .merge(addr_filter, left_on="cs_bill_addr_sk", right_on="ca_address_sk", how="inner")
        .groupby("i_item_id", as_index=False)
        .agg(total_sales=("cs_ext_sales_price", "sum"))
    )

    # Web sales
    ws = (
        web_sales.merge(item[item["i_item_id"].isin(item_ids)], left_on="ws_item_sk", right_on="i_item_sk", how="inner")
        .merge(date_filter, left_on="ws_sold_date_sk", right_on="d_date_sk", how="inner")
        .merge(addr_filter, left_on="ws_bill_addr_sk", right_on="ca_address_sk", how="inner")
        .groupby("i_item_id", as_index=False)
        .agg(total_sales=("ws_ext_sales_price", "sum"))
    )

    # Union and aggregate
    combined = ctx.concat([ss, cs, ws])
    result = (
        combined.groupby("i_item_id", as_index=False)
        .agg(total_sales=("total_sales", "sum"))
        .sort_values(["total_sales", "i_item_id"])
        .head(100)
    )

    return result


# =============================================================================
# Q60: Three-Channel Sales by Item (Category Filter)
# =============================================================================


def q60_expression_impl(ctx: DataFrameContext) -> Any:
    """Q60: Three-channel sales by item with category filter (Polars).

    Similar to Q56 but filtered by item category instead of color.
    """

    col = ctx.col

    # Parameters
    params = get_parameters(60)
    year = params.get("year", 1998)
    month = params.get("month", 8)
    gmt_offset = params.get("gmt_offset", -5)
    category = params.get("category", "Children")

    # Get tables
    store_sales = ctx.get_table("store_sales")
    catalog_sales = ctx.get_table("catalog_sales")
    web_sales = ctx.get_table("web_sales")
    date_dim = ctx.get_table("date_dim")
    customer_address = ctx.get_table("customer_address")
    item = ctx.get_table("item")

    # Get item IDs for the category
    item_ids = item.filter(col("i_category") == category).select("i_item_id").unique()

    # Filter date
    date_filter = date_dim.filter((col("d_year") == year) & (col("d_moy") == month))

    # Filter address by GMT offset
    addr_filter = customer_address.filter(col("ca_gmt_offset") == gmt_offset)

    # Store sales CTE
    ss = (
        store_sales.join(item, left_on="ss_item_sk", right_on="i_item_sk", how="inner")
        .join(date_filter, left_on="ss_sold_date_sk", right_on="d_date_sk", how="inner")
        .join(addr_filter, left_on="ss_addr_sk", right_on="ca_address_sk", how="inner")
        .join(item_ids, on="i_item_id", how="semi")
        .group_by("i_item_id")
        .agg(col("ss_ext_sales_price").sum().alias("total_sales"))
    )

    # Catalog sales CTE
    cs = (
        catalog_sales.join(item, left_on="cs_item_sk", right_on="i_item_sk", how="inner")
        .join(date_filter, left_on="cs_sold_date_sk", right_on="d_date_sk", how="inner")
        .join(addr_filter, left_on="cs_bill_addr_sk", right_on="ca_address_sk", how="inner")
        .join(item_ids, on="i_item_id", how="semi")
        .group_by("i_item_id")
        .agg(col("cs_ext_sales_price").sum().alias("total_sales"))
    )

    # Web sales CTE
    ws = (
        web_sales.join(item, left_on="ws_item_sk", right_on="i_item_sk", how="inner")
        .join(date_filter, left_on="ws_sold_date_sk", right_on="d_date_sk", how="inner")
        .join(addr_filter, left_on="ws_bill_addr_sk", right_on="ca_address_sk", how="inner")
        .join(item_ids, on="i_item_id", how="semi")
        .group_by("i_item_id")
        .agg(col("ws_ext_sales_price").sum().alias("total_sales"))
    )

    # Union all channels
    combined = ctx.concat([ss, cs, ws])

    # Final aggregation
    result = (
        combined.group_by("i_item_id")
        .agg(col("total_sales").sum().alias("total_sales"))
        .sort(["i_item_id", "total_sales"])
        .head(100)
    )

    return result


def q60_pandas_impl(ctx: DataFrameContext) -> Any:
    """Q60: Three-channel sales by item with category filter (Pandas)."""

    # Parameters
    params = get_parameters(60)
    year = params.get("year", 1998)
    month = params.get("month", 8)
    gmt_offset = params.get("gmt_offset", -5)
    category = params.get("category", "Children")

    # Get tables
    store_sales = ctx.get_table("store_sales")
    catalog_sales = ctx.get_table("catalog_sales")
    web_sales = ctx.get_table("web_sales")
    date_dim = ctx.get_table("date_dim")
    customer_address = ctx.get_table("customer_address")
    item = ctx.get_table("item")

    # Get item IDs for the category
    item_ids = item[item["i_category"] == category]["i_item_id"].unique()

    # Filter date
    date_filter = date_dim[(date_dim["d_year"] == year) & (date_dim["d_moy"] == month)]

    # Filter address
    addr_filter = customer_address[customer_address["ca_gmt_offset"] == gmt_offset]

    # Store sales
    ss = (
        store_sales.merge(
            item[item["i_item_id"].isin(item_ids)], left_on="ss_item_sk", right_on="i_item_sk", how="inner"
        )
        .merge(date_filter, left_on="ss_sold_date_sk", right_on="d_date_sk", how="inner")
        .merge(addr_filter, left_on="ss_addr_sk", right_on="ca_address_sk", how="inner")
        .groupby("i_item_id", as_index=False)
        .agg(total_sales=("ss_ext_sales_price", "sum"))
    )

    # Catalog sales
    cs = (
        catalog_sales.merge(
            item[item["i_item_id"].isin(item_ids)], left_on="cs_item_sk", right_on="i_item_sk", how="inner"
        )
        .merge(date_filter, left_on="cs_sold_date_sk", right_on="d_date_sk", how="inner")
        .merge(addr_filter, left_on="cs_bill_addr_sk", right_on="ca_address_sk", how="inner")
        .groupby("i_item_id", as_index=False)
        .agg(total_sales=("cs_ext_sales_price", "sum"))
    )

    # Web sales
    ws = (
        web_sales.merge(item[item["i_item_id"].isin(item_ids)], left_on="ws_item_sk", right_on="i_item_sk", how="inner")
        .merge(date_filter, left_on="ws_sold_date_sk", right_on="d_date_sk", how="inner")
        .merge(addr_filter, left_on="ws_bill_addr_sk", right_on="ca_address_sk", how="inner")
        .groupby("i_item_id", as_index=False)
        .agg(total_sales=("ws_ext_sales_price", "sum"))
    )

    # Union and aggregate
    combined = ctx.concat([ss, cs, ws])
    result = (
        combined.groupby("i_item_id", as_index=False)
        .agg(total_sales=("total_sales", "sum"))
        .sort_values(["i_item_id", "total_sales"])
        .head(100)
    )

    return result


# =============================================================================
# Q71: Three-Channel Sales by Brand and Time
# =============================================================================


def q71_expression_impl(ctx: DataFrameContext) -> Any:
    """Q71: Three-channel sales by brand and meal time (Polars).

    Union web, catalog, and store sales for a specific month/year,
    join with item and time_dim, aggregate by brand and time.
    """

    col = ctx.col

    # Parameters
    params = get_parameters(71)
    year = params.get("year", 1999)
    month = params.get("month", 11)

    # Get tables
    web_sales = ctx.get_table("web_sales")
    catalog_sales = ctx.get_table("catalog_sales")
    store_sales = ctx.get_table("store_sales")
    date_dim = ctx.get_table("date_dim")
    item = ctx.get_table("item")
    time_dim = ctx.get_table("time_dim")

    # Filter date
    date_filter = date_dim.filter((col("d_year") == year) & (col("d_moy") == month))

    # Web sales
    ws = web_sales.join(date_filter, left_on="ws_sold_date_sk", right_on="d_date_sk", how="inner").select(
        [
            col("ws_ext_sales_price").alias("ext_price"),
            col("ws_item_sk").alias("sold_item_sk"),
            col("ws_sold_time_sk").alias("time_sk"),
        ]
    )

    # Catalog sales
    cs = catalog_sales.join(date_filter, left_on="cs_sold_date_sk", right_on="d_date_sk", how="inner").select(
        [
            col("cs_ext_sales_price").alias("ext_price"),
            col("cs_item_sk").alias("sold_item_sk"),
            col("cs_sold_time_sk").alias("time_sk"),
        ]
    )

    # Store sales
    ss = store_sales.join(date_filter, left_on="ss_sold_date_sk", right_on="d_date_sk", how="inner").select(
        [
            col("ss_ext_sales_price").alias("ext_price"),
            col("ss_item_sk").alias("sold_item_sk"),
            col("ss_sold_time_sk").alias("time_sk"),
        ]
    )

    # Union all channels
    combined = ctx.concat([ws, cs, ss])

    # Join with item and time_dim
    result = (
        combined.join(item.filter(col("i_manager_id") == 1), left_on="sold_item_sk", right_on="i_item_sk", how="inner")
        .join(
            time_dim.filter(col("t_meal_time").is_in(["breakfast", "dinner"])),
            left_on="time_sk",
            right_on="t_time_sk",
            how="inner",
        )
        .group_by(["i_brand_id", "i_brand", "t_hour", "t_minute"])
        .agg(col("ext_price").sum().alias("ext_price"))
        .sort(["ext_price", "i_brand_id"], descending=[True, False])
    )

    return result


def q71_pandas_impl(ctx: DataFrameContext) -> Any:
    """Q71: Three-channel sales by brand and meal time (Pandas)."""

    # Parameters
    params = get_parameters(71)
    year = params.get("year", 1999)
    month = params.get("month", 11)

    # Get tables
    web_sales = ctx.get_table("web_sales")
    catalog_sales = ctx.get_table("catalog_sales")
    store_sales = ctx.get_table("store_sales")
    date_dim = ctx.get_table("date_dim")
    item = ctx.get_table("item")
    time_dim = ctx.get_table("time_dim")

    # Filter date
    date_filter = date_dim[(date_dim["d_year"] == year) & (date_dim["d_moy"] == month)]

    # Web sales
    ws = web_sales.merge(date_filter, left_on="ws_sold_date_sk", right_on="d_date_sk", how="inner")[
        ["ws_ext_sales_price", "ws_item_sk", "ws_sold_time_sk"]
    ].rename(
        columns={
            "ws_ext_sales_price": "ext_price",
            "ws_item_sk": "sold_item_sk",
            "ws_sold_time_sk": "time_sk",
        }
    )

    # Catalog sales
    cs = catalog_sales.merge(date_filter, left_on="cs_sold_date_sk", right_on="d_date_sk", how="inner")[
        ["cs_ext_sales_price", "cs_item_sk", "cs_sold_time_sk"]
    ].rename(
        columns={
            "cs_ext_sales_price": "ext_price",
            "cs_item_sk": "sold_item_sk",
            "cs_sold_time_sk": "time_sk",
        }
    )

    # Store sales
    ss = store_sales.merge(date_filter, left_on="ss_sold_date_sk", right_on="d_date_sk", how="inner")[
        ["ss_ext_sales_price", "ss_item_sk", "ss_sold_time_sk"]
    ].rename(
        columns={
            "ss_ext_sales_price": "ext_price",
            "ss_item_sk": "sold_item_sk",
            "ss_sold_time_sk": "time_sk",
        }
    )

    # Union all channels
    combined = ctx.concat([ws, cs, ss])

    # Filter item and time
    item_filter = item[item["i_manager_id"] == 1]
    time_filter = time_dim[time_dim["t_meal_time"].isin(["breakfast", "dinner"])]

    # Join with item and time_dim
    result = (
        combined.merge(item_filter, left_on="sold_item_sk", right_on="i_item_sk", how="inner")
        .merge(time_filter, left_on="time_sk", right_on="t_time_sk", how="inner")
        .groupby(["i_brand_id", "i_brand", "t_hour", "t_minute"], as_index=False)
        .agg(ext_price=("ext_price", "sum"))
        .sort_values(["ext_price", "i_brand_id"], ascending=[False, True])
    )

    return result


# =============================================================================
# Q74: Store/Web Customer Year-over-Year Growth
# =============================================================================


def q74_expression_impl(ctx: DataFrameContext) -> Any:
    """Q74: Store/Web customer year-over-year growth comparison (Polars).

    Find customers where web sales growth exceeds store sales growth
    between consecutive years.
    """
    col = ctx.col
    lit = ctx.lit

    # Parameters
    params = get_parameters(74)
    year = params.get("year", 1998)

    # Get tables
    customer = ctx.get_table("customer")
    store_sales = ctx.get_table("store_sales")
    web_sales = ctx.get_table("web_sales")
    date_dim = ctx.get_table("date_dim")

    # Filter date for both years
    date_year1 = date_dim.filter(col("d_year") == year)
    date_year2 = date_dim.filter(col("d_year") == year + 1)

    # Store sales year 1
    ss_y1 = (
        store_sales.join(customer, left_on="ss_customer_sk", right_on="c_customer_sk", how="inner")
        .join(date_year1, left_on="ss_sold_date_sk", right_on="d_date_sk", how="inner")
        .group_by(["c_customer_id", "c_first_name", "c_last_name"])
        .agg(col("ss_net_paid").sum().alias("year_total"))
        .with_columns(
            [
                lit(year).alias("year"),
                lit("s").alias("sale_type"),
            ]
        )
    )

    # Store sales year 2
    ss_y2 = (
        store_sales.join(customer, left_on="ss_customer_sk", right_on="c_customer_sk", how="inner")
        .join(date_year2, left_on="ss_sold_date_sk", right_on="d_date_sk", how="inner")
        .group_by(["c_customer_id", "c_first_name", "c_last_name"])
        .agg(col("ss_net_paid").sum().alias("year_total"))
        .with_columns(
            [
                lit(year + 1).alias("year"),
                lit("s").alias("sale_type"),
            ]
        )
    )

    # Web sales year 1
    ws_y1 = (
        web_sales.join(customer, left_on="ws_bill_customer_sk", right_on="c_customer_sk", how="inner")
        .join(date_year1, left_on="ws_sold_date_sk", right_on="d_date_sk", how="inner")
        .group_by(["c_customer_id", "c_first_name", "c_last_name"])
        .agg(col("ws_net_paid").sum().alias("year_total"))
        .with_columns(
            [
                lit(year).alias("year"),
                lit("w").alias("sale_type"),
            ]
        )
    )

    # Web sales year 2
    ws_y2 = (
        web_sales.join(customer, left_on="ws_bill_customer_sk", right_on="c_customer_sk", how="inner")
        .join(date_year2, left_on="ws_sold_date_sk", right_on="d_date_sk", how="inner")
        .group_by(["c_customer_id", "c_first_name", "c_last_name"])
        .agg(col("ws_net_paid").sum().alias("year_total"))
        .with_columns(
            [
                lit(year + 1).alias("year"),
                lit("w").alias("sale_type"),
            ]
        )
    )

    # Join all four: s_y1, s_y2, w_y1, w_y2
    result = (
        ss_y1.select([col("c_customer_id"), col("year_total").alias("ss_y1_total")])
        .join(
            ss_y2.select([col("c_customer_id"), col("year_total").alias("ss_y2_total")]),
            on="c_customer_id",
            how="inner",
        )
        .join(
            ws_y1.select([col("c_customer_id"), col("year_total").alias("ws_y1_total")]),
            on="c_customer_id",
            how="inner",
        )
        .join(
            ws_y2.select([col("c_customer_id"), col("year_total").alias("ws_y2_total")]),
            on="c_customer_id",
            how="inner",
        )
        .join(
            ss_y2.select(["c_customer_id", "c_first_name", "c_last_name"]),
            on="c_customer_id",
            how="inner",
        )
    )

    # Filter where both first year totals > 0 and web growth > store growth
    result = result.filter(
        (col("ss_y1_total") > 0)
        & (col("ws_y1_total") > 0)
        & (
            ctx.when(col("ws_y1_total") > 0).then(col("ws_y2_total") / col("ws_y1_total")).otherwise(None)
            > ctx.when(col("ss_y1_total") > 0).then(col("ss_y2_total") / col("ss_y1_total")).otherwise(None)
        )
    )

    result = result.select(["c_customer_id", "c_first_name", "c_last_name"]).head(100)

    return result


def q74_pandas_impl(ctx: DataFrameContext) -> Any:
    """Q74: Store/Web customer year-over-year growth comparison (Pandas)."""
    # Parameters
    params = get_parameters(74)
    year = params.get("year", 1998)

    # Get tables
    customer = ctx.get_table("customer")
    store_sales = ctx.get_table("store_sales")
    web_sales = ctx.get_table("web_sales")
    date_dim = ctx.get_table("date_dim")

    # Filter dates
    date_year1 = date_dim[date_dim["d_year"] == year]
    date_year2 = date_dim[date_dim["d_year"] == year + 1]

    # Store sales year 1
    ss_y1 = (
        store_sales.merge(customer, left_on="ss_customer_sk", right_on="c_customer_sk", how="inner")
        .merge(date_year1, left_on="ss_sold_date_sk", right_on="d_date_sk", how="inner")
        .groupby(["c_customer_id", "c_first_name", "c_last_name"], as_index=False)
        .agg(ss_y1_total=("ss_net_paid", "sum"))
    )

    # Store sales year 2
    ss_y2 = (
        store_sales.merge(customer, left_on="ss_customer_sk", right_on="c_customer_sk", how="inner")
        .merge(date_year2, left_on="ss_sold_date_sk", right_on="d_date_sk", how="inner")
        .groupby(["c_customer_id"], as_index=False)
        .agg(ss_y2_total=("ss_net_paid", "sum"))
    )

    # Web sales year 1
    ws_y1 = (
        web_sales.merge(customer, left_on="ws_bill_customer_sk", right_on="c_customer_sk", how="inner")
        .merge(date_year1, left_on="ws_sold_date_sk", right_on="d_date_sk", how="inner")
        .groupby(["c_customer_id"], as_index=False)
        .agg(ws_y1_total=("ws_net_paid", "sum"))
    )

    # Web sales year 2
    ws_y2 = (
        web_sales.merge(customer, left_on="ws_bill_customer_sk", right_on="c_customer_sk", how="inner")
        .merge(date_year2, left_on="ws_sold_date_sk", right_on="d_date_sk", how="inner")
        .groupby(["c_customer_id"], as_index=False)
        .agg(ws_y2_total=("ws_net_paid", "sum"))
    )

    # Join all
    result = (
        ss_y1.merge(ss_y2, on="c_customer_id", how="inner")
        .merge(ws_y1, on="c_customer_id", how="inner")
        .merge(ws_y2, on="c_customer_id", how="inner")
    )

    # Filter: first year totals > 0 and web growth > store growth
    def safe_ratio(num, denom):
        return num / denom if denom > 0 else None

    result = result[
        (result["ss_y1_total"] > 0)
        & (result["ws_y1_total"] > 0)
        & result.apply(
            lambda r: (safe_ratio(r["ws_y2_total"], r["ws_y1_total"]) or 0)
            > (safe_ratio(r["ss_y2_total"], r["ss_y1_total"]) or 0),
            axis=1,
        )
    ]

    result = result[["c_customer_id", "c_first_name", "c_last_name"]].head(100)

    return result


# =============================================================================
# Q76: Three-Channel Sales with NULL Column Filter
# =============================================================================


def q76_expression_impl(ctx: DataFrameContext) -> Any:
    """Q76: Three-channel sales analysis with NULL column filter (Polars).

    Union store, web, catalog sales where a specific column is NULL,
    aggregate by channel, year, quarter, and category.
    """

    col = ctx.col
    lit = ctx.lit

    # Parameters
    params = get_parameters(76)
    null_col_ss = params.get("null_col_ss", "ss_customer_sk")
    null_col_ws = params.get("null_col_ws", "ws_bill_customer_sk")
    null_col_cs = params.get("null_col_cs", "cs_bill_customer_sk")

    # Get tables
    store_sales = ctx.get_table("store_sales")
    web_sales = ctx.get_table("web_sales")
    catalog_sales = ctx.get_table("catalog_sales")
    date_dim = ctx.get_table("date_dim")
    item = ctx.get_table("item")

    # Store sales with NULL filter
    ss = (
        store_sales.filter(col(null_col_ss).is_null())
        .join(date_dim, left_on="ss_sold_date_sk", right_on="d_date_sk", how="inner")
        .join(item, left_on="ss_item_sk", right_on="i_item_sk", how="inner")
        .select(
            [
                lit("store").alias("channel"),
                lit(null_col_ss).alias("col_name"),
                col("d_year"),
                col("d_qoy"),
                col("i_category"),
                col("ss_ext_sales_price").alias("ext_sales_price"),
            ]
        )
    )

    # Web sales with NULL filter
    ws = (
        web_sales.filter(col(null_col_ws).is_null())
        .join(date_dim, left_on="ws_sold_date_sk", right_on="d_date_sk", how="inner")
        .join(item, left_on="ws_item_sk", right_on="i_item_sk", how="inner")
        .select(
            [
                lit("web").alias("channel"),
                lit(null_col_ws).alias("col_name"),
                col("d_year"),
                col("d_qoy"),
                col("i_category"),
                col("ws_ext_sales_price").alias("ext_sales_price"),
            ]
        )
    )

    # Catalog sales with NULL filter
    cs = (
        catalog_sales.filter(col(null_col_cs).is_null())
        .join(date_dim, left_on="cs_sold_date_sk", right_on="d_date_sk", how="inner")
        .join(item, left_on="cs_item_sk", right_on="i_item_sk", how="inner")
        .select(
            [
                lit("catalog").alias("channel"),
                lit(null_col_cs).alias("col_name"),
                col("d_year"),
                col("d_qoy"),
                col("i_category"),
                col("cs_ext_sales_price").alias("ext_sales_price"),
            ]
        )
    )

    # Union all channels
    combined = ctx.concat([ss, ws, cs])

    # Aggregate
    result = (
        combined.group_by(["channel", "col_name", "d_year", "d_qoy", "i_category"])
        .agg(
            [
                ctx.count().alias("sales_cnt"),
                col("ext_sales_price").sum().alias("sales_amt"),
            ]
        )
        .sort(["channel", "col_name", "d_year", "d_qoy", "i_category"])
        .head(100)
    )

    return result


def q76_pandas_impl(ctx: DataFrameContext) -> Any:
    """Q76: Three-channel sales analysis with NULL column filter (Pandas)."""

    # Parameters
    params = get_parameters(76)
    null_col_ss = params.get("null_col_ss", "ss_customer_sk")
    null_col_ws = params.get("null_col_ws", "ws_bill_customer_sk")
    null_col_cs = params.get("null_col_cs", "cs_bill_customer_sk")

    # Get tables
    store_sales = ctx.get_table("store_sales")
    web_sales = ctx.get_table("web_sales")
    catalog_sales = ctx.get_table("catalog_sales")
    date_dim = ctx.get_table("date_dim")
    item = ctx.get_table("item")

    # Store sales with NULL filter
    ss = (
        store_sales[store_sales[null_col_ss].isna()]
        .merge(date_dim, left_on="ss_sold_date_sk", right_on="d_date_sk", how="inner")
        .merge(item, left_on="ss_item_sk", right_on="i_item_sk", how="inner")
    )
    ss["channel"] = "store"
    ss["col_name"] = null_col_ss
    ss = ss[["channel", "col_name", "d_year", "d_qoy", "i_category", "ss_ext_sales_price"]].rename(
        columns={"ss_ext_sales_price": "ext_sales_price"}
    )

    # Web sales with NULL filter
    ws = (
        web_sales[web_sales[null_col_ws].isna()]
        .merge(date_dim, left_on="ws_sold_date_sk", right_on="d_date_sk", how="inner")
        .merge(item, left_on="ws_item_sk", right_on="i_item_sk", how="inner")
    )
    ws["channel"] = "web"
    ws["col_name"] = null_col_ws
    ws = ws[["channel", "col_name", "d_year", "d_qoy", "i_category", "ws_ext_sales_price"]].rename(
        columns={"ws_ext_sales_price": "ext_sales_price"}
    )

    # Catalog sales with NULL filter
    cs = (
        catalog_sales[catalog_sales[null_col_cs].isna()]
        .merge(date_dim, left_on="cs_sold_date_sk", right_on="d_date_sk", how="inner")
        .merge(item, left_on="cs_item_sk", right_on="i_item_sk", how="inner")
    )
    cs["channel"] = "catalog"
    cs["col_name"] = null_col_cs
    cs = cs[["channel", "col_name", "d_year", "d_qoy", "i_category", "cs_ext_sales_price"]].rename(
        columns={"cs_ext_sales_price": "ext_sales_price"}
    )

    # Union all channels
    combined = ctx.concat([ss, ws, cs])

    # Aggregate
    result = (
        combined.groupby(["channel", "col_name", "d_year", "d_qoy", "i_category"], as_index=False)
        .agg(sales_cnt=("ext_sales_price", "count"), sales_amt=("ext_sales_price", "sum"))
        .sort_values(["channel", "col_name", "d_year", "d_qoy", "i_category"])
        .head(100)
    )

    return result


# =============================================================================
# Q97: Store/Catalog Customer-Item Overlap Analysis
# =============================================================================


def q97_expression_impl(ctx: DataFrameContext) -> Any:
    """Q97: Store/catalog customer-item overlap with FULL OUTER JOIN (Polars).

    Analyzes customer-item purchase overlap between store and catalog channels.
    Uses FULL OUTER JOIN to find customers who bought:
    - Store only
    - Catalog only
    - Both channels

    Tables: store_sales, catalog_sales, date_dim
    """

    params = get_parameters(97)
    dms = params.get("dms", 1200)

    store_sales = ctx.get_table("store_sales")
    catalog_sales = ctx.get_table("catalog_sales")
    date_dim = ctx.get_table("date_dim")
    col = ctx.col
    lit = ctx.lit

    # Filter date_dim for the month range
    date_filtered = date_dim.filter((col("d_month_seq") >= lit(dms)) & (col("d_month_seq") <= lit(dms + 11)))

    # Store sales customer-item pairs
    ssci = (
        store_sales.join(date_filtered, left_on="ss_sold_date_sk", right_on="d_date_sk")
        .group_by(["ss_customer_sk", "ss_item_sk"])
        .agg(lit(1).alias("_count"))
        .select(
            [
                col("ss_customer_sk").alias("ss_customer_sk"),
                col("ss_item_sk").alias("ss_item_sk"),
            ]
        )
    )

    # Catalog sales customer-item pairs
    csci = (
        catalog_sales.join(date_filtered, left_on="cs_sold_date_sk", right_on="d_date_sk")
        .group_by(["cs_bill_customer_sk", "cs_item_sk"])
        .agg(lit(1).alias("_count"))
        .select(
            [
                col("cs_bill_customer_sk").alias("cs_customer_sk"),
                col("cs_item_sk").alias("cs_item_sk"),
            ]
        )
    )

    # Full outer join on customer_sk and item_sk
    joined = ssci.join(
        csci,
        left_on=["ss_customer_sk", "ss_item_sk"],
        right_on=["cs_customer_sk", "cs_item_sk"],
        how="full",
    )

    # Calculate overlap counts
    result = joined.select(
        [
            ctx.when(col("ss_customer_sk").is_not_null() & col("cs_customer_sk").is_null())
            .then(lit(1))
            .otherwise(lit(0))
            .sum()
            .alias("store_only"),
            ctx.when(col("ss_customer_sk").is_null() & col("cs_customer_sk").is_not_null())
            .then(lit(1))
            .otherwise(lit(0))
            .sum()
            .alias("catalog_only"),
            ctx.when(col("ss_customer_sk").is_not_null() & col("cs_customer_sk").is_not_null())
            .then(lit(1))
            .otherwise(lit(0))
            .sum()
            .alias("store_and_catalog"),
        ]
    )

    return result


def q97_pandas_impl(ctx: DataFrameContext) -> Any:
    """Q97: Store/catalog customer-item overlap with FULL OUTER JOIN (Pandas)."""
    import pandas as pd

    params = get_parameters(97)
    dms = params.get("dms", 1200)

    store_sales = ctx.get_table("store_sales")
    catalog_sales = ctx.get_table("catalog_sales")
    date_dim = ctx.get_table("date_dim")

    # Filter date_dim for the month range
    date_filtered = date_dim[(date_dim["d_month_seq"] >= dms) & (date_dim["d_month_seq"] <= dms + 11)]

    # Store sales customer-item pairs
    ss_joined = store_sales.merge(date_filtered[["d_date_sk"]], left_on="ss_sold_date_sk", right_on="d_date_sk")
    ssci = ss_joined.groupby(["ss_customer_sk", "ss_item_sk"], as_index=False).size()
    ssci = ssci[["ss_customer_sk", "ss_item_sk"]].drop_duplicates()

    # Catalog sales customer-item pairs
    cs_joined = catalog_sales.merge(date_filtered[["d_date_sk"]], left_on="cs_sold_date_sk", right_on="d_date_sk")
    csci = cs_joined.groupby(["cs_bill_customer_sk", "cs_item_sk"], as_index=False).size()
    csci = csci[["cs_bill_customer_sk", "cs_item_sk"]].drop_duplicates()
    csci = csci.rename(columns={"cs_bill_customer_sk": "cs_customer_sk", "cs_item_sk": "cs_item_sk_r"})

    # Full outer join
    joined = ssci.merge(
        csci,
        left_on=["ss_customer_sk", "ss_item_sk"],
        right_on=["cs_customer_sk", "cs_item_sk_r"],
        how="outer",
    )

    # Calculate overlap counts
    store_only = ((joined["ss_customer_sk"].notna()) & (joined["cs_customer_sk"].isna())).sum()
    catalog_only = ((joined["ss_customer_sk"].isna()) & (joined["cs_customer_sk"].notna())).sum()
    store_and_catalog = ((joined["ss_customer_sk"].notna()) & (joined["cs_customer_sk"].notna())).sum()

    result = pd.DataFrame(
        {
            "store_only": [store_only],
            "catalog_only": [catalog_only],
            "store_and_catalog": [store_and_catalog],
        }
    )

    return result


# =============================================================================
# Q49: Three-Channel Return Ratio Ranking
# =============================================================================


def q49_expression_impl(ctx: DataFrameContext) -> Any:
    """Q49: Three-channel return ratio ranking with RANK window function (Polars).

    For each channel (web, catalog, store), calculates return and currency ratios
    per item, ranks them, and returns top 10 in each ranking category.

    Tables: web_sales, web_returns, catalog_sales, catalog_returns,
            store_sales, store_returns, date_dim
    """

    params = get_parameters(49)
    year = params.get("year", 2001)
    month = params.get("month", 12)

    web_sales = ctx.get_table("web_sales")
    web_returns = ctx.get_table("web_returns")
    catalog_sales = ctx.get_table("catalog_sales")
    catalog_returns = ctx.get_table("catalog_returns")
    store_sales = ctx.get_table("store_sales")
    store_returns = ctx.get_table("store_returns")
    date_dim = ctx.get_table("date_dim")
    col = ctx.col
    lit = ctx.lit

    # Filter date_dim
    date_filtered = date_dim.filter((col("d_year") == lit(year)) & (col("d_moy") == lit(month)))

    # Web channel
    ws_joined = (
        web_sales.join(
            web_returns,
            left_on=["ws_order_number", "ws_item_sk"],
            right_on=["wr_order_number", "wr_item_sk"],
            how="left",
        )
        .join(date_filtered, left_on="ws_sold_date_sk", right_on="d_date_sk")
        .filter(
            (col("wr_return_amt") > lit(10000))
            & (col("ws_net_profit") > lit(1))
            & (col("ws_net_paid") > lit(0))
            & (col("ws_quantity") > lit(0))
        )
    )

    web_ratios = (
        ws_joined.group_by("ws_item_sk")
        .agg(
            [
                (
                    col("wr_return_quantity").fill_null(0).sum().cast_float64()
                    / col("ws_quantity").fill_null(0).sum().cast_float64()
                ).alias("return_ratio"),
                (
                    col("wr_return_amt").fill_null(0).sum().cast_float64()
                    / col("ws_net_paid").fill_null(0).sum().cast_float64()
                ).alias("currency_ratio"),
            ]
        )
        .with_columns(
            [
                col("return_ratio").rank(method="min").alias("return_rank"),
                col("currency_ratio").rank(method="min").alias("currency_rank"),
            ]
        )
        .filter((col("return_rank") <= lit(10)) | (col("currency_rank") <= lit(10)))
        .with_columns(lit("web").alias("channel"))
        .select(["channel", col("ws_item_sk").alias("item"), "return_ratio", "return_rank", "currency_rank"])
    )

    # Catalog channel
    cs_joined = (
        catalog_sales.join(
            catalog_returns,
            left_on=["cs_order_number", "cs_item_sk"],
            right_on=["cr_order_number", "cr_item_sk"],
            how="left",
        )
        .join(date_filtered, left_on="cs_sold_date_sk", right_on="d_date_sk")
        .filter(
            (col("cr_return_amount") > lit(10000))
            & (col("cs_net_profit") > lit(1))
            & (col("cs_net_paid") > lit(0))
            & (col("cs_quantity") > lit(0))
        )
    )

    catalog_ratios = (
        cs_joined.group_by("cs_item_sk")
        .agg(
            [
                (
                    col("cr_return_quantity").fill_null(0).sum().cast_float64()
                    / col("cs_quantity").fill_null(0).sum().cast_float64()
                ).alias("return_ratio"),
                (
                    col("cr_return_amount").fill_null(0).sum().cast_float64()
                    / col("cs_net_paid").fill_null(0).sum().cast_float64()
                ).alias("currency_ratio"),
            ]
        )
        .with_columns(
            [
                col("return_ratio").rank(method="min").alias("return_rank"),
                col("currency_ratio").rank(method="min").alias("currency_rank"),
            ]
        )
        .filter((col("return_rank") <= lit(10)) | (col("currency_rank") <= lit(10)))
        .with_columns(lit("catalog").alias("channel"))
        .select(["channel", col("cs_item_sk").alias("item"), "return_ratio", "return_rank", "currency_rank"])
    )

    # Store channel
    ss_joined = (
        store_sales.join(
            store_returns,
            left_on=["ss_ticket_number", "ss_item_sk"],
            right_on=["sr_ticket_number", "sr_item_sk"],
            how="left",
        )
        .join(date_filtered, left_on="ss_sold_date_sk", right_on="d_date_sk")
        .filter(
            (col("sr_return_amt") > lit(10000))
            & (col("ss_net_profit") > lit(1))
            & (col("ss_net_paid") > lit(0))
            & (col("ss_quantity") > lit(0))
        )
    )

    store_ratios = (
        ss_joined.group_by("ss_item_sk")
        .agg(
            [
                (
                    col("sr_return_quantity").fill_null(0).sum().cast_float64()
                    / col("ss_quantity").fill_null(0).sum().cast_float64()
                ).alias("return_ratio"),
                (
                    col("sr_return_amt").fill_null(0).sum().cast_float64()
                    / col("ss_net_paid").fill_null(0).sum().cast_float64()
                ).alias("currency_ratio"),
            ]
        )
        .with_columns(
            [
                col("return_ratio").rank(method="min").alias("return_rank"),
                col("currency_ratio").rank(method="min").alias("currency_rank"),
            ]
        )
        .filter((col("return_rank") <= lit(10)) | (col("currency_rank") <= lit(10)))
        .with_columns(lit("store").alias("channel"))
        .select(["channel", col("ss_item_sk").alias("item"), "return_ratio", "return_rank", "currency_rank"])
    )

    # Union all channels
    result = (
        ctx.concat([web_ratios, catalog_ratios, store_ratios])
        .sort(["channel", "return_rank", "currency_rank", "item"])
        .limit(100)
    )

    return result


def q49_pandas_impl(ctx: DataFrameContext) -> Any:
    """Q49: Three-channel return ratio ranking with RANK window function (Pandas)."""

    params = get_parameters(49)
    year = params.get("year", 2001)
    month = params.get("month", 12)

    web_sales = ctx.get_table("web_sales")
    web_returns = ctx.get_table("web_returns")
    catalog_sales = ctx.get_table("catalog_sales")
    catalog_returns = ctx.get_table("catalog_returns")
    store_sales = ctx.get_table("store_sales")
    store_returns = ctx.get_table("store_returns")
    date_dim = ctx.get_table("date_dim")

    # Filter date_dim
    date_filtered = date_dim[(date_dim["d_year"] == year) & (date_dim["d_moy"] == month)][["d_date_sk"]]

    # Web channel
    ws_joined = web_sales.merge(
        web_returns, left_on=["ws_order_number", "ws_item_sk"], right_on=["wr_order_number", "wr_item_sk"], how="left"
    )
    ws_joined = ws_joined.merge(date_filtered, left_on="ws_sold_date_sk", right_on="d_date_sk")
    ws_joined = ws_joined[
        (ws_joined["wr_return_amt"] > 10000)
        & (ws_joined["ws_net_profit"] > 1)
        & (ws_joined["ws_net_paid"] > 0)
        & (ws_joined["ws_quantity"] > 0)
    ]

    web_agg = ws_joined.groupby("ws_item_sk", as_index=False).agg(
        return_qty_sum=("wr_return_quantity", lambda x: x.fillna(0).sum()),
        sales_qty_sum=("ws_quantity", lambda x: x.fillna(0).sum()),
        return_amt_sum=("wr_return_amt", lambda x: x.fillna(0).sum()),
        net_paid_sum=("ws_net_paid", lambda x: x.fillna(0).sum()),
    )
    web_agg["return_ratio"] = web_agg["return_qty_sum"] / web_agg["sales_qty_sum"]
    web_agg["currency_ratio"] = web_agg["return_amt_sum"] / web_agg["net_paid_sum"]
    web_agg["return_rank"] = web_agg["return_ratio"].rank(method="min")
    web_agg["currency_rank"] = web_agg["currency_ratio"].rank(method="min")
    web_filtered = web_agg[(web_agg["return_rank"] <= 10) | (web_agg["currency_rank"] <= 10)]
    web_filtered = web_filtered.assign(channel="web")
    web_result = web_filtered[["channel", "ws_item_sk", "return_ratio", "return_rank", "currency_rank"]].rename(
        columns={"ws_item_sk": "item"}
    )

    # Catalog channel
    cs_joined = catalog_sales.merge(
        catalog_returns,
        left_on=["cs_order_number", "cs_item_sk"],
        right_on=["cr_order_number", "cr_item_sk"],
        how="left",
    )
    cs_joined = cs_joined.merge(date_filtered, left_on="cs_sold_date_sk", right_on="d_date_sk")
    cs_joined = cs_joined[
        (cs_joined["cr_return_amount"] > 10000)
        & (cs_joined["cs_net_profit"] > 1)
        & (cs_joined["cs_net_paid"] > 0)
        & (cs_joined["cs_quantity"] > 0)
    ]

    catalog_agg = cs_joined.groupby("cs_item_sk", as_index=False).agg(
        return_qty_sum=("cr_return_quantity", lambda x: x.fillna(0).sum()),
        sales_qty_sum=("cs_quantity", lambda x: x.fillna(0).sum()),
        return_amt_sum=("cr_return_amount", lambda x: x.fillna(0).sum()),
        net_paid_sum=("cs_net_paid", lambda x: x.fillna(0).sum()),
    )
    catalog_agg["return_ratio"] = catalog_agg["return_qty_sum"] / catalog_agg["sales_qty_sum"]
    catalog_agg["currency_ratio"] = catalog_agg["return_amt_sum"] / catalog_agg["net_paid_sum"]
    catalog_agg["return_rank"] = catalog_agg["return_ratio"].rank(method="min")
    catalog_agg["currency_rank"] = catalog_agg["currency_ratio"].rank(method="min")
    catalog_filtered = catalog_agg[(catalog_agg["return_rank"] <= 10) | (catalog_agg["currency_rank"] <= 10)]
    catalog_filtered = catalog_filtered.assign(channel="catalog")
    catalog_result = catalog_filtered[["channel", "cs_item_sk", "return_ratio", "return_rank", "currency_rank"]].rename(
        columns={"cs_item_sk": "item"}
    )

    # Store channel
    ss_joined = store_sales.merge(
        store_returns,
        left_on=["ss_ticket_number", "ss_item_sk"],
        right_on=["sr_ticket_number", "sr_item_sk"],
        how="left",
    )
    ss_joined = ss_joined.merge(date_filtered, left_on="ss_sold_date_sk", right_on="d_date_sk")
    ss_joined = ss_joined[
        (ss_joined["sr_return_amt"] > 10000)
        & (ss_joined["ss_net_profit"] > 1)
        & (ss_joined["ss_net_paid"] > 0)
        & (ss_joined["ss_quantity"] > 0)
    ]

    store_agg = ss_joined.groupby("ss_item_sk", as_index=False).agg(
        return_qty_sum=("sr_return_quantity", lambda x: x.fillna(0).sum()),
        sales_qty_sum=("ss_quantity", lambda x: x.fillna(0).sum()),
        return_amt_sum=("sr_return_amt", lambda x: x.fillna(0).sum()),
        net_paid_sum=("ss_net_paid", lambda x: x.fillna(0).sum()),
    )
    store_agg["return_ratio"] = store_agg["return_qty_sum"] / store_agg["sales_qty_sum"]
    store_agg["currency_ratio"] = store_agg["return_amt_sum"] / store_agg["net_paid_sum"]
    store_agg["return_rank"] = store_agg["return_ratio"].rank(method="min")
    store_agg["currency_rank"] = store_agg["currency_ratio"].rank(method="min")
    store_filtered = store_agg[(store_agg["return_rank"] <= 10) | (store_agg["currency_rank"] <= 10)]
    store_filtered = store_filtered.assign(channel="store")
    store_result = store_filtered[["channel", "ss_item_sk", "return_ratio", "return_rank", "currency_rank"]].rename(
        columns={"ss_item_sk": "item"}
    )

    # Union all channels
    result = (
        ctx.concat([web_result, catalog_result, store_result])
        .sort_values(["channel", "return_rank", "currency_rank", "item"])
        .head(100)
    )

    return result


# =============================================================================
# Q75: Three-Channel Sales with Returns Year-over-Year
# =============================================================================


def q75_expression_impl(ctx: DataFrameContext) -> Any:
    """Q75: Three-channel sales with returns, year-over-year comparison (Polars).

    Union three channels (catalog, store, web), each joining sales with returns.
    Aggregate by year, brand, class, category, manufact.
    Compare current year with previous year, filter where ratio < 0.9.

    Tables: catalog_sales, catalog_returns, store_sales, store_returns,
            web_sales, web_returns, item, date_dim
    """

    params = get_parameters(75)
    year = params.get("year", 2001)
    category = params.get("category", "Books")

    catalog_sales = ctx.get_table("catalog_sales")
    catalog_returns = ctx.get_table("catalog_returns")
    store_sales = ctx.get_table("store_sales")
    store_returns = ctx.get_table("store_returns")
    web_sales = ctx.get_table("web_sales")
    web_returns = ctx.get_table("web_returns")
    item = ctx.get_table("item")
    date_dim = ctx.get_table("date_dim")
    col = ctx.col
    lit = ctx.lit

    # Filter item by category
    item_filtered = item.filter(col("i_category") == lit(category))

    # Catalog sales with returns
    cs_joined = (
        catalog_sales.join(item_filtered, left_on="cs_item_sk", right_on="i_item_sk")
        .join(date_dim, left_on="cs_sold_date_sk", right_on="d_date_sk")
        .join(
            catalog_returns,
            left_on=["cs_order_number", "cs_item_sk"],
            right_on=["cr_order_number", "cr_item_sk"],
            how="left",
        )
        .select(
            [
                col("d_year"),
                col("i_brand_id"),
                col("i_class_id"),
                col("i_category_id"),
                col("i_manufact_id"),
                (col("cs_quantity") - col("cr_return_quantity").fill_null(0)).alias("sales_cnt"),
                (col("cs_ext_sales_price") - col("cr_return_amount").fill_null(0.0)).alias("sales_amt"),
            ]
        )
    )

    # Store sales with returns
    ss_joined = (
        store_sales.join(item_filtered, left_on="ss_item_sk", right_on="i_item_sk")
        .join(date_dim, left_on="ss_sold_date_sk", right_on="d_date_sk")
        .join(
            store_returns,
            left_on=["ss_ticket_number", "ss_item_sk"],
            right_on=["sr_ticket_number", "sr_item_sk"],
            how="left",
        )
        .select(
            [
                col("d_year"),
                col("i_brand_id"),
                col("i_class_id"),
                col("i_category_id"),
                col("i_manufact_id"),
                (col("ss_quantity") - col("sr_return_quantity").fill_null(0)).alias("sales_cnt"),
                (col("ss_ext_sales_price") - col("sr_return_amt").fill_null(0.0)).alias("sales_amt"),
            ]
        )
    )

    # Web sales with returns
    ws_joined = (
        web_sales.join(item_filtered, left_on="ws_item_sk", right_on="i_item_sk")
        .join(date_dim, left_on="ws_sold_date_sk", right_on="d_date_sk")
        .join(
            web_returns,
            left_on=["ws_order_number", "ws_item_sk"],
            right_on=["wr_order_number", "wr_item_sk"],
            how="left",
        )
        .select(
            [
                col("d_year"),
                col("i_brand_id"),
                col("i_class_id"),
                col("i_category_id"),
                col("i_manufact_id"),
                (col("ws_quantity") - col("wr_return_quantity").fill_null(0)).alias("sales_cnt"),
                (col("ws_ext_sales_price") - col("wr_return_amt").fill_null(0.0)).alias("sales_amt"),
            ]
        )
    )

    # Union all channels
    combined = ctx.concat([cs_joined, ss_joined, ws_joined])

    # Aggregate by year and item attributes
    all_sales = combined.group_by(["d_year", "i_brand_id", "i_class_id", "i_category_id", "i_manufact_id"]).agg(
        [
            col("sales_cnt").sum().alias("sales_cnt"),
            col("sales_amt").sum().alias("sales_amt"),
        ]
    )

    # Current year
    curr_yr = all_sales.filter(col("d_year") == lit(year))

    # Previous year
    prev_yr = all_sales.filter(col("d_year") == lit(year - 1))

    # Join current and previous year
    result = (
        curr_yr.join(
            prev_yr,
            on=["i_brand_id", "i_class_id", "i_category_id", "i_manufact_id"],
            suffix="_prev",
        )
        .filter((col("sales_cnt").cast_float64() / col("sales_cnt_prev").cast_float64()) < lit(0.9))
        .select(
            [
                col("d_year_prev").alias("prev_year"),
                col("d_year").alias("year"),
                col("i_brand_id"),
                col("i_class_id"),
                col("i_category_id"),
                col("i_manufact_id"),
                col("sales_cnt_prev").alias("prev_yr_cnt"),
                col("sales_cnt").alias("curr_yr_cnt"),
                (col("sales_cnt") - col("sales_cnt_prev")).alias("sales_cnt_diff"),
                (col("sales_amt") - col("sales_amt_prev")).alias("sales_amt_diff"),
            ]
        )
        .sort(["sales_cnt_diff", "sales_amt_diff"])
        .limit(100)
    )

    return result


def q75_pandas_impl(ctx: DataFrameContext) -> Any:
    """Q75: Three-channel sales with returns, year-over-year comparison (Pandas)."""

    params = get_parameters(75)
    year = params.get("year", 2001)
    category = params.get("category", "Books")

    catalog_sales = ctx.get_table("catalog_sales")
    catalog_returns = ctx.get_table("catalog_returns")
    store_sales = ctx.get_table("store_sales")
    store_returns = ctx.get_table("store_returns")
    web_sales = ctx.get_table("web_sales")
    web_returns = ctx.get_table("web_returns")
    item = ctx.get_table("item")
    date_dim = ctx.get_table("date_dim")

    # Filter item by category
    item_filtered = item[item["i_category"] == category]

    # Catalog sales with returns
    cs_joined = catalog_sales.merge(item_filtered, left_on="cs_item_sk", right_on="i_item_sk")
    cs_joined = cs_joined.merge(date_dim, left_on="cs_sold_date_sk", right_on="d_date_sk")
    cs_joined = cs_joined.merge(
        catalog_returns,
        left_on=["cs_order_number", "cs_item_sk"],
        right_on=["cr_order_number", "cr_item_sk"],
        how="left",
    )
    cs_joined["sales_cnt"] = cs_joined["cs_quantity"] - cs_joined["cr_return_quantity"].fillna(0)
    cs_joined["sales_amt"] = cs_joined["cs_ext_sales_price"] - cs_joined["cr_return_amount"].fillna(0.0)
    cs_data = cs_joined[
        ["d_year", "i_brand_id", "i_class_id", "i_category_id", "i_manufact_id", "sales_cnt", "sales_amt"]
    ]

    # Store sales with returns
    ss_joined = store_sales.merge(item_filtered, left_on="ss_item_sk", right_on="i_item_sk")
    ss_joined = ss_joined.merge(date_dim, left_on="ss_sold_date_sk", right_on="d_date_sk")
    ss_joined = ss_joined.merge(
        store_returns,
        left_on=["ss_ticket_number", "ss_item_sk"],
        right_on=["sr_ticket_number", "sr_item_sk"],
        how="left",
    )
    ss_joined["sales_cnt"] = ss_joined["ss_quantity"] - ss_joined["sr_return_quantity"].fillna(0)
    ss_joined["sales_amt"] = ss_joined["ss_ext_sales_price"] - ss_joined["sr_return_amt"].fillna(0.0)
    ss_data = ss_joined[
        ["d_year", "i_brand_id", "i_class_id", "i_category_id", "i_manufact_id", "sales_cnt", "sales_amt"]
    ]

    # Web sales with returns
    ws_joined = web_sales.merge(item_filtered, left_on="ws_item_sk", right_on="i_item_sk")
    ws_joined = ws_joined.merge(date_dim, left_on="ws_sold_date_sk", right_on="d_date_sk")
    ws_joined = ws_joined.merge(
        web_returns, left_on=["ws_order_number", "ws_item_sk"], right_on=["wr_order_number", "wr_item_sk"], how="left"
    )
    ws_joined["sales_cnt"] = ws_joined["ws_quantity"] - ws_joined["wr_return_quantity"].fillna(0)
    ws_joined["sales_amt"] = ws_joined["ws_ext_sales_price"] - ws_joined["wr_return_amt"].fillna(0.0)
    ws_data = ws_joined[
        ["d_year", "i_brand_id", "i_class_id", "i_category_id", "i_manufact_id", "sales_cnt", "sales_amt"]
    ]

    # Union all channels
    combined = ctx.concat([cs_data, ss_data, ws_data])

    # Aggregate by year and item attributes
    all_sales = combined.groupby(
        ["d_year", "i_brand_id", "i_class_id", "i_category_id", "i_manufact_id"], as_index=False
    ).agg(sales_cnt=("sales_cnt", "sum"), sales_amt=("sales_amt", "sum"))

    # Current year
    curr_yr = all_sales[all_sales["d_year"] == year]

    # Previous year
    prev_yr = all_sales[all_sales["d_year"] == year - 1]

    # Join current and previous year
    joined = curr_yr.merge(
        prev_yr,
        on=["i_brand_id", "i_class_id", "i_category_id", "i_manufact_id"],
        suffixes=("", "_prev"),
    )

    # Filter where ratio < 0.9
    joined = joined[joined["sales_cnt"] / joined["sales_cnt_prev"] < 0.9]

    result = joined.assign(
        prev_year=joined["d_year_prev"],
        year=joined["d_year"],
        prev_yr_cnt=joined["sales_cnt_prev"],
        curr_yr_cnt=joined["sales_cnt"],
        sales_cnt_diff=joined["sales_cnt"] - joined["sales_cnt_prev"],
        sales_amt_diff=joined["sales_amt"] - joined["sales_amt_prev"],
    )[
        [
            "prev_year",
            "year",
            "i_brand_id",
            "i_class_id",
            "i_category_id",
            "i_manufact_id",
            "prev_yr_cnt",
            "curr_yr_cnt",
            "sales_cnt_diff",
            "sales_amt_diff",
        ]
    ]

    result = result.sort_values(["sales_cnt_diff", "sales_amt_diff"]).head(100)

    return result


# =============================================================================
# Q78: Three-Channel Sales without Returns Comparison
# =============================================================================


def q78_expression_impl(ctx: DataFrameContext) -> Any:
    """Q78: Three-channel sales comparison excluding returned items (Polars).

    For each channel, aggregate sales by year/item/customer excluding returned items.
    Compare store sales to combined web+catalog sales.

    Tables: store_sales, store_returns, catalog_sales, catalog_returns,
            web_sales, web_returns, date_dim
    """

    params = get_parameters(78)
    year = params.get("year", 2000)

    store_sales = ctx.get_table("store_sales")
    store_returns = ctx.get_table("store_returns")
    catalog_sales = ctx.get_table("catalog_sales")
    catalog_returns = ctx.get_table("catalog_returns")
    web_sales = ctx.get_table("web_sales")
    web_returns = ctx.get_table("web_returns")
    date_dim = ctx.get_table("date_dim")
    col = ctx.col
    lit = ctx.lit

    # Store sales - exclude returned items
    # Note: Use sr_return_amt.is_null() since sr_ticket_number is dropped as join key
    ss = (
        store_sales.join(
            store_returns,
            left_on=["ss_ticket_number", "ss_item_sk"],
            right_on=["sr_ticket_number", "sr_item_sk"],
            how="left",
        )
        .join(date_dim, left_on="ss_sold_date_sk", right_on="d_date_sk")
        .filter(col("sr_return_amt").is_null())
        .group_by(["d_year", "ss_item_sk", "ss_customer_sk"])
        .agg(
            [
                col("ss_quantity").sum().alias("ss_qty"),
                col("ss_wholesale_cost").sum().alias("ss_wc"),
                col("ss_sales_price").sum().alias("ss_sp"),
            ]
        )
        .rename({"d_year": "ss_sold_year"})
    )

    # Catalog sales - exclude returned items
    # Note: Use cr_return_amount.is_null() since cr_order_number is dropped as join key
    cs = (
        catalog_sales.join(
            catalog_returns,
            left_on=["cs_order_number", "cs_item_sk"],
            right_on=["cr_order_number", "cr_item_sk"],
            how="left",
        )
        .join(date_dim, left_on="cs_sold_date_sk", right_on="d_date_sk")
        .filter(col("cr_return_amount").is_null())
        .group_by(["d_year", "cs_item_sk", "cs_bill_customer_sk"])
        .agg(
            [
                col("cs_quantity").sum().alias("cs_qty"),
                col("cs_wholesale_cost").sum().alias("cs_wc"),
                col("cs_sales_price").sum().alias("cs_sp"),
            ]
        )
        .rename({"d_year": "cs_sold_year", "cs_bill_customer_sk": "cs_customer_sk"})
    )

    # Web sales - exclude returned items
    # Note: Use wr_return_amt.is_null() since wr_order_number is dropped as join key
    ws = (
        web_sales.join(
            web_returns,
            left_on=["ws_order_number", "ws_item_sk"],
            right_on=["wr_order_number", "wr_item_sk"],
            how="left",
        )
        .join(date_dim, left_on="ws_sold_date_sk", right_on="d_date_sk")
        .filter(col("wr_return_amt").is_null())
        .group_by(["d_year", "ws_item_sk", "ws_bill_customer_sk"])
        .agg(
            [
                col("ws_quantity").sum().alias("ws_qty"),
                col("ws_wholesale_cost").sum().alias("ws_wc"),
                col("ws_sales_price").sum().alias("ws_sp"),
            ]
        )
        .rename({"d_year": "ws_sold_year", "ws_bill_customer_sk": "ws_customer_sk"})
    )

    # Join store with web and catalog
    result = (
        ss.join(
            ws,
            left_on=["ss_sold_year", "ss_item_sk", "ss_customer_sk"],
            right_on=["ws_sold_year", "ws_item_sk", "ws_customer_sk"],
            how="left",
        )
        .join(
            cs,
            left_on=["ss_sold_year", "ss_item_sk", "ss_customer_sk"],
            right_on=["cs_sold_year", "cs_item_sk", "cs_customer_sk"],
            how="left",
        )
        .filter(
            (col("ss_sold_year") == lit(year))
            & ((col("ws_qty").fill_null(0) > lit(0)) | (col("cs_qty").fill_null(0) > lit(0)))
        )
        .with_columns(
            [
                (
                    col("ss_qty").cast_float64()
                    / (col("ws_qty").fill_null(0) + col("cs_qty").fill_null(0)).cast_float64()
                )
                .round(2)
                .alias("ratio"),
                (col("ws_qty").fill_null(0) + col("cs_qty").fill_null(0)).alias("other_chan_qty"),
                (col("ws_wc").fill_null(0) + col("cs_wc").fill_null(0)).alias("other_chan_wholesale_cost"),
                (col("ws_sp").fill_null(0) + col("cs_sp").fill_null(0)).alias("other_chan_sales_price"),
            ]
        )
        .select(
            [
                "ss_sold_year",
                "ss_item_sk",
                "ss_customer_sk",
                "ratio",
                col("ss_qty").alias("store_qty"),
                col("ss_wc").alias("store_wholesale_cost"),
                col("ss_sp").alias("store_sales_price"),
                "other_chan_qty",
                "other_chan_wholesale_cost",
                "other_chan_sales_price",
            ]
        )
        .sort(
            [
                "ss_sold_year",
                "ss_item_sk",
                "ss_customer_sk",
                "store_qty",
                "store_wholesale_cost",
                "store_sales_price",
                "other_chan_qty",
                "other_chan_wholesale_cost",
                "other_chan_sales_price",
                "ratio",
            ],
            descending=[False, False, False, True, True, True, False, False, False, False],
        )
        .limit(100)
    )

    return result


def q78_pandas_impl(ctx: DataFrameContext) -> Any:
    """Q78: Three-channel sales comparison excluding returned items (Pandas)."""
    params = get_parameters(78)
    year = params.get("year", 2000)

    store_sales = ctx.get_table("store_sales")
    store_returns = ctx.get_table("store_returns")
    catalog_sales = ctx.get_table("catalog_sales")
    catalog_returns = ctx.get_table("catalog_returns")
    web_sales = ctx.get_table("web_sales")
    web_returns = ctx.get_table("web_returns")
    date_dim = ctx.get_table("date_dim")

    # Store sales - exclude returned items
    # Note: Use sr_return_amt for null check since sr_ticket_number may be dropped as join key
    ss = store_sales.merge(
        store_returns,
        left_on=["ss_ticket_number", "ss_item_sk"],
        right_on=["sr_ticket_number", "sr_item_sk"],
        how="left",
    )
    ss = ss.merge(date_dim, left_on="ss_sold_date_sk", right_on="d_date_sk")
    ss = ss[ss["sr_return_amt"].isna()]
    ss_agg = ss.groupby(["d_year", "ss_item_sk", "ss_customer_sk"], as_index=False).agg(
        ss_qty=("ss_quantity", "sum"),
        ss_wc=("ss_wholesale_cost", "sum"),
        ss_sp=("ss_sales_price", "sum"),
    )
    ss_agg = ss_agg.rename(columns={"d_year": "ss_sold_year"})

    # Catalog sales - exclude returned items
    # Note: Use cr_return_amount for null check since cr_order_number may be dropped as join key
    cs = catalog_sales.merge(
        catalog_returns,
        left_on=["cs_order_number", "cs_item_sk"],
        right_on=["cr_order_number", "cr_item_sk"],
        how="left",
    )
    cs = cs.merge(date_dim, left_on="cs_sold_date_sk", right_on="d_date_sk")
    cs = cs[cs["cr_return_amount"].isna()]
    cs_agg = cs.groupby(["d_year", "cs_item_sk", "cs_bill_customer_sk"], as_index=False).agg(
        cs_qty=("cs_quantity", "sum"),
        cs_wc=("cs_wholesale_cost", "sum"),
        cs_sp=("cs_sales_price", "sum"),
    )
    cs_agg = cs_agg.rename(columns={"d_year": "cs_sold_year", "cs_bill_customer_sk": "cs_customer_sk"})

    # Web sales - exclude returned items
    # Note: Use wr_return_amt for null check since wr_order_number may be dropped as join key
    ws = web_sales.merge(
        web_returns, left_on=["ws_order_number", "ws_item_sk"], right_on=["wr_order_number", "wr_item_sk"], how="left"
    )
    ws = ws.merge(date_dim, left_on="ws_sold_date_sk", right_on="d_date_sk")
    ws = ws[ws["wr_return_amt"].isna()]
    ws_agg = ws.groupby(["d_year", "ws_item_sk", "ws_bill_customer_sk"], as_index=False).agg(
        ws_qty=("ws_quantity", "sum"),
        ws_wc=("ws_wholesale_cost", "sum"),
        ws_sp=("ws_sales_price", "sum"),
    )
    ws_agg = ws_agg.rename(columns={"d_year": "ws_sold_year", "ws_bill_customer_sk": "ws_customer_sk"})

    # Join store with web and catalog
    result = ss_agg.merge(
        ws_agg,
        left_on=["ss_sold_year", "ss_item_sk", "ss_customer_sk"],
        right_on=["ws_sold_year", "ws_item_sk", "ws_customer_sk"],
        how="left",
    )
    result = result.merge(
        cs_agg,
        left_on=["ss_sold_year", "ss_item_sk", "ss_customer_sk"],
        right_on=["cs_sold_year", "cs_item_sk", "cs_customer_sk"],
        how="left",
    )

    # Filter
    result = result[
        (result["ss_sold_year"] == year) & ((result["ws_qty"].fillna(0) > 0) | (result["cs_qty"].fillna(0) > 0))
    ]

    # Calculate derived columns
    result["other_chan_qty"] = result["ws_qty"].fillna(0) + result["cs_qty"].fillna(0)
    result["other_chan_wholesale_cost"] = result["ws_wc"].fillna(0) + result["cs_wc"].fillna(0)
    result["other_chan_sales_price"] = result["ws_sp"].fillna(0) + result["cs_sp"].fillna(0)
    result["ratio"] = (result["ss_qty"] / result["other_chan_qty"]).round(2)

    result = result.rename(
        columns={
            "ss_qty": "store_qty",
            "ss_wc": "store_wholesale_cost",
            "ss_sp": "store_sales_price",
        }
    )

    result = result[
        [
            "ss_sold_year",
            "ss_item_sk",
            "ss_customer_sk",
            "ratio",
            "store_qty",
            "store_wholesale_cost",
            "store_sales_price",
            "other_chan_qty",
            "other_chan_wholesale_cost",
            "other_chan_sales_price",
        ]
    ]

    result = result.sort_values(
        [
            "ss_sold_year",
            "ss_item_sk",
            "ss_customer_sk",
            "store_qty",
            "store_wholesale_cost",
            "store_sales_price",
            "other_chan_qty",
            "other_chan_wholesale_cost",
            "other_chan_sales_price",
            "ratio",
        ],
        ascending=[True, True, True, False, False, False, True, True, True, True],
    ).head(100)

    return result


# =============================================================================
# Q4: Customer Year-over-Year Comparison Across All Channels
# =============================================================================


def q4_expression_impl(ctx: DataFrameContext) -> Any:
    """Q4: Customer year-over-year comparison across all channels (Polars).

    Computes customer totals per year for store, catalog, and web channels.
    Compares year N to year N+1, filtering for customers where catalog growth
    exceeds both store and web growth.

    Tables: customer, store_sales, catalog_sales, web_sales, date_dim
    """

    params = get_parameters(4)
    year = params.get("year", 2001)

    customer = ctx.get_table("customer")
    store_sales = ctx.get_table("store_sales")
    catalog_sales = ctx.get_table("catalog_sales")
    web_sales = ctx.get_table("web_sales")
    date_dim = ctx.get_table("date_dim")
    col = ctx.col
    lit = ctx.lit

    # Customer columns for grouping
    cust_cols = [
        "c_customer_id",
        "c_first_name",
        "c_last_name",
        "c_preferred_cust_flag",
        "c_birth_country",
        "c_login",
        "c_email_address",
    ]

    # Build year_total for store sales
    ss_year = (
        store_sales.join(customer, left_on="ss_customer_sk", right_on="c_customer_sk")
        .join(date_dim, left_on="ss_sold_date_sk", right_on="d_date_sk")
        .group_by(cust_cols + ["d_year"])
        .agg(
            [
                (
                    (
                        col("ss_ext_list_price")
                        - col("ss_ext_wholesale_cost")
                        - col("ss_ext_discount_amt")
                        + col("ss_ext_sales_price")
                    )
                    / lit(2)
                )
                .sum()
                .alias("year_total"),
            ]
        )
        .with_columns(lit("s").alias("sale_type"))
    )

    # Build year_total for catalog sales
    cs_year = (
        catalog_sales.join(customer, left_on="cs_bill_customer_sk", right_on="c_customer_sk")
        .join(date_dim, left_on="cs_sold_date_sk", right_on="d_date_sk")
        .group_by(cust_cols + ["d_year"])
        .agg(
            [
                (
                    (
                        col("cs_ext_list_price")
                        - col("cs_ext_wholesale_cost")
                        - col("cs_ext_discount_amt")
                        + col("cs_ext_sales_price")
                    )
                    / lit(2)
                )
                .sum()
                .alias("year_total"),
            ]
        )
        .with_columns(lit("c").alias("sale_type"))
    )

    # Build year_total for web sales
    ws_year = (
        web_sales.join(customer, left_on="ws_bill_customer_sk", right_on="c_customer_sk")
        .join(date_dim, left_on="ws_sold_date_sk", right_on="d_date_sk")
        .group_by(cust_cols + ["d_year"])
        .agg(
            [
                (
                    (
                        col("ws_ext_list_price")
                        - col("ws_ext_wholesale_cost")
                        - col("ws_ext_discount_amt")
                        + col("ws_ext_sales_price")
                    )
                    / lit(2)
                )
                .sum()
                .alias("year_total"),
            ]
        )
        .with_columns(lit("w").alias("sale_type"))
    )

    # Combine all channels
    year_total = ctx.concat([ss_year, cs_year, ws_year])

    # Filter for first year and second year for each channel
    t_s_firstyear = (
        year_total.filter((col("sale_type") == lit("s")) & (col("d_year") == lit(year)))
        .filter(col("year_total") > lit(0))
        .select([col("c_customer_id"), col("year_total").alias("s_first_total")])
    )

    t_s_secyear = year_total.filter((col("sale_type") == lit("s")) & (col("d_year") == lit(year + 1))).select(
        [
            col("c_customer_id"),
            col("c_first_name"),
            col("c_last_name"),
            col("c_preferred_cust_flag"),
            col("year_total").alias("s_sec_total"),
        ]
    )

    t_c_firstyear = (
        year_total.filter((col("sale_type") == lit("c")) & (col("d_year") == lit(year)))
        .filter(col("year_total") > lit(0))
        .select([col("c_customer_id"), col("year_total").alias("c_first_total")])
    )

    t_c_secyear = year_total.filter((col("sale_type") == lit("c")) & (col("d_year") == lit(year + 1))).select(
        [col("c_customer_id"), col("year_total").alias("c_sec_total")]
    )

    t_w_firstyear = (
        year_total.filter((col("sale_type") == lit("w")) & (col("d_year") == lit(year)))
        .filter(col("year_total") > lit(0))
        .select([col("c_customer_id"), col("year_total").alias("w_first_total")])
    )

    t_w_secyear = year_total.filter((col("sale_type") == lit("w")) & (col("d_year") == lit(year + 1))).select(
        [col("c_customer_id"), col("year_total").alias("w_sec_total")]
    )

    # Join all year totals by customer
    result = (
        t_s_secyear.join(t_s_firstyear, on="c_customer_id")
        .join(t_c_firstyear, on="c_customer_id")
        .join(t_c_secyear, on="c_customer_id")
        .join(t_w_firstyear, on="c_customer_id")
        .join(t_w_secyear, on="c_customer_id")
    )

    # Calculate growth ratios and filter
    result = result.with_columns(
        [
            (col("c_sec_total") / col("c_first_total")).alias("c_growth"),
            (col("s_sec_total") / col("s_first_total")).alias("s_growth"),
            (col("w_sec_total") / col("w_first_total")).alias("w_growth"),
        ]
    )

    # Filter: catalog growth > store growth AND catalog growth > web growth
    result = (
        result.filter((col("c_growth") > col("s_growth")) & (col("c_growth") > col("w_growth")))
        .select(
            [
                col("c_customer_id"),
                col("c_first_name"),
                col("c_last_name"),
                col("c_preferred_cust_flag"),
            ]
        )
        .sort(["c_customer_id", "c_first_name", "c_last_name", "c_preferred_cust_flag"])
        .limit(100)
    )

    return result


def q4_pandas_impl(ctx: DataFrameContext) -> Any:
    """Q4: Customer year-over-year comparison across all channels (Pandas)."""

    params = get_parameters(4)
    year = params.get("year", 2001)

    customer = ctx.get_table("customer")
    store_sales = ctx.get_table("store_sales")
    catalog_sales = ctx.get_table("catalog_sales")
    web_sales = ctx.get_table("web_sales")
    date_dim = ctx.get_table("date_dim")

    # Customer columns for grouping
    cust_cols = [
        "c_customer_id",
        "c_first_name",
        "c_last_name",
        "c_preferred_cust_flag",
        "c_birth_country",
        "c_login",
        "c_email_address",
    ]

    # Build year_total for store sales
    ss = store_sales.merge(customer, left_on="ss_customer_sk", right_on="c_customer_sk")
    ss = ss.merge(date_dim[["d_date_sk", "d_year"]], left_on="ss_sold_date_sk", right_on="d_date_sk")
    ss["year_total"] = (
        ss["ss_ext_list_price"] - ss["ss_ext_wholesale_cost"] - ss["ss_ext_discount_amt"] + ss["ss_ext_sales_price"]
    ) / 2
    ss_agg = ss.groupby(cust_cols + ["d_year"], as_index=False).agg(year_total=("year_total", "sum"))
    ss_agg["sale_type"] = "s"

    # Build year_total for catalog sales
    cs = catalog_sales.merge(customer, left_on="cs_bill_customer_sk", right_on="c_customer_sk")
    cs = cs.merge(date_dim[["d_date_sk", "d_year"]], left_on="cs_sold_date_sk", right_on="d_date_sk")
    cs["year_total"] = (
        cs["cs_ext_list_price"] - cs["cs_ext_wholesale_cost"] - cs["cs_ext_discount_amt"] + cs["cs_ext_sales_price"]
    ) / 2
    cs_agg = cs.groupby(cust_cols + ["d_year"], as_index=False).agg(year_total=("year_total", "sum"))
    cs_agg["sale_type"] = "c"

    # Build year_total for web sales
    ws = web_sales.merge(customer, left_on="ws_bill_customer_sk", right_on="c_customer_sk")
    ws = ws.merge(date_dim[["d_date_sk", "d_year"]], left_on="ws_sold_date_sk", right_on="d_date_sk")
    ws["year_total"] = (
        ws["ws_ext_list_price"] - ws["ws_ext_wholesale_cost"] - ws["ws_ext_discount_amt"] + ws["ws_ext_sales_price"]
    ) / 2
    ws_agg = ws.groupby(cust_cols + ["d_year"], as_index=False).agg(year_total=("year_total", "sum"))
    ws_agg["sale_type"] = "w"

    # Combine all channels
    year_total = ctx.concat([ss_agg, cs_agg, ws_agg])

    # Filter for first year and second year for each channel
    t_s_firstyear = year_total[
        (year_total["sale_type"] == "s") & (year_total["d_year"] == year) & (year_total["year_total"] > 0)
    ][["c_customer_id", "year_total"]].rename(columns={"year_total": "s_first_total"})

    t_s_secyear = year_total[(year_total["sale_type"] == "s") & (year_total["d_year"] == year + 1)][
        ["c_customer_id", "c_first_name", "c_last_name", "c_preferred_cust_flag", "year_total"]
    ].rename(columns={"year_total": "s_sec_total"})

    t_c_firstyear = year_total[
        (year_total["sale_type"] == "c") & (year_total["d_year"] == year) & (year_total["year_total"] > 0)
    ][["c_customer_id", "year_total"]].rename(columns={"year_total": "c_first_total"})

    t_c_secyear = year_total[(year_total["sale_type"] == "c") & (year_total["d_year"] == year + 1)][
        ["c_customer_id", "year_total"]
    ].rename(columns={"year_total": "c_sec_total"})

    t_w_firstyear = year_total[
        (year_total["sale_type"] == "w") & (year_total["d_year"] == year) & (year_total["year_total"] > 0)
    ][["c_customer_id", "year_total"]].rename(columns={"year_total": "w_first_total"})

    t_w_secyear = year_total[(year_total["sale_type"] == "w") & (year_total["d_year"] == year + 1)][
        ["c_customer_id", "year_total"]
    ].rename(columns={"year_total": "w_sec_total"})

    # Join all year totals by customer
    result = t_s_secyear.merge(t_s_firstyear, on="c_customer_id")
    result = result.merge(t_c_firstyear, on="c_customer_id")
    result = result.merge(t_c_secyear, on="c_customer_id")
    result = result.merge(t_w_firstyear, on="c_customer_id")
    result = result.merge(t_w_secyear, on="c_customer_id")

    # Calculate growth ratios
    result["c_growth"] = result["c_sec_total"] / result["c_first_total"]
    result["s_growth"] = result["s_sec_total"] / result["s_first_total"]
    result["w_growth"] = result["w_sec_total"] / result["w_first_total"]

    # Filter: catalog growth > store growth AND catalog growth > web growth
    result = result[(result["c_growth"] > result["s_growth"]) & (result["c_growth"] > result["w_growth"])]

    result = (
        result[["c_customer_id", "c_first_name", "c_last_name", "c_preferred_cust_flag"]]
        .sort_values(["c_customer_id", "c_first_name", "c_last_name", "c_preferred_cust_flag"])
        .head(100)
    )

    return result


# =============================================================================
# Q5: Three-Channel Sales-Returns with ROLLUP (Sales/Returns Union)
# =============================================================================


def q5_expression_impl(ctx: DataFrameContext) -> Any:
    """Q5: Three-channel sales-returns with ROLLUP (Polars).

    Each channel unions sales (with 0 returns) and returns (with 0 sales),
    then aggregates by store/page/site and applies ROLLUP(channel, id).

    Tables: store_sales, store_returns, date_dim, store,
            catalog_sales, catalog_returns, catalog_page,
            web_sales, web_returns, web_site
    """
    from datetime import datetime, timedelta

    from .rollup_helper import expand_rollup_expression

    params = get_parameters(5)
    sales_date = params.get("sales_date", "2000-08-23")

    store_sales = ctx.get_table("store_sales")
    store_returns = ctx.get_table("store_returns")
    catalog_sales = ctx.get_table("catalog_sales")
    catalog_returns = ctx.get_table("catalog_returns")
    web_sales = ctx.get_table("web_sales")
    web_returns = ctx.get_table("web_returns")
    date_dim = ctx.get_table("date_dim")
    store = ctx.get_table("store")
    catalog_page = ctx.get_table("catalog_page")
    web_site = ctx.get_table("web_site")
    col = ctx.col
    lit = ctx.lit

    # Parse the sales_date
    start_date = datetime.strptime(sales_date, "%Y-%m-%d").date() if isinstance(sales_date, str) else sales_date
    end_date = start_date + timedelta(days=14)

    # Filter date_dim
    date_filtered = date_dim.filter((col("d_date") >= lit(start_date)) & (col("d_date") <= lit(end_date)))

    # Store sales union store returns
    ss_part = store_sales.select(
        [
            col("ss_store_sk").alias("store_sk"),
            col("ss_sold_date_sk").alias("date_sk"),
            col("ss_ext_sales_price").alias("sales_price"),
            col("ss_net_profit").alias("profit"),
            lit(0.0).alias("return_amt"),
            lit(0.0).alias("net_loss"),
        ]
    )
    sr_part = store_returns.select(
        [
            col("sr_store_sk").alias("store_sk"),
            col("sr_returned_date_sk").alias("date_sk"),
            lit(0.0).alias("sales_price"),
            lit(0.0).alias("profit"),
            col("sr_return_amt").alias("return_amt"),
            col("sr_net_loss").alias("net_loss"),
        ]
    )
    store_union = ctx.concat([ss_part, sr_part])

    ssr = (
        store_union.join(date_filtered, left_on="date_sk", right_on="d_date_sk")
        .join(store, left_on="store_sk", right_on="s_store_sk")
        .group_by("s_store_id")
        .agg(
            [
                col("sales_price").sum().alias("sales"),
                col("profit").sum().alias("profit_sum"),
                col("return_amt").sum().alias("returns"),
                col("net_loss").sum().alias("profit_loss"),
            ]
        )
        .with_columns(
            [
                lit("store channel").alias("channel"),
                (lit("store") + col("s_store_id")).alias("id"),
                (col("profit_sum") - col("profit_loss")).alias("profit"),
            ]
        )
        .select(["channel", "id", "sales", "returns", "profit"])
    )

    # Catalog sales union catalog returns
    cs_part = catalog_sales.select(
        [
            col("cs_catalog_page_sk").alias("page_sk"),
            col("cs_sold_date_sk").alias("date_sk"),
            col("cs_ext_sales_price").alias("sales_price"),
            col("cs_net_profit").alias("profit"),
            lit(0.0).alias("return_amt"),
            lit(0.0).alias("net_loss"),
        ]
    )
    cr_part = catalog_returns.select(
        [
            col("cr_catalog_page_sk").alias("page_sk"),
            col("cr_returned_date_sk").alias("date_sk"),
            lit(0.0).alias("sales_price"),
            lit(0.0).alias("profit"),
            col("cr_return_amount").alias("return_amt"),
            col("cr_net_loss").alias("net_loss"),
        ]
    )
    catalog_union = ctx.concat([cs_part, cr_part])

    csr = (
        catalog_union.join(date_filtered, left_on="date_sk", right_on="d_date_sk")
        .join(catalog_page, left_on="page_sk", right_on="cp_catalog_page_sk")
        .group_by("cp_catalog_page_id")
        .agg(
            [
                col("sales_price").sum().alias("sales"),
                col("profit").sum().alias("profit_sum"),
                col("return_amt").sum().alias("returns"),
                col("net_loss").sum().alias("profit_loss"),
            ]
        )
        .with_columns(
            [
                lit("catalog channel").alias("channel"),
                (lit("catalog_page") + col("cp_catalog_page_id")).alias("id"),
                (col("profit_sum") - col("profit_loss")).alias("profit"),
            ]
        )
        .select(["channel", "id", "sales", "returns", "profit"])
    )

    # Web sales union web returns (with left join to get web_site_sk)
    ws_part = web_sales.select(
        [
            col("ws_web_site_sk").alias("wsr_web_site_sk"),
            col("ws_sold_date_sk").alias("date_sk"),
            col("ws_ext_sales_price").alias("sales_price"),
            col("ws_net_profit").alias("profit"),
            lit(0.0).alias("return_amt"),
            lit(0.0).alias("net_loss"),
        ]
    )
    # Web returns need to get web_site_sk from web_sales
    wr_with_site = web_returns.join(
        web_sales, left_on=["wr_item_sk", "wr_order_number"], right_on=["ws_item_sk", "ws_order_number"], how="left"
    ).select(
        [
            col("ws_web_site_sk").alias("wsr_web_site_sk"),
            col("wr_returned_date_sk").alias("date_sk"),
            lit(0.0).alias("sales_price"),
            lit(0.0).alias("profit"),
            col("wr_return_amt").alias("return_amt"),
            col("wr_net_loss").alias("net_loss"),
        ]
    )
    web_union = ctx.concat([ws_part, wr_with_site])

    wsr = (
        web_union.join(date_filtered, left_on="date_sk", right_on="d_date_sk")
        .join(web_site, left_on="wsr_web_site_sk", right_on="web_site_sk")
        .group_by("web_site_id")
        .agg(
            [
                col("sales_price").sum().alias("sales"),
                col("profit").sum().alias("profit_sum"),
                col("return_amt").sum().alias("returns"),
                col("net_loss").sum().alias("profit_loss"),
            ]
        )
        .with_columns(
            [
                lit("web channel").alias("channel"),
                (lit("web_site") + col("web_site_id")).alias("id"),
                (col("profit_sum") - col("profit_loss")).alias("profit"),
            ]
        )
        .select(["channel", "id", "sales", "returns", "profit"])
    )

    # Union all channels
    combined = ctx.concat([ssr, csr, wsr])

    # Apply ROLLUP(channel, id)
    result = expand_rollup_expression(
        combined,
        group_cols=["channel", "id"],
        agg_exprs=[
            col("sales").sum().alias("sales"),
            col("returns").sum().alias("returns"),
            col("profit").sum().alias("profit"),
        ],
        ctx=ctx,
    )

    result = result.sort(["channel", "id"]).limit(100)

    return result


def q5_pandas_impl(ctx: DataFrameContext) -> Any:
    """Q5: Three-channel sales-returns with ROLLUP (Pandas)."""
    from datetime import datetime, timedelta

    import pandas as pd

    from .rollup_helper import expand_rollup_pandas

    params = get_parameters(5)
    sales_date = params.get("sales_date", "2000-08-23")

    store_sales = ctx.get_table("store_sales")
    store_returns = ctx.get_table("store_returns")
    catalog_sales = ctx.get_table("catalog_sales")
    catalog_returns = ctx.get_table("catalog_returns")
    web_sales = ctx.get_table("web_sales")
    web_returns = ctx.get_table("web_returns")
    date_dim = ctx.get_table("date_dim")
    store = ctx.get_table("store")
    catalog_page = ctx.get_table("catalog_page")
    web_site = ctx.get_table("web_site")

    # Parse the sales_date
    start_date = datetime.strptime(sales_date, "%Y-%m-%d").date() if isinstance(sales_date, str) else sales_date
    end_date = start_date + timedelta(days=14)

    # Convert date_dim d_date column for comparison
    date_dim_copy = date_dim.copy()
    if hasattr(date_dim_copy["d_date"].iloc[0] if len(date_dim_copy) > 0 else None, "date"):
        date_dim_copy["d_date"] = pd.to_datetime(date_dim_copy["d_date"]).dt.date

    # Filter date_dim
    date_filtered = date_dim_copy[(date_dim_copy["d_date"] >= start_date) & (date_dim_copy["d_date"] <= end_date)][
        ["d_date_sk"]
    ]

    # Store sales union store returns
    ss_part = store_sales[["ss_store_sk", "ss_sold_date_sk", "ss_ext_sales_price", "ss_net_profit"]].copy()
    ss_part = ss_part.rename(
        columns={
            "ss_store_sk": "store_sk",
            "ss_sold_date_sk": "date_sk",
            "ss_ext_sales_price": "sales_price",
            "ss_net_profit": "profit",
        }
    )
    ss_part["return_amt"] = 0.0
    ss_part["net_loss"] = 0.0

    sr_part = store_returns[["sr_store_sk", "sr_returned_date_sk", "sr_return_amt", "sr_net_loss"]].copy()
    sr_part = sr_part.rename(
        columns={
            "sr_store_sk": "store_sk",
            "sr_returned_date_sk": "date_sk",
            "sr_return_amt": "return_amt",
            "sr_net_loss": "net_loss",
        }
    )
    sr_part["sales_price"] = 0.0
    sr_part["profit"] = 0.0

    store_union = ctx.concat([ss_part, sr_part])
    store_union = store_union.merge(date_filtered, left_on="date_sk", right_on="d_date_sk")
    store_union = store_union.merge(store[["s_store_sk", "s_store_id"]], left_on="store_sk", right_on="s_store_sk")

    ssr = store_union.groupby("s_store_id", as_index=False).agg(
        sales=("sales_price", "sum"),
        profit_sum=("profit", "sum"),
        returns=("return_amt", "sum"),
        profit_loss=("net_loss", "sum"),
    )
    ssr["channel"] = "store channel"
    ssr["id"] = "store" + ssr["s_store_id"].astype(str)
    ssr["profit"] = ssr["profit_sum"] - ssr["profit_loss"]
    ssr = ssr[["channel", "id", "sales", "returns", "profit"]]

    # Catalog sales union catalog returns
    cs_part = catalog_sales[["cs_catalog_page_sk", "cs_sold_date_sk", "cs_ext_sales_price", "cs_net_profit"]].copy()
    cs_part = cs_part.rename(
        columns={
            "cs_catalog_page_sk": "page_sk",
            "cs_sold_date_sk": "date_sk",
            "cs_ext_sales_price": "sales_price",
            "cs_net_profit": "profit",
        }
    )
    cs_part["return_amt"] = 0.0
    cs_part["net_loss"] = 0.0

    cr_part = catalog_returns[["cr_catalog_page_sk", "cr_returned_date_sk", "cr_return_amount", "cr_net_loss"]].copy()
    cr_part = cr_part.rename(
        columns={
            "cr_catalog_page_sk": "page_sk",
            "cr_returned_date_sk": "date_sk",
            "cr_return_amount": "return_amt",
            "cr_net_loss": "net_loss",
        }
    )
    cr_part["sales_price"] = 0.0
    cr_part["profit"] = 0.0

    catalog_union = ctx.concat([cs_part, cr_part])
    catalog_union = catalog_union.merge(date_filtered, left_on="date_sk", right_on="d_date_sk")
    catalog_union = catalog_union.merge(
        catalog_page[["cp_catalog_page_sk", "cp_catalog_page_id"]], left_on="page_sk", right_on="cp_catalog_page_sk"
    )

    csr = catalog_union.groupby("cp_catalog_page_id", as_index=False).agg(
        sales=("sales_price", "sum"),
        profit_sum=("profit", "sum"),
        returns=("return_amt", "sum"),
        profit_loss=("net_loss", "sum"),
    )
    csr["channel"] = "catalog channel"
    csr["id"] = "catalog_page" + csr["cp_catalog_page_id"].astype(str)
    csr["profit"] = csr["profit_sum"] - csr["profit_loss"]
    csr = csr[["channel", "id", "sales", "returns", "profit"]]

    # Web sales union web returns (with left join to get web_site_sk)
    ws_part = web_sales[["ws_web_site_sk", "ws_sold_date_sk", "ws_ext_sales_price", "ws_net_profit"]].copy()
    ws_part = ws_part.rename(
        columns={
            "ws_web_site_sk": "wsr_web_site_sk",
            "ws_sold_date_sk": "date_sk",
            "ws_ext_sales_price": "sales_price",
            "ws_net_profit": "profit",
        }
    )
    ws_part["return_amt"] = 0.0
    ws_part["net_loss"] = 0.0

    # Web returns need to get web_site_sk from web_sales
    wr_with_site = web_returns.merge(
        web_sales[["ws_item_sk", "ws_order_number", "ws_web_site_sk"]],
        left_on=["wr_item_sk", "wr_order_number"],
        right_on=["ws_item_sk", "ws_order_number"],
        how="left",
    )
    wr_part = wr_with_site[["ws_web_site_sk", "wr_returned_date_sk", "wr_return_amt", "wr_net_loss"]].copy()
    wr_part = wr_part.rename(
        columns={
            "ws_web_site_sk": "wsr_web_site_sk",
            "wr_returned_date_sk": "date_sk",
            "wr_return_amt": "return_amt",
            "wr_net_loss": "net_loss",
        }
    )
    wr_part["sales_price"] = 0.0
    wr_part["profit"] = 0.0

    web_union = ctx.concat([ws_part, wr_part])
    web_union = web_union.merge(date_filtered, left_on="date_sk", right_on="d_date_sk")
    web_union = web_union.merge(
        web_site[["web_site_sk", "web_site_id"]], left_on="wsr_web_site_sk", right_on="web_site_sk"
    )

    wsr = web_union.groupby("web_site_id", as_index=False).agg(
        sales=("sales_price", "sum"),
        profit_sum=("profit", "sum"),
        returns=("return_amt", "sum"),
        profit_loss=("net_loss", "sum"),
    )
    wsr["channel"] = "web channel"
    wsr["id"] = "web_site" + wsr["web_site_id"].astype(str)
    wsr["profit"] = wsr["profit_sum"] - wsr["profit_loss"]
    wsr = wsr[["channel", "id", "sales", "returns", "profit"]]

    # Union all channels
    combined = ctx.concat([ssr, csr, wsr])

    # Apply ROLLUP(channel, id)
    result = expand_rollup_pandas(
        combined,
        group_cols=["channel", "id"],
        agg_dict={
            "sales": ("sales", "sum"),
            "returns": ("returns", "sum"),
            "profit": ("profit", "sum"),
        },
        ctx=ctx,
    )

    result = result.sort_values(["channel", "id"]).head(100)

    return result


# =============================================================================
# Q10: Customer Demographics with Multi-Channel Semi-Joins
# =============================================================================


def q10_expression_impl(ctx: DataFrameContext) -> Any:
    """Q10: Customer demographics with multi-channel semi-joins (Polars).

    Find customers who bought from store AND (web OR catalog) in a given period,
    then report demographic aggregates.

    Tables: customer, customer_address, customer_demographics, store_sales,
            web_sales, catalog_sales, date_dim
    """

    params = get_parameters(10)
    year = params.get("year", 2002)
    month = params.get("month", 2)
    counties = params.get(
        "counties", ["Walker County", "Richland County", "Gaines County", "Douglas County", "Dona Ana County"]
    )

    customer = ctx.get_table("customer")
    customer_address = ctx.get_table("customer_address")
    customer_demographics = ctx.get_table("customer_demographics")
    store_sales = ctx.get_table("store_sales")
    web_sales = ctx.get_table("web_sales")
    catalog_sales = ctx.get_table("catalog_sales")
    date_dim = ctx.get_table("date_dim")
    col = ctx.col
    lit = ctx.lit

    # Filter dates for the month range
    date_filtered = date_dim.filter(
        (col("d_year") == lit(year)) & (col("d_moy") >= lit(month)) & (col("d_moy") <= lit(month + 3))
    )

    # Get store sales customers
    ss_customers = (
        store_sales.join(date_filtered, left_on="ss_sold_date_sk", right_on="d_date_sk")
        .select("ss_customer_sk")
        .unique()
    )

    # Get web sales customers
    ws_customers = (
        web_sales.join(date_filtered, left_on="ws_sold_date_sk", right_on="d_date_sk")
        .select(col("ws_bill_customer_sk").alias("customer_sk"))
        .unique()
    )

    # Get catalog sales customers
    cs_customers = (
        catalog_sales.join(date_filtered, left_on="cs_sold_date_sk", right_on="d_date_sk")
        .select(col("cs_ship_customer_sk").alias("customer_sk"))
        .unique()
    )

    # Web OR catalog customers
    ws_or_cs = ctx.concat([ws_customers, cs_customers]).unique()

    # Customers in store AND (web OR catalog)
    ss_and_wc = ss_customers.join(ws_or_cs, left_on="ss_customer_sk", right_on="customer_sk")

    # Build base: customer -> customer_address -> customer_demographics
    base = (
        customer.join(customer_address, left_on="c_current_addr_sk", right_on="ca_address_sk")
        .filter(col("ca_county").is_in(counties))
        .join(customer_demographics, left_on="c_current_cdemo_sk", right_on="cd_demo_sk")
        .join(ss_and_wc, left_on="c_customer_sk", right_on="ss_customer_sk")
    )

    # Group by demographics
    result = (
        base.group_by(
            [
                "cd_gender",
                "cd_marital_status",
                "cd_education_status",
                "cd_purchase_estimate",
                "cd_credit_rating",
                "cd_dep_count",
                "cd_dep_employed_count",
                "cd_dep_college_count",
            ]
        )
        .agg(ctx.count().alias("cnt"))
        .with_columns(
            [
                col("cnt").alias("cnt1"),
                col("cnt").alias("cnt2"),
                col("cnt").alias("cnt3"),
                col("cnt").alias("cnt4"),
                col("cnt").alias("cnt5"),
                col("cnt").alias("cnt6"),
            ]
        )
        .select(
            [
                "cd_gender",
                "cd_marital_status",
                "cd_education_status",
                "cnt1",
                "cd_purchase_estimate",
                "cnt2",
                "cd_credit_rating",
                "cnt3",
                "cd_dep_count",
                "cnt4",
                "cd_dep_employed_count",
                "cnt5",
                "cd_dep_college_count",
                "cnt6",
            ]
        )
        .sort(
            [
                "cd_gender",
                "cd_marital_status",
                "cd_education_status",
                "cd_purchase_estimate",
                "cd_credit_rating",
                "cd_dep_count",
                "cd_dep_employed_count",
                "cd_dep_college_count",
            ]
        )
        .limit(100)
    )

    return result


def q10_pandas_impl(ctx: DataFrameContext) -> Any:
    """Q10: Customer demographics with multi-channel semi-joins (Pandas)."""

    params = get_parameters(10)
    year = params.get("year", 2002)
    month = params.get("month", 2)
    counties = params.get(
        "counties", ["Walker County", "Richland County", "Gaines County", "Douglas County", "Dona Ana County"]
    )

    customer = ctx.get_table("customer")
    customer_address = ctx.get_table("customer_address")
    customer_demographics = ctx.get_table("customer_demographics")
    store_sales = ctx.get_table("store_sales")
    web_sales = ctx.get_table("web_sales")
    catalog_sales = ctx.get_table("catalog_sales")
    date_dim = ctx.get_table("date_dim")

    # Filter dates for the month range
    date_filtered = date_dim[
        (date_dim["d_year"] == year) & (date_dim["d_moy"] >= month) & (date_dim["d_moy"] <= month + 3)
    ][["d_date_sk"]]

    # Get store sales customers
    ss = store_sales.merge(date_filtered, left_on="ss_sold_date_sk", right_on="d_date_sk")
    ss_customers = ss["ss_customer_sk"].drop_duplicates()

    # Get web sales customers
    ws = web_sales.merge(date_filtered, left_on="ws_sold_date_sk", right_on="d_date_sk")
    ws_customers = ws["ws_bill_customer_sk"].drop_duplicates()

    # Get catalog sales customers
    cs = catalog_sales.merge(date_filtered, left_on="cs_sold_date_sk", right_on="d_date_sk")
    cs_customers = cs["cs_ship_customer_sk"].drop_duplicates()

    # Web OR catalog customers
    ws_or_cs = ctx.concat([ws_customers, cs_customers]).drop_duplicates()

    # Customers in store AND (web OR catalog)
    ss_and_wc = ss_customers[ss_customers.isin(ws_or_cs)]

    # Build base: customer -> customer_address -> customer_demographics
    base = customer.merge(customer_address, left_on="c_current_addr_sk", right_on="ca_address_sk")
    base = base[base["ca_county"].isin(counties)]
    base = base.merge(customer_demographics, left_on="c_current_cdemo_sk", right_on="cd_demo_sk")
    base = base[base["c_customer_sk"].isin(ss_and_wc)]

    # Group by demographics
    group_cols = [
        "cd_gender",
        "cd_marital_status",
        "cd_education_status",
        "cd_purchase_estimate",
        "cd_credit_rating",
        "cd_dep_count",
        "cd_dep_employed_count",
        "cd_dep_college_count",
    ]
    # Use ctx.groupby_size for Dask compatibility
    result = ctx.groupby_size(base, group_cols, name="cnt")

    # Add duplicate count columns as per query spec
    result["cnt1"] = result["cnt"]
    result["cnt2"] = result["cnt"]
    result["cnt3"] = result["cnt"]
    result["cnt4"] = result["cnt"]
    result["cnt5"] = result["cnt"]
    result["cnt6"] = result["cnt"]

    result = result[
        [
            "cd_gender",
            "cd_marital_status",
            "cd_education_status",
            "cnt1",
            "cd_purchase_estimate",
            "cnt2",
            "cd_credit_rating",
            "cnt3",
            "cd_dep_count",
            "cnt4",
            "cd_dep_employed_count",
            "cnt5",
            "cd_dep_college_count",
            "cnt6",
        ]
    ]

    result = result.sort_values(group_cols).head(100)

    return result


# =============================================================================
# Q11: Store/Web Year-over-Year Customer Comparison
# =============================================================================


def q11_expression_impl(ctx: DataFrameContext) -> Any:
    """Q11: Store/Web year-over-year customer comparison (Polars).

    Similar to Q4 but only two channels (store and web).
    Filters customers where web growth > store growth.

    Tables: customer, store_sales, web_sales, date_dim
    """

    params = get_parameters(11)
    year = params.get("year", 2001)

    customer = ctx.get_table("customer")
    store_sales = ctx.get_table("store_sales")
    web_sales = ctx.get_table("web_sales")
    date_dim = ctx.get_table("date_dim")
    col = ctx.col
    lit = ctx.lit

    # Customer columns for grouping
    cust_cols = [
        "c_customer_id",
        "c_first_name",
        "c_last_name",
        "c_preferred_cust_flag",
        "c_birth_country",
        "c_login",
        "c_email_address",
    ]

    # Build year_total for store sales
    ss_year = (
        store_sales.join(customer, left_on="ss_customer_sk", right_on="c_customer_sk")
        .join(date_dim, left_on="ss_sold_date_sk", right_on="d_date_sk")
        .group_by(cust_cols + ["d_year"])
        .agg(
            [
                (col("ss_ext_list_price") - col("ss_ext_discount_amt")).sum().alias("year_total"),
            ]
        )
        .with_columns(lit("s").alias("sale_type"))
    )

    # Build year_total for web sales
    ws_year = (
        web_sales.join(customer, left_on="ws_bill_customer_sk", right_on="c_customer_sk")
        .join(date_dim, left_on="ws_sold_date_sk", right_on="d_date_sk")
        .group_by(cust_cols + ["d_year"])
        .agg(
            [
                (col("ws_ext_list_price") - col("ws_ext_discount_amt")).sum().alias("year_total"),
            ]
        )
        .with_columns(lit("w").alias("sale_type"))
    )

    # Combine channels
    year_total = ctx.concat([ss_year, ws_year])

    # Filter for first year and second year for each channel
    t_s_firstyear = (
        year_total.filter((col("sale_type") == lit("s")) & (col("d_year") == lit(year)))
        .filter(col("year_total") > lit(0))
        .select([col("c_customer_id"), col("year_total").alias("s_first_total")])
    )

    t_s_secyear = year_total.filter((col("sale_type") == lit("s")) & (col("d_year") == lit(year + 1))).select(
        [
            col("c_customer_id"),
            col("c_first_name"),
            col("c_last_name"),
            col("c_preferred_cust_flag"),
            col("year_total").alias("s_sec_total"),
        ]
    )

    t_w_firstyear = (
        year_total.filter((col("sale_type") == lit("w")) & (col("d_year") == lit(year)))
        .filter(col("year_total") > lit(0))
        .select([col("c_customer_id"), col("year_total").alias("w_first_total")])
    )

    t_w_secyear = year_total.filter((col("sale_type") == lit("w")) & (col("d_year") == lit(year + 1))).select(
        [col("c_customer_id"), col("year_total").alias("w_sec_total")]
    )

    # Join all year totals by customer
    result = (
        t_s_secyear.join(t_s_firstyear, on="c_customer_id")
        .join(t_w_firstyear, on="c_customer_id")
        .join(t_w_secyear, on="c_customer_id")
    )

    # Calculate growth ratios and filter
    result = result.with_columns(
        [
            ctx.when(col("w_first_total") > lit(0))
            .then(col("w_sec_total") / col("w_first_total"))
            .otherwise(lit(0.0))
            .alias("w_growth"),
            ctx.when(col("s_first_total") > lit(0))
            .then(col("s_sec_total") / col("s_first_total"))
            .otherwise(lit(0.0))
            .alias("s_growth"),
        ]
    )

    # Filter: web growth > store growth
    result = (
        result.filter(col("w_growth") > col("s_growth"))
        .select(
            [
                col("c_customer_id"),
                col("c_first_name"),
                col("c_last_name"),
                col("c_preferred_cust_flag"),
            ]
        )
        .sort(["c_customer_id", "c_first_name", "c_last_name", "c_preferred_cust_flag"])
        .limit(100)
    )

    return result


def q11_pandas_impl(ctx: DataFrameContext) -> Any:
    """Q11: Store/Web year-over-year customer comparison (Pandas)."""

    params = get_parameters(11)
    year = params.get("year", 2001)

    customer = ctx.get_table("customer")
    store_sales = ctx.get_table("store_sales")
    web_sales = ctx.get_table("web_sales")
    date_dim = ctx.get_table("date_dim")

    # Customer columns for grouping
    cust_cols = [
        "c_customer_id",
        "c_first_name",
        "c_last_name",
        "c_preferred_cust_flag",
        "c_birth_country",
        "c_login",
        "c_email_address",
    ]

    # Build year_total for store sales
    ss = store_sales.merge(customer, left_on="ss_customer_sk", right_on="c_customer_sk")
    ss = ss.merge(date_dim[["d_date_sk", "d_year"]], left_on="ss_sold_date_sk", right_on="d_date_sk")
    ss["year_total"] = ss["ss_ext_list_price"] - ss["ss_ext_discount_amt"]
    ss_agg = ss.groupby(cust_cols + ["d_year"], as_index=False).agg(year_total=("year_total", "sum"))
    ss_agg["sale_type"] = "s"

    # Build year_total for web sales
    ws = web_sales.merge(customer, left_on="ws_bill_customer_sk", right_on="c_customer_sk")
    ws = ws.merge(date_dim[["d_date_sk", "d_year"]], left_on="ws_sold_date_sk", right_on="d_date_sk")
    ws["year_total"] = ws["ws_ext_list_price"] - ws["ws_ext_discount_amt"]
    ws_agg = ws.groupby(cust_cols + ["d_year"], as_index=False).agg(year_total=("year_total", "sum"))
    ws_agg["sale_type"] = "w"

    # Combine channels
    year_total = ctx.concat([ss_agg, ws_agg])

    # Filter for first year and second year for each channel
    t_s_firstyear = year_total[
        (year_total["sale_type"] == "s") & (year_total["d_year"] == year) & (year_total["year_total"] > 0)
    ][["c_customer_id", "year_total"]].rename(columns={"year_total": "s_first_total"})

    t_s_secyear = year_total[(year_total["sale_type"] == "s") & (year_total["d_year"] == year + 1)][
        ["c_customer_id", "c_first_name", "c_last_name", "c_preferred_cust_flag", "year_total"]
    ].rename(columns={"year_total": "s_sec_total"})

    t_w_firstyear = year_total[
        (year_total["sale_type"] == "w") & (year_total["d_year"] == year) & (year_total["year_total"] > 0)
    ][["c_customer_id", "year_total"]].rename(columns={"year_total": "w_first_total"})

    t_w_secyear = year_total[(year_total["sale_type"] == "w") & (year_total["d_year"] == year + 1)][
        ["c_customer_id", "year_total"]
    ].rename(columns={"year_total": "w_sec_total"})

    # Join all year totals by customer
    result = t_s_secyear.merge(t_s_firstyear, on="c_customer_id")
    result = result.merge(t_w_firstyear, on="c_customer_id")
    result = result.merge(t_w_secyear, on="c_customer_id")

    # Calculate growth ratios
    result["w_growth"] = result.apply(
        lambda r: r["w_sec_total"] / r["w_first_total"] if r["w_first_total"] > 0 else 0.0, axis=1
    )
    result["s_growth"] = result.apply(
        lambda r: r["s_sec_total"] / r["s_first_total"] if r["s_first_total"] > 0 else 0.0, axis=1
    )

    # Filter: web growth > store growth
    result = result[result["w_growth"] > result["s_growth"]]

    result = (
        result[["c_customer_id", "c_first_name", "c_last_name", "c_preferred_cust_flag"]]
        .sort_values(["c_customer_id", "c_first_name", "c_last_name", "c_preferred_cust_flag"])
        .head(100)
    )

    return result


# =============================================================================
# Q35: Customer Demographics with Multi-Channel Semi-Joins (Quarterly)
# =============================================================================


def q35_expression_impl(ctx: DataFrameContext) -> Any:
    """Q35: Customer demographics with multi-channel semi-joins (Polars).

    Similar to Q10 but uses quarterly filter and groups by ca_state.
    Find customers who bought from store AND (web OR catalog) in first 3 quarters.

    Tables: customer, customer_address, customer_demographics, store_sales,
            web_sales, catalog_sales, date_dim
    """

    params = get_parameters(35)
    year = params.get("year", 2002)

    customer = ctx.get_table("customer")
    customer_address = ctx.get_table("customer_address")
    customer_demographics = ctx.get_table("customer_demographics")
    store_sales = ctx.get_table("store_sales")
    web_sales = ctx.get_table("web_sales")
    catalog_sales = ctx.get_table("catalog_sales")
    date_dim = ctx.get_table("date_dim")
    col = ctx.col
    lit = ctx.lit

    # Filter dates for first 3 quarters
    date_filtered = date_dim.filter((col("d_year") == lit(year)) & (col("d_qoy") < lit(4)))

    # Get store sales customers
    ss_customers = (
        store_sales.join(date_filtered, left_on="ss_sold_date_sk", right_on="d_date_sk")
        .select("ss_customer_sk")
        .unique()
    )

    # Get web sales customers
    ws_customers = (
        web_sales.join(date_filtered, left_on="ws_sold_date_sk", right_on="d_date_sk")
        .select(col("ws_bill_customer_sk").alias("customer_sk"))
        .unique()
    )

    # Get catalog sales customers
    cs_customers = (
        catalog_sales.join(date_filtered, left_on="cs_sold_date_sk", right_on="d_date_sk")
        .select(col("cs_ship_customer_sk").alias("customer_sk"))
        .unique()
    )

    # Web OR catalog customers
    ws_or_cs = ctx.concat([ws_customers, cs_customers]).unique()

    # Customers in store AND (web OR catalog)
    ss_and_wc = ss_customers.join(ws_or_cs, left_on="ss_customer_sk", right_on="customer_sk")

    # Build base: customer -> customer_address -> customer_demographics
    base = (
        customer.join(customer_address, left_on="c_current_addr_sk", right_on="ca_address_sk")
        .join(customer_demographics, left_on="c_current_cdemo_sk", right_on="cd_demo_sk")
        .join(ss_and_wc, left_on="c_customer_sk", right_on="ss_customer_sk")
    )

    # Group by demographics + state
    group_cols = [
        "ca_state",
        "cd_gender",
        "cd_marital_status",
        "cd_dep_count",
        "cd_dep_employed_count",
        "cd_dep_college_count",
    ]

    result = (
        base.group_by(group_cols)
        .agg(
            [
                ctx.count().alias("cnt1"),
                col("cd_dep_count").sum().alias("sum_dep_count"),
                col("cd_dep_count").min().alias("min_dep_count"),
                col("cd_dep_count").max().alias("max_dep_count"),
                col("cd_dep_count").mean().alias("avg_dep_count"),
                col("cd_dep_employed_count").sum().alias("sum_dep_employed"),
                col("cd_dep_employed_count").min().alias("min_dep_employed"),
                col("cd_dep_employed_count").max().alias("max_dep_employed"),
                col("cd_dep_employed_count").mean().alias("avg_dep_employed"),
                col("cd_dep_college_count").sum().alias("sum_dep_college"),
                col("cd_dep_college_count").min().alias("min_dep_college"),
                col("cd_dep_college_count").max().alias("max_dep_college"),
                col("cd_dep_college_count").mean().alias("avg_dep_college"),
            ]
        )
        .with_columns(
            [
                col("cnt1").alias("cnt2"),
                col("cnt1").alias("cnt3"),
            ]
        )
        .sort(group_cols)
        .limit(100)
    )

    return result


def q35_pandas_impl(ctx: DataFrameContext) -> Any:
    """Q35: Customer demographics with multi-channel semi-joins (Pandas)."""

    params = get_parameters(35)
    year = params.get("year", 2002)

    customer = ctx.get_table("customer")
    customer_address = ctx.get_table("customer_address")
    customer_demographics = ctx.get_table("customer_demographics")
    store_sales = ctx.get_table("store_sales")
    web_sales = ctx.get_table("web_sales")
    catalog_sales = ctx.get_table("catalog_sales")
    date_dim = ctx.get_table("date_dim")

    # Filter dates for first 3 quarters
    date_filtered = date_dim[(date_dim["d_year"] == year) & (date_dim["d_qoy"] < 4)][["d_date_sk"]]

    # Get store sales customers
    ss = store_sales.merge(date_filtered, left_on="ss_sold_date_sk", right_on="d_date_sk")
    ss_customers = ss["ss_customer_sk"].drop_duplicates()

    # Get web sales customers
    ws = web_sales.merge(date_filtered, left_on="ws_sold_date_sk", right_on="d_date_sk")
    ws_customers = ws["ws_bill_customer_sk"].drop_duplicates()

    # Get catalog sales customers
    cs = catalog_sales.merge(date_filtered, left_on="cs_sold_date_sk", right_on="d_date_sk")
    cs_customers = cs["cs_ship_customer_sk"].drop_duplicates()

    # Web OR catalog customers (compute to set for Dask .isin() compatibility)
    ws_or_cs = ctx.concat([ws_customers, cs_customers]).drop_duplicates()
    ws_or_cs_set = ctx.to_set(ws_or_cs)

    # Customers in store AND (web OR catalog) - compute to set
    ss_customers_set = ctx.to_set(ss_customers)
    ss_and_wc = ss_customers_set.intersection(ws_or_cs_set)

    # Build base
    base = customer.merge(customer_address, left_on="c_current_addr_sk", right_on="ca_address_sk")
    base = base.merge(customer_demographics, left_on="c_current_cdemo_sk", right_on="cd_demo_sk")
    base = base[base["c_customer_sk"].isin(ss_and_wc)]

    # Group by demographics + state
    group_cols = [
        "ca_state",
        "cd_gender",
        "cd_marital_status",
        "cd_dep_count",
        "cd_dep_employed_count",
        "cd_dep_college_count",
    ]

    agg_spec = {
        "cnt1": ("c_customer_sk", "count"),
        "sum_dep_count": ("cd_dep_count", "sum"),
        "min_dep_count": ("cd_dep_count", "min"),
        "max_dep_count": ("cd_dep_count", "max"),
        "avg_dep_count": ("cd_dep_count", "mean"),
        "sum_dep_employed": ("cd_dep_employed_count", "sum"),
        "min_dep_employed": ("cd_dep_employed_count", "min"),
        "max_dep_employed": ("cd_dep_employed_count", "max"),
        "avg_dep_employed": ("cd_dep_employed_count", "mean"),
        "sum_dep_college": ("cd_dep_college_count", "sum"),
        "min_dep_college": ("cd_dep_college_count", "min"),
        "max_dep_college": ("cd_dep_college_count", "max"),
        "avg_dep_college": ("cd_dep_college_count", "mean"),
    }
    result = ctx.groupby_agg(base, group_cols, agg_spec, as_index=False)

    result["cnt2"] = result["cnt1"]
    result["cnt3"] = result["cnt1"]

    result = result.sort_values(group_cols).head(100)

    return result


# =============================================================================
# Q38: Three-Channel Customer INTERSECT
# =============================================================================


def q38_expression_impl(ctx: DataFrameContext) -> Any:
    """Q38: Count customers who bought in all three channels on same date (Polars).

    INTERSECT of store, catalog, and web customers with date.

    Tables: customer, store_sales, catalog_sales, web_sales, date_dim
    """

    params = get_parameters(38)
    dms = params.get("dms", 1200)

    customer = ctx.get_table("customer")
    store_sales = ctx.get_table("store_sales")
    catalog_sales = ctx.get_table("catalog_sales")
    web_sales = ctx.get_table("web_sales")
    date_dim = ctx.get_table("date_dim")
    col = ctx.col
    lit = ctx.lit

    # Filter dates
    date_filtered = date_dim.filter((col("d_month_seq") >= lit(dms)) & (col("d_month_seq") <= lit(dms + 11)))

    # Store customers with date
    ss_cust = (
        store_sales.join(date_filtered, left_on="ss_sold_date_sk", right_on="d_date_sk")
        .join(customer, left_on="ss_customer_sk", right_on="c_customer_sk")
        .select(["c_last_name", "c_first_name", "d_date"])
        .unique()
    )

    # Catalog customers with date
    cs_cust = (
        catalog_sales.join(date_filtered, left_on="cs_sold_date_sk", right_on="d_date_sk")
        .join(customer, left_on="cs_bill_customer_sk", right_on="c_customer_sk")
        .select(["c_last_name", "c_first_name", "d_date"])
        .unique()
    )

    # Web customers with date
    ws_cust = (
        web_sales.join(date_filtered, left_on="ws_sold_date_sk", right_on="d_date_sk")
        .join(customer, left_on="ws_bill_customer_sk", right_on="c_customer_sk")
        .select(["c_last_name", "c_first_name", "d_date"])
        .unique()
    )

    # INTERSECT: customers in all three channels
    intersect_1 = ss_cust.join(cs_cust, on=["c_last_name", "c_first_name", "d_date"], how="inner")
    intersect_all = intersect_1.join(ws_cust, on=["c_last_name", "c_first_name", "d_date"], how="inner")

    # Count
    result = intersect_all.select(ctx.count().alias("count"))

    return result


def q38_pandas_impl(ctx: DataFrameContext) -> Any:
    """Q38: Count customers who bought in all three channels on same date (Pandas)."""
    params = get_parameters(38)
    dms = params.get("dms", 1200)

    customer = ctx.get_table("customer")
    store_sales = ctx.get_table("store_sales")
    catalog_sales = ctx.get_table("catalog_sales")
    web_sales = ctx.get_table("web_sales")
    date_dim = ctx.get_table("date_dim")

    # Filter dates
    date_filtered = date_dim[(date_dim["d_month_seq"] >= dms) & (date_dim["d_month_seq"] <= dms + 11)][
        ["d_date_sk", "d_date"]
    ]

    # Store customers with date
    ss = store_sales.merge(date_filtered, left_on="ss_sold_date_sk", right_on="d_date_sk")
    ss = ss.merge(
        customer[["c_customer_sk", "c_last_name", "c_first_name"]], left_on="ss_customer_sk", right_on="c_customer_sk"
    )
    ss_cust = ss[["c_last_name", "c_first_name", "d_date"]].drop_duplicates()

    # Catalog customers with date
    cs = catalog_sales.merge(date_filtered, left_on="cs_sold_date_sk", right_on="d_date_sk")
    cs = cs.merge(
        customer[["c_customer_sk", "c_last_name", "c_first_name"]],
        left_on="cs_bill_customer_sk",
        right_on="c_customer_sk",
    )
    cs_cust = cs[["c_last_name", "c_first_name", "d_date"]].drop_duplicates()

    # Web customers with date
    ws = web_sales.merge(date_filtered, left_on="ws_sold_date_sk", right_on="d_date_sk")
    ws = ws.merge(
        customer[["c_customer_sk", "c_last_name", "c_first_name"]],
        left_on="ws_bill_customer_sk",
        right_on="c_customer_sk",
    )
    ws_cust = ws[["c_last_name", "c_first_name", "d_date"]].drop_duplicates()

    # INTERSECT: customers in all three channels
    intersect_1 = ss_cust.merge(cs_cust, on=["c_last_name", "c_first_name", "d_date"])
    intersect_all = intersect_1.merge(ws_cust, on=["c_last_name", "c_first_name", "d_date"])

    # Count
    import pandas as pd

    result = pd.DataFrame({"count": [len(intersect_all)]})

    return result


# =============================================================================
# Q87: Three-Channel Customer EXCEPT (Store Only)
# =============================================================================


def q87_expression_impl(ctx: DataFrameContext) -> Any:
    """Q87: Count customers who bought from store but NOT catalog/web (Polars).

    EXCEPT pattern: store customers minus catalog and web customers.

    Tables: customer, store_sales, catalog_sales, web_sales, date_dim
    """

    params = get_parameters(87)
    dms = params.get("dms", 1200)

    customer = ctx.get_table("customer")
    store_sales = ctx.get_table("store_sales")
    catalog_sales = ctx.get_table("catalog_sales")
    web_sales = ctx.get_table("web_sales")
    date_dim = ctx.get_table("date_dim")
    col = ctx.col
    lit = ctx.lit

    # Filter dates
    date_filtered = date_dim.filter((col("d_month_seq") >= lit(dms)) & (col("d_month_seq") <= lit(dms + 11)))

    # Store customers with date
    ss_cust = (
        store_sales.join(date_filtered, left_on="ss_sold_date_sk", right_on="d_date_sk")
        .join(customer, left_on="ss_customer_sk", right_on="c_customer_sk")
        .select(["c_last_name", "c_first_name", "d_date"])
        .unique()
    )

    # Catalog customers with date
    cs_cust = (
        catalog_sales.join(date_filtered, left_on="cs_sold_date_sk", right_on="d_date_sk")
        .join(customer, left_on="cs_bill_customer_sk", right_on="c_customer_sk")
        .select(["c_last_name", "c_first_name", "d_date"])
        .unique()
    )

    # Web customers with date
    ws_cust = (
        web_sales.join(date_filtered, left_on="ws_sold_date_sk", right_on="d_date_sk")
        .join(customer, left_on="ws_bill_customer_sk", right_on="c_customer_sk")
        .select(["c_last_name", "c_first_name", "d_date"])
        .unique()
    )

    # EXCEPT: store customers minus catalog and web
    # Use anti-join pattern for EXCEPT
    except_cs = ss_cust.join(cs_cust, on=["c_last_name", "c_first_name", "d_date"], how="anti")
    except_all = except_cs.join(ws_cust, on=["c_last_name", "c_first_name", "d_date"], how="anti")

    # Count
    result = except_all.select(ctx.count().alias("count"))

    return result


def q87_pandas_impl(ctx: DataFrameContext) -> Any:
    """Q87: Count customers who bought from store but NOT catalog/web (Pandas)."""
    params = get_parameters(87)
    dms = params.get("dms", 1200)

    customer = ctx.get_table("customer")
    store_sales = ctx.get_table("store_sales")
    catalog_sales = ctx.get_table("catalog_sales")
    web_sales = ctx.get_table("web_sales")
    date_dim = ctx.get_table("date_dim")

    # Filter dates
    date_filtered = date_dim[(date_dim["d_month_seq"] >= dms) & (date_dim["d_month_seq"] <= dms + 11)][
        ["d_date_sk", "d_date"]
    ]

    # Store customers with date
    ss = store_sales.merge(date_filtered, left_on="ss_sold_date_sk", right_on="d_date_sk")
    ss = ss.merge(
        customer[["c_customer_sk", "c_last_name", "c_first_name"]], left_on="ss_customer_sk", right_on="c_customer_sk"
    )
    ss_cust = ss[["c_last_name", "c_first_name", "d_date"]].drop_duplicates()

    # Catalog customers with date
    cs = catalog_sales.merge(date_filtered, left_on="cs_sold_date_sk", right_on="d_date_sk")
    cs = cs.merge(
        customer[["c_customer_sk", "c_last_name", "c_first_name"]],
        left_on="cs_bill_customer_sk",
        right_on="c_customer_sk",
    )
    cs_cust = cs[["c_last_name", "c_first_name", "d_date"]].drop_duplicates()

    # Web customers with date
    ws = web_sales.merge(date_filtered, left_on="ws_sold_date_sk", right_on="d_date_sk")
    ws = ws.merge(
        customer[["c_customer_sk", "c_last_name", "c_first_name"]],
        left_on="ws_bill_customer_sk",
        right_on="c_customer_sk",
    )
    ws_cust = ws[["c_last_name", "c_first_name", "d_date"]].drop_duplicates()

    # EXCEPT: store customers minus catalog and web
    # Merge with indicator to find store-only
    ss_with_cs = ss_cust.merge(cs_cust, on=["c_last_name", "c_first_name", "d_date"], how="left", indicator=True)
    except_cs = ss_with_cs[ss_with_cs["_merge"] == "left_only"][["c_last_name", "c_first_name", "d_date"]]

    except_cs_with_ws = except_cs.merge(
        ws_cust, on=["c_last_name", "c_first_name", "d_date"], how="left", indicator=True
    )
    except_all = except_cs_with_ws[except_cs_with_ws["_merge"] == "left_only"]

    # Count
    import pandas as pd

    result = pd.DataFrame({"count": [len(except_all)]})

    return result


# =============================================================================
# Q69: Customer Demographics for Store-Only Customers
# =============================================================================


def q69_expression_impl(ctx: DataFrameContext) -> Any:
    """Q69: Customer demographics for store-only customers (Polars).

    Finds customers who purchased from store but NOT from web or catalog.
    Uses anti-join pattern for the NOT EXISTS logic.

    Tables: customer, customer_address, customer_demographics, store_sales,
            web_sales, catalog_sales, date_dim
    """

    params = get_parameters(69)
    year = params.get("year", 2001)
    month = params.get("month", 4)
    states = params.get("states", ["KY", "GA", "NM"])

    customer = ctx.get_table("customer")
    customer_address = ctx.get_table("customer_address")
    customer_demographics = ctx.get_table("customer_demographics")
    store_sales = ctx.get_table("store_sales")
    web_sales = ctx.get_table("web_sales")
    catalog_sales = ctx.get_table("catalog_sales")
    date_dim = ctx.get_table("date_dim")
    col = ctx.col
    lit = ctx.lit

    # Filter date_dim for the period
    date_filtered = date_dim.filter(
        (col("d_year") == lit(year)) & (col("d_moy") >= lit(month)) & (col("d_moy") <= lit(month + 2))
    )

    # Filter customer_address by states
    ca_filtered = customer_address.filter(col("ca_state").is_in(states))

    # Store customers during the period
    ss_customers = (
        store_sales.join(date_filtered, left_on="ss_sold_date_sk", right_on="d_date_sk")
        .select(["ss_customer_sk"])
        .unique()
        .rename({"ss_customer_sk": "customer_sk"})
    )

    # Web customers during the period
    ws_customers = (
        web_sales.join(date_filtered, left_on="ws_sold_date_sk", right_on="d_date_sk")
        .select(["ws_bill_customer_sk"])
        .unique()
        .rename({"ws_bill_customer_sk": "customer_sk"})
    )

    # Catalog customers during the period
    cs_customers = (
        catalog_sales.join(date_filtered, left_on="cs_sold_date_sk", right_on="d_date_sk")
        .select(["cs_ship_customer_sk"])
        .unique()
        .rename({"cs_ship_customer_sk": "customer_sk"})
    )

    # Store-only customers: in store but NOT in web AND NOT in catalog
    store_only = ss_customers.join(ws_customers, on="customer_sk", how="anti")
    store_only = store_only.join(cs_customers, on="customer_sk", how="anti")

    # Join with customer to get demographics
    result = (
        customer.join(ca_filtered, left_on="c_current_addr_sk", right_on="ca_address_sk")
        .join(customer_demographics, left_on="c_current_cdemo_sk", right_on="cd_demo_sk")
        .join(store_only, left_on="c_customer_sk", right_on="customer_sk", how="semi")
        .group_by(
            [
                "cd_gender",
                "cd_marital_status",
                "cd_education_status",
                "cd_purchase_estimate",
                "cd_credit_rating",
            ]
        )
        .agg(ctx.count().alias("cnt1"))
        .with_columns(
            [
                col("cnt1").alias("cnt2"),
                col("cnt1").alias("cnt3"),
            ]
        )
        .sort(
            [
                "cd_gender",
                "cd_marital_status",
                "cd_education_status",
                "cd_purchase_estimate",
                "cd_credit_rating",
            ]
        )
        .head(100)
    )

    return result


def q69_pandas_impl(ctx: DataFrameContext) -> Any:
    """Q69: Customer demographics for store-only customers (Pandas)."""
    params = get_parameters(69)
    year = params.get("year", 2001)
    month = params.get("month", 4)
    states = params.get("states", ["KY", "GA", "NM"])

    customer = ctx.get_table("customer")
    customer_address = ctx.get_table("customer_address")
    customer_demographics = ctx.get_table("customer_demographics")
    store_sales = ctx.get_table("store_sales")
    web_sales = ctx.get_table("web_sales")
    catalog_sales = ctx.get_table("catalog_sales")
    date_dim = ctx.get_table("date_dim")

    # Filter date_dim for the period
    date_filtered = date_dim[
        (date_dim["d_year"] == year) & (date_dim["d_moy"] >= month) & (date_dim["d_moy"] <= month + 2)
    ][["d_date_sk"]]

    # Filter customer_address by states
    ca_filtered = customer_address[customer_address["ca_state"].isin(states)]

    # Store customers during the period
    ss = store_sales.merge(date_filtered, left_on="ss_sold_date_sk", right_on="d_date_sk")
    ss_customers = ss[["ss_customer_sk"]].drop_duplicates()
    ss_customers = ss_customers.rename(columns={"ss_customer_sk": "customer_sk"})

    # Web customers during the period
    ws = web_sales.merge(date_filtered, left_on="ws_sold_date_sk", right_on="d_date_sk")
    ws_customers = ws[["ws_bill_customer_sk"]].drop_duplicates()
    ws_customers = ws_customers.rename(columns={"ws_bill_customer_sk": "customer_sk"})

    # Catalog customers during the period
    cs = catalog_sales.merge(date_filtered, left_on="cs_sold_date_sk", right_on="d_date_sk")
    cs_customers = cs[["cs_ship_customer_sk"]].drop_duplicates()
    cs_customers = cs_customers.rename(columns={"cs_ship_customer_sk": "customer_sk"})

    # Store-only: anti-join with web and catalog
    store_only = ss_customers.merge(ws_customers, on="customer_sk", how="left", indicator=True)
    store_only = store_only[store_only["_merge"] == "left_only"][["customer_sk"]]

    store_only = store_only.merge(cs_customers, on="customer_sk", how="left", indicator=True)
    store_only = store_only[store_only["_merge"] == "left_only"][["customer_sk"]]

    # Join customer with address and demographics
    cust = customer.merge(ca_filtered, left_on="c_current_addr_sk", right_on="ca_address_sk")
    cust = cust.merge(customer_demographics, left_on="c_current_cdemo_sk", right_on="cd_demo_sk")

    # Semi-join with store_only customers
    result = cust.merge(store_only, left_on="c_customer_sk", right_on="customer_sk")

    # Group by and aggregate
    result = result.groupby(
        [
            "cd_gender",
            "cd_marital_status",
            "cd_education_status",
            "cd_purchase_estimate",
            "cd_credit_rating",
        ],
        as_index=False,
    ).size()
    result = result.rename(columns={"size": "cnt1"})
    result["cnt2"] = result["cnt1"]
    result["cnt3"] = result["cnt1"]

    # Sort and limit
    result = result.sort_values(
        [
            "cd_gender",
            "cd_marital_status",
            "cd_education_status",
            "cd_purchase_estimate",
            "cd_credit_rating",
        ]
    ).head(100)

    return result


# =============================================================================
# Q40: Catalog Sales Before/After Returns Comparison
# =============================================================================


def q40_expression_impl(ctx: DataFrameContext) -> Any:
    """Q40: Catalog sales before/after with returns comparison (Polars).

    Compares catalog sales (minus refunds) before and after a specific date.
    Left joins with catalog_returns to handle refunded amounts.

    Tables: catalog_sales, catalog_returns, warehouse, item, date_dim
    """
    from datetime import datetime, timedelta

    params = get_parameters(40)
    year = params.get("year", 2000)
    # Midpoint date for before/after split
    sales_date_str = f"{year}-02-01"
    sales_date = datetime.strptime(sales_date_str, "%Y-%m-%d").date()

    catalog_sales = ctx.get_table("catalog_sales")
    catalog_returns = ctx.get_table("catalog_returns")
    warehouse = ctx.get_table("warehouse")
    item = ctx.get_table("item")
    date_dim = ctx.get_table("date_dim")
    col = ctx.col
    lit = ctx.lit

    # Filter dates: 30 days before and after sales_date
    start_date = sales_date - timedelta(days=30)
    end_date = sales_date + timedelta(days=30)

    date_filtered = date_dim.filter((col("d_date") >= lit(start_date)) & (col("d_date") <= lit(end_date)))

    # Filter items by price range
    item_filtered = item.filter((col("i_current_price") >= lit(0.99)) & (col("i_current_price") <= lit(1.49)))

    # Left join catalog_sales with catalog_returns
    cs_with_cr = catalog_sales.join(
        catalog_returns,
        left_on=["cs_order_number", "cs_item_sk"],
        right_on=["cr_order_number", "cr_item_sk"],
        how="left",
    )

    # Join with other dimensions
    result = (
        cs_with_cr.join(date_filtered, left_on="cs_sold_date_sk", right_on="d_date_sk")
        .join(warehouse, left_on="cs_warehouse_sk", right_on="w_warehouse_sk")
        .join(item_filtered, left_on="cs_item_sk", right_on="i_item_sk")
    )

    # Compute sales_before and sales_after
    result = result.with_columns(
        [
            ctx.when(col("d_date") < lit(sales_date))
            .then(col("cs_sales_price") - col("cr_refunded_cash").fill_null(0))
            .otherwise(lit(0))
            .alias("sales_before_val"),
            ctx.when(col("d_date") >= lit(sales_date))
            .then(col("cs_sales_price") - col("cr_refunded_cash").fill_null(0))
            .otherwise(lit(0))
            .alias("sales_after_val"),
        ]
    )

    # Aggregate by warehouse state and item
    result = (
        result.group_by(["w_state", "i_item_id"])
        .agg(
            [
                ctx.sum("sales_before_val").alias("sales_before"),
                ctx.sum("sales_after_val").alias("sales_after"),
            ]
        )
        .sort(["w_state", "i_item_id"])
        .head(100)
    )

    return result


def q40_pandas_impl(ctx: DataFrameContext) -> Any:
    """Q40: Catalog sales before/after with returns comparison (Pandas)."""
    from datetime import datetime, timedelta

    params = get_parameters(40)
    year = params.get("year", 2000)
    sales_date_str = f"{year}-02-01"
    sales_date = datetime.strptime(sales_date_str, "%Y-%m-%d").date()

    catalog_sales = ctx.get_table("catalog_sales")
    catalog_returns = ctx.get_table("catalog_returns")
    warehouse = ctx.get_table("warehouse")
    item = ctx.get_table("item")
    date_dim = ctx.get_table("date_dim")

    # Filter dates: 30 days before and after sales_date
    start_date = sales_date - timedelta(days=30)
    end_date = sales_date + timedelta(days=30)

    # Handle date comparison (might be datetime or date objects)
    import pandas as pd

    start_date_pd = pd.Timestamp(start_date)
    end_date_pd = pd.Timestamp(end_date)
    sales_date_pd = pd.Timestamp(sales_date)

    date_filtered = date_dim[(date_dim["d_date"] >= start_date_pd) & (date_dim["d_date"] <= end_date_pd)][
        ["d_date_sk", "d_date"]
    ]

    # Filter items by price range
    item_filtered = item[(item["i_current_price"] >= 0.99) & (item["i_current_price"] <= 1.49)][
        ["i_item_sk", "i_item_id"]
    ]

    # Left join catalog_sales with catalog_returns
    cs_with_cr = catalog_sales.merge(
        catalog_returns[["cr_order_number", "cr_item_sk", "cr_refunded_cash"]],
        left_on=["cs_order_number", "cs_item_sk"],
        right_on=["cr_order_number", "cr_item_sk"],
        how="left",
    )

    # Join with dimensions
    result = cs_with_cr.merge(date_filtered, left_on="cs_sold_date_sk", right_on="d_date_sk")
    result = result.merge(
        warehouse[["w_warehouse_sk", "w_state"]], left_on="cs_warehouse_sk", right_on="w_warehouse_sk"
    )
    result = result.merge(item_filtered, left_on="cs_item_sk", right_on="i_item_sk")

    # Fill null refunded amounts
    result["cr_refunded_cash"] = result["cr_refunded_cash"].fillna(0)

    # Compute sales_before and sales_after
    result["sales_before_val"] = result.apply(
        lambda row: row["cs_sales_price"] - row["cr_refunded_cash"] if row["d_date"] < sales_date_pd else 0,
        axis=1,
    )
    result["sales_after_val"] = result.apply(
        lambda row: row["cs_sales_price"] - row["cr_refunded_cash"] if row["d_date"] >= sales_date_pd else 0,
        axis=1,
    )

    # Aggregate by warehouse state and item
    result = result.groupby(["w_state", "i_item_id"], as_index=False).agg(
        {
            "sales_before_val": "sum",
            "sales_after_val": "sum",
        }
    )
    result = result.rename(
        columns={
            "sales_before_val": "sales_before",
            "sales_after_val": "sales_after",
        }
    )

    # Sort and limit
    result = result.sort_values(["w_state", "i_item_id"]).head(100)

    return result


# =============================================================================
# Q16: Catalog Orders from Multiple Warehouses Not Returned
# =============================================================================


def q16_expression_impl(ctx: DataFrameContext) -> Any:
    """Q16: Catalog orders shipped from multiple warehouses not returned (Polars).

    Finds catalog orders that were shipped from different warehouses
    AND have not been returned.

    Tables: catalog_sales, catalog_returns, date_dim, customer_address, call_center
    """
    from datetime import datetime, timedelta

    params = get_parameters(16)
    year = params.get("year", 2002)
    months = params.get("months", [3])
    state = params.get("state", "TN")
    county = params.get("county", "Williamson County")

    catalog_sales = ctx.get_table("catalog_sales")
    catalog_returns = ctx.get_table("catalog_returns")
    date_dim = ctx.get_table("date_dim")
    customer_address = ctx.get_table("customer_address")
    call_center = ctx.get_table("call_center")
    col = ctx.col
    lit = ctx.lit

    # Date filter: month start + 60 days
    month = months[0] if months else 3
    start_date = datetime(year, month, 1).date()
    end_date = start_date + timedelta(days=60)

    date_filtered = date_dim.filter((col("d_date") >= lit(start_date)) & (col("d_date") <= lit(end_date)))

    # Filter customer_address by state
    ca_filtered = customer_address.filter(col("ca_state") == lit(state))

    # Filter call_center by county
    cc_filtered = call_center.filter(col("cc_county") == lit(county))

    # Get orders that are NOT returned
    returned_orders = catalog_returns.select(["cr_order_number"]).unique()

    # Find orders shipped from multiple warehouses
    # First, identify order numbers that have different warehouses
    order_warehouses = catalog_sales.select(["cs_order_number", "cs_warehouse_sk"]).unique()

    # Self-join to find orders with different warehouses
    multi_warehouse_orders = (
        order_warehouses.join(
            order_warehouses.rename({"cs_warehouse_sk": "cs_warehouse_sk_2"}),
            on="cs_order_number",
        )
        .filter(col("cs_warehouse_sk") != col("cs_warehouse_sk_2"))
        .select(["cs_order_number"])
        .unique()
    )

    # Main query: catalog_sales joined with dimensions
    result = (
        catalog_sales.join(date_filtered, left_on="cs_ship_date_sk", right_on="d_date_sk")
        .join(ca_filtered, left_on="cs_ship_addr_sk", right_on="ca_address_sk")
        .join(cc_filtered, left_on="cs_call_center_sk", right_on="cc_call_center_sk")
        # Semi-join with multi-warehouse orders
        .join(multi_warehouse_orders, on="cs_order_number", how="semi")
        # Anti-join with returned orders
        .join(returned_orders, left_on="cs_order_number", right_on="cr_order_number", how="anti")
    )

    # Aggregate
    result = result.select(
        [
            col("cs_order_number").n_unique().alias("order count"),
            ctx.sum("cs_ext_ship_cost").alias("total shipping cost"),
            ctx.sum("cs_net_profit").alias("total net profit"),
        ]
    )

    return result


def q16_pandas_impl(ctx: DataFrameContext) -> Any:
    """Q16: Catalog orders shipped from multiple warehouses not returned (Pandas)."""
    from datetime import datetime, timedelta

    import pandas as pd

    params = get_parameters(16)
    year = params.get("year", 2002)
    months = params.get("months", [3])
    state = params.get("state", "TN")
    county = params.get("county", "Williamson County")

    catalog_sales = ctx.get_table("catalog_sales")
    catalog_returns = ctx.get_table("catalog_returns")
    date_dim = ctx.get_table("date_dim")
    customer_address = ctx.get_table("customer_address")
    call_center = ctx.get_table("call_center")

    # Date filter
    # Note: Use datetime.date for comparisons as d_date column may contain date objects
    month = months[0] if months else 3
    start_date = datetime(year, month, 1).date()
    end_date = start_date + timedelta(days=60)

    date_filtered = date_dim[(date_dim["d_date"] >= start_date) & (date_dim["d_date"] <= end_date)][["d_date_sk"]]

    # Filter dimensions
    ca_filtered = customer_address[customer_address["ca_state"] == state]
    cc_filtered = call_center[call_center["cc_county"] == county]

    # Get returned orders
    returned_orders = catalog_returns[["cr_order_number"]].drop_duplicates()

    # Find orders with multiple warehouses (use ctx.groupby_size for Dask compatibility)
    order_warehouses = catalog_sales[["cs_order_number", "cs_warehouse_sk"]].drop_duplicates()
    ow_count = ctx.groupby_size(order_warehouses, "cs_order_number", name="wh_count")
    multi_warehouse_orders = ow_count[ow_count["wh_count"] > 1][["cs_order_number"]]

    # Main join
    result = catalog_sales.merge(date_filtered, left_on="cs_ship_date_sk", right_on="d_date_sk")
    result = result.merge(ca_filtered[["ca_address_sk"]], left_on="cs_ship_addr_sk", right_on="ca_address_sk")
    result = result.merge(cc_filtered[["cc_call_center_sk"]], left_on="cs_call_center_sk", right_on="cc_call_center_sk")

    # Semi-join with multi-warehouse orders
    result = result.merge(multi_warehouse_orders, on="cs_order_number")

    # Anti-join with returned orders
    result = result.merge(
        returned_orders, left_on="cs_order_number", right_on="cr_order_number", how="left", indicator=True
    )
    result = result[result["_merge"] == "left_only"]

    # Aggregate
    order_count = result["cs_order_number"].nunique()
    total_shipping = result["cs_ext_ship_cost"].sum()
    total_profit = result["cs_net_profit"].sum()

    result = pd.DataFrame(
        {
            "order count": [order_count],
            "total shipping cost": [total_shipping],
            "total net profit": [total_profit],
        }
    )

    return result


# =============================================================================
# Q17: Store Sales/Returns + Catalog Sales Statistics
# =============================================================================


def q17_expression_impl(ctx: DataFrameContext) -> Any:
    """Q17: Store sales/returns + catalog sales analysis with statistics (Polars).

    Joins store sales with store returns (same customer, item, ticket) then
    with catalog sales (same customer, item). Computes quantity counts, averages,
    and standard deviations for each channel.

    Tables: store_sales, store_returns, catalog_sales, date_dim, store, item
    """

    params = get_parameters(17)
    year = params.get("year", 2001)
    quarter = params.get("quarter", 1)

    store_sales = ctx.get_table("store_sales")
    store_returns = ctx.get_table("store_returns")
    catalog_sales = ctx.get_table("catalog_sales")
    date_dim = ctx.get_table("date_dim")
    store = ctx.get_table("store")
    item = ctx.get_table("item")
    col = ctx.col
    lit = ctx.lit

    # Filter date_dim for store sales (Q1 of year)
    quarter_name = f"{year}Q{quarter}"
    d1 = date_dim.filter(col("d_quarter_name") == lit(quarter_name))

    # Filter date_dim for returns (Q1-Q3)
    quarter_names_ret = [f"{year}Q{q}" for q in range(quarter, min(quarter + 3, 5))]
    d2 = date_dim.filter(col("d_quarter_name").is_in(quarter_names_ret))

    # Filter date_dim for catalog sales (years)
    d3 = date_dim.filter(col("d_year").is_in([year, year + 1, year + 2]))

    # Join store_sales with date, item, store
    # Select only needed columns from date_dim to avoid column conflicts in later joins
    ss_joined = (
        store_sales.join(d1.select("d_date_sk"), left_on="ss_sold_date_sk", right_on="d_date_sk")
        .join(item, left_on="ss_item_sk", right_on="i_item_sk")
        .join(store, left_on="ss_store_sk", right_on="s_store_sk")
    )

    # Join with store_returns (same customer, item, ticket)
    # Select only needed columns from date_dim to avoid column conflicts
    sr_joined = store_returns.join(d2.select("d_date_sk"), left_on="sr_returned_date_sk", right_on="d_date_sk")

    ss_sr = ss_joined.join(
        sr_joined,
        left_on=["ss_customer_sk", "ss_item_sk", "ss_ticket_number"],
        right_on=["sr_customer_sk", "sr_item_sk", "sr_ticket_number"],
    )

    # Join with catalog_sales (same customer, item from returns)
    # Note: sr_customer_sk and sr_item_sk were dropped in previous join,
    # use preserved left-side keys ss_customer_sk and ss_item_sk
    # Select only needed columns from date_dim to avoid column conflicts
    cs_joined = catalog_sales.join(d3.select("d_date_sk"), left_on="cs_sold_date_sk", right_on="d_date_sk")

    result = ss_sr.join(
        cs_joined,
        left_on=["ss_customer_sk", "ss_item_sk"],
        right_on=["cs_bill_customer_sk", "cs_item_sk"],
    )

    # Aggregate by item and state
    result = (
        result.group_by(["i_item_id", "i_item_desc", "s_state"])
        .agg(
            [
                ctx.count("ss_quantity").alias("store_sales_quantitycount"),
                col("ss_quantity").mean().alias("store_sales_quantityave"),
                col("ss_quantity").std().alias("store_sales_quantitystdev"),
                ctx.count("sr_return_quantity").alias("store_returns_quantitycount"),
                col("sr_return_quantity").mean().alias("store_returns_quantityave"),
                col("sr_return_quantity").std().alias("store_returns_quantitystdev"),
                ctx.count("cs_quantity").alias("catalog_sales_quantitycount"),
                col("cs_quantity").mean().alias("catalog_sales_quantityave"),
                col("cs_quantity").std().alias("catalog_sales_quantitystdev"),
            ]
        )
        .with_columns(
            [
                (col("store_sales_quantitystdev") / col("store_sales_quantityave")).alias("store_sales_quantitycov"),
                (col("store_returns_quantitystdev") / col("store_returns_quantityave")).alias(
                    "store_returns_quantitycov"
                ),
                (col("catalog_sales_quantitystdev") / col("catalog_sales_quantityave")).alias(
                    "catalog_sales_quantitycov"
                ),
            ]
        )
        .sort(["i_item_id", "i_item_desc", "s_state"])
        .head(100)
    )

    return result


def q17_pandas_impl(ctx: DataFrameContext) -> Any:
    """Q17: Store sales/returns + catalog sales analysis with statistics (Pandas)."""
    params = get_parameters(17)
    year = params.get("year", 2001)
    quarter = params.get("quarter", 1)

    store_sales = ctx.get_table("store_sales")
    store_returns = ctx.get_table("store_returns")
    catalog_sales = ctx.get_table("catalog_sales")
    date_dim = ctx.get_table("date_dim")
    store = ctx.get_table("store")
    item = ctx.get_table("item")

    # Filter date_dim for store sales (Q1 of year)
    quarter_name = f"{year}Q{quarter}"
    d1 = date_dim[date_dim["d_quarter_name"] == quarter_name][["d_date_sk"]]

    # Filter date_dim for returns (Q1-Q3)
    quarter_names_ret = [f"{year}Q{q}" for q in range(quarter, min(quarter + 3, 5))]
    d2 = date_dim[date_dim["d_quarter_name"].isin(quarter_names_ret)][["d_date_sk"]]

    # Filter date_dim for catalog sales (years)
    d3 = date_dim[date_dim["d_year"].isin([year, year + 1, year + 2])][["d_date_sk"]]

    # Join store_sales with date, item, store
    ss_joined = store_sales.merge(d1, left_on="ss_sold_date_sk", right_on="d_date_sk")
    ss_joined = ss_joined.merge(
        item[["i_item_sk", "i_item_id", "i_item_desc"]], left_on="ss_item_sk", right_on="i_item_sk"
    )
    ss_joined = ss_joined.merge(store[["s_store_sk", "s_state"]], left_on="ss_store_sk", right_on="s_store_sk")

    # Join with store_returns
    sr_joined = store_returns.merge(d2, left_on="sr_returned_date_sk", right_on="d_date_sk")

    ss_sr = ss_joined.merge(
        sr_joined,
        left_on=["ss_customer_sk", "ss_item_sk", "ss_ticket_number"],
        right_on=["sr_customer_sk", "sr_item_sk", "sr_ticket_number"],
    )

    # Join with catalog_sales
    cs_joined = catalog_sales.merge(d3, left_on="cs_sold_date_sk", right_on="d_date_sk")

    result = ss_sr.merge(
        cs_joined,
        left_on=["sr_customer_sk", "sr_item_sk"],
        right_on=["cs_bill_customer_sk", "cs_item_sk"],
    )

    # Aggregate by item and state
    result = result.groupby(["i_item_id", "i_item_desc", "s_state"], as_index=False).agg(
        {
            "ss_quantity": ["count", "mean", "std"],
            "sr_return_quantity": ["count", "mean", "std"],
            "cs_quantity": ["count", "mean", "std"],
        }
    )

    # Flatten column names
    result.columns = [
        "i_item_id",
        "i_item_desc",
        "s_state",
        "store_sales_quantitycount",
        "store_sales_quantityave",
        "store_sales_quantitystdev",
        "store_returns_quantitycount",
        "store_returns_quantityave",
        "store_returns_quantitystdev",
        "catalog_sales_quantitycount",
        "catalog_sales_quantityave",
        "catalog_sales_quantitystdev",
    ]

    # Add coefficient of variation columns
    result["store_sales_quantitycov"] = result["store_sales_quantitystdev"] / result["store_sales_quantityave"]
    result["store_returns_quantitycov"] = result["store_returns_quantitystdev"] / result["store_returns_quantityave"]
    result["catalog_sales_quantitycov"] = result["catalog_sales_quantitystdev"] / result["catalog_sales_quantityave"]

    # Sort and limit
    result = result.sort_values(["i_item_id", "i_item_desc", "s_state"]).head(100)

    return result


# =============================================================================
# Q18: Catalog Sales with Demographics and ROLLUP
# =============================================================================


def q18_expression_impl(ctx: DataFrameContext) -> Any:
    """Q18: Catalog sales with customer demographics and ROLLUP (Polars).

    Joins catalog_sales with customer demographics and address,
    aggregates with ROLLUP on item, country, state, county.

    Tables: catalog_sales, customer_demographics, customer, customer_address,
            date_dim, item
    """

    from .rollup_helper import expand_rollup_expression

    params = get_parameters(18)
    year = params.get("year", 2001)
    states = params.get("states", ["TX", "OH", "TX", "OR", "NM", "KY", "VA"])
    cd_gender = params.get("cd_gender", "M")
    cd_education_status = params.get("cd_education_status", "College")

    catalog_sales = ctx.get_table("catalog_sales")
    customer_demographics = ctx.get_table("customer_demographics")
    customer = ctx.get_table("customer")
    customer_address = ctx.get_table("customer_address")
    date_dim = ctx.get_table("date_dim")
    item = ctx.get_table("item")
    col = ctx.col
    lit = ctx.lit

    # Filter date_dim
    date_filtered = date_dim.filter(col("d_year") == lit(year))

    # Filter customer_demographics for bill cdemo (cd1)
    cd1 = customer_demographics.filter(
        (col("cd_gender") == lit(cd_gender)) & (col("cd_education_status") == lit(cd_education_status))
    )

    # Filter customer_address
    ca_filtered = customer_address.filter(col("ca_state").is_in(states))

    # Join catalog_sales with dimensions
    cs_joined = (
        catalog_sales.join(date_filtered, left_on="cs_sold_date_sk", right_on="d_date_sk")
        .join(item, left_on="cs_item_sk", right_on="i_item_sk")
        .join(cd1, left_on="cs_bill_cdemo_sk", right_on="cd_demo_sk")
        .join(customer, left_on="cs_bill_customer_sk", right_on="c_customer_sk")
        .join(ca_filtered, left_on="c_current_addr_sk", right_on="ca_address_sk")
        .join(
            customer_demographics.rename({"cd_demo_sk": "cd2_demo_sk", "cd_dep_count": "cd2_dep_count"}),
            left_on="c_current_cdemo_sk",
            right_on="cd2_demo_sk",
        )
    )

    # Aggregation expressions for ROLLUP
    agg_exprs = [
        col("cs_quantity").cast_float64().mean().alias("agg1"),
        col("cs_list_price").cast_float64().mean().alias("agg2"),
        col("cs_coupon_amt").cast_float64().mean().alias("agg3"),
        col("cs_sales_price").cast_float64().mean().alias("agg4"),
        col("cs_net_profit").cast_float64().mean().alias("agg5"),
        col("c_birth_year").cast_float64().mean().alias("agg6"),
        col("cd_dep_count").cast_float64().mean().alias("agg7"),
    ]

    # Expand ROLLUP (i_item_id, ca_country, ca_state, ca_county)
    group_cols = ["i_item_id", "ca_country", "ca_state", "ca_county"]
    result = expand_rollup_expression(cs_joined, group_cols, agg_exprs, ctx)

    # Sort and limit
    result = result.sort(["ca_country", "ca_state", "ca_county", "i_item_id"], nulls_last=True).head(100)

    return result


def q18_pandas_impl(ctx: DataFrameContext) -> Any:
    """Q18: Catalog sales with customer demographics and ROLLUP (Pandas)."""
    from .rollup_helper import expand_rollup_pandas

    params = get_parameters(18)
    year = params.get("year", 2001)
    states = params.get("states", ["TX", "OH", "TX", "OR", "NM", "KY", "VA"])
    cd_gender = params.get("cd_gender", "M")
    cd_education_status = params.get("cd_education_status", "College")

    catalog_sales = ctx.get_table("catalog_sales")
    customer_demographics = ctx.get_table("customer_demographics")
    customer = ctx.get_table("customer")
    customer_address = ctx.get_table("customer_address")
    date_dim = ctx.get_table("date_dim")
    item = ctx.get_table("item")

    # Filter date_dim
    date_filtered = date_dim[date_dim["d_year"] == year][["d_date_sk"]]

    # Filter customer_demographics for bill cdemo (cd1)
    cd1 = customer_demographics[
        (customer_demographics["cd_gender"] == cd_gender)
        & (customer_demographics["cd_education_status"] == cd_education_status)
    ][["cd_demo_sk", "cd_dep_count"]]

    # Filter customer_address
    ca_filtered = customer_address[customer_address["ca_state"].isin(states)]

    # Join catalog_sales with dimensions
    cs_joined = catalog_sales.merge(date_filtered, left_on="cs_sold_date_sk", right_on="d_date_sk")
    cs_joined = cs_joined.merge(item[["i_item_sk", "i_item_id"]], left_on="cs_item_sk", right_on="i_item_sk")
    cs_joined = cs_joined.merge(cd1, left_on="cs_bill_cdemo_sk", right_on="cd_demo_sk")
    cs_joined = cs_joined.merge(
        customer[["c_customer_sk", "c_current_cdemo_sk", "c_current_addr_sk", "c_birth_year"]],
        left_on="cs_bill_customer_sk",
        right_on="c_customer_sk",
    )
    cs_joined = cs_joined.merge(
        ca_filtered[["ca_address_sk", "ca_country", "ca_state", "ca_county"]],
        left_on="c_current_addr_sk",
        right_on="ca_address_sk",
    )
    cs_joined = cs_joined.merge(
        customer_demographics[["cd_demo_sk", "cd_dep_count"]].rename(
            columns={"cd_demo_sk": "cd2_demo_sk", "cd_dep_count": "cd2_dep_count"}
        ),
        left_on="c_current_cdemo_sk",
        right_on="cd2_demo_sk",
    )

    # Expand ROLLUP
    agg_dict = {
        "agg1": ("cs_quantity", "mean"),
        "agg2": ("cs_list_price", "mean"),
        "agg3": ("cs_coupon_amt", "mean"),
        "agg4": ("cs_sales_price", "mean"),
        "agg5": ("cs_net_profit", "mean"),
        "agg6": ("c_birth_year", "mean"),
        "agg7": ("cd_dep_count", "mean"),
    }

    result = expand_rollup_pandas(cs_joined, ["i_item_id", "ca_country", "ca_state", "ca_county"], agg_dict, ctx)

    # Sort and limit
    result = result.sort_values(
        ["ca_country", "ca_state", "ca_county", "i_item_id"],
        na_position="last",
    ).head(100)

    return result


# =============================================================================
# Q29: Store Sales/Returns + Catalog Sales Aggregation
# =============================================================================


def q29_expression_impl(ctx: DataFrameContext) -> Any:
    """Q29: Store sales/returns + catalog sales aggregation (Polars).

    Similar to Q17: joins store sales with returns and catalog sales,
    aggregates quantities by item and store.

    Tables: store_sales, store_returns, catalog_sales, date_dim, store, item
    """

    params = get_parameters(29)
    year = params.get("year", 1999)
    months = params.get("months", [1, 2, 3])

    store_sales = ctx.get_table("store_sales")
    store_returns = ctx.get_table("store_returns")
    catalog_sales = ctx.get_table("catalog_sales")
    date_dim = ctx.get_table("date_dim")
    store = ctx.get_table("store")
    item = ctx.get_table("item")
    col = ctx.col
    lit = ctx.lit

    # Filter date_dim for store sales
    month = months[0] if months else 1
    d1 = date_dim.filter((col("d_moy") == lit(month)) & (col("d_year") == lit(year)))

    # Filter date_dim for returns (range of months)
    d2 = date_dim.filter((col("d_moy") >= lit(month)) & (col("d_moy") <= lit(month + 3)) & (col("d_year") == lit(year)))

    # Filter date_dim for catalog sales (years)
    d3 = date_dim.filter(col("d_year").is_in([year, year + 1, year + 2]))

    # Join store_sales with date, item, store
    # Select only needed columns from date_dim to avoid column conflicts in later joins
    ss_joined = (
        store_sales.join(d1.select("d_date_sk"), left_on="ss_sold_date_sk", right_on="d_date_sk")
        .join(item, left_on="ss_item_sk", right_on="i_item_sk")
        .join(store, left_on="ss_store_sk", right_on="s_store_sk")
    )

    # Join with store_returns
    # Select only needed columns from date_dim to avoid column conflicts
    sr_joined = store_returns.join(d2.select("d_date_sk"), left_on="sr_returned_date_sk", right_on="d_date_sk")

    ss_sr = ss_joined.join(
        sr_joined,
        left_on=["ss_customer_sk", "ss_item_sk", "ss_ticket_number"],
        right_on=["sr_customer_sk", "sr_item_sk", "sr_ticket_number"],
    )

    # Join with catalog_sales
    # Note: sr_customer_sk and sr_item_sk were dropped in previous join,
    # use preserved left-side keys ss_customer_sk and ss_item_sk
    # Select only needed columns from date_dim to avoid column conflicts
    cs_joined = catalog_sales.join(d3.select("d_date_sk"), left_on="cs_sold_date_sk", right_on="d_date_sk")

    result = ss_sr.join(
        cs_joined,
        left_on=["ss_customer_sk", "ss_item_sk"],
        right_on=["cs_bill_customer_sk", "cs_item_sk"],
    )

    # Aggregate by item and store
    result = (
        result.group_by(["i_item_id", "i_item_desc", "s_store_id", "s_store_name"])
        .agg(
            [
                ctx.sum("ss_quantity").alias("store_sales_quantity"),
                ctx.sum("sr_return_quantity").alias("store_returns_quantity"),
                ctx.sum("cs_quantity").alias("catalog_sales_quantity"),
            ]
        )
        .sort(["i_item_id", "i_item_desc", "s_store_id", "s_store_name"])
        .head(100)
    )

    return result


def q29_pandas_impl(ctx: DataFrameContext) -> Any:
    """Q29: Store sales/returns + catalog sales aggregation (Pandas)."""
    params = get_parameters(29)
    year = params.get("year", 1999)
    months = params.get("months", [1, 2, 3])

    store_sales = ctx.get_table("store_sales")
    store_returns = ctx.get_table("store_returns")
    catalog_sales = ctx.get_table("catalog_sales")
    date_dim = ctx.get_table("date_dim")
    store = ctx.get_table("store")
    item = ctx.get_table("item")

    # Filter date_dim for store sales
    month = months[0] if months else 1
    d1 = date_dim[(date_dim["d_moy"] == month) & (date_dim["d_year"] == year)][["d_date_sk"]]

    # Filter date_dim for returns
    d2 = date_dim[(date_dim["d_moy"] >= month) & (date_dim["d_moy"] <= month + 3) & (date_dim["d_year"] == year)][
        ["d_date_sk"]
    ]

    # Filter date_dim for catalog sales
    d3 = date_dim[date_dim["d_year"].isin([year, year + 1, year + 2])][["d_date_sk"]]

    # Join store_sales with date, item, store
    ss_joined = store_sales.merge(d1, left_on="ss_sold_date_sk", right_on="d_date_sk")
    ss_joined = ss_joined.merge(
        item[["i_item_sk", "i_item_id", "i_item_desc"]], left_on="ss_item_sk", right_on="i_item_sk"
    )
    ss_joined = ss_joined.merge(
        store[["s_store_sk", "s_store_id", "s_store_name"]], left_on="ss_store_sk", right_on="s_store_sk"
    )

    # Join with store_returns
    sr_joined = store_returns.merge(d2, left_on="sr_returned_date_sk", right_on="d_date_sk")

    ss_sr = ss_joined.merge(
        sr_joined,
        left_on=["ss_customer_sk", "ss_item_sk", "ss_ticket_number"],
        right_on=["sr_customer_sk", "sr_item_sk", "sr_ticket_number"],
    )

    # Join with catalog_sales
    cs_joined = catalog_sales.merge(d3, left_on="cs_sold_date_sk", right_on="d_date_sk")

    result = ss_sr.merge(
        cs_joined,
        left_on=["sr_customer_sk", "sr_item_sk"],
        right_on=["cs_bill_customer_sk", "cs_item_sk"],
    )

    # Aggregate by item and store
    result = result.groupby(["i_item_id", "i_item_desc", "s_store_id", "s_store_name"], as_index=False).agg(
        {
            "ss_quantity": "sum",
            "sr_return_quantity": "sum",
            "cs_quantity": "sum",
        }
    )
    result = result.rename(
        columns={
            "ss_quantity": "store_sales_quantity",
            "sr_return_quantity": "store_returns_quantity",
            "cs_quantity": "catalog_sales_quantity",
        }
    )

    # Sort and limit
    result = result.sort_values(["i_item_id", "i_item_desc", "s_store_id", "s_store_name"]).head(100)

    return result


# =============================================================================
# Q27: Store Sales with Demographics and ROLLUP
# =============================================================================


def q27_expression_impl(ctx: DataFrameContext) -> Any:
    """Q27: Store sales with customer demographics and ROLLUP (Polars).

    Joins store_sales with customer demographics, date_dim, store, and item.
    Aggregates with ROLLUP on item_id and state.

    Tables: store_sales, customer_demographics, date_dim, store, item
    """

    from .rollup_helper import expand_rollup_expression

    params = get_parameters(27)
    year = params.get("year", 2002)
    gender = params.get("gender", "M")
    marital_status = params.get("marital_status", "S")
    education = params.get("education", "College")
    states = params.get("states", ["TN", "TX", "FL", "CA", "NY", "PA"])

    store_sales = ctx.get_table("store_sales")
    customer_demographics = ctx.get_table("customer_demographics")
    date_dim = ctx.get_table("date_dim")
    store = ctx.get_table("store")
    item = ctx.get_table("item")
    col = ctx.col
    lit = ctx.lit

    # Filter date_dim
    date_filtered = date_dim.filter(col("d_year") == lit(year))

    # Filter customer_demographics
    cd_filtered = customer_demographics.filter(
        (col("cd_gender") == lit(gender))
        & (col("cd_marital_status") == lit(marital_status))
        & (col("cd_education_status") == lit(education))
    )

    # Filter store by states
    store_filtered = store.filter(col("s_state").is_in(states))

    # Join all tables
    ss_joined = (
        store_sales.join(date_filtered, left_on="ss_sold_date_sk", right_on="d_date_sk")
        .join(item, left_on="ss_item_sk", right_on="i_item_sk")
        .join(store_filtered, left_on="ss_store_sk", right_on="s_store_sk")
        .join(cd_filtered, left_on="ss_cdemo_sk", right_on="cd_demo_sk")
    )

    # Aggregation expressions for ROLLUP
    agg_exprs = [
        col("ss_quantity").mean().alias("agg1"),
        col("ss_list_price").mean().alias("agg2"),
        col("ss_coupon_amt").mean().alias("agg3"),
        col("ss_sales_price").mean().alias("agg4"),
    ]

    # Expand ROLLUP (i_item_id, s_state)
    group_cols = ["i_item_id", "s_state"]
    result = expand_rollup_expression(ss_joined, group_cols, agg_exprs, ctx)

    # Sort and limit
    result = result.sort(["i_item_id", "s_state"], nulls_last=True).head(100)

    return result


def q27_pandas_impl(ctx: DataFrameContext) -> Any:
    """Q27: Store sales with customer demographics and ROLLUP (Pandas)."""
    from .rollup_helper import expand_rollup_pandas

    params = get_parameters(27)
    year = params.get("year", 2002)
    gender = params.get("gender", "M")
    marital_status = params.get("marital_status", "S")
    education = params.get("education", "College")
    states = params.get("states", ["TN", "TX", "FL", "CA", "NY", "PA"])

    store_sales = ctx.get_table("store_sales")
    customer_demographics = ctx.get_table("customer_demographics")
    date_dim = ctx.get_table("date_dim")
    store = ctx.get_table("store")
    item = ctx.get_table("item")

    # Filter date_dim
    date_filtered = date_dim[date_dim["d_year"] == year][["d_date_sk"]]

    # Filter customer_demographics
    cd_filtered = customer_demographics[
        (customer_demographics["cd_gender"] == gender)
        & (customer_demographics["cd_marital_status"] == marital_status)
        & (customer_demographics["cd_education_status"] == education)
    ][["cd_demo_sk"]]

    # Filter store by states
    store_filtered = store[store["s_state"].isin(states)][["s_store_sk", "s_state"]]

    # Join all tables
    ss_joined = store_sales.merge(date_filtered, left_on="ss_sold_date_sk", right_on="d_date_sk")
    ss_joined = ss_joined.merge(item[["i_item_sk", "i_item_id"]], left_on="ss_item_sk", right_on="i_item_sk")
    ss_joined = ss_joined.merge(store_filtered, left_on="ss_store_sk", right_on="s_store_sk")
    ss_joined = ss_joined.merge(cd_filtered, left_on="ss_cdemo_sk", right_on="cd_demo_sk")

    # Expand ROLLUP
    agg_dict = {
        "agg1": ("ss_quantity", "mean"),
        "agg2": ("ss_list_price", "mean"),
        "agg3": ("ss_coupon_amt", "mean"),
        "agg4": ("ss_sales_price", "mean"),
    }

    result = expand_rollup_pandas(ss_joined, ["i_item_id", "s_state"], agg_dict, ctx)

    # Sort and limit
    result = result.sort_values(["i_item_id", "s_state"], na_position="last").head(100)

    return result


# =============================================================================
# Q93: Store Sales with Returns - Actual Sales Calculation
# =============================================================================


def q93_expression_impl(ctx: DataFrameContext) -> Any:
    """Q93: Store sales with returns actual sales calculation (Polars).

    Left joins store_sales with store_returns to compute actual sales
    (sales - returns) filtered by return reason.

    Tables: store_sales, store_returns, reason
    """

    params = get_parameters(93)
    reason = params.get("reason", "reason 28")

    store_sales = ctx.get_table("store_sales")
    store_returns = ctx.get_table("store_returns")
    reason_table = ctx.get_table("reason")
    col = ctx.col
    lit = ctx.lit

    # Filter reason
    reason_filtered = reason_table.filter(col("r_reason_desc") == lit(reason))

    # Left join store_sales with store_returns
    ss_with_sr = store_sales.join(
        store_returns,
        left_on=["ss_item_sk", "ss_ticket_number"],
        right_on=["sr_item_sk", "sr_ticket_number"],
        how="left",
    )

    # Join with reason
    ss_with_reason = ss_with_sr.join(reason_filtered, left_on="sr_reason_sk", right_on="r_reason_sk")

    # Compute actual sales
    result = ss_with_reason.with_columns(
        ctx.when(col("sr_return_quantity").is_not_null())
        .then((col("ss_quantity") - col("sr_return_quantity")) * col("ss_sales_price"))
        .otherwise(col("ss_quantity") * col("ss_sales_price"))
        .alias("act_sales")
    )

    # Aggregate by customer
    result = (
        result.group_by("ss_customer_sk")
        .agg(ctx.sum("act_sales").alias("sumsales"))
        .sort(["sumsales", "ss_customer_sk"])
        .head(100)
    )

    return result


def q93_pandas_impl(ctx: DataFrameContext) -> Any:
    """Q93: Store sales with returns actual sales calculation (Pandas)."""
    params = get_parameters(93)
    reason = params.get("reason", "reason 28")

    store_sales = ctx.get_table("store_sales")
    store_returns = ctx.get_table("store_returns")
    reason_table = ctx.get_table("reason")

    # Filter reason
    reason_filtered = reason_table[reason_table["r_reason_desc"] == reason]

    # Left join store_sales with store_returns
    ss_with_sr = store_sales.merge(
        store_returns[["sr_item_sk", "sr_ticket_number", "sr_return_quantity", "sr_reason_sk"]],
        left_on=["ss_item_sk", "ss_ticket_number"],
        right_on=["sr_item_sk", "sr_ticket_number"],
        how="left",
    )

    # Join with reason
    ss_with_reason = ss_with_sr.merge(reason_filtered[["r_reason_sk"]], left_on="sr_reason_sk", right_on="r_reason_sk")

    # Compute actual sales
    ss_with_reason["act_sales"] = ss_with_reason.apply(
        lambda row: (row["ss_quantity"] - row["sr_return_quantity"]) * row["ss_sales_price"]
        if row["sr_return_quantity"] is not None
        and not (
            isinstance(row["sr_return_quantity"], float) and row["sr_return_quantity"] != row["sr_return_quantity"]
        )
        else row["ss_quantity"] * row["ss_sales_price"],
        axis=1,
    )

    # Aggregate by customer
    result = ss_with_reason.groupby("ss_customer_sk", as_index=False).agg({"act_sales": "sum"})
    result = result.rename(columns={"act_sales": "sumsales"})

    # Sort and limit
    result = result.sort_values(["sumsales", "ss_customer_sk"]).head(100)

    return result


# =============================================================================
# Q94: Web Orders from Multiple Warehouses Not Returned
# =============================================================================


def q94_expression_impl(ctx: DataFrameContext) -> Any:
    """Q94: Web orders shipped from multiple warehouses not returned (Polars).

    Similar to Q16 but for web channel. Finds web orders shipped from
    different warehouses that were not returned.

    Tables: web_sales, web_returns, date_dim, customer_address, web_site
    """
    from datetime import datetime, timedelta

    params = get_parameters(94)
    year = params.get("year", 1999)
    month = params.get("month", 2)
    states = params.get("states", ["TX", "OR", "AZ"])

    web_sales = ctx.get_table("web_sales")
    web_returns = ctx.get_table("web_returns")
    date_dim = ctx.get_table("date_dim")
    customer_address = ctx.get_table("customer_address")
    web_site = ctx.get_table("web_site")
    col = ctx.col
    lit = ctx.lit

    # Date filter
    start_date = datetime(year, month, 1).date()
    end_date = start_date + timedelta(days=60)

    date_filtered = date_dim.filter((col("d_date") >= lit(start_date)) & (col("d_date") <= lit(end_date)))

    # Filter customer_address by states
    ca_filtered = customer_address.filter(col("ca_state").is_in(states))

    # Filter web_site by company
    ws_filtered = web_site.filter(col("web_company_name") == lit("pri"))

    # Get orders that are NOT returned
    returned_orders = web_returns.select(["wr_order_number"]).unique()

    # Find orders shipped from multiple warehouses
    order_warehouses = web_sales.select(["ws_order_number", "ws_warehouse_sk"]).unique()

    # Self-join to find orders with different warehouses
    multi_warehouse_orders = (
        order_warehouses.join(
            order_warehouses.rename({"ws_warehouse_sk": "ws_warehouse_sk_2"}),
            on="ws_order_number",
        )
        .filter(col("ws_warehouse_sk") != col("ws_warehouse_sk_2"))
        .select(["ws_order_number"])
        .unique()
    )

    # Main query
    result = (
        web_sales.join(date_filtered, left_on="ws_ship_date_sk", right_on="d_date_sk")
        .join(ca_filtered, left_on="ws_ship_addr_sk", right_on="ca_address_sk")
        .join(ws_filtered, left_on="ws_web_site_sk", right_on="web_site_sk")
        .join(multi_warehouse_orders, on="ws_order_number", how="semi")
        .join(returned_orders, left_on="ws_order_number", right_on="wr_order_number", how="anti")
    )

    # Aggregate
    result = result.select(
        [
            col("ws_order_number").n_unique().alias("order count"),
            ctx.sum("ws_ext_ship_cost").alias("total shipping cost"),
            ctx.sum("ws_net_profit").alias("total net profit"),
        ]
    )

    return result


def q94_pandas_impl(ctx: DataFrameContext) -> Any:
    """Q94: Web orders shipped from multiple warehouses not returned (Pandas)."""
    from datetime import datetime, timedelta

    import pandas as pd

    params = get_parameters(94)
    year = params.get("year", 1999)
    month = params.get("month", 2)
    states = params.get("states", ["TX", "OR", "AZ"])

    web_sales = ctx.get_table("web_sales")
    web_returns = ctx.get_table("web_returns")
    date_dim = ctx.get_table("date_dim")
    customer_address = ctx.get_table("customer_address")
    web_site = ctx.get_table("web_site")

    # Date filter
    start_date = pd.Timestamp(datetime(year, month, 1))
    end_date = start_date + timedelta(days=60)

    date_filtered = date_dim[(date_dim["d_date"] >= start_date) & (date_dim["d_date"] <= end_date)][["d_date_sk"]]

    # Filter dimensions
    ca_filtered = customer_address[customer_address["ca_state"].isin(states)]
    ws_filtered = web_site[web_site["web_company_name"] == "pri"]

    # Get returned orders
    returned_orders = web_returns[["wr_order_number"]].drop_duplicates()

    # Find orders with multiple warehouses (use ctx.groupby_size for Dask compatibility)
    order_warehouses = web_sales[["ws_order_number", "ws_warehouse_sk"]].drop_duplicates()
    ow_count = ctx.groupby_size(order_warehouses, "ws_order_number", name="wh_count")
    multi_warehouse_orders = ow_count[ow_count["wh_count"] > 1][["ws_order_number"]]

    # Main join
    result = web_sales.merge(date_filtered, left_on="ws_ship_date_sk", right_on="d_date_sk")
    result = result.merge(ca_filtered[["ca_address_sk"]], left_on="ws_ship_addr_sk", right_on="ca_address_sk")
    result = result.merge(ws_filtered[["web_site_sk"]], left_on="ws_web_site_sk", right_on="web_site_sk")

    # Semi-join with multi-warehouse orders
    result = result.merge(multi_warehouse_orders, on="ws_order_number")

    # Anti-join with returned orders
    result = result.merge(
        returned_orders, left_on="ws_order_number", right_on="wr_order_number", how="left", indicator=True
    )
    result = result[result["_merge"] == "left_only"]

    # Aggregate
    order_count = result["ws_order_number"].nunique()
    total_shipping = result["ws_ext_ship_cost"].sum()
    total_profit = result["ws_net_profit"].sum()

    result = pd.DataFrame(
        {
            "order count": [order_count],
            "total shipping cost": [total_shipping],
            "total net profit": [total_profit],
        }
    )

    return result


# =============================================================================
# Q80: Three-Channel Sales-Returns with ROLLUP
# =============================================================================


def q80_expression_impl(ctx: DataFrameContext) -> Any:
    """Q80: Three-channel sales-returns with ROLLUP aggregation (Polars).

    Each channel: join sales with returns, aggregate by store/catalog_page/web_site.
    Union all channels, then GROUP BY ROLLUP(channel, id).

    Tables: store_sales, store_returns, date_dim, store, item, promotion,
            catalog_sales, catalog_returns, catalog_page,
            web_sales, web_returns, web_site
    """
    from datetime import datetime, timedelta

    from .rollup_helper import expand_rollup_expression

    params = get_parameters(80)
    sales_date = params.get("sales_date", "2000-08-23")

    store_sales = ctx.get_table("store_sales")
    store_returns = ctx.get_table("store_returns")
    catalog_sales = ctx.get_table("catalog_sales")
    catalog_returns = ctx.get_table("catalog_returns")
    web_sales = ctx.get_table("web_sales")
    web_returns = ctx.get_table("web_returns")
    date_dim = ctx.get_table("date_dim")
    store = ctx.get_table("store")
    catalog_page = ctx.get_table("catalog_page")
    web_site = ctx.get_table("web_site")
    item = ctx.get_table("item")
    promotion = ctx.get_table("promotion")
    col = ctx.col
    lit = ctx.lit

    # Parse the sales_date
    start_date = datetime.strptime(sales_date, "%Y-%m-%d").date() if isinstance(sales_date, str) else sales_date
    end_date = start_date + timedelta(days=30)

    # Filter date_dim
    date_filtered = date_dim.filter((col("d_date") >= lit(start_date)) & (col("d_date") <= lit(end_date)))

    # Filter item and promotion
    item_filtered = item.filter(col("i_current_price") > lit(50))
    promo_filtered = promotion.filter(col("p_channel_tv") == lit("N"))

    # Store sales-returns
    ssr = (
        store_sales.join(
            store_returns,
            left_on=["ss_item_sk", "ss_ticket_number"],
            right_on=["sr_item_sk", "sr_ticket_number"],
            how="left",
        )
        .join(date_filtered, left_on="ss_sold_date_sk", right_on="d_date_sk")
        .join(store, left_on="ss_store_sk", right_on="s_store_sk")
        .join(item_filtered, left_on="ss_item_sk", right_on="i_item_sk")
        .join(promo_filtered, left_on="ss_promo_sk", right_on="p_promo_sk")
        .group_by("s_store_id")
        .agg(
            [
                col("ss_ext_sales_price").sum().alias("sales"),
                col("sr_return_amt").fill_null(0).sum().alias("returns"),
                (col("ss_net_profit") - col("sr_net_loss").fill_null(0)).sum().alias("profit"),
            ]
        )
        .with_columns(
            [
                lit("store channel").alias("channel"),
                (lit("store") + col("s_store_id")).alias("id"),
            ]
        )
        .select(["channel", "id", "sales", "returns", "profit"])
    )

    # Catalog sales-returns
    csr = (
        catalog_sales.join(
            catalog_returns,
            left_on=["cs_item_sk", "cs_order_number"],
            right_on=["cr_item_sk", "cr_order_number"],
            how="left",
        )
        .join(date_filtered, left_on="cs_sold_date_sk", right_on="d_date_sk")
        .join(catalog_page, left_on="cs_catalog_page_sk", right_on="cp_catalog_page_sk")
        .join(item_filtered, left_on="cs_item_sk", right_on="i_item_sk")
        .join(promo_filtered, left_on="cs_promo_sk", right_on="p_promo_sk")
        .group_by("cp_catalog_page_id")
        .agg(
            [
                col("cs_ext_sales_price").sum().alias("sales"),
                col("cr_return_amount").fill_null(0).sum().alias("returns"),
                (col("cs_net_profit") - col("cr_net_loss").fill_null(0)).sum().alias("profit"),
            ]
        )
        .with_columns(
            [
                lit("catalog channel").alias("channel"),
                (lit("catalog_page") + col("cp_catalog_page_id")).alias("id"),
            ]
        )
        .select(["channel", "id", "sales", "returns", "profit"])
    )

    # Web sales-returns
    wsr = (
        web_sales.join(
            web_returns,
            left_on=["ws_item_sk", "ws_order_number"],
            right_on=["wr_item_sk", "wr_order_number"],
            how="left",
        )
        .join(date_filtered, left_on="ws_sold_date_sk", right_on="d_date_sk")
        .join(web_site, left_on="ws_web_site_sk", right_on="web_site_sk")
        .join(item_filtered, left_on="ws_item_sk", right_on="i_item_sk")
        .join(promo_filtered, left_on="ws_promo_sk", right_on="p_promo_sk")
        .group_by("web_site_id")
        .agg(
            [
                col("ws_ext_sales_price").sum().alias("sales"),
                col("wr_return_amt").fill_null(0).sum().alias("returns"),
                (col("ws_net_profit") - col("wr_net_loss").fill_null(0)).sum().alias("profit"),
            ]
        )
        .with_columns(
            [
                lit("web channel").alias("channel"),
                (lit("web_site") + col("web_site_id")).alias("id"),
            ]
        )
        .select(["channel", "id", "sales", "returns", "profit"])
    )

    # Union all channels
    combined = ctx.concat([ssr, csr, wsr])

    # Apply ROLLUP(channel, id)
    result = expand_rollup_expression(
        combined,
        group_cols=["channel", "id"],
        agg_exprs=[
            col("sales").sum().alias("sales"),
            col("returns").sum().alias("returns"),
            col("profit").sum().alias("profit"),
        ],
        ctx=ctx,
    )

    result = result.sort(["channel", "id"]).limit(100)

    return result


def q80_pandas_impl(ctx: DataFrameContext) -> Any:
    """Q80: Three-channel sales-returns with ROLLUP aggregation (Pandas)."""
    from datetime import datetime, timedelta

    import pandas as pd

    from .rollup_helper import expand_rollup_pandas

    params = get_parameters(80)
    sales_date = params.get("sales_date", "2000-08-23")

    store_sales = ctx.get_table("store_sales")
    store_returns = ctx.get_table("store_returns")
    catalog_sales = ctx.get_table("catalog_sales")
    catalog_returns = ctx.get_table("catalog_returns")
    web_sales = ctx.get_table("web_sales")
    web_returns = ctx.get_table("web_returns")
    date_dim = ctx.get_table("date_dim")
    store = ctx.get_table("store")
    catalog_page = ctx.get_table("catalog_page")
    web_site = ctx.get_table("web_site")
    item = ctx.get_table("item")
    promotion = ctx.get_table("promotion")

    # Parse the sales_date
    start_date = datetime.strptime(sales_date, "%Y-%m-%d").date() if isinstance(sales_date, str) else sales_date
    end_date = start_date + timedelta(days=30)

    # Convert date_dim d_date column for comparison
    date_dim_copy = date_dim.copy()
    if hasattr(date_dim_copy["d_date"].iloc[0] if len(date_dim_copy) > 0 else None, "date"):
        date_dim_copy["d_date"] = pd.to_datetime(date_dim_copy["d_date"]).dt.date

    # Filter date_dim
    date_filtered = date_dim_copy[(date_dim_copy["d_date"] >= start_date) & (date_dim_copy["d_date"] <= end_date)][
        ["d_date_sk"]
    ]

    # Filter item and promotion
    item_filtered = item[item["i_current_price"] > 50][["i_item_sk"]]
    promo_filtered = promotion[promotion["p_channel_tv"] == "N"][["p_promo_sk"]]

    # Store sales-returns
    ss = store_sales.merge(
        store_returns,
        left_on=["ss_item_sk", "ss_ticket_number"],
        right_on=["sr_item_sk", "sr_ticket_number"],
        how="left",
    )
    ss = ss.merge(date_filtered, left_on="ss_sold_date_sk", right_on="d_date_sk")
    ss = ss.merge(store[["s_store_sk", "s_store_id"]], left_on="ss_store_sk", right_on="s_store_sk")
    ss = ss.merge(item_filtered, left_on="ss_item_sk", right_on="i_item_sk")
    ss = ss.merge(promo_filtered, left_on="ss_promo_sk", right_on="p_promo_sk")

    ss["sr_return_amt"] = ss["sr_return_amt"].fillna(0)
    ss["sr_net_loss"] = ss["sr_net_loss"].fillna(0)
    ss["profit_row"] = ss["ss_net_profit"] - ss["sr_net_loss"]

    ssr = ss.groupby("s_store_id", as_index=False).agg(
        sales=("ss_ext_sales_price", "sum"),
        returns=("sr_return_amt", "sum"),
        profit=("profit_row", "sum"),
    )
    ssr["channel"] = "store channel"
    ssr["id"] = "store" + ssr["s_store_id"].astype(str)
    ssr = ssr[["channel", "id", "sales", "returns", "profit"]]

    # Catalog sales-returns
    cs = catalog_sales.merge(
        catalog_returns,
        left_on=["cs_item_sk", "cs_order_number"],
        right_on=["cr_item_sk", "cr_order_number"],
        how="left",
    )
    cs = cs.merge(date_filtered, left_on="cs_sold_date_sk", right_on="d_date_sk")
    cs = cs.merge(
        catalog_page[["cp_catalog_page_sk", "cp_catalog_page_id"]],
        left_on="cs_catalog_page_sk",
        right_on="cp_catalog_page_sk",
    )
    cs = cs.merge(item_filtered, left_on="cs_item_sk", right_on="i_item_sk")
    cs = cs.merge(promo_filtered, left_on="cs_promo_sk", right_on="p_promo_sk")

    cs["cr_return_amount"] = cs["cr_return_amount"].fillna(0)
    cs["cr_net_loss"] = cs["cr_net_loss"].fillna(0)
    cs["profit_row"] = cs["cs_net_profit"] - cs["cr_net_loss"]

    csr = cs.groupby("cp_catalog_page_id", as_index=False).agg(
        sales=("cs_ext_sales_price", "sum"),
        returns=("cr_return_amount", "sum"),
        profit=("profit_row", "sum"),
    )
    csr["channel"] = "catalog channel"
    csr["id"] = "catalog_page" + csr["cp_catalog_page_id"].astype(str)
    csr = csr[["channel", "id", "sales", "returns", "profit"]]

    # Web sales-returns
    ws = web_sales.merge(
        web_returns, left_on=["ws_item_sk", "ws_order_number"], right_on=["wr_item_sk", "wr_order_number"], how="left"
    )
    ws = ws.merge(date_filtered, left_on="ws_sold_date_sk", right_on="d_date_sk")
    ws = ws.merge(web_site[["web_site_sk", "web_site_id"]], left_on="ws_web_site_sk", right_on="web_site_sk")
    ws = ws.merge(item_filtered, left_on="ws_item_sk", right_on="i_item_sk")
    ws = ws.merge(promo_filtered, left_on="ws_promo_sk", right_on="p_promo_sk")

    ws["wr_return_amt"] = ws["wr_return_amt"].fillna(0)
    ws["wr_net_loss"] = ws["wr_net_loss"].fillna(0)
    ws["profit_row"] = ws["ws_net_profit"] - ws["wr_net_loss"]

    wsr = ws.groupby("web_site_id", as_index=False).agg(
        sales=("ws_ext_sales_price", "sum"),
        returns=("wr_return_amt", "sum"),
        profit=("profit_row", "sum"),
    )
    wsr["channel"] = "web channel"
    wsr["id"] = "web_site" + wsr["web_site_id"].astype(str)
    wsr = wsr[["channel", "id", "sales", "returns", "profit"]]

    # Union all channels
    combined = ctx.concat([ssr, csr, wsr])

    # Apply ROLLUP(channel, id)
    result = expand_rollup_pandas(
        combined,
        group_cols=["channel", "id"],
        agg_dict={
            "sales": ("sales", "sum"),
            "returns": ("returns", "sum"),
            "profit": ("profit", "sum"),
        },
        ctx=ctx,
    )

    result = result.sort_values(["channel", "id"]).head(100)

    return result


# =============================================================================
# Q77: Three-Channel Sales-Returns with ROLLUP (Separate CTEs)
# =============================================================================


def q77_expression_impl(ctx: DataFrameContext) -> Any:
    """Q77: Three-channel sales-returns with separate CTEs and ROLLUP (Polars).

    Aggregates sales and returns separately for each channel (store, catalog, web),
    then joins and unions with ROLLUP for subtotals.

    Tables: store_sales, store_returns, catalog_sales, catalog_returns,
            web_sales, web_returns, date_dim, store, web_page
    """
    from datetime import datetime, timedelta

    from .rollup_helper import expand_rollup_expression

    params = get_parameters(77)
    sales_date = params.get("sales_date", "2000-08-23")

    store_sales = ctx.get_table("store_sales")
    store_returns = ctx.get_table("store_returns")
    catalog_sales = ctx.get_table("catalog_sales")
    catalog_returns = ctx.get_table("catalog_returns")
    web_sales = ctx.get_table("web_sales")
    web_returns = ctx.get_table("web_returns")
    date_dim = ctx.get_table("date_dim")
    store = ctx.get_table("store")
    web_page = ctx.get_table("web_page")
    col = ctx.col
    lit = ctx.lit

    # Parse the sales_date
    start_date = datetime.strptime(sales_date, "%Y-%m-%d").date() if isinstance(sales_date, str) else sales_date
    end_date = start_date + timedelta(days=30)

    # Filter date_dim
    date_filtered = date_dim.filter((col("d_date") >= lit(start_date)) & (col("d_date") <= lit(end_date)))

    # Store sales
    # Note: Use ss_store_sk for grouping as s_store_sk is dropped after join
    ss = (
        store_sales.join(date_filtered, left_on="ss_sold_date_sk", right_on="d_date_sk")
        .join(store, left_on="ss_store_sk", right_on="s_store_sk")
        .group_by("ss_store_sk")
        .agg(
            [
                col("ss_ext_sales_price").sum().alias("sales"),
                col("ss_net_profit").sum().alias("profit"),
            ]
        )
    )

    # Store returns
    # Note: Use sr_store_sk for grouping as s_store_sk is dropped after join
    sr = (
        store_returns.join(date_filtered, left_on="sr_returned_date_sk", right_on="d_date_sk")
        .join(store, left_on="sr_store_sk", right_on="s_store_sk")
        .group_by("sr_store_sk")
        .agg(
            [
                col("sr_return_amt").sum().alias("returns"),
                col("sr_net_loss").sum().alias("profit_loss"),
            ]
        )
    )

    # Join store sales and returns
    store_combined = (
        ss.join(sr, left_on="ss_store_sk", right_on="sr_store_sk", how="left")
        .with_columns(
            [
                lit("store channel").alias("channel"),
                col("ss_store_sk").alias("id"),
                col("returns").fill_null(0).alias("returns"),
                (col("profit") - col("profit_loss").fill_null(0)).alias("profit_final"),
            ]
        )
        .select(["channel", "id", "sales", "returns", col("profit_final").alias("profit")])
    )

    # Catalog sales
    cs = (
        catalog_sales.join(date_filtered, left_on="cs_sold_date_sk", right_on="d_date_sk")
        .group_by("cs_call_center_sk")
        .agg(
            [
                col("cs_ext_sales_price").sum().alias("sales"),
                col("cs_net_profit").sum().alias("profit"),
            ]
        )
    )

    # Catalog returns
    cr = (
        catalog_returns.join(date_filtered, left_on="cr_returned_date_sk", right_on="d_date_sk")
        .group_by("cr_call_center_sk")
        .agg(
            [
                col("cr_return_amount").sum().alias("returns"),
                col("cr_net_loss").sum().alias("profit_loss"),
            ]
        )
    )

    # Cross join catalog sales and returns (as per SQL)
    catalog_combined = (
        cs.join(cr, how="cross")
        .with_columns(
            [
                lit("catalog channel").alias("channel"),
                col("cs_call_center_sk").alias("id"),
                (col("profit") - col("profit_loss")).alias("profit_final"),
            ]
        )
        .select(["channel", "id", "sales", "returns", col("profit_final").alias("profit")])
    )

    # Web sales
    # Note: Use ws_web_page_sk for grouping as wp_web_page_sk is dropped after join
    ws = (
        web_sales.join(date_filtered, left_on="ws_sold_date_sk", right_on="d_date_sk")
        .join(web_page, left_on="ws_web_page_sk", right_on="wp_web_page_sk")
        .group_by("ws_web_page_sk")
        .agg(
            [
                col("ws_ext_sales_price").sum().alias("sales"),
                col("ws_net_profit").sum().alias("profit"),
            ]
        )
    )

    # Web returns
    # Note: Use wr_web_page_sk for grouping as wp_web_page_sk is dropped after join
    wr = (
        web_returns.join(date_filtered, left_on="wr_returned_date_sk", right_on="d_date_sk")
        .join(web_page, left_on="wr_web_page_sk", right_on="wp_web_page_sk")
        .group_by("wr_web_page_sk")
        .agg(
            [
                col("wr_return_amt").sum().alias("returns"),
                col("wr_net_loss").sum().alias("profit_loss"),
            ]
        )
    )

    # Join web sales and returns
    web_combined = (
        ws.join(wr, left_on="ws_web_page_sk", right_on="wr_web_page_sk", how="left")
        .with_columns(
            [
                lit("web channel").alias("channel"),
                col("ws_web_page_sk").alias("id"),
                col("returns").fill_null(0).alias("returns"),
                (col("profit") - col("profit_loss").fill_null(0)).alias("profit_final"),
            ]
        )
        .select(["channel", "id", "sales", "returns", col("profit_final").alias("profit")])
    )

    # Union all channels
    combined = ctx.concat([store_combined, catalog_combined, web_combined])

    # Apply ROLLUP(channel, id)
    result = expand_rollup_expression(
        combined,
        group_cols=["channel", "id"],
        agg_exprs=[
            col("sales").sum().alias("sales"),
            col("returns").sum().alias("returns"),
            col("profit").sum().alias("profit"),
        ],
        ctx=ctx,
    )

    result = result.sort(["channel", "id"]).limit(100)

    return result


def q77_pandas_impl(ctx: DataFrameContext) -> Any:
    """Q77: Three-channel sales-returns with separate CTEs and ROLLUP (Pandas)."""
    from datetime import datetime, timedelta

    import pandas as pd

    from .rollup_helper import expand_rollup_pandas

    params = get_parameters(77)
    sales_date = params.get("sales_date", "2000-08-23")

    store_sales = ctx.get_table("store_sales")
    store_returns = ctx.get_table("store_returns")
    catalog_sales = ctx.get_table("catalog_sales")
    catalog_returns = ctx.get_table("catalog_returns")
    web_sales = ctx.get_table("web_sales")
    web_returns = ctx.get_table("web_returns")
    date_dim = ctx.get_table("date_dim")
    store = ctx.get_table("store")
    web_page = ctx.get_table("web_page")

    # Parse the sales_date
    start_date = datetime.strptime(sales_date, "%Y-%m-%d").date() if isinstance(sales_date, str) else sales_date
    end_date = start_date + timedelta(days=30)

    # Convert date_dim d_date column for comparison
    date_dim_copy = date_dim.copy()
    if len(date_dim_copy) > 0 and hasattr(date_dim_copy["d_date"].iloc[0], "date"):
        date_dim_copy["d_date"] = pd.to_datetime(date_dim_copy["d_date"]).dt.date

    # Filter date_dim
    date_filtered = date_dim_copy[(date_dim_copy["d_date"] >= start_date) & (date_dim_copy["d_date"] <= end_date)][
        ["d_date_sk"]
    ]

    # Store sales
    ss = store_sales.merge(date_filtered, left_on="ss_sold_date_sk", right_on="d_date_sk")
    ss = ss.merge(store[["s_store_sk"]], left_on="ss_store_sk", right_on="s_store_sk")
    ss_agg = ss.groupby("s_store_sk", as_index=False).agg(
        sales=("ss_ext_sales_price", "sum"),
        profit=("ss_net_profit", "sum"),
    )

    # Store returns
    sr = store_returns.merge(date_filtered, left_on="sr_returned_date_sk", right_on="d_date_sk")
    sr = sr.merge(store[["s_store_sk"]], left_on="sr_store_sk", right_on="s_store_sk")
    sr_agg = sr.groupby("s_store_sk", as_index=False).agg(
        returns=("sr_return_amt", "sum"),
        profit_loss=("sr_net_loss", "sum"),
    )

    # Join store sales and returns
    store_combined = ss_agg.merge(sr_agg, on="s_store_sk", how="left")
    store_combined["returns"] = store_combined["returns"].fillna(0)
    store_combined["profit_loss"] = store_combined["profit_loss"].fillna(0)
    store_combined["profit"] = store_combined["profit"] - store_combined["profit_loss"]
    store_combined["channel"] = "store channel"
    store_combined["id"] = store_combined["s_store_sk"]
    store_combined = store_combined[["channel", "id", "sales", "returns", "profit"]]

    # Catalog sales
    cs = catalog_sales.merge(date_filtered, left_on="cs_sold_date_sk", right_on="d_date_sk")
    cs_agg = cs.groupby("cs_call_center_sk", as_index=False).agg(
        sales=("cs_ext_sales_price", "sum"),
        profit=("cs_net_profit", "sum"),
    )

    # Catalog returns
    cr = catalog_returns.merge(date_filtered, left_on="cr_returned_date_sk", right_on="d_date_sk")
    cr_agg = cr.groupby("cr_call_center_sk", as_index=False).agg(
        returns=("cr_return_amount", "sum"),
        profit_loss=("cr_net_loss", "sum"),
    )

    # Cross join catalog sales and returns
    cs_agg["_key"] = 1
    cr_agg["_key"] = 1
    catalog_combined = cs_agg.merge(cr_agg, on="_key").drop(columns=["_key"])
    catalog_combined["profit"] = catalog_combined["profit"] - catalog_combined["profit_loss"]
    catalog_combined["channel"] = "catalog channel"
    catalog_combined["id"] = catalog_combined["cs_call_center_sk"]
    catalog_combined = catalog_combined[["channel", "id", "sales", "returns", "profit"]]

    # Web sales
    ws = web_sales.merge(date_filtered, left_on="ws_sold_date_sk", right_on="d_date_sk")
    ws = ws.merge(web_page[["wp_web_page_sk"]], left_on="ws_web_page_sk", right_on="wp_web_page_sk")
    ws_agg = ws.groupby("wp_web_page_sk", as_index=False).agg(
        sales=("ws_ext_sales_price", "sum"),
        profit=("ws_net_profit", "sum"),
    )

    # Web returns
    wr = web_returns.merge(date_filtered, left_on="wr_returned_date_sk", right_on="d_date_sk")
    wr = wr.merge(web_page[["wp_web_page_sk"]], left_on="wr_web_page_sk", right_on="wp_web_page_sk")
    wr_agg = wr.groupby("wp_web_page_sk", as_index=False).agg(
        returns=("wr_return_amt", "sum"),
        profit_loss=("wr_net_loss", "sum"),
    )

    # Join web sales and returns
    web_combined = ws_agg.merge(wr_agg, on="wp_web_page_sk", how="left")
    web_combined["returns"] = web_combined["returns"].fillna(0)
    web_combined["profit_loss"] = web_combined["profit_loss"].fillna(0)
    web_combined["profit"] = web_combined["profit"] - web_combined["profit_loss"]
    web_combined["channel"] = "web channel"
    web_combined["id"] = web_combined["wp_web_page_sk"]
    web_combined = web_combined[["channel", "id", "sales", "returns", "profit"]]

    # Union all channels
    combined = ctx.concat([store_combined, catalog_combined, web_combined])

    # Apply ROLLUP(channel, id)
    result = expand_rollup_pandas(
        combined,
        group_cols=["channel", "id"],
        agg_dict={
            "sales": ("sales", "sum"),
            "returns": ("returns", "sum"),
            "profit": ("profit", "sum"),
        },
        ctx=ctx,
    )

    result = result.sort_values(["channel", "id"]).head(100)

    return result


# =============================================================================
# Q58: Three-Channel Item Sales by Week with Balance Check
# =============================================================================


def q58_expression_impl(ctx: DataFrameContext) -> Any:
    """Q58: Three-channel item sales by week with balance check (Polars).

    For each channel, aggregate sales by item for items sold in a specific week.
    Return items where all three channels have sales within 10% of each other.

    Tables: store_sales, catalog_sales, web_sales, item, date_dim
    """
    from datetime import date as date_type

    params = get_parameters(58)
    sales_date_str = params.get("sales_date", "2000-01-03")
    sales_date = date_type.fromisoformat(sales_date_str)

    store_sales = ctx.get_table("store_sales")
    catalog_sales = ctx.get_table("catalog_sales")
    web_sales = ctx.get_table("web_sales")
    item = ctx.get_table("item")
    date_dim = ctx.get_table("date_dim")
    col = ctx.col
    lit = ctx.lit

    # Get the week_seq for the sales_date
    week_seq_df = date_dim.filter(col("d_date") == lit(sales_date)).select("d_week_seq")

    # Get all dates in that week
    dates_in_week = date_dim.join(week_seq_df, on="d_week_seq").select("d_date_sk")

    # Store sales by item
    ss_items = (
        store_sales.join(item, left_on="ss_item_sk", right_on="i_item_sk")
        .join(dates_in_week, left_on="ss_sold_date_sk", right_on="d_date_sk")
        .group_by("i_item_id")
        .agg(col("ss_ext_sales_price").sum().alias("ss_item_rev"))
    )

    # Catalog sales by item
    cs_items = (
        catalog_sales.join(item, left_on="cs_item_sk", right_on="i_item_sk")
        .join(dates_in_week, left_on="cs_sold_date_sk", right_on="d_date_sk")
        .group_by("i_item_id")
        .agg(col("cs_ext_sales_price").sum().alias("cs_item_rev"))
    )

    # Web sales by item
    ws_items = (
        web_sales.join(item, left_on="ws_item_sk", right_on="i_item_sk")
        .join(dates_in_week, left_on="ws_sold_date_sk", right_on="d_date_sk")
        .group_by("i_item_id")
        .agg(col("ws_ext_sales_price").sum().alias("ws_item_rev"))
    )

    # Join all three
    result = (
        ss_items.join(cs_items, on="i_item_id")
        .join(ws_items, on="i_item_id")
        # Filter where all three are within 10% of each other
        .filter(
            (col("ss_item_rev") >= col("cs_item_rev") * lit(0.9))
            & (col("ss_item_rev") <= col("cs_item_rev") * lit(1.1))
            & (col("ss_item_rev") >= col("ws_item_rev") * lit(0.9))
            & (col("ss_item_rev") <= col("ws_item_rev") * lit(1.1))
            & (col("cs_item_rev") >= col("ss_item_rev") * lit(0.9))
            & (col("cs_item_rev") <= col("ss_item_rev") * lit(1.1))
            & (col("cs_item_rev") >= col("ws_item_rev") * lit(0.9))
            & (col("cs_item_rev") <= col("ws_item_rev") * lit(1.1))
            & (col("ws_item_rev") >= col("ss_item_rev") * lit(0.9))
            & (col("ws_item_rev") <= col("ss_item_rev") * lit(1.1))
            & (col("ws_item_rev") >= col("cs_item_rev") * lit(0.9))
            & (col("ws_item_rev") <= col("cs_item_rev") * lit(1.1))
        )
        .with_columns(
            [
                ((col("ss_item_rev") + col("cs_item_rev") + col("ws_item_rev")) / lit(3)).alias("average"),
            ]
        )
        .with_columns(
            [
                (col("ss_item_rev") / col("average") * lit(100)).alias("ss_dev"),
                (col("cs_item_rev") / col("average") * lit(100)).alias("cs_dev"),
                (col("ws_item_rev") / col("average") * lit(100)).alias("ws_dev"),
            ]
        )
        .select(
            [
                col("i_item_id").alias("item_id"),
                "ss_item_rev",
                "ss_dev",
                "cs_item_rev",
                "cs_dev",
                "ws_item_rev",
                "ws_dev",
                "average",
            ]
        )
        .sort(["item_id", "ss_item_rev"])
        .limit(100)
    )

    return result


def q58_pandas_impl(ctx: DataFrameContext) -> Any:
    """Q58: Three-channel item sales by week with balance check (Pandas)."""
    params = get_parameters(58)
    sales_date = params.get("sales_date", "2000-01-03")

    store_sales = ctx.get_table("store_sales")
    catalog_sales = ctx.get_table("catalog_sales")
    web_sales = ctx.get_table("web_sales")
    item = ctx.get_table("item")
    date_dim = ctx.get_table("date_dim")

    # Get the week_seq for the sales_date
    week_seq = date_dim[date_dim["d_date"] == sales_date]["d_week_seq"].iloc[0]

    # Get all dates in that week
    dates_in_week = date_dim[date_dim["d_week_seq"] == week_seq][["d_date_sk"]]

    # Store sales by item
    ss_joined = store_sales.merge(item, left_on="ss_item_sk", right_on="i_item_sk")
    ss_joined = ss_joined.merge(dates_in_week, left_on="ss_sold_date_sk", right_on="d_date_sk")
    ss_items = ss_joined.groupby("i_item_id", as_index=False).agg(ss_item_rev=("ss_ext_sales_price", "sum"))

    # Catalog sales by item
    cs_joined = catalog_sales.merge(item, left_on="cs_item_sk", right_on="i_item_sk")
    cs_joined = cs_joined.merge(dates_in_week, left_on="cs_sold_date_sk", right_on="d_date_sk")
    cs_items = cs_joined.groupby("i_item_id", as_index=False).agg(cs_item_rev=("cs_ext_sales_price", "sum"))

    # Web sales by item
    ws_joined = web_sales.merge(item, left_on="ws_item_sk", right_on="i_item_sk")
    ws_joined = ws_joined.merge(dates_in_week, left_on="ws_sold_date_sk", right_on="d_date_sk")
    ws_items = ws_joined.groupby("i_item_id", as_index=False).agg(ws_item_rev=("ws_ext_sales_price", "sum"))

    # Join all three
    result = ss_items.merge(cs_items, on="i_item_id")
    result = result.merge(ws_items, on="i_item_id")

    # Filter where all three are within 10% of each other
    result = result[
        (result["ss_item_rev"] >= result["cs_item_rev"] * 0.9)
        & (result["ss_item_rev"] <= result["cs_item_rev"] * 1.1)
        & (result["ss_item_rev"] >= result["ws_item_rev"] * 0.9)
        & (result["ss_item_rev"] <= result["ws_item_rev"] * 1.1)
        & (result["cs_item_rev"] >= result["ss_item_rev"] * 0.9)
        & (result["cs_item_rev"] <= result["ss_item_rev"] * 1.1)
        & (result["cs_item_rev"] >= result["ws_item_rev"] * 0.9)
        & (result["cs_item_rev"] <= result["ws_item_rev"] * 1.1)
        & (result["ws_item_rev"] >= result["ss_item_rev"] * 0.9)
        & (result["ws_item_rev"] <= result["ss_item_rev"] * 1.1)
        & (result["ws_item_rev"] >= result["cs_item_rev"] * 0.9)
        & (result["ws_item_rev"] <= result["cs_item_rev"] * 1.1)
    ]

    # Calculate average and deviations
    result["average"] = (result["ss_item_rev"] + result["cs_item_rev"] + result["ws_item_rev"]) / 3
    result["ss_dev"] = result["ss_item_rev"] / result["average"] * 100
    result["cs_dev"] = result["cs_item_rev"] / result["average"] * 100
    result["ws_dev"] = result["ws_item_rev"] / result["average"] * 100

    result = result.rename(columns={"i_item_id": "item_id"})
    result = result[["item_id", "ss_item_rev", "ss_dev", "cs_item_rev", "cs_dev", "ws_item_rev", "ws_dev", "average"]]

    result = result.sort_values(["item_id", "ss_item_rev"]).head(100)

    return result


# =============================================================================
# Q54: Catalog+Web Customer Store Revenue Segments
# =============================================================================


def q54_expression_impl(ctx: DataFrameContext) -> Any:
    """Q54: Catalog+web customers' store revenue segments (Polars).

    Find customers who bought items in a category from catalog+web,
    then aggregate their store sales in following months.
    Segment customers by revenue buckets.

    Tables: catalog_sales, web_sales, store_sales, customer, customer_address,
            store, item, date_dim
    """

    params = get_parameters(54)
    year = params.get("year", 1998)
    month = params.get("month", 12)
    category = params.get("category", "Women")
    item_class = params.get("class", "maternity")

    catalog_sales = ctx.get_table("catalog_sales")
    web_sales = ctx.get_table("web_sales")
    store_sales = ctx.get_table("store_sales")
    customer = ctx.get_table("customer")
    customer_address = ctx.get_table("customer_address")
    store = ctx.get_table("store")
    item = ctx.get_table("item")
    date_dim = ctx.get_table("date_dim")
    col = ctx.col
    lit = ctx.lit

    # Filter item by category and class
    item_filtered = item.filter((col("i_category") == lit(category)) & (col("i_class") == lit(item_class)))

    # Date filter for initial purchase
    date_filtered = date_dim.filter((col("d_year") == lit(year)) & (col("d_moy") == lit(month)))

    # Union catalog and web sales
    cs_sales = catalog_sales.select(
        [
            col("cs_sold_date_sk").alias("sold_date_sk"),
            col("cs_bill_customer_sk").alias("customer_sk"),
            col("cs_item_sk").alias("item_sk"),
        ]
    )

    ws_sales = web_sales.select(
        [
            col("ws_sold_date_sk").alias("sold_date_sk"),
            col("ws_bill_customer_sk").alias("customer_sk"),
            col("ws_item_sk").alias("item_sk"),
        ]
    )

    cs_or_ws = ctx.concat([cs_sales, ws_sales])

    # Find customers who bought in the category
    # Note: c_customer_sk is dropped after join (right key), use customer_sk instead
    my_customers = (
        cs_or_ws.join(item_filtered, left_on="item_sk", right_on="i_item_sk")
        .join(date_filtered, left_on="sold_date_sk", right_on="d_date_sk")
        .join(customer, left_on="customer_sk", right_on="c_customer_sk")
        .select([col("customer_sk").alias("c_customer_sk"), "c_current_addr_sk"])
        .unique()
    )

    # Get the month_seq for following 3 months
    month_seq_df = (
        date_dim.filter((col("d_year") == lit(year)) & (col("d_moy") == lit(month))).select("d_month_seq").unique()
    )

    # Dates for months +1 to +3
    following_dates = (
        date_dim.join(month_seq_df, how="cross")
        .filter(
            (col("d_month_seq") >= col("d_month_seq_right") + lit(1))
            & (col("d_month_seq") <= col("d_month_seq_right") + lit(3))
        )
        .select("d_date_sk")
    )

    # Calculate store sales revenue for these customers
    my_revenue = (
        my_customers.join(customer_address, left_on="c_current_addr_sk", right_on="ca_address_sk")
        .join(store, left_on=["ca_county", "ca_state"], right_on=["s_county", "s_state"])
        .join(store_sales, left_on="c_customer_sk", right_on="ss_customer_sk")
        .join(following_dates, left_on="ss_sold_date_sk", right_on="d_date_sk")
        .group_by("c_customer_sk")
        .agg(col("ss_ext_sales_price").sum().alias("revenue"))
    )

    # Create segments (revenue / 50)
    segments = my_revenue.with_columns((col("revenue") / lit(50)).cast_int64().alias("segment"))

    # Aggregate by segment
    result = (
        segments.group_by("segment")
        .agg(ctx.len().alias("num_customers"))
        .with_columns((col("segment") * lit(50)).alias("segment_base"))
        .select(["segment", "num_customers", "segment_base"])
        .sort(["segment", "num_customers"])
        .limit(100)
    )

    return result


def q54_pandas_impl(ctx: DataFrameContext) -> Any:
    """Q54: Catalog+web customers' store revenue segments (Pandas)."""

    params = get_parameters(54)
    year = params.get("year", 1998)
    month = params.get("month", 12)
    category = params.get("category", "Women")
    item_class = params.get("class", "maternity")

    catalog_sales = ctx.get_table("catalog_sales")
    web_sales = ctx.get_table("web_sales")
    store_sales = ctx.get_table("store_sales")
    customer = ctx.get_table("customer")
    customer_address = ctx.get_table("customer_address")
    store = ctx.get_table("store")
    item = ctx.get_table("item")
    date_dim = ctx.get_table("date_dim")

    # Filter item by category and class
    item_filtered = item[(item["i_category"] == category) & (item["i_class"] == item_class)]

    # Date filter for initial purchase
    date_filtered = date_dim[(date_dim["d_year"] == year) & (date_dim["d_moy"] == month)]

    # Union catalog and web sales
    cs_sales = catalog_sales[["cs_sold_date_sk", "cs_bill_customer_sk", "cs_item_sk"]].rename(
        columns={"cs_sold_date_sk": "sold_date_sk", "cs_bill_customer_sk": "customer_sk", "cs_item_sk": "item_sk"}
    )
    ws_sales = web_sales[["ws_sold_date_sk", "ws_bill_customer_sk", "ws_item_sk"]].rename(
        columns={"ws_sold_date_sk": "sold_date_sk", "ws_bill_customer_sk": "customer_sk", "ws_item_sk": "item_sk"}
    )
    cs_or_ws = ctx.concat([cs_sales, ws_sales])

    # Find customers who bought in the category
    merged = cs_or_ws.merge(item_filtered[["i_item_sk"]], left_on="item_sk", right_on="i_item_sk")
    merged = merged.merge(date_filtered[["d_date_sk"]], left_on="sold_date_sk", right_on="d_date_sk")
    merged = merged.merge(
        customer[["c_customer_sk", "c_current_addr_sk"]], left_on="customer_sk", right_on="c_customer_sk"
    )
    my_customers = merged[["c_customer_sk", "c_current_addr_sk"]].drop_duplicates()

    # Get the month_seq for following 3 months
    month_seq = date_dim[(date_dim["d_year"] == year) & (date_dim["d_moy"] == month)]["d_month_seq"].iloc[0]

    # Dates for months +1 to +3
    following_dates = date_dim[(date_dim["d_month_seq"] >= month_seq + 1) & (date_dim["d_month_seq"] <= month_seq + 3)][
        ["d_date_sk"]
    ]

    # Calculate store sales revenue for these customers
    my_revenue = my_customers.merge(
        customer_address[["ca_address_sk", "ca_county", "ca_state"]],
        left_on="c_current_addr_sk",
        right_on="ca_address_sk",
    )
    my_revenue = my_revenue.merge(
        store[["s_county", "s_state"]], left_on=["ca_county", "ca_state"], right_on=["s_county", "s_state"]
    )
    my_revenue = my_revenue.merge(
        store_sales[["ss_customer_sk", "ss_sold_date_sk", "ss_ext_sales_price"]],
        left_on="c_customer_sk",
        right_on="ss_customer_sk",
    )
    my_revenue = my_revenue.merge(following_dates, left_on="ss_sold_date_sk", right_on="d_date_sk")

    revenue_agg = my_revenue.groupby("c_customer_sk", as_index=False).agg(revenue=("ss_ext_sales_price", "sum"))

    # Create segments (revenue / 50)
    revenue_agg["segment"] = (revenue_agg["revenue"] / 50).astype(int)

    # Aggregate by segment
    result = revenue_agg.groupby("segment", as_index=False).agg(num_customers=("segment", "count"))
    result["segment_base"] = result["segment"] * 50

    result = result[["segment", "num_customers", "segment_base"]]
    result = result.sort_values(["segment", "num_customers"]).head(100)

    return result


# =============================================================================
# Q44: Store Sales Item Ranking - Best and Worst Performers
# =============================================================================


def q44_expression_impl(ctx: DataFrameContext) -> Any:
    """Q44: Store sales item ranking - best and worst performers (Polars).

    Ranks items by average net profit for a specific store, finding top 10
    best and worst performers that exceed 90% of the store's overall average.

    Tables: store_sales, item
    """

    params = get_parameters(44)
    store_sk = params.get("store_sk", 4)
    null_col = params.get("null_col", "ss_addr_sk")

    col = ctx.col
    lit = ctx.lit

    store_sales = ctx.get_table("store_sales")
    item = ctx.get_table("item")

    # Filter to specific store
    ss_store = store_sales.filter(col("ss_store_sk") == lit(store_sk))

    threshold = 0.9

    # Item averages
    item_avg = ss_store.group_by("ss_item_sk").agg(col("ss_net_profit").mean().alias("rank_col"))

    # Compute threshold from store average
    ss_null = store_sales.filter((col("ss_store_sk") == lit(store_sk)) & col(null_col).is_null())
    store_baseline = ss_null.select(col("ss_net_profit").mean().alias("baseline"))

    # Join and filter items above threshold
    item_with_baseline = item_avg.join(store_baseline, how="cross")
    qualified = item_with_baseline.filter(col("rank_col") > (lit(threshold) * col("baseline")))

    # Ascending rank (best performers - highest profit)
    ascending = (
        qualified.with_columns(col("rank_col").rank(method="min", descending=False).alias("rnk"))
        .filter(col("rnk") <= 10)
        .select(["ss_item_sk", "rnk"])
        .rename({"ss_item_sk": "item_sk_asc", "rnk": "rnk_asc"})
    )

    # Descending rank (worst performers - lowest profit among qualified)
    descending = (
        qualified.with_columns(col("rank_col").rank(method="min", descending=True).alias("rnk"))
        .filter(col("rnk") <= 10)
        .select(["ss_item_sk", "rnk"])
        .rename({"ss_item_sk": "item_sk_desc", "rnk": "rnk_desc"})
    )

    # Join on rank
    result = ascending.join(descending, left_on="rnk_asc", right_on="rnk_desc")

    # Join with item for names
    result = (
        result.join(item, left_on="item_sk_asc", right_on="i_item_sk")
        .rename({"i_product_name": "best_performing"})
        .select(["rnk_asc", "best_performing", "item_sk_desc"])
    )

    result = (
        result.join(item, left_on="item_sk_desc", right_on="i_item_sk")
        .rename({"i_product_name": "worst_performing"})
        .select(["rnk_asc", "best_performing", "worst_performing"])
        .rename({"rnk_asc": "rnk"})
        .sort("rnk")
    )

    return result


def q44_pandas_impl(ctx: DataFrameContext) -> Any:
    """Q44: Store sales item ranking - best and worst performers (Pandas)."""
    params = get_parameters(44)
    store_sk = params.get("store_sk", 4)
    null_col = params.get("null_col", "ss_addr_sk")

    store_sales = ctx.get_table("store_sales")
    item = ctx.get_table("item")

    # Filter to specific store
    ss_store = store_sales[store_sales["ss_store_sk"] == store_sk]

    # Compute store average threshold
    ss_null = ss_store[ss_store[null_col].isna()]
    baseline = ss_null["ss_net_profit"].mean()
    threshold = 0.9 * baseline

    # Item averages
    item_avg = ss_store.groupby("ss_item_sk", as_index=False).agg(rank_col=("ss_net_profit", "mean"))

    # Filter items above threshold
    qualified = item_avg[item_avg["rank_col"] > threshold].copy()

    # Ascending rank (best - highest values)
    qualified["rnk_asc"] = qualified["rank_col"].rank(method="min", ascending=True)
    ascending = qualified[qualified["rnk_asc"] <= 10][["ss_item_sk", "rnk_asc"]].copy()
    ascending = ascending.rename(columns={"ss_item_sk": "item_sk_asc"})

    # Descending rank (worst - lowest among qualified)
    qualified["rnk_desc"] = qualified["rank_col"].rank(method="min", ascending=False)
    descending = qualified[qualified["rnk_desc"] <= 10][["ss_item_sk", "rnk_desc"]].copy()
    descending = descending.rename(columns={"ss_item_sk": "item_sk_desc"})

    # Join on rank
    result = ascending.merge(descending, left_on="rnk_asc", right_on="rnk_desc")

    # Join with item for names
    result = result.merge(
        item[["i_item_sk", "i_product_name"]],
        left_on="item_sk_asc",
        right_on="i_item_sk",
    )
    result = result.rename(columns={"i_product_name": "best_performing"})

    result = result.merge(
        item[["i_item_sk", "i_product_name"]],
        left_on="item_sk_desc",
        right_on="i_item_sk",
        suffixes=("", "_2"),
    )
    result = result.rename(columns={"i_product_name": "worst_performing"})

    result = result[["rnk_asc", "best_performing", "worst_performing"]].copy()
    result = result.rename(columns={"rnk_asc": "rnk"})
    result = result.sort_values("rnk")

    return result


# =============================================================================
# Q59: Store Weekly Sales Year-over-Year Comparison
# =============================================================================


def q59_expression_impl(ctx: DataFrameContext) -> Any:
    """Q59: Store weekly sales year-over-year comparison (Polars).

    Compares weekly sales by day of week for stores between current year
    and prior year (52 weeks earlier). Computes ratios for each day.

    Tables: store_sales, date_dim, store
    """

    params = get_parameters(59)
    dms = params.get("d_month_seq", 1212)  # Month sequence starting point

    col = ctx.col
    lit = ctx.lit

    store_sales = ctx.get_table("store_sales")
    date_dim = ctx.get_table("date_dim")
    store = ctx.get_table("store")

    # Join store_sales with date_dim
    ss_with_date = store_sales.join(date_dim, left_on="ss_sold_date_sk", right_on="d_date_sk")

    # Aggregate by week_seq and store with day-of-week sales
    wss = ss_with_date.group_by(["d_week_seq", "ss_store_sk"]).agg(
        [
            ctx.when(col("d_day_name") == lit("Sunday"))
            .then(col("ss_sales_price"))
            .otherwise(None)
            .sum()
            .alias("sun_sales"),
            ctx.when(col("d_day_name") == lit("Monday"))
            .then(col("ss_sales_price"))
            .otherwise(None)
            .sum()
            .alias("mon_sales"),
            ctx.when(col("d_day_name") == lit("Tuesday"))
            .then(col("ss_sales_price"))
            .otherwise(None)
            .sum()
            .alias("tue_sales"),
            ctx.when(col("d_day_name") == lit("Wednesday"))
            .then(col("ss_sales_price"))
            .otherwise(None)
            .sum()
            .alias("wed_sales"),
            ctx.when(col("d_day_name") == lit("Thursday"))
            .then(col("ss_sales_price"))
            .otherwise(None)
            .sum()
            .alias("thu_sales"),
            ctx.when(col("d_day_name") == lit("Friday"))
            .then(col("ss_sales_price"))
            .otherwise(None)
            .sum()
            .alias("fri_sales"),
            ctx.when(col("d_day_name") == lit("Saturday"))
            .then(col("ss_sales_price"))
            .otherwise(None)
            .sum()
            .alias("sat_sales"),
        ]
    )

    # Current year: month_seq between dms and dms+11
    date_filtered_1 = date_dim.filter((col("d_month_seq") >= lit(dms)) & (col("d_month_seq") <= lit(dms + 11)))

    y = (
        wss.join(store, left_on="ss_store_sk", right_on="s_store_sk")
        .join(date_filtered_1.select(["d_week_seq"]).unique(), on="d_week_seq")
        .select(
            [
                col("s_store_name").alias("s_store_name1"),
                col("s_store_id").alias("s_store_id1"),
                col("d_week_seq").alias("d_week_seq1"),
                col("sun_sales").alias("sun_sales1"),
                col("mon_sales").alias("mon_sales1"),
                col("tue_sales").alias("tue_sales1"),
                col("wed_sales").alias("wed_sales1"),
                col("thu_sales").alias("thu_sales1"),
                col("fri_sales").alias("fri_sales1"),
                col("sat_sales").alias("sat_sales1"),
            ]
        )
    )

    # Prior year: month_seq between dms+12 and dms+23
    date_filtered_2 = date_dim.filter((col("d_month_seq") >= lit(dms + 12)) & (col("d_month_seq") <= lit(dms + 23)))

    x = (
        wss.join(store, left_on="ss_store_sk", right_on="s_store_sk")
        .join(date_filtered_2.select(["d_week_seq"]).unique(), on="d_week_seq")
        .select(
            [
                col("s_store_id").alias("s_store_id2"),
                col("d_week_seq").alias("d_week_seq2"),
                col("sun_sales").alias("sun_sales2"),
                col("mon_sales").alias("mon_sales2"),
                col("tue_sales").alias("tue_sales2"),
                col("wed_sales").alias("wed_sales2"),
                col("thu_sales").alias("thu_sales2"),
                col("fri_sales").alias("fri_sales2"),
                col("sat_sales").alias("sat_sales2"),
            ]
        )
    )

    # Join on store_id and week_seq offset by 52
    result = (
        y.join(
            x,
            left_on=["s_store_id1", (col("d_week_seq1") + lit(52))],
            right_on=["s_store_id2", "d_week_seq2"],
        )
        .with_columns(
            [
                (col("sun_sales1") / col("sun_sales2")).alias("sun_ratio"),
                (col("mon_sales1") / col("mon_sales2")).alias("mon_ratio"),
                (col("tue_sales1") / col("tue_sales2")).alias("tue_ratio"),
                (col("wed_sales1") / col("wed_sales2")).alias("wed_ratio"),
                (col("thu_sales1") / col("thu_sales2")).alias("thu_ratio"),
                (col("fri_sales1") / col("fri_sales2")).alias("fri_ratio"),
                (col("sat_sales1") / col("sat_sales2")).alias("sat_ratio"),
            ]
        )
        .select(
            [
                "s_store_name1",
                "s_store_id1",
                "d_week_seq1",
                "sun_ratio",
                "mon_ratio",
                "tue_ratio",
                "wed_ratio",
                "thu_ratio",
                "fri_ratio",
                "sat_ratio",
            ]
        )
        .sort(["s_store_name1", "s_store_id1", "d_week_seq1"])
        .head(100)
    )

    return result


def q59_pandas_impl(ctx: DataFrameContext) -> Any:
    """Q59: Store weekly sales year-over-year comparison (Pandas)."""
    params = get_parameters(59)
    dms = params.get("d_month_seq", 1212)

    store_sales = ctx.get_table("store_sales")
    date_dim = ctx.get_table("date_dim")
    store = ctx.get_table("store")

    # Join store_sales with date_dim
    ss_with_date = store_sales.merge(date_dim, left_on="ss_sold_date_sk", right_on="d_date_sk")

    # Create day-of-week columns
    ss_with_date["sun_sales"] = ss_with_date.apply(
        lambda r: r["ss_sales_price"] if r["d_day_name"] == "Sunday" else None, axis=1
    )
    ss_with_date["mon_sales"] = ss_with_date.apply(
        lambda r: r["ss_sales_price"] if r["d_day_name"] == "Monday" else None, axis=1
    )
    ss_with_date["tue_sales"] = ss_with_date.apply(
        lambda r: r["ss_sales_price"] if r["d_day_name"] == "Tuesday" else None, axis=1
    )
    ss_with_date["wed_sales"] = ss_with_date.apply(
        lambda r: r["ss_sales_price"] if r["d_day_name"] == "Wednesday" else None, axis=1
    )
    ss_with_date["thu_sales"] = ss_with_date.apply(
        lambda r: r["ss_sales_price"] if r["d_day_name"] == "Thursday" else None, axis=1
    )
    ss_with_date["fri_sales"] = ss_with_date.apply(
        lambda r: r["ss_sales_price"] if r["d_day_name"] == "Friday" else None, axis=1
    )
    ss_with_date["sat_sales"] = ss_with_date.apply(
        lambda r: r["ss_sales_price"] if r["d_day_name"] == "Saturday" else None, axis=1
    )

    # Aggregate by week_seq and store
    wss = ss_with_date.groupby(["d_week_seq", "ss_store_sk"], as_index=False).agg(
        {
            "sun_sales": "sum",
            "mon_sales": "sum",
            "tue_sales": "sum",
            "wed_sales": "sum",
            "thu_sales": "sum",
            "fri_sales": "sum",
            "sat_sales": "sum",
        }
    )

    # Join with store
    wss = wss.merge(store, left_on="ss_store_sk", right_on="s_store_sk")

    # Current year filter
    weeks_1 = date_dim[(date_dim["d_month_seq"] >= dms) & (date_dim["d_month_seq"] <= dms + 11)]["d_week_seq"].unique()
    y = wss[wss["d_week_seq"].isin(weeks_1)].copy()
    y = y.rename(
        columns={
            "s_store_name": "s_store_name1",
            "s_store_id": "s_store_id1",
            "d_week_seq": "d_week_seq1",
            "sun_sales": "sun_sales1",
            "mon_sales": "mon_sales1",
            "tue_sales": "tue_sales1",
            "wed_sales": "wed_sales1",
            "thu_sales": "thu_sales1",
            "fri_sales": "fri_sales1",
            "sat_sales": "sat_sales1",
        }
    )

    # Prior year filter
    weeks_2 = date_dim[(date_dim["d_month_seq"] >= dms + 12) & (date_dim["d_month_seq"] <= dms + 23)][
        "d_week_seq"
    ].unique()
    x = wss[wss["d_week_seq"].isin(weeks_2)].copy()
    x = x.rename(
        columns={
            "s_store_id": "s_store_id2",
            "d_week_seq": "d_week_seq2",
            "sun_sales": "sun_sales2",
            "mon_sales": "mon_sales2",
            "tue_sales": "tue_sales2",
            "wed_sales": "wed_sales2",
            "thu_sales": "thu_sales2",
            "fri_sales": "fri_sales2",
            "sat_sales": "sat_sales2",
        }
    )

    # Prepare join key
    y["join_key"] = y["d_week_seq1"] + 52

    # Join on store_id and week offset
    result = y.merge(
        x[
            [
                "s_store_id2",
                "d_week_seq2",
                "sun_sales2",
                "mon_sales2",
                "tue_sales2",
                "wed_sales2",
                "thu_sales2",
                "fri_sales2",
                "sat_sales2",
            ]
        ],
        left_on=["s_store_id1", "join_key"],
        right_on=["s_store_id2", "d_week_seq2"],
    )

    # Compute ratios
    result["sun_ratio"] = result["sun_sales1"] / result["sun_sales2"]
    result["mon_ratio"] = result["mon_sales1"] / result["mon_sales2"]
    result["tue_ratio"] = result["tue_sales1"] / result["tue_sales2"]
    result["wed_ratio"] = result["wed_sales1"] / result["wed_sales2"]
    result["thu_ratio"] = result["thu_sales1"] / result["thu_sales2"]
    result["fri_ratio"] = result["fri_sales1"] / result["fri_sales2"]
    result["sat_ratio"] = result["sat_sales1"] / result["sat_sales2"]

    result = result[
        [
            "s_store_name1",
            "s_store_id1",
            "d_week_seq1",
            "sun_ratio",
            "mon_ratio",
            "tue_ratio",
            "wed_ratio",
            "thu_ratio",
            "fri_ratio",
            "sat_ratio",
        ]
    ]
    result = result.sort_values(["s_store_name1", "s_store_id1", "d_week_seq1"]).head(100)

    return result


# =============================================================================
# Q61: Promotional Sales vs Total Sales Ratio
# =============================================================================


def q61_expression_impl(ctx: DataFrameContext) -> Any:
    """Q61: Promotional sales vs total sales ratio (Polars).

    Compares promotional sales to total sales for a category,
    filtered by customer/store GMT offset.

    Tables: store_sales, store, promotion, date_dim, customer,
            customer_address, item
    """

    params = get_parameters(61)
    year = params.get("year", 1998)
    month = params.get("month", 11)
    gmt_offset = params.get("gmt_offset", -5.0)
    category = params.get("category", "Jewelry")

    col = ctx.col
    lit = ctx.lit

    store_sales = ctx.get_table("store_sales")
    store = ctx.get_table("store")
    promotion = ctx.get_table("promotion")
    date_dim = ctx.get_table("date_dim")
    customer = ctx.get_table("customer")
    customer_address = ctx.get_table("customer_address")
    item = ctx.get_table("item")

    # Filter dimensions
    date_filtered = date_dim.filter((col("d_year") == lit(year)) & (col("d_moy") == lit(month)))
    store_filtered = store.filter(col("s_gmt_offset") == lit(gmt_offset))
    ca_filtered = customer_address.filter(col("ca_gmt_offset") == lit(gmt_offset))
    item_filtered = item.filter(col("i_category") == lit(category))
    promo_filtered = promotion.filter(
        (col("p_channel_dmail") == lit("Y")) | (col("p_channel_email") == lit("Y")) | (col("p_channel_tv") == lit("Y"))
    )

    # Base join for all sales
    base = (
        store_sales.join(date_filtered, left_on="ss_sold_date_sk", right_on="d_date_sk")
        .join(store_filtered, left_on="ss_store_sk", right_on="s_store_sk")
        .join(customer, left_on="ss_customer_sk", right_on="c_customer_sk")
        .join(ca_filtered, left_on="c_current_addr_sk", right_on="ca_address_sk")
        .join(item_filtered, left_on="ss_item_sk", right_on="i_item_sk")
    )

    # Total sales
    total = base.select(ctx.sum("ss_ext_sales_price").alias("total"))

    # Promotional sales
    promo = base.join(promo_filtered, left_on="ss_promo_sk", right_on="p_promo_sk").select(
        ctx.sum("ss_ext_sales_price").alias("promotions")
    )

    # Combine
    result = promo.join(total, how="cross").with_columns(
        ((col("promotions") / col("total")) * lit(100)).alias("promo_pct")
    )

    return result


def q61_pandas_impl(ctx: DataFrameContext) -> Any:
    """Q61: Promotional sales vs total sales ratio (Pandas)."""
    params = get_parameters(61)
    year = params.get("year", 1998)
    month = params.get("month", 11)
    gmt_offset = params.get("gmt_offset", -5.0)
    category = params.get("category", "Jewelry")

    store_sales = ctx.get_table("store_sales")
    store = ctx.get_table("store")
    promotion = ctx.get_table("promotion")
    date_dim = ctx.get_table("date_dim")
    customer = ctx.get_table("customer")
    customer_address = ctx.get_table("customer_address")
    item = ctx.get_table("item")

    # Filter dimensions
    date_filtered = date_dim[(date_dim["d_year"] == year) & (date_dim["d_moy"] == month)]
    store_filtered = store[store["s_gmt_offset"] == gmt_offset]
    ca_filtered = customer_address[customer_address["ca_gmt_offset"] == gmt_offset]
    item_filtered = item[item["i_category"] == category]
    promo_filtered = promotion[
        (promotion["p_channel_dmail"] == "Y")
        | (promotion["p_channel_email"] == "Y")
        | (promotion["p_channel_tv"] == "Y")
    ]

    # Base join
    base = store_sales.merge(date_filtered[["d_date_sk"]], left_on="ss_sold_date_sk", right_on="d_date_sk")
    base = base.merge(store_filtered[["s_store_sk"]], left_on="ss_store_sk", right_on="s_store_sk")
    base = base.merge(
        customer[["c_customer_sk", "c_current_addr_sk"]], left_on="ss_customer_sk", right_on="c_customer_sk"
    )
    base = base.merge(ca_filtered[["ca_address_sk"]], left_on="c_current_addr_sk", right_on="ca_address_sk")
    base = base.merge(item_filtered[["i_item_sk"]], left_on="ss_item_sk", right_on="i_item_sk")

    # Total sales
    total = base["ss_ext_sales_price"].sum()

    # Promotional sales
    promo_base = base.merge(promo_filtered[["p_promo_sk"]], left_on="ss_promo_sk", right_on="p_promo_sk")
    promotions = promo_base["ss_ext_sales_price"].sum()

    import pandas as pd

    result = pd.DataFrame(
        {
            "promotions": [promotions],
            "total": [total],
            "promo_pct": [promotions / total * 100 if total else 0],
        }
    )

    return result


# =============================================================================
# Q95: Web Orders from Multiple Warehouses WITH Returns
# =============================================================================


def q95_expression_impl(ctx: DataFrameContext) -> Any:
    """Q95: Web orders from multiple warehouses WITH returns (Polars).

    Similar to Q94 but finds orders that WERE returned (instead of not returned).
    Uses semi-join instead of anti-join for returns.

    Tables: web_sales, web_returns, date_dim, customer_address, web_site
    """
    from datetime import datetime, timedelta

    params = get_parameters(95)
    year = params.get("year", 1999)
    month = params.get("month", 2)
    states = params.get("states", ["TX", "OR", "AZ"])

    col = ctx.col
    lit = ctx.lit

    web_sales = ctx.get_table("web_sales")
    web_returns = ctx.get_table("web_returns")
    date_dim = ctx.get_table("date_dim")
    customer_address = ctx.get_table("customer_address")
    web_site = ctx.get_table("web_site")

    # Date filter
    start_date = datetime(year, month, 1).date()
    end_date = start_date + timedelta(days=60)

    date_filtered = date_dim.filter((col("d_date") >= lit(start_date)) & (col("d_date") <= lit(end_date)))

    # Filter customer_address by states
    ca_filtered = customer_address.filter(col("ca_state").is_in(states))

    # Filter web_site by company
    ws_filtered = web_site.filter(col("web_company_name") == lit("pri"))

    # Find orders shipped from multiple warehouses (CTE ws_wh)
    order_warehouses = web_sales.select(["ws_order_number", "ws_warehouse_sk"]).unique()
    multi_warehouse_orders = (
        order_warehouses.join(
            order_warehouses.rename({"ws_warehouse_sk": "ws_warehouse_sk_2"}),
            on="ws_order_number",
        )
        .filter(col("ws_warehouse_sk") != col("ws_warehouse_sk_2"))
        .select(["ws_order_number"])
        .unique()
    )

    # Get returned orders that are also multi-warehouse
    returned_orders = (
        web_returns.select(["wr_order_number"])
        .join(multi_warehouse_orders, left_on="wr_order_number", right_on="ws_order_number")
        .select(["wr_order_number"])
        .unique()
    )

    # Main query
    result = (
        web_sales.join(date_filtered, left_on="ws_ship_date_sk", right_on="d_date_sk")
        .join(ca_filtered, left_on="ws_ship_addr_sk", right_on="ca_address_sk")
        .join(ws_filtered, left_on="ws_web_site_sk", right_on="web_site_sk")
        # Semi-join: orders in multi_warehouse AND returned
        .join(multi_warehouse_orders, on="ws_order_number", how="semi")
        .join(returned_orders, left_on="ws_order_number", right_on="wr_order_number", how="semi")
    )

    # Aggregate
    result = result.select(
        [
            col("ws_order_number").n_unique().alias("order count"),
            ctx.sum("ws_ext_ship_cost").alias("total shipping cost"),
            ctx.sum("ws_net_profit").alias("total net profit"),
        ]
    )

    return result


def q95_pandas_impl(ctx: DataFrameContext) -> Any:
    """Q95: Web orders from multiple warehouses WITH returns (Pandas)."""
    from datetime import datetime, timedelta

    import pandas as pd

    params = get_parameters(95)
    year = params.get("year", 1999)
    month = params.get("month", 2)
    states = params.get("states", ["TX", "OR", "AZ"])

    web_sales = ctx.get_table("web_sales")
    web_returns = ctx.get_table("web_returns")
    date_dim = ctx.get_table("date_dim")
    customer_address = ctx.get_table("customer_address")
    web_site = ctx.get_table("web_site")

    # Date filter
    start_date = datetime(year, month, 1).date()
    end_date = start_date + timedelta(days=60)

    date_filtered = date_dim[(date_dim["d_date"] >= start_date) & (date_dim["d_date"] <= end_date)]
    ca_filtered = customer_address[customer_address["ca_state"].isin(states)]
    ws_filtered = web_site[web_site["web_company_name"] == "pri"]

    # Find orders with multiple warehouses (use ctx.groupby_size for Dask compatibility)
    order_warehouses = web_sales[["ws_order_number", "ws_warehouse_sk"]].drop_duplicates()
    ow_count = ctx.groupby_size(order_warehouses, "ws_order_number", name="wh_count")
    multi_warehouse_orders = ow_count[ow_count["wh_count"] > 1][["ws_order_number"]]

    # Get returned orders that are also multi-warehouse
    returned_orders = web_returns[["wr_order_number"]].drop_duplicates()
    returned_multi_wh = returned_orders.merge(
        multi_warehouse_orders,
        left_on="wr_order_number",
        right_on="ws_order_number",
    )[["wr_order_number"]]

    # Main join
    result = web_sales.merge(date_filtered[["d_date_sk"]], left_on="ws_ship_date_sk", right_on="d_date_sk")
    result = result.merge(ca_filtered[["ca_address_sk"]], left_on="ws_ship_addr_sk", right_on="ca_address_sk")
    result = result.merge(ws_filtered[["web_site_sk"]], left_on="ws_web_site_sk", right_on="web_site_sk")

    # Semi-join with multi-warehouse AND returned
    result = result.merge(multi_warehouse_orders, on="ws_order_number")
    result = result.merge(returned_multi_wh, left_on="ws_order_number", right_on="wr_order_number")

    # Aggregate
    order_count = result["ws_order_number"].nunique()
    total_shipping = result["ws_ext_ship_cost"].sum()
    total_profit = result["ws_net_profit"].sum()

    result = pd.DataFrame(
        {
            "order count": [order_count],
            "total shipping cost": [total_shipping],
            "total net profit": [total_profit],
        }
    )

    return result


# =============================================================================
# Q85: Web Sales Returns with Demographics
# =============================================================================


def q85_expression_impl(ctx: DataFrameContext) -> Any:
    """Q85: Web sales returns with demographics analysis (Polars).

    Analyzes web returns by reason, with complex filtering on customer
    demographics (marital status, education) and address (states, net profit).

    Tables: web_sales, web_returns, web_page, customer_demographics,
            customer_address, date_dim, reason
    """

    params = get_parameters(85)
    year = params.get("year", 1998)

    col = ctx.col
    lit = ctx.lit

    web_sales = ctx.get_table("web_sales")
    web_returns = ctx.get_table("web_returns")
    web_page = ctx.get_table("web_page")
    customer_demographics = ctx.get_table("customer_demographics")
    customer_address = ctx.get_table("customer_address")
    date_dim = ctx.get_table("date_dim")
    reason = ctx.get_table("reason")

    # Date filter
    date_filtered = date_dim.filter(col("d_year") == lit(year))

    # Join web_sales with web_returns
    ws_wr = web_sales.join(
        web_returns,
        left_on=["ws_item_sk", "ws_order_number"],
        right_on=["wr_item_sk", "wr_order_number"],
    )

    # Join with web_page
    ws_wr = ws_wr.join(web_page, left_on="ws_web_page_sk", right_on="wp_web_page_sk")

    # Join with date_dim
    ws_wr = ws_wr.join(date_filtered, left_on="ws_sold_date_sk", right_on="d_date_sk")

    # Join with customer_demographics (cd1 - refunded)
    # Use .columns for platform-agnostic column name access
    cd_columns = customer_demographics.columns
    cd1 = customer_demographics.rename({c: f"cd1_{c}" for c in cd_columns})
    ws_wr = ws_wr.join(cd1, left_on="wr_refunded_cdemo_sk", right_on="cd1_cd_demo_sk")

    # Join with customer_demographics (cd2 - returning)
    cd2 = customer_demographics.rename({c: f"cd2_{c}" for c in cd_columns})
    ws_wr = ws_wr.join(cd2, left_on="wr_returning_cdemo_sk", right_on="cd2_cd_demo_sk")

    # Join with customer_address
    ws_wr = ws_wr.join(customer_address, left_on="wr_refunded_addr_sk", right_on="ca_address_sk")

    # Join with reason
    ws_wr = ws_wr.join(reason, left_on="wr_reason_sk", right_on="r_reason_sk")

    # Complex filter conditions - simplified for implementation
    # Original has OR conditions on (marital_status, education, sales_price)
    # and OR conditions on (states, net_profit)
    # We'll implement a representative version
    demo_filter = (col("cd1_cd_marital_status") == col("cd2_cd_marital_status")) & (
        col("cd1_cd_education_status") == col("cd2_cd_education_status")
    )

    country_filter = col("ca_country") == lit("United States")

    filtered = ws_wr.filter(demo_filter & country_filter)

    # Aggregate by reason
    result = (
        filtered.group_by("r_reason_desc")
        .agg(
            [
                ctx.mean("ws_quantity").alias("avg_quantity"),
                ctx.mean("wr_refunded_cash").alias("avg_refunded_cash"),
                ctx.mean("wr_fee").alias("avg_fee"),
            ]
        )
        .with_columns(col("r_reason_desc").str.slice(0, 20).alias("reason_desc_short"))
        .sort(["reason_desc_short", "avg_quantity", "avg_refunded_cash", "avg_fee"])
        .head(100)
    )

    return result


def q85_pandas_impl(ctx: DataFrameContext) -> Any:
    """Q85: Web sales returns with demographics analysis (Pandas)."""
    params = get_parameters(85)
    year = params.get("year", 1998)

    web_sales = ctx.get_table("web_sales")
    web_returns = ctx.get_table("web_returns")
    web_page = ctx.get_table("web_page")
    customer_demographics = ctx.get_table("customer_demographics")
    customer_address = ctx.get_table("customer_address")
    date_dim = ctx.get_table("date_dim")
    reason = ctx.get_table("reason")

    # Date filter
    date_filtered = date_dim[date_dim["d_year"] == year]

    # Join web_sales with web_returns
    ws_wr = web_sales.merge(
        web_returns,
        left_on=["ws_item_sk", "ws_order_number"],
        right_on=["wr_item_sk", "wr_order_number"],
    )

    # Join with web_page
    ws_wr = ws_wr.merge(web_page, left_on="ws_web_page_sk", right_on="wp_web_page_sk")

    # Join with date_dim
    ws_wr = ws_wr.merge(date_filtered[["d_date_sk"]], left_on="ws_sold_date_sk", right_on="d_date_sk")

    # Join with customer_demographics (cd1)
    cd1 = customer_demographics.copy()
    cd1.columns = [f"cd1_{c}" for c in cd1.columns]
    ws_wr = ws_wr.merge(cd1, left_on="wr_refunded_cdemo_sk", right_on="cd1_cd_demo_sk")

    # Join with customer_demographics (cd2)
    cd2 = customer_demographics.copy()
    cd2.columns = [f"cd2_{c}" for c in cd2.columns]
    ws_wr = ws_wr.merge(cd2, left_on="wr_returning_cdemo_sk", right_on="cd2_cd_demo_sk")

    # Join with customer_address
    ws_wr = ws_wr.merge(customer_address, left_on="wr_refunded_addr_sk", right_on="ca_address_sk")

    # Join with reason
    ws_wr = ws_wr.merge(reason, left_on="wr_reason_sk", right_on="r_reason_sk")

    # Filter
    filtered = ws_wr[
        (ws_wr["cd1_cd_marital_status"] == ws_wr["cd2_cd_marital_status"])
        & (ws_wr["cd1_cd_education_status"] == ws_wr["cd2_cd_education_status"])
        & (ws_wr["ca_country"] == "United States")
    ]

    # Aggregate by reason
    result = filtered.groupby("r_reason_desc", as_index=False).agg(
        {
            "ws_quantity": "mean",
            "wr_refunded_cash": "mean",
            "wr_fee": "mean",
        }
    )
    result = result.rename(
        columns={
            "ws_quantity": "avg_quantity",
            "wr_refunded_cash": "avg_refunded_cash",
            "wr_fee": "avg_fee",
        }
    )
    result["reason_desc_short"] = result["r_reason_desc"].str[:20]
    result = result.sort_values(["reason_desc_short", "avg_quantity", "avg_refunded_cash", "avg_fee"]).head(100)

    return result


# =============================================================================
# Q66: Web/Catalog Sales Monthly Warehouse Analysis
# =============================================================================


def q66_expression_impl(ctx: DataFrameContext) -> Any:
    """Q66: Web/catalog sales monthly warehouse analysis (Polars).

    Union of web and catalog sales aggregated by warehouse and month,
    filtered by ship mode carriers and time range.

    Tables: web_sales, catalog_sales, warehouse, date_dim, time_dim, ship_mode
    """

    params = get_parameters(66)
    year = params.get("year", 2001)
    ship_carriers = params.get("ship_carriers", ["DIAMOND", "AIRBORNE"])
    time_start = params.get("time_start", 30838)

    col = ctx.col
    lit = ctx.lit

    web_sales = ctx.get_table("web_sales")
    catalog_sales = ctx.get_table("catalog_sales")
    warehouse = ctx.get_table("warehouse")
    date_dim = ctx.get_table("date_dim")
    time_dim = ctx.get_table("time_dim")
    ship_mode = ctx.get_table("ship_mode")

    # Filters
    date_filtered = date_dim.filter(col("d_year") == lit(year))
    time_filtered = time_dim.filter((col("t_time") >= lit(time_start)) & (col("t_time") <= lit(time_start + 28800)))
    sm_filtered = ship_mode.filter(col("sm_carrier").is_in(ship_carriers))
    carriers_str = ",".join(ship_carriers)

    # Helper to build monthly aggregations
    def build_monthly_aggs(sales_col: str, net_col: str, qty_col: str):
        """Build monthly sum expressions."""
        aggs = []
        for month in range(1, 13):
            month_names = ["jan", "feb", "mar", "apr", "may", "jun", "jul", "aug", "sep", "oct", "nov", "dec"]
            mname = month_names[month - 1]
            aggs.append(
                ctx.when(col("d_moy") == lit(month))
                .then(col(sales_col) * col(qty_col))
                .otherwise(lit(0))
                .sum()
                .alias(f"{mname}_sales")
            )
            aggs.append(
                ctx.when(col("d_moy") == lit(month))
                .then(col(net_col) * col(qty_col))
                .otherwise(lit(0))
                .sum()
                .alias(f"{mname}_net")
            )
        return aggs

    # Web sales aggregation
    ws = (
        web_sales.join(warehouse, left_on="ws_warehouse_sk", right_on="w_warehouse_sk")
        .join(date_filtered, left_on="ws_sold_date_sk", right_on="d_date_sk")
        .join(time_filtered, left_on="ws_sold_time_sk", right_on="t_time_sk")
        .join(sm_filtered, left_on="ws_ship_mode_sk", right_on="sm_ship_mode_sk")
    )

    ws_agg = (
        ws.group_by(["w_warehouse_name", "w_warehouse_sq_ft", "w_city", "w_county", "w_state", "w_country", "d_year"])
        .agg(build_monthly_aggs("ws_ext_sales_price", "ws_net_profit", "ws_quantity"))
        .with_columns(
            [
                lit(carriers_str).alias("ship_carriers"),
                col("d_year").alias("year"),
            ]
        )
    )

    # Catalog sales aggregation
    cs = (
        catalog_sales.join(warehouse, left_on="cs_warehouse_sk", right_on="w_warehouse_sk")
        .join(date_filtered, left_on="cs_sold_date_sk", right_on="d_date_sk")
        .join(time_filtered, left_on="cs_sold_time_sk", right_on="t_time_sk")
        .join(sm_filtered, left_on="cs_ship_mode_sk", right_on="sm_ship_mode_sk")
    )

    cs_agg = (
        cs.group_by(["w_warehouse_name", "w_warehouse_sq_ft", "w_city", "w_county", "w_state", "w_country", "d_year"])
        .agg(build_monthly_aggs("cs_ext_sales_price", "cs_net_profit", "cs_quantity"))
        .with_columns(
            [
                lit(carriers_str).alias("ship_carriers"),
                col("d_year").alias("year"),
            ]
        )
    )

    # Union
    combined = ctx.concat([ws_agg, cs_agg])

    # Final aggregation
    group_cols = [
        "w_warehouse_name",
        "w_warehouse_sq_ft",
        "w_city",
        "w_county",
        "w_state",
        "w_country",
        "ship_carriers",
        "year",
    ]

    month_names = ["jan", "feb", "mar", "apr", "may", "jun", "jul", "aug", "sep", "oct", "nov", "dec"]
    agg_exprs = []
    for mname in month_names:
        agg_exprs.append(ctx.sum(f"{mname}_sales").alias(f"{mname}_sales"))
        agg_exprs.append(ctx.sum(f"{mname}_net").alias(f"{mname}_net"))

    result = combined.group_by(group_cols).agg(agg_exprs).sort("w_warehouse_name").head(100)

    return result


def q66_pandas_impl(ctx: DataFrameContext) -> Any:
    """Q66: Web/catalog sales monthly warehouse analysis (Pandas)."""

    params = get_parameters(66)
    year = params.get("year", 2001)
    ship_carriers = params.get("ship_carriers", ["DIAMOND", "AIRBORNE"])
    time_start = params.get("time_start", 30838)

    web_sales = ctx.get_table("web_sales")
    catalog_sales = ctx.get_table("catalog_sales")
    warehouse = ctx.get_table("warehouse")
    date_dim = ctx.get_table("date_dim")
    time_dim = ctx.get_table("time_dim")
    ship_mode = ctx.get_table("ship_mode")

    # Filters
    date_filtered = date_dim[date_dim["d_year"] == year]
    time_filtered = time_dim[(time_dim["t_time"] >= time_start) & (time_dim["t_time"] <= time_start + 28800)]
    sm_filtered = ship_mode[ship_mode["sm_carrier"].isin(ship_carriers)]
    carriers_str = ",".join(ship_carriers)

    month_names = ["jan", "feb", "mar", "apr", "may", "jun", "jul", "aug", "sep", "oct", "nov", "dec"]

    def process_channel(sales_df, wh_col, date_col, time_col, sm_col, sales_col, net_col, qty_col):
        """Process a sales channel."""
        df = sales_df.merge(warehouse, left_on=wh_col, right_on="w_warehouse_sk")
        df = df.merge(date_filtered[["d_date_sk", "d_year", "d_moy"]], left_on=date_col, right_on="d_date_sk")
        df = df.merge(time_filtered[["t_time_sk"]], left_on=time_col, right_on="t_time_sk")
        df = df.merge(sm_filtered[["sm_ship_mode_sk"]], left_on=sm_col, right_on="sm_ship_mode_sk")

        # Add monthly columns - use default argument to bind month value
        for month in range(1, 13):
            mname = month_names[month - 1]
            df[f"{mname}_sales"] = df.apply(
                lambda r, m=month: r[sales_col] * r[qty_col] if r["d_moy"] == m else 0, axis=1
            )
            df[f"{mname}_net"] = df.apply(lambda r, m=month: r[net_col] * r[qty_col] if r["d_moy"] == m else 0, axis=1)

        # Aggregate
        group_cols = ["w_warehouse_name", "w_warehouse_sq_ft", "w_city", "w_county", "w_state", "w_country", "d_year"]
        agg_dict = {f"{m}_sales": "sum" for m in month_names}
        agg_dict.update({f"{m}_net": "sum" for m in month_names})

        result = df.groupby(group_cols, as_index=False).agg(agg_dict)
        result["ship_carriers"] = carriers_str
        result["year"] = result["d_year"]
        return result

    # Process web sales
    ws_agg = process_channel(
        web_sales,
        "ws_warehouse_sk",
        "ws_sold_date_sk",
        "ws_sold_time_sk",
        "ws_ship_mode_sk",
        "ws_ext_sales_price",
        "ws_net_profit",
        "ws_quantity",
    )

    # Process catalog sales
    cs_agg = process_channel(
        catalog_sales,
        "cs_warehouse_sk",
        "cs_sold_date_sk",
        "cs_sold_time_sk",
        "cs_ship_mode_sk",
        "cs_ext_sales_price",
        "cs_net_profit",
        "cs_quantity",
    )

    # Union
    combined = ctx.concat([ws_agg, cs_agg])

    # Final aggregation
    group_cols = [
        "w_warehouse_name",
        "w_warehouse_sq_ft",
        "w_city",
        "w_county",
        "w_state",
        "w_country",
        "ship_carriers",
        "year",
    ]
    agg_dict = {f"{m}_sales": "sum" for m in month_names}
    agg_dict.update({f"{m}_net": "sum" for m in month_names})

    result = combined.groupby(group_cols, as_index=False).agg(agg_dict)
    result = result.sort_values("w_warehouse_name").head(100)

    return result


# =============================================================================
# Q8: Store Sales Net Profit by Store with Zip Code Analysis
# =============================================================================


def q8_expression_impl(ctx: DataFrameContext) -> Any:
    """Q8: Store sales net profit analysis with zip code filtering (Polars)."""

    params = get_parameters(8)
    year = params.get("year", 1998)
    qoy = params.get("qoy", 2)
    zip_codes = params.get("zip_codes", ["24128", "76232", "65084", "87816", "83926"])

    # Get tables
    store_sales = ctx.get_table("store_sales")
    date_dim = ctx.get_table("date_dim")
    store = ctx.get_table("store")
    customer_address = ctx.get_table("customer_address")
    customer = ctx.get_table("customer")

    col = ctx.col
    lit = ctx.lit

    # Filter date dimension
    date_filtered = date_dim.filter((col("d_qoy") == lit(qoy)) & (col("d_year") == lit(year)))

    # Find zip codes with preferred customers (count > 10)
    preferred_zips = (
        customer_address.join(
            customer.filter(col("c_preferred_cust_flag") == lit("Y")),
            left_on="ca_address_sk",
            right_on="c_current_addr_sk",
        )
        .with_columns(col("ca_zip").cast_string().str.slice(0, 5).alias("zip5"))
        .group_by("zip5")
        .agg(ctx.len().alias("cnt"))
        .filter(col("cnt") > lit(10))
        .select("zip5")
    )

    # Create list of target zip codes (first 5 chars)
    target_zips = customer_address.filter(col("ca_zip").cast_string().str.slice(0, 5).is_in(zip_codes)).select(
        col("ca_zip").cast_string().str.slice(0, 5).alias("zip5")
    )

    # Intersect: zip codes that are both target zips and preferred customer zips
    valid_zips = target_zips.join(preferred_zips, on="zip5", how="semi")

    # Join store_sales with date, store
    result = (
        store_sales.join(date_filtered, left_on="ss_sold_date_sk", right_on="d_date_sk")
        .join(store, left_on="ss_store_sk", right_on="s_store_sk")
        .with_columns(col("s_zip").cast_string().str.slice(0, 2).alias("store_zip2"))
    )

    # Join with valid zips by comparing first 2 chars
    valid_zips_2char = valid_zips.with_columns(col("zip5").str.slice(0, 2).alias("zip2")).select("zip2").unique()

    result = result.join(valid_zips_2char, left_on="store_zip2", right_on="zip2")

    # Aggregate
    result = (
        result.group_by("s_store_name")
        .agg(ctx.sum("ss_net_profit").alias("sum_net_profit"))
        .sort("s_store_name")
        .head(100)
    )

    return result


def q8_pandas_impl(ctx: DataFrameContext) -> Any:
    """Q8: Store sales net profit analysis with zip code filtering (Pandas)."""
    params = get_parameters(8)
    year = params.get("year", 1998)
    qoy = params.get("qoy", 2)
    zip_codes = params.get("zip_codes", ["24128", "76232", "65084", "87816", "83926"])

    # Get tables
    store_sales = ctx.get_table("store_sales")
    date_dim = ctx.get_table("date_dim")
    store = ctx.get_table("store")
    customer_address = ctx.get_table("customer_address")
    customer = ctx.get_table("customer")

    # Filter date dimension
    date_filtered = date_dim[(date_dim["d_qoy"] == qoy) & (date_dim["d_year"] == year)]

    # Find zip codes with preferred customers (count > 10)
    preferred_cust = customer[customer["c_preferred_cust_flag"] == "Y"]
    ca_pref = customer_address.merge(
        preferred_cust[["c_customer_sk", "c_current_addr_sk"]],
        left_on="ca_address_sk",
        right_on="c_current_addr_sk",
    )
    ca_pref["zip5"] = ca_pref["ca_zip"].str[:5]
    # Use ctx.groupby_size for Dask compatibility
    zip_counts = ctx.groupby_size(ca_pref, "zip5", name="cnt")
    preferred_zips = set(zip_counts[zip_counts["cnt"] > 10]["zip5"])

    # Target zip codes (first 5 chars)
    target_zips = {z[:5] for z in zip_codes}

    # Intersect
    valid_zips = target_zips & preferred_zips
    valid_zips_2char = {z[:2] for z in valid_zips}

    # Join store_sales with date, store
    result = store_sales.merge(date_filtered[["d_date_sk"]], left_on="ss_sold_date_sk", right_on="d_date_sk")
    result = result.merge(store, left_on="ss_store_sk", right_on="s_store_sk")
    result["store_zip2"] = result["s_zip"].str[:2]

    # Filter by valid zip prefixes
    result = result[result["store_zip2"].isin(valid_zips_2char)]

    # Aggregate
    result = (
        result.groupby("s_store_name", as_index=False)
        .agg(sum_net_profit=("ss_net_profit", "sum"))
        .sort_values("s_store_name")
        .head(100)
    )

    return result


# =============================================================================
# Q9: Store Sales Extended Price Conditional Analysis
# =============================================================================


def q9_expression_impl(ctx: DataFrameContext) -> Any:
    """Q9: Extended price analysis by quantity buckets (Polars)."""

    params = get_parameters(9)
    quantity_ranges = params.get("quantity_ranges", [(1, 20), (21, 40), (41, 60), (61, 80), (81, 100)])
    # Row count thresholds for CASE-WHEN (simplified for DataFrame)
    thresholds = params.get("thresholds", [1000, 1000, 1000, 1000, 1000])

    store_sales = ctx.get_table("store_sales")
    reason = ctx.get_table("reason")

    col = ctx.col
    lit = ctx.lit

    # Compute aggregations for each quantity bucket
    buckets = []
    for i, (q_min, q_max) in enumerate(quantity_ranges):
        bucket = store_sales.filter((col("ss_quantity") >= lit(q_min)) & (col("ss_quantity") <= lit(q_max)))
        cnt_df = bucket.select(ctx.len().alias("cnt"))
        cnt = cnt_df.scalar(0, 0) if hasattr(cnt_df, "scalar") else len(bucket)

        # Determine which average to return based on count
        threshold = thresholds[i] if i < len(thresholds) else 1000

        if cnt > threshold:
            # Return avg of ss_ext_discount_amt
            avg_val = bucket.select(ctx.mean("ss_ext_discount_amt").alias("avg_val"))
        else:
            # Return avg of ss_net_paid
            avg_val = bucket.select(ctx.mean("ss_net_paid").alias("avg_val"))

        if hasattr(avg_val, "scalar"):
            val = avg_val.scalar(0, 0)
        else:
            val = avg_val.to_numpy()[0, 0] if len(avg_val) > 0 else None

        buckets.append(val)

    # Join with reason table (r_reason_sk = 1) to get single row
    result = reason.filter(col("r_reason_sk") == lit(1)).select(
        lit(buckets[0]).alias("bucket1") if len(buckets) > 0 else lit(None).alias("bucket1"),
        lit(buckets[1]).alias("bucket2") if len(buckets) > 1 else lit(None).alias("bucket2"),
        lit(buckets[2]).alias("bucket3") if len(buckets) > 2 else lit(None).alias("bucket3"),
        lit(buckets[3]).alias("bucket4") if len(buckets) > 3 else lit(None).alias("bucket4"),
        lit(buckets[4]).alias("bucket5") if len(buckets) > 4 else lit(None).alias("bucket5"),
    )

    return result


def q9_pandas_impl(ctx: DataFrameContext) -> Any:
    """Q9: Extended price analysis by quantity buckets (Pandas)."""
    import pandas as pd

    params = get_parameters(9)
    quantity_ranges = params.get("quantity_ranges", [(1, 20), (21, 40), (41, 60), (61, 80), (81, 100)])
    thresholds = params.get("thresholds", [1000, 1000, 1000, 1000, 1000])

    store_sales = ctx.get_table("store_sales")

    # Compute aggregations for each quantity bucket
    buckets = []
    for i, (q_min, q_max) in enumerate(quantity_ranges):
        bucket = store_sales[(store_sales["ss_quantity"] >= q_min) & (store_sales["ss_quantity"] <= q_max)]
        cnt = len(bucket)

        threshold = thresholds[i] if i < len(thresholds) else 1000

        avg_val = bucket["ss_ext_discount_amt"].mean() if cnt > threshold else bucket["ss_net_paid"].mean()

        buckets.append(avg_val)

    # Create single row result (matching reason table with r_reason_sk = 1)
    result = pd.DataFrame(
        {
            "bucket1": [buckets[0]] if len(buckets) > 0 else [None],
            "bucket2": [buckets[1]] if len(buckets) > 1 else [None],
            "bucket3": [buckets[2]] if len(buckets) > 2 else [None],
            "bucket4": [buckets[3]] if len(buckets) > 3 else [None],
            "bucket5": [buckets[4]] if len(buckets) > 4 else [None],
        }
    )

    return result


# =============================================================================
# Q28: Store Sales Extended Price Analysis by Quantity Buckets
# =============================================================================


def q28_expression_impl(ctx: DataFrameContext) -> Any:
    """Q28: Extended price analysis across 6 quantity buckets (Polars)."""

    params = get_parameters(28)
    quantity_ranges = params.get("quantity_ranges", [(0, 5), (6, 10), (11, 15), (16, 20), (21, 25), (26, 30)])
    list_prices = params.get("list_prices", [90, 91, 92, 93, 94, 95])
    coupon_amts = params.get("coupon_amts", [1000, 2000, 3000, 4000, 5000, 6000])
    wholesale_costs = params.get("wholesale_costs", [10, 20, 30, 40, 50, 60])

    store_sales = ctx.get_table("store_sales")
    col = ctx.col
    lit = ctx.lit

    # Compute stats for each bucket
    bucket_results = []
    for i, (q_min, q_max) in enumerate(quantity_ranges):
        lp = list_prices[i] if i < len(list_prices) else 90
        ca = coupon_amts[i] if i < len(coupon_amts) else 1000
        wc = wholesale_costs[i] if i < len(wholesale_costs) else 10

        bucket = store_sales.filter(
            (col("ss_quantity") >= lit(q_min))
            & (col("ss_quantity") <= lit(q_max))
            & (
                ((col("ss_list_price") >= lit(lp)) & (col("ss_list_price") <= lit(lp + 10)))
                | ((col("ss_coupon_amt") >= lit(ca)) & (col("ss_coupon_amt") <= lit(ca + 1000)))
                | ((col("ss_wholesale_cost") >= lit(wc)) & (col("ss_wholesale_cost") <= lit(wc + 20)))
            )
        )

        stats = bucket.select(
            ctx.mean("ss_list_price").alias(f"B{i + 1}_LP"),
            ctx.len().alias(f"B{i + 1}_CNT"),
            col("ss_list_price").n_unique().alias(f"B{i + 1}_CNTD"),
        )

        bucket_results.append(stats)

    # Cross join all bucket results
    result = bucket_results[0]
    for br in bucket_results[1:]:
        result = result.join(br, how="cross")

    return result


def q28_pandas_impl(ctx: DataFrameContext) -> Any:
    """Q28: Extended price analysis across 6 quantity buckets (Pandas)."""
    import pandas as pd

    params = get_parameters(28)
    quantity_ranges = params.get("quantity_ranges", [(0, 5), (6, 10), (11, 15), (16, 20), (21, 25), (26, 30)])
    list_prices = params.get("list_prices", [90, 91, 92, 93, 94, 95])
    coupon_amts = params.get("coupon_amts", [1000, 2000, 3000, 4000, 5000, 6000])
    wholesale_costs = params.get("wholesale_costs", [10, 20, 30, 40, 50, 60])

    store_sales = ctx.get_table("store_sales")

    # Compute stats for each bucket
    result_dict = {}
    for i, (q_min, q_max) in enumerate(quantity_ranges):
        lp = list_prices[i] if i < len(list_prices) else 90
        ca = coupon_amts[i] if i < len(coupon_amts) else 1000
        wc = wholesale_costs[i] if i < len(wholesale_costs) else 10

        bucket = store_sales[
            (store_sales["ss_quantity"] >= q_min)
            & (store_sales["ss_quantity"] <= q_max)
            & (
                ((store_sales["ss_list_price"] >= lp) & (store_sales["ss_list_price"] <= lp + 10))
                | ((store_sales["ss_coupon_amt"] >= ca) & (store_sales["ss_coupon_amt"] <= ca + 1000))
                | ((store_sales["ss_wholesale_cost"] >= wc) & (store_sales["ss_wholesale_cost"] <= wc + 20))
            )
        ]

        result_dict[f"B{i + 1}_LP"] = bucket["ss_list_price"].mean()
        result_dict[f"B{i + 1}_CNT"] = len(bucket)
        result_dict[f"B{i + 1}_CNTD"] = bucket["ss_list_price"].nunique()

    # Create single row result
    result = pd.DataFrame([result_dict])

    return result


# =============================================================================
# Q88: Store Sales Time Period Analysis
# =============================================================================


def q88_expression_impl(ctx: DataFrameContext) -> Any:
    """Q88: Store sales count by 8 half-hour time periods (Polars)."""

    params = get_parameters(88)
    dep_counts = params.get("dep_counts", [0, 1, 3])
    store_name = params.get("store_name", "ese")

    store_sales = ctx.get_table("store_sales")
    household_demographics = ctx.get_table("household_demographics")
    time_dim = ctx.get_table("time_dim")
    store = ctx.get_table("store")

    col = ctx.col
    lit = ctx.lit

    # Filter store
    store_filtered = store.filter(col("s_store_name") == lit(store_name))

    # Filter household demographics - matching dep_count and vehicle_count constraint
    hd_filters = None
    for dc in dep_counts:
        cond = (col("hd_dep_count") == lit(dc)) & (col("hd_vehicle_count") <= lit(dc + 2))
        hd_filters = cond if hd_filters is None else hd_filters | cond

    hd_filtered = household_demographics.filter(hd_filters)

    # Join base tables
    base = (
        store_sales.join(hd_filtered, left_on="ss_hdemo_sk", right_on="hd_demo_sk")
        .join(store_filtered, left_on="ss_store_sk", right_on="s_store_sk")
        .join(time_dim, left_on="ss_sold_time_sk", right_on="t_time_sk")
    )

    # Time periods: 8:30-9, 9-9:30, 9:30-10, 10-10:30, 10:30-11, 11-11:30, 11:30-12, 12-12:30
    time_periods = [
        ("h8_30_to_9", 8, 30, 60),  # hour=8, minute>=30
        ("h9_to_9_30", 9, 0, 30),  # hour=9, minute<30
        ("h9_30_to_10", 9, 30, 60),  # hour=9, minute>=30
        ("h10_to_10_30", 10, 0, 30),  # hour=10, minute<30
        ("h10_30_to_11", 10, 30, 60),  # hour=10, minute>=30
        ("h11_to_11_30", 11, 0, 30),  # hour=11, minute<30
        ("h11_30_to_12", 11, 30, 60),  # hour=11, minute>=30
        ("h12_to_12_30", 12, 0, 30),  # hour=12, minute<30
    ]

    # Compute counts for each period
    period_results = []
    for name, hour, min_start, min_end in time_periods:
        if min_end == 60:
            # minute >= min_start
            period_count = base.filter((col("t_hour") == lit(hour)) & (col("t_minute") >= lit(min_start))).select(
                ctx.len().alias(name)
            )
        else:
            # minute < min_end (i.e., minute < 30)
            period_count = base.filter((col("t_hour") == lit(hour)) & (col("t_minute") < lit(min_end))).select(
                ctx.len().alias(name)
            )
        period_results.append(period_count)

    # Cross join all period results
    result = period_results[0]
    for pr in period_results[1:]:
        result = result.join(pr, how="cross")

    return result


def q88_pandas_impl(ctx: DataFrameContext) -> Any:
    """Q88: Store sales count by 8 half-hour time periods (Pandas)."""
    import pandas as pd

    params = get_parameters(88)
    dep_counts = params.get("dep_counts", [0, 1, 3])
    store_name = params.get("store_name", "ese")

    store_sales = ctx.get_table("store_sales")
    household_demographics = ctx.get_table("household_demographics")
    time_dim = ctx.get_table("time_dim")
    store = ctx.get_table("store")

    # Filter store
    store_filtered = store[store["s_store_name"] == store_name]

    # Filter household demographics
    hd_mask = pd.Series([False] * len(household_demographics))
    for dc in dep_counts:
        cond = (household_demographics["hd_dep_count"] == dc) & (household_demographics["hd_vehicle_count"] <= dc + 2)
        hd_mask = hd_mask | cond
    hd_filtered = household_demographics[hd_mask]

    # Join base tables
    base = store_sales.merge(hd_filtered[["hd_demo_sk"]], left_on="ss_hdemo_sk", right_on="hd_demo_sk")
    base = base.merge(store_filtered[["s_store_sk"]], left_on="ss_store_sk", right_on="s_store_sk")
    base = base.merge(time_dim[["t_time_sk", "t_hour", "t_minute"]], left_on="ss_sold_time_sk", right_on="t_time_sk")

    # Time periods
    time_periods = [
        ("h8_30_to_9", 8, 30, 60),
        ("h9_to_9_30", 9, 0, 30),
        ("h9_30_to_10", 9, 30, 60),
        ("h10_to_10_30", 10, 0, 30),
        ("h10_30_to_11", 10, 30, 60),
        ("h11_to_11_30", 11, 0, 30),
        ("h11_30_to_12", 11, 30, 60),
        ("h12_to_12_30", 12, 0, 30),
    ]

    # Compute counts for each period
    result_dict = {}
    for name, hour, min_start, min_end in time_periods:
        if min_end == 60:
            period_data = base[(base["t_hour"] == hour) & (base["t_minute"] >= min_start)]
        else:
            period_data = base[(base["t_hour"] == hour) & (base["t_minute"] < min_end)]
        result_dict[name] = len(period_data)

    result = pd.DataFrame([result_dict])

    return result


# =============================================================================
# Q14: Cross-channel Item Sales Analysis (Simplified - Part 1)
# =============================================================================


def q14_expression_impl(ctx: DataFrameContext) -> Any:
    """Q14: Cross-channel item sales analysis (Polars).

    Finds items sold across all three channels and computes sales above average.
    """

    from .rollup_helper import expand_rollup_expression

    params = get_parameters(14)
    year = params.get("year", 1999)

    # Get tables
    store_sales = ctx.get_table("store_sales")
    catalog_sales = ctx.get_table("catalog_sales")
    web_sales = ctx.get_table("web_sales")
    item = ctx.get_table("item")
    date_dim = ctx.get_table("date_dim")

    col = ctx.col
    lit = ctx.lit

    # Filter date dimension for the 3-year period
    date_filtered = date_dim.filter((col("d_year") >= lit(year)) & (col("d_year") <= lit(year + 2)))

    # Get items from each channel with brand/class/category
    ss_items = (
        store_sales.join(date_filtered, left_on="ss_sold_date_sk", right_on="d_date_sk")
        .join(item, left_on="ss_item_sk", right_on="i_item_sk")
        .select("i_brand_id", "i_class_id", "i_category_id")
        .unique()
    )

    cs_items = (
        catalog_sales.join(date_filtered, left_on="cs_sold_date_sk", right_on="d_date_sk")
        .join(item, left_on="cs_item_sk", right_on="i_item_sk")
        .select("i_brand_id", "i_class_id", "i_category_id")
        .unique()
    )

    ws_items = (
        web_sales.join(date_filtered, left_on="ws_sold_date_sk", right_on="d_date_sk")
        .join(item, left_on="ws_item_sk", right_on="i_item_sk")
        .select("i_brand_id", "i_class_id", "i_category_id")
        .unique()
    )

    # Intersect: items in all three channels
    cross_items = ss_items.join(cs_items, on=["i_brand_id", "i_class_id", "i_category_id"], how="semi")
    cross_items = cross_items.join(ws_items, on=["i_brand_id", "i_class_id", "i_category_id"], how="semi")

    # Get item_sk for cross items
    cross_item_sks = item.join(cross_items, on=["i_brand_id", "i_class_id", "i_category_id"]).select("i_item_sk")

    # Compute average sales across all channels
    ss_for_avg = store_sales.join(date_filtered, left_on="ss_sold_date_sk", right_on="d_date_sk").select(
        (col("ss_quantity") * col("ss_list_price")).alias("sales")
    )
    cs_for_avg = catalog_sales.join(date_filtered, left_on="cs_sold_date_sk", right_on="d_date_sk").select(
        (col("cs_quantity") * col("cs_list_price")).alias("sales")
    )
    ws_for_avg = web_sales.join(date_filtered, left_on="ws_sold_date_sk", right_on="d_date_sk").select(
        (col("ws_quantity") * col("ws_list_price")).alias("sales")
    )
    all_sales = ctx.concat([ss_for_avg, cs_for_avg, ws_for_avg])
    avg_sales = all_sales.select(ctx.mean("sales").alias("average_sales"))

    avg_sales_val = avg_sales.scalar(0, 0) if hasattr(avg_sales, "scalar") else avg_sales.to_numpy()[0, 0]

    # Filter for target month in year+2
    date_target = date_dim.filter((col("d_year") == lit(year + 2)) & (col("d_moy") == lit(11)))

    # Compute sales for each channel in target period for cross items
    def channel_sales(sales_df, date_col, item_col, qty_col, price_col, channel_name):
        df = (
            sales_df.join(date_target, left_on=date_col, right_on="d_date_sk")
            .join(cross_item_sks, left_on=item_col, right_on="i_item_sk", how="semi")
            .join(item, left_on=item_col, right_on="i_item_sk")
        )
        grouped = df.group_by(["i_brand_id", "i_class_id", "i_category_id"]).agg(
            (ctx.sum(qty_col) * ctx.mean(price_col)).alias("sales"),
            ctx.len().alias("number_sales"),
        )
        # Filter by average sales
        grouped = grouped.filter(col("sales") > lit(avg_sales_val))
        grouped = grouped.with_columns(lit(channel_name).alias("channel"))
        return grouped

    ss_result = channel_sales(store_sales, "ss_sold_date_sk", "ss_item_sk", "ss_quantity", "ss_list_price", "store")
    cs_result = channel_sales(catalog_sales, "cs_sold_date_sk", "cs_item_sk", "cs_quantity", "cs_list_price", "catalog")
    ws_result = channel_sales(web_sales, "ws_sold_date_sk", "ws_item_sk", "ws_quantity", "ws_list_price", "web")

    # Union results
    combined = ctx.concat([ss_result, cs_result, ws_result])

    # Apply ROLLUP
    group_cols = ["channel", "i_brand_id", "i_class_id", "i_category_id"]
    agg_exprs = [
        ctx.sum("sales").alias("sum_sales"),
        ctx.sum("number_sales").alias("sum_number_sales"),
    ]

    result = expand_rollup_expression(combined, group_cols, agg_exprs, ctx)
    result = result.sort(["channel", "i_brand_id", "i_class_id", "i_category_id"]).head(100)

    return result


def q14_pandas_impl(ctx: DataFrameContext) -> Any:
    """Q14: Cross-channel item sales analysis (Pandas)."""

    from .rollup_helper import expand_rollup_pandas

    params = get_parameters(14)
    year = params.get("year", 1999)

    # Get tables
    store_sales = ctx.get_table("store_sales")
    catalog_sales = ctx.get_table("catalog_sales")
    web_sales = ctx.get_table("web_sales")
    item = ctx.get_table("item")
    date_dim = ctx.get_table("date_dim")

    # Filter date dimension
    date_filtered = date_dim[(date_dim["d_year"] >= year) & (date_dim["d_year"] <= year + 2)]

    # Get items from each channel
    ss_items = store_sales.merge(date_filtered[["d_date_sk"]], left_on="ss_sold_date_sk", right_on="d_date_sk")
    ss_items = ss_items.merge(item, left_on="ss_item_sk", right_on="i_item_sk")
    ss_items = ss_items[["i_brand_id", "i_class_id", "i_category_id"]].drop_duplicates()

    cs_items = catalog_sales.merge(date_filtered[["d_date_sk"]], left_on="cs_sold_date_sk", right_on="d_date_sk")
    cs_items = cs_items.merge(item, left_on="cs_item_sk", right_on="i_item_sk")
    cs_items = cs_items[["i_brand_id", "i_class_id", "i_category_id"]].drop_duplicates()

    ws_items = web_sales.merge(date_filtered[["d_date_sk"]], left_on="ws_sold_date_sk", right_on="d_date_sk")
    ws_items = ws_items.merge(item, left_on="ws_item_sk", right_on="i_item_sk")
    ws_items = ws_items[["i_brand_id", "i_class_id", "i_category_id"]].drop_duplicates()

    # Intersect
    cross_items = ss_items.merge(cs_items, on=["i_brand_id", "i_class_id", "i_category_id"])
    cross_items = cross_items.merge(ws_items, on=["i_brand_id", "i_class_id", "i_category_id"])

    # Get item SKs
    cross_item_sks = item.merge(cross_items, on=["i_brand_id", "i_class_id", "i_category_id"])["i_item_sk"]

    # Compute average sales
    ss_sales = store_sales.merge(date_filtered[["d_date_sk"]], left_on="ss_sold_date_sk", right_on="d_date_sk")
    ss_sales["sales"] = ss_sales["ss_quantity"] * ss_sales["ss_list_price"]

    cs_sales = catalog_sales.merge(date_filtered[["d_date_sk"]], left_on="cs_sold_date_sk", right_on="d_date_sk")
    cs_sales["sales"] = cs_sales["cs_quantity"] * cs_sales["cs_list_price"]

    ws_sales = web_sales.merge(date_filtered[["d_date_sk"]], left_on="ws_sold_date_sk", right_on="d_date_sk")
    ws_sales["sales"] = ws_sales["ws_quantity"] * ws_sales["ws_list_price"]

    # Compute average sales across all channels (Dask-compatible)
    all_sales = ctx.concat([ss_sales[["sales"]], cs_sales[["sales"]], ws_sales[["sales"]]])
    avg_sales_val = all_sales["sales"].mean()
    # Compute if lazy (Dask)
    if hasattr(avg_sales_val, "compute"):
        avg_sales_val = avg_sales_val.compute()

    # Target date
    date_target = date_dim[(date_dim["d_year"] == year + 2) & (date_dim["d_moy"] == 11)]

    def channel_sales(sales_df, date_col, item_col, qty_col, price_col, channel_name):
        df = sales_df.merge(date_target[["d_date_sk"]], left_on=date_col, right_on="d_date_sk")
        df = df[df[item_col].isin(cross_item_sks)]
        df = df.merge(item, left_on=item_col, right_on="i_item_sk")
        df["sales"] = df[qty_col] * df[price_col]

        grouped = df.groupby(["i_brand_id", "i_class_id", "i_category_id"], as_index=False).agg(
            sales=("sales", "sum"),
            number_sales=(item_col, "count"),
        )
        grouped = grouped[grouped["sales"] > avg_sales_val]
        grouped["channel"] = channel_name
        return grouped

    ss_result = channel_sales(store_sales, "ss_sold_date_sk", "ss_item_sk", "ss_quantity", "ss_list_price", "store")
    cs_result = channel_sales(catalog_sales, "cs_sold_date_sk", "cs_item_sk", "cs_quantity", "cs_list_price", "catalog")
    ws_result = channel_sales(web_sales, "ws_sold_date_sk", "ws_item_sk", "ws_quantity", "ws_list_price", "web")

    combined = ctx.concat([ss_result, cs_result, ws_result])

    # Apply ROLLUP
    group_cols = ["channel", "i_brand_id", "i_class_id", "i_category_id"]
    agg_dict = {
        "sum_sales": ("sales", "sum"),
        "sum_number_sales": ("number_sales", "sum"),
    }

    result = expand_rollup_pandas(combined, group_cols, agg_dict, ctx)
    result = result.sort_values(["channel", "i_brand_id", "i_class_id", "i_category_id"]).head(100)

    return result


# =============================================================================
# Q23: Frequent Items and Best Customers Analysis
# =============================================================================


def q23_expression_impl(ctx: DataFrameContext) -> Any:
    """Q23: Sum of catalog+web sales for frequent items by best customers (Polars)."""

    params = get_parameters(23)
    year = params.get("year", 2000)
    month = params.get("month", 4)
    top_percent = params.get("top_percent", 95)

    # Get tables
    store_sales = ctx.get_table("store_sales")
    catalog_sales = ctx.get_table("catalog_sales")
    web_sales = ctx.get_table("web_sales")
    customer = ctx.get_table("customer")
    item = ctx.get_table("item")
    date_dim = ctx.get_table("date_dim")

    col = ctx.col
    lit = ctx.lit

    # Date range for frequent items (4 years)
    date_4yr = date_dim.filter((col("d_year") >= lit(year)) & (col("d_year") <= lit(year + 3)))

    # Frequent store sales items: items sold >4 times on same date
    # Note: i_item_sk is dropped after join (right key), use ss_item_sk instead
    frequent_items = (
        store_sales.join(date_4yr, left_on="ss_sold_date_sk", right_on="d_date_sk")
        .join(item, left_on="ss_item_sk", right_on="i_item_sk")
        .with_columns(col("i_item_desc").str.slice(0, 30).alias("itemdesc"))
        .group_by(["itemdesc", "ss_item_sk", "d_date"])
        .agg(ctx.len().alias("cnt"))
        .filter(col("cnt") > lit(4))
        .select(col("ss_item_sk").alias("i_item_sk"))
        .unique()
    )

    # Max store sales by customer in 4-year period
    # Note: c_customer_sk is dropped after join, use ss_customer_sk instead
    max_store_sales = (
        store_sales.join(customer, left_on="ss_customer_sk", right_on="c_customer_sk")
        .join(date_4yr, left_on="ss_sold_date_sk", right_on="d_date_sk")
        .group_by("ss_customer_sk")
        .agg((ctx.sum("ss_quantity") * ctx.mean("ss_sales_price")).alias("csales"))
        .select(ctx.max_("csales").alias("tpcds_cmax"))
    )

    if hasattr(max_store_sales, "scalar"):
        max_sales_val = max_store_sales.scalar(0, 0)
    else:
        max_sales_val = max_store_sales.to_numpy()[0, 0]

    threshold = (top_percent / 100.0) * max_sales_val if max_sales_val else 0

    # Best customers: total store sales > threshold
    # Note: c_customer_sk is dropped after join, use ss_customer_sk instead
    best_customers = (
        store_sales.join(customer, left_on="ss_customer_sk", right_on="c_customer_sk")
        .group_by("ss_customer_sk")
        .agg((ctx.sum("ss_quantity") * ctx.mean("ss_sales_price")).alias("ssales"))
        .filter(col("ssales") > lit(threshold))
        .select(col("ss_customer_sk").alias("c_customer_sk"))
    )

    # Target date
    date_target = date_dim.filter((col("d_year") == lit(year)) & (col("d_moy") == lit(month)))

    # Catalog sales for frequent items by best customers
    cs_sales = (
        catalog_sales.join(date_target, left_on="cs_sold_date_sk", right_on="d_date_sk")
        .join(frequent_items, left_on="cs_item_sk", right_on="i_item_sk", how="semi")
        .join(best_customers, left_on="cs_bill_customer_sk", right_on="c_customer_sk", how="semi")
        .select((col("cs_quantity") * col("cs_list_price")).alias("sales"))
    )

    # Web sales for frequent items by best customers
    ws_sales = (
        web_sales.join(date_target, left_on="ws_sold_date_sk", right_on="d_date_sk")
        .join(frequent_items, left_on="ws_item_sk", right_on="i_item_sk", how="semi")
        .join(best_customers, left_on="ws_bill_customer_sk", right_on="c_customer_sk", how="semi")
        .select((col("ws_quantity") * col("ws_list_price")).alias("sales"))
    )

    # Union and sum
    all_sales = ctx.concat([cs_sales, ws_sales])
    result = all_sales.select(ctx.sum("sales").alias("sum_sales"))

    return result


def q23_pandas_impl(ctx: DataFrameContext) -> Any:
    """Q23: Sum of catalog+web sales for frequent items by best customers (Pandas)."""
    import pandas as pd

    params = get_parameters(23)
    year = params.get("year", 2000)
    month = params.get("month", 4)
    top_percent = params.get("top_percent", 95)

    # Get tables
    store_sales = ctx.get_table("store_sales")
    catalog_sales = ctx.get_table("catalog_sales")
    web_sales = ctx.get_table("web_sales")
    customer = ctx.get_table("customer")
    item = ctx.get_table("item")
    date_dim = ctx.get_table("date_dim")

    # Date range for 4 years
    date_4yr = date_dim[(date_dim["d_year"] >= year) & (date_dim["d_year"] <= year + 3)]

    # Frequent items
    ss_items = store_sales.merge(date_4yr[["d_date_sk", "d_date"]], left_on="ss_sold_date_sk", right_on="d_date_sk")
    ss_items = ss_items.merge(item[["i_item_sk", "i_item_desc"]], left_on="ss_item_sk", right_on="i_item_sk")
    ss_items["itemdesc"] = ss_items["i_item_desc"].str[:30]

    # Use ctx.groupby_size for Dask compatibility
    freq_counts = ctx.groupby_size(ss_items, ["itemdesc", "i_item_sk", "d_date"], name="cnt")
    frequent_item_sks = set(freq_counts[freq_counts["cnt"] > 4]["i_item_sk"])

    # Max store sales by customer (rewritten for Dask compatibility)
    ss_cust = store_sales.merge(customer[["c_customer_sk"]], left_on="ss_customer_sk", right_on="c_customer_sk")
    ss_cust = ss_cust.merge(date_4yr[["d_date_sk"]], left_on="ss_sold_date_sk", right_on="d_date_sk")
    # Compute sales per row first, then aggregate
    ss_cust["_sales"] = ss_cust["ss_quantity"] * ss_cust["ss_sales_price"]
    cust_sales = ss_cust.groupby("c_customer_sk", as_index=False).agg(csales=("_sales", "sum"))
    max_sales_val = cust_sales["csales"].max()
    # Compute if lazy (Dask)
    if hasattr(max_sales_val, "compute"):
        max_sales_val = max_sales_val.compute()

    threshold = (top_percent / 100.0) * max_sales_val if max_sales_val else 0

    # Best customers (rewritten for Dask compatibility)
    all_cust_sales = store_sales.merge(customer[["c_customer_sk"]], left_on="ss_customer_sk", right_on="c_customer_sk")
    all_cust_sales["_sales"] = all_cust_sales["ss_quantity"] * all_cust_sales["ss_sales_price"]
    all_cust_sales_agg = all_cust_sales.groupby("c_customer_sk", as_index=False).agg(ssales=("_sales", "sum"))
    best_customer_sks = set(all_cust_sales_agg[all_cust_sales_agg["ssales"] > threshold]["c_customer_sk"])

    # Target date
    date_target = date_dim[(date_dim["d_year"] == year) & (date_dim["d_moy"] == month)]

    # Catalog sales
    cs = catalog_sales.merge(date_target[["d_date_sk"]], left_on="cs_sold_date_sk", right_on="d_date_sk")
    cs = cs[cs["cs_item_sk"].isin(frequent_item_sks)]
    cs = cs[cs["cs_bill_customer_sk"].isin(best_customer_sks)]
    cs["sales"] = cs["cs_quantity"] * cs["cs_list_price"]

    # Web sales
    ws = web_sales.merge(date_target[["d_date_sk"]], left_on="ws_sold_date_sk", right_on="d_date_sk")
    ws = ws[ws["ws_item_sk"].isin(frequent_item_sks)]
    ws = ws[ws["ws_bill_customer_sk"].isin(best_customer_sks)]
    ws["sales"] = ws["ws_quantity"] * ws["ws_list_price"]

    total = cs["sales"].sum() + ws["sales"].sum()
    result = pd.DataFrame({"sum_sales": [total]})

    return result


# =============================================================================
# Q24: Store Sales Returns Analysis by Color
# =============================================================================


def q24_expression_impl(ctx: DataFrameContext) -> Any:
    """Q24: Store sales with returns filtered by market and color (Polars)."""

    params = get_parameters(24)
    market_id = params.get("market_id", 8)
    color = params.get("color", "chiffon")

    # Get tables
    store_sales = ctx.get_table("store_sales")
    store_returns = ctx.get_table("store_returns")
    store = ctx.get_table("store")
    item = ctx.get_table("item")
    customer = ctx.get_table("customer")
    customer_address = ctx.get_table("customer_address")

    col = ctx.col
    lit = ctx.lit

    # Filter store by market_id
    store_filtered = store.filter(col("s_market_id") == lit(market_id))

    # Join store_sales with store_returns on ticket_number and item_sk
    ssales = (
        store_sales.join(
            store_returns,
            left_on=["ss_ticket_number", "ss_item_sk"],
            right_on=["sr_ticket_number", "sr_item_sk"],
        )
        .join(store_filtered, left_on="ss_store_sk", right_on="s_store_sk")
        .join(item, left_on="ss_item_sk", right_on="i_item_sk")
        .join(customer, left_on="ss_customer_sk", right_on="c_customer_sk")
        .join(customer_address, left_on="c_current_addr_sk", right_on="ca_address_sk")
    )

    # Filter: c_birth_country <> upper(ca_country) and s_zip = ca_zip
    ssales = ssales.filter(
        (col("c_birth_country") != col("ca_country").str.to_uppercase()) & (col("s_zip") == col("ca_zip"))
    )

    # Group and compute netpaid
    ssales_agg = ssales.group_by(
        [
            "c_last_name",
            "c_first_name",
            "s_store_name",
            "ca_state",
            "s_state",
            "i_color",
            "i_current_price",
            "i_manager_id",
            "i_units",
            "i_size",
        ]
    ).agg(ctx.sum("ss_net_paid").alias("netpaid"))

    # Compute average netpaid - use platform-agnostic scalar() method
    avg_netpaid = ssales_agg.select(ctx.mean("netpaid").alias("avg_netpaid"))
    avg_val = avg_netpaid.scalar(0, 0)

    threshold = 0.05 * avg_val if avg_val else 0

    # Filter by color and aggregate
    result = (
        ssales_agg.filter(col("i_color") == lit(color))
        .group_by(["c_last_name", "c_first_name", "s_store_name"])
        .agg(ctx.sum("netpaid").alias("paid"))
        .filter(col("paid") > lit(threshold))
        .sort(["c_last_name", "c_first_name", "s_store_name"])
    )

    return result


def q24_pandas_impl(ctx: DataFrameContext) -> Any:
    """Q24: Store sales with returns filtered by market and color (Pandas)."""
    params = get_parameters(24)
    market_id = params.get("market_id", 8)
    color = params.get("color", "chiffon")

    # Get tables
    store_sales = ctx.get_table("store_sales")
    store_returns = ctx.get_table("store_returns")
    store = ctx.get_table("store")
    item = ctx.get_table("item")
    customer = ctx.get_table("customer")
    customer_address = ctx.get_table("customer_address")

    # Filter store
    store_filtered = store[store["s_market_id"] == market_id]

    # Join tables
    ssales = store_sales.merge(
        store_returns,
        left_on=["ss_ticket_number", "ss_item_sk"],
        right_on=["sr_ticket_number", "sr_item_sk"],
    )
    ssales = ssales.merge(store_filtered, left_on="ss_store_sk", right_on="s_store_sk")
    ssales = ssales.merge(item, left_on="ss_item_sk", right_on="i_item_sk")
    ssales = ssales.merge(customer, left_on="ss_customer_sk", right_on="c_customer_sk")
    ssales = ssales.merge(customer_address, left_on="c_current_addr_sk", right_on="ca_address_sk")

    # Filter - use query for Dask compatibility (avoids index alignment issues)
    # Note: str.upper() doesn't work in query, need to pre-compute
    ssales["_ca_country_upper"] = ssales["ca_country"].str.upper()
    ssales = ssales.query("c_birth_country != _ca_country_upper and s_zip == ca_zip")

    # Aggregate
    group_cols = [
        "c_last_name",
        "c_first_name",
        "s_store_name",
        "ca_state",
        "s_state",
        "i_color",
        "i_current_price",
        "i_manager_id",
        "i_units",
        "i_size",
    ]
    ssales_agg = ctx.groupby_agg(ssales, group_cols, {"netpaid": ("ss_net_paid", "sum")}, as_index=False)

    # Compute average and threshold (compute for Dask compatibility)
    avg_val = ssales_agg["netpaid"].mean()
    if hasattr(avg_val, "compute"):
        avg_val = avg_val.compute()
    threshold = 0.05 * avg_val if avg_val else 0

    # Filter by color using query for Dask compatibility
    color_data = ssales_agg.query("i_color == @_color", local_dict={"_color": color})
    result = ctx.groupby_agg(
        color_data, ["c_last_name", "c_first_name", "s_store_name"], {"paid": ("netpaid", "sum")}, as_index=False
    )
    # Use ctx.filter_gt for Dask-safe filtering (avoids index alignment issues)
    result = ctx.filter_gt(result, "paid", threshold)
    result = result.sort_values(["c_last_name", "c_first_name", "s_store_name"])

    return result


# =============================================================================
# Q21: Inventory Before/After Date Analysis
# =============================================================================


def q21_expression_impl(ctx: DataFrameContext) -> Any:
    """Q21: Inventory analysis comparing quantities before/after a date (Polars)."""
    from datetime import date, timedelta

    params = get_parameters(21)
    year = params.get("year", 2000)
    month = params.get("month", 1)
    price_min = params.get("price_min", 0.99)
    price_max = params.get("price_max", 1.49)

    # Get tables
    inventory = ctx.get_table("inventory")
    warehouse = ctx.get_table("warehouse")
    item = ctx.get_table("item")
    date_dim = ctx.get_table("date_dim")

    col = ctx.col
    lit = ctx.lit

    # Define the sales date (middle of the date range)
    sales_date = date(year, month, 31)
    date_start = sales_date - timedelta(days=30)
    date_end = sales_date + timedelta(days=30)

    # Filter items by price range
    item_filtered = item.filter((col("i_current_price") >= lit(price_min)) & (col("i_current_price") <= lit(price_max)))

    # Filter dates
    date_filtered = date_dim.filter((col("d_date") >= lit(date_start)) & (col("d_date") <= lit(date_end)))

    # Join inventory with warehouse, item, date
    inv_data = (
        inventory.join(item_filtered, left_on="inv_item_sk", right_on="i_item_sk")
        .join(warehouse, left_on="inv_warehouse_sk", right_on="w_warehouse_sk")
        .join(date_filtered, left_on="inv_date_sk", right_on="d_date_sk")
    )

    # Compute before/after quantities
    inv_data = inv_data.with_columns(
        [
            ctx.when(col("d_date") < lit(sales_date))
            .then(col("inv_quantity_on_hand"))
            .otherwise(lit(0))
            .alias("inv_before"),
            ctx.when(col("d_date") >= lit(sales_date))
            .then(col("inv_quantity_on_hand"))
            .otherwise(lit(0))
            .alias("inv_after"),
        ]
    )

    # Aggregate by warehouse and item
    grouped = inv_data.group_by(["w_warehouse_name", "i_item_id"]).agg(
        [
            ctx.sum("inv_before").alias("inv_before"),
            ctx.sum("inv_after").alias("inv_after"),
        ]
    )

    # Filter: ratio of after/before between 2/3 and 3/2
    result = (
        grouped.filter(
            (col("inv_before") > lit(0))
            & ((col("inv_after") / col("inv_before")) >= lit(2.0 / 3.0))
            & ((col("inv_after") / col("inv_before")) <= lit(3.0 / 2.0))
        )
        .sort(["w_warehouse_name", "i_item_id"])
        .head(100)
    )

    return result


def q21_pandas_impl(ctx: DataFrameContext) -> Any:
    """Q21: Inventory analysis comparing quantities before/after a date (Pandas)."""
    from datetime import date, timedelta

    params = get_parameters(21)
    year = params.get("year", 2000)
    month = params.get("month", 1)
    price_min = params.get("price_min", 0.99)
    price_max = params.get("price_max", 1.49)

    # Get tables
    inventory = ctx.get_table("inventory")
    warehouse = ctx.get_table("warehouse")
    item = ctx.get_table("item")
    date_dim = ctx.get_table("date_dim")

    # Define date range
    # Note: Use datetime.date consistently as d_date column may contain date objects
    sales_date = date(year, month, 31)
    date_start = sales_date - timedelta(days=30)
    date_end = sales_date + timedelta(days=30)

    # Filter items
    item_filtered = item[(item["i_current_price"] >= price_min) & (item["i_current_price"] <= price_max)]

    # Filter dates - use date objects directly
    date_filtered = date_dim[(date_dim["d_date"] >= date_start) & (date_dim["d_date"] <= date_end)]

    # Join tables
    inv_data = inventory.merge(item_filtered, left_on="inv_item_sk", right_on="i_item_sk")
    inv_data = inv_data.merge(warehouse, left_on="inv_warehouse_sk", right_on="w_warehouse_sk")
    inv_data = inv_data.merge(date_filtered, left_on="inv_date_sk", right_on="d_date_sk")

    # Compute before/after - use date objects directly for comparison
    inv_data["inv_before"] = inv_data.apply(
        lambda r: r["inv_quantity_on_hand"] if r["d_date"] < sales_date else 0,
        axis=1,
    )
    inv_data["inv_after"] = inv_data.apply(
        lambda r: r["inv_quantity_on_hand"] if r["d_date"] >= sales_date else 0,
        axis=1,
    )

    # Aggregate
    grouped = inv_data.groupby(["w_warehouse_name", "i_item_id"], as_index=False).agg(
        inv_before=("inv_before", "sum"),
        inv_after=("inv_after", "sum"),
    )

    # Filter by ratio
    result = grouped[grouped["inv_before"] > 0].copy()
    result["ratio"] = result["inv_after"] / result["inv_before"]
    result = result[(result["ratio"] >= 2.0 / 3.0) & (result["ratio"] <= 3.0 / 2.0)]
    result = result[["w_warehouse_name", "i_item_id", "inv_before", "inv_after"]]
    result = result.sort_values(["w_warehouse_name", "i_item_id"]).head(100)

    return result


# =============================================================================
# Q22: Inventory Month Analysis with ROLLUP
# =============================================================================


def q22_expression_impl(ctx: DataFrameContext) -> Any:
    """Q22: Inventory analysis with ROLLUP by product attributes (Polars)."""

    from .rollup_helper import expand_rollup_expression

    params = get_parameters(22)
    year = params.get("year", 2001)
    months = params.get("months", [1, 2, 3, 4, 5, 6])

    # Get tables
    inventory = ctx.get_table("inventory")
    item = ctx.get_table("item")
    date_dim = ctx.get_table("date_dim")

    col = ctx.col
    lit = ctx.lit

    # Filter dates by month_seq range (approximated by year and months)
    # d_month_seq is typically: year * 12 + month - 1
    # For months param, filter by d_moy in months and d_year
    date_filtered = date_dim.filter((col("d_year") == lit(year)) & (col("d_moy").is_in(months)))

    # Join inventory with item and date
    inv_data = inventory.join(date_filtered, left_on="inv_date_sk", right_on="d_date_sk").join(
        item, left_on="inv_item_sk", right_on="i_item_sk"
    )

    # Group for ROLLUP
    group_cols = ["i_product_name", "i_brand", "i_class", "i_category"]
    agg_exprs = [ctx.mean("inv_quantity_on_hand").alias("qoh")]

    result = expand_rollup_expression(inv_data, group_cols, agg_exprs, ctx)
    result = result.sort(["qoh", "i_product_name", "i_brand", "i_class", "i_category"]).head(100)

    return result


def q22_pandas_impl(ctx: DataFrameContext) -> Any:
    """Q22: Inventory analysis with ROLLUP by product attributes (Pandas)."""
    from .rollup_helper import expand_rollup_pandas

    params = get_parameters(22)
    year = params.get("year", 2001)
    months = params.get("months", [1, 2, 3, 4, 5, 6])

    # Get tables
    inventory = ctx.get_table("inventory")
    item = ctx.get_table("item")
    date_dim = ctx.get_table("date_dim")

    # Filter dates
    date_filtered = date_dim[(date_dim["d_year"] == year) & (date_dim["d_moy"].isin(months))]

    # Join tables
    inv_data = inventory.merge(date_filtered, left_on="inv_date_sk", right_on="d_date_sk")
    inv_data = inv_data.merge(item, left_on="inv_item_sk", right_on="i_item_sk")

    # Group for ROLLUP
    group_cols = ["i_product_name", "i_brand", "i_class", "i_category"]
    agg_dict = {"qoh": ("inv_quantity_on_hand", "mean")}

    result = expand_rollup_pandas(inv_data, group_cols, agg_dict, ctx)
    result = result.sort_values(["qoh", "i_product_name", "i_brand", "i_class", "i_category"]).head(100)

    return result


# =============================================================================
# Q39: Inventory Variance by Month (Two-part query)
# =============================================================================


def q39_expression_impl(ctx: DataFrameContext) -> Any:
    """Q39: Inventory variance analysis comparing consecutive months (Polars)."""

    params = get_parameters(39)
    year = params.get("year", 2001)
    months = params.get("months", [1, 2])
    month1 = months[0] if len(months) > 0 else 1
    month2 = months[1] if len(months) > 1 else 2

    # Get tables
    inventory = ctx.get_table("inventory")
    item = ctx.get_table("item")
    warehouse = ctx.get_table("warehouse")
    date_dim = ctx.get_table("date_dim")

    col = ctx.col
    lit = ctx.lit

    # Filter dates for the year
    date_filtered = date_dim.filter(col("d_year") == lit(year))

    # Join inventory with item, warehouse, date
    inv_data = (
        inventory.join(item, left_on="inv_item_sk", right_on="i_item_sk")
        .join(warehouse, left_on="inv_warehouse_sk", right_on="w_warehouse_sk")
        .join(date_filtered, left_on="inv_date_sk", right_on="d_date_sk")
    )

    # Group by warehouse, item, month and compute stddev and mean
    # Note: w_warehouse_sk and i_item_sk are dropped after joins, use preserved left-side keys
    grouped = inv_data.group_by(["w_warehouse_name", "inv_warehouse_sk", "inv_item_sk", "d_moy"]).agg(
        [
            ctx.std("inv_quantity_on_hand").alias("stdev"),
            ctx.mean("inv_quantity_on_hand").alias("mean"),
        ]
    )

    # Compute coefficient of variation (cov = stdev/mean)
    grouped = grouped.with_columns(
        ctx.when(col("mean") != lit(0)).then(col("stdev") / col("mean")).otherwise(lit(None)).alias("cov")
    )

    # Filter to items with cov > 1
    inv_cov = grouped.filter(
        ctx.when(col("mean") == lit(0)).then(lit(False)).otherwise(col("stdev") / col("mean") > lit(1))
    )

    # Self-join for consecutive months
    # Note: Using preserved left-side keys inv_warehouse_sk and inv_item_sk
    inv1 = inv_cov.filter(col("d_moy") == lit(month1)).select(
        [
            col("inv_warehouse_sk").alias("inv1_w_sk"),
            col("inv_item_sk").alias("inv1_i_sk"),
            col("d_moy").alias("inv1_moy"),
            col("mean").alias("inv1_mean"),
            col("cov").alias("inv1_cov"),
        ]
    )

    inv2 = inv_cov.filter(col("d_moy") == lit(month2)).select(
        [
            col("inv_warehouse_sk").alias("inv2_w_sk"),
            col("inv_item_sk").alias("inv2_i_sk"),
            col("d_moy").alias("inv2_moy"),
            col("mean").alias("inv2_mean"),
            col("cov").alias("inv2_cov"),
        ]
    )

    result = inv1.join(
        inv2,
        left_on=["inv1_w_sk", "inv1_i_sk"],
        right_on=["inv2_w_sk", "inv2_i_sk"],
    ).sort(["inv1_w_sk", "inv1_i_sk", "inv1_moy", "inv1_mean", "inv1_cov"])

    return result


def q39_pandas_impl(ctx: DataFrameContext) -> Any:
    """Q39: Inventory variance analysis comparing consecutive months (Pandas)."""
    params = get_parameters(39)
    year = params.get("year", 2001)
    months = params.get("months", [1, 2])
    month1 = months[0] if len(months) > 0 else 1
    month2 = months[1] if len(months) > 1 else 2

    # Get tables
    inventory = ctx.get_table("inventory")
    item = ctx.get_table("item")
    warehouse = ctx.get_table("warehouse")
    date_dim = ctx.get_table("date_dim")

    # Filter dates
    date_filtered = date_dim[date_dim["d_year"] == year]

    # Join tables
    inv_data = inventory.merge(item, left_on="inv_item_sk", right_on="i_item_sk")
    inv_data = inv_data.merge(warehouse, left_on="inv_warehouse_sk", right_on="w_warehouse_sk")
    inv_data = inv_data.merge(date_filtered, left_on="inv_date_sk", right_on="d_date_sk")

    # Group and compute stats
    grouped = inv_data.groupby(["w_warehouse_name", "w_warehouse_sk", "i_item_sk", "d_moy"], as_index=False).agg(
        stdev=("inv_quantity_on_hand", "std"),
        mean=("inv_quantity_on_hand", "mean"),
    )

    # Compute cov
    grouped["cov"] = grouped.apply(lambda r: r["stdev"] / r["mean"] if r["mean"] != 0 else None, axis=1)

    # Filter cov > 1
    inv_cov = grouped[(grouped["mean"] != 0) & (grouped["stdev"] / grouped["mean"] > 1)].copy()

    # Get month1 and month2 data
    inv1 = inv_cov[inv_cov["d_moy"] == month1][["w_warehouse_sk", "i_item_sk", "d_moy", "mean", "cov"]].copy()
    inv1.columns = ["inv1_w_sk", "inv1_i_sk", "inv1_moy", "inv1_mean", "inv1_cov"]

    inv2 = inv_cov[inv_cov["d_moy"] == month2][["w_warehouse_sk", "i_item_sk", "d_moy", "mean", "cov"]].copy()
    inv2.columns = ["inv2_w_sk", "inv2_i_sk", "inv2_moy", "inv2_mean", "inv2_cov"]

    # Join
    result = inv1.merge(inv2, left_on=["inv1_w_sk", "inv1_i_sk"], right_on=["inv2_w_sk", "inv2_i_sk"])
    result = result.sort_values(["inv1_w_sk", "inv1_i_sk", "inv1_moy", "inv1_mean", "inv1_cov"])

    return result


# =============================================================================
# Q64: Cross-Sales Store/Catalog Analysis with Income Band
# =============================================================================


def q64_expression_impl(ctx: DataFrameContext) -> Any:
    """Q64: Complex cross-sales analysis with income band filtering (Polars)."""

    params = get_parameters(64)
    year = params.get("year", 1999)
    colors = params.get("colors", ["slate", "blanched", "burnished", "chartreuse", "peru", "thistle"])
    price_min = params.get("price_min", 0)
    price_max = params.get("price_max", 85)

    # Get tables
    store_sales = ctx.get_table("store_sales")
    store_returns = ctx.get_table("store_returns")
    catalog_sales = ctx.get_table("catalog_sales")
    catalog_returns = ctx.get_table("catalog_returns")
    date_dim = ctx.get_table("date_dim")
    store = ctx.get_table("store")
    customer = ctx.get_table("customer")
    customer_demographics = ctx.get_table("customer_demographics")
    promotion = ctx.get_table("promotion")
    household_demographics = ctx.get_table("household_demographics")
    customer_address = ctx.get_table("customer_address")
    income_band = ctx.get_table("income_band")
    item = ctx.get_table("item")

    col = ctx.col
    lit = ctx.lit

    # cs_ui: catalog sales items where sales > 2 * refunds
    cs_with_returns = catalog_sales.join(
        catalog_returns,
        left_on=["cs_item_sk", "cs_order_number"],
        right_on=["cr_item_sk", "cr_order_number"],
    )
    cs_ui = (
        cs_with_returns.group_by("cs_item_sk")
        .agg(
            [
                ctx.sum("cs_ext_list_price").alias("sale"),
                (ctx.sum("cr_refunded_cash") + ctx.sum("cr_reversed_charge") + ctx.sum("cr_store_credit")).alias(
                    "refund"
                ),
            ]
        )
        .filter(col("sale") > lit(2) * col("refund"))
        .select("cs_item_sk")
    )

    # Filter item by color and price
    item_filtered = item.filter(
        col("i_color").is_in(colors)
        & (col("i_current_price") >= lit(price_min))
        & (col("i_current_price") <= lit(price_max + 10))
    )

    # Join store_sales with store_returns
    ss_sr = store_sales.join(
        store_returns,
        left_on=["ss_item_sk", "ss_ticket_number"],
        right_on=["sr_item_sk", "sr_ticket_number"],
    )

    # Filter by cs_ui items
    ss_sr = ss_sr.join(cs_ui, left_on="ss_item_sk", right_on="cs_item_sk", how="semi")

    # Join with other tables
    cross_sales = (
        ss_sr.join(item_filtered, left_on="ss_item_sk", right_on="i_item_sk")
        .join(store, left_on="ss_store_sk", right_on="s_store_sk")
        .join(customer, left_on="ss_customer_sk", right_on="c_customer_sk")
        .join(
            customer_demographics.select(["cd_demo_sk", "cd_marital_status"]).rename(
                {"cd_marital_status": "cd1_marital_status"}
            ),
            left_on="ss_cdemo_sk",
            right_on="cd_demo_sk",
        )
        .join(promotion, left_on="ss_promo_sk", right_on="p_promo_sk")
        .join(
            household_demographics.select(["hd_demo_sk", "hd_income_band_sk"]).rename(
                {"hd_income_band_sk": "hd1_ib_sk"}
            ),
            left_on="ss_hdemo_sk",
            right_on="hd_demo_sk",
        )
        .join(
            customer_address.select(
                ["ca_address_sk", "ca_street_number", "ca_street_name", "ca_city", "ca_zip"]
            ).rename(
                {
                    "ca_street_number": "b_street_number",
                    "ca_street_name": "b_street_name",
                    "ca_city": "b_city",
                    "ca_zip": "b_zip",
                }
            ),
            left_on="ss_addr_sk",
            right_on="ca_address_sk",
        )
        .join(
            date_dim.select(["d_date_sk", "d_year"]).rename({"d_year": "syear"}),
            left_on="ss_sold_date_sk",
            right_on="d_date_sk",
        )
        .join(
            customer_demographics.select(["cd_demo_sk", "cd_marital_status"]).rename(
                {"cd_marital_status": "cd2_marital_status", "cd_demo_sk": "cd2_demo_sk"}
            ),
            left_on="c_current_cdemo_sk",
            right_on="cd2_demo_sk",
        )
        .join(
            household_demographics.select(["hd_demo_sk", "hd_income_band_sk"]).rename(
                {"hd_income_band_sk": "hd2_ib_sk", "hd_demo_sk": "hd2_demo_sk"}
            ),
            left_on="c_current_hdemo_sk",
            right_on="hd2_demo_sk",
        )
        .join(
            customer_address.select(
                ["ca_address_sk", "ca_street_number", "ca_street_name", "ca_city", "ca_zip"]
            ).rename(
                {
                    "ca_address_sk": "c_addr_sk",
                    "ca_street_number": "c_street_number",
                    "ca_street_name": "c_street_name",
                    "ca_city": "c_city",
                    "ca_zip": "c_zip",
                }
            ),
            left_on="c_current_addr_sk",
            right_on="c_addr_sk",
        )
        .join(
            income_band.select(["ib_income_band_sk"]).rename({"ib_income_band_sk": "ib1_sk"}),
            left_on="hd1_ib_sk",
            right_on="ib1_sk",
        )
        .join(
            income_band.select(["ib_income_band_sk"]).rename({"ib_income_band_sk": "ib2_sk"}),
            left_on="hd2_ib_sk",
            right_on="ib2_sk",
        )
    )

    # Filter: marital status different
    cross_sales = cross_sales.filter(col("cd1_marital_status") != col("cd2_marital_status"))

    # Group by multiple columns
    # Note: i_item_sk is dropped after join, use ss_item_sk instead
    grouped = cross_sales.group_by(
        [
            "i_product_name",
            "ss_item_sk",
            "s_store_name",
            "s_zip",
            "b_street_number",
            "b_street_name",
            "b_city",
            "b_zip",
            "c_street_number",
            "c_street_name",
            "c_city",
            "c_zip",
            "syear",
        ]
    ).agg(
        [
            ctx.len().alias("cnt"),
            ctx.sum("ss_wholesale_cost").alias("s1"),
            ctx.sum("ss_list_price").alias("s2"),
            ctx.sum("ss_coupon_amt").alias("s3"),
        ]
    )

    # Self-join for year comparison
    cs1 = grouped.filter(col("syear") == lit(year))
    cs2 = grouped.filter(col("syear") == lit(year + 1))

    result = (
        cs1.join(
            cs2,
            on=["ss_item_sk", "s_store_name", "s_zip"],
            suffix="_y2",
        )
        .filter(col("cnt_y2") <= col("cnt"))
        .select(
            [
                "i_product_name",
                "s_store_name",
                "s_zip",
                "b_street_number",
                "b_street_name",
                "b_city",
                "b_zip",
                "c_street_number",
                "c_street_name",
                "c_city",
                "c_zip",
                "syear",
                "cnt",
                "s1",
                "s2",
                "s3",
                col("s1_y2").alias("s12"),
                col("s2_y2").alias("s22"),
                col("s3_y2").alias("s32"),
                col("syear_y2").alias("syear2"),
                col("cnt_y2").alias("cnt2"),
            ]
        )
        .sort(["i_product_name", "s_store_name", "cnt2", "s1", "s12"])
    )

    return result


def q64_pandas_impl(ctx: DataFrameContext) -> Any:
    """Q64: Complex cross-sales analysis with income band filtering (Pandas)."""
    params = get_parameters(64)
    year = params.get("year", 1999)
    colors = params.get("colors", ["slate", "blanched", "burnished", "chartreuse", "peru", "thistle"])
    price_min = params.get("price_min", 0)
    price_max = params.get("price_max", 85)

    # Get tables
    store_sales = ctx.get_table("store_sales")
    store_returns = ctx.get_table("store_returns")
    catalog_sales = ctx.get_table("catalog_sales")
    catalog_returns = ctx.get_table("catalog_returns")
    date_dim = ctx.get_table("date_dim")
    store = ctx.get_table("store")
    customer = ctx.get_table("customer")
    customer_demographics = ctx.get_table("customer_demographics")
    promotion = ctx.get_table("promotion")
    household_demographics = ctx.get_table("household_demographics")
    customer_address = ctx.get_table("customer_address")
    income_band = ctx.get_table("income_band")
    item = ctx.get_table("item")

    # cs_ui: catalog sales where sales > 2 * refund
    cs_cr = catalog_sales.merge(
        catalog_returns,
        left_on=["cs_item_sk", "cs_order_number"],
        right_on=["cr_item_sk", "cr_order_number"],
    )
    cs_agg = cs_cr.groupby("cs_item_sk", as_index=False).agg(
        sale=("cs_ext_list_price", "sum"),
        refund_cash=("cr_refunded_cash", "sum"),
        refund_charge=("cr_reversed_charge", "sum"),
        refund_credit=("cr_store_credit", "sum"),
    )
    cs_agg["refund"] = cs_agg["refund_cash"] + cs_agg["refund_charge"] + cs_agg["refund_credit"]
    cs_ui_items = set(cs_agg[cs_agg["sale"] > 2 * cs_agg["refund"]]["cs_item_sk"])

    # Filter item
    item_filtered = item[
        item["i_color"].isin(colors)
        & (item["i_current_price"] >= price_min)
        & (item["i_current_price"] <= price_max + 10)
    ]

    # Join store_sales with returns
    ss_sr = store_sales.merge(
        store_returns,
        left_on=["ss_item_sk", "ss_ticket_number"],
        right_on=["sr_item_sk", "sr_ticket_number"],
    )
    ss_sr = ss_sr[ss_sr["ss_item_sk"].isin(cs_ui_items)]

    # Join with other tables
    cross_sales = ss_sr.merge(item_filtered, left_on="ss_item_sk", right_on="i_item_sk")
    cross_sales = cross_sales.merge(store, left_on="ss_store_sk", right_on="s_store_sk")
    cross_sales = cross_sales.merge(customer, left_on="ss_customer_sk", right_on="c_customer_sk")

    cd1 = customer_demographics[["cd_demo_sk", "cd_marital_status"]].copy()
    cd1.columns = ["cd1_demo_sk", "cd1_marital_status"]
    cross_sales = cross_sales.merge(cd1, left_on="ss_cdemo_sk", right_on="cd1_demo_sk")

    cross_sales = cross_sales.merge(promotion, left_on="ss_promo_sk", right_on="p_promo_sk")

    hd1 = household_demographics[["hd_demo_sk", "hd_income_band_sk"]].copy()
    hd1.columns = ["hd1_demo_sk", "hd1_ib_sk"]
    cross_sales = cross_sales.merge(hd1, left_on="ss_hdemo_sk", right_on="hd1_demo_sk")

    ca1 = customer_address[["ca_address_sk", "ca_street_number", "ca_street_name", "ca_city", "ca_zip"]].copy()
    ca1.columns = ["ca1_address_sk", "b_street_number", "b_street_name", "b_city", "b_zip"]
    cross_sales = cross_sales.merge(ca1, left_on="ss_addr_sk", right_on="ca1_address_sk")

    d1 = date_dim[["d_date_sk", "d_year"]].copy()
    d1.columns = ["d1_date_sk", "syear"]
    cross_sales = cross_sales.merge(d1, left_on="ss_sold_date_sk", right_on="d1_date_sk")

    cd2 = customer_demographics[["cd_demo_sk", "cd_marital_status"]].copy()
    cd2.columns = ["cd2_demo_sk", "cd2_marital_status"]
    cross_sales = cross_sales.merge(cd2, left_on="c_current_cdemo_sk", right_on="cd2_demo_sk")

    hd2 = household_demographics[["hd_demo_sk", "hd_income_band_sk"]].copy()
    hd2.columns = ["hd2_demo_sk", "hd2_ib_sk"]
    cross_sales = cross_sales.merge(hd2, left_on="c_current_hdemo_sk", right_on="hd2_demo_sk")

    ca2 = customer_address[["ca_address_sk", "ca_street_number", "ca_street_name", "ca_city", "ca_zip"]].copy()
    ca2.columns = ["ca2_address_sk", "c_street_number", "c_street_name", "c_city", "c_zip"]
    cross_sales = cross_sales.merge(ca2, left_on="c_current_addr_sk", right_on="ca2_address_sk")

    ib1 = income_band[["ib_income_band_sk"]].copy()
    ib1.columns = ["ib1_sk"]
    cross_sales = cross_sales.merge(ib1, left_on="hd1_ib_sk", right_on="ib1_sk")

    ib2 = income_band[["ib_income_band_sk"]].copy()
    ib2.columns = ["ib2_sk"]
    cross_sales = cross_sales.merge(ib2, left_on="hd2_ib_sk", right_on="ib2_sk")

    # Filter marital status different
    cross_sales = cross_sales[cross_sales["cd1_marital_status"] != cross_sales["cd2_marital_status"]]

    # Group
    grouped = cross_sales.groupby(
        [
            "i_product_name",
            "i_item_sk",
            "s_store_name",
            "s_zip",
            "b_street_number",
            "b_street_name",
            "b_city",
            "b_zip",
            "c_street_number",
            "c_street_name",
            "c_city",
            "c_zip",
            "syear",
        ],
        as_index=False,
    ).agg(
        cnt=("ss_item_sk", "count"),
        s1=("ss_wholesale_cost", "sum"),
        s2=("ss_list_price", "sum"),
        s3=("ss_coupon_amt", "sum"),
    )

    # Self-join for year comparison
    cs1 = grouped[grouped["syear"] == year].copy()
    cs2 = grouped[grouped["syear"] == year + 1].copy()
    cs2 = cs2.rename(columns={"syear": "syear2", "cnt": "cnt2", "s1": "s12", "s2": "s22", "s3": "s32"})
    cs2 = cs2[["i_item_sk", "s_store_name", "s_zip", "syear2", "cnt2", "s12", "s22", "s32"]]

    result = cs1.merge(cs2, on=["i_item_sk", "s_store_name", "s_zip"])
    result = result[result["cnt2"] <= result["cnt"]]
    result = result.sort_values(["i_product_name", "s_store_name", "cnt2", "s1", "s12"])

    return result


# =============================================================================
# Q84: Customer Income Band Filter
# =============================================================================


def q84_expression_impl(ctx: DataFrameContext) -> Any:
    """Q84: Customer lookup filtered by city and income band (Polars)."""

    params = get_parameters(84)
    city = params.get("city", "Edgewood")
    income_min = params.get("income_min", 38128)
    income_max = params.get("income_max", income_min + 50000)

    # Get tables
    customer = ctx.get_table("customer")
    customer_address = ctx.get_table("customer_address")
    customer_demographics = ctx.get_table("customer_demographics")
    household_demographics = ctx.get_table("household_demographics")
    income_band = ctx.get_table("income_band")
    store_returns = ctx.get_table("store_returns")

    col = ctx.col
    lit = ctx.lit

    # Filter by city
    ca_filtered = customer_address.filter(col("ca_city") == lit(city))

    # Filter income band
    ib_filtered = income_band.filter(
        (col("ib_lower_bound") >= lit(income_min)) & (col("ib_upper_bound") <= lit(income_max))
    )

    # Join tables
    result = (
        customer.join(ca_filtered, left_on="c_current_addr_sk", right_on="ca_address_sk")
        .join(customer_demographics, left_on="c_current_cdemo_sk", right_on="cd_demo_sk")
        .join(household_demographics, left_on="c_current_hdemo_sk", right_on="hd_demo_sk")
        .join(ib_filtered, left_on="hd_income_band_sk", right_on="ib_income_band_sk")
        .join(store_returns, left_on="c_current_cdemo_sk", right_on="sr_cdemo_sk", how="semi")
    )

    # Select and format output
    result = (
        result.with_columns(
            (ctx.coalesce(col("c_last_name"), lit("")) + lit(", ") + ctx.coalesce(col("c_first_name"), lit(""))).alias(
                "customername"
            )
        )
        .select(
            [
                col("c_customer_id").alias("customer_id"),
                "customername",
            ]
        )
        .sort("customer_id")
        .head(100)
    )

    return result


def q84_pandas_impl(ctx: DataFrameContext) -> Any:
    """Q84: Customer lookup filtered by city and income band (Pandas)."""
    params = get_parameters(84)
    city = params.get("city", "Edgewood")
    income_min = params.get("income_min", 38128)
    income_max = params.get("income_max", income_min + 50000)

    # Get tables
    customer = ctx.get_table("customer")
    customer_address = ctx.get_table("customer_address")
    customer_demographics = ctx.get_table("customer_demographics")
    household_demographics = ctx.get_table("household_demographics")
    income_band = ctx.get_table("income_band")
    store_returns = ctx.get_table("store_returns")

    # Filter by city
    ca_filtered = customer_address[customer_address["ca_city"] == city]

    # Filter income band
    ib_filtered = income_band[
        (income_band["ib_lower_bound"] >= income_min) & (income_band["ib_upper_bound"] <= income_max)
    ]

    # Join tables
    result = customer.merge(ca_filtered, left_on="c_current_addr_sk", right_on="ca_address_sk")
    result = result.merge(customer_demographics, left_on="c_current_cdemo_sk", right_on="cd_demo_sk")
    result = result.merge(household_demographics, left_on="c_current_hdemo_sk", right_on="hd_demo_sk")
    result = result.merge(ib_filtered, left_on="hd_income_band_sk", right_on="ib_income_band_sk")

    # Filter to customers with store returns (semi-join)
    sr_cdemo_sks = set(store_returns["sr_cdemo_sk"].dropna())
    result = result[result["c_current_cdemo_sk"].isin(sr_cdemo_sks)]

    # Format output
    result["customername"] = result["c_last_name"].fillna("") + ", " + result["c_first_name"].fillna("")
    result = result[["c_customer_id", "customername"]].copy()
    result.columns = ["customer_id", "customername"]
    result = result.sort_values("customer_id").head(100)

    return result


# =============================================================================
# Query Registration
# =============================================================================


def _register_all_queries() -> None:
    """Register all TPC-DS DataFrame queries."""
    # Q3: Date/Item Brand Sales
    register_query(
        DataFrameQuery(
            query_id="Q3",
            query_name="Date/Item Brand Sales",
            description="Reports sales for items by brand for a specific month and manufacturer",
            categories=[QueryCategory.JOIN, QueryCategory.AGGREGATE, QueryCategory.SORT, QueryCategory.TPCDS],
            expression_impl=q3_expression_impl,
            pandas_impl=q3_pandas_impl,
        )
    )

    # Q2: Web/Catalog Weekly Sales Year-over-Year
    register_query(
        DataFrameQuery(
            query_id="Q2",
            query_name="Web/Catalog Weekly Sales Year-over-Year",
            description="Compares weekly web+catalog sales ratios year-over-year by day of week",
            categories=[
                QueryCategory.MULTI_JOIN,
                QueryCategory.AGGREGATE,
                QueryCategory.ANALYTICAL,
                QueryCategory.TPCDS,
            ],
            expression_impl=q2_expression_impl,
            pandas_impl=q2_pandas_impl,
        )
    )

    # Q19: Store Sales Item/Customer Analysis
    register_query(
        DataFrameQuery(
            query_id="Q19",
            query_name="Store Sales Item/Customer Analysis",
            description="Reports sales by item brand for customers, filtered by manager and zip codes",
            categories=[
                QueryCategory.MULTI_JOIN,
                QueryCategory.AGGREGATE,
                QueryCategory.SORT,
                QueryCategory.TPCDS,
            ],
            expression_impl=q19_expression_impl,
            pandas_impl=q19_pandas_impl,
        )
    )

    # Q42: Date/Item Category Sales
    register_query(
        DataFrameQuery(
            query_id="Q42",
            query_name="Date/Item Category Sales",
            description="Reports sales by item category for a specific month and year",
            categories=[QueryCategory.JOIN, QueryCategory.AGGREGATE, QueryCategory.SORT, QueryCategory.TPCDS],
            expression_impl=q42_expression_impl,
            pandas_impl=q42_pandas_impl,
        )
    )

    # Q43: Store Sales Day Analysis
    register_query(
        DataFrameQuery(
            query_id="Q43",
            query_name="Store Sales Day Analysis",
            description="Reports store sales by day of week for a specific year",
            categories=[QueryCategory.JOIN, QueryCategory.AGGREGATE, QueryCategory.SORT, QueryCategory.TPCDS],
            expression_impl=q43_expression_impl,
            pandas_impl=q43_pandas_impl,
        )
    )

    # Q52: Date/Brand Extended Sales
    register_query(
        DataFrameQuery(
            query_id="Q52",
            query_name="Date/Brand Extended Sales",
            description="Reports extended sales by brand for a specific month and year",
            categories=[QueryCategory.JOIN, QueryCategory.AGGREGATE, QueryCategory.SORT, QueryCategory.TPCDS],
            expression_impl=q52_expression_impl,
            pandas_impl=q52_pandas_impl,
        )
    )

    # Q55: Brand Manager Sales
    register_query(
        DataFrameQuery(
            query_id="Q55",
            query_name="Brand Manager Sales",
            description="Reports brand sales for items managed by a specific manager",
            categories=[QueryCategory.JOIN, QueryCategory.AGGREGATE, QueryCategory.SORT, QueryCategory.TPCDS],
            expression_impl=q55_expression_impl,
            pandas_impl=q55_pandas_impl,
        )
    )

    # Q96: Store Sales Time Count
    register_query(
        DataFrameQuery(
            query_id="Q96",
            query_name="Store Sales Time Count",
            description="Counts store sales for specific time periods",
            categories=[QueryCategory.MULTI_JOIN, QueryCategory.AGGREGATE, QueryCategory.TPCDS],
            expression_impl=q96_expression_impl,
            pandas_impl=q96_pandas_impl,
        )
    )

    # Q7: Promotion Analysis
    register_query(
        DataFrameQuery(
            query_id="Q7",
            query_name="Promotion Analysis",
            description="Reports promotional impact on store sales for specific customer demographics",
            categories=[QueryCategory.MULTI_JOIN, QueryCategory.AGGREGATE, QueryCategory.SORT, QueryCategory.TPCDS],
            expression_impl=q7_expression_impl,
            pandas_impl=q7_pandas_impl,
        )
    )

    # Q25: Store/Catalog Sales Item Analysis
    register_query(
        DataFrameQuery(
            query_id="Q25",
            query_name="Store/Catalog Sales Item Analysis",
            description="Compares store and catalog sales for items in specific quarters",
            categories=[QueryCategory.MULTI_JOIN, QueryCategory.AGGREGATE, QueryCategory.SORT, QueryCategory.TPCDS],
            expression_impl=q25_expression_impl,
            pandas_impl=q25_pandas_impl,
        )
    )

    # Q53: Store Manufacturer Sales
    register_query(
        DataFrameQuery(
            query_id="Q53",
            query_name="Store Manufacturer Sales",
            description="Reports store sales by manufacturer for specific months",
            categories=[QueryCategory.MULTI_JOIN, QueryCategory.AGGREGATE, QueryCategory.SORT, QueryCategory.TPCDS],
            expression_impl=q53_expression_impl,
            pandas_impl=q53_pandas_impl,
        )
    )

    # Q63: Store Manufacturer Profit
    register_query(
        DataFrameQuery(
            query_id="Q63",
            query_name="Store Manufacturer Profit",
            description="Reports store profit by manufacturer for specific months",
            categories=[QueryCategory.MULTI_JOIN, QueryCategory.AGGREGATE, QueryCategory.SORT, QueryCategory.TPCDS],
            expression_impl=q63_expression_impl,
            pandas_impl=q63_pandas_impl,
        )
    )

    # Q65: Store Sales Item Profit
    register_query(
        DataFrameQuery(
            query_id="Q65",
            query_name="Store Sales Item Profit",
            description="Reports store sales item revenue analysis",
            categories=[QueryCategory.MULTI_JOIN, QueryCategory.AGGREGATE, QueryCategory.SORT, QueryCategory.TPCDS],
            expression_impl=q65_expression_impl,
            pandas_impl=q65_pandas_impl,
        )
    )

    # Q68: Store Sales Customer Household
    register_query(
        DataFrameQuery(
            query_id="Q68",
            query_name="Store Sales Customer Household",
            description="Reports customer household purchases by city",
            categories=[QueryCategory.MULTI_JOIN, QueryCategory.AGGREGATE, QueryCategory.SORT, QueryCategory.TPCDS],
            expression_impl=q68_expression_impl,
            pandas_impl=q68_pandas_impl,
        )
    )

    # Q73: Store Sales Household Vehicle
    register_query(
        DataFrameQuery(
            query_id="Q73",
            query_name="Store Sales Household Vehicle",
            description="Reports customer household purchases by vehicle count",
            categories=[QueryCategory.MULTI_JOIN, QueryCategory.AGGREGATE, QueryCategory.SORT, QueryCategory.TPCDS],
            expression_impl=q73_expression_impl,
            pandas_impl=q73_pandas_impl,
        )
    )

    # Q79: Store Sales Customer/Store Profit
    register_query(
        DataFrameQuery(
            query_id="Q79",
            query_name="Store Sales Customer/Store Profit",
            description="Reports customer profit by store for specific household demographics",
            categories=[QueryCategory.MULTI_JOIN, QueryCategory.AGGREGATE, QueryCategory.SORT, QueryCategory.TPCDS],
            expression_impl=q79_expression_impl,
            pandas_impl=q79_pandas_impl,
        )
    )

    # Q89: Store Item Sales Profit
    register_query(
        DataFrameQuery(
            query_id="Q89",
            query_name="Store Item Sales Profit",
            description="Reports monthly store sales average and deviation",
            categories=[QueryCategory.MULTI_JOIN, QueryCategory.AGGREGATE, QueryCategory.SORT, QueryCategory.TPCDS],
            expression_impl=q89_expression_impl,
            pandas_impl=q89_pandas_impl,
        )
    )

    # Q98: Store Sales Item Band
    register_query(
        DataFrameQuery(
            query_id="Q98",
            query_name="Store Sales Item Band",
            description="Reports store sales by item category for specific categories",
            categories=[QueryCategory.JOIN, QueryCategory.AGGREGATE, QueryCategory.SORT, QueryCategory.TPCDS],
            expression_impl=q98_expression_impl,
            pandas_impl=q98_pandas_impl,
        )
    )

    # ==========================================================================
    # Moderate Queries - CTEs, subqueries, and more complex patterns
    # ==========================================================================

    # Q1: Customer Returns Analysis
    register_query(
        DataFrameQuery(
            query_id="Q1",
            query_name="Customer Returns Analysis",
            description="Finds customers with above-average returns for their state",
            categories=[QueryCategory.MULTI_JOIN, QueryCategory.AGGREGATE, QueryCategory.SUBQUERY, QueryCategory.TPCDS],
            expression_impl=q1_expression_impl,
            pandas_impl=q1_pandas_impl,
        )
    )

    # Q6: Customer State/Item Analysis
    register_query(
        DataFrameQuery(
            query_id="Q6",
            query_name="Customer State/Item Analysis",
            description="Finds items where customers are concentrated by state",
            categories=[QueryCategory.MULTI_JOIN, QueryCategory.AGGREGATE, QueryCategory.SUBQUERY, QueryCategory.TPCDS],
            expression_impl=q6_expression_impl,
            pandas_impl=q6_pandas_impl,
        )
    )

    # Q12: Web Sales Item Analysis
    register_query(
        DataFrameQuery(
            query_id="Q12",
            query_name="Web Sales Item Analysis",
            description="Reports web sales by item category",
            categories=[QueryCategory.JOIN, QueryCategory.AGGREGATE, QueryCategory.SORT, QueryCategory.TPCDS],
            expression_impl=q12_expression_impl,
            pandas_impl=q12_pandas_impl,
        )
    )

    # Q15: Catalog Sales Analysis
    register_query(
        DataFrameQuery(
            query_id="Q15",
            query_name="Catalog Sales Analysis",
            description="Reports catalog sales for customers in specific zip codes",
            categories=[QueryCategory.MULTI_JOIN, QueryCategory.AGGREGATE, QueryCategory.SORT, QueryCategory.TPCDS],
            expression_impl=q15_expression_impl,
            pandas_impl=q15_pandas_impl,
        )
    )

    # Q20: Catalog Sales Item Analysis
    register_query(
        DataFrameQuery(
            query_id="Q20",
            query_name="Catalog Sales Item Analysis",
            description="Reports catalog sales by item category",
            categories=[QueryCategory.JOIN, QueryCategory.AGGREGATE, QueryCategory.SORT, QueryCategory.TPCDS],
            expression_impl=q20_expression_impl,
            pandas_impl=q20_pandas_impl,
        )
    )

    # Q26: Catalog Sales Promo Analysis
    register_query(
        DataFrameQuery(
            query_id="Q26",
            query_name="Catalog Sales Promo Analysis",
            description="Reports catalog sales promotion effectiveness",
            categories=[QueryCategory.MULTI_JOIN, QueryCategory.AGGREGATE, QueryCategory.SORT, QueryCategory.TPCDS],
            expression_impl=q26_expression_impl,
            pandas_impl=q26_pandas_impl,
        )
    )

    # Q32: Catalog Sales Excess Discount
    register_query(
        DataFrameQuery(
            query_id="Q32",
            query_name="Catalog Sales Excess Discount",
            description="Finds items sold with excess discount",
            categories=[QueryCategory.JOIN, QueryCategory.AGGREGATE, QueryCategory.SUBQUERY, QueryCategory.TPCDS],
            expression_impl=q32_expression_impl,
            pandas_impl=q32_pandas_impl,
        )
    )

    # Q82: Store Sales Inventory
    register_query(
        DataFrameQuery(
            query_id="Q82",
            query_name="Store Sales Inventory",
            description="Reports items in inventory with specific constraints",
            categories=[QueryCategory.MULTI_JOIN, QueryCategory.AGGREGATE, QueryCategory.TPCDS],
            expression_impl=q82_expression_impl,
            pandas_impl=q82_pandas_impl,
        )
    )

    # Q92: Web Sales Discount
    register_query(
        DataFrameQuery(
            query_id="Q92",
            query_name="Web Sales Discount",
            description="Finds web items sold with excess discount",
            categories=[QueryCategory.JOIN, QueryCategory.AGGREGATE, QueryCategory.SUBQUERY, QueryCategory.TPCDS],
            expression_impl=q92_expression_impl,
            pandas_impl=q92_pandas_impl,
        )
    )

    # ==========================================================================
    # Complex Queries - Advanced patterns
    # ==========================================================================

    # Q37: Item Inventory Analysis
    register_query(
        DataFrameQuery(
            query_id="Q37",
            query_name="Item Inventory Analysis",
            description="Reports items in inventory within price and date constraints",
            categories=[QueryCategory.MULTI_JOIN, QueryCategory.AGGREGATE, QueryCategory.TPCDS],
            expression_impl=q37_expression_impl,
            pandas_impl=q37_pandas_impl,
        )
    )

    # Q46: Store Sales Household Analysis
    register_query(
        DataFrameQuery(
            query_id="Q46",
            query_name="Store Sales Household Analysis",
            description="Reports store sales by customer household across cities",
            categories=[QueryCategory.MULTI_JOIN, QueryCategory.AGGREGATE, QueryCategory.SORT, QueryCategory.TPCDS],
            expression_impl=q46_expression_impl,
            pandas_impl=q46_pandas_impl,
        )
    )

    # Q50: Store Sales Returns Analysis
    register_query(
        DataFrameQuery(
            query_id="Q50",
            query_name="Store Sales Returns Analysis",
            description="Analyzes store sales return patterns",
            categories=[QueryCategory.MULTI_JOIN, QueryCategory.AGGREGATE, QueryCategory.SORT, QueryCategory.TPCDS],
            expression_impl=q50_expression_impl,
            pandas_impl=q50_pandas_impl,
        )
    )

    # Q72: Catalog Sales Inventory
    register_query(
        DataFrameQuery(
            query_id="Q72",
            query_name="Catalog Sales Inventory",
            description="Reports catalog sales vs inventory analysis",
            categories=[
                QueryCategory.MULTI_JOIN,
                QueryCategory.AGGREGATE,
                QueryCategory.ANALYTICAL,
                QueryCategory.TPCDS,
            ],
            expression_impl=q72_expression_impl,
            pandas_impl=q72_pandas_impl,
        )
    )

    # ==========================================================================
    # Additional Queries - Batch 1 Implementation
    # ==========================================================================

    # Q13: Store Sales Demographics Analysis
    register_query(
        DataFrameQuery(
            query_id="Q13",
            query_name="Store Sales Demographics Analysis",
            description="Computes aggregate statistics filtered by demographics and geography",
            categories=[QueryCategory.MULTI_JOIN, QueryCategory.AGGREGATE, QueryCategory.TPCDS],
            expression_impl=q13_expression_impl,
            pandas_impl=q13_pandas_impl,
        )
    )

    # Q34: Store Sales County Household
    register_query(
        DataFrameQuery(
            query_id="Q34",
            query_name="Store Sales County Household",
            description="Analyzes customer purchases by county and household demographics",
            categories=[QueryCategory.MULTI_JOIN, QueryCategory.AGGREGATE, QueryCategory.SUBQUERY, QueryCategory.TPCDS],
            expression_impl=q34_expression_impl,
            pandas_impl=q34_pandas_impl,
        )
    )

    # Q48: Store Sales Quantity Demographics
    register_query(
        DataFrameQuery(
            query_id="Q48",
            query_name="Store Sales Quantity Demographics",
            description="Computes sum of store sales quantity filtered by demographics",
            categories=[QueryCategory.MULTI_JOIN, QueryCategory.AGGREGATE, QueryCategory.TPCDS],
            expression_impl=q48_expression_impl,
            pandas_impl=q48_pandas_impl,
        )
    )

    # Q62: Web Sales Delivery Analysis
    register_query(
        DataFrameQuery(
            query_id="Q62",
            query_name="Web Sales Delivery Analysis",
            description="Analyzes web sales delivery times by warehouse, ship mode, and web site",
            categories=[QueryCategory.MULTI_JOIN, QueryCategory.AGGREGATE, QueryCategory.SORT, QueryCategory.TPCDS],
            expression_impl=q62_expression_impl,
            pandas_impl=q62_pandas_impl,
        )
    )

    # Q99: Catalog Sales Delivery Analysis
    register_query(
        DataFrameQuery(
            query_id="Q99",
            query_name="Catalog Sales Delivery Analysis",
            description="Analyzes catalog sales delivery times by warehouse, ship mode, and call center",
            categories=[QueryCategory.MULTI_JOIN, QueryCategory.AGGREGATE, QueryCategory.SORT, QueryCategory.TPCDS],
            expression_impl=q99_expression_impl,
            pandas_impl=q99_pandas_impl,
        )
    )

    # Q45: Web Sales Customer Zip Analysis
    register_query(
        DataFrameQuery(
            query_id="Q45",
            query_name="Web Sales Customer Zip Analysis",
            description="Reports web sales by customer zip code with OR filter on zip or item",
            categories=[QueryCategory.MULTI_JOIN, QueryCategory.AGGREGATE, QueryCategory.SORT, QueryCategory.TPCDS],
            expression_impl=q45_expression_impl,
            pandas_impl=q45_pandas_impl,
        )
    )

    # Q90: Web Sales AM/PM Ratio Analysis
    register_query(
        DataFrameQuery(
            query_id="Q90",
            query_name="Web Sales AM/PM Ratio Analysis",
            description="Computes the ratio of AM to PM web sales counts",
            categories=[QueryCategory.MULTI_JOIN, QueryCategory.AGGREGATE, QueryCategory.TPCDS],
            expression_impl=q90_expression_impl,
            pandas_impl=q90_pandas_impl,
        )
    )

    # Q91: Call Center Returns Analysis
    register_query(
        DataFrameQuery(
            query_id="Q91",
            query_name="Call Center Returns Analysis",
            description="Reports catalog returns by call center with demographic filtering",
            categories=[QueryCategory.MULTI_JOIN, QueryCategory.AGGREGATE, QueryCategory.SORT, QueryCategory.TPCDS],
            expression_impl=q91_expression_impl,
            pandas_impl=q91_pandas_impl,
        )
    )

    # ==========================================================================
    # Batch 2: Returns Queries
    # ==========================================================================

    # Q30: Web Returns Customer Analysis
    register_query(
        DataFrameQuery(
            query_id="Q30",
            query_name="Web Returns Customer Analysis",
            description="Identifies customers with web return amounts above state average",
            categories=[QueryCategory.MULTI_JOIN, QueryCategory.AGGREGATE, QueryCategory.SORT, QueryCategory.TPCDS],
            expression_impl=q30_expression_impl,
            pandas_impl=q30_pandas_impl,
        )
    )

    # Q81: Catalog Returns Customer Analysis
    register_query(
        DataFrameQuery(
            query_id="Q81",
            query_name="Catalog Returns Customer Analysis",
            description="Identifies customers with catalog return amounts above state average",
            categories=[QueryCategory.MULTI_JOIN, QueryCategory.AGGREGATE, QueryCategory.SORT, QueryCategory.TPCDS],
            expression_impl=q81_expression_impl,
            pandas_impl=q81_pandas_impl,
        )
    )

    # Q83: Cross-Channel Returns Analysis
    register_query(
        DataFrameQuery(
            query_id="Q83",
            query_name="Cross-Channel Returns Analysis",
            description="Compares return quantities across store, catalog, and web channels",
            categories=[
                QueryCategory.MULTI_JOIN,
                QueryCategory.AGGREGATE,
                QueryCategory.SORT,
                QueryCategory.TPCDS,
            ],
            expression_impl=q83_expression_impl,
            pandas_impl=q83_pandas_impl,
        )
    )

    # Q41: Item Dimension Analysis
    register_query(
        DataFrameQuery(
            query_id="Q41",
            query_name="Item Dimension Analysis",
            description="Finds product names from items matching manufacturer and attribute criteria",
            categories=[QueryCategory.FILTER, QueryCategory.SORT, QueryCategory.TPCDS],
            expression_impl=q41_expression_impl,
            pandas_impl=q41_pandas_impl,
        )
    )

    # ==========================================================================
    # Batch 3: Window Function Queries
    # ==========================================================================

    # Q86: Web Sales ROLLUP with Rank
    register_query(
        DataFrameQuery(
            query_id="Q86",
            query_name="Web Sales ROLLUP with Rank",
            description="Aggregates web sales with ROLLUP and hierarchical ranking",
            categories=[
                QueryCategory.MULTI_JOIN,
                QueryCategory.AGGREGATE,
                QueryCategory.ANALYTICAL,
                QueryCategory.TPCDS,
            ],
            expression_impl=q86_expression_impl,
            pandas_impl=q86_pandas_impl,
        )
    )

    # Q36: Gross Margin ROLLUP with Rank
    register_query(
        DataFrameQuery(
            query_id="Q36",
            query_name="Gross Margin ROLLUP with Rank",
            description="Computes gross margin with ROLLUP and hierarchical ranking by state",
            categories=[
                QueryCategory.MULTI_JOIN,
                QueryCategory.AGGREGATE,
                QueryCategory.ANALYTICAL,
                QueryCategory.TPCDS,
            ],
            expression_impl=q36_expression_impl,
            pandas_impl=q36_pandas_impl,
        )
    )

    # Q51: Cumulative Web/Store Sales
    register_query(
        DataFrameQuery(
            query_id="Q51",
            query_name="Cumulative Web/Store Sales",
            description="Compares cumulative web and store sales where web exceeds store",
            categories=[
                QueryCategory.MULTI_JOIN,
                QueryCategory.AGGREGATE,
                QueryCategory.ANALYTICAL,
                QueryCategory.TPCDS,
            ],
            expression_impl=q51_expression_impl,
            pandas_impl=q51_pandas_impl,
        )
    )

    # Q47: Store Sales Rolling Average
    register_query(
        DataFrameQuery(
            query_id="Q47",
            query_name="Store Sales Rolling Average",
            description="Analyzes monthly store sales with rolling average deviation",
            categories=[
                QueryCategory.MULTI_JOIN,
                QueryCategory.AGGREGATE,
                QueryCategory.ANALYTICAL,
                QueryCategory.TPCDS,
            ],
            expression_impl=q47_expression_impl,
            pandas_impl=q47_pandas_impl,
        )
    )

    # Q57: Catalog Sales Rolling Average
    register_query(
        DataFrameQuery(
            query_id="Q57",
            query_name="Catalog Sales Rolling Average",
            description="Analyzes monthly catalog sales with rolling average deviation",
            categories=[
                QueryCategory.MULTI_JOIN,
                QueryCategory.AGGREGATE,
                QueryCategory.ANALYTICAL,
                QueryCategory.TPCDS,
            ],
            expression_impl=q57_expression_impl,
            pandas_impl=q57_pandas_impl,
        )
    )

    # Q67: Extensive ROLLUP with Rank
    register_query(
        DataFrameQuery(
            query_id="Q67",
            query_name="Extensive ROLLUP with Rank",
            description="8-column ROLLUP on item/date/store with hierarchical ranking",
            categories=[
                QueryCategory.MULTI_JOIN,
                QueryCategory.AGGREGATE,
                QueryCategory.ANALYTICAL,
                QueryCategory.TPCDS,
            ],
            expression_impl=q67_expression_impl,
            pandas_impl=q67_pandas_impl,
        )
    )

    # Q70: Store Sales ROLLUP with Subquery Filter
    register_query(
        DataFrameQuery(
            query_id="Q70",
            query_name="Store Sales ROLLUP with Subquery Filter",
            description="State/county ROLLUP with top-5 states subquery filter",
            categories=[
                QueryCategory.MULTI_JOIN,
                QueryCategory.AGGREGATE,
                QueryCategory.ANALYTICAL,
                QueryCategory.SUBQUERY,
                QueryCategory.TPCDS,
            ],
            expression_impl=q70_expression_impl,
            pandas_impl=q70_pandas_impl,
        )
    )

    # ==========================================================================
    # Batch 4: Multi-Channel UNION Queries
    # ==========================================================================

    # Q31: Store/Web Quarterly Sales Growth by County
    register_query(
        DataFrameQuery(
            query_id="Q31",
            query_name="Store/Web Quarterly Sales Growth",
            description="Compares Q1->Q2 and Q2->Q3 growth between store and web channels by county",
            categories=[
                QueryCategory.MULTI_JOIN,
                QueryCategory.AGGREGATE,
                QueryCategory.TPCDS,
            ],
            expression_impl=q31_expression_impl,
            pandas_impl=q31_pandas_impl,
        )
    )

    # Q33: Three-Channel Sales by Manufacturer (Category Filter)
    register_query(
        DataFrameQuery(
            query_id="Q33",
            query_name="Three-Channel Sales by Manufacturer",
            description="Union of store/catalog/web sales by manufacturer filtered by category",
            categories=[
                QueryCategory.MULTI_JOIN,
                QueryCategory.AGGREGATE,
                QueryCategory.ANALYTICAL,
                QueryCategory.TPCDS,
            ],
            expression_impl=q33_expression_impl,
            pandas_impl=q33_pandas_impl,
        )
    )

    # Q56: Three-Channel Sales by Item (Color Filter)
    register_query(
        DataFrameQuery(
            query_id="Q56",
            query_name="Three-Channel Sales by Item (Color)",
            description="Union of store/catalog/web sales by item filtered by color",
            categories=[
                QueryCategory.MULTI_JOIN,
                QueryCategory.AGGREGATE,
                QueryCategory.ANALYTICAL,
                QueryCategory.TPCDS,
            ],
            expression_impl=q56_expression_impl,
            pandas_impl=q56_pandas_impl,
        )
    )

    # Q60: Three-Channel Sales by Item (Category Filter)
    register_query(
        DataFrameQuery(
            query_id="Q60",
            query_name="Three-Channel Sales by Item (Category)",
            description="Union of store/catalog/web sales by item filtered by category",
            categories=[
                QueryCategory.MULTI_JOIN,
                QueryCategory.AGGREGATE,
                QueryCategory.ANALYTICAL,
                QueryCategory.TPCDS,
            ],
            expression_impl=q60_expression_impl,
            pandas_impl=q60_pandas_impl,
        )
    )

    # Q71: Three-Channel Sales by Brand and Time
    register_query(
        DataFrameQuery(
            query_id="Q71",
            query_name="Three-Channel Sales by Brand and Time",
            description="Union of web/catalog/store sales by brand and meal time",
            categories=[
                QueryCategory.MULTI_JOIN,
                QueryCategory.AGGREGATE,
                QueryCategory.ANALYTICAL,
                QueryCategory.TPCDS,
            ],
            expression_impl=q71_expression_impl,
            pandas_impl=q71_pandas_impl,
        )
    )

    # Q74: Store/Web Customer Year-over-Year Growth
    register_query(
        DataFrameQuery(
            query_id="Q74",
            query_name="Store/Web Customer Year-over-Year Growth",
            description="Finds customers where web sales growth exceeds store sales growth",
            categories=[
                QueryCategory.MULTI_JOIN,
                QueryCategory.AGGREGATE,
                QueryCategory.ANALYTICAL,
                QueryCategory.TPCDS,
            ],
            expression_impl=q74_expression_impl,
            pandas_impl=q74_pandas_impl,
        )
    )

    # Q76: Three-Channel Sales with NULL Column Filter
    register_query(
        DataFrameQuery(
            query_id="Q76",
            query_name="Three-Channel Sales with NULL Column Filter",
            description="Sales analysis across channels where specific columns are NULL",
            categories=[
                QueryCategory.MULTI_JOIN,
                QueryCategory.AGGREGATE,
                QueryCategory.ANALYTICAL,
                QueryCategory.TPCDS,
            ],
            expression_impl=q76_expression_impl,
            pandas_impl=q76_pandas_impl,
        )
    )

    # Q97: Store/Catalog Customer-Item Overlap
    register_query(
        DataFrameQuery(
            query_id="Q97",
            query_name="Store/Catalog Customer-Item Overlap",
            description="Analyzes customer-item purchase overlap between store and catalog channels",
            categories=[
                QueryCategory.MULTI_JOIN,
                QueryCategory.AGGREGATE,
                QueryCategory.ANALYTICAL,
                QueryCategory.TPCDS,
            ],
            expression_impl=q97_expression_impl,
            pandas_impl=q97_pandas_impl,
        )
    )

    # Q49: Three-Channel Return Ratio Ranking
    register_query(
        DataFrameQuery(
            query_id="Q49",
            query_name="Three-Channel Return Ratio Ranking",
            description="Calculates return and currency ratios per item across channels with ranking",
            categories=[
                QueryCategory.MULTI_JOIN,
                QueryCategory.AGGREGATE,
                QueryCategory.WINDOW,
                QueryCategory.ANALYTICAL,
                QueryCategory.TPCDS,
            ],
            expression_impl=q49_expression_impl,
            pandas_impl=q49_pandas_impl,
        )
    )

    # Q75: Three-Channel Sales with Returns Year-over-Year
    register_query(
        DataFrameQuery(
            query_id="Q75",
            query_name="Three-Channel Sales with Returns Year-over-Year",
            description="Year-over-year comparison of net sales (sales - returns) by item attributes",
            categories=[
                QueryCategory.MULTI_JOIN,
                QueryCategory.AGGREGATE,
                QueryCategory.ANALYTICAL,
                QueryCategory.TPCDS,
            ],
            expression_impl=q75_expression_impl,
            pandas_impl=q75_pandas_impl,
        )
    )

    # Q78: Three-Channel Sales without Returns Comparison
    register_query(
        DataFrameQuery(
            query_id="Q78",
            query_name="Three-Channel Sales without Returns Comparison",
            description="Compares store sales to combined web+catalog sales excluding returned items",
            categories=[
                QueryCategory.MULTI_JOIN,
                QueryCategory.AGGREGATE,
                QueryCategory.ANALYTICAL,
                QueryCategory.TPCDS,
            ],
            expression_impl=q78_expression_impl,
            pandas_impl=q78_pandas_impl,
        )
    )

    # Q80: Three-Channel Sales-Returns with ROLLUP
    register_query(
        DataFrameQuery(
            query_id="Q80",
            query_name="Three-Channel Sales-Returns with ROLLUP",
            description="Aggregates sales and returns across channels with ROLLUP for subtotals",
            categories=[
                QueryCategory.MULTI_JOIN,
                QueryCategory.AGGREGATE,
                QueryCategory.ANALYTICAL,
                QueryCategory.TPCDS,
            ],
            expression_impl=q80_expression_impl,
            pandas_impl=q80_pandas_impl,
        )
    )

    # Q77: Three-Channel Sales-Returns with ROLLUP (Separate CTEs)
    register_query(
        DataFrameQuery(
            query_id="Q77",
            query_name="Three-Channel Sales-Returns with Separate CTEs",
            description="Aggregates sales and returns separately for each channel with ROLLUP",
            categories=[
                QueryCategory.MULTI_JOIN,
                QueryCategory.AGGREGATE,
                QueryCategory.ANALYTICAL,
                QueryCategory.TPCDS,
            ],
            expression_impl=q77_expression_impl,
            pandas_impl=q77_pandas_impl,
        )
    )

    # Q58: Three-Channel Item Sales by Week Balance Check
    register_query(
        DataFrameQuery(
            query_id="Q58",
            query_name="Three-Channel Item Sales Balance Check",
            description="Items with balanced sales across all channels within 10% range",
            categories=[
                QueryCategory.MULTI_JOIN,
                QueryCategory.AGGREGATE,
                QueryCategory.ANALYTICAL,
                QueryCategory.TPCDS,
            ],
            expression_impl=q58_expression_impl,
            pandas_impl=q58_pandas_impl,
        )
    )

    # Q54: Catalog+Web Customer Store Revenue Segments
    register_query(
        DataFrameQuery(
            query_id="Q54",
            query_name="Catalog+Web Customer Store Revenue Segments",
            description="Segments catalog/web customers by their store sales revenue",
            categories=[
                QueryCategory.MULTI_JOIN,
                QueryCategory.AGGREGATE,
                QueryCategory.ANALYTICAL,
                QueryCategory.TPCDS,
            ],
            expression_impl=q54_expression_impl,
            pandas_impl=q54_pandas_impl,
        )
    )

    # Q4: Customer Year-over-Year Comparison Across All Channels
    register_query(
        DataFrameQuery(
            query_id="Q4",
            query_name="Customer Year-over-Year Comparison",
            description="Compares customer totals across store/catalog/web, filtering for catalog growth",
            categories=[
                QueryCategory.MULTI_JOIN,
                QueryCategory.AGGREGATE,
                QueryCategory.ANALYTICAL,
                QueryCategory.TPCDS,
            ],
            expression_impl=q4_expression_impl,
            pandas_impl=q4_pandas_impl,
        )
    )

    # Q5: Three-Channel Sales-Returns with ROLLUP (Sales/Returns Union)
    register_query(
        DataFrameQuery(
            query_id="Q5",
            query_name="Three-Channel Sales-Returns Union with ROLLUP",
            description="Unions sales and returns per channel then aggregates with ROLLUP",
            categories=[
                QueryCategory.MULTI_JOIN,
                QueryCategory.AGGREGATE,
                QueryCategory.ANALYTICAL,
                QueryCategory.TPCDS,
            ],
            expression_impl=q5_expression_impl,
            pandas_impl=q5_pandas_impl,
        )
    )

    # Q10: Customer Demographics with Multi-Channel Semi-Joins
    register_query(
        DataFrameQuery(
            query_id="Q10",
            query_name="Customer Demographics Multi-Channel",
            description="Demographics for customers in store AND (web OR catalog)",
            categories=[
                QueryCategory.MULTI_JOIN,
                QueryCategory.AGGREGATE,
                QueryCategory.SUBQUERY,
                QueryCategory.TPCDS,
            ],
            expression_impl=q10_expression_impl,
            pandas_impl=q10_pandas_impl,
        )
    )

    # Q11: Store/Web Year-over-Year Customer Comparison
    register_query(
        DataFrameQuery(
            query_id="Q11",
            query_name="Store/Web Year-over-Year Customer Comparison",
            description="Compares customer totals between store and web channels year-over-year",
            categories=[
                QueryCategory.MULTI_JOIN,
                QueryCategory.AGGREGATE,
                QueryCategory.ANALYTICAL,
                QueryCategory.TPCDS,
            ],
            expression_impl=q11_expression_impl,
            pandas_impl=q11_pandas_impl,
        )
    )

    # Q35: Customer Demographics with Multi-Channel Semi-Joins (Quarterly)
    register_query(
        DataFrameQuery(
            query_id="Q35",
            query_name="Customer Demographics Multi-Channel Quarterly",
            description="Demographics with state for customers in store AND (web OR catalog)",
            categories=[
                QueryCategory.MULTI_JOIN,
                QueryCategory.AGGREGATE,
                QueryCategory.SUBQUERY,
                QueryCategory.TPCDS,
            ],
            expression_impl=q35_expression_impl,
            pandas_impl=q35_pandas_impl,
        )
    )

    # Q38: Three-Channel Customer INTERSECT
    register_query(
        DataFrameQuery(
            query_id="Q38",
            query_name="Three-Channel Customer INTERSECT",
            description="Count customers who bought in all three channels on same date",
            categories=[
                QueryCategory.MULTI_JOIN,
                QueryCategory.SUBQUERY,
                QueryCategory.TPCDS,
            ],
            expression_impl=q38_expression_impl,
            pandas_impl=q38_pandas_impl,
        )
    )

    # Q87: Three-Channel Customer EXCEPT (Store Only)
    register_query(
        DataFrameQuery(
            query_id="Q87",
            query_name="Three-Channel Customer EXCEPT",
            description="Count customers who bought from store but NOT catalog/web",
            categories=[
                QueryCategory.MULTI_JOIN,
                QueryCategory.SUBQUERY,
                QueryCategory.TPCDS,
            ],
            expression_impl=q87_expression_impl,
            pandas_impl=q87_pandas_impl,
        )
    )

    # Q69: Customer Demographics for Store-Only Customers
    register_query(
        DataFrameQuery(
            query_id="Q69",
            query_name="Customer Demographics Store-Only",
            description="Demographics for customers who only shop in store (NOT web/catalog)",
            categories=[
                QueryCategory.MULTI_JOIN,
                QueryCategory.AGGREGATE,
                QueryCategory.SUBQUERY,
                QueryCategory.TPCDS,
            ],
            expression_impl=q69_expression_impl,
            pandas_impl=q69_pandas_impl,
        )
    )

    # Q40: Catalog Sales Before/After with Returns
    register_query(
        DataFrameQuery(
            query_id="Q40",
            query_name="Catalog Sales Before/After Returns",
            description="Compares catalog sales before/after date accounting for returns",
            categories=[
                QueryCategory.MULTI_JOIN,
                QueryCategory.AGGREGATE,
                QueryCategory.ANALYTICAL,
                QueryCategory.TPCDS,
            ],
            expression_impl=q40_expression_impl,
            pandas_impl=q40_pandas_impl,
        )
    )

    # Q16: Catalog Orders from Multiple Warehouses Not Returned
    register_query(
        DataFrameQuery(
            query_id="Q16",
            query_name="Catalog Multi-Warehouse Orders Not Returned",
            description="Catalog orders shipped from multiple warehouses not returned",
            categories=[
                QueryCategory.MULTI_JOIN,
                QueryCategory.AGGREGATE,
                QueryCategory.SUBQUERY,
                QueryCategory.TPCDS,
            ],
            expression_impl=q16_expression_impl,
            pandas_impl=q16_pandas_impl,
        )
    )

    # Q17: Store Sales/Returns + Catalog Sales Statistics
    register_query(
        DataFrameQuery(
            query_id="Q17",
            query_name="Store/Catalog Sales-Returns Statistics",
            description="Store sales/returns joined with catalog sales with quantity statistics",
            categories=[
                QueryCategory.MULTI_JOIN,
                QueryCategory.AGGREGATE,
                QueryCategory.ANALYTICAL,
                QueryCategory.TPCDS,
            ],
            expression_impl=q17_expression_impl,
            pandas_impl=q17_pandas_impl,
        )
    )

    # Q18: Catalog Sales with Demographics and ROLLUP
    register_query(
        DataFrameQuery(
            query_id="Q18",
            query_name="Catalog Sales Demographics ROLLUP",
            description="Catalog sales with demographics aggregated with ROLLUP on geography",
            categories=[
                QueryCategory.MULTI_JOIN,
                QueryCategory.AGGREGATE,
                QueryCategory.ANALYTICAL,
                QueryCategory.TPCDS,
            ],
            expression_impl=q18_expression_impl,
            pandas_impl=q18_pandas_impl,
        )
    )

    # Q29: Store Sales/Returns + Catalog Sales Aggregation
    register_query(
        DataFrameQuery(
            query_id="Q29",
            query_name="Store/Catalog Sales-Returns Aggregation",
            description="Store sales/returns joined with catalog sales aggregated by item/store",
            categories=[
                QueryCategory.MULTI_JOIN,
                QueryCategory.AGGREGATE,
                QueryCategory.ANALYTICAL,
                QueryCategory.TPCDS,
            ],
            expression_impl=q29_expression_impl,
            pandas_impl=q29_pandas_impl,
        )
    )

    # Q27: Store Sales with Demographics and ROLLUP
    register_query(
        DataFrameQuery(
            query_id="Q27",
            query_name="Store Sales Demographics ROLLUP",
            description="Store sales with demographics aggregated with ROLLUP on item/state",
            categories=[
                QueryCategory.MULTI_JOIN,
                QueryCategory.AGGREGATE,
                QueryCategory.ANALYTICAL,
                QueryCategory.TPCDS,
            ],
            expression_impl=q27_expression_impl,
            pandas_impl=q27_pandas_impl,
        )
    )

    # Q93: Store Sales with Returns - Actual Sales
    register_query(
        DataFrameQuery(
            query_id="Q93",
            query_name="Store Sales Returns Actual Sales",
            description="Store sales with returns computing actual sales by customer filtered by reason",
            categories=[
                QueryCategory.MULTI_JOIN,
                QueryCategory.AGGREGATE,
                QueryCategory.ANALYTICAL,
                QueryCategory.TPCDS,
            ],
            expression_impl=q93_expression_impl,
            pandas_impl=q93_pandas_impl,
        )
    )

    # Q94: Web Orders from Multiple Warehouses Not Returned
    register_query(
        DataFrameQuery(
            query_id="Q94",
            query_name="Web Multi-Warehouse Orders Not Returned",
            description="Web orders shipped from multiple warehouses that were not returned",
            categories=[
                QueryCategory.MULTI_JOIN,
                QueryCategory.AGGREGATE,
                QueryCategory.SUBQUERY,
                QueryCategory.TPCDS,
            ],
            expression_impl=q94_expression_impl,
            pandas_impl=q94_pandas_impl,
        )
    )

    # Q44: Store Sales Item Ranking - Best and Worst Performers
    register_query(
        DataFrameQuery(
            query_id="Q44",
            query_name="Store Sales Item Ranking",
            description="Ranks items by net profit for a store, finding best and worst performers",
            categories=[
                QueryCategory.MULTI_JOIN,
                QueryCategory.AGGREGATE,
                QueryCategory.WINDOW,
                QueryCategory.TPCDS,
            ],
            expression_impl=q44_expression_impl,
            pandas_impl=q44_pandas_impl,
        )
    )

    # Q59: Store Weekly Sales Year-over-Year Comparison
    register_query(
        DataFrameQuery(
            query_id="Q59",
            query_name="Store Weekly Sales Year-over-Year",
            description="Compares weekly sales by day of week between current and prior year",
            categories=[
                QueryCategory.MULTI_JOIN,
                QueryCategory.AGGREGATE,
                QueryCategory.ANALYTICAL,
                QueryCategory.TPCDS,
            ],
            expression_impl=q59_expression_impl,
            pandas_impl=q59_pandas_impl,
        )
    )

    # Q61: Promotional Sales vs Total Sales Ratio
    register_query(
        DataFrameQuery(
            query_id="Q61",
            query_name="Promotional Sales Ratio",
            description="Compares promotional sales to total sales for a category by GMT offset",
            categories=[
                QueryCategory.MULTI_JOIN,
                QueryCategory.AGGREGATE,
                QueryCategory.ANALYTICAL,
                QueryCategory.TPCDS,
            ],
            expression_impl=q61_expression_impl,
            pandas_impl=q61_pandas_impl,
        )
    )

    # Q95: Web Orders from Multiple Warehouses WITH Returns
    register_query(
        DataFrameQuery(
            query_id="Q95",
            query_name="Web Multi-Warehouse Orders With Returns",
            description="Web orders shipped from multiple warehouses that were returned",
            categories=[
                QueryCategory.MULTI_JOIN,
                QueryCategory.AGGREGATE,
                QueryCategory.SUBQUERY,
                QueryCategory.TPCDS,
            ],
            expression_impl=q95_expression_impl,
            pandas_impl=q95_pandas_impl,
        )
    )

    # Q85: Web Sales Returns with Demographics
    register_query(
        DataFrameQuery(
            query_id="Q85",
            query_name="Web Sales Returns Demographics",
            description="Analyzes web returns by reason with customer demographics filtering",
            categories=[
                QueryCategory.MULTI_JOIN,
                QueryCategory.AGGREGATE,
                QueryCategory.ANALYTICAL,
                QueryCategory.TPCDS,
            ],
            expression_impl=q85_expression_impl,
            pandas_impl=q85_pandas_impl,
        )
    )

    # Q66: Web/Catalog Sales Monthly Warehouse Analysis
    register_query(
        DataFrameQuery(
            query_id="Q66",
            query_name="Web/Catalog Warehouse Monthly Sales",
            description="Union of web and catalog sales aggregated by warehouse and month",
            categories=[
                QueryCategory.MULTI_JOIN,
                QueryCategory.AGGREGATE,
                QueryCategory.ANALYTICAL,
                QueryCategory.TPCDS,
            ],
            expression_impl=q66_expression_impl,
            pandas_impl=q66_pandas_impl,
        )
    )

    # Q8: Store Sales Net Profit by Store with Zip Code Analysis
    register_query(
        DataFrameQuery(
            query_id="Q8",
            query_name="Store Sales Zip Code Net Profit",
            description="Analyzes store net profit filtered by preferred customer zip codes",
            categories=[
                QueryCategory.MULTI_JOIN,
                QueryCategory.AGGREGATE,
                QueryCategory.SUBQUERY,
                QueryCategory.TPCDS,
            ],
            expression_impl=q8_expression_impl,
            pandas_impl=q8_pandas_impl,
        )
    )

    # Q9: Store Sales Extended Price Conditional Analysis
    register_query(
        DataFrameQuery(
            query_id="Q9",
            query_name="Store Sales Quantity Bucket Analysis",
            description="Computes conditional averages across quantity buckets",
            categories=[
                QueryCategory.AGGREGATE,
                QueryCategory.ANALYTICAL,
                QueryCategory.TPCDS,
            ],
            expression_impl=q9_expression_impl,
            pandas_impl=q9_pandas_impl,
        )
    )

    # Q14: Cross-channel Item Sales Analysis
    register_query(
        DataFrameQuery(
            query_id="Q14",
            query_name="Cross-channel Item Sales",
            description="Finds items sold across all channels with ROLLUP aggregation",
            categories=[
                QueryCategory.MULTI_JOIN,
                QueryCategory.AGGREGATE,
                QueryCategory.ANALYTICAL,
                QueryCategory.TPCDS,
            ],
            expression_impl=q14_expression_impl,
            pandas_impl=q14_pandas_impl,
        )
    )

    # Q23: Frequent Items and Best Customers Analysis
    register_query(
        DataFrameQuery(
            query_id="Q23",
            query_name="Frequent Items Best Customers",
            description="Sum of catalog+web sales for frequent items by best customers",
            categories=[
                QueryCategory.MULTI_JOIN,
                QueryCategory.AGGREGATE,
                QueryCategory.SUBQUERY,
                QueryCategory.TPCDS,
            ],
            expression_impl=q23_expression_impl,
            pandas_impl=q23_pandas_impl,
        )
    )

    # Q24: Store Sales Returns Analysis by Color
    register_query(
        DataFrameQuery(
            query_id="Q24",
            query_name="Store Sales Returns by Color",
            description="Store sales with returns filtered by market and color threshold",
            categories=[
                QueryCategory.MULTI_JOIN,
                QueryCategory.AGGREGATE,
                QueryCategory.SUBQUERY,
                QueryCategory.TPCDS,
            ],
            expression_impl=q24_expression_impl,
            pandas_impl=q24_pandas_impl,
        )
    )

    # Q28: Store Sales Extended Price Analysis by Quantity Buckets
    register_query(
        DataFrameQuery(
            query_id="Q28",
            query_name="Store Sales Quantity Bucket Extended",
            description="Extended price analysis across 6 quantity buckets with filters",
            categories=[
                QueryCategory.AGGREGATE,
                QueryCategory.ANALYTICAL,
                QueryCategory.TPCDS,
            ],
            expression_impl=q28_expression_impl,
            pandas_impl=q28_pandas_impl,
        )
    )

    # Q88: Store Sales Time Period Analysis
    register_query(
        DataFrameQuery(
            query_id="Q88",
            query_name="Store Sales Time Period Counts",
            description="Counts store sales by 8 half-hour time periods with demographics",
            categories=[
                QueryCategory.MULTI_JOIN,
                QueryCategory.AGGREGATE,
                QueryCategory.ANALYTICAL,
                QueryCategory.TPCDS,
            ],
            expression_impl=q88_expression_impl,
            pandas_impl=q88_pandas_impl,
        )
    )

    # Q21: Inventory Before/After Date Analysis
    register_query(
        DataFrameQuery(
            query_id="Q21",
            query_name="Inventory Before/After Date",
            description="Compares inventory quantities before and after a specific date by warehouse",
            categories=[
                QueryCategory.MULTI_JOIN,
                QueryCategory.AGGREGATE,
                QueryCategory.ANALYTICAL,
                QueryCategory.TPCDS,
            ],
            expression_impl=q21_expression_impl,
            pandas_impl=q21_pandas_impl,
        )
    )

    # Q22: Inventory ROLLUP Analysis
    register_query(
        DataFrameQuery(
            query_id="Q22",
            query_name="Inventory ROLLUP by Product",
            description="Analyzes inventory with ROLLUP by product name, brand, class, category",
            categories=[
                QueryCategory.MULTI_JOIN,
                QueryCategory.AGGREGATE,
                QueryCategory.ANALYTICAL,
                QueryCategory.TPCDS,
            ],
            expression_impl=q22_expression_impl,
            pandas_impl=q22_pandas_impl,
        )
    )

    # Q39: Inventory Variance by Month
    register_query(
        DataFrameQuery(
            query_id="Q39",
            query_name="Inventory Variance Monthly",
            description="Compares inventory variance (coefficient of variation) across consecutive months",
            categories=[
                QueryCategory.MULTI_JOIN,
                QueryCategory.AGGREGATE,
                QueryCategory.ANALYTICAL,
                QueryCategory.TPCDS,
            ],
            expression_impl=q39_expression_impl,
            pandas_impl=q39_pandas_impl,
        )
    )

    # Q64: Cross-Sales with Income Band
    register_query(
        DataFrameQuery(
            query_id="Q64",
            query_name="Cross-Sales Income Band",
            description="Complex cross-sales analysis with income band filtering and year-over-year comparison",
            categories=[
                QueryCategory.MULTI_JOIN,
                QueryCategory.AGGREGATE,
                QueryCategory.SUBQUERY,
                QueryCategory.ANALYTICAL,
                QueryCategory.TPCDS,
            ],
            expression_impl=q64_expression_impl,
            pandas_impl=q64_pandas_impl,
        )
    )

    # Q84: Customer Income Band Filter
    register_query(
        DataFrameQuery(
            query_id="Q84",
            query_name="Customer Income Band Lookup",
            description="Customer lookup filtered by city and income band with store returns",
            categories=[
                QueryCategory.MULTI_JOIN,
                QueryCategory.SUBQUERY,
                QueryCategory.TPCDS,
            ],
            expression_impl=q84_expression_impl,
            pandas_impl=q84_pandas_impl,
        )
    )


# Register all queries when module is imported
_register_all_queries()
