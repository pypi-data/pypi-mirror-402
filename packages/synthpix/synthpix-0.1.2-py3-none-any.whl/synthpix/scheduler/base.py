"""BaseFlowFieldScheduler abstract class."""

import glob
import os
from abc import ABC, abstractmethod
from typing import Any

import jax
import jax.numpy as jnp
import numpy as np
from typing_extensions import Self

from synthpix.scheduler.protocol import FileEndedError, SchedulerProtocol
from synthpix.types import PRNGKey, SchedulerData
from synthpix.utils import SYNTHPIX_SCOPE, get_logger

logger = get_logger(__name__, scope=SYNTHPIX_SCOPE)


class BaseFlowFieldScheduler(ABC, SchedulerProtocol):
    """Abstract class for scheduling access to flow field data.

    This class provides iteration, looping, caching, and batch loading.
    Subclasses must implement:
    - file-specific loading
    - y-slice extraction logic.
    """

    _file_pattern = "*"
    _file_list: list[str]

    @property
    def file_list(self) -> list[str]:
        """Returns the list of files used by the scheduler.

        Returns:
            List of file paths.
        """
        return self._file_list

    @file_list.setter
    def file_list(self, value: list[str]) -> None:
        """Sets the list of files used by the scheduler.

        Args:
            value: List of file paths.
        """
        self._file_list = value

    def __init__(  # noqa: PLR0912
        self,
        file_list: list[str],
        randomize: bool = False,
        loop: bool = False,
        key: PRNGKey | None = None,
    ) -> None:
        """Initializes the scheduler.

        Args:
            file_list:  List of file paths to flow field datasets.
            randomize: If True, shuffle the order of files.
            loop: If True, loop over the dataset indefinitely.
            key: Random key for reproducibility.

        Raises:
            ValueError: If file_list is not a list of file paths or if no
                valid files are found.
        """
        # Check if file_list is a list of files or directories
        self._file_list = []
        if (
            file_list is None
            or not isinstance(file_list, list)
            or not all(isinstance(f, str) for f in file_list)
        ):
            raise ValueError("file_list must be a list of file paths.")
        for file_path in file_list:
            if os.path.isdir(file_path):
                logger.debug(f"Searching for files in {file_path}")
                pattern = os.path.join(file_path, self._file_pattern)
                found_files = sorted(glob.glob(pattern, recursive=True))
                logger.debug(f"Found {len(found_files)} files in {file_path}")
                self._file_list.extend(found_files)
            else:
                self._file_list.append(file_path)

        if not self._file_list:
            raise ValueError("The file_list must not be empty.")

        for file_path in self._file_list:
            if not isinstance(file_path, str):
                raise ValueError("All file paths must be strings.")
            if not os.path.isfile(file_path):
                raise ValueError(f"File {file_path} does not exist.")

        if not isinstance(randomize, bool):
            raise ValueError("randomize must be a boolean value.")
        self.randomize = randomize

        if key is not None:
            self.key = key
        else:
            self.key = jax.random.PRNGKey(0)
            cpu = jax.devices("cpu")[0]
            self.key = jax.device_put(self.key, cpu)

        if not isinstance(loop, bool):
            raise ValueError("loop must be a boolean value.")
        self.loop = loop

        self.index = 0

        if self.randomize:
            self.key, shuffle_key = jax.random.split(self.key)
            cpu = jax.devices("cpu")[0]
            file_list_indices = jnp.arange(len(self.file_list), device=cpu)
            file_list_indices = jax.random.permutation(
                shuffle_key, file_list_indices
            )
            self.file_list = [
                self.file_list[i] for i in file_list_indices.tolist()
            ]

        self._cached_data: SchedulerData | None = None
        self._cached_file: str | None = None
        self._slice_idx: int = 0

        logger.debug(
            f"Initialized with {len(self.file_list)} files, "
            f"randomize={self.randomize}, loop={self.loop}"
        )

    def __len__(self) -> int:
        """Returns the number of files in the dataset.

        Returns:
            Number of files in file_list.
        """
        return len(self.file_list)

    def __iter__(self) -> Self:
        """Returns the iterator instance itself.

        Returns:
            The iterator instance.
        """
        return self

    def reset(self) -> None:
        """Resets the state."""
        self.index = 0
        self._slice_idx = 0
        self._cached_data = None
        self._cached_file = None
        if self.randomize:
            self.key, shuffle_key = jax.random.split(self.key)
            cpu = jax.devices("cpu")[0]
            file_list_indices = jnp.arange(len(self.file_list), device=cpu)
            file_list_indices = jax.random.permutation(
                shuffle_key, file_list_indices
            )
            self.file_list = [
                self.file_list[i] for i in file_list_indices.tolist()
            ]

    @property
    def state(self) -> dict[str, Any]:
        """Returns the state of the scheduler.

        Returns:
            Dictionary containing the scheduler state.
        """
        state_dict = {
            "index": self.index,
            "slice_idx": self._slice_idx,
            "randomize": self.randomize,
            "loop": self.loop,
            "file_list": self.file_list,
        }
        # Save key if it exists
        if hasattr(self, "key") and self.key is not None:
            state_dict["key"] = self.key

        return state_dict

    @state.setter
    def state(self, value: dict[str, Any]) -> None:
        """Sets the state of the scheduler.

        Args:
            value: Dictionary containing the scheduler state.

        Raises:
            KeyError: If state is missing required keys.
        """
        self.index = value["index"]
        self._slice_idx = value["slice_idx"]
        self.randomize = value["randomize"]
        self.loop = value["loop"]
        if "file_list" in value:
            self.file_list = value["file_list"]

        if "key" in value:
            self.key = value["key"]

        # Invalidate cache on state restore
        self._cached_data = None
        self._cached_file = None

    @property
    def grain_iterator(self) -> Any | None:
        """Returns the underlying Grain iterator if available.

        Returns:
            None for legacy schedulers.
        """
        return None

    def _get_next(self) -> SchedulerData:
        """Returns the next flow field slice from the dataset.

        NOTE: This outputs a single flow field slice, not a batch.

        Returns:
            A single flow field slice.

        Raises:
            StopIteration: If no more data and loop is False.
        """
        while self.index < len(self.file_list) or self.loop:
            # Handle loop around
            if self.index >= len(self.file_list):
                self.reset()

            # Get the current file path
            path = self.file_list[self.index]

            # Load and cache the file if not already cached
            if self._cached_file != path:
                self._cached_file = None
                self._cached_data = None
                try:
                    self._cached_data = self.load_file(path)
                    self._cached_file = path
                    self._slice_idx = 0
                except Exception as e:
                    logger.error(f"Error loading file {path}: {e}")
                    self.index += 1
                    continue  # Skip to the next file

            try:
                sample = self.get_next_slice()
                self._slice_idx += 1
                return sample
            except FileEndedError:
                self.index += 1
                continue  # Skip to the next file

        raise StopIteration

    def get_batch(self, batch_size: int) -> SchedulerData:  # noqa: PLR0912
        """Retrieves a batch of flow fields using the current scheduler state.

        This method repeatedly calls `__next__()` to store a batch
        of flow field slices.

        Args:
            batch_size: Number of flow field slices to retrieve.

        Returns:
            SchedulerData containing the batch of flow field slices.

        Raises:
            StopIteration: If the dataset is exhausted before reaching the
                desired batch size and `loop` is set to False.
            ValueError: If batch_size is invalid or batch assembly fails.
        """
        batch: list[SchedulerData] = []
        mask = np.ones((batch_size,), dtype=bool)
        for _ in range(batch_size):
            try:
                scheduler_data = self._get_next()
                batch.append(scheduler_data)
            except StopIteration:
                break
        if len(batch) == 0:
            if not self.loop:
                raise StopIteration
            self.reset()
        if len(batch) < batch_size and not self.loop:
            mask = np.zeros((batch_size,), dtype=bool)
            mask[: len(batch)] = True

        logger.debug(f"Loaded batch of {len(batch)} flow field slices.")

        images1, images2 = None, None
        if all(data.images1 is not None for data in batch):
            images1 = np.stack(
                [d.images1 for d in batch if d.images1 is not None]
            )
        if all(data.images2 is not None for data in batch):
            images2 = np.stack(
                [d.images2 for d in batch if d.images2 is not None]
            )

        flow_fields = np.stack([data.flow_fields for data in batch])

        if batch and any(data.files is not None for data in batch):
            if not all(
                data.files is not None and len(data.files) <= 1
                for data in batch
            ):
                raise ValueError("Inconsistent files information in batch.")

            per_item_files: list[str] = []
            for data in batch:
                if not data.files:
                    per_item_files.append("")
                else:
                    per_item_files.append(data.files[0])

            if len(batch) < batch_size:
                pad_size = batch_size - len(batch)
                per_item_files += [""] * pad_size

            files = tuple(per_item_files)
        else:
            files = None

        # Pad if needed
        if len(batch) < batch_size:
            pad_size = batch_size - len(batch)
            flow_fields = np.pad(
                flow_fields,
                ((0, pad_size), (0, 0), (0, 0), (0, 0)),
                mode="constant",
                constant_values=0,
            )
            if images1 is not None:
                images1 = np.pad(
                    images1,
                    ((0, pad_size), (0, 0), (0, 0)),
                    mode="constant",
                    constant_values=0,
                )
            if images2 is not None:
                images2 = np.pad(
                    images2,
                    ((0, pad_size), (0, 0), (0, 0)),
                    mode="constant",
                    constant_values=0,
                )

        return SchedulerData(
            flow_fields=flow_fields,
            images1=images1,
            images2=images2,
            mask=mask,
            files=files,
        )

    @abstractmethod
    def load_file(self, file_path: str) -> SchedulerData:
        """Loads a file and returns the dataset for caching.

        Args:
            file_path: Path to the file to be loaded.

        Returns:
            The loaded dataset.
        """

    @abstractmethod
    def get_next_slice(self) -> SchedulerData:
        """Extracts the next slice from the cached data.

        Returns:
            SchedulerData containing the next flow field slice
                (and optionally images).
        """

    @abstractmethod
    def get_flow_fields_shape(self) -> tuple[int, int, int]:
        """Returns the shape of the flow field.

        Returns:
            Shape of the flow field.
        """

    @classmethod
    @abstractmethod
    def from_config(cls, config: dict) -> Self:
        """Creates a BaseFlowFieldScheduler instance from a configuration.

        Args:
            config:
                Configuration dictionary containing the scheduler parameters.

        Returns:
            A BaseFlowFieldScheduler instance.
        """
