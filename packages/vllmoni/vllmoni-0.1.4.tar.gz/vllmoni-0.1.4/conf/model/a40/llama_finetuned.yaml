# conf/model/a40/llama_finetuned.yaml
model_name: "/home/ldap_users/isabellvorkastner/dida-projects/genial4kmu/outputs/finetuning/25-06-13_14:57_grpo/model"
model_name_short: "llama3.1-8B-finetuned-old-prompt-600"
gpu_memory_utilization: 0.9
temperature: 0
max_tokens: 1000
top_p: 0.95
max_model_len: 40000
tensor_parallel_size: 1
quantization: null
tool_call_parser: llama3_json
