2026-01-14 16:01:02 | INFO | P01 | === RUN START ===
2026-01-14 16:01:02 | INFO | P01 | project=Simple Tokenizer Demo
2026-01-14 16:01:02 | INFO | P01 | repo_dir=toy-gpt-train
2026-01-14 16:01:02 | INFO | P01 | python=3.14.0
2026-01-14 16:01:02 | INFO | P01 | os=Windows 11
2026-01-14 16:01:02 | INFO | P01 | shell=powershell
2026-01-14 16:01:02 | INFO | P01 | cwd=.
2026-01-14 16:01:02 | INFO | P01 | github_actions=False
2026-01-14 16:01:02 | ERROR | P01 | Corpus file not found at C:\Users\edaci\Documents\datafun\toy-gpt-train\src\corpus\000_cat_dog.txt
2026-01-14 16:01:02 | INFO | P01 | Tokenizer initialized with 0 tokens.
2026-01-14 16:01:02 | INFO | P01 | First 10 tokens: []
2026-01-14 16:01:02 | INFO | P01 | Total number of tokens: 0
2026-01-14 16:01:02 | INFO | P01 | No tokens available to calculate average length.
2026-01-14 16:02:25 | INFO | P01 | === RUN START ===
2026-01-14 16:02:25 | INFO | P01 | project=Simple Tokenizer Demo
2026-01-14 16:02:25 | INFO | P01 | repo_dir=toy-gpt-train
2026-01-14 16:02:25 | INFO | P01 | python=3.14.0
2026-01-14 16:02:25 | INFO | P01 | os=Windows 11
2026-01-14 16:02:25 | INFO | P01 | shell=powershell
2026-01-14 16:02:25 | INFO | P01 | cwd=.
2026-01-14 16:02:25 | INFO | P01 | github_actions=False
2026-01-14 16:02:25 | INFO | P01 | Tokenizer initialized with 24 tokens.
2026-01-14 16:02:25 | INFO | P01 | First 10 tokens: ['the', 'cat', 'sat', 'on', 'the', 'mat', 'the', 'dog', 'sat', 'on']
2026-01-14 16:02:25 | INFO | P01 | Total number of tokens: 24
2026-01-14 16:02:25 | INFO | P01 | Average token length: 2.83
2026-01-14 16:02:52 | INFO | P01 | === RUN START ===
2026-01-14 16:02:52 | INFO | P01 | project=Vocabulary Demo
2026-01-14 16:02:52 | INFO | P01 | repo_dir=toy-gpt-train
2026-01-14 16:02:52 | INFO | P01 | python=3.14.0
2026-01-14 16:02:52 | INFO | P01 | os=Windows 11
2026-01-14 16:02:52 | INFO | P01 | shell=powershell
2026-01-14 16:02:52 | INFO | P01 | cwd=.
2026-01-14 16:02:52 | INFO | P01 | github_actions=False
2026-01-14 16:03:13 | INFO | P01 | === RUN START ===
2026-01-14 16:03:13 | INFO | P01 | project=Vocabulary Demo
2026-01-14 16:03:13 | INFO | P01 | repo_dir=toy-gpt-train
2026-01-14 16:03:13 | INFO | P01 | python=3.14.0
2026-01-14 16:03:13 | INFO | P01 | os=Windows 11
2026-01-14 16:03:13 | INFO | P01 | shell=powershell
2026-01-14 16:03:13 | INFO | P01 | cwd=.
2026-01-14 16:03:13 | INFO | P01 | github_actions=False
2026-01-14 16:03:52 | INFO | P01 | === RUN START ===
2026-01-14 16:03:52 | INFO | P01 | project=Vocabulary Demo
2026-01-14 16:03:52 | INFO | P01 | repo_dir=toy-gpt-train
2026-01-14 16:03:52 | INFO | P01 | python=3.14.0
2026-01-14 16:03:52 | INFO | P01 | os=Windows 11
2026-01-14 16:03:52 | INFO | P01 | shell=powershell
2026-01-14 16:03:52 | INFO | P01 | cwd=.
2026-01-14 16:03:52 | INFO | P01 | github_actions=False
2026-01-14 16:03:53 | INFO | P01 | Tokenizer initialized with 24 tokens.
2026-01-14 16:03:53 | INFO | P01 | Vocabulary initialized with 8 unique tokens.
2026-01-14 16:03:53 | INFO | P01 | Vocabulary size: 8
2026-01-14 16:03:53 | INFO | P01 | Sample token: 'the' | ID: 7 | Frequency: 8
2026-01-14 16:04:19 | INFO | P01 | === RUN START ===
2026-01-14 16:04:19 | INFO | P01 | project=Simple Next-Token Model Demo
2026-01-14 16:04:19 | INFO | P01 | repo_dir=toy-gpt-train
2026-01-14 16:04:19 | INFO | P01 | python=3.14.0
2026-01-14 16:04:19 | INFO | P01 | os=Windows 11
2026-01-14 16:04:19 | INFO | P01 | shell=powershell
2026-01-14 16:04:19 | INFO | P01 | cwd=.
2026-01-14 16:04:19 | INFO | P01 | github_actions=False
2026-01-14 16:04:19 | INFO | P01 | Model initialized with vocabulary size 6.
2026-01-14 16:04:19 | INFO | P01 | Input token ID: 0
2026-01-14 16:04:19 | INFO | P01 | Output probabilities:
2026-01-14 16:04:19 | INFO | P01 |   Token ID 0 -> 0.1667
2026-01-14 16:04:19 | INFO | P01 |   Token ID 1 -> 0.1667
2026-01-14 16:04:19 | INFO | P01 |   Token ID 2 -> 0.1667
2026-01-14 16:04:19 | INFO | P01 |   Token ID 3 -> 0.1667
2026-01-14 16:04:19 | INFO | P01 |   Token ID 4 -> 0.1667
2026-01-14 16:04:19 | INFO | P01 |   Token ID 5 -> 0.1667
2026-01-14 16:36:10 | INFO | P01 | === RUN START ===
2026-01-14 16:36:10 | INFO | P01 | project=Simple Next-Token Model Demo
2026-01-14 16:36:10 | INFO | P01 | repo_dir=toy-gpt-train
2026-01-14 16:36:10 | INFO | P01 | python=3.14.0
2026-01-14 16:36:10 | INFO | P01 | os=Windows 11
2026-01-14 16:36:10 | INFO | P01 | shell=powershell
2026-01-14 16:36:10 | INFO | P01 | cwd=.
2026-01-14 16:36:10 | INFO | P01 | github_actions=False
2026-01-14 16:36:17 | INFO | P01 | === RUN START ===
2026-01-14 16:36:17 | INFO | P01 | project=Simple Next-Token Model Demo
2026-01-14 16:36:17 | INFO | P01 | repo_dir=toy-gpt-train
2026-01-14 16:36:17 | INFO | P01 | python=3.14.0
2026-01-14 16:36:17 | INFO | P01 | os=Windows 11
2026-01-14 16:36:17 | INFO | P01 | shell=powershell
2026-01-14 16:36:17 | INFO | P01 | cwd=.
2026-01-14 16:36:17 | INFO | P01 | github_actions=False
2026-01-14 16:36:45 | INFO | P01 | === RUN START ===
2026-01-14 16:36:45 | INFO | P01 | project=Simple Next-Token Model Demo
2026-01-14 16:36:45 | INFO | P01 | repo_dir=toy-gpt-train
2026-01-14 16:36:45 | INFO | P01 | python=3.14.0
2026-01-14 16:36:45 | INFO | P01 | os=Windows 11
2026-01-14 16:36:45 | INFO | P01 | shell=powershell
2026-01-14 16:36:45 | INFO | P01 | cwd=.
2026-01-14 16:36:45 | INFO | P01 | github_actions=False
2026-01-14 16:36:45 | INFO | P01 | Tokenizer initialized with 24 tokens.
2026-01-14 16:36:45 | INFO | P01 | Vocabulary initialized with 8 unique tokens.
2026-01-14 16:36:45 | INFO | P01 | Model initialized with vocabulary size 8.
2026-01-14 16:36:45 | INFO | P01 | Input token ID: 0
2026-01-14 16:36:45 | INFO | P01 | Output probabilities:
2026-01-14 16:36:45 | INFO | P01 |   Token ID 0 -> 0.1250
2026-01-14 16:36:45 | INFO | P01 |   Token ID 1 -> 0.1250
2026-01-14 16:36:45 | INFO | P01 |   Token ID 2 -> 0.1250
2026-01-14 16:36:45 | INFO | P01 |   Token ID 3 -> 0.1250
2026-01-14 16:36:45 | INFO | P01 |   Token ID 4 -> 0.1250
2026-01-14 16:36:45 | INFO | P01 |   Token ID 5 -> 0.1250
2026-01-14 16:36:45 | INFO | P01 |   Token ID 6 -> 0.1250
2026-01-14 16:36:45 | INFO | P01 |   Token ID 7 -> 0.1250
2026-01-14 16:38:36 | INFO | P01 | === RUN START ===
2026-01-14 16:38:36 | INFO | P01 | project=Simple Next-Token Model Demo
2026-01-14 16:38:36 | INFO | P01 | repo_dir=toy-gpt-train
2026-01-14 16:38:36 | INFO | P01 | python=3.14.0
2026-01-14 16:38:36 | INFO | P01 | os=Windows 11
2026-01-14 16:38:36 | INFO | P01 | shell=powershell
2026-01-14 16:38:36 | INFO | P01 | cwd=.
2026-01-14 16:38:36 | INFO | P01 | github_actions=False
2026-01-14 16:38:36 | INFO | P01 | Tokenizer initialized with 24 tokens.
2026-01-14 16:38:36 | INFO | P01 | Vocabulary initialized with 8 unique tokens.
2026-01-14 16:38:36 | INFO | P01 | Model initialized with vocabulary size 8.
2026-01-14 16:38:36 | INFO | P01 | Input token ID: 0
2026-01-14 16:38:36 | INFO | P01 | Output probabilities:
2026-01-14 16:38:36 | INFO | P01 |   Token ID 0 -> 0.1250
2026-01-14 16:38:36 | INFO | P01 |   Token ID 1 -> 0.1250
2026-01-14 16:38:36 | INFO | P01 |   Token ID 2 -> 0.1250
2026-01-14 16:38:36 | INFO | P01 |   Token ID 3 -> 0.1250
2026-01-14 16:38:36 | INFO | P01 |   Token ID 4 -> 0.1250
2026-01-14 16:38:36 | INFO | P01 |   Token ID 5 -> 0.1250
2026-01-14 16:38:36 | INFO | P01 |   Token ID 6 -> 0.1250
2026-01-14 16:38:36 | INFO | P01 |   Token ID 7 -> 0.1250
2026-01-14 16:38:49 | INFO | P01 | === RUN START ===
2026-01-14 16:38:49 | INFO | P01 | project=Training Demo: Next-Token Softmax Regression
2026-01-14 16:38:49 | INFO | P01 | repo_dir=toy-gpt-train
2026-01-14 16:38:49 | INFO | P01 | python=3.14.0
2026-01-14 16:38:49 | INFO | P01 | os=Windows 11
2026-01-14 16:38:49 | INFO | P01 | shell=powershell
2026-01-14 16:38:49 | INFO | P01 | cwd=.
2026-01-14 16:38:49 | INFO | P01 | github_actions=False
2026-01-14 16:38:49 | INFO | P01 | Tokenizer initialized with 24 tokens.
2026-01-14 16:38:49 | INFO | P01 | Vocabulary initialized with 8 unique tokens.
2026-01-14 16:38:49 | INFO | P01 | Model initialized with vocabulary size 8.
2026-01-14 16:38:49 | INFO | P01 | Tokenizer initialized with 24 tokens.
2026-01-14 16:38:49 | INFO | P01 | Vocabulary initialized with 8 unique tokens.
2026-01-14 16:38:49 | INFO | P01 | Created 23 training pairs.
2026-01-14 16:38:49 | INFO | P01 | Vocabulary size: 8
2026-01-14 16:38:49 | INFO | P01 | Model initialized with vocabulary size 8.
2026-01-14 16:38:49 | INFO | P01 | Epoch 1/50 | avg_loss=2.045639 | accuracy=0.304
2026-01-14 16:38:49 | INFO | P01 | Epoch 2/50 | avg_loss=1.900554 | accuracy=0.478
2026-01-14 16:38:49 | INFO | P01 | Epoch 3/50 | avg_loss=1.771327 | accuracy=0.478
2026-01-14 16:38:49 | INFO | P01 | Epoch 4/50 | avg_loss=1.657399 | accuracy=0.478
2026-01-14 16:38:49 | INFO | P01 | Epoch 5/50 | avg_loss=1.557705 | accuracy=0.478
2026-01-14 16:38:49 | INFO | P01 | Epoch 6/50 | avg_loss=1.470796 | accuracy=0.478
2026-01-14 16:38:49 | INFO | P01 | Epoch 7/50 | avg_loss=1.395057 | accuracy=0.478
2026-01-14 16:38:49 | INFO | P01 | Epoch 8/50 | avg_loss=1.328918 | accuracy=0.478
2026-01-14 16:38:49 | INFO | P01 | Epoch 9/50 | avg_loss=1.270963 | accuracy=0.478
2026-01-14 16:38:49 | INFO | P01 | Epoch 10/50 | avg_loss=1.219980 | accuracy=0.478
2026-01-14 16:38:49 | INFO | P01 | Epoch 11/50 | avg_loss=1.174945 | accuracy=0.478
2026-01-14 16:38:49 | INFO | P01 | Epoch 12/50 | avg_loss=1.135004 | accuracy=0.478
2026-01-14 16:38:49 | INFO | P01 | Epoch 13/50 | avg_loss=1.099445 | accuracy=0.478
2026-01-14 16:38:49 | INFO | P01 | Epoch 14/50 | avg_loss=1.067668 | accuracy=0.478
2026-01-14 16:38:49 | INFO | P01 | Epoch 15/50 | avg_loss=1.039168 | accuracy=0.478
2026-01-14 16:38:49 | INFO | P01 | Epoch 16/50 | avg_loss=1.013518 | accuracy=0.478
2026-01-14 16:38:49 | INFO | P01 | Epoch 17/50 | avg_loss=0.990356 | accuracy=0.478
2026-01-14 16:38:49 | INFO | P01 | Epoch 18/50 | avg_loss=0.969372 | accuracy=0.478
2026-01-14 16:38:49 | INFO | P01 | Epoch 19/50 | avg_loss=0.950303 | accuracy=0.478
2026-01-14 16:38:49 | INFO | P01 | Epoch 20/50 | avg_loss=0.932922 | accuracy=0.478
2026-01-14 16:38:49 | INFO | P01 | Epoch 21/50 | avg_loss=0.917034 | accuracy=0.478
2026-01-14 16:38:49 | INFO | P01 | Epoch 22/50 | avg_loss=0.902472 | accuracy=0.478
2026-01-14 16:38:49 | INFO | P01 | Epoch 23/50 | avg_loss=0.889090 | accuracy=0.478
2026-01-14 16:38:49 | INFO | P01 | Epoch 24/50 | avg_loss=0.876761 | accuracy=0.478
2026-01-14 16:38:49 | INFO | P01 | Epoch 25/50 | avg_loss=0.865377 | accuracy=0.478
2026-01-14 16:38:49 | INFO | P01 | Epoch 26/50 | avg_loss=0.854841 | accuracy=0.478
2026-01-14 16:38:49 | INFO | P01 | Epoch 27/50 | avg_loss=0.845068 | accuracy=0.478
2026-01-14 16:38:49 | INFO | P01 | Epoch 28/50 | avg_loss=0.835986 | accuracy=0.478
2026-01-14 16:38:49 | INFO | P01 | Epoch 29/50 | avg_loss=0.827529 | accuracy=0.478
2026-01-14 16:38:49 | INFO | P01 | Epoch 30/50 | avg_loss=0.819638 | accuracy=0.478
2026-01-14 16:38:49 | INFO | P01 | Epoch 31/50 | avg_loss=0.812264 | accuracy=0.478
2026-01-14 16:38:49 | INFO | P01 | Epoch 32/50 | avg_loss=0.805361 | accuracy=0.478
2026-01-14 16:38:49 | INFO | P01 | Epoch 33/50 | avg_loss=0.798887 | accuracy=0.478
2026-01-14 16:38:49 | INFO | P01 | Epoch 34/50 | avg_loss=0.792807 | accuracy=0.478
2026-01-14 16:38:49 | INFO | P01 | Epoch 35/50 | avg_loss=0.787089 | accuracy=0.478
2026-01-14 16:38:49 | INFO | P01 | Epoch 36/50 | avg_loss=0.781703 | accuracy=0.478
2026-01-14 16:38:49 | INFO | P01 | Epoch 37/50 | avg_loss=0.776622 | accuracy=0.478
2026-01-14 16:38:49 | INFO | P01 | Epoch 38/50 | avg_loss=0.771824 | accuracy=0.478
2026-01-14 16:38:49 | INFO | P01 | Epoch 39/50 | avg_loss=0.767286 | accuracy=0.478
2026-01-14 16:38:49 | INFO | P01 | Epoch 40/50 | avg_loss=0.762990 | accuracy=0.478
2026-01-14 16:38:49 | INFO | P01 | Epoch 41/50 | avg_loss=0.758917 | accuracy=0.478
2026-01-14 16:38:49 | INFO | P01 | Epoch 42/50 | avg_loss=0.755052 | accuracy=0.478
2026-01-14 16:38:49 | INFO | P01 | Epoch 43/50 | avg_loss=0.751380 | accuracy=0.478
2026-01-14 16:38:49 | INFO | P01 | Epoch 44/50 | avg_loss=0.747888 | accuracy=0.478
2026-01-14 16:38:49 | INFO | P01 | Epoch 45/50 | avg_loss=0.744564 | accuracy=0.478
2026-01-14 16:38:49 | INFO | P01 | Epoch 46/50 | avg_loss=0.741396 | accuracy=0.478
2026-01-14 16:38:49 | INFO | P01 | Epoch 47/50 | avg_loss=0.738374 | accuracy=0.478
2026-01-14 16:38:49 | INFO | P01 | Epoch 48/50 | avg_loss=0.735489 | accuracy=0.478
2026-01-14 16:38:49 | INFO | P01 | Epoch 49/50 | avg_loss=0.732732 | accuracy=0.478
2026-01-14 16:38:49 | INFO | P01 | Epoch 50/50 | avg_loss=0.730095 | accuracy=0.478
2026-01-14 16:38:49 | INFO | P01 | Wrote training log to C:\Users\edaci\Documents\datafun\toy-gpt-train\outputs\train_log.csv
2026-01-14 16:38:49 | INFO | P01 | After training, most likely next token after 'cat' is 'lay' (id=2).
2026-01-14 16:43:33 | INFO | P01 | === RUN START ===
2026-01-14 16:43:33 | INFO | P01 | project=Training Demo: Next-Token Softmax Regression
2026-01-14 16:43:33 | INFO | P01 | repo_dir=toy-gpt-train
2026-01-14 16:43:33 | INFO | P01 | python=3.14.0
2026-01-14 16:43:33 | INFO | P01 | os=Windows 11
2026-01-14 16:43:33 | INFO | P01 | shell=powershell
2026-01-14 16:43:33 | INFO | P01 | cwd=.
2026-01-14 16:43:33 | INFO | P01 | github_actions=False
2026-01-14 16:43:33 | INFO | P01 | Tokenizer initialized with 24 tokens.
2026-01-14 16:43:33 | INFO | P01 | Vocabulary initialized with 8 unique tokens.
2026-01-14 16:43:33 | INFO | P01 | Model initialized with vocabulary size 8.
2026-01-14 16:43:33 | INFO | P01 | Tokenizer initialized with 24 tokens.
2026-01-14 16:43:33 | INFO | P01 | Vocabulary initialized with 8 unique tokens.
2026-01-14 16:43:33 | INFO | P01 | Created 23 training pairs.
2026-01-14 16:43:33 | INFO | P01 | Vocabulary size: 8
2026-01-14 16:43:33 | INFO | P01 | Model initialized with vocabulary size 8.
2026-01-14 16:43:33 | INFO | P01 | Epoch 1/50 | avg_loss=2.045639 | accuracy=0.304
2026-01-14 16:43:33 | INFO | P01 | Epoch 2/50 | avg_loss=1.900554 | accuracy=0.478
2026-01-14 16:43:33 | INFO | P01 | Epoch 3/50 | avg_loss=1.771327 | accuracy=0.478
2026-01-14 16:43:33 | INFO | P01 | Epoch 4/50 | avg_loss=1.657399 | accuracy=0.478
2026-01-14 16:43:33 | INFO | P01 | Epoch 5/50 | avg_loss=1.557705 | accuracy=0.478
2026-01-14 16:43:33 | INFO | P01 | Epoch 6/50 | avg_loss=1.470796 | accuracy=0.478
2026-01-14 16:43:33 | INFO | P01 | Epoch 7/50 | avg_loss=1.395057 | accuracy=0.478
2026-01-14 16:43:33 | INFO | P01 | Epoch 8/50 | avg_loss=1.328918 | accuracy=0.478
2026-01-14 16:43:33 | INFO | P01 | Epoch 9/50 | avg_loss=1.270963 | accuracy=0.478
2026-01-14 16:43:33 | INFO | P01 | Epoch 10/50 | avg_loss=1.219980 | accuracy=0.478
2026-01-14 16:43:33 | INFO | P01 | Epoch 11/50 | avg_loss=1.174945 | accuracy=0.478
2026-01-14 16:43:33 | INFO | P01 | Epoch 12/50 | avg_loss=1.135004 | accuracy=0.478
2026-01-14 16:43:33 | INFO | P01 | Epoch 13/50 | avg_loss=1.099445 | accuracy=0.478
2026-01-14 16:43:33 | INFO | P01 | Epoch 14/50 | avg_loss=1.067668 | accuracy=0.478
2026-01-14 16:43:33 | INFO | P01 | Epoch 15/50 | avg_loss=1.039168 | accuracy=0.478
2026-01-14 16:43:33 | INFO | P01 | Epoch 16/50 | avg_loss=1.013518 | accuracy=0.478
2026-01-14 16:43:33 | INFO | P01 | Epoch 17/50 | avg_loss=0.990356 | accuracy=0.478
2026-01-14 16:43:33 | INFO | P01 | Epoch 18/50 | avg_loss=0.969372 | accuracy=0.478
2026-01-14 16:43:33 | INFO | P01 | Epoch 19/50 | avg_loss=0.950303 | accuracy=0.478
2026-01-14 16:43:33 | INFO | P01 | Epoch 20/50 | avg_loss=0.932922 | accuracy=0.478
2026-01-14 16:43:33 | INFO | P01 | Epoch 21/50 | avg_loss=0.917034 | accuracy=0.478
2026-01-14 16:43:33 | INFO | P01 | Epoch 22/50 | avg_loss=0.902472 | accuracy=0.478
2026-01-14 16:43:33 | INFO | P01 | Epoch 23/50 | avg_loss=0.889090 | accuracy=0.478
2026-01-14 16:43:33 | INFO | P01 | Epoch 24/50 | avg_loss=0.876761 | accuracy=0.478
2026-01-14 16:43:33 | INFO | P01 | Epoch 25/50 | avg_loss=0.865377 | accuracy=0.478
2026-01-14 16:43:33 | INFO | P01 | Epoch 26/50 | avg_loss=0.854841 | accuracy=0.478
2026-01-14 16:43:33 | INFO | P01 | Epoch 27/50 | avg_loss=0.845068 | accuracy=0.478
2026-01-14 16:43:33 | INFO | P01 | Epoch 28/50 | avg_loss=0.835986 | accuracy=0.478
2026-01-14 16:43:33 | INFO | P01 | Epoch 29/50 | avg_loss=0.827529 | accuracy=0.478
2026-01-14 16:43:33 | INFO | P01 | Epoch 30/50 | avg_loss=0.819638 | accuracy=0.478
2026-01-14 16:43:33 | INFO | P01 | Epoch 31/50 | avg_loss=0.812264 | accuracy=0.478
2026-01-14 16:43:33 | INFO | P01 | Epoch 32/50 | avg_loss=0.805361 | accuracy=0.478
2026-01-14 16:43:33 | INFO | P01 | Epoch 33/50 | avg_loss=0.798887 | accuracy=0.478
2026-01-14 16:43:33 | INFO | P01 | Epoch 34/50 | avg_loss=0.792807 | accuracy=0.478
2026-01-14 16:43:33 | INFO | P01 | Epoch 35/50 | avg_loss=0.787089 | accuracy=0.478
2026-01-14 16:43:33 | INFO | P01 | Epoch 36/50 | avg_loss=0.781703 | accuracy=0.478
2026-01-14 16:43:33 | INFO | P01 | Epoch 37/50 | avg_loss=0.776622 | accuracy=0.478
2026-01-14 16:43:33 | INFO | P01 | Epoch 38/50 | avg_loss=0.771824 | accuracy=0.478
2026-01-14 16:43:33 | INFO | P01 | Epoch 39/50 | avg_loss=0.767286 | accuracy=0.478
2026-01-14 16:43:33 | INFO | P01 | Epoch 40/50 | avg_loss=0.762990 | accuracy=0.478
2026-01-14 16:43:33 | INFO | P01 | Epoch 41/50 | avg_loss=0.758917 | accuracy=0.478
2026-01-14 16:43:33 | INFO | P01 | Epoch 42/50 | avg_loss=0.755052 | accuracy=0.478
2026-01-14 16:43:33 | INFO | P01 | Epoch 43/50 | avg_loss=0.751380 | accuracy=0.478
2026-01-14 16:43:33 | INFO | P01 | Epoch 44/50 | avg_loss=0.747888 | accuracy=0.478
2026-01-14 16:43:33 | INFO | P01 | Epoch 45/50 | avg_loss=0.744564 | accuracy=0.478
2026-01-14 16:43:33 | INFO | P01 | Epoch 46/50 | avg_loss=0.741396 | accuracy=0.478
2026-01-14 16:43:33 | INFO | P01 | Epoch 47/50 | avg_loss=0.738374 | accuracy=0.478
2026-01-14 16:43:33 | INFO | P01 | Epoch 48/50 | avg_loss=0.735489 | accuracy=0.478
2026-01-14 16:43:33 | INFO | P01 | Epoch 49/50 | avg_loss=0.732732 | accuracy=0.478
2026-01-14 16:43:33 | INFO | P01 | Epoch 50/50 | avg_loss=0.730095 | accuracy=0.478
2026-01-14 16:43:33 | INFO | P01 | Wrote training log to C:\Users\edaci\Documents\datafun\toy-gpt-train\outputs\train_log.csv
2026-01-14 16:43:33 | INFO | P01 | After training, most likely next token after 'cat' is 'lay' (id=2).
2026-01-14 16:43:33 | INFO | P01 | Top next-token predictions after 'cat':
2026-01-14 16:43:33 | INFO | P01 |   'lay' (id=2): 0.4183
2026-01-14 16:43:33 | INFO | P01 |   'sat' (id=6): 0.3984
2026-01-14 16:43:33 | INFO | P01 |   'cat' (id=0): 0.0306
2026-01-16 11:56:56 | INFO | TRAIN | === RUN START ===
2026-01-16 11:56:56 | INFO | TRAIN | project=Training Demo: Next-Token Softmax Regression (Context-2)
2026-01-16 11:56:56 | INFO | TRAIN | repo_dir=train-300-context-2
2026-01-16 11:56:56 | INFO | TRAIN | python=3.14.0
2026-01-16 11:56:56 | INFO | TRAIN | os=Windows 11
2026-01-16 11:56:56 | INFO | TRAIN | shell=powershell
2026-01-16 11:56:56 | INFO | TRAIN | cwd=.
2026-01-16 11:56:56 | INFO | TRAIN | github_actions=False
2026-01-16 11:56:56 | INFO | TOKEN | Tokenizer initialized with 24 tokens.
2026-01-16 11:56:56 | INFO | VOCAB | Vocabulary initialized with 8 unique tokens.
2026-01-16 11:56:56 | INFO | TRAIN | Created 22 training pairs.
2026-01-16 11:56:56 | INFO | MODEL | Model initialized with vocabulary size 8 (context-2).
2026-01-16 11:56:56 | INFO | TRAIN | Epoch 1/50 | avg_loss=2.063953 | accuracy=0.227
2026-01-16 11:56:56 | INFO | TRAIN | Epoch 2/50 | avg_loss=1.949229 | accuracy=0.636
2026-01-16 11:56:56 | INFO | TRAIN | Epoch 3/50 | avg_loss=1.841472 | accuracy=0.636
2026-01-16 11:56:56 | INFO | TRAIN | Epoch 4/50 | avg_loss=1.740749 | accuracy=0.636
2026-01-16 11:56:56 | INFO | TRAIN | Epoch 5/50 | avg_loss=1.647012 | accuracy=0.636
2026-01-16 11:56:56 | INFO | TRAIN | Epoch 6/50 | avg_loss=1.560108 | accuracy=0.636
2026-01-16 11:56:56 | INFO | TRAIN | Epoch 7/50 | avg_loss=1.479793 | accuracy=0.636
2026-01-16 11:56:56 | INFO | TRAIN | Epoch 8/50 | avg_loss=1.405753 | accuracy=0.636
2026-01-16 11:56:56 | INFO | TRAIN | Epoch 9/50 | avg_loss=1.337621 | accuracy=0.636
2026-01-16 11:56:56 | INFO | TRAIN | Epoch 10/50 | avg_loss=1.275000 | accuracy=0.636
2026-01-16 11:56:56 | INFO | TRAIN | Epoch 11/50 | avg_loss=1.217477 | accuracy=0.636
2026-01-16 11:56:56 | INFO | TRAIN | Epoch 12/50 | avg_loss=1.164640 | accuracy=0.636
2026-01-16 11:56:56 | INFO | TRAIN | Epoch 13/50 | avg_loss=1.116089 | accuracy=0.636
2026-01-16 11:56:56 | INFO | TRAIN | Epoch 14/50 | avg_loss=1.071445 | accuracy=0.636
2026-01-16 11:56:56 | INFO | TRAIN | Epoch 15/50 | avg_loss=1.030353 | accuracy=0.636
2026-01-16 11:56:56 | INFO | TRAIN | Epoch 16/50 | avg_loss=0.992489 | accuracy=0.636
2026-01-16 11:56:56 | INFO | TRAIN | Epoch 17/50 | avg_loss=0.957554 | accuracy=0.636
2026-01-16 11:56:56 | INFO | TRAIN | Epoch 18/50 | avg_loss=0.925279 | accuracy=0.636
2026-01-16 11:56:56 | INFO | TRAIN | Epoch 19/50 | avg_loss=0.895421 | accuracy=0.636
2026-01-16 11:56:56 | INFO | TRAIN | Epoch 20/50 | avg_loss=0.867761 | accuracy=0.636
2026-01-16 11:56:56 | INFO | TRAIN | Epoch 21/50 | avg_loss=0.842103 | accuracy=0.636
2026-01-16 11:56:56 | INFO | TRAIN | Epoch 22/50 | avg_loss=0.818269 | accuracy=0.636
2026-01-16 11:56:56 | INFO | TRAIN | Epoch 23/50 | avg_loss=0.796101 | accuracy=0.636
2026-01-16 11:56:56 | INFO | TRAIN | Epoch 24/50 | avg_loss=0.775454 | accuracy=0.636
2026-01-16 11:56:56 | INFO | TRAIN | Epoch 25/50 | avg_loss=0.756201 | accuracy=0.636
2026-01-16 11:56:56 | INFO | TRAIN | Epoch 26/50 | avg_loss=0.738224 | accuracy=0.636
2026-01-16 11:56:56 | INFO | TRAIN | Epoch 27/50 | avg_loss=0.721419 | accuracy=0.636
2026-01-16 11:56:56 | INFO | TRAIN | Epoch 28/50 | avg_loss=0.705690 | accuracy=0.636
2026-01-16 11:56:56 | INFO | TRAIN | Epoch 29/50 | avg_loss=0.690950 | accuracy=0.636
2026-01-16 11:56:56 | INFO | TRAIN | Epoch 30/50 | avg_loss=0.677122 | accuracy=0.636
2026-01-16 11:56:56 | INFO | TRAIN | Epoch 31/50 | avg_loss=0.664134 | accuracy=0.636
2026-01-16 11:56:56 | INFO | TRAIN | Epoch 32/50 | avg_loss=0.651922 | accuracy=0.636
2026-01-16 11:56:56 | INFO | TRAIN | Epoch 33/50 | avg_loss=0.640427 | accuracy=0.636
2026-01-16 11:56:56 | INFO | TRAIN | Epoch 34/50 | avg_loss=0.629594 | accuracy=0.636
2026-01-16 11:56:56 | INFO | TRAIN | Epoch 35/50 | avg_loss=0.619376 | accuracy=0.636
2026-01-16 11:56:56 | INFO | TRAIN | Epoch 36/50 | avg_loss=0.609727 | accuracy=0.636
2026-01-16 11:56:56 | INFO | TRAIN | Epoch 37/50 | avg_loss=0.600605 | accuracy=0.636
2026-01-16 11:56:56 | INFO | TRAIN | Epoch 38/50 | avg_loss=0.591974 | accuracy=0.636
2026-01-16 11:56:56 | INFO | TRAIN | Epoch 39/50 | avg_loss=0.583800 | accuracy=0.636
2026-01-16 11:56:56 | INFO | TRAIN | Epoch 40/50 | avg_loss=0.576050 | accuracy=0.636
2026-01-16 11:56:56 | INFO | TRAIN | Epoch 41/50 | avg_loss=0.568696 | accuracy=0.636
2026-01-16 11:56:56 | INFO | TRAIN | Epoch 42/50 | avg_loss=0.561711 | accuracy=0.636
2026-01-16 11:56:56 | INFO | TRAIN | Epoch 43/50 | avg_loss=0.555070 | accuracy=0.636
2026-01-16 11:56:56 | INFO | TRAIN | Epoch 44/50 | avg_loss=0.548752 | accuracy=0.636
2026-01-16 11:56:56 | INFO | TRAIN | Epoch 45/50 | avg_loss=0.542735 | accuracy=0.636
2026-01-16 11:56:56 | INFO | TRAIN | Epoch 46/50 | avg_loss=0.536999 | accuracy=0.636
2026-01-16 11:56:56 | INFO | TRAIN | Epoch 47/50 | avg_loss=0.531529 | accuracy=0.636
2026-01-16 11:56:56 | INFO | TRAIN | Epoch 48/50 | avg_loss=0.526306 | accuracy=0.636
2026-01-16 11:56:56 | INFO | TRAIN | Epoch 49/50 | avg_loss=0.521316 | accuracy=0.636
2026-01-16 11:56:56 | INFO | TRAIN | Epoch 50/50 | avg_loss=0.516545 | accuracy=0.636
2026-01-16 11:56:56 | INFO | IO | Wrote training log to C:\Users\edaci\Documents\datafun\toy-gpt\train-300-context-2\outputs\train_log.csv
2026-01-16 11:56:56 | INFO | IO | Wrote vocabulary to C:\Users\edaci\Documents\datafun\toy-gpt\train-300-context-2\artifacts\01_vocabulary.csv
2026-01-16 11:56:56 | INFO | IO | Wrote model weights to C:\Users\edaci\Documents\datafun\toy-gpt\train-300-context-2\artifacts\02_model_weights.csv
2026-01-16 11:56:56 | INFO | IO | Wrote meta to C:\Users\edaci\Documents\datafun\toy-gpt\train-300-context-2\artifacts\00_meta.json
2026-01-16 11:56:56 | INFO | TRAIN | After training, most likely next token after 'the'|'cat' is 'lay' (ID: 2).
2026-01-16 11:57:03 | INFO | INFER | === RUN START ===
2026-01-16 11:57:03 | INFO | INFER | project=Inference Demo: Load Artifacts and Generate Text (Context-2)
2026-01-16 11:57:03 | INFO | INFER | repo_dir=train-300-context-2
2026-01-16 11:57:03 | INFO | INFER | python=3.14.0
2026-01-16 11:57:03 | INFO | INFER | os=Windows 11
2026-01-16 11:57:03 | INFO | INFER | shell=powershell
2026-01-16 11:57:03 | INFO | INFER | cwd=.
2026-01-16 11:57:03 | INFO | INFER | github_actions=False
2026-01-16 11:57:03 | INFO | MODEL | Model initialized with vocabulary size 8 (context-2).
2026-01-16 11:57:03 | INFO | INFER | Loaded repo_name=train-300-context-2 model_kind=context2
2026-01-16 11:57:03 | INFO | INFER | Vocab size: 8
2026-01-16 11:57:03 | INFO | INFER | Start token: cat
2026-01-16 11:57:03 | INFO | INFER | Context-2 bootstrap: (cat, cat)
2026-01-16 11:57:03 | INFO | INFER | Top next-token predictions after cat|cat:
2026-01-16 11:57:03 | INFO | INFER |   cat (ID 0): 0.1250
2026-01-16 11:57:03 | INFO | INFER |   dog (ID 1): 0.1250
2026-01-16 11:57:03 | INFO | INFER |   lay (ID 2): 0.1250
2026-01-16 11:57:03 | INFO | INFER | Generated sequence:
2026-01-16 11:57:03 | INFO | INFER |   cat cat cat cat cat cat cat cat cat cat cat
2026-01-16 12:22:53 | INFO | TRAIN | === RUN START ===
2026-01-16 12:22:53 | INFO | TRAIN | project=Training Demo: Next-Token Softmax Regression (Context-2)
2026-01-16 12:22:53 | INFO | TRAIN | repo_dir=train-300-context-2
2026-01-16 12:22:53 | INFO | TRAIN | python=3.14.0
2026-01-16 12:22:53 | INFO | TRAIN | os=Windows 11
2026-01-16 12:22:53 | INFO | TRAIN | shell=powershell
2026-01-16 12:22:53 | INFO | TRAIN | cwd=.
2026-01-16 12:22:53 | INFO | TRAIN | github_actions=False
2026-01-16 12:22:53 | INFO | TOKEN | Tokenizer initialized with 24 tokens.
2026-01-16 12:22:53 | INFO | VOCAB | Vocabulary initialized with 8 unique tokens.
2026-01-16 12:22:53 | INFO | TRAIN | Created 22 training pairs.
2026-01-16 12:22:53 | INFO | MODEL | Model initialized with vocabulary size 8 (context-2).
2026-01-16 12:22:53 | INFO | TRAIN | Epoch 1/50 | avg_loss=2.063953 | accuracy=0.227
2026-01-16 12:22:53 | INFO | TRAIN | Epoch 2/50 | avg_loss=1.949229 | accuracy=0.636
2026-01-16 12:22:53 | INFO | TRAIN | Epoch 3/50 | avg_loss=1.841472 | accuracy=0.636
2026-01-16 12:22:53 | INFO | TRAIN | Epoch 4/50 | avg_loss=1.740749 | accuracy=0.636
2026-01-16 12:22:53 | INFO | TRAIN | Epoch 5/50 | avg_loss=1.647012 | accuracy=0.636
2026-01-16 12:22:53 | INFO | TRAIN | Epoch 6/50 | avg_loss=1.560108 | accuracy=0.636
2026-01-16 12:22:53 | INFO | TRAIN | Epoch 7/50 | avg_loss=1.479793 | accuracy=0.636
2026-01-16 12:22:53 | INFO | TRAIN | Epoch 8/50 | avg_loss=1.405753 | accuracy=0.636
2026-01-16 12:22:53 | INFO | TRAIN | Epoch 9/50 | avg_loss=1.337621 | accuracy=0.636
2026-01-16 12:22:53 | INFO | TRAIN | Epoch 10/50 | avg_loss=1.275000 | accuracy=0.636
2026-01-16 12:22:53 | INFO | TRAIN | Epoch 11/50 | avg_loss=1.217477 | accuracy=0.636
2026-01-16 12:22:53 | INFO | TRAIN | Epoch 12/50 | avg_loss=1.164640 | accuracy=0.636
2026-01-16 12:22:53 | INFO | TRAIN | Epoch 13/50 | avg_loss=1.116089 | accuracy=0.636
2026-01-16 12:22:53 | INFO | TRAIN | Epoch 14/50 | avg_loss=1.071445 | accuracy=0.636
2026-01-16 12:22:53 | INFO | TRAIN | Epoch 15/50 | avg_loss=1.030353 | accuracy=0.636
2026-01-16 12:22:53 | INFO | TRAIN | Epoch 16/50 | avg_loss=0.992489 | accuracy=0.636
2026-01-16 12:22:53 | INFO | TRAIN | Epoch 17/50 | avg_loss=0.957554 | accuracy=0.636
2026-01-16 12:22:53 | INFO | TRAIN | Epoch 18/50 | avg_loss=0.925279 | accuracy=0.636
2026-01-16 12:22:53 | INFO | TRAIN | Epoch 19/50 | avg_loss=0.895421 | accuracy=0.636
2026-01-16 12:22:53 | INFO | TRAIN | Epoch 20/50 | avg_loss=0.867761 | accuracy=0.636
2026-01-16 12:22:53 | INFO | TRAIN | Epoch 21/50 | avg_loss=0.842103 | accuracy=0.636
2026-01-16 12:22:53 | INFO | TRAIN | Epoch 22/50 | avg_loss=0.818269 | accuracy=0.636
2026-01-16 12:22:53 | INFO | TRAIN | Epoch 23/50 | avg_loss=0.796101 | accuracy=0.636
2026-01-16 12:22:53 | INFO | TRAIN | Epoch 24/50 | avg_loss=0.775454 | accuracy=0.636
2026-01-16 12:22:53 | INFO | TRAIN | Epoch 25/50 | avg_loss=0.756201 | accuracy=0.636
2026-01-16 12:22:53 | INFO | TRAIN | Epoch 26/50 | avg_loss=0.738224 | accuracy=0.636
2026-01-16 12:22:53 | INFO | TRAIN | Epoch 27/50 | avg_loss=0.721419 | accuracy=0.636
2026-01-16 12:22:53 | INFO | TRAIN | Epoch 28/50 | avg_loss=0.705690 | accuracy=0.636
2026-01-16 12:22:53 | INFO | TRAIN | Epoch 29/50 | avg_loss=0.690950 | accuracy=0.636
2026-01-16 12:22:53 | INFO | TRAIN | Epoch 30/50 | avg_loss=0.677122 | accuracy=0.636
2026-01-16 12:22:53 | INFO | TRAIN | Epoch 31/50 | avg_loss=0.664134 | accuracy=0.636
2026-01-16 12:22:53 | INFO | TRAIN | Epoch 32/50 | avg_loss=0.651922 | accuracy=0.636
2026-01-16 12:22:53 | INFO | TRAIN | Epoch 33/50 | avg_loss=0.640427 | accuracy=0.636
2026-01-16 12:22:53 | INFO | TRAIN | Epoch 34/50 | avg_loss=0.629594 | accuracy=0.636
2026-01-16 12:22:53 | INFO | TRAIN | Epoch 35/50 | avg_loss=0.619376 | accuracy=0.636
2026-01-16 12:22:53 | INFO | TRAIN | Epoch 36/50 | avg_loss=0.609727 | accuracy=0.636
2026-01-16 12:22:53 | INFO | TRAIN | Epoch 37/50 | avg_loss=0.600605 | accuracy=0.636
2026-01-16 12:22:53 | INFO | TRAIN | Epoch 38/50 | avg_loss=0.591974 | accuracy=0.636
2026-01-16 12:22:53 | INFO | TRAIN | Epoch 39/50 | avg_loss=0.583800 | accuracy=0.636
2026-01-16 12:22:53 | INFO | TRAIN | Epoch 40/50 | avg_loss=0.576050 | accuracy=0.636
2026-01-16 12:22:53 | INFO | TRAIN | Epoch 41/50 | avg_loss=0.568696 | accuracy=0.636
2026-01-16 12:22:53 | INFO | TRAIN | Epoch 42/50 | avg_loss=0.561711 | accuracy=0.636
2026-01-16 12:22:53 | INFO | TRAIN | Epoch 43/50 | avg_loss=0.555070 | accuracy=0.636
2026-01-16 12:22:53 | INFO | TRAIN | Epoch 44/50 | avg_loss=0.548752 | accuracy=0.636
2026-01-16 12:22:53 | INFO | TRAIN | Epoch 45/50 | avg_loss=0.542735 | accuracy=0.636
2026-01-16 12:22:53 | INFO | TRAIN | Epoch 46/50 | avg_loss=0.536999 | accuracy=0.636
2026-01-16 12:22:53 | INFO | TRAIN | Epoch 47/50 | avg_loss=0.531529 | accuracy=0.636
2026-01-16 12:22:53 | INFO | TRAIN | Epoch 48/50 | avg_loss=0.526306 | accuracy=0.636
2026-01-16 12:22:53 | INFO | TRAIN | Epoch 49/50 | avg_loss=0.521316 | accuracy=0.636
2026-01-16 12:22:53 | INFO | TRAIN | Epoch 50/50 | avg_loss=0.516545 | accuracy=0.636
2026-01-16 12:22:53 | INFO | IO | Wrote training log to C:\Users\edaci\Documents\datafun\toy-gpt\train-300-context-2\outputs\train_log.csv
2026-01-16 12:22:53 | INFO | IO | Wrote vocabulary to C:\Users\edaci\Documents\datafun\toy-gpt\train-300-context-2\artifacts\01_vocabulary.csv
2026-01-16 12:22:53 | INFO | IO | Wrote model weights to C:\Users\edaci\Documents\datafun\toy-gpt\train-300-context-2\artifacts\02_model_weights.csv
2026-01-16 12:22:53 | INFO | IO | Wrote meta to C:\Users\edaci\Documents\datafun\toy-gpt\train-300-context-2\artifacts\00_meta.json
2026-01-16 12:22:53 | INFO | TRAIN | After training, most likely next token after 'the'|'cat' is 'lay' (ID: 2).
2026-01-16 12:22:56 | INFO | INFER | === RUN START ===
2026-01-16 12:22:56 | INFO | INFER | project=Inference Demo: Load Artifacts and Generate Text (Context-2)
2026-01-16 12:22:56 | INFO | INFER | repo_dir=train-300-context-2
2026-01-16 12:22:56 | INFO | INFER | python=3.14.0
2026-01-16 12:22:56 | INFO | INFER | os=Windows 11
2026-01-16 12:22:56 | INFO | INFER | shell=powershell
2026-01-16 12:22:56 | INFO | INFER | cwd=.
2026-01-16 12:22:56 | INFO | INFER | github_actions=False
2026-01-16 12:22:56 | INFO | MODEL | Model initialized with vocabulary size 8 (context-2).
2026-01-16 12:22:56 | INFO | INFER | Loaded repo_name=train-300-context-2 model_kind=context2
2026-01-16 12:22:56 | INFO | INFER | Vocab size: 8
2026-01-16 12:22:56 | INFO | INFER | Start token: cat
2026-01-16 12:22:56 | INFO | INFER | Context-2 bootstrap: (cat, cat)
2026-01-16 12:22:56 | INFO | INFER | Top next-token predictions after cat|cat:
2026-01-16 12:22:56 | INFO | INFER |   cat (ID 0): 0.1250
2026-01-16 12:22:56 | INFO | INFER |   dog (ID 1): 0.1250
2026-01-16 12:22:56 | INFO | INFER |   lay (ID 2): 0.1250
2026-01-16 12:22:56 | INFO | INFER | Generated sequence:
2026-01-16 12:22:56 | INFO | INFER |   cat cat cat cat cat cat cat cat cat cat cat
2026-01-16 13:16:14 | INFO | TOKEN | === RUN START ===
2026-01-16 13:16:14 | INFO | TOKEN | project=Simple Tokenizer Demo
2026-01-16 13:16:14 | INFO | TOKEN | repo_dir=train-300-context-2
2026-01-16 13:16:14 | INFO | TOKEN | python=3.14.0
2026-01-16 13:16:14 | INFO | TOKEN | os=Windows 11
2026-01-16 13:16:14 | INFO | TOKEN | shell=powershell
2026-01-16 13:16:14 | INFO | TOKEN | cwd=.
2026-01-16 13:16:14 | INFO | TOKEN | github_actions=False
2026-01-16 13:16:14 | INFO | TOKEN | Tokenizer initialized with 24 tokens.
2026-01-16 13:16:14 | INFO | TOKEN | First 10 tokens: ['the', 'cat', 'sat', 'on', 'the', 'mat', 'the', 'dog', 'sat', 'on']
2026-01-16 13:16:14 | INFO | TOKEN | Total number of tokens: 24
2026-01-16 13:16:14 | INFO | TOKEN | Average token length: 2.83
2026-01-16 13:16:14 | INFO | VOCAB | === RUN START ===
2026-01-16 13:16:14 | INFO | VOCAB | project=Vocabulary Demo
2026-01-16 13:16:14 | INFO | VOCAB | repo_dir=train-300-context-2
2026-01-16 13:16:14 | INFO | VOCAB | python=3.14.0
2026-01-16 13:16:14 | INFO | VOCAB | os=Windows 11
2026-01-16 13:16:14 | INFO | VOCAB | shell=powershell
2026-01-16 13:16:14 | INFO | VOCAB | cwd=.
2026-01-16 13:16:14 | INFO | VOCAB | github_actions=False
2026-01-16 13:16:14 | INFO | TOKEN | Tokenizer initialized with 24 tokens.
2026-01-16 13:16:14 | INFO | VOCAB | Vocabulary initialized with 8 unique tokens.
2026-01-16 13:16:14 | INFO | VOCAB | Vocabulary size: 8
2026-01-16 13:16:14 | INFO | VOCAB | Sample token: 'the' | ID: 7 | Frequency: 8
2026-01-16 13:16:14 | INFO | MODEL | === RUN START ===
2026-01-16 13:16:14 | INFO | MODEL | project=Simple Next-Token Model Demo (Context-2)
2026-01-16 13:16:14 | INFO | MODEL | repo_dir=train-300-context-2
2026-01-16 13:16:14 | INFO | MODEL | python=3.14.0
2026-01-16 13:16:14 | INFO | MODEL | os=Windows 11
2026-01-16 13:16:14 | INFO | MODEL | shell=powershell
2026-01-16 13:16:14 | INFO | MODEL | cwd=.
2026-01-16 13:16:14 | INFO | MODEL | github_actions=False
2026-01-16 13:16:14 | INFO | TOKEN | Tokenizer initialized with 24 tokens.
2026-01-16 13:16:14 | INFO | VOCAB | Vocabulary initialized with 8 unique tokens.
2026-01-16 13:16:14 | INFO | MODEL | Model initialized with vocabulary size 8 (context-2).
2026-01-16 13:16:14 | INFO | MODEL | Input tokens: 'cat' (ID 0), 'sat' (ID 6)
2026-01-16 13:16:14 | INFO | MODEL | Output probabilities for next token:
2026-01-16 13:16:14 | INFO | MODEL |   'cat' (ID 0) -> 0.1250
2026-01-16 13:16:14 | INFO | MODEL |   'dog' (ID 1) -> 0.1250
2026-01-16 13:16:14 | INFO | MODEL |   'lay' (ID 2) -> 0.1250
2026-01-16 13:16:14 | INFO | MODEL |   'mat' (ID 3) -> 0.1250
2026-01-16 13:16:14 | INFO | MODEL |   'on' (ID 4) -> 0.1250
2026-01-16 13:16:14 | INFO | MODEL |   'rug' (ID 5) -> 0.1250
2026-01-16 13:16:14 | INFO | MODEL |   'sat' (ID 6) -> 0.1250
2026-01-16 13:16:14 | INFO | MODEL |   'the' (ID 7) -> 0.1250
2026-01-16 13:16:14 | INFO | TRAIN | === RUN START ===
2026-01-16 13:16:14 | INFO | TRAIN | project=Training Demo: Next-Token Softmax Regression (Context-2)
2026-01-16 13:16:14 | INFO | TRAIN | repo_dir=train-300-context-2
2026-01-16 13:16:14 | INFO | TRAIN | python=3.14.0
2026-01-16 13:16:14 | INFO | TRAIN | os=Windows 11
2026-01-16 13:16:14 | INFO | TRAIN | shell=powershell
2026-01-16 13:16:14 | INFO | TRAIN | cwd=.
2026-01-16 13:16:14 | INFO | TRAIN | github_actions=False
2026-01-16 13:16:14 | INFO | TOKEN | Tokenizer initialized with 24 tokens.
2026-01-16 13:16:14 | INFO | VOCAB | Vocabulary initialized with 8 unique tokens.
2026-01-16 13:16:14 | INFO | TRAIN | Created 22 training pairs.
2026-01-16 13:16:14 | INFO | MODEL | Model initialized with vocabulary size 8 (context-2).
2026-01-16 13:16:14 | INFO | TRAIN | Epoch 1/50 | avg_loss=2.063953 | accuracy=0.227
2026-01-16 13:16:14 | INFO | TRAIN | Epoch 2/50 | avg_loss=1.949229 | accuracy=0.636
2026-01-16 13:16:14 | INFO | TRAIN | Epoch 3/50 | avg_loss=1.841472 | accuracy=0.636
2026-01-16 13:16:14 | INFO | TRAIN | Epoch 4/50 | avg_loss=1.740749 | accuracy=0.636
2026-01-16 13:16:14 | INFO | TRAIN | Epoch 5/50 | avg_loss=1.647012 | accuracy=0.636
2026-01-16 13:16:14 | INFO | TRAIN | Epoch 6/50 | avg_loss=1.560108 | accuracy=0.636
2026-01-16 13:16:14 | INFO | TRAIN | Epoch 7/50 | avg_loss=1.479793 | accuracy=0.636
2026-01-16 13:16:14 | INFO | TRAIN | Epoch 8/50 | avg_loss=1.405753 | accuracy=0.636
2026-01-16 13:16:14 | INFO | TRAIN | Epoch 9/50 | avg_loss=1.337621 | accuracy=0.636
2026-01-16 13:16:14 | INFO | TRAIN | Epoch 10/50 | avg_loss=1.275000 | accuracy=0.636
2026-01-16 13:16:14 | INFO | TRAIN | Epoch 11/50 | avg_loss=1.217477 | accuracy=0.636
2026-01-16 13:16:14 | INFO | TRAIN | Epoch 12/50 | avg_loss=1.164640 | accuracy=0.636
2026-01-16 13:16:14 | INFO | TRAIN | Epoch 13/50 | avg_loss=1.116089 | accuracy=0.636
2026-01-16 13:16:14 | INFO | TRAIN | Epoch 14/50 | avg_loss=1.071445 | accuracy=0.636
2026-01-16 13:16:14 | INFO | TRAIN | Epoch 15/50 | avg_loss=1.030353 | accuracy=0.636
2026-01-16 13:16:14 | INFO | TRAIN | Epoch 16/50 | avg_loss=0.992489 | accuracy=0.636
2026-01-16 13:16:14 | INFO | TRAIN | Epoch 17/50 | avg_loss=0.957554 | accuracy=0.636
2026-01-16 13:16:14 | INFO | TRAIN | Epoch 18/50 | avg_loss=0.925279 | accuracy=0.636
2026-01-16 13:16:14 | INFO | TRAIN | Epoch 19/50 | avg_loss=0.895421 | accuracy=0.636
2026-01-16 13:16:14 | INFO | TRAIN | Epoch 20/50 | avg_loss=0.867761 | accuracy=0.636
2026-01-16 13:16:14 | INFO | TRAIN | Epoch 21/50 | avg_loss=0.842103 | accuracy=0.636
2026-01-16 13:16:14 | INFO | TRAIN | Epoch 22/50 | avg_loss=0.818269 | accuracy=0.636
2026-01-16 13:16:14 | INFO | TRAIN | Epoch 23/50 | avg_loss=0.796101 | accuracy=0.636
2026-01-16 13:16:14 | INFO | TRAIN | Epoch 24/50 | avg_loss=0.775454 | accuracy=0.636
2026-01-16 13:16:14 | INFO | TRAIN | Epoch 25/50 | avg_loss=0.756201 | accuracy=0.636
2026-01-16 13:16:14 | INFO | TRAIN | Epoch 26/50 | avg_loss=0.738224 | accuracy=0.636
2026-01-16 13:16:14 | INFO | TRAIN | Epoch 27/50 | avg_loss=0.721419 | accuracy=0.636
2026-01-16 13:16:14 | INFO | TRAIN | Epoch 28/50 | avg_loss=0.705690 | accuracy=0.636
2026-01-16 13:16:14 | INFO | TRAIN | Epoch 29/50 | avg_loss=0.690950 | accuracy=0.636
2026-01-16 13:16:14 | INFO | TRAIN | Epoch 30/50 | avg_loss=0.677122 | accuracy=0.636
2026-01-16 13:16:14 | INFO | TRAIN | Epoch 31/50 | avg_loss=0.664134 | accuracy=0.636
2026-01-16 13:16:14 | INFO | TRAIN | Epoch 32/50 | avg_loss=0.651922 | accuracy=0.636
2026-01-16 13:16:14 | INFO | TRAIN | Epoch 33/50 | avg_loss=0.640427 | accuracy=0.636
2026-01-16 13:16:14 | INFO | TRAIN | Epoch 34/50 | avg_loss=0.629594 | accuracy=0.636
2026-01-16 13:16:14 | INFO | TRAIN | Epoch 35/50 | avg_loss=0.619376 | accuracy=0.636
2026-01-16 13:16:14 | INFO | TRAIN | Epoch 36/50 | avg_loss=0.609727 | accuracy=0.636
2026-01-16 13:16:14 | INFO | TRAIN | Epoch 37/50 | avg_loss=0.600605 | accuracy=0.636
2026-01-16 13:16:14 | INFO | TRAIN | Epoch 38/50 | avg_loss=0.591974 | accuracy=0.636
2026-01-16 13:16:14 | INFO | TRAIN | Epoch 39/50 | avg_loss=0.583800 | accuracy=0.636
2026-01-16 13:16:14 | INFO | TRAIN | Epoch 40/50 | avg_loss=0.576050 | accuracy=0.636
2026-01-16 13:16:14 | INFO | TRAIN | Epoch 41/50 | avg_loss=0.568696 | accuracy=0.636
2026-01-16 13:16:14 | INFO | TRAIN | Epoch 42/50 | avg_loss=0.561711 | accuracy=0.636
2026-01-16 13:16:14 | INFO | TRAIN | Epoch 43/50 | avg_loss=0.555070 | accuracy=0.636
2026-01-16 13:16:14 | INFO | TRAIN | Epoch 44/50 | avg_loss=0.548752 | accuracy=0.636
2026-01-16 13:16:14 | INFO | TRAIN | Epoch 45/50 | avg_loss=0.542735 | accuracy=0.636
2026-01-16 13:16:14 | INFO | TRAIN | Epoch 46/50 | avg_loss=0.536999 | accuracy=0.636
2026-01-16 13:16:14 | INFO | TRAIN | Epoch 47/50 | avg_loss=0.531529 | accuracy=0.636
2026-01-16 13:16:14 | INFO | TRAIN | Epoch 48/50 | avg_loss=0.526306 | accuracy=0.636
2026-01-16 13:16:14 | INFO | TRAIN | Epoch 49/50 | avg_loss=0.521316 | accuracy=0.636
2026-01-16 13:16:14 | INFO | TRAIN | Epoch 50/50 | avg_loss=0.516545 | accuracy=0.636
2026-01-16 13:16:14 | INFO | IO | Wrote training log to C:\Users\edaci\Documents\datafun\toy-gpt\train-300-context-2\outputs\train_log.csv
2026-01-16 13:16:14 | INFO | IO | Wrote vocabulary to C:\Users\edaci\Documents\datafun\toy-gpt\train-300-context-2\artifacts\01_vocabulary.csv
2026-01-16 13:16:14 | INFO | IO | Wrote model weights to C:\Users\edaci\Documents\datafun\toy-gpt\train-300-context-2\artifacts\02_model_weights.csv
2026-01-16 13:16:14 | INFO | IO | Wrote meta to C:\Users\edaci\Documents\datafun\toy-gpt\train-300-context-2\artifacts\00_meta.json
2026-01-16 13:16:14 | INFO | TRAIN | After training, most likely next token after 'the'|'cat' is 'lay' (ID: 2).
2026-01-16 13:16:14 | INFO | INFER | === RUN START ===
2026-01-16 13:16:14 | INFO | INFER | project=Inference Demo: Load Artifacts and Generate Text (Context-2)
2026-01-16 13:16:14 | INFO | INFER | repo_dir=train-300-context-2
2026-01-16 13:16:14 | INFO | INFER | python=3.14.0
2026-01-16 13:16:14 | INFO | INFER | os=Windows 11
2026-01-16 13:16:14 | INFO | INFER | shell=powershell
2026-01-16 13:16:14 | INFO | INFER | cwd=.
2026-01-16 13:16:14 | INFO | INFER | github_actions=False
2026-01-16 13:16:14 | INFO | MODEL | Model initialized with vocabulary size 8 (context-2).
2026-01-16 13:16:14 | INFO | INFER | Loaded repo_name=train-300-context-2 model_kind=context2
2026-01-16 13:16:14 | INFO | INFER | Vocab size: 8
2026-01-16 13:16:14 | INFO | INFER | Start token: cat
2026-01-16 13:16:14 | INFO | INFER | Context-2 bootstrap: (cat, cat)
2026-01-16 13:16:14 | INFO | INFER | Top next-token predictions after cat|cat:
2026-01-16 13:16:14 | INFO | INFER |   cat (ID 0): 0.1250
2026-01-16 13:16:14 | INFO | INFER |   dog (ID 1): 0.1250
2026-01-16 13:16:14 | INFO | INFER |   lay (ID 2): 0.1250
2026-01-16 13:16:14 | INFO | INFER | Generated sequence:
2026-01-16 13:16:14 | INFO | INFER |   cat cat cat cat cat cat cat cat cat cat cat
2026-01-16 18:11:05 | INFO | TRAIN | === RUN START ===
2026-01-16 18:11:05 | INFO | TRAIN | project=Training Demo: Next-Token Softmax Regression (Context-2)
2026-01-16 18:11:05 | INFO | TRAIN | repo_dir=train-300-context-2
2026-01-16 18:11:05 | INFO | TRAIN | python=3.14.0
2026-01-16 18:11:05 | INFO | TRAIN | os=Windows 11
2026-01-16 18:11:05 | INFO | TRAIN | shell=powershell
2026-01-16 18:11:05 | INFO | TRAIN | cwd=.
2026-01-16 18:11:05 | INFO | TRAIN | github_actions=False
2026-01-16 18:11:05 | INFO | TOKEN | Tokenizer initialized with 24 tokens.
2026-01-16 18:11:05 | INFO | VOCAB | Vocabulary initialized with 8 unique tokens.
2026-01-16 18:11:05 | INFO | TRAIN | Created 22 training pairs.
2026-01-16 18:11:05 | INFO | MODEL | Model initialized with vocabulary size 8 (context-2).
2026-01-16 18:11:05 | INFO | TRAIN | Epoch 1/50 | avg_loss=2.063953 | accuracy=0.227
2026-01-16 18:11:05 | INFO | TRAIN | Epoch 2/50 | avg_loss=1.949229 | accuracy=0.636
2026-01-16 18:11:05 | INFO | TRAIN | Epoch 3/50 | avg_loss=1.841472 | accuracy=0.636
2026-01-16 18:11:05 | INFO | TRAIN | Epoch 4/50 | avg_loss=1.740749 | accuracy=0.636
2026-01-16 18:11:05 | INFO | TRAIN | Epoch 5/50 | avg_loss=1.647012 | accuracy=0.636
2026-01-16 18:11:05 | INFO | TRAIN | Epoch 6/50 | avg_loss=1.560108 | accuracy=0.636
2026-01-16 18:11:05 | INFO | TRAIN | Epoch 7/50 | avg_loss=1.479793 | accuracy=0.636
2026-01-16 18:11:05 | INFO | TRAIN | Epoch 8/50 | avg_loss=1.405753 | accuracy=0.636
2026-01-16 18:11:05 | INFO | TRAIN | Epoch 9/50 | avg_loss=1.337621 | accuracy=0.636
2026-01-16 18:11:05 | INFO | TRAIN | Epoch 10/50 | avg_loss=1.275000 | accuracy=0.636
2026-01-16 18:11:05 | INFO | TRAIN | Epoch 11/50 | avg_loss=1.217477 | accuracy=0.636
2026-01-16 18:11:05 | INFO | TRAIN | Epoch 12/50 | avg_loss=1.164640 | accuracy=0.636
2026-01-16 18:11:05 | INFO | TRAIN | Epoch 13/50 | avg_loss=1.116089 | accuracy=0.636
2026-01-16 18:11:05 | INFO | TRAIN | Epoch 14/50 | avg_loss=1.071445 | accuracy=0.636
2026-01-16 18:11:05 | INFO | TRAIN | Epoch 15/50 | avg_loss=1.030353 | accuracy=0.636
2026-01-16 18:11:05 | INFO | TRAIN | Epoch 16/50 | avg_loss=0.992489 | accuracy=0.636
2026-01-16 18:11:05 | INFO | TRAIN | Epoch 17/50 | avg_loss=0.957554 | accuracy=0.636
2026-01-16 18:11:05 | INFO | TRAIN | Epoch 18/50 | avg_loss=0.925279 | accuracy=0.636
2026-01-16 18:11:05 | INFO | TRAIN | Epoch 19/50 | avg_loss=0.895421 | accuracy=0.636
2026-01-16 18:11:05 | INFO | TRAIN | Epoch 20/50 | avg_loss=0.867761 | accuracy=0.636
2026-01-16 18:11:05 | INFO | TRAIN | Epoch 21/50 | avg_loss=0.842103 | accuracy=0.636
2026-01-16 18:11:05 | INFO | TRAIN | Epoch 22/50 | avg_loss=0.818269 | accuracy=0.636
2026-01-16 18:11:05 | INFO | TRAIN | Epoch 23/50 | avg_loss=0.796101 | accuracy=0.636
2026-01-16 18:11:05 | INFO | TRAIN | Epoch 24/50 | avg_loss=0.775454 | accuracy=0.636
2026-01-16 18:11:05 | INFO | TRAIN | Epoch 25/50 | avg_loss=0.756201 | accuracy=0.636
2026-01-16 18:11:05 | INFO | TRAIN | Epoch 26/50 | avg_loss=0.738224 | accuracy=0.636
2026-01-16 18:11:05 | INFO | TRAIN | Epoch 27/50 | avg_loss=0.721419 | accuracy=0.636
2026-01-16 18:11:05 | INFO | TRAIN | Epoch 28/50 | avg_loss=0.705690 | accuracy=0.636
2026-01-16 18:11:05 | INFO | TRAIN | Epoch 29/50 | avg_loss=0.690950 | accuracy=0.636
2026-01-16 18:11:05 | INFO | TRAIN | Epoch 30/50 | avg_loss=0.677122 | accuracy=0.636
2026-01-16 18:11:05 | INFO | TRAIN | Epoch 31/50 | avg_loss=0.664134 | accuracy=0.636
2026-01-16 18:11:05 | INFO | TRAIN | Epoch 32/50 | avg_loss=0.651922 | accuracy=0.636
2026-01-16 18:11:05 | INFO | TRAIN | Epoch 33/50 | avg_loss=0.640427 | accuracy=0.636
2026-01-16 18:11:05 | INFO | TRAIN | Epoch 34/50 | avg_loss=0.629594 | accuracy=0.636
2026-01-16 18:11:05 | INFO | TRAIN | Epoch 35/50 | avg_loss=0.619376 | accuracy=0.636
2026-01-16 18:11:05 | INFO | TRAIN | Epoch 36/50 | avg_loss=0.609727 | accuracy=0.636
2026-01-16 18:11:05 | INFO | TRAIN | Epoch 37/50 | avg_loss=0.600605 | accuracy=0.636
2026-01-16 18:11:05 | INFO | TRAIN | Epoch 38/50 | avg_loss=0.591974 | accuracy=0.636
2026-01-16 18:11:05 | INFO | TRAIN | Epoch 39/50 | avg_loss=0.583800 | accuracy=0.636
2026-01-16 18:11:05 | INFO | TRAIN | Epoch 40/50 | avg_loss=0.576050 | accuracy=0.636
2026-01-16 18:11:05 | INFO | TRAIN | Epoch 41/50 | avg_loss=0.568696 | accuracy=0.636
2026-01-16 18:11:05 | INFO | TRAIN | Epoch 42/50 | avg_loss=0.561711 | accuracy=0.636
2026-01-16 18:11:05 | INFO | TRAIN | Epoch 43/50 | avg_loss=0.555070 | accuracy=0.636
2026-01-16 18:11:05 | INFO | TRAIN | Epoch 44/50 | avg_loss=0.548752 | accuracy=0.636
2026-01-16 18:11:05 | INFO | TRAIN | Epoch 45/50 | avg_loss=0.542735 | accuracy=0.636
2026-01-16 18:11:05 | INFO | TRAIN | Epoch 46/50 | avg_loss=0.536999 | accuracy=0.636
2026-01-16 18:11:05 | INFO | TRAIN | Epoch 47/50 | avg_loss=0.531529 | accuracy=0.636
2026-01-16 18:11:05 | INFO | TRAIN | Epoch 48/50 | avg_loss=0.526306 | accuracy=0.636
2026-01-16 18:11:05 | INFO | TRAIN | Epoch 49/50 | avg_loss=0.521316 | accuracy=0.636
2026-01-16 18:11:05 | INFO | TRAIN | Epoch 50/50 | avg_loss=0.516545 | accuracy=0.636
2026-01-16 18:11:05 | INFO | IO | Wrote training log to C:\Users\edaci\Documents\datafun\toy-gpt\train-300-context-2\outputs\train_log.csv
2026-01-16 18:11:05 | INFO | IO | Wrote vocabulary to C:\Users\edaci\Documents\datafun\toy-gpt\train-300-context-2\artifacts\01_vocabulary.csv
2026-01-16 18:11:05 | INFO | IO | Wrote model weights to C:\Users\edaci\Documents\datafun\toy-gpt\train-300-context-2\artifacts\02_model_weights.csv
2026-01-16 18:11:05 | INFO | IO | Wrote meta to C:\Users\edaci\Documents\datafun\toy-gpt\train-300-context-2\artifacts\00_meta.json
2026-01-16 18:11:05 | INFO | TRAIN | After training, most likely next token after 'the'|'cat' is 'lay' (ID: 2).
2026-01-16 18:11:10 | INFO | INFER | === RUN START ===
2026-01-16 18:11:10 | INFO | INFER | project=Inference Demo: Load Artifacts and Generate Text (Context-2)
2026-01-16 18:11:10 | INFO | INFER | repo_dir=train-300-context-2
2026-01-16 18:11:10 | INFO | INFER | python=3.14.0
2026-01-16 18:11:10 | INFO | INFER | os=Windows 11
2026-01-16 18:11:10 | INFO | INFER | shell=powershell
2026-01-16 18:11:10 | INFO | INFER | cwd=.
2026-01-16 18:11:10 | INFO | INFER | github_actions=False
2026-01-16 18:11:10 | INFO | MODEL | Model initialized with vocabulary size 8 (context-2).
2026-01-16 18:11:10 | INFO | INFER | Loaded repo_name=train-300-context-2 model_kind=context2
2026-01-16 18:11:10 | INFO | INFER | Vocab size: 8
2026-01-16 18:11:10 | INFO | INFER | Start token: cat
2026-01-16 18:11:10 | INFO | INFER | Context-2 bootstrap: (cat, cat)
2026-01-16 18:11:10 | INFO | INFER | Top next-token predictions after cat|cat:
2026-01-16 18:11:10 | INFO | INFER |   cat (ID 0): 0.1250
2026-01-16 18:11:10 | INFO | INFER |   dog (ID 1): 0.1250
2026-01-16 18:11:10 | INFO | INFER |   lay (ID 2): 0.1250
2026-01-16 18:11:10 | INFO | INFER | Generated sequence:
2026-01-16 18:11:10 | INFO | INFER |   cat cat cat cat cat cat cat cat cat cat cat
2026-01-16 18:11:23 | INFO | TOKEN | === RUN START ===
2026-01-16 18:11:23 | INFO | TOKEN | project=Simple Tokenizer Demo
2026-01-16 18:11:23 | INFO | TOKEN | repo_dir=train-300-context-2
2026-01-16 18:11:23 | INFO | TOKEN | python=3.14.0
2026-01-16 18:11:23 | INFO | TOKEN | os=Windows 11
2026-01-16 18:11:23 | INFO | TOKEN | shell=powershell
2026-01-16 18:11:23 | INFO | TOKEN | cwd=.
2026-01-16 18:11:23 | INFO | TOKEN | github_actions=False
2026-01-16 18:11:23 | INFO | TOKEN | Tokenizer initialized with 24 tokens.
2026-01-16 18:11:23 | INFO | TOKEN | First 10 tokens: ['the', 'cat', 'sat', 'on', 'the', 'mat', 'the', 'dog', 'sat', 'on']
2026-01-16 18:11:23 | INFO | TOKEN | Total number of tokens: 24
2026-01-16 18:11:23 | INFO | TOKEN | Average token length: 2.83
2026-01-16 18:11:23 | INFO | VOCAB | === RUN START ===
2026-01-16 18:11:23 | INFO | VOCAB | project=Vocabulary Demo
2026-01-16 18:11:23 | INFO | VOCAB | repo_dir=train-300-context-2
2026-01-16 18:11:23 | INFO | VOCAB | python=3.14.0
2026-01-16 18:11:23 | INFO | VOCAB | os=Windows 11
2026-01-16 18:11:23 | INFO | VOCAB | shell=powershell
2026-01-16 18:11:23 | INFO | VOCAB | cwd=.
2026-01-16 18:11:23 | INFO | VOCAB | github_actions=False
2026-01-16 18:11:23 | INFO | TOKEN | Tokenizer initialized with 24 tokens.
2026-01-16 18:11:23 | INFO | VOCAB | Vocabulary initialized with 8 unique tokens.
2026-01-16 18:11:23 | INFO | VOCAB | Vocabulary size: 8
2026-01-16 18:11:23 | INFO | VOCAB | Sample token: 'the' | ID: 7 | Frequency: 8
2026-01-16 18:11:23 | INFO | MODEL | === RUN START ===
2026-01-16 18:11:23 | INFO | MODEL | project=Simple Next-Token Model Demo (Context-2)
2026-01-16 18:11:23 | INFO | MODEL | repo_dir=train-300-context-2
2026-01-16 18:11:23 | INFO | MODEL | python=3.14.0
2026-01-16 18:11:23 | INFO | MODEL | os=Windows 11
2026-01-16 18:11:23 | INFO | MODEL | shell=powershell
2026-01-16 18:11:23 | INFO | MODEL | cwd=.
2026-01-16 18:11:23 | INFO | MODEL | github_actions=False
2026-01-16 18:11:23 | INFO | TOKEN | Tokenizer initialized with 24 tokens.
2026-01-16 18:11:23 | INFO | VOCAB | Vocabulary initialized with 8 unique tokens.
2026-01-16 18:11:23 | INFO | MODEL | Model initialized with vocabulary size 8 (context-2).
2026-01-16 18:11:23 | INFO | MODEL | Input tokens: 'cat' (ID 0), 'sat' (ID 6)
2026-01-16 18:11:23 | INFO | MODEL | Output probabilities for next token:
2026-01-16 18:11:23 | INFO | MODEL |   'cat' (ID 0) -> 0.1250
2026-01-16 18:11:23 | INFO | MODEL |   'dog' (ID 1) -> 0.1250
2026-01-16 18:11:23 | INFO | MODEL |   'lay' (ID 2) -> 0.1250
2026-01-16 18:11:23 | INFO | MODEL |   'mat' (ID 3) -> 0.1250
2026-01-16 18:11:23 | INFO | MODEL |   'on' (ID 4) -> 0.1250
2026-01-16 18:11:23 | INFO | MODEL |   'rug' (ID 5) -> 0.1250
2026-01-16 18:11:23 | INFO | MODEL |   'sat' (ID 6) -> 0.1250
2026-01-16 18:11:23 | INFO | MODEL |   'the' (ID 7) -> 0.1250
2026-01-16 18:11:23 | INFO | TRAIN | === RUN START ===
2026-01-16 18:11:23 | INFO | TRAIN | project=Training Demo: Next-Token Softmax Regression (Context-2)
2026-01-16 18:11:23 | INFO | TRAIN | repo_dir=train-300-context-2
2026-01-16 18:11:23 | INFO | TRAIN | python=3.14.0
2026-01-16 18:11:23 | INFO | TRAIN | os=Windows 11
2026-01-16 18:11:23 | INFO | TRAIN | shell=powershell
2026-01-16 18:11:23 | INFO | TRAIN | cwd=.
2026-01-16 18:11:23 | INFO | TRAIN | github_actions=False
2026-01-16 18:11:23 | INFO | TOKEN | Tokenizer initialized with 24 tokens.
2026-01-16 18:11:23 | INFO | VOCAB | Vocabulary initialized with 8 unique tokens.
2026-01-16 18:11:23 | INFO | TRAIN | Created 22 training pairs.
2026-01-16 18:11:23 | INFO | MODEL | Model initialized with vocabulary size 8 (context-2).
2026-01-16 18:11:23 | INFO | TRAIN | Epoch 1/50 | avg_loss=2.063953 | accuracy=0.227
2026-01-16 18:11:23 | INFO | TRAIN | Epoch 2/50 | avg_loss=1.949229 | accuracy=0.636
2026-01-16 18:11:23 | INFO | TRAIN | Epoch 3/50 | avg_loss=1.841472 | accuracy=0.636
2026-01-16 18:11:23 | INFO | TRAIN | Epoch 4/50 | avg_loss=1.740749 | accuracy=0.636
2026-01-16 18:11:23 | INFO | TRAIN | Epoch 5/50 | avg_loss=1.647012 | accuracy=0.636
2026-01-16 18:11:23 | INFO | TRAIN | Epoch 6/50 | avg_loss=1.560108 | accuracy=0.636
2026-01-16 18:11:23 | INFO | TRAIN | Epoch 7/50 | avg_loss=1.479793 | accuracy=0.636
2026-01-16 18:11:23 | INFO | TRAIN | Epoch 8/50 | avg_loss=1.405753 | accuracy=0.636
2026-01-16 18:11:23 | INFO | TRAIN | Epoch 9/50 | avg_loss=1.337621 | accuracy=0.636
2026-01-16 18:11:23 | INFO | TRAIN | Epoch 10/50 | avg_loss=1.275000 | accuracy=0.636
2026-01-16 18:11:23 | INFO | TRAIN | Epoch 11/50 | avg_loss=1.217477 | accuracy=0.636
2026-01-16 18:11:23 | INFO | TRAIN | Epoch 12/50 | avg_loss=1.164640 | accuracy=0.636
2026-01-16 18:11:23 | INFO | TRAIN | Epoch 13/50 | avg_loss=1.116089 | accuracy=0.636
2026-01-16 18:11:23 | INFO | TRAIN | Epoch 14/50 | avg_loss=1.071445 | accuracy=0.636
2026-01-16 18:11:23 | INFO | TRAIN | Epoch 15/50 | avg_loss=1.030353 | accuracy=0.636
2026-01-16 18:11:23 | INFO | TRAIN | Epoch 16/50 | avg_loss=0.992489 | accuracy=0.636
2026-01-16 18:11:23 | INFO | TRAIN | Epoch 17/50 | avg_loss=0.957554 | accuracy=0.636
2026-01-16 18:11:23 | INFO | TRAIN | Epoch 18/50 | avg_loss=0.925279 | accuracy=0.636
2026-01-16 18:11:23 | INFO | TRAIN | Epoch 19/50 | avg_loss=0.895421 | accuracy=0.636
2026-01-16 18:11:23 | INFO | TRAIN | Epoch 20/50 | avg_loss=0.867761 | accuracy=0.636
2026-01-16 18:11:23 | INFO | TRAIN | Epoch 21/50 | avg_loss=0.842103 | accuracy=0.636
2026-01-16 18:11:23 | INFO | TRAIN | Epoch 22/50 | avg_loss=0.818269 | accuracy=0.636
2026-01-16 18:11:23 | INFO | TRAIN | Epoch 23/50 | avg_loss=0.796101 | accuracy=0.636
2026-01-16 18:11:23 | INFO | TRAIN | Epoch 24/50 | avg_loss=0.775454 | accuracy=0.636
2026-01-16 18:11:23 | INFO | TRAIN | Epoch 25/50 | avg_loss=0.756201 | accuracy=0.636
2026-01-16 18:11:23 | INFO | TRAIN | Epoch 26/50 | avg_loss=0.738224 | accuracy=0.636
2026-01-16 18:11:23 | INFO | TRAIN | Epoch 27/50 | avg_loss=0.721419 | accuracy=0.636
2026-01-16 18:11:23 | INFO | TRAIN | Epoch 28/50 | avg_loss=0.705690 | accuracy=0.636
2026-01-16 18:11:23 | INFO | TRAIN | Epoch 29/50 | avg_loss=0.690950 | accuracy=0.636
2026-01-16 18:11:23 | INFO | TRAIN | Epoch 30/50 | avg_loss=0.677122 | accuracy=0.636
2026-01-16 18:11:23 | INFO | TRAIN | Epoch 31/50 | avg_loss=0.664134 | accuracy=0.636
2026-01-16 18:11:23 | INFO | TRAIN | Epoch 32/50 | avg_loss=0.651922 | accuracy=0.636
2026-01-16 18:11:23 | INFO | TRAIN | Epoch 33/50 | avg_loss=0.640427 | accuracy=0.636
2026-01-16 18:11:23 | INFO | TRAIN | Epoch 34/50 | avg_loss=0.629594 | accuracy=0.636
2026-01-16 18:11:23 | INFO | TRAIN | Epoch 35/50 | avg_loss=0.619376 | accuracy=0.636
2026-01-16 18:11:23 | INFO | TRAIN | Epoch 36/50 | avg_loss=0.609727 | accuracy=0.636
2026-01-16 18:11:23 | INFO | TRAIN | Epoch 37/50 | avg_loss=0.600605 | accuracy=0.636
2026-01-16 18:11:23 | INFO | TRAIN | Epoch 38/50 | avg_loss=0.591974 | accuracy=0.636
2026-01-16 18:11:23 | INFO | TRAIN | Epoch 39/50 | avg_loss=0.583800 | accuracy=0.636
2026-01-16 18:11:23 | INFO | TRAIN | Epoch 40/50 | avg_loss=0.576050 | accuracy=0.636
2026-01-16 18:11:23 | INFO | TRAIN | Epoch 41/50 | avg_loss=0.568696 | accuracy=0.636
2026-01-16 18:11:23 | INFO | TRAIN | Epoch 42/50 | avg_loss=0.561711 | accuracy=0.636
2026-01-16 18:11:23 | INFO | TRAIN | Epoch 43/50 | avg_loss=0.555070 | accuracy=0.636
2026-01-16 18:11:23 | INFO | TRAIN | Epoch 44/50 | avg_loss=0.548752 | accuracy=0.636
2026-01-16 18:11:23 | INFO | TRAIN | Epoch 45/50 | avg_loss=0.542735 | accuracy=0.636
2026-01-16 18:11:23 | INFO | TRAIN | Epoch 46/50 | avg_loss=0.536999 | accuracy=0.636
2026-01-16 18:11:23 | INFO | TRAIN | Epoch 47/50 | avg_loss=0.531529 | accuracy=0.636
2026-01-16 18:11:23 | INFO | TRAIN | Epoch 48/50 | avg_loss=0.526306 | accuracy=0.636
2026-01-16 18:11:23 | INFO | TRAIN | Epoch 49/50 | avg_loss=0.521316 | accuracy=0.636
2026-01-16 18:11:23 | INFO | TRAIN | Epoch 50/50 | avg_loss=0.516545 | accuracy=0.636
2026-01-16 18:11:23 | INFO | IO | Wrote training log to C:\Users\edaci\Documents\datafun\toy-gpt\train-300-context-2\outputs\train_log.csv
2026-01-16 18:11:23 | INFO | IO | Wrote vocabulary to C:\Users\edaci\Documents\datafun\toy-gpt\train-300-context-2\artifacts\01_vocabulary.csv
2026-01-16 18:11:23 | INFO | IO | Wrote model weights to C:\Users\edaci\Documents\datafun\toy-gpt\train-300-context-2\artifacts\02_model_weights.csv
2026-01-16 18:11:23 | INFO | IO | Wrote meta to C:\Users\edaci\Documents\datafun\toy-gpt\train-300-context-2\artifacts\00_meta.json
2026-01-16 18:11:23 | INFO | TRAIN | After training, most likely next token after 'the'|'cat' is 'lay' (ID: 2).
2026-01-16 18:36:24 | INFO | TRAIN | === RUN START ===
2026-01-16 18:36:24 | INFO | TRAIN | project=Training Demo: Next-Token Softmax Regression (Context-2)
2026-01-16 18:36:24 | INFO | TRAIN | repo_dir=train-300-context-2
2026-01-16 18:36:24 | INFO | TRAIN | python=3.14.0
2026-01-16 18:36:24 | INFO | TRAIN | os=Windows 11
2026-01-16 18:36:24 | INFO | TRAIN | shell=powershell
2026-01-16 18:36:24 | INFO | TRAIN | cwd=.
2026-01-16 18:36:24 | INFO | TRAIN | github_actions=False
2026-01-16 18:36:24 | INFO | TOKEN | Tokenizer initialized with 24 tokens.
2026-01-16 18:36:24 | INFO | VOCAB | Vocabulary initialized with 8 unique tokens.
2026-01-16 18:36:24 | INFO | TRAIN | Created 22 training pairs.
2026-01-16 18:36:24 | INFO | MODEL | Model initialized with vocabulary size 8 (context-2).
2026-01-16 18:36:24 | INFO | TRAIN | Epoch 1/50 | avg_loss=2.063953 | accuracy=0.227
2026-01-16 18:36:24 | INFO | TRAIN | Epoch 2/50 | avg_loss=1.949229 | accuracy=0.636
2026-01-16 18:36:24 | INFO | TRAIN | Epoch 3/50 | avg_loss=1.841472 | accuracy=0.636
2026-01-16 18:36:24 | INFO | TRAIN | Epoch 4/50 | avg_loss=1.740749 | accuracy=0.636
2026-01-16 18:36:24 | INFO | TRAIN | Epoch 5/50 | avg_loss=1.647012 | accuracy=0.636
2026-01-16 18:36:24 | INFO | TRAIN | Epoch 6/50 | avg_loss=1.560108 | accuracy=0.636
2026-01-16 18:36:24 | INFO | TRAIN | Epoch 7/50 | avg_loss=1.479793 | accuracy=0.636
2026-01-16 18:36:24 | INFO | TRAIN | Epoch 8/50 | avg_loss=1.405753 | accuracy=0.636
2026-01-16 18:36:24 | INFO | TRAIN | Epoch 9/50 | avg_loss=1.337621 | accuracy=0.636
2026-01-16 18:36:24 | INFO | TRAIN | Epoch 10/50 | avg_loss=1.275000 | accuracy=0.636
2026-01-16 18:36:24 | INFO | TRAIN | Epoch 11/50 | avg_loss=1.217477 | accuracy=0.636
2026-01-16 18:36:24 | INFO | TRAIN | Epoch 12/50 | avg_loss=1.164640 | accuracy=0.636
2026-01-16 18:36:24 | INFO | TRAIN | Epoch 13/50 | avg_loss=1.116089 | accuracy=0.636
2026-01-16 18:36:24 | INFO | TRAIN | Epoch 14/50 | avg_loss=1.071445 | accuracy=0.636
2026-01-16 18:36:24 | INFO | TRAIN | Epoch 15/50 | avg_loss=1.030353 | accuracy=0.636
2026-01-16 18:36:24 | INFO | TRAIN | Epoch 16/50 | avg_loss=0.992489 | accuracy=0.636
2026-01-16 18:36:24 | INFO | TRAIN | Epoch 17/50 | avg_loss=0.957554 | accuracy=0.636
2026-01-16 18:36:24 | INFO | TRAIN | Epoch 18/50 | avg_loss=0.925279 | accuracy=0.636
2026-01-16 18:36:24 | INFO | TRAIN | Epoch 19/50 | avg_loss=0.895421 | accuracy=0.636
2026-01-16 18:36:24 | INFO | TRAIN | Epoch 20/50 | avg_loss=0.867761 | accuracy=0.636
2026-01-16 18:36:24 | INFO | TRAIN | Epoch 21/50 | avg_loss=0.842103 | accuracy=0.636
2026-01-16 18:36:24 | INFO | TRAIN | Epoch 22/50 | avg_loss=0.818269 | accuracy=0.636
2026-01-16 18:36:24 | INFO | TRAIN | Epoch 23/50 | avg_loss=0.796101 | accuracy=0.636
2026-01-16 18:36:24 | INFO | TRAIN | Epoch 24/50 | avg_loss=0.775454 | accuracy=0.636
2026-01-16 18:36:24 | INFO | TRAIN | Epoch 25/50 | avg_loss=0.756201 | accuracy=0.636
2026-01-16 18:36:24 | INFO | TRAIN | Epoch 26/50 | avg_loss=0.738224 | accuracy=0.636
2026-01-16 18:36:24 | INFO | TRAIN | Epoch 27/50 | avg_loss=0.721419 | accuracy=0.636
2026-01-16 18:36:24 | INFO | TRAIN | Epoch 28/50 | avg_loss=0.705690 | accuracy=0.636
2026-01-16 18:36:24 | INFO | TRAIN | Epoch 29/50 | avg_loss=0.690950 | accuracy=0.636
2026-01-16 18:36:24 | INFO | TRAIN | Epoch 30/50 | avg_loss=0.677122 | accuracy=0.636
2026-01-16 18:36:24 | INFO | TRAIN | Epoch 31/50 | avg_loss=0.664134 | accuracy=0.636
2026-01-16 18:36:24 | INFO | TRAIN | Epoch 32/50 | avg_loss=0.651922 | accuracy=0.636
2026-01-16 18:36:24 | INFO | TRAIN | Epoch 33/50 | avg_loss=0.640427 | accuracy=0.636
2026-01-16 18:36:24 | INFO | TRAIN | Epoch 34/50 | avg_loss=0.629594 | accuracy=0.636
2026-01-16 18:36:24 | INFO | TRAIN | Epoch 35/50 | avg_loss=0.619376 | accuracy=0.636
2026-01-16 18:36:24 | INFO | TRAIN | Epoch 36/50 | avg_loss=0.609727 | accuracy=0.636
2026-01-16 18:36:24 | INFO | TRAIN | Epoch 37/50 | avg_loss=0.600605 | accuracy=0.636
2026-01-16 18:36:24 | INFO | TRAIN | Epoch 38/50 | avg_loss=0.591974 | accuracy=0.636
2026-01-16 18:36:24 | INFO | TRAIN | Epoch 39/50 | avg_loss=0.583800 | accuracy=0.636
2026-01-16 18:36:24 | INFO | TRAIN | Epoch 40/50 | avg_loss=0.576050 | accuracy=0.636
2026-01-16 18:36:24 | INFO | TRAIN | Epoch 41/50 | avg_loss=0.568696 | accuracy=0.636
2026-01-16 18:36:24 | INFO | TRAIN | Epoch 42/50 | avg_loss=0.561711 | accuracy=0.636
2026-01-16 18:36:24 | INFO | TRAIN | Epoch 43/50 | avg_loss=0.555070 | accuracy=0.636
2026-01-16 18:36:24 | INFO | TRAIN | Epoch 44/50 | avg_loss=0.548752 | accuracy=0.636
2026-01-16 18:36:24 | INFO | TRAIN | Epoch 45/50 | avg_loss=0.542735 | accuracy=0.636
2026-01-16 18:36:24 | INFO | TRAIN | Epoch 46/50 | avg_loss=0.536999 | accuracy=0.636
2026-01-16 18:36:24 | INFO | TRAIN | Epoch 47/50 | avg_loss=0.531529 | accuracy=0.636
2026-01-16 18:36:24 | INFO | TRAIN | Epoch 48/50 | avg_loss=0.526306 | accuracy=0.636
2026-01-16 18:36:24 | INFO | TRAIN | Epoch 49/50 | avg_loss=0.521316 | accuracy=0.636
2026-01-16 18:36:24 | INFO | TRAIN | Epoch 50/50 | avg_loss=0.516545 | accuracy=0.636
2026-01-16 18:36:24 | INFO | IO | Wrote training log to C:\Users\edaci\Documents\datafun\toy-gpt\train-300-context-2\outputs\train_log.csv
2026-01-16 18:36:24 | INFO | IO | Wrote vocabulary to C:\Users\edaci\Documents\datafun\toy-gpt\train-300-context-2\artifacts\01_vocabulary.csv
2026-01-16 18:36:24 | INFO | IO | Wrote model weights to C:\Users\edaci\Documents\datafun\toy-gpt\train-300-context-2\artifacts\02_model_weights.csv
2026-01-16 18:36:24 | INFO | IO | Wrote meta to C:\Users\edaci\Documents\datafun\toy-gpt\train-300-context-2\artifacts\00_meta.json
2026-01-16 18:36:24 | INFO | TRAIN | After training, most likely next token after 'the'|'cat' is 'lay' (ID: 2).
2026-01-16 18:36:34 | INFO | INFER | === RUN START ===
2026-01-16 18:36:34 | INFO | INFER | project=Inference Demo: Load Artifacts and Generate Text (Context-2)
2026-01-16 18:36:34 | INFO | INFER | repo_dir=train-300-context-2
2026-01-16 18:36:34 | INFO | INFER | python=3.14.0
2026-01-16 18:36:34 | INFO | INFER | os=Windows 11
2026-01-16 18:36:34 | INFO | INFER | shell=powershell
2026-01-16 18:36:34 | INFO | INFER | cwd=.
2026-01-16 18:36:34 | INFO | INFER | github_actions=False
2026-01-16 18:36:34 | INFO | MODEL | Model initialized with vocabulary size 8 (context-2).
2026-01-16 18:36:34 | INFO | INFER | Loaded repo_name=train-300-context-2 model_kind=context2
2026-01-16 18:36:34 | INFO | INFER | Vocab size: 8
2026-01-16 18:36:34 | INFO | INFER | Start token: cat
2026-01-16 18:36:34 | INFO | INFER | Context-2 bootstrap: (cat, cat)
2026-01-16 18:36:34 | INFO | INFER | Top next-token predictions after cat|cat:
2026-01-16 18:36:34 | INFO | INFER |   cat (ID 0): 0.1250
2026-01-16 18:36:34 | INFO | INFER |   dog (ID 1): 0.1250
2026-01-16 18:36:34 | INFO | INFER |   lay (ID 2): 0.1250
2026-01-16 18:36:34 | INFO | INFER | Generated sequence:
2026-01-16 18:36:34 | INFO | INFER |   cat cat cat cat cat cat cat cat cat cat cat
2026-01-16 18:40:47 | INFO | TRAIN | === RUN START ===
2026-01-16 18:40:47 | INFO | TRAIN | project=Training Demo: Next-Token Softmax Regression (Context-2)
2026-01-16 18:40:47 | INFO | TRAIN | repo_dir=train-300-context-2
2026-01-16 18:40:47 | INFO | TRAIN | python=3.14.0
2026-01-16 18:40:47 | INFO | TRAIN | os=Windows 11
2026-01-16 18:40:47 | INFO | TRAIN | shell=powershell
2026-01-16 18:40:47 | INFO | TRAIN | cwd=.
2026-01-16 18:40:47 | INFO | TRAIN | github_actions=False
2026-01-16 18:40:47 | INFO | TOKEN | Tokenizer initialized with 24 tokens.
2026-01-16 18:40:47 | INFO | VOCAB | Vocabulary initialized with 8 unique tokens.
2026-01-16 18:40:47 | INFO | TRAIN | Created 22 training pairs.
2026-01-16 18:40:47 | INFO | MODEL | Model initialized with vocabulary size 8 (context-2).
2026-01-16 18:40:47 | INFO | TRAIN | Epoch 1/50 | avg_loss=2.063953 | accuracy=0.227
2026-01-16 18:40:47 | INFO | TRAIN | Epoch 2/50 | avg_loss=1.949229 | accuracy=0.636
2026-01-16 18:40:47 | INFO | TRAIN | Epoch 3/50 | avg_loss=1.841472 | accuracy=0.636
2026-01-16 18:40:47 | INFO | TRAIN | Epoch 4/50 | avg_loss=1.740749 | accuracy=0.636
2026-01-16 18:40:47 | INFO | TRAIN | Epoch 5/50 | avg_loss=1.647012 | accuracy=0.636
2026-01-16 18:40:47 | INFO | TRAIN | Epoch 6/50 | avg_loss=1.560108 | accuracy=0.636
2026-01-16 18:40:47 | INFO | TRAIN | Epoch 7/50 | avg_loss=1.479793 | accuracy=0.636
2026-01-16 18:40:47 | INFO | TRAIN | Epoch 8/50 | avg_loss=1.405753 | accuracy=0.636
2026-01-16 18:40:47 | INFO | TRAIN | Epoch 9/50 | avg_loss=1.337621 | accuracy=0.636
2026-01-16 18:40:47 | INFO | TRAIN | Epoch 10/50 | avg_loss=1.275000 | accuracy=0.636
2026-01-16 18:40:47 | INFO | TRAIN | Epoch 11/50 | avg_loss=1.217477 | accuracy=0.636
2026-01-16 18:40:47 | INFO | TRAIN | Epoch 12/50 | avg_loss=1.164640 | accuracy=0.636
2026-01-16 18:40:47 | INFO | TRAIN | Epoch 13/50 | avg_loss=1.116089 | accuracy=0.636
2026-01-16 18:40:47 | INFO | TRAIN | Epoch 14/50 | avg_loss=1.071445 | accuracy=0.636
2026-01-16 18:40:47 | INFO | TRAIN | Epoch 15/50 | avg_loss=1.030353 | accuracy=0.636
2026-01-16 18:40:47 | INFO | TRAIN | Epoch 16/50 | avg_loss=0.992489 | accuracy=0.636
2026-01-16 18:40:47 | INFO | TRAIN | Epoch 17/50 | avg_loss=0.957554 | accuracy=0.636
2026-01-16 18:40:47 | INFO | TRAIN | Epoch 18/50 | avg_loss=0.925279 | accuracy=0.636
2026-01-16 18:40:47 | INFO | TRAIN | Epoch 19/50 | avg_loss=0.895421 | accuracy=0.636
2026-01-16 18:40:47 | INFO | TRAIN | Epoch 20/50 | avg_loss=0.867761 | accuracy=0.636
2026-01-16 18:40:47 | INFO | TRAIN | Epoch 21/50 | avg_loss=0.842103 | accuracy=0.636
2026-01-16 18:40:47 | INFO | TRAIN | Epoch 22/50 | avg_loss=0.818269 | accuracy=0.636
2026-01-16 18:40:47 | INFO | TRAIN | Epoch 23/50 | avg_loss=0.796101 | accuracy=0.636
2026-01-16 18:40:47 | INFO | TRAIN | Epoch 24/50 | avg_loss=0.775454 | accuracy=0.636
2026-01-16 18:40:47 | INFO | TRAIN | Epoch 25/50 | avg_loss=0.756201 | accuracy=0.636
2026-01-16 18:40:47 | INFO | TRAIN | Epoch 26/50 | avg_loss=0.738224 | accuracy=0.636
2026-01-16 18:40:47 | INFO | TRAIN | Epoch 27/50 | avg_loss=0.721419 | accuracy=0.636
2026-01-16 18:40:47 | INFO | TRAIN | Epoch 28/50 | avg_loss=0.705690 | accuracy=0.636
2026-01-16 18:40:47 | INFO | TRAIN | Epoch 29/50 | avg_loss=0.690950 | accuracy=0.636
2026-01-16 18:40:47 | INFO | TRAIN | Epoch 30/50 | avg_loss=0.677122 | accuracy=0.636
2026-01-16 18:40:47 | INFO | TRAIN | Epoch 31/50 | avg_loss=0.664134 | accuracy=0.636
2026-01-16 18:40:47 | INFO | TRAIN | Epoch 32/50 | avg_loss=0.651922 | accuracy=0.636
2026-01-16 18:40:47 | INFO | TRAIN | Epoch 33/50 | avg_loss=0.640427 | accuracy=0.636
2026-01-16 18:40:47 | INFO | TRAIN | Epoch 34/50 | avg_loss=0.629594 | accuracy=0.636
2026-01-16 18:40:47 | INFO | TRAIN | Epoch 35/50 | avg_loss=0.619376 | accuracy=0.636
2026-01-16 18:40:47 | INFO | TRAIN | Epoch 36/50 | avg_loss=0.609727 | accuracy=0.636
2026-01-16 18:40:47 | INFO | TRAIN | Epoch 37/50 | avg_loss=0.600605 | accuracy=0.636
2026-01-16 18:40:47 | INFO | TRAIN | Epoch 38/50 | avg_loss=0.591974 | accuracy=0.636
2026-01-16 18:40:47 | INFO | TRAIN | Epoch 39/50 | avg_loss=0.583800 | accuracy=0.636
2026-01-16 18:40:47 | INFO | TRAIN | Epoch 40/50 | avg_loss=0.576050 | accuracy=0.636
2026-01-16 18:40:47 | INFO | TRAIN | Epoch 41/50 | avg_loss=0.568696 | accuracy=0.636
2026-01-16 18:40:47 | INFO | TRAIN | Epoch 42/50 | avg_loss=0.561711 | accuracy=0.636
2026-01-16 18:40:47 | INFO | TRAIN | Epoch 43/50 | avg_loss=0.555070 | accuracy=0.636
2026-01-16 18:40:47 | INFO | TRAIN | Epoch 44/50 | avg_loss=0.548752 | accuracy=0.636
2026-01-16 18:40:47 | INFO | TRAIN | Epoch 45/50 | avg_loss=0.542735 | accuracy=0.636
2026-01-16 18:40:47 | INFO | TRAIN | Epoch 46/50 | avg_loss=0.536999 | accuracy=0.636
2026-01-16 18:40:47 | INFO | TRAIN | Epoch 47/50 | avg_loss=0.531529 | accuracy=0.636
2026-01-16 18:40:47 | INFO | TRAIN | Epoch 48/50 | avg_loss=0.526306 | accuracy=0.636
2026-01-16 18:40:47 | INFO | TRAIN | Epoch 49/50 | avg_loss=0.521316 | accuracy=0.636
2026-01-16 18:40:47 | INFO | TRAIN | Epoch 50/50 | avg_loss=0.516545 | accuracy=0.636
2026-01-16 18:40:47 | INFO | IO | Wrote training log to C:\Users\edaci\Documents\datafun\toy-gpt\train-300-context-2\outputs\train_log.csv
2026-01-16 18:40:47 | INFO | IO | Wrote vocabulary to C:\Users\edaci\Documents\datafun\toy-gpt\train-300-context-2\artifacts\01_vocabulary.csv
2026-01-16 18:40:47 | INFO | IO | Wrote model weights to C:\Users\edaci\Documents\datafun\toy-gpt\train-300-context-2\artifacts\02_model_weights.csv
2026-01-16 18:40:47 | INFO | IO | Wrote meta to C:\Users\edaci\Documents\datafun\toy-gpt\train-300-context-2\artifacts\00_meta.json
2026-01-16 18:40:47 | INFO | TRAIN | After training, most likely next token after 'the'|'cat' is 'lay' (ID: 2).
2026-01-16 18:40:52 | INFO | INFER | === RUN START ===
2026-01-16 18:40:52 | INFO | INFER | project=Inference Demo: Load Artifacts and Generate Text (Context-2)
2026-01-16 18:40:52 | INFO | INFER | repo_dir=train-300-context-2
2026-01-16 18:40:52 | INFO | INFER | python=3.14.0
2026-01-16 18:40:52 | INFO | INFER | os=Windows 11
2026-01-16 18:40:52 | INFO | INFER | shell=powershell
2026-01-16 18:40:52 | INFO | INFER | cwd=.
2026-01-16 18:40:52 | INFO | INFER | github_actions=False
2026-01-16 18:40:52 | INFO | MODEL | Model initialized with vocabulary size 8 (context-2).
2026-01-16 18:40:52 | INFO | INFER | Loaded repo_name=train-300-context-2 model_kind=context2
2026-01-16 18:40:52 | INFO | INFER | Vocab size: 8
2026-01-16 18:40:52 | INFO | INFER | Start token: cat
2026-01-16 18:40:52 | INFO | INFER | Context-2 bootstrap: (cat, cat)
2026-01-16 18:40:52 | INFO | INFER | Top next-token predictions after cat|cat:
2026-01-16 18:40:52 | INFO | INFER |   cat (ID 0): 0.1250
2026-01-16 18:40:52 | INFO | INFER |   dog (ID 1): 0.1250
2026-01-16 18:40:52 | INFO | INFER |   lay (ID 2): 0.1250
2026-01-16 18:40:52 | INFO | INFER | Generated sequence:
2026-01-16 18:40:52 | INFO | INFER |   cat cat cat cat cat cat cat cat cat cat cat
