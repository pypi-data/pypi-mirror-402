default_model: "gemini-3-flash-preview:cloud"
provider: "ollama"
models:
  "gemini-3-flash-preview:cloud":
    model_name: "ollama_chat/gemini-3-flash-preview:cloud"
    api_base: "http://localhost:11434" # Default local, likely overridden or proxied if cloud
    # api_key will be loaded from env OLLAMA_API_KEY

  "devstral-small-2:24b-cloud":
    model_name: "ollama_chat/devstral-small-2:24b-cloud"
    api_base: "http://localhost:11434"

# Example OpenAI config for reference
# provider: "openai"
# models:
#   "gpt-4o":
#     model_name: "gpt-4o"
