# -*- coding: utf-8 -*-
"""
èŠå¤©å¼•æ“

æ•´åˆ RAG å’Œ Skillsï¼Œæ”¯æŒ LangChain Agent å’Œæµå¼è¾“å‡º
"""
from typing import Iterator, Optional, List, Any, Callable
from ..llm import LLM
from .rag_engine import RAGEngine
from .skill_manager import SkillManager


class ChatEngine:
    """
    èŠå¤©å¼•æ“

    æ•´åˆ RAG å’Œ Skillsï¼Œæä¾›ç»Ÿä¸€çš„èŠå¤©æ¥å£
    """

    def __init__(
        self,
        llm: LLM,
        rag_engine: Optional[RAGEngine] = None,
        skill_manager: Optional[SkillManager] = None,
        system_prompt: str = ""
    ):
        """
        åˆå§‹åŒ–èŠå¤©å¼•æ“

        Args:
            llm: LLM å®ä¾‹
            rag_engine: RAG å¼•æ“ï¼ˆå¯é€‰ï¼‰
            skill_manager: Skill ç®¡ç†å™¨ï¼ˆå¯é€‰ï¼‰
            system_prompt: ç³»ç»Ÿæç¤ºè¯
        """
        self.llm = llm
        self.rag_engine = rag_engine
        self.skill_manager = skill_manager
        self.system_prompt = system_prompt

    def _convert_history_to_messages(self, history: Optional[List[dict]]) -> List:
        """
        å°†å†å²æ¶ˆæ¯è½¬æ¢ä¸º LangChain æ¶ˆæ¯æ ¼å¼
        
        Args:
            history: å†å²æ¶ˆæ¯åˆ—è¡¨ [{"role": "user", "content": "..."}, ...]
            
        Returns:
            LangChain æ¶ˆæ¯åˆ—è¡¨
        """
        from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
        
        messages = []
        
        # è½¬æ¢å†å²æ¶ˆæ¯ï¼ˆä¸åŒ…æ‹¬ç³»ç»Ÿæç¤ºè¯ï¼Œç³»ç»Ÿæç¤ºè¯ä¼šåœ¨è°ƒç”¨æ—¶å•ç‹¬æ·»åŠ ï¼‰
        if history:
            for msg in history:
                if not isinstance(msg, dict):
                    continue
                role = msg.get("role", "")
                content = msg.get("content", "")
                if not content:
                    continue
                    
                if role == "user":
                    messages.append(HumanMessage(content=content))
                elif role == "assistant":
                    messages.append(AIMessage(content=content))
                elif role == "system":
                    messages.append(SystemMessage(content=content))
        
        return messages

    def chat(
        self,
        query: str,
        use_rag: bool = True,
        use_tools: bool = True,
        history: Optional[List[dict]] = None
    ) -> str:
        """
        èŠå¤©æ–¹æ³•ï¼ˆéæµå¼ï¼‰

        Args:
            query: ç”¨æˆ·é—®é¢˜
            use_rag: æ˜¯å¦ä½¿ç”¨ RAG æ¨¡å¼
            use_tools: æ˜¯å¦ä½¿ç”¨å·¥å…·
            history: å†å²æ¶ˆæ¯åˆ—è¡¨ [{"role": "user", "content": "..."}, ...]

        Returns:
            LLM ç”Ÿæˆçš„å›ç­”
        """
        # å¦‚æœæœ‰å·¥å…·ä¸”å¯ç”¨å·¥å…·è°ƒç”¨ï¼Œä½¿ç”¨å¸¦å·¥å…·çš„èŠå¤©
        if use_tools and self.skill_manager:
            loaded_skills = self.skill_manager.list_loaded_skills()
            if len(loaded_skills) > 0:
                print(f"ğŸ”§ æ£€æµ‹åˆ° {len(loaded_skills)} ä¸ªå·²åŠ è½½çš„ skillsï¼Œå°è¯•ä½¿ç”¨å·¥å…·è°ƒç”¨")
                return self._chat_with_tools(query, use_rag=use_rag, history=history)
            else:
                print("âš ï¸  æ²¡æœ‰å·²åŠ è½½çš„ skillsï¼Œè·³è¿‡å·¥å…·è°ƒç”¨")
        
        if use_rag and self.rag_engine:
            return self._chat_with_rag(query, history=history)
        else:
            return self._chat_with_llm(query, history=history)

    def chat_stream(
        self,
        query: str,
        use_rag: bool = True,
        use_tools: bool = True,
        history: Optional[List[dict]] = None
    ) -> Iterator[str]:
        """
        æµå¼èŠå¤©æ–¹æ³•

        Args:
            query: ç”¨æˆ·é—®é¢˜
            use_rag: æ˜¯å¦ä½¿ç”¨ RAG æ¨¡å¼
            use_tools: æ˜¯å¦ä½¿ç”¨å·¥å…·
            history: å†å²æ¶ˆæ¯åˆ—è¡¨ [{"role": "user", "content": "..."}, ...]

        Yields:
            æ¯ä¸ª token çš„å­—ç¬¦ä¸²ç‰‡æ®µ
        """
        # å¦‚æœæœ‰å·¥å…·ä¸”å¯ç”¨å·¥å…·è°ƒç”¨ï¼Œä½¿ç”¨å¸¦å·¥å…·çš„æµå¼èŠå¤©
        if use_tools and self.skill_manager and len(self.skill_manager.list_loaded_skills()) > 0:
            yield from self._chat_with_tools_stream(query, use_rag=use_rag, history=history)
        elif use_rag and self.rag_engine:
            yield from self._chat_with_rag_stream(query, history=history)
        else:
            yield from self._chat_with_llm_stream(query, history=history)

    def _chat_with_rag(self, query: str, history: Optional[List[dict]] = None) -> str:
        """
        RAG æ¨¡å¼å¯¹è¯ï¼ˆéæµå¼ï¼‰
        """
        if not self.rag_engine:
            return self._chat_with_llm(query, history=history)

        # æ£€ç´¢ç›¸å…³æ–‡æ¡£
        context = self.rag_engine.search(query, top_k=5)

        # æ„å»ºæ¶ˆæ¯åˆ—è¡¨
        messages = self._convert_history_to_messages(history)
        
        if context:
            # æ·»åŠ  RAG ä¸Šä¸‹æ–‡åˆ°ç³»ç»Ÿæç¤ºè¯
            rag_prompt = f"""åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ã€‚å¦‚æœä¸Šä¸‹æ–‡ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·ç›´æ¥è¯´ä¸çŸ¥é“ã€‚

ä¸Šä¸‹æ–‡:
{context}"""
            
            # æ›´æ–°ç³»ç»Ÿæ¶ˆæ¯æˆ–æ·»åŠ æ–°çš„ç³»ç»Ÿæ¶ˆæ¯
            if messages and isinstance(messages[0], type(messages[0])) and hasattr(messages[0], 'content'):
                from langchain_core.messages import SystemMessage
                if isinstance(messages[0], SystemMessage):
                    messages[0].content = f"{messages[0].content}\n\n{rag_prompt}"
                else:
                    messages.insert(0, SystemMessage(content=rag_prompt))
            else:
                from langchain_core.messages import SystemMessage
                messages.insert(0, SystemMessage(content=rag_prompt))
        
        # æ·»åŠ å½“å‰ç”¨æˆ·æ¶ˆæ¯
        from langchain_core.messages import HumanMessage
        messages.append(HumanMessage(content=query))

        # è°ƒç”¨ LLMï¼ˆå¦‚æœæœ‰å†å²æ¶ˆæ¯ï¼Œä½¿ç”¨æ¶ˆæ¯åˆ—è¡¨ï¼›å¦åˆ™ä½¿ç”¨å­—ç¬¦ä¸²ï¼‰
        if len(messages) > 1:
            return self.llm.invoke(messages)
        else:
            # æ²¡æœ‰å†å²ï¼Œä½¿ç”¨ç®€å•å­—ç¬¦ä¸²æ ¼å¼
            prompt = f"""åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ã€‚å¦‚æœä¸Šä¸‹æ–‡ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·ç›´æ¥è¯´ä¸çŸ¥é“ã€‚

ä¸Šä¸‹æ–‡:
{context}

é—®é¢˜: {query}

å›ç­”:"""
            return self.llm.invoke(prompt)

    def _chat_with_rag_stream(self, query: str, history: Optional[List[dict]] = None) -> Iterator[str]:
        """
        RAG æ¨¡å¼å¯¹è¯ï¼ˆæµå¼ï¼‰
        """
        if not self.rag_engine:
            yield from self._chat_with_llm_stream(query, history=history)
            return

        # æ£€ç´¢ç›¸å…³æ–‡æ¡£
        context = self.rag_engine.search(query, top_k=5)

        # æ„å»ºæ¶ˆæ¯åˆ—è¡¨
        messages = self._convert_history_to_messages(history)
        
        if context:
            # æ·»åŠ  RAG ä¸Šä¸‹æ–‡åˆ°ç³»ç»Ÿæç¤ºè¯
            rag_prompt = f"""åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ã€‚å¦‚æœä¸Šä¸‹æ–‡ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·ç›´æ¥è¯´ä¸çŸ¥é“ã€‚

ä¸Šä¸‹æ–‡:
{context}"""
            
            # æ›´æ–°ç³»ç»Ÿæ¶ˆæ¯æˆ–æ·»åŠ æ–°çš„ç³»ç»Ÿæ¶ˆæ¯
            if messages and isinstance(messages[0], type(messages[0])) and hasattr(messages[0], 'content'):
                from langchain_core.messages import SystemMessage
                if isinstance(messages[0], SystemMessage):
                    messages[0].content = f"{messages[0].content}\n\n{rag_prompt}"
                else:
                    messages.insert(0, SystemMessage(content=rag_prompt))
            else:
                from langchain_core.messages import SystemMessage
                messages.insert(0, SystemMessage(content=rag_prompt))
        
        # æ·»åŠ å½“å‰ç”¨æˆ·æ¶ˆæ¯
        from langchain_core.messages import HumanMessage
        messages.append(HumanMessage(content=query))

        # æµå¼è°ƒç”¨ LLMï¼ˆå¦‚æœæœ‰å†å²æ¶ˆæ¯ï¼Œä½¿ç”¨æ¶ˆæ¯åˆ—è¡¨ï¼›å¦åˆ™ä½¿ç”¨å­—ç¬¦ä¸²ï¼‰
        if len(messages) > 1:
            yield from self.llm.stream(messages)
        else:
            # æ²¡æœ‰å†å²ï¼Œä½¿ç”¨ç®€å•å­—ç¬¦ä¸²æ ¼å¼
            prompt = f"""åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ã€‚å¦‚æœä¸Šä¸‹æ–‡ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·ç›´æ¥è¯´ä¸çŸ¥é“ã€‚

ä¸Šä¸‹æ–‡:
{context}

é—®é¢˜: {query}

å›ç­”:"""
            yield from self.llm.stream(prompt)

    def _chat_with_llm(self, query: str, history: Optional[List[dict]] = None) -> str:
        """
        çº¯ LLM æ¨¡å¼å¯¹è¯ï¼ˆéæµå¼ï¼‰
        """
        # æ„å»ºæ¶ˆæ¯åˆ—è¡¨
        messages = self._convert_history_to_messages(history)
        
        # æ·»åŠ å½“å‰ç”¨æˆ·æ¶ˆæ¯
        from langchain_core.messages import HumanMessage
        messages.append(HumanMessage(content=query))
        
        # å¦‚æœæœ‰å†å²æ¶ˆæ¯ï¼Œä½¿ç”¨æ¶ˆæ¯åˆ—è¡¨ï¼›å¦åˆ™ä½¿ç”¨ç®€å•å­—ç¬¦ä¸²æ ¼å¼
        if len(messages) > 1:
            return self.llm.invoke(messages)
        else:
            # æ²¡æœ‰å†å²ï¼Œä½¿ç”¨ç®€å•å­—ç¬¦ä¸²æ ¼å¼
            if self.system_prompt:
                prompt = f"{self.system_prompt}\n\nç”¨æˆ·: {query}"
            else:
                prompt = query
            return self.llm.invoke(prompt)

    def _chat_with_llm_stream(self, query: str, history: Optional[List[dict]] = None) -> Iterator[str]:
        """
        çº¯ LLM æ¨¡å¼å¯¹è¯ï¼ˆæµå¼ï¼‰
        """
        # æ„å»ºæ¶ˆæ¯åˆ—è¡¨
        messages = self._convert_history_to_messages(history)
        
        # æ·»åŠ å½“å‰ç”¨æˆ·æ¶ˆæ¯
        from langchain_core.messages import HumanMessage
        messages.append(HumanMessage(content=query))
        
        # å¦‚æœæœ‰å†å²æ¶ˆæ¯ï¼Œä½¿ç”¨æ¶ˆæ¯åˆ—è¡¨ï¼›å¦åˆ™ä½¿ç”¨ç®€å•å­—ç¬¦ä¸²æ ¼å¼
        if len(messages) > 1:
            yield from self.llm.stream(messages)
        else:
            # æ²¡æœ‰å†å²ï¼Œä½¿ç”¨ç®€å•å­—ç¬¦ä¸²æ ¼å¼
            if self.system_prompt:
                prompt = f"{self.system_prompt}\n\nç”¨æˆ·: {query}"
            else:
                prompt = query
            yield from self.llm.stream(prompt)

    def _chat_with_tools(self, query: str, use_rag: bool = True, history: Optional[List[dict]] = None) -> str:
        """
        ä½¿ç”¨å·¥å…·çš„å¯¹è¯æ¨¡å¼ï¼ˆéæµå¼ï¼‰
        
        æ”¯æŒä¸¤ç§æ–¹å¼ï¼š
        1. ç›´æ¥ Function Callingï¼ˆå¦‚æœæ¨¡å‹æ”¯æŒï¼‰ï¼šä½¿ç”¨ bind_tools
        2. Agent æ¨¡å¼ï¼ˆfallbackï¼‰ï¼šä½¿ç”¨ create_agent API
        """
        from langchain_core.messages import HumanMessage, AIMessage, ToolMessage
        from langchain.agents import create_agent

        # è·å–å·¥å…·
        if not self.skill_manager:
            if use_rag and self.rag_engine:
                return self._chat_with_rag(query, history=history)
            else:
                return self._chat_with_llm(query, history=history)

        try:
            langchain_tools = self.skill_manager.get_tools()
        except Exception as e:
            print(f"âš ï¸  è·å–å·¥å…·å¤±è´¥: {str(e)}ï¼Œé€€å›æ™®é€šæ¨¡å¼")
            if use_rag and self.rag_engine:
                return self._chat_with_rag(query, history=history)
            else:
                return self._chat_with_llm(query, history=history)

        if not langchain_tools:
            if use_rag and self.rag_engine:
                return self._chat_with_rag(query, history=history)
            else:
                return self._chat_with_llm(query, history=history)

        # æ„å»ºç³»ç»Ÿæç¤ºè¯
        base_prompt = self.system_prompt or "ä½ æ˜¯ BitwiseAIï¼Œä¸“æ³¨äºç¡¬ä»¶æŒ‡ä»¤éªŒè¯å’Œè°ƒè¯•æ—¥å¿—åˆ†æçš„ AI åŠ©æ‰‹ã€‚"
        
        # å¦‚æœæœ‰ RAGï¼Œæ£€ç´¢ç›¸å…³æ–‡æ¡£
        context = ""
        if use_rag and self.rag_engine:
            context_docs = self.rag_engine.search(query, top_k=5)
            if context_docs:
                context = f"\n\nç›¸å…³æ–‡æ¡£ä¸Šä¸‹æ–‡:\n{context_docs}\n"
        
        system_prompt_text = base_prompt + context if context else base_prompt

        # å°è¯•ä½¿ç”¨ç›´æ¥ Function Callingï¼ˆå¦‚æœæ¨¡å‹æ”¯æŒï¼‰
        try:
            # ä½¿ç”¨ bind_tools ç»‘å®šå·¥å…·åˆ°æ¨¡å‹ï¼ˆåŸç”Ÿ Function Callingï¼‰
            if hasattr(self.llm.client, 'bind_tools'):
                print(f"ğŸ”§ ä½¿ç”¨åŸç”Ÿ Function Callingï¼Œæ¨¡å‹: {type(self.llm.client).__name__}")
                print(f"ğŸ”§ å¯ç”¨å·¥å…· ({len(langchain_tools)} ä¸ª):")
                for tool in langchain_tools:
                    print(f"   - {tool.name}: {tool.description[:50]}...")
                
                # ç»‘å®šå·¥å…·åˆ°æ¨¡å‹
                model_with_tools = self.llm.client.bind_tools(langchain_tools)
                
                # æ„å»ºæ¶ˆæ¯ï¼ˆåŒ…å«å†å²æ¶ˆæ¯ï¼‰
                messages = self._convert_history_to_messages(history)
                # æ›´æ–°ç³»ç»Ÿæç¤ºè¯
                if messages and isinstance(messages[0], type(messages[0])) and hasattr(messages[0], 'content'):
                    from langchain_core.messages import SystemMessage
                    if isinstance(messages[0], SystemMessage):
                        messages[0].content = system_prompt_text
                    else:
                        messages.insert(0, SystemMessage(content=system_prompt_text))
                else:
                    from langchain_core.messages import SystemMessage
                    messages.insert(0, SystemMessage(content=system_prompt_text))
                
                # æ·»åŠ å½“å‰ç”¨æˆ·æ¶ˆæ¯
                messages.append(HumanMessage(content=query))
                
                # è°ƒç”¨æ¨¡å‹
                response = model_with_tools.invoke(messages)
                
                # æ£€æŸ¥æ˜¯å¦æœ‰å·¥å…·è°ƒç”¨
                if hasattr(response, 'tool_calls') and response.tool_calls:
                    print(f"ğŸ”§ æ£€æµ‹åˆ° {len(response.tool_calls)} ä¸ªå·¥å…·è°ƒç”¨")
                    
                    # æ‰§è¡Œå·¥å…·è°ƒç”¨
                    tool_messages = []
                    for tool_call in response.tool_calls:
                        tool_name = tool_call.get('name', '')
                        tool_args = tool_call.get('args', {})
                        tool_id = tool_call.get('id', '')
                        
                        # æŸ¥æ‰¾å¯¹åº”çš„å·¥å…·
                        tool_func = None
                        for tool in langchain_tools:
                            if tool.name == tool_name:
                                tool_func = tool
                                break
                        
                        if tool_func:
                            try:
                                # æ‰§è¡Œå·¥å…·
                                tool_result = tool_func.invoke(tool_args)
                                tool_messages.append(
                                    ToolMessage(
                                        content=str(tool_result),
                                        tool_call_id=tool_id
                                    )
                                )
                                print(f"   âœ“ {tool_name}({tool_args}) = {tool_result}")
                            except Exception as e:
                                tool_messages.append(
                                    ToolMessage(
                                        content=f"å·¥å…·æ‰§è¡Œå¤±è´¥: {str(e)}",
                                        tool_call_id=tool_id
                                    )
                                )
                                print(f"   âŒ {tool_name} æ‰§è¡Œå¤±è´¥: {e}")
                        else:
                            tool_messages.append(
                                ToolMessage(
                                    content=f"å·¥å…·ä¸å­˜åœ¨: {tool_name}",
                                    tool_call_id=tool_id
                                )
                            )
                    
                    # å°†å·¥å…·ç»“æœæ·»åŠ åˆ°æ¶ˆæ¯å†å²ï¼Œå†æ¬¡è°ƒç”¨æ¨¡å‹
                    messages.append(response)
                    messages.extend(tool_messages)
                    
                    # è·å–æœ€ç»ˆå›ç­”
                    final_response = model_with_tools.invoke(messages)
                    return final_response.content
                else:
                    # æ²¡æœ‰å·¥å…·è°ƒç”¨ï¼Œç›´æ¥è¿”å›å›ç­”
                    return response.content
            else:
                # æ¨¡å‹ä¸æ”¯æŒ bind_toolsï¼Œä½¿ç”¨ Agent æ¨¡å¼
                raise AttributeError("æ¨¡å‹ä¸æ”¯æŒ bind_toolsï¼Œä½¿ç”¨ Agent æ¨¡å¼")
                
        except (AttributeError, Exception) as e:
            # Fallback: ä½¿ç”¨ Agent æ¨¡å¼
            print(f"âš ï¸  ç›´æ¥ Function Calling ä¸å¯ç”¨: {str(e)}")
            print(f"ğŸ”§ ä½¿ç”¨ Agent æ¨¡å¼ï¼Œæ¨¡å‹: {type(self.llm.client).__name__}")
            print(f"ğŸ”§ å¯ç”¨å·¥å…· ({len(langchain_tools)} ä¸ª):")
            for tool in langchain_tools:
                print(f"   - {tool.name}: {tool.description[:50]}...")
            
            try:
                # ä½¿ç”¨ create_agent APIï¼ˆLangChain 1.1.0+ï¼‰
                agent = create_agent(
                    model=self.llm.client,
                    tools=langchain_tools,
                    system_prompt=system_prompt_text
                )
                
                # æ„å»ºæ¶ˆæ¯ï¼ˆåŒ…å«å†å²æ¶ˆæ¯ï¼‰
                messages = self._convert_history_to_messages(history)
                # æ›´æ–°ç³»ç»Ÿæç¤ºè¯
                if messages and isinstance(messages[0], type(messages[0])) and hasattr(messages[0], 'content'):
                    from langchain_core.messages import SystemMessage
                    if isinstance(messages[0], SystemMessage):
                        messages[0].content = system_prompt_text
                    else:
                        messages.insert(0, SystemMessage(content=system_prompt_text))
                else:
                    from langchain_core.messages import SystemMessage
                    messages.insert(0, SystemMessage(content=system_prompt_text))
                
                # æ·»åŠ å½“å‰ç”¨æˆ·æ¶ˆæ¯
                messages.append(HumanMessage(content=query))
                
                # è°ƒç”¨ agent
                result = agent.invoke({"messages": messages})
                
                # ä»ç»“æœä¸­æå–æœ€åä¸€æ¡æ¶ˆæ¯çš„å†…å®¹
                messages = result.get("messages", [])
                if messages:
                    ai_messages = [m for m in messages if isinstance(m, AIMessage)]
                    if ai_messages:
                        return ai_messages[-1].content
                    return str(messages[-1].content) if hasattr(messages[-1], 'content') else str(messages[-1])
                
                return str(result)
            except Exception as agent_error:
                print(f"âš ï¸  Agent æ‰§è¡Œå¤±è´¥: {str(agent_error)}ï¼Œé€€å›æ™®é€šæ¨¡å¼")
                if use_rag and self.rag_engine:
                    return self._chat_with_rag(query, history=history)
                else:
                    return self._chat_with_llm(query, history=history)

    def _chat_with_tools_stream(self, query: str, use_rag: bool = True, history: Optional[List[dict]] = None) -> Iterator[str]:
        """
        ä½¿ç”¨å·¥å…·çš„å¯¹è¯æ¨¡å¼ï¼ˆæµå¼ï¼‰
        
        å®ç°çœŸæ­£çš„æµå¼ä¼ è¾“ï¼š
        1. å¦‚æœæ¨¡å‹æ”¯æŒ bind_toolsï¼Œå·¥å…·è°ƒç”¨åæµå¼è·å–æœ€ç»ˆå›ç­”
        2. å¯¹äº Agent æ¨¡å¼ï¼Œä½¿ç”¨æµå¼ Agent æ‰§è¡Œå™¨
        """
        from langchain_core.messages import HumanMessage, AIMessage, ToolMessage, SystemMessage
        from langchain.agents import create_agent

        # è·å–å·¥å…·
        if not self.skill_manager:
            if use_rag and self.rag_engine:
                yield from self._chat_with_rag_stream(query, history=history)
            else:
                yield from self._chat_with_llm_stream(query, history=history)
            return

        try:
            langchain_tools = self.skill_manager.get_tools()
        except Exception as e:
            print(f"âš ï¸  è·å–å·¥å…·å¤±è´¥: {str(e)}ï¼Œé€€å›æ™®é€šæ¨¡å¼")
            if use_rag and self.rag_engine:
                yield from self._chat_with_rag_stream(query, history=history)
            else:
                yield from self._chat_with_llm_stream(query, history=history)
            return

        if not langchain_tools:
            if use_rag and self.rag_engine:
                yield from self._chat_with_rag_stream(query, history=history)
            else:
                yield from self._chat_with_llm_stream(query, history=history)
            return

        # æ„å»ºç³»ç»Ÿæç¤ºè¯
        base_prompt = self.system_prompt or "ä½ æ˜¯ BitwiseAIï¼Œä¸“æ³¨äºç¡¬ä»¶æŒ‡ä»¤éªŒè¯å’Œè°ƒè¯•æ—¥å¿—åˆ†æçš„ AI åŠ©æ‰‹ã€‚"
        
        # å¦‚æœæœ‰ RAGï¼Œæ£€ç´¢ç›¸å…³æ–‡æ¡£
        context = ""
        if use_rag and self.rag_engine:
            context_docs = self.rag_engine.search(query, top_k=5)
            if context_docs:
                context = f"\n\nç›¸å…³æ–‡æ¡£ä¸Šä¸‹æ–‡:\n{context_docs}\n"
        
        system_prompt_text = base_prompt + context if context else base_prompt

        # å°è¯•ä½¿ç”¨ç›´æ¥ Function Callingï¼ˆå¦‚æœæ¨¡å‹æ”¯æŒï¼‰
        try:
            # ä½¿ç”¨ bind_tools ç»‘å®šå·¥å…·åˆ°æ¨¡å‹ï¼ˆåŸç”Ÿ Function Callingï¼‰
            if hasattr(self.llm.client, 'bind_tools'):
                print(f"ğŸ”§ ä½¿ç”¨åŸç”Ÿ Function Callingï¼ˆæµå¼ï¼‰ï¼Œæ¨¡å‹: {type(self.llm.client).__name__}")
                print(f"ğŸ”§ å¯ç”¨å·¥å…· ({len(langchain_tools)} ä¸ª):")
                for tool in langchain_tools:
                    print(f"   - {tool.name}: {tool.description[:50]}...")
                
                # ç»‘å®šå·¥å…·åˆ°æ¨¡å‹
                model_with_tools = self.llm.client.bind_tools(langchain_tools)
                
                # æ„å»ºæ¶ˆæ¯ï¼ˆåŒ…å«å†å²æ¶ˆæ¯ï¼‰
                messages = self._convert_history_to_messages(history)
                # æ›´æ–°ç³»ç»Ÿæç¤ºè¯
                if messages and isinstance(messages[0], type(messages[0])) and hasattr(messages[0], 'content'):
                    from langchain_core.messages import SystemMessage
                    if isinstance(messages[0], SystemMessage):
                        messages[0].content = system_prompt_text
                    else:
                        messages.insert(0, SystemMessage(content=system_prompt_text))
                else:
                    from langchain_core.messages import SystemMessage
                    messages.insert(0, SystemMessage(content=system_prompt_text))
                
                # æ·»åŠ å½“å‰ç”¨æˆ·æ¶ˆæ¯
                messages.append(HumanMessage(content=query))
                
                # è°ƒç”¨æ¨¡å‹ï¼ˆéæµå¼ï¼Œè·å–å·¥å…·è°ƒç”¨ï¼‰
                response = model_with_tools.invoke(messages)
                
                # æ£€æŸ¥æ˜¯å¦æœ‰å·¥å…·è°ƒç”¨
                if hasattr(response, 'tool_calls') and response.tool_calls:
                    print(f"ğŸ”§ æ£€æµ‹åˆ° {len(response.tool_calls)} ä¸ªå·¥å…·è°ƒç”¨")
                    
                    # æ‰§è¡Œå·¥å…·è°ƒç”¨
                    tool_messages = []
                    for tool_call in response.tool_calls:
                        tool_name = tool_call.get('name', '')
                        tool_args = tool_call.get('args', {})
                        tool_id = tool_call.get('id', '')
                        
                        # æŸ¥æ‰¾å¯¹åº”çš„å·¥å…·
                        tool_func = None
                        for tool in langchain_tools:
                            if tool.name == tool_name:
                                tool_func = tool
                                break
                        
                        if tool_func:
                            try:
                                # æ‰§è¡Œå·¥å…·
                                tool_result = tool_func.invoke(tool_args)
                                tool_messages.append(
                                    ToolMessage(
                                        content=str(tool_result),
                                        tool_call_id=tool_id
                                    )
                                )
                                print(f"   âœ“ {tool_name}({tool_args}) = {tool_result}")
                            except Exception as e:
                                tool_messages.append(
                                    ToolMessage(
                                        content=f"å·¥å…·æ‰§è¡Œå¤±è´¥: {str(e)}",
                                        tool_call_id=tool_id
                                    )
                                )
                                print(f"   âŒ {tool_name} æ‰§è¡Œå¤±è´¥: {e}")
                        else:
                            tool_messages.append(
                                ToolMessage(
                                    content=f"å·¥å…·ä¸å­˜åœ¨: {tool_name}",
                                    tool_call_id=tool_id
                                )
                            )
                    
                    # å°†å·¥å…·ç»“æœæ·»åŠ åˆ°æ¶ˆæ¯å†å²
                    messages.append(response)
                    messages.extend(tool_messages)
                    
                    # æµå¼è·å–æœ€ç»ˆå›ç­”
                    print(f"ğŸ”§ æµå¼è·å–æœ€ç»ˆå›ç­”...")
                    for chunk in model_with_tools.stream(messages):
                        if hasattr(chunk, 'content') and chunk.content:
                            yield chunk.content
                        elif isinstance(chunk, str):
                            yield chunk
                else:
                    # æ²¡æœ‰å·¥å…·è°ƒç”¨ï¼Œæµå¼è¿”å›å›ç­”
                    for chunk in model_with_tools.stream(messages):
                        if hasattr(chunk, 'content') and chunk.content:
                            yield chunk.content
                        elif isinstance(chunk, str):
                            yield chunk
            else:
                # æ¨¡å‹ä¸æ”¯æŒ bind_toolsï¼Œä½¿ç”¨ Agent æ¨¡å¼
                raise AttributeError("æ¨¡å‹ä¸æ”¯æŒ bind_toolsï¼Œä½¿ç”¨ Agent æ¨¡å¼")
                
        except (AttributeError, Exception) as e:
            # Fallback: ä½¿ç”¨ Agent æ¨¡å¼
            # æ³¨æ„ï¼šLangChain Agent çš„æµå¼è¾“å‡ºæ¯”è¾ƒå¤æ‚ï¼Œè¿™é‡Œå…ˆè·å–å®Œæ•´å›ç­”ï¼Œç„¶åæµå¼è¾“å‡º
            print(f"âš ï¸  ç›´æ¥ Function Calling ä¸å¯ç”¨: {str(e)}")
            print(f"ğŸ”§ ä½¿ç”¨ Agent æ¨¡å¼ï¼Œæ¨¡å‹: {type(self.llm.client).__name__}")
            
            try:
                # ä½¿ç”¨ create_agent API
                agent = create_agent(
                    model=self.llm.client,
                    tools=langchain_tools,
                    system_prompt=system_prompt_text
                )
                
                # æ„å»ºæ¶ˆæ¯ï¼ˆåŒ…å«å†å²æ¶ˆæ¯ï¼‰
                messages = self._convert_history_to_messages(history)
                # æ›´æ–°ç³»ç»Ÿæç¤ºè¯
                if messages and isinstance(messages[0], type(messages[0])) and hasattr(messages[0], 'content'):
                    from langchain_core.messages import SystemMessage
                    if isinstance(messages[0], SystemMessage):
                        messages[0].content = system_prompt_text
                    else:
                        messages.insert(0, SystemMessage(content=system_prompt_text))
                else:
                    from langchain_core.messages import SystemMessage
                    messages.insert(0, SystemMessage(content=system_prompt_text))
                
                # æ·»åŠ å½“å‰ç”¨æˆ·æ¶ˆæ¯
                messages.append(HumanMessage(content=query))
                
                # æ‰§è¡Œ Agentï¼ˆéæµå¼ï¼‰
                result = agent.invoke({"messages": messages})
                messages = result.get("messages", [])
                ai_messages = [m for m in messages if isinstance(m, AIMessage)]
                
                if ai_messages:
                    # è·å–æœ€ç»ˆå›ç­”å†…å®¹
                    content = ai_messages[-1].content
                    # æµå¼è¾“å‡ºï¼ˆé€å­—ç¬¦ï¼Œè‡³å°‘æä¾›æµå¼ä½“éªŒï¼‰
                    # æ³¨æ„ï¼šè¿™æ˜¯ç®€åŒ–å®ç°ï¼ŒçœŸæ­£çš„ Agent æµå¼éœ€è¦æ›´å¤æ‚çš„å¤„ç†
                    for char in content:
                        yield char
                else:
                    # å¦‚æœæ²¡æœ‰ AI æ¶ˆæ¯ï¼Œè¾“å‡ºæ•´ä¸ªç»“æœ
                    yield str(result)
                        
            except Exception as agent_error:
                print(f"âš ï¸  Agent æ‰§è¡Œå¤±è´¥: {str(agent_error)}ï¼Œé€€å›æ™®é€šæ¨¡å¼")
                if use_rag and self.rag_engine:
                    yield from self._chat_with_rag_stream(query, history=history)
                else:
                    yield from self._chat_with_llm_stream(query, history=history)


__all__ = ["ChatEngine"]

