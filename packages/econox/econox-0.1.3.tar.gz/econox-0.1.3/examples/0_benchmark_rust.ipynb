{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zAzbPjmFkQyU"
      },
      "source": [
        "# üìì Econox Benchmark: Accuracy & Scalability\n",
        "\n",
        "This notebook demonstrates the capabilities of **Econox** framework.\n",
        "We validate its accuracy by replicating the classic Rust (1987) model and demonstrate its overwhelming speed advantage through automatic differentiation and GPU acceleration.\n",
        "\n",
        "Êú¨„Éé„Éº„Éà„Éñ„ÉÉ„ÇØ„Åß„ÅØ„ÄÅ**Econox** „Éï„É¨„Éº„É†„ÉØ„Éº„ÇØ„ÅÆÊÄßËÉΩ„ÇíÂÆüË®º„Åó„Åæ„Åô„ÄÇ\n",
        "Rust (1987) „É¢„Éá„É´„ÅÆÂÜçÁèæ„Å´„Çà„Çã„ÄåÊ≠£Á¢∫ÊÄß„Äç„ÅÆÊ§úË®º„Å®„ÄÅËá™ÂãïÂæÆÂàÜ„ÉªGPUÊ¥ªÁî®„Å´„Çà„Çã„ÄåÂúßÂÄíÁöÑ„Å™Ë®àÁÆóÈÄüÂ∫¶„Äç„Çí„Éô„É≥„ÉÅ„Éû„Éº„ÇØ„Åó„Åæ„Åô„ÄÇ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "55JpY7FLkQyX",
        "outputId": "a7bc7dfd-3496-4126-ef33-a4ad4094ff0d"
      },
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# 1. Setup & Installation\n",
        "# ==========================================\n",
        "%pip install pandas matplotlib seaborn scipy requests econox\n",
        "\n",
        "# Ensure JAX and jaxlib are compatible with CUDA environment\n",
        "%pip install --upgrade \"jax[cuda]\" jaxlib\n",
        "\n",
        "print(\"\\n*** IMPORTANT: If you encounter JAX/CUDA related errors after this cell, please restart the Colab runtime (Runtime -> Restart runtime...) to ensure all updated packages are loaded correctly. ***\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L1MtJp8vkQyY",
        "outputId": "74ddbad4-187c-4f70-d145-e1d3dcb30f0d"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import econox as ecx\n",
        "import optimistix as optx\n",
        "# Enable Float64 for precision\n",
        "jax.config.update(\"jax_enable_x64\", True)\n",
        "\n",
        "# Plot settings\n",
        "sns.set_theme(style=\"whitegrid\")\n",
        "print(f\"JAX Backend: {jax.devices()[0]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2KbkZqpDkQyZ"
      },
      "source": [
        "## 2. Part 1: Replication of Rust (1987)\n",
        "\n",
        "We verify Econox's accuracy by replicating the results of John Rust's classic paper (1987) on bus engine replacement. We estimate the structural parameters $(\\theta, RC)$ using the original \"Group 4\" dataset.\n",
        "\n",
        "**Ê§úË®º:** Rust (1987) „ÅÆ„Éê„Çπ„Ç®„É≥„Ç∏„É≥‰∫§Êèõ„É¢„Éá„É´„ÇíÂÜçÁèæ„Åó„ÄÅEconox„ÅÆÊé®ÂÆöÁ≤æÂ∫¶„ÇíÊ§úË®º„Åó„Åæ„Åô„ÄÇ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FBWGT7uXkQya",
        "outputId": "04647b03-34be-479e-eb09-8ebfc91fe50f"
      },
      "outputs": [],
      "source": [
        "# --- 1. Load Data (Rust 1987 Group 4) ---\n",
        "# Original dataset from the ruspy project (MIT License)\n",
        "# https://github.com/OpenSourceEconomics/ruspy\n",
        "url = \"https://raw.githubusercontent.com/ito-haru/econox/main/examples/data/rust1987_group4.csv\"\n",
        "df_rust = pd.read_csv(url)\n",
        "\n",
        "# Process data into Econox format\n",
        "states = df_rust[\"state\"].values.astype(int)\n",
        "choices = df_rust[\"decision\"].values.astype(int)\n",
        "num_obs = len(states)\n",
        "num_states = 90  # Standard Rust grid size\n",
        "\n",
        "# --- 2. Define Model Components ---\n",
        "p = np.array([0.3919, 0.5953, 0.0128])\n",
        "\n",
        "# Transition Matrix: (S*A, S)\n",
        "T = np.zeros((num_states * 2, num_states))\n",
        "for s in range(num_states):\n",
        "    # Action 0: Keep\n",
        "    for k, prob in enumerate(p):\n",
        "        T[s * 2, min(s + k, num_states - 1)] += prob\n",
        "    # Action 1: Replace\n",
        "    T[s * 2 + 1, 0] = 1.0\n",
        "\n",
        "# Feature Matrix: (S, A, Params) -> Params = [theta, RC]\n",
        "F = np.zeros((num_states, 2, 2))\n",
        "# Action 0: Cost = -theta * s\n",
        "F[:, 0, 0] = -0.001 * np.arange(num_states)\n",
        "# Action 1: Cost = -RC\n",
        "F[:, 1, 1] = -1.0\n",
        "\n",
        "# --- 3. Run Econox Estimation ---\n",
        "# Convert to JAX arrays\n",
        "T_jax = jnp.array(T)\n",
        "F_jax = jnp.array(F)\n",
        "obs = {\n",
        "    \"state_indices\": jnp.array(states),\n",
        "    \"choice_indices\": jnp.array(choices),\n",
        "    \"weights\": jnp.ones(num_obs)\n",
        "}\n",
        "\n",
        "# Build Econox Estimator\n",
        "model = ecx.Model.from_data(num_states, 2, {\"feat\": F_jax}, transitions=T_jax)\n",
        "utility = ecx.LinearUtility((\"theta\", \"RC\"), \"feat\")\n",
        "solver = ecx.ValueIterationSolver(\n",
        "    utility=utility,\n",
        "    dist=ecx.GumbelDistribution(),\n",
        "    discount_factor=0.9999\n",
        "    )\n",
        "param_space = ecx.ParameterSpace.create(\n",
        "    initial_params={\"theta\": 2.0, \"RC\": 10.0},\n",
        "    constraints={\"theta\": \"free\", \"RC\": \"free\"}\n",
        ")\n",
        "\n",
        "estimator = ecx.Estimator(\n",
        "    model=model, param_space=param_space, solver=solver,\n",
        "    method=ecx.MaximumLikelihood(),\n",
        "    optimizer=ecx.optim.Minimizer(method=optx.BFGS(rtol=1e-6, atol=1e-6))\n",
        ")\n",
        "\n",
        "print(\"Starting Estimation...\")\n",
        "start = time.time()\n",
        "result = estimator.fit(obs, sample_size=num_obs)\n",
        "jax.block_until_ready(result.loss)\n",
        "end = time.time()\n",
        "\n",
        "# --- 4. Validation ---\n",
        "theta_est = float(result.params[\"theta\"])\n",
        "rc_est = float(result.params[\"RC\"])\n",
        "\n",
        "print(f\"\\n‚úÖ Estimation Complete in {end - start:.2f}s\")\n",
        "print(f\"{'Parameter':<10} | {'Rust (1987)':<15} | {'Econox':<15} | {'Diff':<10}\")\n",
        "print(\"-\" * 60)\n",
        "print(f\"{'Theta':<10} | {'2.293':<15} | {theta_est:<15.4f} | {abs(theta_est - 2.293):.4f}\")\n",
        "print(f\"{'RC':<10} | {'10.075':<15} | {rc_est:<15.4f} | {abs(rc_est - 10.075):.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WnkrO1T3kQyb"
      },
      "source": [
        "## 3. Part 2: Scalability Benchmark\n",
        "\n",
        "We demonstrate the scalability of Econox by comparing it with a standard NumPy implementation.\n",
        "The key advantage of Econox is **Auto-Differentiation**. While NumPy (Numerical Differentiation) requires solving the model $2 \\times N$ times per step (where $N$ is the number of parameters), Econox computes all gradients in a single pass.\n",
        "\n",
        "**ÈÄüÂ∫¶ÊØîËºÉ:** NumPyÔºàÊï∞ÂÄ§ÂæÆÂàÜÔºâ„Å®EconoxÔºàËá™ÂãïÂæÆÂàÜÔºâ„ÅÆÊé®ÂÆöÈÄüÂ∫¶„ÇíÊØîËºÉ„Åó„Åæ„Åô„ÄÇ\n",
        "„Éë„É©„É°„Éº„ÇøÊï∞„ÅåÂ¢ó„Åà„ÅüÈöõ„ÄÅEconox„ÅØË®àÁÆóÊôÇÈñì„Åå„Åª„Å®„Çì„Å©Â¢óÂä†„Åó„Å™„ÅÑ„Åì„Å®„ÇíÁ§∫„Åó„Åæ„Åô„ÄÇ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CNO0oST0kQyb",
        "outputId": "d1cac7e6-eabc-4542-a514-fc6ba0c3a565"
      },
      "outputs": [],
      "source": [
        "from econox.optim import FixedPoint\n",
        "# --- Benchmark Configuration ---\n",
        "S_BENCH = 3000       # Large State Space\n",
        "P_BENCH = 50         # Many Parameters (Heterogeneity)\n",
        "N_STEPS = 100        # Estimated steps to convergence\n",
        "\n",
        "print(f\"üöÄ Running Benchmark: S={S_BENCH}, Params={P_BENCH}\")\n",
        "print(f\"‚ÑπÔ∏è  Total time is estimated by extrapolating single-step performance to {N_STEPS} iterations.\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "# --- 1. Econox Implementation (Auto-Diff) ---\n",
        "def measure_econox(device_type, compute_num_steps:bool):\n",
        "    try: device = jax.devices(device_type)[0]\n",
        "    except: return np.nan\n",
        "\n",
        "    with jax.default_device(device):\n",
        "        # Setup dummy large model\n",
        "        key = jax.random.PRNGKey(0)\n",
        "        key_T, key_F = jax.random.split(key)\n",
        "\n",
        "        # Transition Matrix: (S*A, S) - ensure rows sum to 1 and are non-negative\n",
        "        # Generate random logits and apply softmax to ensure valid probabilities\n",
        "        T_logits = jax.random.uniform(key_T, (S_BENCH * 2, S_BENCH), minval=0.0, maxval=1.0)\n",
        "        T = jax.nn.softmax(T_logits, axis=-1)\n",
        "\n",
        "        # Feature Matrix: (S, A, Params) - ensure values are within a reasonable negative range for costs\n",
        "        F = jax.random.uniform(key_F, (S_BENCH, 2, P_BENCH), minval=-10.0, maxval=-0.1)\n",
        "\n",
        "        model = ecx.Model.from_data(S_BENCH, 2, {\"f\": F}, transitions=T)\n",
        "        util = ecx.LinearUtility(tuple(f\"p{i}\" for i in range(P_BENCH)), \"f\")\n",
        "        solver = ecx.ValueIterationSolver(\n",
        "            utility=util,\n",
        "            dist=ecx.GumbelDistribution(),\n",
        "            discount_factor=0.99,\n",
        "            numerical_solver=FixedPoint(max_steps=2000, method=optx.FixedPointIteration(rtol=1e-8,atol=1e-8))\n",
        "        )\n",
        "        params = {f\"p{i}\": jnp.array(1.0) for i in range(P_BENCH)}\n",
        "\n",
        "        # Get Steps to converge\n",
        "        if compute_num_steps:\n",
        "          result = solver.solve(params, model)\n",
        "          solver_steps = result.aux[\"num_steps\"]\n",
        "          print(\"   Steps to converge:\",solver_steps)\n",
        "        else:\n",
        "          solver_steps = np.nan\n",
        "\n",
        "        @jax.jit\n",
        "        def step_fn(p):\n",
        "            return jax.value_and_grad(\n",
        "                lambda _p: jnp.sum(solver.solve(_p, model).solution)\n",
        "            )(p)\n",
        "\n",
        "        # --- 1. Warmup (Compilation) Time ---\n",
        "        print(f\"   Running JIT Compilation on {device_type.upper()}...\", end=\"\")\n",
        "        start_compile = time.time()\n",
        "        out = step_fn(params)\n",
        "        jax.tree_util.tree_map(lambda x: x.block_until_ready(), out)\n",
        "        compile_time = time.time() - start_compile\n",
        "        print(f\" Done ({compile_time:.2f}s)\")\n",
        "\n",
        "        # --- 2. Execution Time ---\n",
        "        start_run = time.time()\n",
        "        out = step_fn(params)\n",
        "        jax.tree_util.tree_map(lambda x: x.block_until_ready(), out)\n",
        "        run_time_per_step = time.time() - start_run\n",
        "        total_estimation_time = run_time_per_step * N_STEPS\n",
        "\n",
        "        return compile_time, total_estimation_time, solver_steps\n",
        "\n",
        "t_compile_cpu, t_econox_cpu, solver_steps_cpu = measure_econox(\"cpu\", True)\n",
        "t_compile_gpu, t_econox_gpu, _ = measure_econox(\"gpu\", False)\n",
        "\n",
        "t_total_cpu = t_compile_cpu + t_econox_cpu\n",
        "t_total_gpu = t_compile_gpu + t_econox_gpu\n",
        "\n",
        "# --- 2. NumPy Implementation (No Auto-Diff) ---\n",
        "def measure_numpy_solver(S, P, steps, solver_steps):\n",
        "    \"\"\"\n",
        "    Simulates a full estimation step (Utility -> Bellman -> Likelihood) using NumPy.\n",
        "    \"\"\"\n",
        "    # 1. Setup Dummy Data\n",
        "    np.random.seed(42)\n",
        "\n",
        "    # Transition Matrix (Dense approximation): (S*A, S)\n",
        "    T = np.random.rand(S * 2, S)\n",
        "    T = T / T.sum(axis=1, keepdims=True)\n",
        "\n",
        "    # Feature Matrix: (S, 2, P)\n",
        "    F = np.random.rand(S, 2, P)\n",
        "\n",
        "    # Initial Params & Value Function\n",
        "    params = np.ones(P)\n",
        "    V = np.zeros(S)\n",
        "    discount = 0.99\n",
        "\n",
        "    # 2. Measurement\n",
        "    # Measure time for: Utility calc -> Fixed Point Iteration -> Log-Likelihood\n",
        "    start = time.time()\n",
        "\n",
        "    # A. Utility Calculation: (S, 2)\n",
        "    U = np.dot(F, params)\n",
        "\n",
        "    # B. Bellman Operator (Run 10 iters to average)\n",
        "    for _ in range(10):\n",
        "        # Expected Value: (S*2,) -> (S, 2)\n",
        "        EV = np.dot(T, V).reshape(S, 2)\n",
        "\n",
        "        # Choice Specific Value\n",
        "        v_choice = U + discount * EV\n",
        "\n",
        "        # LogSumExp (Stable implementation with max subtraction)\n",
        "        v_max = np.max(v_choice, axis=1, keepdims=True)\n",
        "        V = v_max.flatten() + np.log(np.sum(np.exp(v_choice - v_max), axis=1))\n",
        "\n",
        "    # C. Choice Probabilities (Softmax)\n",
        "    # P(a|s) = exp(v(s,a) - V(s))\n",
        "    P_choice = np.exp(v_choice - V[:, None])\n",
        "\n",
        "    # D. Log-Likelihood\n",
        "    ll = np.sum(np.log(P_choice + 1e-10))\n",
        "\n",
        "    end = time.time()\n",
        "\n",
        "    # 3. Total Time Estimation\n",
        "    # Formula: (Time per FP Iter) * (Total FP Iters) * (Numerical Diff Cost) * (Estimation Steps)\n",
        "    # Assumptions: the same FP iters to converge as Econox Solver, 2*P evals for gradient (central diff), 100 estimation steps.\n",
        "    time_per_iter = (end - start) / 10.0\n",
        "    estimated_total_time = time_per_iter * solver_steps * (2 * P) * steps\n",
        "\n",
        "    return estimated_total_time\n",
        "\n",
        "t_numpy = measure_numpy_solver(S_BENCH, P_BENCH, N_STEPS, solver_steps_cpu)\n",
        "\n",
        "# --- 3. Results ---\n",
        "def format_time(s):\n",
        "    if np.isnan(s): return \"N/A\"\n",
        "    if s > 86400: return f\"{s/86400:.1f} days\"\n",
        "    if s > 3600: return f\"{s/3600:.1f} hours\"\n",
        "    if s > 60: return f\"{s/60:.1f} mins\"\n",
        "    return f\"{s:.1f} sec\"\n",
        "\n",
        "def format_res(t_compile, t_total):\n",
        "    if np.isnan(t_compile) or np.isnan(t_total): return \"N/A\"\n",
        "    return f\"{format_time(t_total)}(Incl.{format_time(t_compile)} compile)\"\n",
        "\n",
        "print(f\"\\nüèÜ Estimated Total Estimation Time (100 Steps)\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"NumPy (CPU)      : {format_time(t_numpy)}\")\n",
        "print(f\"Econox (CPU)     : {format_res(t_compile_cpu,t_total_cpu)}\")\n",
        "print(f\"Econox (GPU)     : {format_res(t_compile_gpu,t_total_gpu)}\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "# Speedup Calculation\n",
        "speedup_cpu = t_numpy / t_total_cpu\n",
        "print(f\"‚ö° Speedup (vs NumPy): {speedup_cpu:.1f}x (CPU) | \", end=\"\")\n",
        "if not np.isnan(t_total_gpu):\n",
        "    print(f\"{t_numpy / t_total_gpu:.1f}x (GPU)\")\n",
        "else:\n",
        "    print(\"GPU N/A\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nw0r3aBDkQyc"
      },
      "source": [
        "## 4. Visualization / ÁµêÊûú„ÅÆÂèØË¶ñÂåñ\n",
        "\n",
        "Visualizing the impact of Auto-Differentiation and Hardware Acceleration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 520
        },
        "id": "yz3-hkIxkQyc",
        "outputId": "93ec7bc6-9f08-447c-e269-bf2ffd560de4"
      },
      "outputs": [],
      "source": [
        "data = {\n",
        "    \"Method\": [\"NumPy (Numerical Diff)\", \"Econox (CPU / Auto-Diff)\", \"Econox (GPU / Auto-Diff)\"],\n",
        "    \"Time (Seconds)\": [float(t_numpy), float(t_total_cpu), float(t_total_gpu) if not np.isnan(t_econox_gpu) else 0.0],\n",
        "    \"Color\": [\"gray\", \"#ff7f0e\", \"#d62728\"]\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "if np.isnan(t_econox_gpu): df = df[df[\"Method\"] != \"Econox (GPU / Auto-Diff)\"]\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.barplot(data=df, x=\"Time (Seconds)\", y=\"Method\", hue=\"Method\", palette=df[\"Color\"].tolist(), legend=False)\n",
        "plt.xscale(\"log\")\n",
        "plt.title(f\"Estimation Time Comparison (Log Scale)\\nS={S_BENCH}, Params={P_BENCH}\")\n",
        "plt.xlabel(\"Total Time (Seconds) - Lower is Better\")\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Econox",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
