"""Job management for async skill creation and HITL tracking."""

from __future__ import annotations

import asyncio
import json
import logging
import uuid
from datetime import UTC, datetime
from pathlib import Path
from typing import Any

# Local imports
from ..common.security import resolve_path_within_root

# Import Pydantic models from schemas per FastAPI best practices
from .schemas import DeepUnderstandingState, JobState, TDDWorkflowState

logger = logging.getLogger(__name__)


def _sanitize_for_log(value: Any) -> str:
    """Remove newline characters from values before logging to prevent log injection."""
    text = str(value)
    return text.replace("\r", "").replace("\n", "")


def _is_safe_job_id(job_id: str) -> bool:
    """Return True if the job_id is safe to use as a filename component.

    This restricts job IDs to a conservative character set to prevent
    path traversal or injection when constructing paths like
    SESSION_DIR / f"{job_id}.json".
    """
    if not job_id:
        return False
    # Allow only ASCII letters, digits, dash and underscore.
    # UUIDs generated by create_job() fit within this set.
    for ch in job_id:
        if not (ch.isalnum() or ch in "-_"):
            return False
    return True


# In-memory job store (use Redis in production)
JOBS: dict[str, JobState] = {}
_HITL_RESPONSE_WAITERS: dict[str, asyncio.Future[dict[str, Any]]] = {}


def create_job() -> str:
    """Create a new job and return its unique ID."""
    job_id = str(uuid.uuid4())
    JOBS[job_id] = JobState(job_id=job_id)
    return job_id


def get_job(job_id: str) -> JobState | None:
    """Retrieve a job by its ID, loading from disk if necessary."""
    job = JOBS.get(job_id)
    if job:
        return job

    # Fallback to disk loading for persistence
    return load_job_session(job_id)


async def wait_for_hitl_response(job_id: str, timeout: float = 3600.0) -> dict[str, Any]:
    """Wait for user to provide HITL response via API.

    This uses an event-driven waiter instead of polling to avoid:
    - unnecessary latency (poll interval)
    - duplicate prompts in clients that poll immediately after POSTing a response

    Falls back to the legacy in-memory `job.hitl_response` field if a response
    was already set before the waiter was registered (rare, but safe).
    """
    job = JOBS[job_id]

    # Fast-path: response already present (legacy behavior / rare races).
    if job.hitl_response is not None:
        response = job.hitl_response
        job.hitl_response = None
        job.updated_at = datetime.now(UTC)
        return response

    loop = asyncio.get_running_loop()
    waiter: asyncio.Future[dict[str, Any]] = loop.create_future()
    _HITL_RESPONSE_WAITERS[job_id] = waiter

    try:
        response = await asyncio.wait_for(waiter, timeout=timeout)
    except TimeoutError as exc:
        raise TimeoutError("HITL response timed out") from exc
    finally:
        # Only clear if we're still the active waiter for this job.
        if _HITL_RESPONSE_WAITERS.get(job_id) is waiter:
            _HITL_RESPONSE_WAITERS.pop(job_id, None)

    # Clear for next interaction and update timestamp when response received.
    job.hitl_response = None
    job.updated_at = datetime.now(UTC)
    return response


def notify_hitl_response(job_id: str, response: dict[str, Any]) -> None:
    """Notify the in-flight HITL waiter (if any) that a response arrived."""
    waiter = _HITL_RESPONSE_WAITERS.get(job_id)
    if waiter is None or waiter.done():
        return
    waiter.set_result(response)


# =============================================================================
# Session Persistence
# =============================================================================


# Session directory for persistence
SESSION_DIR = Path(".skill_fleet_sessions")
SESSION_DIR.mkdir(exist_ok=True)


def save_job_session(job_id: str) -> bool:
    """Save job state to disk for persistence.

    Args:
        job_id: The job ID to save

    Returns:
        True if save succeeded, False otherwise
    """
    if not _is_safe_job_id(job_id):
        logger.warning("Cannot save session: unsafe job id %s", _sanitize_for_log(job_id))
        return False

    job = JOBS.get(job_id)
    if not job:
        logger.warning("Cannot save session: job %s not found", _sanitize_for_log(job_id))
        return False

    try:
        session_file = resolve_path_within_root(SESSION_DIR, f"{job_id}.json")
        session_data = job.model_dump(mode="json", exclude_none=True)
        session_file.write_text(json.dumps(session_data, indent=2, default=str), encoding="utf-8")
        logger.debug("Saved session for job %s", _sanitize_for_log(job_id))
        return True
    except ValueError as e:
        logger.warning(
            "Cannot save session for job %s: %s",
            _sanitize_for_log(job_id),
            _sanitize_for_log(e),
        )
        return False
    except Exception as e:
        logger.error(
            "Failed to save session for job %s: %s",
            _sanitize_for_log(job_id),
            _sanitize_for_log(e),
        )
        return False


def load_job_session(job_id: str) -> JobState | None:
    """Load job state from disk.

    Args:
        job_id: The job ID to load

    Returns:
        JobState if found, None otherwise
    """
    if not _is_safe_job_id(job_id):
        logger.warning("Cannot load session: unsafe job id %s", _sanitize_for_log(job_id))
        return None

    try:
        session_file = resolve_path_within_root(SESSION_DIR, f"{job_id}.json")
        if not session_file.exists():
            return None

        session_data = json.loads(session_file.read_text(encoding="utf-8"))

        # Reconstruct JobState from saved data
        # Need to handle nested models properly
        tdd_data = session_data.pop("tdd_workflow", {})
        deep_data = session_data.pop("deep_understanding", {})

        job = JobState(**session_data)

        # Restore nested models
        if tdd_data:
            job.tdd_workflow = TDDWorkflowState(**tdd_data)
        if deep_data:
            job.deep_understanding = DeepUnderstandingState(**deep_data)

        JOBS[job_id] = job
        logger.info("Loaded session for job %s", _sanitize_for_log(job_id))
        return job

    except ValueError as e:
        logger.warning(
            "Cannot load session for job %s: %s",
            _sanitize_for_log(job_id),
            _sanitize_for_log(e),
        )
        return None

    except Exception as e:
        logger.error(
            "Failed to load session for job %s: %s",
            _sanitize_for_log(job_id),
            _sanitize_for_log(e),
        )
        return None


def list_saved_sessions() -> list[str]:
    """List all saved session IDs.

    Returns:
        List of job IDs with saved sessions
    """
    try:
        return [f.stem for f in SESSION_DIR.glob("*.json")]
    except Exception:
        return []


def delete_job_session(job_id: str) -> bool:
    """Delete a saved session.

    Args:
        job_id: The job ID to delete

    Returns:
        True if deletion succeeded, False otherwise
    """
    if not _is_safe_job_id(job_id):
        logger.warning("Cannot delete session: unsafe job id %s", _sanitize_for_log(job_id))
        return False

    try:
        session_file = resolve_path_within_root(SESSION_DIR, f"{job_id}.json")
        if session_file.exists():
            session_file.unlink()
            return True
        return False
    except ValueError as e:
        logger.warning(
            "Cannot delete session for job %s: %s",
            _sanitize_for_log(job_id),
            _sanitize_for_log(e),
        )
        return False
    except Exception as e:
        logger.error(
            "Failed to delete session for job %s: %s",
            _sanitize_for_log(job_id),
            _sanitize_for_log(e),
        )
        return False


def cleanup_old_sessions(max_age_hours: float = 24.0) -> int:
    """Clean up old session files.

    Args:
        max_age_hours: Maximum age in hours before deletion

    Returns:
        Number of sessions cleaned up
    """
    import time

    cleaned = 0
    cutoff_time = time.time() - (max_age_hours * 3600)

    try:
        for session_file in SESSION_DIR.glob("*.json"):
            if session_file.stat().st_mtime < cutoff_time:
                try:
                    session_file.unlink()
                    cleaned += 1
                except Exception:
                    # Ignore errors when deleting individual files
                    pass
    except Exception:
        # Ignore errors during cleanup (e.g., directory doesn't exist)
        pass

    if cleaned > 0:
        logger.info(f"Cleaned up {cleaned} old session(s)")

    return cleaned
