# config.yaml - Single source of truth for `skills-fleet` LLM configuration
#
# Default model: gemini/gemini-3-flash-preview (LiteLLM format for Gemini 3 Flash)
#
# Required env:
#   - GOOGLE_API_KEY
#   - LITELLM_API_KEY
#
# Optional env:
#   - DSPY_TEMPERATURE (override temperature globally)
#
# LiteLLM v1.80.8-stable.1+ required for Gemini 3 Flash features

models:
  default: gemini/gemini-3-flash-preview

  registry:
    gemini/gemini-3-flash-preview:
      model: gemini-3-flash-preview
      model_type: chat
      env: GOOGLE_API_KEY
      env_fallback: LITELLM_API_KEY
      timeout: 60
      parameters:
        temperature: 1.0
        max_tokens: 8192

    gemini/gemini-3-pro-preview:
      model: gemini-3-pro-preview
      model_type: chat
      env: GOOGLE_API_KEY
      env_fallback: LITELLM_API_KEY
      timeout: 120
      parameters:
        temperature: 1.0  # Gemini 3 models require temperature=1.0 per documentation
        max_tokens: 16384

roles:
  router:
    model: gemini/gemini-3-flash-preview
    description: "Routes requests and selects workflow patterns"
    capabilities: [pattern_detection, team_selection, complexity_assessment]
    parameter_overrides:
      reasoning_effort: high

  planner:
    model: gemini/gemini-3-flash-preview
    description: "Creates detailed plans and orchestrates skill selection"
    capabilities: [task_decomposition, skill_orchestration, dependency_analysis]
    parameter_overrides: {}

  worker:
    model: gemini/gemini-3-flash-preview
    description: "Executes work steps"
    capabilities: [file_operations, skill_execution, content_generation]
    parameter_overrides: {}

  judge:
    model: gemini/gemini-3-flash-preview
    description: "Evaluates results and validates completion quality"
    capabilities: [quality_assessment, validation, critique_generation]
    parameter_overrides: {}

tasks:
  # Skill Creation Workflow (6-step process)
  skill_understand:
    model: gemini/gemini-3-flash-preview
    role: planner
    parameters:
      reasoning_effort: medium

  skill_plan:
    model: gemini/gemini-3-flash-preview
    role: planner
    parameters:
      reasoning_effort: high

  skill_initialize:
    model: gemini/gemini-3-flash-preview
    role: worker
    parameters:
      reasoning_effort: high

  skill_edit:
    model: gemini/gemini-3-flash-preview
    role: worker
    parameters:
      reasoning_effort: high

  skill_package:
    model: gemini/gemini-3-flash-preview
    role: worker
    parameters:
      reasoning_effort: medium

  skill_validate:
    model: gemini/gemini-3-flash-preview
    role: judge
    parameters:
      reasoning_effort: high

  # Conversational Agent (Interactive CLI)
  conversational_agent:
    model: gemini/gemini-3-flash-preview
    role: planner
    parameters:
      reasoning_effort: high  # High thinking for complex reasoning in conversation

  # Evaluation & Optimization Tasks
  skill_evaluate:
    model: gemini/gemini-3-flash-preview
    role: judge
    parameters:
      temperature: 1.0  # Keep 1.0 for Gemini 3 models
      # Note: reasoning_effort removed - not supported by Gemini via LiteLLM

  skill_optimize:
    model: gemini/gemini-3-flash-preview  # Using flash for reliability
    role: judge
    parameters:
      temperature: 1.0  # Gemini 3 models require temperature=1.0
      # Note: reasoning_effort removed - not supported by Gemini via LiteLLM

# Optimization Configuration
optimization:
  # Default optimizer to use
  default_optimizer: miprov2

  # MIPROv2 optimizer settings
  miprov2:
    auto: "medium"  # Balance between optimization depth and cost (light/medium/heavy)
    num_threads: 4
    max_bootstrapped_demos: 4
    max_labeled_demos: 4
    verbose: true

  # BootstrapFewShot optimizer settings
  bootstrap_fewshot:
    max_bootstrapped_demos: 4
    max_labeled_demos: 4
    max_rounds: 1
    max_errors: 5

  # Training data paths
  training:
    gold_skills_path: "config/training/gold_skills.json"
    quality_criteria_path: "config/training/quality_criteria.yaml"
    optimized_programs_dir: "config/optimized"

# Evaluation Configuration
evaluation:
  # Number of threads for parallel evaluation
  num_threads: 8
  display_progress: true
  display_table: true

  # Quality thresholds
  thresholds:
    minimum_quality: 0.6
    target_quality: 0.8
    excellent_quality: 0.9

  # Metrics weights for overall score
  metric_weights:
    pattern_count: 0.15
    has_anti_patterns: 0.10
    has_key_insights: 0.10
    has_real_world_impact: 0.10
    has_quick_reference: 0.10
    has_common_mistakes: 0.10
    has_red_flags: 0.05
    frontmatter_completeness: 0.15
    code_examples_quality: 0.15
