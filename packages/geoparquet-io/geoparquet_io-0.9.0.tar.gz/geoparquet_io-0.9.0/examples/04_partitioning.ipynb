{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Partitioning GeoParquet Files\n\nThis notebook covers partitioning large datasets:\n- When to partition vs. sort\n- Partitioning by H3 cells\n- Partitioning by quadkey\n- Reading partitioned datasets\n\n**Note**: Partitioning requires datasets with at least 100+ rows per partition to be effective. The examples below show the API patterns - use them with your larger datasets."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geoparquet_io as gpio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When to Partition vs. Sort\n",
    "\n",
    "| Approach | File Structure | Best For |\n",
    "|----------|---------------|----------|\n",
    "| **Sorting only** | Single file | Datasets under 10GB, most queries |\n",
    "| **Partitioning** | Many files | Very large datasets (10GB+), known query patterns |\n",
    "\n",
    "Partitioning adds overhead (more files, metadata per file) but enables:\n",
    "- Query engines to skip entire files\n",
    "- Parallel reads from multiple files\n",
    "- Easier updates to specific regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sample data\n",
    "table = gpio.read(\"data/sample.parquet\")\n",
    "table.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partitioning by H3 Cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partition by H3 (requires larger datasets)\n",
    "# Uncomment to run with your data:\n",
    "\n",
    "# from pathlib import Path\n",
    "# import shutil\n",
    "#\n",
    "# output_dir = Path('/tmp/h3_partitions')\n",
    "# if output_dir.exists():\n",
    "#     shutil.rmtree(output_dir)\n",
    "#\n",
    "# stats = gpio.read('your_large_file.parquet') \\\n",
    "#     .add_h3(resolution=9) \\\n",
    "#     .partition_by_h3(str(output_dir), resolution=4)\n",
    "#\n",
    "# print(f\"Created {stats['file_count']} partition files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See what was created (after running partition above)\n",
    "# for f in sorted(output_dir.rglob('*.parquet'))[:5]:\n",
    "#     print(f.relative_to(output_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partitioning by Quadkey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partition by quadkey (requires larger datasets)\n",
    "# Uncomment to run with your data:\n",
    "\n",
    "# output_dir = Path('/tmp/quadkey_partitions')\n",
    "# if output_dir.exists():\n",
    "#     shutil.rmtree(output_dir)\n",
    "#\n",
    "# stats = gpio.read('your_large_file.parquet') \\\n",
    "#     .add_quadkey(resolution=12) \\\n",
    "#     .partition_by_quadkey(str(output_dir), partition_resolution=4)\n",
    "#\n",
    "# print(f\"Created {stats['file_count']} partition files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See what was created (after running partition above)\n",
    "# for f in sorted(output_dir.rglob('*.parquet'))[:5]:\n",
    "#     print(f.relative_to(output_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Partitioned Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all partitions back (after running partition above)\n",
    "# combined = gpio.read_partition('/tmp/h3_partitions/')\n",
    "# combined.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read with glob pattern\n",
    "# combined = gpio.read_partition('/tmp/h3_partitions/h3_cell=*/*.parquet')\n",
    "# print(f\"Read {combined.num_rows} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Practices\n",
    "\n",
    "1. **Sort before partitioning**: Always add bbox and sort within partitions\n",
    "2. **Choose appropriate resolution**: Aim for partition files of 50-200MB each\n",
    "3. **Use coarser resolution for partitioning**: The partition column can be at a different resolution than the index column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recommended workflow for large datasets\n",
    "# Uncomment to run with your data:\n",
    "\n",
    "# output_dir = Path('/tmp/optimized_partitions')\n",
    "# if output_dir.exists():\n",
    "#     shutil.rmtree(output_dir)\n",
    "#\n",
    "# # Add fine-grained H3 index, sort by Hilbert, partition at coarse level\n",
    "# stats = gpio.read('your_large_file.parquet') \\\n",
    "#     .add_bbox() \\\n",
    "#     .add_h3(resolution=9) \\\n",
    "#     .sort_hilbert() \\\n",
    "#     .partition_by_h3(str(output_dir), resolution=4)\n",
    "#\n",
    "# print(f\"Created {stats['file_count']} optimized partition files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up temporary directories (after running partitions)\n",
    "# for d in ['/tmp/h3_partitions', '/tmp/quadkey_partitions', '/tmp/optimized_partitions']:\n",
    "#     p = Path(d)\n",
    "#     if p.exists():\n",
    "#         shutil.rmtree(p)\n",
    "#         print(f\"Removed {d}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Next Steps\n\n- [05_cloud_workflows.ipynb](05_cloud_workflows.ipynb) - Cloud storage integration\n- [Partition Guide](https://geoparquet.io/guide/partition/) - CLI partitioning options"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
