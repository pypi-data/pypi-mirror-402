{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cloud Storage Workflows\n",
    "\n",
    "This notebook covers working with cloud storage:\n",
    "- Uploading to S3, GCS, and Azure\n",
    "- S3-compatible storage (MinIO, source.coop)\n",
    "- Authentication options\n",
    "\n",
    "**Note**: Cloud operations require appropriate credentials and network access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uploading to S3\n",
    "\n",
    "Use the `.upload()` method to write directly to cloud storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload to S3 (requires AWS credentials)\n",
    "# Uncomment to run:\n",
    "\n",
    "# gpio.read('data/sample.parquet') \\\n",
    "#     .add_bbox() \\\n",
    "#     .sort_hilbert() \\\n",
    "#     .upload('s3://my-bucket/data/buildings.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using AWS Profiles\n",
    "\n",
    "For non-default credentials, use the `profile` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload with specific AWS profile\n",
    "# Uncomment to run:\n",
    "\n",
    "# gpio.read('data/sample.parquet') \\\n",
    "#     .add_bbox() \\\n",
    "#     .sort_hilbert() \\\n",
    "#     .upload(\n",
    "#         's3://my-bucket/data/buildings.parquet',\n",
    "#         profile='my-aws-profile'\n",
    "#     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S3-Compatible Storage\n",
    "\n",
    "For MinIO, source.coop, or other S3-compatible storage, use the `s3_endpoint` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload to MinIO\n",
    "# Uncomment to run:\n",
    "\n",
    "# gpio.read('data/sample.parquet') \\\n",
    "#     .add_bbox() \\\n",
    "#     .sort_hilbert() \\\n",
    "#     .upload(\n",
    "#         's3://my-bucket/data/buildings.parquet',\n",
    "#         s3_endpoint='minio.example.com:9000',\n",
    "#         s3_use_ssl=False  # For local MinIO without SSL\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload to source.coop\n",
    "# Uncomment to run:\n",
    "\n",
    "# gpio.read('data/sample.parquet') \\\n",
    "#     .add_bbox() \\\n",
    "#     .sort_hilbert() \\\n",
    "#     .upload(\n",
    "#         's3://my-repo/data/buildings.parquet',\n",
    "#         s3_endpoint='data.source.coop',\n",
    "#         s3_use_ssl=True\n",
    "#     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload with Compression Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full control over output format\n",
    "# Uncomment to run:\n",
    "\n",
    "# gpio.read('data/sample.parquet') \\\n",
    "#     .add_bbox() \\\n",
    "#     .sort_hilbert() \\\n",
    "#     .upload(\n",
    "#         's3://my-bucket/data/buildings.parquet',\n",
    "#         compression='ZSTD',\n",
    "#         compression_level=15,\n",
    "#         row_group_size_mb=128,\n",
    "#         profile='my-aws'\n",
    "#     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Pipeline with Cloud Upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete workflow: read, filter, transform, upload\n",
    "# Uncomment to run:\n",
    "\n",
    "# gpio.read('data/sample.parquet') \\\n",
    "#     .extract(limit=1000) \\\n",
    "#     .add_bbox() \\\n",
    "#     .add_h3(resolution=9) \\\n",
    "#     .sort_hilbert() \\\n",
    "#     .upload('s3://my-bucket/processed/buildings.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Remote Files\n",
    "\n",
    "For reading from remote storage, use the CLI which has full remote file support.\n",
    "\n",
    "```bash\n",
    "# Read from S3 and process\n",
    "gpio add bbox s3://bucket/input.parquet | gpio sort hilbert - output.parquet\n",
    "\n",
    "# Read from HTTPS\n",
    "gpio extract --limit 1000 https://example.com/data.parquet output.parquet\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Python workflows, download first then process\n",
    "\n",
    "# Uncomment to run:\n",
    "# subprocess.run([\n",
    "#     'gpio', 'extract', '--limit', '1000',\n",
    "#     's3://bucket/large.parquet', '/tmp/local.parquet'\n",
    "# ])\n",
    "\n",
    "# Then use Python API\n",
    "# gpio.read('/tmp/local.parquet').add_bbox().sort_hilbert().write('/tmp/processed.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GCS and Azure\n",
    "\n",
    "GCS and Azure Blob Storage are also supported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload to GCS\n",
    "# Uncomment to run:\n",
    "\n",
    "# gpio.read('data/sample.parquet') \\\n",
    "#     .add_bbox() \\\n",
    "#     .sort_hilbert() \\\n",
    "#     .upload('gs://my-bucket/data/buildings.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload to Azure Blob Storage\n",
    "# Uncomment to run:\n",
    "\n",
    "# gpio.read('data/sample.parquet') \\\n",
    "#     .add_bbox() \\\n",
    "#     .sort_hilbert() \\\n",
    "#     .upload('az://container/data/buildings.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "| Storage | URL Format | Auth |\n",
    "|---------|------------|------|\n",
    "| AWS S3 | `s3://bucket/path` | AWS credentials, `profile` param |\n",
    "| S3-compatible | `s3://bucket/path` + `s3_endpoint` | AWS credentials |\n",
    "| GCS | `gs://bucket/path` | GCS credentials |\n",
    "| Azure | `az://container/path` | Azure credentials |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## More Resources\n\n- [Remote Files Guide](https://geoparquet.io/guide/remote-files/) - CLI remote file support\n- [Upload Guide](https://geoparquet.io/cli/upload/) - CLI upload options"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
