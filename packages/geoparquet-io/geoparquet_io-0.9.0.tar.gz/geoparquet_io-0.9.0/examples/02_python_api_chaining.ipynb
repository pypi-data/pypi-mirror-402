{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python API Chaining and Pipelines\n",
    "\n",
    "This notebook covers advanced usage patterns:\n",
    "- Method chaining patterns\n",
    "- Reusable pipelines with `pipe()`\n",
    "- Using the `ops` module for PyArrow integration\n",
    "- Performance comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.parquet as pq\n",
    "\n",
    "import geoparquet_io as gpio\n",
    "from geoparquet_io.api import Table, ops, pipe, read"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method Chaining Patterns\n",
    "\n",
    "All transformation methods return a new Table, enabling fluent chains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple chain\n",
    "result = gpio.read(\"data/sample.parquet\").add_bbox().sort_hilbert()\n",
    "\n",
    "result.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complex chain with filtering\n",
    "result = (\n",
    "    gpio.read(\"data/sample.parquet\")\n",
    "    .extract(limit=50)\n",
    "    .add_bbox()\n",
    "    .add_quadkey(resolution=12)\n",
    "    .sort_hilbert()\n",
    ")\n",
    "\n",
    "print(f\"Columns: {result.column_names}\")\n",
    "print(f\"Rows: {result.num_rows}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reusable Pipelines with `pipe()`\n",
    "\n",
    "Define standard processing pipelines that can be applied to any table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a reusable optimization pipeline\n",
    "optimize = pipe(\n",
    "    lambda t: t.add_bbox(),\n",
    "    lambda t: t.sort_hilbert(),\n",
    ")\n",
    "\n",
    "# Apply to a file\n",
    "result = optimize(read(\"data/sample.parquet\"))\n",
    "result.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More complex pipeline with parameters\n",
    "def create_enrichment_pipeline(h3_resolution=9, quadkey_resolution=12):\n",
    "    return pipe(\n",
    "        lambda t: t.add_bbox(),\n",
    "        lambda t: t.add_h3(resolution=h3_resolution),\n",
    "        lambda t: t.add_quadkey(resolution=quadkey_resolution),\n",
    "        lambda t: t.sort_hilbert(),\n",
    "    )\n",
    "\n",
    "\n",
    "# Create different pipelines\n",
    "detailed_pipeline = create_enrichment_pipeline(h3_resolution=9)\n",
    "coarse_pipeline = create_enrichment_pipeline(h3_resolution=6, quadkey_resolution=8)\n",
    "\n",
    "# Apply the detailed pipeline\n",
    "result = detailed_pipeline(read(\"data/sample.parquet\"))\n",
    "print(f\"Columns: {result.column_names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the `ops` Module\n",
    "\n",
    "For integration with existing PyArrow workflows, use the pure functions in `ops`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read with PyArrow directly\n",
    "arrow_table = pq.read_table(\"data/sample.parquet\")\n",
    "print(f\"PyArrow table: {arrow_table.num_rows} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply ops functions\n",
    "arrow_table = ops.add_bbox(arrow_table)\n",
    "arrow_table = ops.add_quadkey(arrow_table, resolution=12)\n",
    "arrow_table = ops.sort_hilbert(arrow_table)\n",
    "\n",
    "print(f\"Columns: {arrow_table.column_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap result in Table for proper GeoParquet output\n",
    "table = Table(arrow_table)\n",
    "table.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting Between APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From gpio.Table to PyArrow\n",
    "table = gpio.read(\"data/sample.parquet\")\n",
    "arrow_table = table.to_arrow()\n",
    "print(f\"Arrow table: {type(arrow_table)}\")\n",
    "\n",
    "# From PyArrow to gpio.Table\n",
    "table_again = Table(arrow_table)\n",
    "print(f\"GPIO table: {type(table_again)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditional Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smart_optimize(input_path):\n",
    "    \"\"\"Apply different processing based on data characteristics.\"\"\"\n",
    "    table = gpio.read(input_path)\n",
    "\n",
    "    # Always add bbox and sort\n",
    "    result = table.add_bbox().sort_hilbert()\n",
    "\n",
    "    # Add H3 for larger datasets (useful for later partitioning)\n",
    "    if table.num_rows > 100:\n",
    "        result = result.add_h3(resolution=9)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "result = smart_optimize(\"data/sample.parquet\")\n",
    "print(f\"Columns: {result.column_names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_process(input_path, output_path):\n",
    "    \"\"\"Process a file with error handling.\"\"\"\n",
    "    try:\n",
    "        gpio.read(input_path).add_bbox().sort_hilbert().write(output_path)\n",
    "        return True, None\n",
    "    except Exception as e:\n",
    "        return False, str(e)\n",
    "\n",
    "\n",
    "success, error = safe_process(\"data/sample.parquet\", \"/tmp/safe_output.parquet\")\n",
    "print(f\"Success: {success}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "- [03_spatial_indices.ipynb](03_spatial_indices.ipynb) - Understanding spatial indices\n",
    "- [04_partitioning.ipynb](04_partitioning.ipynb) - Partitioning large datasets"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
