{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Evaluation Dataset Generation\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook demonstrates how to generate high-quality RAG (Retrieval-Augmented Generation) evaluation datasets using the SDG Hub framework. It creates question-answer pairs with ground truth context that can be used to evaluate RAG systems.\n",
    "\n",
    "## What This Notebook Does\n",
    "\n",
    "This notebook will:\n",
    "\n",
    "1. **Construct Input Dataset**: Show how to prepare documents with outlines for the RAG evaluation flow\n",
    "2. **Generate RAG Evaluation Dataset**: Run the RAG Evaluation flow to create question-answer pairs with:\n",
    "   - Topic extraction from documents\n",
    "   - Conceptual question generation\n",
    "   - Question evolution for better quality\n",
    "   - Answer generation with grounding\n",
    "   - Groundedness scoring and filtering\n",
    "   - Ground truth context extraction\n",
    "3. **Visualize Results**: Display sample generated responses\n",
    "4. **Post-process for Evaluation**: Convert the output to evaluation-ready formats (e.g., for RAGAS)\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- SDG Hub installed and configured\n",
    "- Model endpoint configured via environment variables (see Environment Variables Setup section below)\n",
    "\n",
    "```bash \n",
    "git clone https://github.com/Red-Hat-AI-Innovation-Team/sdg_hub.git\n",
    "cd sdg_hub\n",
    "pip install .[examples]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "\n",
    "from sdg_hub import Flow, FlowRegistry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required to run the flow with async mode\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Prepare Input Dataset\n",
    "\n",
    "The RAG Evaluation flow requires:\n",
    "- **document**: The full text content of the document\n",
    "- **document_outline**: A concise title or summary that represents the document\n",
    "\n",
    "You can prepare this from various sources:\n",
    "- PDF documents (extract text and create outlines)\n",
    "- Text files\n",
    "- Existing datasets\n",
    "- Web content\n",
    "\n",
    "Below are example functions to help construct the input dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset_from_text(text: str, document_outline: str, chunk_size: int = 3000, overlap: int = 500):\n",
    "    \"\"\"\n",
    "    Prepare dataset from a single text document by chunking it.\n",
    "    \n",
    "    Args:\n",
    "        text: Full document text\n",
    "        document_outline: Title or summary of the document\n",
    "        chunk_size: Maximum characters per chunk\n",
    "        overlap: Overlap between chunks to maintain context (must be < chunk_size)\n",
    "        \n",
    "    Returns:\n",
    "        Dataset with document and document_outline columns\n",
    "    \"\"\"\n",
    "    # Validate parameters\n",
    "    if overlap >= chunk_size:\n",
    "        raise ValueError(f\"overlap ({overlap}) must be less than chunk_size ({chunk_size})\")\n",
    "    \n",
    "    if chunk_size <= 0:\n",
    "        raise ValueError(f\"chunk_size must be positive, got {chunk_size}\")\n",
    "    \n",
    "    # Simple chunking by character count with overlap\n",
    "    chunks = []\n",
    "    step_size = chunk_size - overlap\n",
    "    \n",
    "    for i in range(0, len(text), step_size):\n",
    "        chunk = text[i:i + chunk_size]\n",
    "        if chunk.strip():\n",
    "            chunks.append(chunk)\n",
    "    \n",
    "    # Create dataset\n",
    "    dataset = Dataset.from_dict({\n",
    "        \"document\": chunks,\n",
    "        \"document_outline\": [document_outline] * len(chunks)\n",
    "    })\n",
    "    \n",
    "    print(f\"Created {len(chunks)} chunks from document\")\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def prepare_dataset_from_pdf(pdf_path: str, document_outline: str, max_pages: int = None):\n",
    "    \"\"\"\n",
    "    Prepare dataset from a PDF file.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path: Path to PDF file\n",
    "        document_outline: Title or summary of the document\n",
    "        max_pages: Maximum number of pages to process (None for all)\n",
    "        \n",
    "    Returns:\n",
    "        Dataset with document and document_outline columns\n",
    "    \"\"\"\n",
    "    try:\n",
    "        from PyPDF2 import PdfReader\n",
    "    except ImportError:\n",
    "        raise ImportError(\"PyPDF2 is required. Install with: pip install PyPDF2\")\n",
    "    \n",
    "    reader = PdfReader(pdf_path)\n",
    "    text = \"\"\n",
    "    \n",
    "    pages_to_read = reader.pages[:max_pages] if max_pages else reader.pages\n",
    "    for page in pages_to_read:\n",
    "        text += page.extract_text() + \"\\n\"\n",
    "    \n",
    "    return prepare_dataset_from_text(text, document_outline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Create Dataset from IBM Annual Report\n",
    "\n",
    "Here's an example using the IBM 2024 Annual Report. It will extract text from the first 20 pages and create chunks for processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = \"ibm-annual-report-2024.pdf\"\n",
    "\n",
    "if not os.path.exists(pdf_path):\n",
    "    raise FileNotFoundError(\n",
    "        f\"PDF file not found: {pdf_path}\\n\"\n",
    "    )\n",
    "\n",
    "input_dataset = prepare_dataset_from_pdf(pdf_path, \"IBM 2024 Annual Report Summary\", max_pages=20)\n",
    "print(f\"\\nInput dataset columns: {input_dataset.column_names}\")\n",
    "print(f\"Number of samples: {len(input_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Discover and Load the RAG Evaluation Flow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the RAG Evaluation flow\n",
    "flow_name = \"RAG Evaluation Dataset Flow\"\n",
    "flow_path = FlowRegistry.get_flow_path(flow_name)\n",
    "\n",
    "flow = Flow.from_yaml(flow_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Configure Model\n",
    "\n",
    "Set up the model configuration for the flow. This uses environment variables for configuration.\n",
    "\n",
    "**IMPORTANT:** Before running the cells below, make sure to set the following environment variables:\n",
    "\n",
    "```bash\n",
    "export INFERENCE_MODEL=\"your-model-name\"\n",
    "export URL=\"your-api-endpoint\"\n",
    "export API_KEY=\"your-api-key\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_model_config(flow_object):\n",
    "    \"\"\"Configure the model for the flow based on environment variables.\"\"\"\n",
    "    model = os.getenv(\"INFERENCE_MODEL\", \"\")\n",
    "    api_base = os.getenv(\"URL\", \"\")\n",
    "    api_key = os.getenv(\"API_KEY\", \"\")\n",
    "    \n",
    "    if model and not model.startswith(\"openai/\") and not model.startswith(\"ollama/\"):\n",
    "        model = \"openai/\" + model\n",
    "    \n",
    "    if not model:\n",
    "        raise ValueError(\"INFERENCE_MODEL environment variable must be set\")\n",
    "    \n",
    "    print(f\"Configuring model: {model}\")\n",
    "    \n",
    "    flow_object.set_model_config(\n",
    "        model=model,\n",
    "        api_base=api_base if api_base else None,\n",
    "        api_key=api_key if api_key else None,\n",
    "    )\n",
    "    \n",
    "    return flow_object\n",
    "\n",
    "# Configure the model\n",
    "flow = set_model_config(flow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Generate RAG Evaluation Dataset\n",
    "\n",
    "Run the flow to generate question-answer pairs with ground truth context. The flow will:\n",
    "1. Extract topics from documents\n",
    "2. Generate conceptual questions\n",
    "3. Evolve questions for better quality\n",
    "4. Generate answers with grounding\n",
    "5. Score groundedness and filter low-quality pairs\n",
    "6. Extract ground truth context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get runtime parameters\n",
    "max_concurrency = int(os.getenv(\"MAX_CONCURRENCY\", \"10\"))\n",
    "\n",
    "# Optional: Configure runtime parameters for specific blocks\n",
    "runtime_params = {}\n",
    "\n",
    "print(\"This may take several minutes depending on dataset size and model speed...\\n\")\n",
    "\n",
    "# Generate the dataset\n",
    "generated_data = flow.generate(\n",
    "    input_dataset, \n",
    "    runtime_params=runtime_params, \n",
    "    max_concurrency=max_concurrency\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Visualize Generated Results\n",
    "\n",
    "Let's examine some of the generated question-answer pairs to assess quality.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = generated_data.to_pandas()\n",
    "\n",
    "print(f\"Total records: {len(df)}\")\n",
    "print(\"\\nColumns:\", list(df.columns))\n",
    "\n",
    "print(\"\\nSAMPLE GENERATED RECORDS\")\n",
    "\n",
    "sample_cols = [\"topic\", \"question\", \"response\", \"ground_truth_context\"]\n",
    "\n",
    "for i, row in df.head(3).iterrows():\n",
    "    print(\"\\n\")\n",
    "    for col in sample_cols:\n",
    "        if col in df:\n",
    "            val = row[col]\n",
    "            text = str(val)\n",
    "            if len(text) > 200:\n",
    "                text = text[:200] + \"...\"\n",
    "            print(f\"{col.title()}: {text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_columns = ['question', 'response', 'ground_truth_context']\n",
    "\n",
    "print(\"DETAILED VIEW (First Record)\")\n",
    "\n",
    "first = df.iloc[0]\n",
    "\n",
    "for col in display_columns:\n",
    "    if col in df and pd.notna(first[col]):\n",
    "        print(f\"\\n{col.upper()}:\")\n",
    "        print(first[col], \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Post-process for Evaluation\n",
    "\n",
    "Convert the generated dataset to evaluation-ready formats. This prepares the data for use with evaluation frameworks like RAGAS.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "def prepare_for_ragas_evaluation(generated_df: pd.DataFrame, output_file: str = None):\n",
    "    \"\"\"\n",
    "    Convert generated dataset to RAGAS evaluation format.\n",
    "    \n",
    "    RAGAS expects:\n",
    "    - question: The question\n",
    "    - answer: The generated answer\n",
    "    - contexts: List of context strings (usually one)\n",
    "    - ground_truth: The ground truth answer (can be same as answer or use ground_truth_context)\n",
    "    \n",
    "    Args:\n",
    "        generated_df: DataFrame from flow generation\n",
    "        output_file: Optional path to save JSONL file\n",
    "        \n",
    "    Returns:\n",
    "        List of dictionaries in RAGAS format\n",
    "    \"\"\"\n",
    "    ragas_data = []\n",
    "    \n",
    "    for _, row in generated_df.iterrows():\n",
    "        question = row.get('question', '')\n",
    "        answer = row.get('response', '')\n",
    "        context = row.get('document', row.get('context', ''))\n",
    "        ground_truth = row.get('ground_truth_context', answer)\n",
    "        \n",
    "        ragas_record = {\n",
    "            \"question\": str(question),\n",
    "            \"answer\": str(answer),\n",
    "            \"contexts\": [str(context)] if context else [\"\"],\n",
    "            \"ground_truth\": str(ground_truth)\n",
    "        }\n",
    "        \n",
    "        ragas_data.append(ragas_record)\n",
    "    \n",
    "    if output_file:\n",
    "        output_file = Path(output_file)\n",
    "        output_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        with output_file.open(\"w\") as f:\n",
    "            for record in ragas_data:\n",
    "                f.write(json.dumps(record, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    return ragas_data\n",
    "\n",
    "ragas_data = prepare_for_ragas_evaluation(df, output_file=\"rag_evaluation_dataset.jsonl\")\n",
    "\n",
    "print(f\"\\nâœ… Prepared {len(ragas_data)} records for evaluation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the full generated dataset\n",
    "output_csv = \"rag_evaluation_full_results.csv\"\n",
    "generated_data.to_csv(output_csv, index=False)\n",
    "print(f\"Saved full results to {output_csv}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "ðŸŽ‰ You have successfully:\n",
    "\n",
    "1. âœ… Prepared input dataset with documents and outlines\n",
    "2. âœ… Generated RAG evaluation dataset with question-answer pairs\n",
    "3. âœ… Visualized generated results\n",
    "4. âœ… Post-processed data for evaluation frameworks\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Use the generated `rag_evaluation_dataset.jsonl` file with RAGAS or other evaluation frameworks\n",
    "- Analyze the quality of generated questions and answers\n",
    "- Fine-tune the flow parameters or prompts if needed\n",
    "- Scale up to larger datasets for comprehensive evaluation\n",
    "\n",
    "> **Note:**  \n",
    "> In a real RAG system, the model-generated answer comes from retrieved context, \n",
    "> so it will often differ from the ground truth.\n",
    "\n",
    "### Example: Using with RAGAS\n",
    "\n",
    "```python\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import faithfulness, answer_relevancy, context_precision, context_recall\n",
    "from datasets import load_dataset\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "import os\n",
    "\n",
    "# Load the prepared dataset\n",
    "dataset = load_dataset(\"json\", data_files=\"rag_evaluation_dataset.jsonl\", split=\"train\")\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=os.getenv(\"INFERENCE_MODEL\", \"\"),\n",
    "    temperature=0,\n",
    "    base_url=os.getenv(\"URL\", \"\"),\n",
    "    api_key=os.getenv(\"API_KEY\", \"\")\n",
    ")\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"nomic-ai/nomic-embed-text-v1.5\",\n",
    "    model_kwargs={'device': 'cpu', \"trust_remote_code\": True},\n",
    "    encode_kwargs={'normalize_embeddings': True}\n",
    ")\n",
    "\n",
    "# Run RAGAS evaluation\n",
    "results = evaluate(\n",
    "    dataset=dataset,\n",
    "    metrics=[faithfulness, answer_relevancy, context_precision, context_recall],\n",
    "    llm=llm,\n",
    "    embeddings=embeddings\n",
    ")\n",
    "\n",
    "print(f\"\\n{results}\")\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
