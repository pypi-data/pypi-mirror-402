"""æ•°æ®éªŒè¯å™¨.

æä¾›å„ç§æ•°æ®çš„è´¨é‡æ£€æŸ¥å’Œå®Œæ•´æ€§éªŒè¯åŠŸèƒ½ã€‚
"""

from datetime import timedelta
from pathlib import Path

import pandas as pd

from cryptoservice.config.logging import get_logger
from cryptoservice.models import Freq, IntegrityReport
from cryptoservice.storage.database import Database as AsyncMarketDB

logger = get_logger(__name__)


# TODO: æ—¶é—´è¿ç»­æ€§æ£€éªŒ
class DataValidator:
    """æ•°æ®éªŒè¯å™¨."""

    def __init__(self) -> None:
        """åˆå§‹åŒ–æ•°æ®éªŒè¯å™¨."""
        self.validation_errors: list[str] = []

    def validate_kline_data(self, data: list, symbol: str) -> list:
        """éªŒè¯Kçº¿æ•°æ®è´¨é‡."""
        if not data:
            return data

        valid_data = []
        issues = []

        for i, kline in enumerate(data):
            try:
                # æ£€æŸ¥æ•°æ®ç»“æ„
                if len(kline) < 8:
                    issues.append(f"è®°å½•{i}: æ•°æ®å­—æ®µä¸è¶³")
                    continue

                # æ£€æŸ¥ä»·æ ¼æ•°æ®æœ‰æ•ˆæ€§
                open_price = float(kline[1])
                high_price = float(kline[2])
                low_price = float(kline[3])
                close_price = float(kline[4])
                volume = float(kline[5])

                # åŸºç¡€é€»è¾‘æ£€æŸ¥
                if high_price < max(open_price, close_price, low_price):
                    issues.append(f"è®°å½•{i}: æœ€é«˜ä»·å¼‚å¸¸")
                    continue

                if low_price > min(open_price, close_price, high_price):
                    issues.append(f"è®°å½•{i}: æœ€ä½ä»·å¼‚å¸¸")
                    continue

                if volume < 0:
                    issues.append(f"è®°å½•{i}: æˆäº¤é‡ä¸ºè´Ÿ")
                    continue

                valid_data.append(kline)

            except (ValueError, IndexError) as e:
                issues.append(f"è®°å½•{i}: æ•°æ®æ ¼å¼é”™è¯¯ - {e}")
                continue

        if issues:
            issue_count = len(issues)
            total_count = len(data)
            if issue_count > total_count * 0.1:  # è¶…è¿‡10%çš„æ•°æ®æœ‰é—®é¢˜
                logger.warning(
                    "data_quality_issue",
                    symbol=symbol,
                    invalid_records=issue_count,
                    total_records=total_count,
                )
                self.validation_errors.extend(issues[:5])  # ä¿å­˜å‰5ä¸ªé”™è¯¯

        return valid_data

    def get_validation_errors(self) -> list[str]:
        """è·å–éªŒè¯é”™è¯¯åˆ—è¡¨."""
        return self.validation_errors.copy()

    def clear_validation_errors(self):
        """æ¸…é™¤éªŒè¯é”™è¯¯."""
        self.validation_errors.clear()

    def _validate_open_interest_list(self, oi_data: list, issues: list[str]) -> list:
        """éªŒè¯æŒä»“é‡æ•°æ®åˆ—è¡¨."""
        valid_oi = []
        for i, oi in enumerate(oi_data):
            try:
                if not all(hasattr(oi, attr) for attr in ["symbol", "open_interest", "time"]):
                    issues.append(f"æŒä»“é‡è®°å½• {i}: ç¼ºå°‘å¿…è¦å­—æ®µ")
                    continue
                if oi.open_interest < 0:
                    issues.append(f"æŒä»“é‡è®°å½• {i}: æŒä»“é‡ä¸ºè´Ÿæ•°")
                    continue
                if oi.time <= 0:
                    issues.append(f"æŒä»“é‡è®°å½• {i}: æ—¶é—´æˆ³æ— æ•ˆ")
                    continue
                valid_oi.append(oi)
            except Exception as e:
                issues.append(f"æŒä»“é‡è®°å½• {i}: éªŒè¯å¤±è´¥ - {e}")
        return valid_oi

    def _validate_long_short_ratio_list(self, lsr_data: list, issues: list[str]) -> list:
        """éªŒè¯å¤šç©ºæ¯”ç‡æ•°æ®åˆ—è¡¨."""
        valid_lsr = []
        for i, lsr in enumerate(lsr_data):
            try:
                if not all(hasattr(lsr, attr) for attr in ["symbol", "long_short_ratio", "time"]):
                    issues.append(f"å¤šç©ºæ¯”ä¾‹è®°å½• {i}: ç¼ºå°‘å¿…è¦å­—æ®µ")
                    continue
                if lsr.long_short_ratio < 0:
                    issues.append(f"å¤šç©ºæ¯”ä¾‹è®°å½• {i}: æ¯”ä¾‹ä¸ºè´Ÿæ•°")
                    continue
                if lsr.time <= 0:
                    issues.append(f"å¤šç©ºæ¯”ä¾‹è®°å½• {i}: æ—¶é—´æˆ³æ— æ•ˆ")
                    continue
                valid_lsr.append(lsr)
            except Exception as e:
                issues.append(f"å¤šç©ºæ¯”ä¾‹è®°å½• {i}: éªŒè¯å¤±è´¥ - {e}")
        return valid_lsr

    def validate_metrics_data(self, data: dict[str, list], symbol: str, url: str) -> dict[str, list] | None:
        """éªŒè¯metricsæ•°æ®çš„å®Œæ•´æ€§å’Œè´¨é‡."""
        try:
            issues: list[str] = []
            validated_data: dict[str, list] = {"open_interest": [], "long_short_ratio": []}
            if oi_data := data.get("open_interest"):
                valid_oi = self._validate_open_interest_list(oi_data, issues)
                validated_data["open_interest"] = valid_oi
                if len(valid_oi) < len(oi_data) * 0.5:
                    logger.warning(
                        "open_interest_quality_low",
                        symbol=symbol,
                        valid_records=len(valid_oi),
                        total_records=len(oi_data),
                    )
            if lsr_data := data.get("long_short_ratio"):
                valid_lsr = self._validate_long_short_ratio_list(lsr_data, issues)
                validated_data["long_short_ratio"] = valid_lsr
                if len(valid_lsr) < len(lsr_data) * 0.5:
                    logger.warning(
                        "long_short_ratio_quality_low",
                        symbol=symbol,
                        valid_records=len(valid_lsr),
                        total_records=len(lsr_data),
                    )
            if issues:
                logger.debug("metrics_validation_issues", symbol=symbol, issues_found=len(issues))
                self.validation_errors.extend(issues[:3])
            if not validated_data["open_interest"] and not validated_data["long_short_ratio"]:
                logger.warning("metrics_validation_empty", symbol=symbol)
                return None
            logger.debug(
                "metrics_validation_passed",
                symbol=symbol,
                open_interest=len(validated_data["open_interest"]),
                long_short_ratio=len(validated_data["long_short_ratio"]),
            )
            return validated_data
        except Exception as e:
            logger.warning("metrics_validation_failed", symbol=symbol, error=str(e))
            return data

    async def _check_sample_data_quality(
        self,
        successful_symbols: list[str],
        start_time: str,
        end_time: str,
        interval: Freq,
        db_file_path: Path,
    ) -> tuple[int, list[str]]:
        """å¯¹æˆåŠŸä¸‹è½½çš„ç¬¦å·æ ·æœ¬è¿›è¡Œæ•°æ®è´¨é‡æ£€æŸ¥."""
        quality_issues = 0
        detailed_issues = []
        sample_symbols = successful_symbols[: min(5, len(successful_symbols))]

        if start_time == end_time:
            logger.debug("æ£€æµ‹åˆ°å•æ—¥æµ‹è¯•æ•°æ®ï¼Œè·³è¿‡è¯¦ç»†å®Œæ•´æ€§æ£€æŸ¥")
            return 0, []

        db = AsyncMarketDB(str(db_file_path))
        for symbol in sample_symbols:
            try:
                check_start_time = pd.to_datetime(start_time).strftime("%Y-%m-%d")
                check_end_time = pd.to_datetime(end_time).strftime("%Y-%m-%d")
                df = await db.select_klines(
                    symbols=[symbol],
                    start_time=check_start_time,
                    end_time=check_end_time,
                    freq=interval,
                    columns=None,
                )
                if df is not None and not df.empty:
                    symbol_data = df.loc[symbol] if symbol in df.index.get_level_values("symbol") else pd.DataFrame()
                    if not symbol_data.empty:
                        time_diff = pd.to_datetime(check_end_time) - pd.to_datetime(check_start_time)
                        expected_points = self._calculate_expected_data_points(time_diff, interval)
                        actual_points = len(symbol_data)
                        completeness = actual_points / expected_points if expected_points > 0 else 0
                        if completeness < 0.8:
                            quality_issues += 1
                            detailed_issues.append(f"{symbol}: æ•°æ®å®Œæ•´æ€§{completeness:.1%} ({actual_points}/{expected_points})")
                    else:
                        quality_issues += 1
                        detailed_issues.append(f"{symbol}: æ— æ³•è¯»å–å·²ä¸‹è½½çš„æ•°æ®")
                else:
                    quality_issues += 1
                    detailed_issues.append(f"{symbol}: æ•°æ®åº“ä¸­æœªæ‰¾åˆ°æ•°æ®")
            except Exception as e:
                quality_issues += 1
                detailed_issues.append(f"{symbol}: æ£€æŸ¥å¤±è´¥ - {e}")
        return quality_issues, detailed_issues

    async def create_integrity_report(
        self,
        symbols: list[str],
        successful_symbols: list[str],
        failed_symbols: list[str],
        missing_periods: list[dict[str, str]],
        start_time: str,
        end_time: str,
        interval: Freq,
        db_file_path: Path,
    ) -> IntegrityReport:
        """åˆ›å»ºæ•°æ®å®Œæ•´æ€§æŠ¥å‘Š."""
        try:
            logger.info("integrity_check_started")
            total_symbols = len(symbols)
            success_count = len(successful_symbols)
            basic_quality_score = success_count / total_symbols if total_symbols > 0 else 0
            quality_issues, detailed_issues = await self._check_sample_data_quality(successful_symbols, start_time, end_time, interval, db_file_path)
            if successful_symbols:
                sample_size = min(5, len(successful_symbols))
                quality_penalty = (quality_issues / sample_size) * 0.3 if sample_size > 0 else 0
                final_quality_score = max(0, basic_quality_score - quality_penalty)
            else:
                final_quality_score = 0
            recommendations = []
            if final_quality_score < 0.5:
                recommendations.append("ğŸš¨ æ•°æ®è´¨é‡ä¸¥é‡ä¸è¶³ï¼Œå»ºè®®é‡æ–°ä¸‹è½½")
            elif final_quality_score < 0.8:
                recommendations.append("âš ï¸ æ•°æ®è´¨é‡ä¸€èˆ¬ï¼Œå»ºè®®æ£€æŸ¥å¤±è´¥çš„äº¤æ˜“å¯¹")
            else:
                recommendations.append("âœ… æ•°æ®è´¨é‡è‰¯å¥½")
            if failed_symbols:
                recommendations.append(f"ğŸ“ {len(failed_symbols)}ä¸ªäº¤æ˜“å¯¹ä¸‹è½½å¤±è´¥ï¼Œå»ºè®®å•ç‹¬é‡è¯•")
            if quality_issues > 0:
                recommendations.append(f"âš ï¸ å‘ç°{quality_issues}ä¸ªæ•°æ®è´¨é‡é—®é¢˜")
                recommendations.extend(detailed_issues[:3])
            if len(failed_symbols) > total_symbols * 0.3:
                recommendations.append("ğŸŒ å¤±è´¥ç‡è¾ƒé«˜ï¼Œå»ºè®®æ£€æŸ¥ç½‘ç»œè¿æ¥å’ŒAPIé™åˆ¶")
            logger.info(f"âœ… å®Œæ•´æ€§æ£€æŸ¥å®Œæˆ: è´¨é‡åˆ†æ•° {final_quality_score:.1%}")
            return IntegrityReport(
                total_symbols=total_symbols,
                successful_symbols=success_count,
                failed_symbols=failed_symbols,
                missing_periods=missing_periods,
                data_quality_score=final_quality_score,
                recommendations=recommendations,
            )
        except Exception as e:
            logger.warning("integrity_check_failed", error=str(e))
            return IntegrityReport(
                total_symbols=len(symbols),
                successful_symbols=len(successful_symbols),
                failed_symbols=failed_symbols,
                missing_periods=missing_periods,
                data_quality_score=(len(successful_symbols) / len(symbols) if symbols else 0),
                recommendations=[f"å®Œæ•´æ€§æ£€æŸ¥å¤±è´¥: {e}", "å»ºè®®æ‰‹åŠ¨éªŒè¯æ•°æ®è´¨é‡"],
            )

    def _calculate_expected_data_points(self, time_diff: timedelta, interval: Freq) -> int:
        """è®¡ç®—æœŸæœ›çš„æ•°æ®ç‚¹æ•°é‡."""
        total_minutes = time_diff.total_seconds() / 60

        interval_minutes = {
            Freq.m1: 1,
            Freq.m3: 3,
            Freq.m5: 5,
            Freq.m15: 15,
            Freq.m30: 30,
            Freq.h1: 60,
            Freq.h4: 240,
            Freq.d1: 1440,
        }.get(interval, 1)

        expected_points = int(total_minutes / interval_minutes)
        return max(1, expected_points)
