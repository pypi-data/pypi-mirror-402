"""Universeç®¡ç†å™¨.

ä¸“é—¨å¤„ç†Universeå®šä¹‰ã€ç®¡ç†å’Œç›¸å…³æ“ä½œã€‚
"""

from datetime import UTC, datetime, timedelta
from pathlib import Path
from typing import TYPE_CHECKING, Any

import pandas as pd

if TYPE_CHECKING:
    from cryptoservice.services.market_service import MarketDataService

from cryptoservice.config.logging import get_logger
from cryptoservice.exceptions import MarketDataFetchError
from cryptoservice.models import Freq, UniverseConfig, UniverseDefinition, UniverseSnapshot
from cryptoservice.utils import RateLimitManager

logger = get_logger(__name__)


class UniverseManager:
    """Universeç®¡ç†å™¨."""

    def __init__(self, market_service: "MarketDataService"):
        """åˆå§‹åŒ–Universeç®¡ç†å™¨."""
        self.market_service = market_service

    async def define_universe(
        self,
        start_date: str,
        end_date: str,
        t1_months: int,
        t2_months: int,
        t3_months: int,
        output_path: Path | str,
        top_k: int | None = None,
        top_ratio: float | None = None,
        description: str | None = None,
        delay_days: int = 7,
        api_delay_seconds: float = 1.0,
        batch_delay_seconds: float = 3.0,
        batch_size: int = 5,
        quote_asset: str = "USDT",
    ) -> UniverseDefinition:
        """å®šä¹‰universeå¹¶ä¿å­˜åˆ°æ–‡ä»¶."""
        try:
            # éªŒè¯å¹¶å‡†å¤‡è¾“å‡ºè·¯å¾„
            output_path_obj = self._validate_and_prepare_path(
                output_path,
                is_file=True,
                file_name=(f"universe_{start_date}_{end_date}_{t1_months}_{t2_months}_{t3_months}_{top_k or top_ratio}.json"),
            )

            # æ ‡å‡†åŒ–æ—¥æœŸæ ¼å¼
            start_date = self._standardize_date_format(start_date)
            end_date = self._standardize_date_format(end_date)

            # åˆ›å»ºé…ç½®
            config = UniverseConfig(
                start_date=start_date,
                end_date=end_date,
                t1_months=t1_months,
                t2_months=t2_months,
                t3_months=t3_months,
                delay_days=delay_days,
                quote_asset=quote_asset,
                top_k=top_k,
                top_ratio=top_ratio,
            )

            logger.info(f"å¼€å§‹å®šä¹‰universe: {start_date} åˆ° {end_date}")
            log_selection_criteria = f"Top-K={top_k}" if top_k else f"Top-Ratio={top_ratio}"
            logger.info(f"å‚æ•°: T1={t1_months}æœˆ, T2={t2_months}æœˆ, T3={t3_months}æœˆ, {log_selection_criteria}")

            # ç”Ÿæˆé‡æ–°é€‰æ‹©æ—¥æœŸåºåˆ—
            rebalance_dates = self._generate_rebalance_dates(start_date, end_date, t2_months)

            logger.info("é‡å¹³è¡¡è®¡åˆ’:")
            logger.info(f"  - æ—¶é—´èŒƒå›´: {start_date} åˆ° {end_date}")
            logger.info(f"  - é‡å¹³è¡¡é—´éš”: æ¯{t2_months}ä¸ªæœˆ")
            logger.info(f"  - æ•°æ®å»¶è¿Ÿ: {delay_days}å¤©")
            logger.info(f"  - T1æ•°æ®çª—å£: {t1_months}ä¸ªæœˆ")
            logger.info(f"  - é‡å¹³è¡¡æ—¥æœŸ: {rebalance_dates}")

            if not rebalance_dates:
                raise ValueError("æ— æ³•ç”Ÿæˆé‡å¹³è¡¡æ—¥æœŸï¼Œè¯·æ£€æŸ¥æ—¶é—´èŒƒå›´å’ŒT2å‚æ•°")

            # æ”¶é›†æ‰€æœ‰å‘¨æœŸçš„snapshots
            all_snapshots = []

            # åœ¨æ¯ä¸ªé‡æ–°é€‰æ‹©æ—¥æœŸè®¡ç®—universe
            for i, rebalance_date in enumerate(rebalance_dates):
                logger.info(f"å¤„ç†æ—¥æœŸ {i + 1}/{len(rebalance_dates)}: {rebalance_date}")

                # è®¡ç®—åŸºå‡†æ—¥æœŸï¼ˆé‡æ–°å¹³è¡¡æ—¥æœŸå‰delay_dayså¤©ï¼‰
                base_date = pd.to_datetime(rebalance_date, utc=True) - timedelta(days=delay_days)
                calculated_t1_end = base_date.strftime("%Y-%m-%d")

                # è®¡ç®—T1å›çœ‹æœŸé—´çš„å¼€å§‹æ—¥æœŸ
                calculated_t1_start = self._subtract_months(calculated_t1_end, t1_months)

                logger.info(
                    f"å‘¨æœŸ {i + 1}: åŸºå‡†æ—¥æœŸ={calculated_t1_end} (é‡æ–°å¹³è¡¡æ—¥æœŸå‰{delay_days}å¤©), T1æ•°æ®æœŸé—´={calculated_t1_start} åˆ° {calculated_t1_end}"
                )

                # è·å–ç¬¦åˆæ¡ä»¶çš„äº¤æ˜“å¯¹å’Œå®ƒä»¬çš„mean daily amount
                universe_symbols, mean_amounts = await self._calculate_universe_for_date(
                    calculated_t1_start,
                    calculated_t1_end,
                    t3_months=t3_months,
                    top_k=top_k,
                    top_ratio=top_ratio,
                    api_delay_seconds=api_delay_seconds,
                    batch_delay_seconds=batch_delay_seconds,
                    batch_size=batch_size,
                    quote_asset=quote_asset,
                )

                # åˆ›å»ºè¯¥å‘¨æœŸçš„snapshot
                snapshot = UniverseSnapshot.create_with_dates_and_timestamps(
                    usage_t1_start=rebalance_date,
                    usage_t1_end=min(
                        end_date,
                        (pd.to_datetime(rebalance_date, utc=True) + pd.DateOffset(months=t1_months)).strftime("%Y-%m-%d"),
                    ),
                    calculated_t1_start=calculated_t1_start,
                    calculated_t1_end=calculated_t1_end,
                    symbols=universe_symbols,
                    mean_daily_amounts=mean_amounts,
                    metadata={
                        "calculated_t1_start": calculated_t1_start,
                        "calculated_t1_end": calculated_t1_end,
                        "delay_days": delay_days,
                        "quote_asset": quote_asset,
                        "selected_symbols_count": len(universe_symbols),
                    },
                )

                all_snapshots.append(snapshot)
                logger.info(f"âœ… æ—¥æœŸ {rebalance_date}: é€‰æ‹©äº† {len(universe_symbols)} ä¸ªäº¤æ˜“å¯¹")

            # åˆ›å»ºå®Œæ•´çš„universeå®šä¹‰
            universe_def = UniverseDefinition(
                config=config,
                snapshots=all_snapshots,
                creation_time=datetime.now(tz=UTC),
                description=description,
            )

            # ä¿å­˜æ±‡æ€»çš„universeå®šä¹‰
            universe_def.save_to_file(output_path_obj)

            logger.info("ğŸ‰ Universeå®šä¹‰å®Œæˆï¼")
            logger.info(f"ğŸ“ åŒ…å« {len(all_snapshots)} ä¸ªé‡æ–°å¹³è¡¡å‘¨æœŸ")
            logger.info(f"ğŸ“‹ æ±‡æ€»æ–‡ä»¶: {output_path_obj}")

            return universe_def

        except Exception as e:
            logger.error(f"å®šä¹‰universeå¤±è´¥: {e}")
            raise MarketDataFetchError(f"å®šä¹‰universeå¤±è´¥: {e}") from e

    async def _fetch_and_calculate_mean_amounts(
        self,
        symbols: list[str],
        start_date: str,
        end_date: str,
        api_delay_seconds: float,
    ) -> dict[str, float]:
        """ä¸ºç»™å®šçš„äº¤æ˜“å¯¹åˆ—è¡¨è·å–å†å²æ•°æ®å¹¶è®¡ç®—å¹³å‡æ—¥äº¤æ˜“é¢."""
        mean_amounts = {}
        logger.info(f"å¼€å§‹é€šè¿‡APIè·å– {len(symbols)} ä¸ªäº¤æ˜“å¯¹çš„å†å²æ•°æ®...")
        universe_rate_manager = RateLimitManager(base_delay=api_delay_seconds)
        start_ts = self.market_service._date_to_timestamp_start(start_date)
        end_ts = self.market_service._date_to_timestamp_end(end_date)
        for i, symbol in enumerate(symbols):
            if i % 10 == 0:
                logger.info(f"å·²å¤„ç† {i}/{len(symbols)} ä¸ªäº¤æ˜“å¯¹...")
            try:
                original_manager = self.market_service.kline_downloader.rate_limit_manager
                self.market_service.kline_downloader.rate_limit_manager = universe_rate_manager
                try:
                    klines_gen = self.market_service.kline_downloader.download_single_symbol(
                        symbol=symbol,
                        start_ts=start_ts,
                        end_ts=end_ts,
                        interval=Freq.d1,
                    )
                    # å°†å¼‚æ­¥ç”Ÿæˆå™¨è½¬æ¢ä¸ºåˆ—è¡¨
                    klines = [kline async for kline in klines_gen]
                finally:
                    self.market_service.kline_downloader.rate_limit_manager = original_manager
                if klines:
                    expected_days = (pd.to_datetime(end_date, utc=True) - pd.to_datetime(start_date, utc=True)).days + 1
                    actual_days = len(klines)
                    if actual_days < expected_days * 0.8:
                        logger.warning(f"äº¤æ˜“å¯¹ {symbol} æ•°æ®ä¸å®Œæ•´: æœŸæœ›{expected_days}å¤©ï¼Œå®é™…{actual_days}å¤©")
                    amounts = [float(kline.quote_volume) for kline in klines if kline.quote_volume]
                    if amounts:
                        mean_amounts[symbol] = sum(amounts) / len(amounts)
                    else:
                        logger.warning(f"äº¤æ˜“å¯¹ {symbol} åœ¨æœŸé—´å†…æ²¡æœ‰æœ‰æ•ˆçš„æˆäº¤é‡æ•°æ®")
            except Exception as e:
                logger.warning(f"è·å– {symbol} æ•°æ®æ—¶å‡ºé”™ï¼Œè·³è¿‡: {e}")
        return mean_amounts

    def _select_top_symbols(
        self,
        mean_amounts: dict[str, float],
        top_k: int | None,
        top_ratio: float | None,
    ) -> tuple[list[str], dict[str, float]]:
        """æ ¹æ®å¹³å‡äº¤æ˜“é¢é€‰æ‹©é¡¶éƒ¨äº¤æ˜“å¯¹."""
        sorted_symbols = sorted(mean_amounts.items(), key=lambda x: x[1], reverse=True)
        if top_ratio is not None:
            num_to_select = int(len(sorted_symbols) * top_ratio)
        elif top_k is not None:
            num_to_select = top_k
        else:
            num_to_select = len(sorted_symbols)
        top_symbols_data = sorted_symbols[:num_to_select]
        universe_symbols = [symbol for symbol, _ in top_symbols_data]
        final_amounts = dict(top_symbols_data)
        if len(universe_symbols) <= 10:
            logger.info(f"é€‰ä¸­çš„äº¤æ˜“å¯¹: {universe_symbols}")
        else:
            logger.info(f"Top 5: {universe_symbols[:5]}")
            logger.info("å®Œæ•´åˆ—è¡¨å·²ä¿å­˜åˆ°æ–‡ä»¶ä¸­")
        return universe_symbols, final_amounts

    async def _calculate_universe_for_date(
        self,
        calculated_t1_start: str,
        calculated_t1_end: str,
        t3_months: int,
        top_k: int | None = None,
        top_ratio: float | None = None,
        api_delay_seconds: float = 1.0,
        batch_delay_seconds: float = 3.0,
        batch_size: int = 5,
        quote_asset: str = "USDT",
    ) -> tuple[list[str], dict[str, float]]:
        """è®¡ç®—æŒ‡å®šæ—¥æœŸçš„universe."""
        try:
            actual_symbols = await self._get_available_symbols_for_period(calculated_t1_start, calculated_t1_end, quote_asset)
            cutoff_date = self._subtract_months(calculated_t1_end, t3_months)
            eligible_symbols = [symbol for symbol in actual_symbols if await self._symbol_exists_before_date(symbol, cutoff_date)]
            if not eligible_symbols:
                logger.warning(f"æ—¥æœŸ {calculated_t1_start} åˆ° {calculated_t1_end}: æ²¡æœ‰æ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„äº¤æ˜“å¯¹")
                return [], {}
            mean_amounts = await self._fetch_and_calculate_mean_amounts(eligible_symbols, calculated_t1_start, calculated_t1_end, api_delay_seconds)
            if not mean_amounts:
                logger.warning("æ— æ³•é€šè¿‡APIè·å–æ•°æ®ï¼Œè¿”å›ç©ºçš„universe")
                return [], {}
            return self._select_top_symbols(mean_amounts, top_k, top_ratio)
        except Exception as e:
            logger.error(f"è®¡ç®—æ—¥æœŸ {calculated_t1_start} åˆ° {calculated_t1_end} çš„universeæ—¶å‡ºé”™: {e}")
            return [], {}

    async def _get_available_symbols_for_period(self, start_date: str, end_date: str, quote_asset: str = "USDT") -> list[str]:
        """è·å–æŒ‡å®šæ—¶é—´æ®µå†…å®é™…å¯ç”¨çš„æ°¸ç»­åˆçº¦äº¤æ˜“å¯¹."""
        try:
            # å…ˆè·å–å½“å‰æ‰€æœ‰æ°¸ç»­åˆçº¦ä½œä¸ºå€™é€‰
            candidate_symbols = await self.market_service.get_perpetual_symbols(only_trading=True, quote_asset=quote_asset)
            logger.info(f"æ£€æŸ¥ {len(candidate_symbols)} ä¸ª{quote_asset}å€™é€‰äº¤æ˜“å¯¹åœ¨ {start_date} åˆ° {end_date} æœŸé—´çš„å¯ç”¨æ€§...")

            available_symbols = []
            batch_size = 50
            for i in range(0, len(candidate_symbols), batch_size):
                batch = candidate_symbols[i : i + batch_size]
                for symbol in batch:
                    # æ£€æŸ¥åœ¨èµ·å§‹æ—¥æœŸæ˜¯å¦æœ‰æ•°æ®
                    if await self.market_service.check_symbol_exists_on_date(symbol, start_date):
                        available_symbols.append(symbol)

                # æ˜¾ç¤ºè¿›åº¦
                processed = min(i + batch_size, len(candidate_symbols))
                logger.info(f"å·²æ£€æŸ¥ {processed}/{len(candidate_symbols)} ä¸ªäº¤æ˜“å¯¹ï¼Œæ‰¾åˆ° {len(available_symbols)} ä¸ªå¯ç”¨äº¤æ˜“å¯¹")

                # é¿å…APIé¢‘ç‡é™åˆ¶
                import time

                time.sleep(0.1)

            logger.info(f"åœ¨ {start_date} åˆ° {end_date} æœŸé—´æ‰¾åˆ° {len(available_symbols)} ä¸ªå¯ç”¨çš„{quote_asset}æ°¸ç»­åˆçº¦äº¤æ˜“å¯¹")
            return available_symbols

        except Exception as e:
            logger.warning(f"è·å–å¯ç”¨äº¤æ˜“å¯¹æ—¶å‡ºé”™: {e}")
            # å¦‚æœAPIæ£€æŸ¥å¤±è´¥ï¼Œè¿”å›å½“å‰æ‰€æœ‰æ°¸ç»­åˆçº¦
            return await self.market_service.get_perpetual_symbols(only_trading=True, quote_asset=quote_asset)

    async def _symbol_exists_before_date(self, symbol: str, cutoff_date: str) -> bool:
        """æ£€æŸ¥äº¤æ˜“å¯¹æ˜¯å¦åœ¨æŒ‡å®šæ—¥æœŸä¹‹å‰å°±å­˜åœ¨."""
        try:
            # æ£€æŸ¥åœ¨cutoff_dateä¹‹å‰æ˜¯å¦æœ‰æ•°æ®
            check_date = (pd.to_datetime(cutoff_date, utc=True) - timedelta(days=1)).strftime("%Y-%m-%d")
            return await self.market_service.check_symbol_exists_on_date(symbol, check_date)
        except Exception:
            # å¦‚æœæ£€æŸ¥å¤±è´¥ï¼Œé»˜è®¤è®¤ä¸ºå­˜åœ¨
            return True

    def _generate_rebalance_dates(self, start_date: str, end_date: str, t2_months: int) -> list[str]:
        """ç”Ÿæˆé‡æ–°é€‰æ‹©universeçš„æ—¥æœŸåºåˆ—."""
        dates = []
        start_date_obj = pd.to_datetime(start_date, utc=True)
        end_date_obj = pd.to_datetime(end_date, utc=True)

        # ä»èµ·å§‹æ—¥æœŸå¼€å§‹ï¼Œæ¯éš”T2ä¸ªæœˆç”Ÿæˆé‡å¹³è¡¡æ—¥æœŸ
        current_date = start_date_obj

        while current_date <= end_date_obj:
            dates.append(current_date.strftime("%Y-%m-%d"))
            current_date = current_date + pd.DateOffset(months=t2_months)

        return dates

    def _subtract_months(self, date_str: str, months: int) -> str:
        """ä»æ—¥æœŸå‡å»æŒ‡å®šæœˆæ•°."""
        date_obj = pd.to_datetime(date_str, utc=True)
        result_date = date_obj - pd.DateOffset(months=months)
        return str(result_date.strftime("%Y-%m-%d"))

    def _standardize_date_format(self, date_str: str) -> str:
        """æ ‡å‡†åŒ–æ—¥æœŸæ ¼å¼ä¸º YYYY-MM-DD."""
        if len(date_str) == 8:  # YYYYMMDD
            return f"{date_str[:4]}-{date_str[4:6]}-{date_str[6:8]}"
        return date_str

    def _validate_and_prepare_path(self, path: Path | str, is_file: bool = False, file_name: str | None = None) -> Path:
        """éªŒè¯å¹¶å‡†å¤‡è·¯å¾„."""
        if not path:
            raise ValueError("è·¯å¾„ä¸èƒ½ä¸ºç©ºï¼Œå¿…é¡»æ‰‹åŠ¨æŒ‡å®š")

        path_obj = Path(path)

        # å¦‚æœæ˜¯æ–‡ä»¶è·¯å¾„ï¼Œç¡®ä¿çˆ¶ç›®å½•å­˜åœ¨
        if is_file:
            if path_obj.is_dir():
                path_obj = path_obj.joinpath(file_name) if file_name else path_obj
            else:
                path_obj.parent.mkdir(parents=True, exist_ok=True)
        else:
            # å¦‚æœæ˜¯ç›®å½•è·¯å¾„ï¼Œç¡®ä¿ç›®å½•å­˜åœ¨
            path_obj.mkdir(parents=True, exist_ok=True)

        return path_obj

    def analyze_universe_data_requirements(
        self,
        universe_def: UniverseDefinition,
        buffer_days: int = 7,
        extend_to_present: bool = True,
    ) -> dict[str, Any]:
        """åˆ†æuniverseæ•°æ®ä¸‹è½½éœ€æ±‚."""
        import pandas as pd

        # æ”¶é›†æ‰€æœ‰çš„äº¤æ˜“å¯¹å’Œå®é™…ä½¿ç”¨æ—¶é—´èŒƒå›´
        all_symbols = set()
        usage_dates = []
        calculation_dates = []

        for snapshot in universe_def.snapshots:
            all_symbols.update(snapshot.symbols)

            # ä½¿ç”¨æœŸé—´ - å®é™…éœ€è¦ä¸‹è½½çš„æ•°æ®
            usage_dates.extend([snapshot.start_date, snapshot.end_date])

            # è®¡ç®—æœŸé—´ - ç”¨äºå®šä¹‰universeçš„æ•°æ®
            calculation_dates.extend(
                [
                    snapshot.calculated_t1_start,
                    snapshot.calculated_t1_end,
                    snapshot.effective_date,
                ]
            )

        # è®¡ç®—æ€»ä½“æ—¶é—´èŒƒå›´
        start_date = pd.to_datetime(min(usage_dates), utc=True) - timedelta(days=buffer_days)
        end_date = pd.to_datetime(max(usage_dates), utc=True) + timedelta(days=buffer_days)

        if extend_to_present:
            end_date = max(end_date, pd.to_datetime("today", utc=True))

        return {
            "unique_symbols": sorted(all_symbols),
            "total_symbols": len(all_symbols),
            "overall_start_date": start_date.strftime("%Y-%m-%d"),
            "overall_end_date": end_date.strftime("%Y-%m-%d"),
            "usage_period_start": pd.to_datetime(min(usage_dates), utc=True).strftime("%Y-%m-%d"),
            "usage_period_end": pd.to_datetime(max(usage_dates), utc=True).strftime("%Y-%m-%d"),
            "calculation_period_start": pd.to_datetime(min(calculation_dates), utc=True).strftime("%Y-%m-%d"),
            "calculation_period_end": pd.to_datetime(max(calculation_dates), utc=True).strftime("%Y-%m-%d"),
            "snapshots_count": len(universe_def.snapshots),
            "note": "æ¨èä½¿ç”¨download_universe_data_by_periodsæ–¹æ³•è¿›è¡Œç²¾ç¡®ä¸‹è½½",
        }
