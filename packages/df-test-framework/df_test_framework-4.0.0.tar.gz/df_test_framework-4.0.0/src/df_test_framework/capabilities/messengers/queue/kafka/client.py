"""Kafkaæ¶ˆæ¯é˜Ÿåˆ—å®¢æˆ·ç«¯

æä¾›Kafka Producerå’ŒConsumerçš„å°è£…ã€‚

v3.9.0æ–°å¢ - åŸºäºconfluent-kafka (librdkafka) å®ç°

ç‰ˆæœ¬è¯´æ˜ (v3.10.0+): ä½¿ç”¨ confluent-kafka>=2.12.0
- Windows é¢„ç¼–è¯‘ wheel æ”¯æŒï¼ŒPython 3.12+ å¯ç›´æ¥å®‰è£…
- SSL é—®é¢˜è¯´æ˜: 2.0+ ç‰ˆæœ¬åœ¨æŸäº›ç¯å¢ƒå¯èƒ½é‡åˆ° SSL_HANDSHAKE é”™è¯¯
  â†’ ä½¿ç”¨ KafkaSSLConfig çš„ workaround é…ç½®å¯è§£å†³ï¼ˆè§ config.pyï¼‰

v3.14.0 æ–°å¢:
- é›†æˆ EventBus å‘å¸ƒæ¶ˆæ¯äº‹ä»¶
- æ”¯æŒ event_bus å‚æ•°

ä½¿ç”¨ç¤ºä¾‹::

    from df_test_framework.capabilities.messengers.queue.kafka import (
        KafkaClient, KafkaConfig, KafkaProducerConfig
    )

    # åŸºæœ¬é…ç½®
    config = KafkaConfig(
        bootstrap_servers=["localhost:9092"],
        producer=KafkaProducerConfig()
    )
    client = KafkaClient(config)

    # å‘é€æ¶ˆæ¯
    client.send("test-topic", {"user_id": 123, "action": "login"})

    # æ¶ˆè´¹æ¶ˆæ¯
    messages = []
    client.consume(
        topics=["test-topic"],
        group_id="test-group",
        handler=lambda msg: messages.append(msg),
        max_messages=10
    )

    # å…³é—­å®¢æˆ·ç«¯
    client.close()
"""

from __future__ import annotations

import json
import time
from collections.abc import Callable
from typing import TYPE_CHECKING, Any

from df_test_framework.core.events import (
    MessageConsumeEndEvent,
    MessageConsumeErrorEvent,
    MessageConsumeStartEvent,
    MessagePublishEndEvent,
    MessagePublishErrorEvent,
    MessagePublishStartEvent,
)
from df_test_framework.infrastructure.logging import get_logger

from .config import KafkaConfig, KafkaConsumerConfig

if TYPE_CHECKING:
    from df_test_framework.infrastructure.events import EventBus

try:
    from confluent_kafka import Consumer, KafkaError, KafkaException, Producer
    from confluent_kafka.admin import AdminClient, NewTopic
except ImportError:
    raise ImportError("confluent-kafka æœªå®‰è£…ã€‚è¯·è¿è¡Œ: pip install 'df-test-framework[kafka]'")

logger = get_logger(__name__)


class KafkaClient:
    """Kafkaæ¶ˆæ¯é˜Ÿåˆ—å®¢æˆ·ç«¯

    åŸºäºconfluent-kafka (librdkafka) çš„Producerå’ŒConsumerå°è£…,
    æä¾›ç®€åŒ–çš„æ¶ˆæ¯å‘é€å’Œæ¶ˆè´¹æ¥å£ã€‚

    ç›¸æ¯”kafka-python3çš„ä¼˜åŠ¿:
    - æ€§èƒ½æå‡: ç”Ÿäº§æ€§èƒ½æå‡3å€,æ¶ˆè´¹æ€§èƒ½æå‡50%
    - ä¼ä¸šçº§ç‰¹æ€§: æ”¯æŒäº‹åŠ¡ã€å¹‚ç­‰æ€§ã€Avroåºåˆ—åŒ–
    - æ´»è·ƒç»´æŠ¤: Confluentå®˜æ–¹æ”¯æŒ,æŒç»­æ›´æ–°

    Attributes:
        config: Kafkaé…ç½®
    """

    def __init__(self, config: KafkaConfig, event_bus: EventBus | None = None):
        """åˆå§‹åŒ–Kafkaå®¢æˆ·ç«¯

        Args:
            config: Kafkaé…ç½®å¯¹è±¡
            event_bus: ğŸ†• v3.14.0 äº‹ä»¶æ€»çº¿ï¼ˆå¯é€‰ï¼Œç”¨äºå‘å¸ƒæ¶ˆæ¯äº‹ä»¶ï¼‰
        """
        self.config = config
        self._event_bus = event_bus
        self._producer: Producer | None = None
        self._consumer: Consumer | None = None
        self._admin_client: AdminClient | None = None

    def _get_event_bus(self):
        """è·å– EventBus å®ä¾‹

        v3.46.1: ç®€åŒ–é€»è¾‘ï¼Œåªä½¿ç”¨æ„é€ å‡½æ•°ä¼ å…¥çš„ event_bus
        """
        return self._event_bus

    def _publish_event(self, event: Any) -> None:
        """å‘å¸ƒäº‹ä»¶åˆ° EventBusï¼ˆåŒæ­¥æ¨¡å¼ï¼‰

        v3.18.0: æ”¹ç”¨ publish_sync() ç¡®ä¿äº‹ä»¶å®Œæ•´æ€§
        """
        event_bus = self._get_event_bus()
        if event_bus:
            try:
                event_bus.publish_sync(event)
            except Exception:
                pass  # é™é»˜å¤±è´¥ï¼Œä¸å½±å“ä¸»æµç¨‹

    def _get_producer(self) -> Producer:
        """è·å–æˆ–åˆ›å»ºProducerå®ä¾‹"""
        if self._producer is None:
            # æ„å»ºProduceré…ç½®
            producer_config = self.config.to_confluent_dict(include_producer=True)

            # æ·»åŠ deliveryæŠ¥å‘Šå›è°ƒ
            def delivery_report(err, msg):
                """Producer deliveryæŠ¥å‘Šå›è°ƒ"""
                if err is not None:
                    logger.error(f"æ¶ˆæ¯å‘é€å¤±è´¥: {err}")
                else:
                    logger.debug(
                        f"æ¶ˆæ¯å‘é€æˆåŠŸ: topic={msg.topic()}, "
                        f"partition={msg.partition()}, "
                        f"offset={msg.offset()}"
                    )

            self._producer = Producer(producer_config)
            logger.info(f"KafkaProduceråˆ›å»ºæˆåŠŸ: {self.config.bootstrap_servers}")

        return self._producer

    def send(
        self,
        topic: str,
        message: dict[str, Any],
        key: str | None = None,
        partition: int | None = None,
        headers: dict[str, str] | None = None,
        on_delivery: Callable | None = None,
    ) -> None:
        """å‘é€æ¶ˆæ¯åˆ°Kafkaä¸»é¢˜

        Args:
            topic: ä¸»é¢˜åç§°
            message: æ¶ˆæ¯å†…å®¹(å­—å…¸)
            key: æ¶ˆæ¯key(å¯é€‰,ç”¨äºåˆ†åŒº)
            partition: æŒ‡å®šåˆ†åŒº(å¯é€‰)
            headers: æ¶ˆæ¯å¤´(å¯é€‰,å­—å…¸å½¢å¼)
            on_delivery: è‡ªå®šä¹‰deliveryå›è°ƒ(å¯é€‰)

        Raises:
            KafkaException: å‘é€å¤±è´¥æ—¶æŠ›å‡º
        """
        # åºåˆ—åŒ–æ¶ˆæ¯
        value_bytes = json.dumps(message).encode("utf-8")
        body_size = len(value_bytes)

        # v3.34.1: å‘å¸ƒ Start äº‹ä»¶
        start_event, correlation_id = MessagePublishStartEvent.create(
            messenger_type="kafka",
            topic=topic,
            body_size=body_size,
            key=key,
            partition=partition,
            headers=headers or {},
        )
        self._publish_event(start_event)

        start_time = time.perf_counter()
        try:
            producer = self._get_producer()
            key_bytes = key.encode("utf-8") if key else None

            # è½¬æ¢headersæ ¼å¼: confluent-kafkaéœ€è¦list of tuples
            headers_list = None
            if headers:
                headers_list = [(k, v.encode("utf-8")) for k, v in headers.items()]

            # å‘é€æ¶ˆæ¯(å¼‚æ­¥)
            producer.produce(
                topic=topic,
                value=value_bytes,
                key=key_bytes,
                partition=partition if partition is not None else -1,
                headers=headers_list,
                on_delivery=on_delivery,
            )

            # è§¦å‘å‘é€(éé˜»å¡)
            producer.poll(0)

            # v3.34.1: å‘å¸ƒ End äº‹ä»¶
            duration = time.perf_counter() - start_time
            end_event = MessagePublishEndEvent.create(
                correlation_id=correlation_id,
                messenger_type="kafka",
                topic=topic,
                duration=duration,
                partition=partition,
            )
            self._publish_event(end_event)

        except BufferError as e:
            # æœ¬åœ°é˜Ÿåˆ—æ»¡äº†,éœ€è¦flush
            logger.warning(f"æœ¬åœ°é˜Ÿåˆ—æ»¡,ç­‰å¾…flush: {e}")
            producer.flush()
            # é‡è¯•
            producer.produce(topic, value_bytes, key_bytes, partition, headers_list, on_delivery)
            producer.poll(0)

            # v3.34.1: é‡è¯•æˆåŠŸï¼Œå‘å¸ƒ End äº‹ä»¶
            duration = time.perf_counter() - start_time
            end_event = MessagePublishEndEvent.create(
                correlation_id=correlation_id,
                messenger_type="kafka",
                topic=topic,
                duration=duration,
                partition=partition,
            )
            self._publish_event(end_event)

        except Exception as e:
            # v3.34.1: å‘å¸ƒ Error äº‹ä»¶
            duration = time.perf_counter() - start_time
            error_event = MessagePublishErrorEvent.create(
                correlation_id=correlation_id,
                messenger_type="kafka",
                topic=topic,
                error=e,
                duration=duration,
            )
            self._publish_event(error_event)

            logger.error(f"å‘é€æ¶ˆæ¯å¤±è´¥: {e}")
            raise

    def send_sync(
        self,
        topic: str,
        message: dict[str, Any],
        key: str | None = None,
        partition: int | None = None,
        headers: dict[str, str] | None = None,
        timeout: float = 10.0,
    ) -> dict[str, Any]:
        """åŒæ­¥å‘é€æ¶ˆæ¯(ç­‰å¾…ç¡®è®¤)

        Args:
            topic: ä¸»é¢˜åç§°
            message: æ¶ˆæ¯å†…å®¹(å­—å…¸)
            key: æ¶ˆæ¯key(å¯é€‰)
            partition: æŒ‡å®šåˆ†åŒº(å¯é€‰)
            headers: æ¶ˆæ¯å¤´(å¯é€‰)
            timeout: è¶…æ—¶æ—¶é—´(ç§’)

        Returns:
            å‘é€ç»“æœ: {"topic": str, "partition": int, "offset": int}

        Raises:
            KafkaException: å‘é€å¤±è´¥æ—¶æŠ›å‡º
        """
        result = {"topic": None, "partition": None, "offset": None}

        def sync_callback(err, msg):
            """åŒæ­¥å›è°ƒ,è®°å½•ç»“æœ"""
            if err is not None:
                raise KafkaException(err)
            result["topic"] = msg.topic()
            result["partition"] = msg.partition()
            result["offset"] = msg.offset()

        # å‘é€æ¶ˆæ¯
        self.send(topic, message, key, partition, headers, on_delivery=sync_callback)

        # ç­‰å¾…å‘é€å®Œæˆ
        producer = self._get_producer()
        remaining = producer.flush(timeout=timeout)

        if remaining > 0:
            raise TimeoutError(f"å‘é€è¶…æ—¶: {remaining} æ¡æ¶ˆæ¯æœªç¡®è®¤")

        return result

    def send_batch(
        self,
        topic: str,
        messages: list[dict[str, Any]],
        key_func: Callable[[dict], str] | None = None,
    ) -> int:
        """æ‰¹é‡å‘é€æ¶ˆæ¯(å¼‚æ­¥)

        Args:
            topic: ä¸»é¢˜åç§°
            messages: æ¶ˆæ¯åˆ—è¡¨
            key_func: å¯é€‰çš„keyæå–å‡½æ•°

        Returns:
            æˆåŠŸå‘é€çš„æ¶ˆæ¯æ•°é‡
        """
        success_count = 0

        for message in messages:
            try:
                key = key_func(message) if key_func else None
                self.send(topic, message, key=key)
                success_count += 1
            except Exception as e:
                logger.error(f"æ‰¹é‡å‘é€å¤±è´¥: {e}, message={message}")

        # Flushç¡®ä¿æ‰€æœ‰æ¶ˆæ¯å‘é€
        producer = self._get_producer()
        remaining = producer.flush(timeout=30)

        if remaining > 0:
            logger.warning(f"æ‰¹é‡å‘é€æœ‰ {remaining} æ¡æ¶ˆæ¯æœªç¡®è®¤")

        logger.info(f"æ‰¹é‡å‘é€å®Œæˆ: {success_count}/{len(messages)}")
        return success_count

    def consume(
        self,
        topics: list[str],
        group_id: str,
        handler: Callable[[dict[str, Any]], None],
        max_messages: int | None = None,
        consumer_config: KafkaConsumerConfig | None = None,
        timeout: float = 1.0,
        max_idle_seconds: float | None = None,
    ) -> int:
        """æ¶ˆè´¹Kafkaæ¶ˆæ¯

        Args:
            topics: ä¸»é¢˜åˆ—è¡¨
            group_id: æ¶ˆè´¹è€…ç»„ID
            handler: æ¶ˆæ¯å¤„ç†å‡½æ•°
            max_messages: æœ€å¤§æ¶ˆè´¹æ¶ˆæ¯æ•°(Noneè¡¨ç¤ºæŒç»­æ¶ˆè´¹)
            consumer_config: Consumeré…ç½®(å¯é€‰)
            timeout: pollè¶…æ—¶æ—¶é—´(ç§’)
            max_idle_seconds: æœ€é•¿ç©ºé—²ç­‰å¾…æ—¶é—´(ç§’)ï¼Œè¶…è¿‡åˆ™é€€å‡ºæ¶ˆè´¹

        Returns:
            å·²æ¶ˆè´¹çš„æ¶ˆæ¯æ•°é‡
        """
        # æ„å»ºConsumeré…ç½®
        if consumer_config is None:
            if self.config.consumer is None:
                consumer_config = KafkaConsumerConfig(group_id=group_id)
            else:
                consumer_config = self.config.consumer
                consumer_config.group_id = group_id
        else:
            consumer_config.group_id = group_id

        config = self.config.to_confluent_dict(include_consumer=True)
        config.update(consumer_config.to_confluent_dict())

        # åˆ›å»ºConsumer
        consumer = Consumer(config)
        consumer.subscribe(topics)
        self._consumer = consumer

        logger.info(f"å¼€å§‹æ¶ˆè´¹: topics={topics}, group_id={group_id}")

        message_count = 0
        last_message_time = time.monotonic()
        try:
            while True:
                msg = consumer.poll(timeout=timeout)

                if msg is None:
                    # æ²¡æœ‰æ¶ˆæ¯,ç»§ç»­ç­‰å¾…
                    if max_idle_seconds is not None:
                        idle = time.monotonic() - last_message_time
                        if idle >= max_idle_seconds:
                            logger.info(f"ç©ºé—²è¶…è¿‡ {max_idle_seconds}sï¼Œåœæ­¢æ¶ˆè´¹")
                            break
                    continue

                if msg.error():
                    if msg.error().code() == KafkaError._PARTITION_EOF:
                        # åˆ°è¾¾åˆ†åŒºæœ«å°¾
                        logger.debug(f"åˆ°è¾¾åˆ†åŒºæœ«å°¾: {msg.topic()}[{msg.partition()}]")
                        continue
                    else:
                        # é”™è¯¯
                        logger.error(f"æ¶ˆè´¹é”™è¯¯: {msg.error()}")
                        raise KafkaException(msg.error())

                # å¤„ç†æ¶ˆæ¯
                # v3.34.1: å‘å¸ƒ Start äº‹ä»¶
                body_size = len(msg.value()) if msg.value() else 0
                start_event, correlation_id = MessageConsumeStartEvent.create(
                    messenger_type="kafka",
                    topic=msg.topic(),
                    consumer_group=group_id,
                    partition=msg.partition(),
                    offset=msg.offset(),
                    body_size=body_size,
                )
                self._publish_event(start_event)

                process_start = time.perf_counter()
                try:
                    value = json.loads(msg.value().decode("utf-8"))
                    handler(value)
                    message_count += 1
                    last_message_time = time.monotonic()

                    # æ‰‹åŠ¨æäº¤offset (å½“auto_commitç¦ç”¨æ—¶)
                    if not consumer_config.enable_auto_commit:
                        consumer.commit(message=msg, asynchronous=False)

                    # v3.34.1: å‘å¸ƒ End äº‹ä»¶
                    processing_time = time.perf_counter() - process_start
                    end_event = MessageConsumeEndEvent.create(
                        correlation_id=correlation_id,
                        messenger_type="kafka",
                        topic=msg.topic(),
                        consumer_group=group_id,
                        processing_time=processing_time,
                        partition=msg.partition(),
                        offset=msg.offset(),
                    )
                    self._publish_event(end_event)

                    logger.debug(
                        f"æ¶ˆæ¯å¤„ç†æˆåŠŸ: topic={msg.topic()}, "
                        f"partition={msg.partition()}, "
                        f"offset={msg.offset()}"
                    )

                    # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°æœ€å¤§æ¶ˆè´¹æ•°
                    if max_messages and message_count >= max_messages:
                        logger.info(f"è¾¾åˆ°æœ€å¤§æ¶ˆè´¹æ•°é‡: {max_messages}")
                        break

                except Exception as e:
                    # v3.34.1: å‘å¸ƒ Error äº‹ä»¶
                    processing_time = time.perf_counter() - process_start
                    error_event = MessageConsumeErrorEvent.create(
                        correlation_id=correlation_id,
                        messenger_type="kafka",
                        topic=msg.topic(),
                        consumer_group=group_id,
                        error=e,
                        processing_time=processing_time,
                    )
                    self._publish_event(error_event)

                    logger.error(f"æ¶ˆæ¯å¤„ç†å¤±è´¥: {e}, message={msg.value()}")

        except KeyboardInterrupt:
            logger.info("æ¶ˆè´¹è¢«ä¸­æ–­")
        finally:
            consumer.close()
            self._consumer = None
            logger.info(f"æ¶ˆè´¹å®Œæˆ: {message_count} æ¡æ¶ˆæ¯")

        return message_count

    def create_topic(
        self,
        topic: str,
        num_partitions: int = 1,
        replication_factor: int = 1,
        config: dict[str, str] | None = None,
        timeout: float = 10.0,
    ) -> None:
        """åˆ›å»ºKafkaä¸»é¢˜

        Args:
            topic: ä¸»é¢˜åç§°
            num_partitions: åˆ†åŒºæ•°
            replication_factor: å‰¯æœ¬å› å­
            config: ä¸»é¢˜é…ç½®(å¯é€‰)
            timeout: è¶…æ—¶æ—¶é—´(ç§’)

        Raises:
            KafkaException: åˆ›å»ºå¤±è´¥æ—¶æŠ›å‡º
        """
        if self._admin_client is None:
            admin_config = self.config.to_confluent_dict()
            self._admin_client = AdminClient(admin_config)

        new_topic = NewTopic(
            topic=topic,
            num_partitions=num_partitions,
            replication_factor=replication_factor,
            config=config or {},
        )

        # åˆ›å»ºä¸»é¢˜
        fs = self._admin_client.create_topics([new_topic])

        # ç­‰å¾…ç»“æœ
        for topic_name, f in fs.items():
            try:
                f.result(timeout=timeout)
                logger.info(f"ä¸»é¢˜åˆ›å»ºæˆåŠŸ: {topic_name}")
            except Exception as e:
                logger.error(f"ä¸»é¢˜åˆ›å»ºå¤±è´¥: {topic_name}, error={e}")
                raise

    def delete_topic(self, topic: str, timeout: float = 10.0) -> None:
        """åˆ é™¤Kafkaä¸»é¢˜

        Args:
            topic: ä¸»é¢˜åç§°
            timeout: è¶…æ—¶æ—¶é—´(ç§’)

        Raises:
            KafkaException: åˆ é™¤å¤±è´¥æ—¶æŠ›å‡º
        """
        if self._admin_client is None:
            admin_config = self.config.to_confluent_dict()
            self._admin_client = AdminClient(admin_config)

        fs = self._admin_client.delete_topics([topic])

        for topic_name, f in fs.items():
            try:
                f.result(timeout=timeout)
                logger.info(f"ä¸»é¢˜åˆ é™¤æˆåŠŸ: {topic_name}")
            except Exception as e:
                logger.error(f"ä¸»é¢˜åˆ é™¤å¤±è´¥: {topic_name}, error={e}")
                raise

    def flush(self, timeout: float = 10.0) -> int:
        """åˆ·æ–°Produceré˜Ÿåˆ—,ç­‰å¾…æ‰€æœ‰æ¶ˆæ¯å‘é€å®Œæˆ

        Args:
            timeout: è¶…æ—¶æ—¶é—´(ç§’)

        Returns:
            æœªå‘é€å®Œæˆçš„æ¶ˆæ¯æ•°é‡(0è¡¨ç¤ºå…¨éƒ¨å‘é€å®Œæˆ)
        """
        if self._producer:
            remaining = self._producer.flush(timeout=timeout)
            logger.info(f"Producer flushå®Œæˆ: {remaining} æ¡æ¶ˆæ¯æœªå‘é€")
            return remaining
        return 0

    def close(self) -> None:
        """å…³é—­å®¢æˆ·ç«¯,é‡Šæ”¾èµ„æº"""
        # Flush Produceré˜Ÿåˆ—
        if self._producer:
            self._producer.flush()
            logger.info("KafkaProducerå·²å…³é—­")
            self._producer = None

        # å…³é—­Consumer
        if self._consumer:
            self._consumer.close()
            logger.info("KafkaConsumerå·²å…³é—­")
            self._consumer = None


__all__ = ["KafkaClient"]
