from .. import graph_components as _graph_components
from .. import _internal_utils
from .. import config as _config
from .. import utils as _utils


class CreateMatrixFromTicks(_graph_components.EpBase):
	"""
		

CREATE_MATRIX_FROM_TICKS

Type: Aggregation

Description: Creates a matrix field from specified input
fields for each bucket.

Python
class name:&nbsp;CreateMatrixFromTicks

Input: Time series of ticks

Output: A time series of ticks, one for each bucket, with
specified matrix fields and propagated fields

Parameters: See parameters
common to generic aggregations.


  BUCKET_INTERVAL
(seconds/ticks)
  BUCKET_INTERVAL_UNITS
(enumerated type)
  OUTPUT_INTERVAL
(seconds)
  OUTPUT_INTERVAL_UNITS
(SECONDS/TICKS)
  IS_RUNNING_AGGR
(Boolean)
  BUCKET_TIME
(BUCKET_START/BUCKET_END)
  BUCKET_END_CRITERIA
(expression)
  BOUNDARY_TICK_BUCKET
(NEW/PREVIOUS)
  PARTIAL_BUCKET_HANDLING
(enumerated type)
  OUTPUT_FIELD_NAMES (a list of
output fields)
    The list of fields should be specified in the following format:

    &lt;INPUT_FIELD_NAME_1&gt;,&lt;MATRIX_FIELD_NAME_1&gt;(&lt;INPUT_FIELD_NAME_1&gt;,&lt;INPUT_FIELD_NAME_3&gt;),&lt;INPUT_FIELD_NAME_2&gt;,&lt;MATRIX_FIELD_NAME_2&gt;(&lt;INPUT_FIELD_NAME_3&gt;)

    Output field names should be one of the following:

    
      One of the input field names, in which case the input field
will be propagated (like &lt;INPUT_FIELD_NAME_1&gt;,&lt;INPUT_FIELD_NAME_2&gt;,...
above).
      A new name (that will define a matrix field, like &lt;MATRIX_FIELD_NAME_1&gt;,&lt;MATRIX_FIELD_NAME_2&gt;
above) along input field name/names(like &lt;INPUT_FIELD_NAME_1&gt;,&lt;INPUT_FIELD_NAME_2&gt;,...
        above) specified in brackets. In this case, a matrix
will be constructed from the specified field/fields.
    
  
  TICK_IN_A_ROW (Boolean)
    In case if specified value is true, input tick's fields going to
be written in same row, otherwise in same column. 

    Default: true

  

Note: All specified matrix output fields must be of the same
type.

Example:

See MATRICES_EXAMPLES.otq.




	"""
	class Parameters:
		bucket_interval = "BUCKET_INTERVAL"
		bucket_interval_units = "BUCKET_INTERVAL_UNITS"
		output_interval = "OUTPUT_INTERVAL"
		output_interval_units = "OUTPUT_INTERVAL_UNITS"
		is_running_aggr = "IS_RUNNING_AGGR"
		bucket_time = "BUCKET_TIME"
		bucket_end_criteria = "BUCKET_END_CRITERIA"
		boundary_tick_bucket = "BOUNDARY_TICK_BUCKET"
		partial_bucket_handling = "PARTIAL_BUCKET_HANDLING"
		output_field_names = "OUTPUT_FIELD_NAMES"
		tick_in_a_row = "TICK_IN_A_ROW"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["bucket_interval", "bucket_interval_units", "output_interval", "output_interval_units", "is_running_aggr", "bucket_time", "bucket_end_criteria", "boundary_tick_bucket", "partial_bucket_handling", "output_field_names", "tick_in_a_row"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["bucket_interval", "_default_bucket_interval", "bucket_interval_units", "_default_bucket_interval_units", "output_interval", "_default_output_interval", "output_interval_units", "_default_output_interval_units", "is_running_aggr", "_default_is_running_aggr", "bucket_time", "_default_bucket_time", "bucket_end_criteria", "_default_bucket_end_criteria", "boundary_tick_bucket", "_default_boundary_tick_bucket", "partial_bucket_handling", "_default_partial_bucket_handling", "output_field_names", "_default_output_field_names", "tick_in_a_row", "_default_tick_in_a_row", "stack_info", "_used_strings"]

	class BucketIntervalUnits:
		DAYS = "DAYS"
		FLEXIBLE = "FLEXIBLE"
		MONTHS = "MONTHS"
		SECONDS = "SECONDS"
		TICKS = "TICKS"

	class OutputIntervalUnits:
		SECONDS = "SECONDS"
		TICKS = "TICKS"

	class BucketTime:
		BUCKET_END = "BUCKET_END"
		BUCKET_START = "BUCKET_START"

	class BoundaryTickBucket:
		NEW = "NEW"
		PREVIOUS = "PREVIOUS"

	class PartialBucketHandling:
		AS_SEPARATE_BUCKET = "AS_SEPARATE_BUCKET"
		DISCARD = "DISCARD"
		MERGE_WITH_PREVIOUS = "MERGE_WITH_PREVIOUS"

	def __init__(self, bucket_interval=0, bucket_interval_units=BucketIntervalUnits.SECONDS, output_interval="", output_interval_units=OutputIntervalUnits.SECONDS, is_running_aggr=False, bucket_time=BucketTime.BUCKET_END, bucket_end_criteria="", boundary_tick_bucket=BoundaryTickBucket.NEW, partial_bucket_handling=PartialBucketHandling.AS_SEPARATE_BUCKET, output_field_names="", tick_in_a_row=True):
		_graph_components.EpBase.__init__(self, "CREATE_MATRIX_FROM_TICKS")
		self._default_bucket_interval = 0
		self.bucket_interval = bucket_interval
		self._default_bucket_interval_units = type(self).BucketIntervalUnits.SECONDS
		self.bucket_interval_units = bucket_interval_units
		self._default_output_interval = ""
		self.output_interval = output_interval
		self._default_output_interval_units = type(self).OutputIntervalUnits.SECONDS
		self.output_interval_units = output_interval_units
		self._default_is_running_aggr = False
		self.is_running_aggr = is_running_aggr
		self._default_bucket_time = type(self).BucketTime.BUCKET_END
		self.bucket_time = bucket_time
		self._default_bucket_end_criteria = ""
		self.bucket_end_criteria = bucket_end_criteria
		self._default_boundary_tick_bucket = type(self).BoundaryTickBucket.NEW
		self.boundary_tick_bucket = boundary_tick_bucket
		self._default_partial_bucket_handling = type(self).PartialBucketHandling.AS_SEPARATE_BUCKET
		self.partial_bucket_handling = partial_bucket_handling
		self._default_output_field_names = ""
		self.output_field_names = output_field_names
		self._default_tick_in_a_row = True
		self.tick_in_a_row = tick_in_a_row
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_bucket_interval(self, value):
		self.bucket_interval = value
		return self

	def set_bucket_interval_units(self, value):
		self.bucket_interval_units = value
		return self

	def set_output_interval(self, value):
		self.output_interval = value
		return self

	def set_output_interval_units(self, value):
		self.output_interval_units = value
		return self

	def set_is_running_aggr(self, value):
		self.is_running_aggr = value
		return self

	def set_bucket_time(self, value):
		self.bucket_time = value
		return self

	def set_bucket_end_criteria(self, value):
		self.bucket_end_criteria = value
		return self

	def set_boundary_tick_bucket(self, value):
		self.boundary_tick_bucket = value
		return self

	def set_partial_bucket_handling(self, value):
		self.partial_bucket_handling = value
		return self

	def set_output_field_names(self, value):
		self.output_field_names = value
		return self

	def set_tick_in_a_row(self, value):
		self.tick_in_a_row = value
		return self

	@staticmethod
	def _get_name():
		return "CREATE_MATRIX_FROM_TICKS"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.bucket_interval != 0: 
			desc += "BUCKET_INTERVAL=" + py_to_str(self.bucket_interval) + ","
		if self.bucket_interval_units != self.BucketIntervalUnits.SECONDS: 
			desc += "BUCKET_INTERVAL_UNITS=" + py_to_str(self.bucket_interval_units) + ","
		if self.output_interval != "": 
			desc += "OUTPUT_INTERVAL=" + py_to_str(self.output_interval) + ","
		if self.output_interval_units != self.OutputIntervalUnits.SECONDS: 
			desc += "OUTPUT_INTERVAL_UNITS=" + py_to_str(self.output_interval_units) + ","
		if self.is_running_aggr != False: 
			desc += "IS_RUNNING_AGGR=" + py_to_str(self.is_running_aggr) + ","
		if self.bucket_time != self.BucketTime.BUCKET_END: 
			desc += "BUCKET_TIME=" + py_to_str(self.bucket_time) + ","
		if self.bucket_end_criteria != "": 
			desc += "BUCKET_END_CRITERIA=" + py_to_str(self.bucket_end_criteria) + ","
		if self.boundary_tick_bucket != self.BoundaryTickBucket.NEW: 
			desc += "BOUNDARY_TICK_BUCKET=" + py_to_str(self.boundary_tick_bucket) + ","
		if self.partial_bucket_handling != self.PartialBucketHandling.AS_SEPARATE_BUCKET: 
			desc += "PARTIAL_BUCKET_HANDLING=" + py_to_str(self.partial_bucket_handling) + ","
		if self.output_field_names != "": 
			desc += "OUTPUT_FIELD_NAMES=" + py_to_str(self.output_field_names) + ","
		if self.tick_in_a_row != True: 
			desc += "TICK_IN_A_ROW=" + py_to_str(self.tick_in_a_row) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class Vwap(_graph_components.EpBase):
	"""
		

VWAP

Type: Aggregation

Description: For each bucket, computes the volume-weighted
average price.

Python
class name:
Vwap

Input: A time series of ticks containing numeric fields named
PRICE and SIZE.

Output: A time series of ticks.

Parameters: See parameters
common to generic aggregations.


  BUCKET_INTERVAL
(seconds/ticks)
  BUCKET_INTERVAL_UNITS
(enumerated type)
  OUTPUT_INTERVAL
(seconds)
  OUTPUT_INTERVAL_UNITS
(enumerated type)
  IS_RUNNING_AGGR
(Boolean)
  BUCKET_TIME
(Boolean)
  BUCKET_END_CRITERIA
(expression)
  BOUNDARY_TICK_BUCKET
(NEW/PREVIOUS)
  ALL_FIELDS_FOR_SLIDING
(Boolean)
  PARTIAL_BUCKET_HANDLING
(enumerated type)
  OUTPUT_FIELD_NAME
(string)
  GROUP_BY
(string)
  GROUPS_TO_DISPLAY
(enumerated)
  BUCKET_END_PER_GROUP
(Boolean)
  PRICE_FIELD_NAME (string)
    The name of the field carrying the price value.
Default: PRICE

  
  SIZE_FIELD_NAME (string)
    The name of the field carrying the size value.
Default: SIZE

  

Notes: See the notes on generic
aggregations.

Examples:

See the VWAP_and_Running_VWAP
example in AGGREGATION_EXAMPLES.otq.


	"""
	class Parameters:
		bucket_interval = "BUCKET_INTERVAL"
		bucket_interval_units = "BUCKET_INTERVAL_UNITS"
		output_interval = "OUTPUT_INTERVAL"
		output_interval_units = "OUTPUT_INTERVAL_UNITS"
		is_running_aggr = "IS_RUNNING_AGGR"
		bucket_time = "BUCKET_TIME"
		bucket_end_criteria = "BUCKET_END_CRITERIA"
		boundary_tick_bucket = "BOUNDARY_TICK_BUCKET"
		all_fields_for_sliding = "ALL_FIELDS_FOR_SLIDING"
		partial_bucket_handling = "PARTIAL_BUCKET_HANDLING"
		output_field_name = "OUTPUT_FIELD_NAME"
		group_by = "GROUP_BY"
		groups_to_display = "GROUPS_TO_DISPLAY"
		bucket_end_per_group = "BUCKET_END_PER_GROUP"
		price_field_name = "PRICE_FIELD_NAME"
		size_field_name = "SIZE_FIELD_NAME"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["bucket_interval", "bucket_interval_units", "output_interval", "output_interval_units", "is_running_aggr", "bucket_time", "bucket_end_criteria", "boundary_tick_bucket", "all_fields_for_sliding", "partial_bucket_handling", "output_field_name", "group_by", "groups_to_display", "bucket_end_per_group", "price_field_name", "size_field_name"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["bucket_interval", "_default_bucket_interval", "bucket_interval_units", "_default_bucket_interval_units", "output_interval", "_default_output_interval", "output_interval_units", "_default_output_interval_units", "is_running_aggr", "_default_is_running_aggr", "bucket_time", "_default_bucket_time", "bucket_end_criteria", "_default_bucket_end_criteria", "boundary_tick_bucket", "_default_boundary_tick_bucket", "all_fields_for_sliding", "_default_all_fields_for_sliding", "partial_bucket_handling", "_default_partial_bucket_handling", "output_field_name", "_default_output_field_name", "group_by", "_default_group_by", "groups_to_display", "_default_groups_to_display", "bucket_end_per_group", "_default_bucket_end_per_group", "price_field_name", "_default_price_field_name", "size_field_name", "_default_size_field_name", "stack_info", "_used_strings"]

	class BucketIntervalUnits:
		DAYS = "DAYS"
		FLEXIBLE = "FLEXIBLE"
		MONTHS = "MONTHS"
		SECONDS = "SECONDS"
		TICKS = "TICKS"

	class OutputIntervalUnits:
		SECONDS = "SECONDS"
		TICKS = "TICKS"

	class BucketTime:
		BUCKET_END = "BUCKET_END"
		BUCKET_START = "BUCKET_START"

	class BoundaryTickBucket:
		NEW = "NEW"
		PREVIOUS = "PREVIOUS"

	class AllFieldsForSliding:
		WHEN_TICKS_EXIT_WINDOW = "WHEN_TICKS_EXIT_WINDOW"
		FALSE = "false"
		TRUE = "true"

	class PartialBucketHandling:
		AS_SEPARATE_BUCKET = "AS_SEPARATE_BUCKET"
		DISCARD = "DISCARD"
		MERGE_WITH_PREVIOUS = "MERGE_WITH_PREVIOUS"

	class GroupsToDisplay:
		ALL = "ALL"
		EVENT_IN_LAST_BUCKET = "EVENT_IN_LAST_BUCKET"

	def __init__(self, bucket_interval=0, bucket_interval_units=BucketIntervalUnits.SECONDS, output_interval="", output_interval_units=OutputIntervalUnits.SECONDS, is_running_aggr=False, bucket_time=BucketTime.BUCKET_END, bucket_end_criteria="", boundary_tick_bucket=BoundaryTickBucket.NEW, all_fields_for_sliding=AllFieldsForSliding.FALSE, partial_bucket_handling=PartialBucketHandling.AS_SEPARATE_BUCKET, output_field_name="VALUE", group_by="", groups_to_display=GroupsToDisplay.ALL, bucket_end_per_group=False, price_field_name="PRICE", size_field_name="SIZE", Out = ""):
		_graph_components.EpBase.__init__(self, "VWAP")
		self._default_bucket_interval = 0
		self.bucket_interval = bucket_interval
		self._default_bucket_interval_units = type(self).BucketIntervalUnits.SECONDS
		self.bucket_interval_units = bucket_interval_units
		self._default_output_interval = ""
		self.output_interval = output_interval
		self._default_output_interval_units = type(self).OutputIntervalUnits.SECONDS
		self.output_interval_units = output_interval_units
		self._default_is_running_aggr = False
		self.is_running_aggr = is_running_aggr
		self._default_bucket_time = type(self).BucketTime.BUCKET_END
		self.bucket_time = bucket_time
		self._default_bucket_end_criteria = ""
		self.bucket_end_criteria = bucket_end_criteria
		self._default_boundary_tick_bucket = type(self).BoundaryTickBucket.NEW
		self.boundary_tick_bucket = boundary_tick_bucket
		self._default_all_fields_for_sliding = type(self).AllFieldsForSliding.FALSE
		self.all_fields_for_sliding = all_fields_for_sliding
		self._default_partial_bucket_handling = type(self).PartialBucketHandling.AS_SEPARATE_BUCKET
		self.partial_bucket_handling = partial_bucket_handling
		self._default_output_field_name = "VALUE"
		self.output_field_name = output_field_name
		self._default_group_by = ""
		self.group_by = group_by
		self._default_groups_to_display = type(self).GroupsToDisplay.ALL
		self.groups_to_display = groups_to_display
		self._default_bucket_end_per_group = False
		self.bucket_end_per_group = bucket_end_per_group
		self._default_price_field_name = "PRICE"
		self.price_field_name = price_field_name
		self._default_size_field_name = "SIZE"
		self.size_field_name = size_field_name
		if Out != "":
			self.output_field_name=Out
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_bucket_interval(self, value):
		self.bucket_interval = value
		return self

	def set_bucket_interval_units(self, value):
		self.bucket_interval_units = value
		return self

	def set_output_interval(self, value):
		self.output_interval = value
		return self

	def set_output_interval_units(self, value):
		self.output_interval_units = value
		return self

	def set_is_running_aggr(self, value):
		self.is_running_aggr = value
		return self

	def set_bucket_time(self, value):
		self.bucket_time = value
		return self

	def set_bucket_end_criteria(self, value):
		self.bucket_end_criteria = value
		return self

	def set_boundary_tick_bucket(self, value):
		self.boundary_tick_bucket = value
		return self

	def set_all_fields_for_sliding(self, value):
		self.all_fields_for_sliding = value
		return self

	def set_partial_bucket_handling(self, value):
		self.partial_bucket_handling = value
		return self

	def set_output_field_name(self, value):
		self.output_field_name = value
		return self

	def set_group_by(self, value):
		self.group_by = value
		return self

	def set_groups_to_display(self, value):
		self.groups_to_display = value
		return self

	def set_bucket_end_per_group(self, value):
		self.bucket_end_per_group = value
		return self

	def set_price_field_name(self, value):
		self.price_field_name = value
		return self

	def set_size_field_name(self, value):
		self.size_field_name = value
		return self

	@staticmethod
	def _get_name():
		return "VWAP"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.bucket_interval != 0: 
			desc += "BUCKET_INTERVAL=" + py_to_str(self.bucket_interval) + ","
		if self.bucket_interval_units != self.BucketIntervalUnits.SECONDS: 
			desc += "BUCKET_INTERVAL_UNITS=" + py_to_str(self.bucket_interval_units) + ","
		if self.output_interval != "": 
			desc += "OUTPUT_INTERVAL=" + py_to_str(self.output_interval) + ","
		if self.output_interval_units != self.OutputIntervalUnits.SECONDS: 
			desc += "OUTPUT_INTERVAL_UNITS=" + py_to_str(self.output_interval_units) + ","
		if self.is_running_aggr != False: 
			desc += "IS_RUNNING_AGGR=" + py_to_str(self.is_running_aggr) + ","
		if self.bucket_time != self.BucketTime.BUCKET_END: 
			desc += "BUCKET_TIME=" + py_to_str(self.bucket_time) + ","
		if self.bucket_end_criteria != "": 
			desc += "BUCKET_END_CRITERIA=" + py_to_str(self.bucket_end_criteria) + ","
		if self.boundary_tick_bucket != self.BoundaryTickBucket.NEW: 
			desc += "BOUNDARY_TICK_BUCKET=" + py_to_str(self.boundary_tick_bucket) + ","
		if self.all_fields_for_sliding != self.AllFieldsForSliding.FALSE: 
			desc += "ALL_FIELDS_FOR_SLIDING=" + py_to_str(self.all_fields_for_sliding) + ","
		if self.partial_bucket_handling != self.PartialBucketHandling.AS_SEPARATE_BUCKET: 
			desc += "PARTIAL_BUCKET_HANDLING=" + py_to_str(self.partial_bucket_handling) + ","
		if self.output_field_name != "VALUE": 
			desc += "OUTPUT_FIELD_NAME=" + py_to_str(self.output_field_name) + ","
		if self.group_by != "": 
			desc += "GROUP_BY=" + py_to_str(self.group_by) + ","
		if self.groups_to_display != self.GroupsToDisplay.ALL: 
			desc += "GROUPS_TO_DISPLAY=" + py_to_str(self.groups_to_display) + ","
		if self.bucket_end_per_group != False: 
			desc += "BUCKET_END_PER_GROUP=" + py_to_str(self.bucket_end_per_group) + ","
		if self.price_field_name != "PRICE": 
			desc += "PRICE_FIELD_NAME=" + py_to_str(self.price_field_name) + ","
		if self.size_field_name != "SIZE": 
			desc += "SIZE_FIELD_NAME=" + py_to_str(self.size_field_name) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


WAvg = Vwap


class Sum(_graph_components.EpBase):
	"""
		

SUM

Type: Aggregation

Description: For each bucket, computes the total value of a
specified numeric attribute.

Python
class name:
Sum

Input: A time series of ticks.

Output: A time series of ticks, one for each bucket.

Parameters: See parameters
common to generic aggregations.


  BUCKET_INTERVAL
(seconds/ticks)
  BUCKET_INTERVAL_UNITS
(enumerated type)
  OUTPUT_INTERVAL
(seconds)
  OUTPUT_INTERVAL_UNITS
(enumerated type)
  IS_RUNNING_AGGR
(Boolean)
  BUCKET_TIME
(Boolean)
  BUCKET_END_CRITERIA
(expression)
  BOUNDARY_TICK_BUCKET
(NEW/PREVIOUS)
  ALL_FIELDS_FOR_SLIDING
(Boolean)
  PARTIAL_BUCKET_HANDLING
(enumerated type)
  OUTPUT_FIELD_NAME
(string)
  GROUP_BY
(string)
  GROUPS_TO_DISPLAY
(enumerated)
  BUCKET_END_PER_GROUP
(Boolean)
  INPUT_FIELD_NAME
(string)
EXPECT_DECIMALS (enumerated type)
This parameter should be set to true
if the input field of this EP may contain high-precision decimal values.
Such values require special handling and are represented using the
DECIMAL128 type. If set to true,
the output field will have the DECIMAL128 type.
When this parameter is set to
IF_INPUT_VAL_IS_DECIMAL, the EP behaves the same way as when this
parameter is set to true when the input
field is of type DECIMAL128, and the same way as when it is set to
false when the input field is not of type
DECIMAL128.
When set to IF_INPUT_VAL_IS_DECIMAL and the input field type
changes during execution (e.g., from DOUBLE to DECIMAL128 or from DECIMAL128
to DOUBLE), the output field type is switched accordingly. Switching from
DECIMAL128 to DOUBLE may result in loss of precision.
Default: false



Examples: See the SUM
example in AGGREGATION_EXAMPLES.otq.


	"""
	class Parameters:
		bucket_interval = "BUCKET_INTERVAL"
		bucket_interval_units = "BUCKET_INTERVAL_UNITS"
		output_interval = "OUTPUT_INTERVAL"
		output_interval_units = "OUTPUT_INTERVAL_UNITS"
		is_running_aggr = "IS_RUNNING_AGGR"
		bucket_time = "BUCKET_TIME"
		bucket_end_criteria = "BUCKET_END_CRITERIA"
		boundary_tick_bucket = "BOUNDARY_TICK_BUCKET"
		all_fields_for_sliding = "ALL_FIELDS_FOR_SLIDING"
		partial_bucket_handling = "PARTIAL_BUCKET_HANDLING"
		output_field_name = "OUTPUT_FIELD_NAME"
		group_by = "GROUP_BY"
		groups_to_display = "GROUPS_TO_DISPLAY"
		bucket_end_per_group = "BUCKET_END_PER_GROUP"
		input_field_name = "INPUT_FIELD_NAME"
		expect_decimals = "EXPECT_DECIMALS"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["bucket_interval", "bucket_interval_units", "output_interval", "output_interval_units", "is_running_aggr", "bucket_time", "bucket_end_criteria", "boundary_tick_bucket", "all_fields_for_sliding", "partial_bucket_handling", "output_field_name", "group_by", "groups_to_display", "bucket_end_per_group", "input_field_name", "expect_decimals"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["bucket_interval", "_default_bucket_interval", "bucket_interval_units", "_default_bucket_interval_units", "output_interval", "_default_output_interval", "output_interval_units", "_default_output_interval_units", "is_running_aggr", "_default_is_running_aggr", "bucket_time", "_default_bucket_time", "bucket_end_criteria", "_default_bucket_end_criteria", "boundary_tick_bucket", "_default_boundary_tick_bucket", "all_fields_for_sliding", "_default_all_fields_for_sliding", "partial_bucket_handling", "_default_partial_bucket_handling", "output_field_name", "_default_output_field_name", "group_by", "_default_group_by", "groups_to_display", "_default_groups_to_display", "bucket_end_per_group", "_default_bucket_end_per_group", "input_field_name", "_default_input_field_name", "expect_decimals", "_default_expect_decimals", "stack_info", "_used_strings"]

	class BucketIntervalUnits:
		DAYS = "DAYS"
		FLEXIBLE = "FLEXIBLE"
		MONTHS = "MONTHS"
		SECONDS = "SECONDS"
		TICKS = "TICKS"

	class OutputIntervalUnits:
		SECONDS = "SECONDS"
		TICKS = "TICKS"

	class BucketTime:
		BUCKET_END = "BUCKET_END"
		BUCKET_START = "BUCKET_START"

	class BoundaryTickBucket:
		NEW = "NEW"
		PREVIOUS = "PREVIOUS"

	class AllFieldsForSliding:
		WHEN_TICKS_EXIT_WINDOW = "WHEN_TICKS_EXIT_WINDOW"
		FALSE = "false"
		TRUE = "true"

	class PartialBucketHandling:
		AS_SEPARATE_BUCKET = "AS_SEPARATE_BUCKET"
		DISCARD = "DISCARD"
		MERGE_WITH_PREVIOUS = "MERGE_WITH_PREVIOUS"

	class GroupsToDisplay:
		ALL = "ALL"
		EVENT_IN_LAST_BUCKET = "EVENT_IN_LAST_BUCKET"

	class ExpectDecimals:
		IF_INPUT_VAL_IS_DECIMAL = "IF_INPUT_VAL_IS_DECIMAL"
		FALSE = "false"
		TRUE = "true"

	def __init__(self, bucket_interval=0, bucket_interval_units=BucketIntervalUnits.SECONDS, output_interval="", output_interval_units=OutputIntervalUnits.SECONDS, is_running_aggr=False, bucket_time=BucketTime.BUCKET_END, bucket_end_criteria="", boundary_tick_bucket=BoundaryTickBucket.NEW, all_fields_for_sliding=AllFieldsForSliding.FALSE, partial_bucket_handling=PartialBucketHandling.AS_SEPARATE_BUCKET, output_field_name="VALUE", group_by="", groups_to_display=GroupsToDisplay.ALL, bucket_end_per_group=False, input_field_name="", expect_decimals=ExpectDecimals.FALSE, In = "", Out = ""):
		_graph_components.EpBase.__init__(self, "SUM")
		self._default_bucket_interval = 0
		self.bucket_interval = bucket_interval
		self._default_bucket_interval_units = type(self).BucketIntervalUnits.SECONDS
		self.bucket_interval_units = bucket_interval_units
		self._default_output_interval = ""
		self.output_interval = output_interval
		self._default_output_interval_units = type(self).OutputIntervalUnits.SECONDS
		self.output_interval_units = output_interval_units
		self._default_is_running_aggr = False
		self.is_running_aggr = is_running_aggr
		self._default_bucket_time = type(self).BucketTime.BUCKET_END
		self.bucket_time = bucket_time
		self._default_bucket_end_criteria = ""
		self.bucket_end_criteria = bucket_end_criteria
		self._default_boundary_tick_bucket = type(self).BoundaryTickBucket.NEW
		self.boundary_tick_bucket = boundary_tick_bucket
		self._default_all_fields_for_sliding = type(self).AllFieldsForSliding.FALSE
		self.all_fields_for_sliding = all_fields_for_sliding
		self._default_partial_bucket_handling = type(self).PartialBucketHandling.AS_SEPARATE_BUCKET
		self.partial_bucket_handling = partial_bucket_handling
		self._default_output_field_name = "VALUE"
		self.output_field_name = output_field_name
		self._default_group_by = ""
		self.group_by = group_by
		self._default_groups_to_display = type(self).GroupsToDisplay.ALL
		self.groups_to_display = groups_to_display
		self._default_bucket_end_per_group = False
		self.bucket_end_per_group = bucket_end_per_group
		self._default_input_field_name = ""
		self.input_field_name = input_field_name
		self._default_expect_decimals = type(self).ExpectDecimals.FALSE
		self.expect_decimals = expect_decimals
		if In != "":
			self.input_field_name=In
		if Out != "":
			self.output_field_name=Out
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_bucket_interval(self, value):
		self.bucket_interval = value
		return self

	def set_bucket_interval_units(self, value):
		self.bucket_interval_units = value
		return self

	def set_output_interval(self, value):
		self.output_interval = value
		return self

	def set_output_interval_units(self, value):
		self.output_interval_units = value
		return self

	def set_is_running_aggr(self, value):
		self.is_running_aggr = value
		return self

	def set_bucket_time(self, value):
		self.bucket_time = value
		return self

	def set_bucket_end_criteria(self, value):
		self.bucket_end_criteria = value
		return self

	def set_boundary_tick_bucket(self, value):
		self.boundary_tick_bucket = value
		return self

	def set_all_fields_for_sliding(self, value):
		self.all_fields_for_sliding = value
		return self

	def set_partial_bucket_handling(self, value):
		self.partial_bucket_handling = value
		return self

	def set_output_field_name(self, value):
		self.output_field_name = value
		return self

	def set_group_by(self, value):
		self.group_by = value
		return self

	def set_groups_to_display(self, value):
		self.groups_to_display = value
		return self

	def set_bucket_end_per_group(self, value):
		self.bucket_end_per_group = value
		return self

	def set_input_field_name(self, value):
		self.input_field_name = value
		return self

	def set_expect_decimals(self, value):
		self.expect_decimals = value
		return self

	@staticmethod
	def _get_name():
		return "SUM"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.bucket_interval != 0: 
			desc += "BUCKET_INTERVAL=" + py_to_str(self.bucket_interval) + ","
		if self.bucket_interval_units != self.BucketIntervalUnits.SECONDS: 
			desc += "BUCKET_INTERVAL_UNITS=" + py_to_str(self.bucket_interval_units) + ","
		if self.output_interval != "": 
			desc += "OUTPUT_INTERVAL=" + py_to_str(self.output_interval) + ","
		if self.output_interval_units != self.OutputIntervalUnits.SECONDS: 
			desc += "OUTPUT_INTERVAL_UNITS=" + py_to_str(self.output_interval_units) + ","
		if self.is_running_aggr != False: 
			desc += "IS_RUNNING_AGGR=" + py_to_str(self.is_running_aggr) + ","
		if self.bucket_time != self.BucketTime.BUCKET_END: 
			desc += "BUCKET_TIME=" + py_to_str(self.bucket_time) + ","
		if self.bucket_end_criteria != "": 
			desc += "BUCKET_END_CRITERIA=" + py_to_str(self.bucket_end_criteria) + ","
		if self.boundary_tick_bucket != self.BoundaryTickBucket.NEW: 
			desc += "BOUNDARY_TICK_BUCKET=" + py_to_str(self.boundary_tick_bucket) + ","
		if self.all_fields_for_sliding != self.AllFieldsForSliding.FALSE: 
			desc += "ALL_FIELDS_FOR_SLIDING=" + py_to_str(self.all_fields_for_sliding) + ","
		if self.partial_bucket_handling != self.PartialBucketHandling.AS_SEPARATE_BUCKET: 
			desc += "PARTIAL_BUCKET_HANDLING=" + py_to_str(self.partial_bucket_handling) + ","
		if self.output_field_name != "VALUE": 
			desc += "OUTPUT_FIELD_NAME=" + py_to_str(self.output_field_name) + ","
		if self.group_by != "": 
			desc += "GROUP_BY=" + py_to_str(self.group_by) + ","
		if self.groups_to_display != self.GroupsToDisplay.ALL: 
			desc += "GROUPS_TO_DISPLAY=" + py_to_str(self.groups_to_display) + ","
		if self.bucket_end_per_group != False: 
			desc += "BUCKET_END_PER_GROUP=" + py_to_str(self.bucket_end_per_group) + ","
		if self.input_field_name != "": 
			desc += "INPUT_FIELD_NAME=" + py_to_str(self.input_field_name) + ","
		if self.expect_decimals != self.ExpectDecimals.FALSE: 
			desc += "EXPECT_DECIMALS=" + py_to_str(self.expect_decimals) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class Clock(_graph_components.EpBase):
	"""
		

CLOCK

Type: Other

Description: Allows tick propagation scheduling.

Python
class name:&nbsp;Clock

Input: A time series of ticks.

Output: A time series of ticks.

Parameters:


  FIELDS (string)
    A mandatory parameter of the following form: FIELD_1
[TYPE_1]=EXPR_1, FIELD_2 [TYPE_2]=EXPR_1, &hellip; ,FIELD_N
[TYPE_N]=EXPR_N.
If field is missing in input tick it will be added, if is present then
updated. Supported type specifications for added fields are string (for
64-character string), string[&lt;size&gt;], varstring (a
variable-length string), double, long, int, short, byte, msectime and
nsectime. If the type specification is omitted, the type will be
detected automatically (long, double or string). Time of parameter
evaluation is determined by TICK_CALCULATION parameter.

  
  ALARM_TIME (expression)
    A mandatory expression that produces a timestamp in milliseconds
when the next tick should be propagated. Time of parameter evaluation
is determined by TICK_CALCULATION parameter.

  
  TICK_CALCULATION (enumerated)
    
Default: _START_TIME
    If TICK_CALCULATION=START_TIME FIELDS and ALARM_TIME are
evaluated
at query start time and if ALARM_TIME is evaluated to non-zero value
then tick propagation is scheduled for that time. If
TICK_CALCULATION=INPUT_TICK FIELDS and ALARM_TIME are evaluated on
every input tick and if ALARM_TIME is evaluated to non-zero value then
tick propagation is scheduled for that time.

     When propagation time is reached a tick is propagated and
FIELDS
and ALARM_TIME are evaluated again, the routine is repeated until
ALARM_TIME is evaluated to zero value.

  

ALARM_TIME and EXPR_ used above are the logical expressions built from
input fields.&nbsp;


Examples:

CLOCK("TEST="test"","_START_TIME+5")

Generate a tick after 5 milliseconds after query start time with a
single field TEST="test".


	"""
	class Parameters:
		fields = "FIELDS"
		alarm_time = "ALARM_TIME"
		tick_calculation = "TICK_CALCULATION"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["fields", "alarm_time", "tick_calculation"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["fields", "_default_fields", "alarm_time", "_default_alarm_time", "tick_calculation", "_default_tick_calculation", "stack_info", "_used_strings"]

	class TickCalculation:
		INPUT_TICK = "INPUT_TICK"
		START_TIME = "START_TIME"

	def __init__(self, fields="", alarm_time="", tick_calculation=TickCalculation.START_TIME):
		_graph_components.EpBase.__init__(self, "CLOCK")
		self._default_fields = ""
		self.fields = fields
		self._default_alarm_time = ""
		self.alarm_time = alarm_time
		self._default_tick_calculation = type(self).TickCalculation.START_TIME
		self.tick_calculation = tick_calculation
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_fields(self, value):
		self.fields = value
		return self

	def set_alarm_time(self, value):
		self.alarm_time = value
		return self

	def set_tick_calculation(self, value):
		self.tick_calculation = value
		return self

	@staticmethod
	def _get_name():
		return "CLOCK"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.fields != "": 
			desc += "FIELDS=" + py_to_str(self.fields) + ","
		if self.alarm_time != "": 
			desc += "ALARM_TIME=" + py_to_str(self.alarm_time) + ","
		if self.tick_calculation != self.TickCalculation.START_TIME: 
			desc += "TICK_CALCULATION=" + py_to_str(self.tick_calculation) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class High(_graph_components.EpBase):
	"""
		

HIGH

Type: Aggregation

Description: For each bucket, computes the maximum value of a
specified numeric field.

Python
class name:
High

Input: A time series of ticks.

Output: A time series of ticks, one tick for each bucket.

Parameters: See parameters
common to generic aggregations.


  BUCKET_INTERVAL
(seconds/ticks)
  BUCKET_INTERVAL_UNITS
(enumerated type)
  OUTPUT_INTERVAL
(seconds)
  OUTPUT_INTERVAL_UNITS
(SECONDS/TICKS)
  IS_RUNNING_AGGR
(Boolean)
  BUCKET_TIME
(Boolean)
  BUCKET_END_CRITERIA
(expression)
  BOUNDARY_TICK_BUCKET
(NEW/PREVIOUS)
  ALL_FIELDS_FOR_SLIDING
(Boolean)
  PARTIAL_BUCKET_HANDLING
(enumerated type)
  OUTPUT_FIELD_NAME
(string)
  GROUP_BY
(string)
  GROUPS_TO_DISPLAY
(enumerated)
  BUCKET_END_PER_GROUP
(Boolean)
  INPUT_FIELD_NAME
(string)
  TIME_SERIES_TYPE (enumerated type)
    If set to STATE_TS, the value
from the input field from the latest tick before the current bucket
will be used as the initial value of the current bucket (unless the
current bucket has a tick at its start time) in the computation of HIGH.
If set to EVENT_TS, only ticks from
the current bucket will be used in the computation of HIGH.
Default: EVENT_TS

  
  EXPECT_LARGE_INTS (enumerated type)
    
      This parameter should be set to 
      true if the input field of
      this EP may contain integer values that consist of 15 digits or more.
      Such large integer values cannot be represented by double without
      precision errors and thus require special handling. If set to true,
      the input field is expected to be a 64-bit integer type (long,
      nsectime, msectime). The output field will also have a 64-bit integer
      type. When no tick belongs to a given time bucket, the output value is set
      to the value of NULL_INT_VAL.
      
      When this parameter is set to IF_INPUT_VAL_IS_LONG_INTEGER, the EP behaves the
      same way as when this parameter is set to true when the
      input field is a 64-bit integer type, and the same way as when it is set to
      false when the input field is not a 64-bit integer type.
      If the input field type changes during execution
      (e.g., from DOUBLE to LONG or from LONG to
      DOUBLE), the output field type is switched accordingly.
      Switching from LONG to DOUBLE may result
      in loss of precision.
      
      Note: EXPECT_LARGE_INTS can be used simultaneously with 
      EXPECT_DECIMALS only if 
      EXPECT_DECIMALS is set to IF_INPUT_VAL_IS_DECIMAL 
      and EXPECT_LARGE_INTS is set to IF_INPUT_VAL_IS_LONG_INTEGER.  
      In this mode, each parameter adapts to the detected input type.  
      If the input becomes a 64-bit integer, EXPECT_LARGE_INTS applies its logic.  
      If the input becomes DECIMAL128, EXPECT_DECIMALS applies its logic.  
      If the input becomes DOUBLE, both parameters revert to false behavior.
      
      Default: false
    

  
  
  EXPECT_DECIMALS (enumerated type)
    
      This parameter should be set to true
      if the input field of this EP may contain high-precision decimal values.
      Such values require special handling and are represented using the
      DECIMAL128 type. If set to true,
      the output field will have the DECIMAL128 type.
      
      When this parameter is set to IF_INPUT_VAL_IS_DECIMAL, the EP behaves
      the same way as when it is set to true for input fields
      of type DECIMAL128, and the same way as when it is set to
      false for other input types. When set to
      IF_INPUT_VAL_IS_DECIMAL and the input field type changes during execution
      (e.g., from DOUBLE to DECIMAL128 or from
      DECIMAL128 to DOUBLE), the output field type is switched
      accordingly. Switching from DECIMAL128 to DOUBLE may result in
      loss of precision.
      
      Note: EXPECT_DECIMALS can be used simultaneously with 
      EXPECT_LARGE_INTS only if 
      EXPECT_DECIMALS is set to IF_INPUT_VAL_IS_DECIMAL 
      and EXPECT_LARGE_INTS is set to IF_INPUT_VAL_IS_LONG_INTEGER.  
      In this mode, each parameter adapts to the detected input type.  
      If the input becomes DECIMAL128, EXPECT_DECIMALS applies its logic.  
      If the input becomes a 64-bit integer, EXPECT_LARGE_INTS applies its logic.  
      If the input becomes DOUBLE, both parameters revert to false behavior.
      
      Default: false
    

  
  NULL_INT_VAL (int64)
    The value of this parameter is considered to be the equivalent
of NaN when EXPECT_LARGE_INTS is set to true or when EXPECT_LARGE_INTS is set to IF_INPUT_VAL_IS_LONG_INTEGER and the input
field is a 64-bit integer type.
Default: 0

  

Notes: See the notes on generic
aggregations.

Example:

See the HIGH example in AGGREGATION_EXAMPLES.otq?HIGH
and STAT1 in AGGREGATION_BASIC_EXAMPLES.otq?STAT1.


	"""
	class Parameters:
		bucket_interval = "BUCKET_INTERVAL"
		bucket_interval_units = "BUCKET_INTERVAL_UNITS"
		output_interval = "OUTPUT_INTERVAL"
		output_interval_units = "OUTPUT_INTERVAL_UNITS"
		is_running_aggr = "IS_RUNNING_AGGR"
		bucket_time = "BUCKET_TIME"
		bucket_end_criteria = "BUCKET_END_CRITERIA"
		boundary_tick_bucket = "BOUNDARY_TICK_BUCKET"
		all_fields_for_sliding = "ALL_FIELDS_FOR_SLIDING"
		partial_bucket_handling = "PARTIAL_BUCKET_HANDLING"
		output_field_name = "OUTPUT_FIELD_NAME"
		group_by = "GROUP_BY"
		groups_to_display = "GROUPS_TO_DISPLAY"
		bucket_end_per_group = "BUCKET_END_PER_GROUP"
		input_field_name = "INPUT_FIELD_NAME"
		time_series_type = "TIME_SERIES_TYPE"
		expect_large_ints = "EXPECT_LARGE_INTS"
		expect_decimals = "EXPECT_DECIMALS"
		null_int_val = "NULL_INT_VAL"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["bucket_interval", "bucket_interval_units", "output_interval", "output_interval_units", "is_running_aggr", "bucket_time", "bucket_end_criteria", "boundary_tick_bucket", "all_fields_for_sliding", "partial_bucket_handling", "output_field_name", "group_by", "groups_to_display", "bucket_end_per_group", "input_field_name", "time_series_type", "expect_large_ints", "expect_decimals", "null_int_val"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["bucket_interval", "_default_bucket_interval", "bucket_interval_units", "_default_bucket_interval_units", "output_interval", "_default_output_interval", "output_interval_units", "_default_output_interval_units", "is_running_aggr", "_default_is_running_aggr", "bucket_time", "_default_bucket_time", "bucket_end_criteria", "_default_bucket_end_criteria", "boundary_tick_bucket", "_default_boundary_tick_bucket", "all_fields_for_sliding", "_default_all_fields_for_sliding", "partial_bucket_handling", "_default_partial_bucket_handling", "output_field_name", "_default_output_field_name", "group_by", "_default_group_by", "groups_to_display", "_default_groups_to_display", "bucket_end_per_group", "_default_bucket_end_per_group", "input_field_name", "_default_input_field_name", "time_series_type", "_default_time_series_type", "expect_large_ints", "_default_expect_large_ints", "expect_decimals", "_default_expect_decimals", "null_int_val", "_default_null_int_val", "stack_info", "_used_strings"]

	class BucketIntervalUnits:
		DAYS = "DAYS"
		FLEXIBLE = "FLEXIBLE"
		MONTHS = "MONTHS"
		SECONDS = "SECONDS"
		TICKS = "TICKS"

	class OutputIntervalUnits:
		SECONDS = "SECONDS"
		TICKS = "TICKS"

	class BucketTime:
		BUCKET_END = "BUCKET_END"
		BUCKET_START = "BUCKET_START"

	class BoundaryTickBucket:
		NEW = "NEW"
		PREVIOUS = "PREVIOUS"

	class AllFieldsForSliding:
		WHEN_TICKS_EXIT_WINDOW = "WHEN_TICKS_EXIT_WINDOW"
		FALSE = "false"
		TRUE = "true"

	class PartialBucketHandling:
		AS_SEPARATE_BUCKET = "AS_SEPARATE_BUCKET"
		DISCARD = "DISCARD"
		MERGE_WITH_PREVIOUS = "MERGE_WITH_PREVIOUS"

	class GroupsToDisplay:
		ALL = "ALL"
		EVENT_IN_LAST_BUCKET = "EVENT_IN_LAST_BUCKET"

	class TimeSeriesType:
		EVENT_TS = "EVENT_TS"
		STATE_TS = "STATE_TS"

	class ExpectLargeInts:
		IF_INPUT_VAL_IS_LONG_INTEGER = "IF_INPUT_VAL_IS_LONG_INTEGER"
		FALSE = "false"
		TRUE = "true"

	class ExpectDecimals:
		IF_INPUT_VAL_IS_DECIMAL = "IF_INPUT_VAL_IS_DECIMAL"
		FALSE = "false"
		TRUE = "true"

	def __init__(self, bucket_interval=0, bucket_interval_units=BucketIntervalUnits.SECONDS, output_interval="", output_interval_units=OutputIntervalUnits.SECONDS, is_running_aggr=False, bucket_time=BucketTime.BUCKET_END, bucket_end_criteria="", boundary_tick_bucket=BoundaryTickBucket.NEW, all_fields_for_sliding=AllFieldsForSliding.FALSE, partial_bucket_handling=PartialBucketHandling.AS_SEPARATE_BUCKET, output_field_name="VALUE", group_by="", groups_to_display=GroupsToDisplay.ALL, bucket_end_per_group=False, input_field_name="", time_series_type=TimeSeriesType.EVENT_TS, expect_large_ints=ExpectLargeInts.FALSE, expect_decimals=ExpectDecimals.FALSE, null_int_val=0, In = "", Out = ""):
		_graph_components.EpBase.__init__(self, "HIGH")
		self._default_bucket_interval = 0
		self.bucket_interval = bucket_interval
		self._default_bucket_interval_units = type(self).BucketIntervalUnits.SECONDS
		self.bucket_interval_units = bucket_interval_units
		self._default_output_interval = ""
		self.output_interval = output_interval
		self._default_output_interval_units = type(self).OutputIntervalUnits.SECONDS
		self.output_interval_units = output_interval_units
		self._default_is_running_aggr = False
		self.is_running_aggr = is_running_aggr
		self._default_bucket_time = type(self).BucketTime.BUCKET_END
		self.bucket_time = bucket_time
		self._default_bucket_end_criteria = ""
		self.bucket_end_criteria = bucket_end_criteria
		self._default_boundary_tick_bucket = type(self).BoundaryTickBucket.NEW
		self.boundary_tick_bucket = boundary_tick_bucket
		self._default_all_fields_for_sliding = type(self).AllFieldsForSliding.FALSE
		self.all_fields_for_sliding = all_fields_for_sliding
		self._default_partial_bucket_handling = type(self).PartialBucketHandling.AS_SEPARATE_BUCKET
		self.partial_bucket_handling = partial_bucket_handling
		self._default_output_field_name = "VALUE"
		self.output_field_name = output_field_name
		self._default_group_by = ""
		self.group_by = group_by
		self._default_groups_to_display = type(self).GroupsToDisplay.ALL
		self.groups_to_display = groups_to_display
		self._default_bucket_end_per_group = False
		self.bucket_end_per_group = bucket_end_per_group
		self._default_input_field_name = ""
		self.input_field_name = input_field_name
		self._default_time_series_type = type(self).TimeSeriesType.EVENT_TS
		self.time_series_type = time_series_type
		self._default_expect_large_ints = type(self).ExpectLargeInts.FALSE
		self.expect_large_ints = expect_large_ints
		self._default_expect_decimals = type(self).ExpectDecimals.FALSE
		self.expect_decimals = expect_decimals
		self._default_null_int_val = 0
		self.null_int_val = null_int_val
		if In != "":
			self.input_field_name=In
		if Out != "":
			self.output_field_name=Out
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_bucket_interval(self, value):
		self.bucket_interval = value
		return self

	def set_bucket_interval_units(self, value):
		self.bucket_interval_units = value
		return self

	def set_output_interval(self, value):
		self.output_interval = value
		return self

	def set_output_interval_units(self, value):
		self.output_interval_units = value
		return self

	def set_is_running_aggr(self, value):
		self.is_running_aggr = value
		return self

	def set_bucket_time(self, value):
		self.bucket_time = value
		return self

	def set_bucket_end_criteria(self, value):
		self.bucket_end_criteria = value
		return self

	def set_boundary_tick_bucket(self, value):
		self.boundary_tick_bucket = value
		return self

	def set_all_fields_for_sliding(self, value):
		self.all_fields_for_sliding = value
		return self

	def set_partial_bucket_handling(self, value):
		self.partial_bucket_handling = value
		return self

	def set_output_field_name(self, value):
		self.output_field_name = value
		return self

	def set_group_by(self, value):
		self.group_by = value
		return self

	def set_groups_to_display(self, value):
		self.groups_to_display = value
		return self

	def set_bucket_end_per_group(self, value):
		self.bucket_end_per_group = value
		return self

	def set_input_field_name(self, value):
		self.input_field_name = value
		return self

	def set_time_series_type(self, value):
		self.time_series_type = value
		return self

	def set_expect_large_ints(self, value):
		self.expect_large_ints = value
		return self

	def set_expect_decimals(self, value):
		self.expect_decimals = value
		return self

	def set_null_int_val(self, value):
		self.null_int_val = value
		return self

	@staticmethod
	def _get_name():
		return "HIGH"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.bucket_interval != 0: 
			desc += "BUCKET_INTERVAL=" + py_to_str(self.bucket_interval) + ","
		if self.bucket_interval_units != self.BucketIntervalUnits.SECONDS: 
			desc += "BUCKET_INTERVAL_UNITS=" + py_to_str(self.bucket_interval_units) + ","
		if self.output_interval != "": 
			desc += "OUTPUT_INTERVAL=" + py_to_str(self.output_interval) + ","
		if self.output_interval_units != self.OutputIntervalUnits.SECONDS: 
			desc += "OUTPUT_INTERVAL_UNITS=" + py_to_str(self.output_interval_units) + ","
		if self.is_running_aggr != False: 
			desc += "IS_RUNNING_AGGR=" + py_to_str(self.is_running_aggr) + ","
		if self.bucket_time != self.BucketTime.BUCKET_END: 
			desc += "BUCKET_TIME=" + py_to_str(self.bucket_time) + ","
		if self.bucket_end_criteria != "": 
			desc += "BUCKET_END_CRITERIA=" + py_to_str(self.bucket_end_criteria) + ","
		if self.boundary_tick_bucket != self.BoundaryTickBucket.NEW: 
			desc += "BOUNDARY_TICK_BUCKET=" + py_to_str(self.boundary_tick_bucket) + ","
		if self.all_fields_for_sliding != self.AllFieldsForSliding.FALSE: 
			desc += "ALL_FIELDS_FOR_SLIDING=" + py_to_str(self.all_fields_for_sliding) + ","
		if self.partial_bucket_handling != self.PartialBucketHandling.AS_SEPARATE_BUCKET: 
			desc += "PARTIAL_BUCKET_HANDLING=" + py_to_str(self.partial_bucket_handling) + ","
		if self.output_field_name != "VALUE": 
			desc += "OUTPUT_FIELD_NAME=" + py_to_str(self.output_field_name) + ","
		if self.group_by != "": 
			desc += "GROUP_BY=" + py_to_str(self.group_by) + ","
		if self.groups_to_display != self.GroupsToDisplay.ALL: 
			desc += "GROUPS_TO_DISPLAY=" + py_to_str(self.groups_to_display) + ","
		if self.bucket_end_per_group != False: 
			desc += "BUCKET_END_PER_GROUP=" + py_to_str(self.bucket_end_per_group) + ","
		if self.input_field_name != "": 
			desc += "INPUT_FIELD_NAME=" + py_to_str(self.input_field_name) + ","
		if self.time_series_type != self.TimeSeriesType.EVENT_TS: 
			desc += "TIME_SERIES_TYPE=" + py_to_str(self.time_series_type) + ","
		if self.expect_large_ints != self.ExpectLargeInts.FALSE: 
			desc += "EXPECT_LARGE_INTS=" + py_to_str(self.expect_large_ints) + ","
		if self.expect_decimals != self.ExpectDecimals.FALSE: 
			desc += "EXPECT_DECIMALS=" + py_to_str(self.expect_decimals) + ","
		if self.null_int_val != 0: 
			desc += "NULL_INT_VAL=" + py_to_str(self.null_int_val) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


Max = High


class Low(_graph_components.EpBase):
	"""
		

LOW

Type: Aggregation

Description: For each bucket, computes the minimum value of a
specified numeric field.

Python
class name:
Low

Input: A time series of ticks.

Output: A time series of ticks, one for each bucket.

Parameters: See parameters
common to generic aggregations.


  BUCKET_INTERVAL
(seconds/ticks)
  BUCKET_INTERVAL_UNITS
(enumerated type)
  OUTPUT_INTERVAL
(seconds)
  OUTPUT_INTERVAL_UNITS
(SECONDS/TICKS)
  IS_RUNNING_AGGR
(Boolean)
  BUCKET_TIME
(Boolean)
  BUCKET_END_CRITERIA
(expression)
  BOUNDARY_TICK_BUCKET
(NEW/PREVIOUS)
  ALL_FIELDS_FOR_SLIDING
(Boolean)
  PARTIAL_BUCKET_HANDLING
(enumerated type)
  OUTPUT_FIELD_NAME
(string)
  GROUP_BY
(string)
  GROUPS_TO_DISPLAY
(enumerated)
  BUCKET_END_PER_GROUP
(Boolean)
  INPUT_FIELD_NAME
(string)
  TIME_SERIES_TYPE (enumerated type)
    If set to STATE_TS, the value from the input field from the
latest tick before the current bucket will be used as the initial value
of the current bucket (unless the current bucket has a tick at its
start time) in the computation of LOW. If set to EVENT_TS, only ticks
from the current bucket will be used in the computation of LOW.
Default: EVENT_TS

  
  EXPECT_LARGE_INTS (enumerated type)
    
      This parameter should be set to 
      true if the input field of
      this EP may contain integer values that consist of 15 digits or more.
      Such large integer values cannot be represented by double without
      precision errors and thus require special handling. If set to true,
      the input field is expected to be a 64-bit integer type (long,
      nsectime, msectime). The output field will also have a 64-bit integer
      type. When no tick belongs to a given time bucket, the output value is set
      to the value of NULL_INT_VAL.
      
      When this parameter is set to IF_INPUT_VAL_IS_LONG_INTEGER, the EP behaves the
      same way as when this parameter is set to true when the
      input field is a 64-bit integer type, and the same way as when it is set to
      false when the input field is not a 64-bit integer type.
      If the input field type changes during execution
      (e.g., from DOUBLE to LONG or from LONG to
      DOUBLE), the output field type is switched accordingly.
      Switching from LONG to DOUBLE may result
      in loss of precision.
      
      Note: EXPECT_LARGE_INTS can be used simultaneously with 
      EXPECT_DECIMALS only if 
      EXPECT_DECIMALS is set to IF_INPUT_VAL_IS_DECIMAL 
      and EXPECT_LARGE_INTS is set to IF_INPUT_VAL_IS_LONG_INTEGER.  
      In this mode, each parameter adapts to the detected input type.  
      If the input becomes a 64-bit integer, EXPECT_LARGE_INTS applies its logic.  
      If the input becomes DECIMAL128, EXPECT_DECIMALS applies its logic.  
      If the input becomes DOUBLE, both parameters revert to false behavior.
      
      Default: false
    

  
  
  EXPECT_DECIMALS (enumerated type)
    
      This parameter should be set to true
      if the input field of this EP may contain high-precision decimal values.
      Such values require special handling and are represented using the
      DECIMAL128 type. If set to true,
      the output field will have the DECIMAL128 type.
      
      When this parameter is set to IF_INPUT_VAL_IS_DECIMAL, the EP behaves
      the same way as when it is set to true for input fields
      of type DECIMAL128, and the same way as when it is set to
      false for other input types. When set to
      IF_INPUT_VAL_IS_DECIMAL and the input field type changes during execution
      (e.g., from DOUBLE to DECIMAL128 or from
      DECIMAL128 to DOUBLE), the output field type is switched
      accordingly. Switching from DECIMAL128 to DOUBLE may result in
      loss of precision.
      
      Note: EXPECT_DECIMALS can be used simultaneously with 
      EXPECT_LARGE_INTS only if 
      EXPECT_DECIMALS is set to IF_INPUT_VAL_IS_DECIMAL 
      and EXPECT_LARGE_INTS is set to IF_INPUT_VAL_IS_LONG_INTEGER.  
      In this mode, each parameter adapts to the detected input type.  
      If the input becomes DECIMAL128, EXPECT_DECIMALS applies its logic.  
      If the input becomes a 64-bit integer, EXPECT_LARGE_INTS applies its logic.  
      If the input becomes DOUBLE, both parameters revert to false behavior.
      
      Default: false
    

  
  NULL_INT_VAL (int64)
    The value of this parameter is considered to be the equivalent
of NaN when EXPECT_LARGE_INTS is set to true or when EXPECT_LARGE_INTS is set to IF_INPUT_VAL_IS_LONG_INTEGER and the input
field is a 64-bit integer type.
Default: 9223372036854775807

  

Notes: See the notes on generic
aggregations.

Examples: See the following:

AGGREGATION_EXAMPLES.otq

AGGREGATION_BASIC_EXAMPLES.otq


	"""
	class Parameters:
		bucket_interval = "BUCKET_INTERVAL"
		bucket_interval_units = "BUCKET_INTERVAL_UNITS"
		output_interval = "OUTPUT_INTERVAL"
		output_interval_units = "OUTPUT_INTERVAL_UNITS"
		is_running_aggr = "IS_RUNNING_AGGR"
		bucket_time = "BUCKET_TIME"
		bucket_end_criteria = "BUCKET_END_CRITERIA"
		boundary_tick_bucket = "BOUNDARY_TICK_BUCKET"
		all_fields_for_sliding = "ALL_FIELDS_FOR_SLIDING"
		partial_bucket_handling = "PARTIAL_BUCKET_HANDLING"
		output_field_name = "OUTPUT_FIELD_NAME"
		group_by = "GROUP_BY"
		groups_to_display = "GROUPS_TO_DISPLAY"
		bucket_end_per_group = "BUCKET_END_PER_GROUP"
		input_field_name = "INPUT_FIELD_NAME"
		time_series_type = "TIME_SERIES_TYPE"
		expect_large_ints = "EXPECT_LARGE_INTS"
		expect_decimals = "EXPECT_DECIMALS"
		null_int_val = "NULL_INT_VAL"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["bucket_interval", "bucket_interval_units", "output_interval", "output_interval_units", "is_running_aggr", "bucket_time", "bucket_end_criteria", "boundary_tick_bucket", "all_fields_for_sliding", "partial_bucket_handling", "output_field_name", "group_by", "groups_to_display", "bucket_end_per_group", "input_field_name", "time_series_type", "expect_large_ints", "expect_decimals", "null_int_val"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["bucket_interval", "_default_bucket_interval", "bucket_interval_units", "_default_bucket_interval_units", "output_interval", "_default_output_interval", "output_interval_units", "_default_output_interval_units", "is_running_aggr", "_default_is_running_aggr", "bucket_time", "_default_bucket_time", "bucket_end_criteria", "_default_bucket_end_criteria", "boundary_tick_bucket", "_default_boundary_tick_bucket", "all_fields_for_sliding", "_default_all_fields_for_sliding", "partial_bucket_handling", "_default_partial_bucket_handling", "output_field_name", "_default_output_field_name", "group_by", "_default_group_by", "groups_to_display", "_default_groups_to_display", "bucket_end_per_group", "_default_bucket_end_per_group", "input_field_name", "_default_input_field_name", "time_series_type", "_default_time_series_type", "expect_large_ints", "_default_expect_large_ints", "expect_decimals", "_default_expect_decimals", "null_int_val", "_default_null_int_val", "stack_info", "_used_strings"]

	class BucketIntervalUnits:
		DAYS = "DAYS"
		FLEXIBLE = "FLEXIBLE"
		MONTHS = "MONTHS"
		SECONDS = "SECONDS"
		TICKS = "TICKS"

	class OutputIntervalUnits:
		SECONDS = "SECONDS"
		TICKS = "TICKS"

	class BucketTime:
		BUCKET_END = "BUCKET_END"
		BUCKET_START = "BUCKET_START"

	class BoundaryTickBucket:
		NEW = "NEW"
		PREVIOUS = "PREVIOUS"

	class AllFieldsForSliding:
		WHEN_TICKS_EXIT_WINDOW = "WHEN_TICKS_EXIT_WINDOW"
		FALSE = "false"
		TRUE = "true"

	class PartialBucketHandling:
		AS_SEPARATE_BUCKET = "AS_SEPARATE_BUCKET"
		DISCARD = "DISCARD"
		MERGE_WITH_PREVIOUS = "MERGE_WITH_PREVIOUS"

	class GroupsToDisplay:
		ALL = "ALL"
		EVENT_IN_LAST_BUCKET = "EVENT_IN_LAST_BUCKET"

	class TimeSeriesType:
		EVENT_TS = "EVENT_TS"
		STATE_TS = "STATE_TS"

	class ExpectLargeInts:
		IF_INPUT_VAL_IS_LONG_INTEGER = "IF_INPUT_VAL_IS_LONG_INTEGER"
		FALSE = "false"
		TRUE = "true"

	class ExpectDecimals:
		IF_INPUT_VAL_IS_DECIMAL = "IF_INPUT_VAL_IS_DECIMAL"
		FALSE = "false"
		TRUE = "true"

	def __init__(self, bucket_interval=0, bucket_interval_units=BucketIntervalUnits.SECONDS, output_interval="", output_interval_units=OutputIntervalUnits.SECONDS, is_running_aggr=False, bucket_time=BucketTime.BUCKET_END, bucket_end_criteria="", boundary_tick_bucket=BoundaryTickBucket.NEW, all_fields_for_sliding=AllFieldsForSliding.FALSE, partial_bucket_handling=PartialBucketHandling.AS_SEPARATE_BUCKET, output_field_name="VALUE", group_by="", groups_to_display=GroupsToDisplay.ALL, bucket_end_per_group=False, input_field_name="", time_series_type=TimeSeriesType.EVENT_TS, expect_large_ints=ExpectLargeInts.FALSE, expect_decimals=ExpectDecimals.FALSE, null_int_val=9223372036854775807, In = "", Out = ""):
		_graph_components.EpBase.__init__(self, "LOW")
		self._default_bucket_interval = 0
		self.bucket_interval = bucket_interval
		self._default_bucket_interval_units = type(self).BucketIntervalUnits.SECONDS
		self.bucket_interval_units = bucket_interval_units
		self._default_output_interval = ""
		self.output_interval = output_interval
		self._default_output_interval_units = type(self).OutputIntervalUnits.SECONDS
		self.output_interval_units = output_interval_units
		self._default_is_running_aggr = False
		self.is_running_aggr = is_running_aggr
		self._default_bucket_time = type(self).BucketTime.BUCKET_END
		self.bucket_time = bucket_time
		self._default_bucket_end_criteria = ""
		self.bucket_end_criteria = bucket_end_criteria
		self._default_boundary_tick_bucket = type(self).BoundaryTickBucket.NEW
		self.boundary_tick_bucket = boundary_tick_bucket
		self._default_all_fields_for_sliding = type(self).AllFieldsForSliding.FALSE
		self.all_fields_for_sliding = all_fields_for_sliding
		self._default_partial_bucket_handling = type(self).PartialBucketHandling.AS_SEPARATE_BUCKET
		self.partial_bucket_handling = partial_bucket_handling
		self._default_output_field_name = "VALUE"
		self.output_field_name = output_field_name
		self._default_group_by = ""
		self.group_by = group_by
		self._default_groups_to_display = type(self).GroupsToDisplay.ALL
		self.groups_to_display = groups_to_display
		self._default_bucket_end_per_group = False
		self.bucket_end_per_group = bucket_end_per_group
		self._default_input_field_name = ""
		self.input_field_name = input_field_name
		self._default_time_series_type = type(self).TimeSeriesType.EVENT_TS
		self.time_series_type = time_series_type
		self._default_expect_large_ints = type(self).ExpectLargeInts.FALSE
		self.expect_large_ints = expect_large_ints
		self._default_expect_decimals = type(self).ExpectDecimals.FALSE
		self.expect_decimals = expect_decimals
		self._default_null_int_val = 9223372036854775807
		self.null_int_val = null_int_val
		if In != "":
			self.input_field_name=In
		if Out != "":
			self.output_field_name=Out
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_bucket_interval(self, value):
		self.bucket_interval = value
		return self

	def set_bucket_interval_units(self, value):
		self.bucket_interval_units = value
		return self

	def set_output_interval(self, value):
		self.output_interval = value
		return self

	def set_output_interval_units(self, value):
		self.output_interval_units = value
		return self

	def set_is_running_aggr(self, value):
		self.is_running_aggr = value
		return self

	def set_bucket_time(self, value):
		self.bucket_time = value
		return self

	def set_bucket_end_criteria(self, value):
		self.bucket_end_criteria = value
		return self

	def set_boundary_tick_bucket(self, value):
		self.boundary_tick_bucket = value
		return self

	def set_all_fields_for_sliding(self, value):
		self.all_fields_for_sliding = value
		return self

	def set_partial_bucket_handling(self, value):
		self.partial_bucket_handling = value
		return self

	def set_output_field_name(self, value):
		self.output_field_name = value
		return self

	def set_group_by(self, value):
		self.group_by = value
		return self

	def set_groups_to_display(self, value):
		self.groups_to_display = value
		return self

	def set_bucket_end_per_group(self, value):
		self.bucket_end_per_group = value
		return self

	def set_input_field_name(self, value):
		self.input_field_name = value
		return self

	def set_time_series_type(self, value):
		self.time_series_type = value
		return self

	def set_expect_large_ints(self, value):
		self.expect_large_ints = value
		return self

	def set_expect_decimals(self, value):
		self.expect_decimals = value
		return self

	def set_null_int_val(self, value):
		self.null_int_val = value
		return self

	@staticmethod
	def _get_name():
		return "LOW"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.bucket_interval != 0: 
			desc += "BUCKET_INTERVAL=" + py_to_str(self.bucket_interval) + ","
		if self.bucket_interval_units != self.BucketIntervalUnits.SECONDS: 
			desc += "BUCKET_INTERVAL_UNITS=" + py_to_str(self.bucket_interval_units) + ","
		if self.output_interval != "": 
			desc += "OUTPUT_INTERVAL=" + py_to_str(self.output_interval) + ","
		if self.output_interval_units != self.OutputIntervalUnits.SECONDS: 
			desc += "OUTPUT_INTERVAL_UNITS=" + py_to_str(self.output_interval_units) + ","
		if self.is_running_aggr != False: 
			desc += "IS_RUNNING_AGGR=" + py_to_str(self.is_running_aggr) + ","
		if self.bucket_time != self.BucketTime.BUCKET_END: 
			desc += "BUCKET_TIME=" + py_to_str(self.bucket_time) + ","
		if self.bucket_end_criteria != "": 
			desc += "BUCKET_END_CRITERIA=" + py_to_str(self.bucket_end_criteria) + ","
		if self.boundary_tick_bucket != self.BoundaryTickBucket.NEW: 
			desc += "BOUNDARY_TICK_BUCKET=" + py_to_str(self.boundary_tick_bucket) + ","
		if self.all_fields_for_sliding != self.AllFieldsForSliding.FALSE: 
			desc += "ALL_FIELDS_FOR_SLIDING=" + py_to_str(self.all_fields_for_sliding) + ","
		if self.partial_bucket_handling != self.PartialBucketHandling.AS_SEPARATE_BUCKET: 
			desc += "PARTIAL_BUCKET_HANDLING=" + py_to_str(self.partial_bucket_handling) + ","
		if self.output_field_name != "VALUE": 
			desc += "OUTPUT_FIELD_NAME=" + py_to_str(self.output_field_name) + ","
		if self.group_by != "": 
			desc += "GROUP_BY=" + py_to_str(self.group_by) + ","
		if self.groups_to_display != self.GroupsToDisplay.ALL: 
			desc += "GROUPS_TO_DISPLAY=" + py_to_str(self.groups_to_display) + ","
		if self.bucket_end_per_group != False: 
			desc += "BUCKET_END_PER_GROUP=" + py_to_str(self.bucket_end_per_group) + ","
		if self.input_field_name != "": 
			desc += "INPUT_FIELD_NAME=" + py_to_str(self.input_field_name) + ","
		if self.time_series_type != self.TimeSeriesType.EVENT_TS: 
			desc += "TIME_SERIES_TYPE=" + py_to_str(self.time_series_type) + ","
		if self.expect_large_ints != self.ExpectLargeInts.FALSE: 
			desc += "EXPECT_LARGE_INTS=" + py_to_str(self.expect_large_ints) + ","
		if self.expect_decimals != self.ExpectDecimals.FALSE: 
			desc += "EXPECT_DECIMALS=" + py_to_str(self.expect_decimals) + ","
		if self.null_int_val != 9223372036854775807: 
			desc += "NULL_INT_VAL=" + py_to_str(self.null_int_val) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


Min = Low


class First(_graph_components.EpBase):
	"""
		

FIRST

Type: Aggregation

Description: For each bucket, outputs a tick that contains
the first value of a specified field in that bucket.

Python
class name:&nbsp;First

Input: A time series of ticks.

Output: A time series of ticks, one tick for each bucket.

Parameters: See parameters
common to generic aggregations.


  BUCKET_INTERVAL
(seconds/ticks)
  BUCKET_INTERVAL_UNITS
(enumerated type)
  OUTPUT_INTERVAL
(seconds)
  OUTPUT_INTERVAL_UNITS
(SECONDS/TICKS)
  IS_RUNNING_AGGR
(Boolean)
  BUCKET_TIME
(Boolean)
  BUCKET_END_CRITERIA
(expression)
  BOUNDARY_TICK_BUCKET
(NEW/PREVIOUS)
  ALL_FIELDS_FOR_SLIDING
(Boolean)
  PARTIAL_BUCKET_HANDLING
(enumerated type)
  OUTPUT_FIELD_NAME
(string)
  GROUP_BY
(string)
  GROUPS_TO_DISPLAY
(enumerated)
  BUCKET_END_PER_GROUP
(Boolean)
  INPUT_FIELD_NAME
(string)
  TIME_SERIES_TYPE (enumerated type)
    If set to STATE_TS, the value
from
the input field from the latest tick before the current bucket will be
used as the initial value of the current bucket (unless the current
bucket has a tick at its start time) in the computation of FIRST.
If set to EVENT_TS, only ticks from
the current bucket will be used in the computation of FIRST.
Default: EVENT_TS

  
  EXPECT_LARGE_INTS (enumerated type)
    
      This parameter should be set to 
      true if the input field of
      this EP may contain integer values that consist of 15 digits or more.
      Such large integer values cannot be represented by double without
      precision errors and thus require special handling. If set to true,
      the input field is expected to be a 64-bit integer type (long,
      nsectime, msectime). The output field will also have a 64-bit integer
      type. When no tick belongs to a given time bucket, the output value is set
      to the value of NULL_INT_VAL.
      
      When this parameter is set to IF_INPUT_VAL_IS_LONG_INTEGER, the EP behaves the
      same way as when this parameter is set to true when the
      input field is a 64-bit integer type, and the same way as when it is set to
      false when the input field is not a 64-bit integer type.
      If the input field type changes during execution
      (e.g., from DOUBLE to LONG or from LONG to
      DOUBLE), the output field type is switched accordingly.
      Switching from LONG to DOUBLE may result
      in loss of precision.
      
      Note: EXPECT_LARGE_INTS can be used simultaneously with 
      EXPECT_DECIMALS only if 
      EXPECT_DECIMALS is set to IF_INPUT_VAL_IS_DECIMAL 
      and EXPECT_LARGE_INTS is set to IF_INPUT_VAL_IS_LONG_INTEGER.  
      In this mode, each parameter adapts to the detected input type.  
      If the input becomes a 64-bit integer, EXPECT_LARGE_INTS applies its logic.  
      If the input becomes DECIMAL128, EXPECT_DECIMALS applies its logic.  
      If the input becomes DOUBLE, both parameters revert to false behavior.
      
      Default: false
    

  
  
  EXPECT_DECIMALS (enumerated type)
    
      This parameter should be set to true
      if the input field of this EP may contain high-precision decimal values.
      Such values require special handling and are represented using the
      DECIMAL128 type. If set to true,
      the output field will have the DECIMAL128 type.
      
      When this parameter is set to IF_INPUT_VAL_IS_DECIMAL, the EP behaves
      the same way as when it is set to true for input fields
      of type DECIMAL128, and the same way as when it is set to
      false for other input types. When set to
      IF_INPUT_VAL_IS_DECIMAL and the input field type changes during execution
      (e.g., from DOUBLE to DECIMAL128 or from
      DECIMAL128 to DOUBLE), the output field type is switched
      accordingly. Switching from DECIMAL128 to DOUBLE may result in
      loss of precision.
      
      Note: EXPECT_DECIMALS can be used simultaneously with 
      EXPECT_LARGE_INTS only if 
      EXPECT_DECIMALS is set to IF_INPUT_VAL_IS_DECIMAL 
      and EXPECT_LARGE_INTS is set to IF_INPUT_VAL_IS_LONG_INTEGER.  
      In this mode, each parameter adapts to the detected input type.  
      If the input becomes DECIMAL128, EXPECT_DECIMALS applies its logic.  
      If the input becomes a 64-bit integer, EXPECT_LARGE_INTS applies its logic.  
      If the input becomes DOUBLE, both parameters revert to false behavior.
      
      Default: false
    

  
  NULL_INT_VAL (int64)
    The value of this parameter is considered to be the equivalent
of NaN when EXPECT_LARGE_INTS is set to true or when EXPECT_LARGE_INTS is set to IF_INPUT_VAL_IS_LONG_INTEGER and the input
field is a 64-bit integer type.
Default: 0

  
  SKIP_TICK_IF
(number)
    If
value of the input field is equal to the value in this parameter, this
tick is ignored in the computation of FIRST. This parameter is
currently only supported for numeric fields.
    Default:&nbsp;&lt;none&gt;

  

Notes: See notes on generic
aggregations.

Only the field specified is propagated, and only one field can be
specified. Also see FIRST_TICK.

Example: See the FIRST
example in AGGREGATION_EXAMPLES.otq.


	"""
	class Parameters:
		bucket_interval = "BUCKET_INTERVAL"
		bucket_interval_units = "BUCKET_INTERVAL_UNITS"
		output_interval = "OUTPUT_INTERVAL"
		output_interval_units = "OUTPUT_INTERVAL_UNITS"
		is_running_aggr = "IS_RUNNING_AGGR"
		bucket_time = "BUCKET_TIME"
		bucket_end_criteria = "BUCKET_END_CRITERIA"
		boundary_tick_bucket = "BOUNDARY_TICK_BUCKET"
		all_fields_for_sliding = "ALL_FIELDS_FOR_SLIDING"
		partial_bucket_handling = "PARTIAL_BUCKET_HANDLING"
		output_field_name = "OUTPUT_FIELD_NAME"
		group_by = "GROUP_BY"
		groups_to_display = "GROUPS_TO_DISPLAY"
		bucket_end_per_group = "BUCKET_END_PER_GROUP"
		input_field_name = "INPUT_FIELD_NAME"
		time_series_type = "TIME_SERIES_TYPE"
		expect_large_ints = "EXPECT_LARGE_INTS"
		expect_decimals = "EXPECT_DECIMALS"
		null_int_val = "NULL_INT_VAL"
		skip_tick_if = "SKIP_TICK_IF"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["bucket_interval", "bucket_interval_units", "output_interval", "output_interval_units", "is_running_aggr", "bucket_time", "bucket_end_criteria", "boundary_tick_bucket", "all_fields_for_sliding", "partial_bucket_handling", "output_field_name", "group_by", "groups_to_display", "bucket_end_per_group", "input_field_name", "time_series_type", "expect_large_ints", "expect_decimals", "null_int_val", "skip_tick_if"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["bucket_interval", "_default_bucket_interval", "bucket_interval_units", "_default_bucket_interval_units", "output_interval", "_default_output_interval", "output_interval_units", "_default_output_interval_units", "is_running_aggr", "_default_is_running_aggr", "bucket_time", "_default_bucket_time", "bucket_end_criteria", "_default_bucket_end_criteria", "boundary_tick_bucket", "_default_boundary_tick_bucket", "all_fields_for_sliding", "_default_all_fields_for_sliding", "partial_bucket_handling", "_default_partial_bucket_handling", "output_field_name", "_default_output_field_name", "group_by", "_default_group_by", "groups_to_display", "_default_groups_to_display", "bucket_end_per_group", "_default_bucket_end_per_group", "input_field_name", "_default_input_field_name", "time_series_type", "_default_time_series_type", "expect_large_ints", "_default_expect_large_ints", "expect_decimals", "_default_expect_decimals", "null_int_val", "_default_null_int_val", "skip_tick_if", "_default_skip_tick_if", "stack_info", "_used_strings"]

	class BucketIntervalUnits:
		DAYS = "DAYS"
		FLEXIBLE = "FLEXIBLE"
		MONTHS = "MONTHS"
		SECONDS = "SECONDS"
		TICKS = "TICKS"

	class OutputIntervalUnits:
		SECONDS = "SECONDS"
		TICKS = "TICKS"

	class BucketTime:
		BUCKET_END = "BUCKET_END"
		BUCKET_START = "BUCKET_START"

	class BoundaryTickBucket:
		NEW = "NEW"
		PREVIOUS = "PREVIOUS"

	class AllFieldsForSliding:
		WHEN_TICKS_EXIT_WINDOW = "WHEN_TICKS_EXIT_WINDOW"
		FALSE = "false"
		TRUE = "true"

	class PartialBucketHandling:
		AS_SEPARATE_BUCKET = "AS_SEPARATE_BUCKET"
		DISCARD = "DISCARD"
		MERGE_WITH_PREVIOUS = "MERGE_WITH_PREVIOUS"

	class GroupsToDisplay:
		ALL = "ALL"
		EVENT_IN_LAST_BUCKET = "EVENT_IN_LAST_BUCKET"

	class TimeSeriesType:
		EVENT_TS = "EVENT_TS"
		STATE_TS = "STATE_TS"

	class ExpectLargeInts:
		IF_INPUT_VAL_IS_LONG_INTEGER = "IF_INPUT_VAL_IS_LONG_INTEGER"
		FALSE = "false"
		TRUE = "true"

	class ExpectDecimals:
		IF_INPUT_VAL_IS_DECIMAL = "IF_INPUT_VAL_IS_DECIMAL"
		FALSE = "false"
		TRUE = "true"

	def __init__(self, bucket_interval=0, bucket_interval_units=BucketIntervalUnits.SECONDS, output_interval="", output_interval_units=OutputIntervalUnits.SECONDS, is_running_aggr=False, bucket_time=BucketTime.BUCKET_END, bucket_end_criteria="", boundary_tick_bucket=BoundaryTickBucket.NEW, all_fields_for_sliding=AllFieldsForSliding.FALSE, partial_bucket_handling=PartialBucketHandling.AS_SEPARATE_BUCKET, output_field_name="VALUE", group_by="", groups_to_display=GroupsToDisplay.ALL, bucket_end_per_group=False, input_field_name="", time_series_type=TimeSeriesType.EVENT_TS, expect_large_ints=ExpectLargeInts.FALSE, expect_decimals=ExpectDecimals.FALSE, null_int_val=0, skip_tick_if="", In = "", Out = ""):
		_graph_components.EpBase.__init__(self, "FIRST")
		self._default_bucket_interval = 0
		self.bucket_interval = bucket_interval
		self._default_bucket_interval_units = type(self).BucketIntervalUnits.SECONDS
		self.bucket_interval_units = bucket_interval_units
		self._default_output_interval = ""
		self.output_interval = output_interval
		self._default_output_interval_units = type(self).OutputIntervalUnits.SECONDS
		self.output_interval_units = output_interval_units
		self._default_is_running_aggr = False
		self.is_running_aggr = is_running_aggr
		self._default_bucket_time = type(self).BucketTime.BUCKET_END
		self.bucket_time = bucket_time
		self._default_bucket_end_criteria = ""
		self.bucket_end_criteria = bucket_end_criteria
		self._default_boundary_tick_bucket = type(self).BoundaryTickBucket.NEW
		self.boundary_tick_bucket = boundary_tick_bucket
		self._default_all_fields_for_sliding = type(self).AllFieldsForSliding.FALSE
		self.all_fields_for_sliding = all_fields_for_sliding
		self._default_partial_bucket_handling = type(self).PartialBucketHandling.AS_SEPARATE_BUCKET
		self.partial_bucket_handling = partial_bucket_handling
		self._default_output_field_name = "VALUE"
		self.output_field_name = output_field_name
		self._default_group_by = ""
		self.group_by = group_by
		self._default_groups_to_display = type(self).GroupsToDisplay.ALL
		self.groups_to_display = groups_to_display
		self._default_bucket_end_per_group = False
		self.bucket_end_per_group = bucket_end_per_group
		self._default_input_field_name = ""
		self.input_field_name = input_field_name
		self._default_time_series_type = type(self).TimeSeriesType.EVENT_TS
		self.time_series_type = time_series_type
		self._default_expect_large_ints = type(self).ExpectLargeInts.FALSE
		self.expect_large_ints = expect_large_ints
		self._default_expect_decimals = type(self).ExpectDecimals.FALSE
		self.expect_decimals = expect_decimals
		self._default_null_int_val = 0
		self.null_int_val = null_int_val
		self._default_skip_tick_if = ""
		self.skip_tick_if = skip_tick_if
		if In != "":
			self.input_field_name=In
		if Out != "":
			self.output_field_name=Out
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_bucket_interval(self, value):
		self.bucket_interval = value
		return self

	def set_bucket_interval_units(self, value):
		self.bucket_interval_units = value
		return self

	def set_output_interval(self, value):
		self.output_interval = value
		return self

	def set_output_interval_units(self, value):
		self.output_interval_units = value
		return self

	def set_is_running_aggr(self, value):
		self.is_running_aggr = value
		return self

	def set_bucket_time(self, value):
		self.bucket_time = value
		return self

	def set_bucket_end_criteria(self, value):
		self.bucket_end_criteria = value
		return self

	def set_boundary_tick_bucket(self, value):
		self.boundary_tick_bucket = value
		return self

	def set_all_fields_for_sliding(self, value):
		self.all_fields_for_sliding = value
		return self

	def set_partial_bucket_handling(self, value):
		self.partial_bucket_handling = value
		return self

	def set_output_field_name(self, value):
		self.output_field_name = value
		return self

	def set_group_by(self, value):
		self.group_by = value
		return self

	def set_groups_to_display(self, value):
		self.groups_to_display = value
		return self

	def set_bucket_end_per_group(self, value):
		self.bucket_end_per_group = value
		return self

	def set_input_field_name(self, value):
		self.input_field_name = value
		return self

	def set_time_series_type(self, value):
		self.time_series_type = value
		return self

	def set_expect_large_ints(self, value):
		self.expect_large_ints = value
		return self

	def set_expect_decimals(self, value):
		self.expect_decimals = value
		return self

	def set_null_int_val(self, value):
		self.null_int_val = value
		return self

	def set_skip_tick_if(self, value):
		self.skip_tick_if = value
		return self

	@staticmethod
	def _get_name():
		return "FIRST"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.bucket_interval != 0: 
			desc += "BUCKET_INTERVAL=" + py_to_str(self.bucket_interval) + ","
		if self.bucket_interval_units != self.BucketIntervalUnits.SECONDS: 
			desc += "BUCKET_INTERVAL_UNITS=" + py_to_str(self.bucket_interval_units) + ","
		if self.output_interval != "": 
			desc += "OUTPUT_INTERVAL=" + py_to_str(self.output_interval) + ","
		if self.output_interval_units != self.OutputIntervalUnits.SECONDS: 
			desc += "OUTPUT_INTERVAL_UNITS=" + py_to_str(self.output_interval_units) + ","
		if self.is_running_aggr != False: 
			desc += "IS_RUNNING_AGGR=" + py_to_str(self.is_running_aggr) + ","
		if self.bucket_time != self.BucketTime.BUCKET_END: 
			desc += "BUCKET_TIME=" + py_to_str(self.bucket_time) + ","
		if self.bucket_end_criteria != "": 
			desc += "BUCKET_END_CRITERIA=" + py_to_str(self.bucket_end_criteria) + ","
		if self.boundary_tick_bucket != self.BoundaryTickBucket.NEW: 
			desc += "BOUNDARY_TICK_BUCKET=" + py_to_str(self.boundary_tick_bucket) + ","
		if self.all_fields_for_sliding != self.AllFieldsForSliding.FALSE: 
			desc += "ALL_FIELDS_FOR_SLIDING=" + py_to_str(self.all_fields_for_sliding) + ","
		if self.partial_bucket_handling != self.PartialBucketHandling.AS_SEPARATE_BUCKET: 
			desc += "PARTIAL_BUCKET_HANDLING=" + py_to_str(self.partial_bucket_handling) + ","
		if self.output_field_name != "VALUE": 
			desc += "OUTPUT_FIELD_NAME=" + py_to_str(self.output_field_name) + ","
		if self.group_by != "": 
			desc += "GROUP_BY=" + py_to_str(self.group_by) + ","
		if self.groups_to_display != self.GroupsToDisplay.ALL: 
			desc += "GROUPS_TO_DISPLAY=" + py_to_str(self.groups_to_display) + ","
		if self.bucket_end_per_group != False: 
			desc += "BUCKET_END_PER_GROUP=" + py_to_str(self.bucket_end_per_group) + ","
		if self.input_field_name != "": 
			desc += "INPUT_FIELD_NAME=" + py_to_str(self.input_field_name) + ","
		if self.time_series_type != self.TimeSeriesType.EVENT_TS: 
			desc += "TIME_SERIES_TYPE=" + py_to_str(self.time_series_type) + ","
		if self.expect_large_ints != self.ExpectLargeInts.FALSE: 
			desc += "EXPECT_LARGE_INTS=" + py_to_str(self.expect_large_ints) + ","
		if self.expect_decimals != self.ExpectDecimals.FALSE: 
			desc += "EXPECT_DECIMALS=" + py_to_str(self.expect_decimals) + ","
		if self.null_int_val != 0: 
			desc += "NULL_INT_VAL=" + py_to_str(self.null_int_val) + ","
		if self.skip_tick_if != "": 
			desc += "SKIP_TICK_IF=" + py_to_str(self.skip_tick_if) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class PrimaryExch(_graph_components.EpBase):
	"""
		

PRIMARY_EXCH

Type: Filter

Description: Propagates the tick if its exchange is the
PRIMARY exchange of the security. The primary exchange information is
supplied through the Reference Database. It expects the security level
symbol (IBM, not IBM.N) and works for both types of databases - whether
built as composite or by exchange.

For composite databases it can be anywhere in the graph. It
works by looking for a field called EXCHANGE and filtering out
ticks where the field does not match the primary exchange for the
security.

For exchange-based databases it has to be the first node on the
graph. It works by simply replacing the security symbol with the
exchange-based symbol (e.g. IBM with IBM.N).

Python
class name:
PrimaryExch

Input: A time series of ticks

Output: A time series of ticks

Parameters:


  DISCARD_ON_MATCH
(Boolean)

Examples: Only keep ticks from the primary exchange:

PRIMARY_EXCH ()


	"""
	class Parameters:
		discard_on_match = "DISCARD_ON_MATCH"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["discard_on_match"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["discard_on_match", "_default_discard_on_match", "stack_info", "_used_strings"]

	def __init__(self, discard_on_match=False):
		_graph_components.EpBase.__init__(self, "PRIMARY_EXCH")
		self._default_discard_on_match = False
		self.discard_on_match = discard_on_match
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_discard_on_match(self, value):
		self.discard_on_match = value
		return self

	@staticmethod
	def _get_name():
		return "PRIMARY_EXCH"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.discard_on_match != False: 
			desc += "DISCARD_ON_MATCH=" + py_to_str(self.discard_on_match) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class GenericAggregation(_graph_components.EpBase):
	"""
		

GENERIC_AGGREGATION

Type: Aggregation

Description: For each bucket, propagates the ticks to
provided query and outputs the result. After each bucket, the query
will be reset.

Python
class name:
GenericAggregation

Input: A time series of ticks.

Output: A new time series of ticks with the values computed
by provided query.

Parameters: See parameters
common to generic aggregations.


  BUCKET_INTERVAL
(seconds/ticks)
  BUCKET_INTERVAL_UNITS
(enumerated type)
  OUTPUT_INTERVAL
(seconds)
  OUTPUT_INTERVAL_UNITS
(SECONDS/TICKS)
  IS_RUNNING_AGGR
(Boolean)
  BUCKET_TIME
(Boolean)
  BUCKET_END_CRITERIA
(expression)
  BOUNDARY_TICK_BUCKET
(NEW/PREVIOUS)
  PARTIAL_BUCKET_HANDLING
(enumerated type)
  QUERY_NAME (string)
    Specifies the query that will be used for computations.
Note that currently only EPs that support dynamic symbol change could
be used in the provided query.
Also, there is no limitation on the number of resulting ticks.
Default: empty

  
  OTQ_PARAMS (string)
    Specifies parameters that will be passed to the query.
Default: empty

  
  BUCKET_DELIMITERS (string)
    When set to D, an extra tick
is created after each bucket. Also, an additional column, called DELIMITER, is added to output ticks. The
extra tick has values of all fields set to the defaults (0,NaN,""),
except the delimiter field, which is set to D.
All other ticks have the DELIMITER
set to 0 zero.
Default: empty

  
  GROUP_BY
(string)
  GROUPS_TO_DISPLAY
(enumerated)
  BUCKET_END_PER_GROUP
(Boolean)

&nbsp;

Note: To be able to use GENERIC_AGGREGATION inside COMPUTE EP, one should add LAST_TICK with
DEFAULT_TICK set at the end of the aggregator query. This must be done
to be sure that aggregator query will return exactly one tick for each
bucket.

Examples: See the following:

GENERIC_AGGREGATION_EXAMPLE.otq


	"""
	class Parameters:
		bucket_interval = "BUCKET_INTERVAL"
		bucket_interval_units = "BUCKET_INTERVAL_UNITS"
		output_interval = "OUTPUT_INTERVAL"
		output_interval_units = "OUTPUT_INTERVAL_UNITS"
		is_running_aggr = "IS_RUNNING_AGGR"
		bucket_time = "BUCKET_TIME"
		bucket_end_criteria = "BUCKET_END_CRITERIA"
		boundary_tick_bucket = "BOUNDARY_TICK_BUCKET"
		partial_bucket_handling = "PARTIAL_BUCKET_HANDLING"
		query_name = "QUERY_NAME"
		otq_params = "OTQ_PARAMS"
		bucket_delimiters = "BUCKET_DELIMITERS"
		group_by = "GROUP_BY"
		groups_to_display = "GROUPS_TO_DISPLAY"
		bucket_end_per_group = "BUCKET_END_PER_GROUP"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["bucket_interval", "bucket_interval_units", "output_interval", "output_interval_units", "is_running_aggr", "bucket_time", "bucket_end_criteria", "boundary_tick_bucket", "partial_bucket_handling", "query_name", "otq_params", "bucket_delimiters", "group_by", "groups_to_display", "bucket_end_per_group"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["bucket_interval", "_default_bucket_interval", "bucket_interval_units", "_default_bucket_interval_units", "output_interval", "_default_output_interval", "output_interval_units", "_default_output_interval_units", "is_running_aggr", "_default_is_running_aggr", "bucket_time", "_default_bucket_time", "bucket_end_criteria", "_default_bucket_end_criteria", "boundary_tick_bucket", "_default_boundary_tick_bucket", "partial_bucket_handling", "_default_partial_bucket_handling", "query_name", "_default_query_name", "otq_params", "_default_otq_params", "bucket_delimiters", "_default_bucket_delimiters", "group_by", "_default_group_by", "groups_to_display", "_default_groups_to_display", "bucket_end_per_group", "_default_bucket_end_per_group", "stack_info", "_used_strings"]

	class BucketIntervalUnits:
		DAYS = "DAYS"
		FLEXIBLE = "FLEXIBLE"
		MONTHS = "MONTHS"
		SECONDS = "SECONDS"
		TICKS = "TICKS"

	class OutputIntervalUnits:
		SECONDS = "SECONDS"
		TICKS = "TICKS"

	class BucketTime:
		BUCKET_END = "BUCKET_END"
		BUCKET_START = "BUCKET_START"

	class BoundaryTickBucket:
		NEW = "NEW"
		PREVIOUS = "PREVIOUS"

	class PartialBucketHandling:
		AS_SEPARATE_BUCKET = "AS_SEPARATE_BUCKET"
		DISCARD = "DISCARD"
		MERGE_WITH_PREVIOUS = "MERGE_WITH_PREVIOUS"

	class GroupsToDisplay:
		ALL = "ALL"
		EVENT_IN_LAST_BUCKET = "EVENT_IN_LAST_BUCKET"

	def __init__(self, bucket_interval=0, bucket_interval_units=BucketIntervalUnits.SECONDS, output_interval="", output_interval_units=OutputIntervalUnits.SECONDS, is_running_aggr=False, bucket_time=BucketTime.BUCKET_END, bucket_end_criteria="", boundary_tick_bucket=BoundaryTickBucket.NEW, partial_bucket_handling=PartialBucketHandling.AS_SEPARATE_BUCKET, query_name="", otq_params="", bucket_delimiters="", group_by="", groups_to_display=GroupsToDisplay.ALL, bucket_end_per_group=False):
		_graph_components.EpBase.__init__(self, "GENERIC_AGGREGATION")
		self._default_bucket_interval = 0
		self.bucket_interval = bucket_interval
		self._default_bucket_interval_units = type(self).BucketIntervalUnits.SECONDS
		self.bucket_interval_units = bucket_interval_units
		self._default_output_interval = ""
		self.output_interval = output_interval
		self._default_output_interval_units = type(self).OutputIntervalUnits.SECONDS
		self.output_interval_units = output_interval_units
		self._default_is_running_aggr = False
		self.is_running_aggr = is_running_aggr
		self._default_bucket_time = type(self).BucketTime.BUCKET_END
		self.bucket_time = bucket_time
		self._default_bucket_end_criteria = ""
		self.bucket_end_criteria = bucket_end_criteria
		self._default_boundary_tick_bucket = type(self).BoundaryTickBucket.NEW
		self.boundary_tick_bucket = boundary_tick_bucket
		self._default_partial_bucket_handling = type(self).PartialBucketHandling.AS_SEPARATE_BUCKET
		self.partial_bucket_handling = partial_bucket_handling
		self._default_query_name = ""
		self.query_name = query_name
		self._default_otq_params = ""
		self.otq_params = otq_params
		self._default_bucket_delimiters = ""
		self.bucket_delimiters = bucket_delimiters
		self._default_group_by = ""
		self.group_by = group_by
		self._default_groups_to_display = type(self).GroupsToDisplay.ALL
		self.groups_to_display = groups_to_display
		self._default_bucket_end_per_group = False
		self.bucket_end_per_group = bucket_end_per_group
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_bucket_interval(self, value):
		self.bucket_interval = value
		return self

	def set_bucket_interval_units(self, value):
		self.bucket_interval_units = value
		return self

	def set_output_interval(self, value):
		self.output_interval = value
		return self

	def set_output_interval_units(self, value):
		self.output_interval_units = value
		return self

	def set_is_running_aggr(self, value):
		self.is_running_aggr = value
		return self

	def set_bucket_time(self, value):
		self.bucket_time = value
		return self

	def set_bucket_end_criteria(self, value):
		self.bucket_end_criteria = value
		return self

	def set_boundary_tick_bucket(self, value):
		self.boundary_tick_bucket = value
		return self

	def set_partial_bucket_handling(self, value):
		self.partial_bucket_handling = value
		return self

	def set_query_name(self, value):
		self.query_name = value
		return self

	def set_otq_params(self, value):
		self.otq_params = value
		return self

	def set_bucket_delimiters(self, value):
		self.bucket_delimiters = value
		return self

	def set_group_by(self, value):
		self.group_by = value
		return self

	def set_groups_to_display(self, value):
		self.groups_to_display = value
		return self

	def set_bucket_end_per_group(self, value):
		self.bucket_end_per_group = value
		return self

	@staticmethod
	def _get_name():
		return "GENERIC_AGGREGATION"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.bucket_interval != 0: 
			desc += "BUCKET_INTERVAL=" + py_to_str(self.bucket_interval) + ","
		if self.bucket_interval_units != self.BucketIntervalUnits.SECONDS: 
			desc += "BUCKET_INTERVAL_UNITS=" + py_to_str(self.bucket_interval_units) + ","
		if self.output_interval != "": 
			desc += "OUTPUT_INTERVAL=" + py_to_str(self.output_interval) + ","
		if self.output_interval_units != self.OutputIntervalUnits.SECONDS: 
			desc += "OUTPUT_INTERVAL_UNITS=" + py_to_str(self.output_interval_units) + ","
		if self.is_running_aggr != False: 
			desc += "IS_RUNNING_AGGR=" + py_to_str(self.is_running_aggr) + ","
		if self.bucket_time != self.BucketTime.BUCKET_END: 
			desc += "BUCKET_TIME=" + py_to_str(self.bucket_time) + ","
		if self.bucket_end_criteria != "": 
			desc += "BUCKET_END_CRITERIA=" + py_to_str(self.bucket_end_criteria) + ","
		if self.boundary_tick_bucket != self.BoundaryTickBucket.NEW: 
			desc += "BOUNDARY_TICK_BUCKET=" + py_to_str(self.boundary_tick_bucket) + ","
		if self.partial_bucket_handling != self.PartialBucketHandling.AS_SEPARATE_BUCKET: 
			desc += "PARTIAL_BUCKET_HANDLING=" + py_to_str(self.partial_bucket_handling) + ","
		if self.query_name != "": 
			desc += "QUERY_NAME=" + py_to_str(self.query_name) + ","
		if self.otq_params != "": 
			desc += "OTQ_PARAMS=" + py_to_str(self.otq_params) + ","
		if self.bucket_delimiters != "": 
			desc += "BUCKET_DELIMITERS=" + py_to_str(self.bucket_delimiters) + ","
		if self.group_by != "": 
			desc += "GROUP_BY=" + py_to_str(self.group_by) + ","
		if self.groups_to_display != self.GroupsToDisplay.ALL: 
			desc += "GROUPS_TO_DISPLAY=" + py_to_str(self.groups_to_display) + ","
		if self.bucket_end_per_group != False: 
			desc += "BUCKET_END_PER_GROUP=" + py_to_str(self.bucket_end_per_group) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class Correlation(_graph_components.EpBase):
	"""
		

CORRELATION

Type: Aggregation

Description: Computes the Pearson correlation coefficient
value between two numeric fields, for each bucket in the aggregation.

Python
class name:&nbsp;Correlation

Input: Two time series of ticks with a numeric field in each
one.

Output: A time series of ticks, one for each bucket, with the
field containing the correlation value.

Parameters: See parameters
common to generic aggregations.


  BUCKET_INTERVAL
(seconds/ticks)
  BUCKET_INTERVAL_UNITS
(enumerated type)
  OUTPUT_INTERVAL
(seconds)
  OUTPUT_INTERVAL_UNITS
(SECONDS/TICKS)
  IS_RUNNING_AGGR
(Boolean)
  BUCKET_TIME
(Boolean)
  BUCKET_END_CRITERIA
(expression)
  BOUNDARY_TICK_BUCKET
(NEW/PREVIOUS)
  PARTIAL_BUCKET_HANDLING
(enumerated type)
  INPUT_FIELD1_NAME
(string)
  INPUT_FIELD2_NAME
(string)
  OUTPUT_FIELD_NAME
(string)
  GROUP_BY
(string)
  GROUPS_TO_DISPLAY
(enumerated)
  BUCKET_END_PER_GROUP
(Boolean)

&nbsp;

Example: See the CORRELATION
example in AGGREGATION_STATISTICS_EXAMPLES.otq.


	"""
	class Parameters:
		bucket_interval = "BUCKET_INTERVAL"
		bucket_interval_units = "BUCKET_INTERVAL_UNITS"
		output_interval = "OUTPUT_INTERVAL"
		output_interval_units = "OUTPUT_INTERVAL_UNITS"
		is_running_aggr = "IS_RUNNING_AGGR"
		bucket_time = "BUCKET_TIME"
		bucket_end_criteria = "BUCKET_END_CRITERIA"
		boundary_tick_bucket = "BOUNDARY_TICK_BUCKET"
		partial_bucket_handling = "PARTIAL_BUCKET_HANDLING"
		input_field1_name = "INPUT_FIELD1_NAME"
		input_field2_name = "INPUT_FIELD2_NAME"
		output_field_name = "OUTPUT_FIELD_NAME"
		group_by = "GROUP_BY"
		groups_to_display = "GROUPS_TO_DISPLAY"
		bucket_end_per_group = "BUCKET_END_PER_GROUP"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["bucket_interval", "bucket_interval_units", "output_interval", "output_interval_units", "is_running_aggr", "bucket_time", "bucket_end_criteria", "boundary_tick_bucket", "partial_bucket_handling", "input_field1_name", "input_field2_name", "output_field_name", "group_by", "groups_to_display", "bucket_end_per_group"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["bucket_interval", "_default_bucket_interval", "bucket_interval_units", "_default_bucket_interval_units", "output_interval", "_default_output_interval", "output_interval_units", "_default_output_interval_units", "is_running_aggr", "_default_is_running_aggr", "bucket_time", "_default_bucket_time", "bucket_end_criteria", "_default_bucket_end_criteria", "boundary_tick_bucket", "_default_boundary_tick_bucket", "partial_bucket_handling", "_default_partial_bucket_handling", "input_field1_name", "_default_input_field1_name", "input_field2_name", "_default_input_field2_name", "output_field_name", "_default_output_field_name", "group_by", "_default_group_by", "groups_to_display", "_default_groups_to_display", "bucket_end_per_group", "_default_bucket_end_per_group", "stack_info", "_used_strings"]

	class BucketIntervalUnits:
		DAYS = "DAYS"
		FLEXIBLE = "FLEXIBLE"
		MONTHS = "MONTHS"
		SECONDS = "SECONDS"
		TICKS = "TICKS"

	class OutputIntervalUnits:
		SECONDS = "SECONDS"
		TICKS = "TICKS"

	class BucketTime:
		BUCKET_END = "BUCKET_END"
		BUCKET_START = "BUCKET_START"

	class BoundaryTickBucket:
		NEW = "NEW"
		PREVIOUS = "PREVIOUS"

	class PartialBucketHandling:
		AS_SEPARATE_BUCKET = "AS_SEPARATE_BUCKET"
		DISCARD = "DISCARD"
		MERGE_WITH_PREVIOUS = "MERGE_WITH_PREVIOUS"

	class GroupsToDisplay:
		ALL = "ALL"
		EVENT_IN_LAST_BUCKET = "EVENT_IN_LAST_BUCKET"

	def __init__(self, bucket_interval=0, bucket_interval_units=BucketIntervalUnits.SECONDS, output_interval="", output_interval_units=OutputIntervalUnits.SECONDS, is_running_aggr=False, bucket_time=BucketTime.BUCKET_END, bucket_end_criteria="", boundary_tick_bucket=BoundaryTickBucket.NEW, partial_bucket_handling=PartialBucketHandling.AS_SEPARATE_BUCKET, input_field1_name="", input_field2_name="", output_field_name="VALUE", group_by="", groups_to_display=GroupsToDisplay.ALL, bucket_end_per_group=False, Out = ""):
		_graph_components.EpBase.__init__(self, "CORRELATION")
		self._default_bucket_interval = 0
		self.bucket_interval = bucket_interval
		self._default_bucket_interval_units = type(self).BucketIntervalUnits.SECONDS
		self.bucket_interval_units = bucket_interval_units
		self._default_output_interval = ""
		self.output_interval = output_interval
		self._default_output_interval_units = type(self).OutputIntervalUnits.SECONDS
		self.output_interval_units = output_interval_units
		self._default_is_running_aggr = False
		self.is_running_aggr = is_running_aggr
		self._default_bucket_time = type(self).BucketTime.BUCKET_END
		self.bucket_time = bucket_time
		self._default_bucket_end_criteria = ""
		self.bucket_end_criteria = bucket_end_criteria
		self._default_boundary_tick_bucket = type(self).BoundaryTickBucket.NEW
		self.boundary_tick_bucket = boundary_tick_bucket
		self._default_partial_bucket_handling = type(self).PartialBucketHandling.AS_SEPARATE_BUCKET
		self.partial_bucket_handling = partial_bucket_handling
		self._default_input_field1_name = ""
		self.input_field1_name = input_field1_name
		self._default_input_field2_name = ""
		self.input_field2_name = input_field2_name
		self._default_output_field_name = "VALUE"
		self.output_field_name = output_field_name
		self._default_group_by = ""
		self.group_by = group_by
		self._default_groups_to_display = type(self).GroupsToDisplay.ALL
		self.groups_to_display = groups_to_display
		self._default_bucket_end_per_group = False
		self.bucket_end_per_group = bucket_end_per_group
		if Out != "":
			self.output_field_name=Out
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_bucket_interval(self, value):
		self.bucket_interval = value
		return self

	def set_bucket_interval_units(self, value):
		self.bucket_interval_units = value
		return self

	def set_output_interval(self, value):
		self.output_interval = value
		return self

	def set_output_interval_units(self, value):
		self.output_interval_units = value
		return self

	def set_is_running_aggr(self, value):
		self.is_running_aggr = value
		return self

	def set_bucket_time(self, value):
		self.bucket_time = value
		return self

	def set_bucket_end_criteria(self, value):
		self.bucket_end_criteria = value
		return self

	def set_boundary_tick_bucket(self, value):
		self.boundary_tick_bucket = value
		return self

	def set_partial_bucket_handling(self, value):
		self.partial_bucket_handling = value
		return self

	def set_input_field1_name(self, value):
		self.input_field1_name = value
		return self

	def set_input_field2_name(self, value):
		self.input_field2_name = value
		return self

	def set_output_field_name(self, value):
		self.output_field_name = value
		return self

	def set_group_by(self, value):
		self.group_by = value
		return self

	def set_groups_to_display(self, value):
		self.groups_to_display = value
		return self

	def set_bucket_end_per_group(self, value):
		self.bucket_end_per_group = value
		return self

	@staticmethod
	def _get_name():
		return "CORRELATION"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.bucket_interval != 0: 
			desc += "BUCKET_INTERVAL=" + py_to_str(self.bucket_interval) + ","
		if self.bucket_interval_units != self.BucketIntervalUnits.SECONDS: 
			desc += "BUCKET_INTERVAL_UNITS=" + py_to_str(self.bucket_interval_units) + ","
		if self.output_interval != "": 
			desc += "OUTPUT_INTERVAL=" + py_to_str(self.output_interval) + ","
		if self.output_interval_units != self.OutputIntervalUnits.SECONDS: 
			desc += "OUTPUT_INTERVAL_UNITS=" + py_to_str(self.output_interval_units) + ","
		if self.is_running_aggr != False: 
			desc += "IS_RUNNING_AGGR=" + py_to_str(self.is_running_aggr) + ","
		if self.bucket_time != self.BucketTime.BUCKET_END: 
			desc += "BUCKET_TIME=" + py_to_str(self.bucket_time) + ","
		if self.bucket_end_criteria != "": 
			desc += "BUCKET_END_CRITERIA=" + py_to_str(self.bucket_end_criteria) + ","
		if self.boundary_tick_bucket != self.BoundaryTickBucket.NEW: 
			desc += "BOUNDARY_TICK_BUCKET=" + py_to_str(self.boundary_tick_bucket) + ","
		if self.partial_bucket_handling != self.PartialBucketHandling.AS_SEPARATE_BUCKET: 
			desc += "PARTIAL_BUCKET_HANDLING=" + py_to_str(self.partial_bucket_handling) + ","
		if self.input_field1_name != "": 
			desc += "INPUT_FIELD1_NAME=" + py_to_str(self.input_field1_name) + ","
		if self.input_field2_name != "": 
			desc += "INPUT_FIELD2_NAME=" + py_to_str(self.input_field2_name) + ","
		if self.output_field_name != "VALUE": 
			desc += "OUTPUT_FIELD_NAME=" + py_to_str(self.output_field_name) + ","
		if self.group_by != "": 
			desc += "GROUP_BY=" + py_to_str(self.group_by) + ","
		if self.groups_to_display != self.GroupsToDisplay.ALL: 
			desc += "GROUPS_TO_DISPLAY=" + py_to_str(self.groups_to_display) + ","
		if self.bucket_end_per_group != False: 
			desc += "BUCKET_END_PER_GROUP=" + py_to_str(self.bucket_end_per_group) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class CorrelationByRank(_graph_components.EpBase):
	"""
		

CORRELATION_BY_RANK

Type: Aggregation

Description: Computes the Spearman rank correlation value
between two numeric fields in the aggregation bucket.

Python
class name:&nbsp;CorrelationByRank

Input: Two time series of ticks with a numeric field in each
one.

Output: A time series of ticks, one for each bucket, with the
field containing rank correlations.

Parameters: See parameters
common to generic aggregations.


  BUCKET_INTERVAL
(seconds/ticks)
  BUCKET_INTERVAL_UNITS
(enumerated type)
  OUTPUT_INTERVAL
(seconds)
  OUTPUT_INTERVAL_UNITS
(SECONDS/TICKS)
  IS_RUNNING_AGGR
(Boolean)
  BUCKET_TIME
(Boolean)
  BUCKET_END_CRITERIA
(expression)
  BOUNDARY_TICK_BUCKET
(NEW/PREVIOUS)
  PARTIAL_BUCKET_HANDLING
(enumerated type)
  INPUT_IS_RANKS (Boolean)
    See also RANKING.

  
  INPUT_FIELD1_NAME
(string)
  INPUT_FIELD2_NAME
(string)
  OUTPUT_FIELD_NAME
(string)
  GROUP_BY
(string)
  GROUPS_TO_DISPLAY
(enumerated)
  BUCKET_END_PER_GROUP
(Boolean)


	"""
	class Parameters:
		bucket_interval = "BUCKET_INTERVAL"
		bucket_interval_units = "BUCKET_INTERVAL_UNITS"
		output_interval = "OUTPUT_INTERVAL"
		output_interval_units = "OUTPUT_INTERVAL_UNITS"
		is_running_aggr = "IS_RUNNING_AGGR"
		bucket_time = "BUCKET_TIME"
		bucket_end_criteria = "BUCKET_END_CRITERIA"
		boundary_tick_bucket = "BOUNDARY_TICK_BUCKET"
		partial_bucket_handling = "PARTIAL_BUCKET_HANDLING"
		input_is_ranks = "INPUT_IS_RANKS"
		input_field1_name = "INPUT_FIELD1_NAME"
		input_field2_name = "INPUT_FIELD2_NAME"
		output_field_name = "OUTPUT_FIELD_NAME"
		group_by = "GROUP_BY"
		groups_to_display = "GROUPS_TO_DISPLAY"
		bucket_end_per_group = "BUCKET_END_PER_GROUP"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["bucket_interval", "bucket_interval_units", "output_interval", "output_interval_units", "is_running_aggr", "bucket_time", "bucket_end_criteria", "boundary_tick_bucket", "partial_bucket_handling", "input_is_ranks", "input_field1_name", "input_field2_name", "output_field_name", "group_by", "groups_to_display", "bucket_end_per_group"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["bucket_interval", "_default_bucket_interval", "bucket_interval_units", "_default_bucket_interval_units", "output_interval", "_default_output_interval", "output_interval_units", "_default_output_interval_units", "is_running_aggr", "_default_is_running_aggr", "bucket_time", "_default_bucket_time", "bucket_end_criteria", "_default_bucket_end_criteria", "boundary_tick_bucket", "_default_boundary_tick_bucket", "partial_bucket_handling", "_default_partial_bucket_handling", "input_is_ranks", "_default_input_is_ranks", "input_field1_name", "_default_input_field1_name", "input_field2_name", "_default_input_field2_name", "output_field_name", "_default_output_field_name", "group_by", "_default_group_by", "groups_to_display", "_default_groups_to_display", "bucket_end_per_group", "_default_bucket_end_per_group", "stack_info", "_used_strings"]

	class BucketIntervalUnits:
		DAYS = "DAYS"
		FLEXIBLE = "FLEXIBLE"
		MONTHS = "MONTHS"
		SECONDS = "SECONDS"
		TICKS = "TICKS"

	class OutputIntervalUnits:
		SECONDS = "SECONDS"
		TICKS = "TICKS"

	class BucketTime:
		BUCKET_END = "BUCKET_END"
		BUCKET_START = "BUCKET_START"

	class BoundaryTickBucket:
		NEW = "NEW"
		PREVIOUS = "PREVIOUS"

	class PartialBucketHandling:
		AS_SEPARATE_BUCKET = "AS_SEPARATE_BUCKET"
		DISCARD = "DISCARD"
		MERGE_WITH_PREVIOUS = "MERGE_WITH_PREVIOUS"

	class GroupsToDisplay:
		ALL = "ALL"
		EVENT_IN_LAST_BUCKET = "EVENT_IN_LAST_BUCKET"

	def __init__(self, bucket_interval=0, bucket_interval_units=BucketIntervalUnits.SECONDS, output_interval="", output_interval_units=OutputIntervalUnits.SECONDS, is_running_aggr=False, bucket_time=BucketTime.BUCKET_END, bucket_end_criteria="", boundary_tick_bucket=BoundaryTickBucket.NEW, partial_bucket_handling=PartialBucketHandling.AS_SEPARATE_BUCKET, input_is_ranks=False, input_field1_name="", input_field2_name="", output_field_name="VALUE", group_by="", groups_to_display=GroupsToDisplay.ALL, bucket_end_per_group=False, Out = ""):
		_graph_components.EpBase.__init__(self, "CORRELATION_BY_RANK")
		self._default_bucket_interval = 0
		self.bucket_interval = bucket_interval
		self._default_bucket_interval_units = type(self).BucketIntervalUnits.SECONDS
		self.bucket_interval_units = bucket_interval_units
		self._default_output_interval = ""
		self.output_interval = output_interval
		self._default_output_interval_units = type(self).OutputIntervalUnits.SECONDS
		self.output_interval_units = output_interval_units
		self._default_is_running_aggr = False
		self.is_running_aggr = is_running_aggr
		self._default_bucket_time = type(self).BucketTime.BUCKET_END
		self.bucket_time = bucket_time
		self._default_bucket_end_criteria = ""
		self.bucket_end_criteria = bucket_end_criteria
		self._default_boundary_tick_bucket = type(self).BoundaryTickBucket.NEW
		self.boundary_tick_bucket = boundary_tick_bucket
		self._default_partial_bucket_handling = type(self).PartialBucketHandling.AS_SEPARATE_BUCKET
		self.partial_bucket_handling = partial_bucket_handling
		self._default_input_is_ranks = False
		self.input_is_ranks = input_is_ranks
		self._default_input_field1_name = ""
		self.input_field1_name = input_field1_name
		self._default_input_field2_name = ""
		self.input_field2_name = input_field2_name
		self._default_output_field_name = "VALUE"
		self.output_field_name = output_field_name
		self._default_group_by = ""
		self.group_by = group_by
		self._default_groups_to_display = type(self).GroupsToDisplay.ALL
		self.groups_to_display = groups_to_display
		self._default_bucket_end_per_group = False
		self.bucket_end_per_group = bucket_end_per_group
		if Out != "":
			self.output_field_name=Out
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_bucket_interval(self, value):
		self.bucket_interval = value
		return self

	def set_bucket_interval_units(self, value):
		self.bucket_interval_units = value
		return self

	def set_output_interval(self, value):
		self.output_interval = value
		return self

	def set_output_interval_units(self, value):
		self.output_interval_units = value
		return self

	def set_is_running_aggr(self, value):
		self.is_running_aggr = value
		return self

	def set_bucket_time(self, value):
		self.bucket_time = value
		return self

	def set_bucket_end_criteria(self, value):
		self.bucket_end_criteria = value
		return self

	def set_boundary_tick_bucket(self, value):
		self.boundary_tick_bucket = value
		return self

	def set_partial_bucket_handling(self, value):
		self.partial_bucket_handling = value
		return self

	def set_input_is_ranks(self, value):
		self.input_is_ranks = value
		return self

	def set_input_field1_name(self, value):
		self.input_field1_name = value
		return self

	def set_input_field2_name(self, value):
		self.input_field2_name = value
		return self

	def set_output_field_name(self, value):
		self.output_field_name = value
		return self

	def set_group_by(self, value):
		self.group_by = value
		return self

	def set_groups_to_display(self, value):
		self.groups_to_display = value
		return self

	def set_bucket_end_per_group(self, value):
		self.bucket_end_per_group = value
		return self

	@staticmethod
	def _get_name():
		return "CORRELATION_BY_RANK"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.bucket_interval != 0: 
			desc += "BUCKET_INTERVAL=" + py_to_str(self.bucket_interval) + ","
		if self.bucket_interval_units != self.BucketIntervalUnits.SECONDS: 
			desc += "BUCKET_INTERVAL_UNITS=" + py_to_str(self.bucket_interval_units) + ","
		if self.output_interval != "": 
			desc += "OUTPUT_INTERVAL=" + py_to_str(self.output_interval) + ","
		if self.output_interval_units != self.OutputIntervalUnits.SECONDS: 
			desc += "OUTPUT_INTERVAL_UNITS=" + py_to_str(self.output_interval_units) + ","
		if self.is_running_aggr != False: 
			desc += "IS_RUNNING_AGGR=" + py_to_str(self.is_running_aggr) + ","
		if self.bucket_time != self.BucketTime.BUCKET_END: 
			desc += "BUCKET_TIME=" + py_to_str(self.bucket_time) + ","
		if self.bucket_end_criteria != "": 
			desc += "BUCKET_END_CRITERIA=" + py_to_str(self.bucket_end_criteria) + ","
		if self.boundary_tick_bucket != self.BoundaryTickBucket.NEW: 
			desc += "BOUNDARY_TICK_BUCKET=" + py_to_str(self.boundary_tick_bucket) + ","
		if self.partial_bucket_handling != self.PartialBucketHandling.AS_SEPARATE_BUCKET: 
			desc += "PARTIAL_BUCKET_HANDLING=" + py_to_str(self.partial_bucket_handling) + ","
		if self.input_is_ranks != False: 
			desc += "INPUT_IS_RANKS=" + py_to_str(self.input_is_ranks) + ","
		if self.input_field1_name != "": 
			desc += "INPUT_FIELD1_NAME=" + py_to_str(self.input_field1_name) + ","
		if self.input_field2_name != "": 
			desc += "INPUT_FIELD2_NAME=" + py_to_str(self.input_field2_name) + ","
		if self.output_field_name != "VALUE": 
			desc += "OUTPUT_FIELD_NAME=" + py_to_str(self.output_field_name) + ","
		if self.group_by != "": 
			desc += "GROUP_BY=" + py_to_str(self.group_by) + ","
		if self.groups_to_display != self.GroupsToDisplay.ALL: 
			desc += "GROUPS_TO_DISPLAY=" + py_to_str(self.groups_to_display) + ","
		if self.bucket_end_per_group != False: 
			desc += "BUCKET_END_PER_GROUP=" + py_to_str(self.bucket_end_per_group) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class FirstTick(_graph_components.EpBase):
	"""
		

FIRST_TICK

Type: Aggregation

Description: For each bucket, propagates only its first tick
(or first n ticks), adding the field with the timestamp of the
tick. If a bucket is empty, no tick is sent to the output stream.

Python
class name:
FirstTick

Input: A time series of ticks.

Output: A time series of ticks, n ticks for each
bucket.

Parameters: See parameters
common to generic aggregations.


  BUCKET_INTERVAL
(seconds/ticks)
  BUCKET_INTERVAL_UNITS
(enumerated type)
  IS_RUNNING_AGGR
(Boolean)
  BUCKET_TIME
(Boolean)
  BUCKET_END_CRITERIA
(expression)
  BOUNDARY_TICK_BUCKET
(NEW/PREVIOUS)
  PARTIAL_BUCKET_HANDLING
(enumerated type)
  NUM_TICKS (integer)
    Specifies the n number of ticks (from the first tick) to
propagate. Setting it to 1 will propagate only the first tick, while 2
will propagate the first 2 ticks.

  
  DEFAULT_TICK ( field_name [type
specification] [(default value)] [,field_name [type specification]
[(default value)]] )
    A comma-separated list of fields that should match to schema of
the input stream.
When DEFAULT_TICK is set, exactly one tick with default values will be
created for each empty bucket. If no default value is specified for a
field in the list, then double/decimal fields will be initialized to
NaN, integer fields (of all sizes) and timestamp fields will be
initialized to 0 and string fields will be initialized to an empty
string.
The list of supported types are listed here.
Default: empty

  
  GROUP_BY
(string)
  GROUPS_TO_DISPLAY
(enumerated)
  BUCKET_END_PER_GROUP
(Boolean)
  KEEP_INITIAL_SCHEMA (Boolean)
    If set to true, and if the input schema changes, all input ticks
after that change will be converted to have the initial input schema .
Default: false

  
  ORDER_BY_TICK_TIME (Boolean)
    If set to true, order output ticks chronologically, according to
the values of TICK_TIME field, across all GROUP_BY keys. The value of
parameter GROUP_BY must be non-empty for this parameter to be set.
Default: false

  

Examples: See the FIRST_LAST_TICK
example in AGGREGATION_FIRST_LAST_EXAMPLES.otq
and FIRST_TICK in AGGREGATION_EXAMPLES.otq.


	"""
	class Parameters:
		bucket_interval = "BUCKET_INTERVAL"
		bucket_interval_units = "BUCKET_INTERVAL_UNITS"
		is_running_aggr = "IS_RUNNING_AGGR"
		bucket_time = "BUCKET_TIME"
		bucket_end_criteria = "BUCKET_END_CRITERIA"
		boundary_tick_bucket = "BOUNDARY_TICK_BUCKET"
		partial_bucket_handling = "PARTIAL_BUCKET_HANDLING"
		num_ticks = "NUM_TICKS"
		default_tick = "DEFAULT_TICK"
		group_by = "GROUP_BY"
		groups_to_display = "GROUPS_TO_DISPLAY"
		bucket_end_per_group = "BUCKET_END_PER_GROUP"
		keep_initial_schema = "KEEP_INITIAL_SCHEMA"
		order_by_tick_time = "ORDER_BY_TICK_TIME"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["bucket_interval", "bucket_interval_units", "is_running_aggr", "bucket_time", "bucket_end_criteria", "boundary_tick_bucket", "partial_bucket_handling", "num_ticks", "default_tick", "group_by", "groups_to_display", "bucket_end_per_group", "keep_initial_schema", "order_by_tick_time"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["bucket_interval", "_default_bucket_interval", "bucket_interval_units", "_default_bucket_interval_units", "is_running_aggr", "_default_is_running_aggr", "bucket_time", "_default_bucket_time", "bucket_end_criteria", "_default_bucket_end_criteria", "boundary_tick_bucket", "_default_boundary_tick_bucket", "partial_bucket_handling", "_default_partial_bucket_handling", "num_ticks", "_default_num_ticks", "default_tick", "_default_default_tick", "group_by", "_default_group_by", "groups_to_display", "_default_groups_to_display", "bucket_end_per_group", "_default_bucket_end_per_group", "keep_initial_schema", "_default_keep_initial_schema", "order_by_tick_time", "_default_order_by_tick_time", "stack_info", "_used_strings"]

	class BucketIntervalUnits:
		DAYS = "DAYS"
		FLEXIBLE = "FLEXIBLE"
		MONTHS = "MONTHS"
		SECONDS = "SECONDS"
		TICKS = "TICKS"

	class BucketTime:
		BUCKET_END = "BUCKET_END"
		BUCKET_START = "BUCKET_START"

	class BoundaryTickBucket:
		NEW = "NEW"
		PREVIOUS = "PREVIOUS"

	class PartialBucketHandling:
		AS_SEPARATE_BUCKET = "AS_SEPARATE_BUCKET"
		DISCARD = "DISCARD"
		MERGE_WITH_PREVIOUS = "MERGE_WITH_PREVIOUS"

	class GroupsToDisplay:
		ALL = "ALL"
		EVENT_IN_LAST_BUCKET = "EVENT_IN_LAST_BUCKET"

	def __init__(self, bucket_interval=0, bucket_interval_units=BucketIntervalUnits.SECONDS, is_running_aggr=False, bucket_time=BucketTime.BUCKET_END, bucket_end_criteria="", boundary_tick_bucket=BoundaryTickBucket.NEW, partial_bucket_handling=PartialBucketHandling.AS_SEPARATE_BUCKET, num_ticks=1, default_tick="", group_by="", groups_to_display=GroupsToDisplay.ALL, bucket_end_per_group=False, keep_initial_schema=False, order_by_tick_time=False):
		_graph_components.EpBase.__init__(self, "FIRST_TICK")
		self._default_bucket_interval = 0
		self.bucket_interval = bucket_interval
		self._default_bucket_interval_units = type(self).BucketIntervalUnits.SECONDS
		self.bucket_interval_units = bucket_interval_units
		self._default_is_running_aggr = False
		self.is_running_aggr = is_running_aggr
		self._default_bucket_time = type(self).BucketTime.BUCKET_END
		self.bucket_time = bucket_time
		self._default_bucket_end_criteria = ""
		self.bucket_end_criteria = bucket_end_criteria
		self._default_boundary_tick_bucket = type(self).BoundaryTickBucket.NEW
		self.boundary_tick_bucket = boundary_tick_bucket
		self._default_partial_bucket_handling = type(self).PartialBucketHandling.AS_SEPARATE_BUCKET
		self.partial_bucket_handling = partial_bucket_handling
		self._default_num_ticks = 1
		self.num_ticks = num_ticks
		self._default_default_tick = ""
		self.default_tick = default_tick
		self._default_group_by = ""
		self.group_by = group_by
		self._default_groups_to_display = type(self).GroupsToDisplay.ALL
		self.groups_to_display = groups_to_display
		self._default_bucket_end_per_group = False
		self.bucket_end_per_group = bucket_end_per_group
		self._default_keep_initial_schema = False
		self.keep_initial_schema = keep_initial_schema
		self._default_order_by_tick_time = False
		self.order_by_tick_time = order_by_tick_time
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_bucket_interval(self, value):
		self.bucket_interval = value
		return self

	def set_bucket_interval_units(self, value):
		self.bucket_interval_units = value
		return self

	def set_is_running_aggr(self, value):
		self.is_running_aggr = value
		return self

	def set_bucket_time(self, value):
		self.bucket_time = value
		return self

	def set_bucket_end_criteria(self, value):
		self.bucket_end_criteria = value
		return self

	def set_boundary_tick_bucket(self, value):
		self.boundary_tick_bucket = value
		return self

	def set_partial_bucket_handling(self, value):
		self.partial_bucket_handling = value
		return self

	def set_num_ticks(self, value):
		self.num_ticks = value
		return self

	def set_default_tick(self, value):
		self.default_tick = value
		return self

	def set_group_by(self, value):
		self.group_by = value
		return self

	def set_groups_to_display(self, value):
		self.groups_to_display = value
		return self

	def set_bucket_end_per_group(self, value):
		self.bucket_end_per_group = value
		return self

	def set_keep_initial_schema(self, value):
		self.keep_initial_schema = value
		return self

	def set_order_by_tick_time(self, value):
		self.order_by_tick_time = value
		return self

	@staticmethod
	def _get_name():
		return "FIRST_TICK"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.bucket_interval != 0: 
			desc += "BUCKET_INTERVAL=" + py_to_str(self.bucket_interval) + ","
		if self.bucket_interval_units != self.BucketIntervalUnits.SECONDS: 
			desc += "BUCKET_INTERVAL_UNITS=" + py_to_str(self.bucket_interval_units) + ","
		if self.is_running_aggr != False: 
			desc += "IS_RUNNING_AGGR=" + py_to_str(self.is_running_aggr) + ","
		if self.bucket_time != self.BucketTime.BUCKET_END: 
			desc += "BUCKET_TIME=" + py_to_str(self.bucket_time) + ","
		if self.bucket_end_criteria != "": 
			desc += "BUCKET_END_CRITERIA=" + py_to_str(self.bucket_end_criteria) + ","
		if self.boundary_tick_bucket != self.BoundaryTickBucket.NEW: 
			desc += "BOUNDARY_TICK_BUCKET=" + py_to_str(self.boundary_tick_bucket) + ","
		if self.partial_bucket_handling != self.PartialBucketHandling.AS_SEPARATE_BUCKET: 
			desc += "PARTIAL_BUCKET_HANDLING=" + py_to_str(self.partial_bucket_handling) + ","
		if self.num_ticks != 1: 
			desc += "NUM_TICKS=" + py_to_str(self.num_ticks) + ","
		if self.default_tick != "": 
			desc += "DEFAULT_TICK=" + py_to_str(self.default_tick) + ","
		if self.group_by != "": 
			desc += "GROUP_BY=" + py_to_str(self.group_by) + ","
		if self.groups_to_display != self.GroupsToDisplay.ALL: 
			desc += "GROUPS_TO_DISPLAY=" + py_to_str(self.groups_to_display) + ","
		if self.bucket_end_per_group != False: 
			desc += "BUCKET_END_PER_GROUP=" + py_to_str(self.bucket_end_per_group) + ","
		if self.keep_initial_schema != False: 
			desc += "KEEP_INITIAL_SCHEMA=" + py_to_str(self.keep_initial_schema) + ","
		if self.order_by_tick_time != False: 
			desc += "ORDER_BY_TICK_TIME=" + py_to_str(self.order_by_tick_time) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class Last(_graph_components.EpBase):
	"""
		

LAST

Type: Aggregation

Description: For each bucket, outputs a tick that contains
the last value of a specified field in that bucket.

Python
class name:
Last

Input: A time series of ticks.

Output: A time series of ticks.

Parameters: See parameters
common to generic aggregations.


  BUCKET_INTERVAL
(seconds/ticks)
  BUCKET_INTERVAL_UNITS
(enumerated type)
  OUTPUT_INTERVAL
(seconds)
  OUTPUT_INTERVAL_UNITS
(SECONDS/TICKS)
  IS_RUNNING_AGGR
(Boolean)
  BUCKET_TIME
(Boolean)
  BUCKET_END_CRITERIA
(expression)
  BOUNDARY_TICK_BUCKET
(NEW/PREVIOUS)
  ALL_FIELDS_FOR_SLIDING
(Boolean)
  PARTIAL_BUCKET_HANDLING
(enumerated type)
  OUTPUT_FIELD_NAME
(string)
  GROUP_BY
(string)
  GROUPS_TO_DISPLAY
(enumerated)
  BUCKET_END_PER_GROUP
(Boolean)
  INPUT_FIELD_NAME
(string)
  TIME_SERIES_TYPE (enumerated type)
    If set to STATE_TS, the value from the input field from the
latest tick before the current bucket will be used as the initial value
of the current bucket (unless the current bucket has a tick at its
start time) in the computation of LAST. If set to EVENT_TS, only ticks
from the current bucket will be used in the computation of LAST.
Default: EVENT_TS

  
  EXPECT_LARGE_INTS (enumerated type)
    
      This parameter should be set to 
      true if the input field of
      this EP may contain integer values that consist of 15 digits or more.
      Such large integer values cannot be represented by double without
      precision errors and thus require special handling. If set to true,
      the input field is expected to be a 64-bit integer type (long,
      nsectime, msectime). The output field will also have a 64-bit integer
      type. When no tick belongs to a given time bucket, the output value is set
      to the value of NULL_INT_VAL.
      
      When this parameter is set to IF_INPUT_VAL_IS_LONG_INTEGER, the EP behaves the
      same way as when this parameter is set to true when the
      input field is a 64-bit integer type, and the same way as when it is set to
      false when the input field is not a 64-bit integer type.
      If the input field type changes during execution
      (e.g., from DOUBLE to LONG or from LONG to
      DOUBLE), the output field type is switched accordingly.
      Switching from LONG to DOUBLE may result
      in loss of precision.
      
      Note: EXPECT_LARGE_INTS can be used simultaneously with 
      EXPECT_DECIMALS only if 
      EXPECT_DECIMALS is set to IF_INPUT_VAL_IS_DECIMAL 
      and EXPECT_LARGE_INTS is set to IF_INPUT_VAL_IS_LONG_INTEGER.  
      In this mode, each parameter adapts to the detected input type.  
      If the input becomes a 64-bit integer, EXPECT_LARGE_INTS applies its logic.  
      If the input becomes DECIMAL128, EXPECT_DECIMALS applies its logic.  
      If the input becomes DOUBLE, both parameters revert to false behavior.
      
      Default: false
    

  
  
  EXPECT_DECIMALS (enumerated type)
    
      This parameter should be set to true
      if the input field of this EP may contain high-precision decimal values.
      Such values require special handling and are represented using the
      DECIMAL128 type. If set to true,
      the output field will have the DECIMAL128 type.
      
      When this parameter is set to IF_INPUT_VAL_IS_DECIMAL, the EP behaves
      the same way as when it is set to true for input fields
      of type DECIMAL128, and the same way as when it is set to
      false for other input types. When set to
      IF_INPUT_VAL_IS_DECIMAL and the input field type changes during execution
      (e.g., from DOUBLE to DECIMAL128 or from
      DECIMAL128 to DOUBLE), the output field type is switched
      accordingly. Switching from DECIMAL128 to DOUBLE may result in
      loss of precision.
      
      Note: EXPECT_DECIMALS can be used simultaneously with 
      EXPECT_LARGE_INTS only if 
      EXPECT_DECIMALS is set to IF_INPUT_VAL_IS_DECIMAL 
      and EXPECT_LARGE_INTS is set to IF_INPUT_VAL_IS_LONG_INTEGER.  
      In this mode, each parameter adapts to the detected input type.  
      If the input becomes DECIMAL128, EXPECT_DECIMALS applies its logic.  
      If the input becomes a 64-bit integer, EXPECT_LARGE_INTS applies its logic.  
      If the input becomes DOUBLE, both parameters revert to false behavior.
      
      Default: false
    

  
  NULL_INT_VAL (int64)
    The value of this parameter is considered to be the equivalent
of NaN when EXPECT_LARGE_INTS is set to true or when EXPECT_LARGE_INTS is set to IF_INPUT_VAL_IS_LONG_INTEGER and the input
field is a 64-bit integer type.
Default: 0

  
  FWD_FILL_IF(number)
    If value of the input field is equal to the value in this
parameter, this tick is ignored in the computation of LAST. This
parameter is currently only supported for numeric fields.
Default: &lt;none&gt;

  

Notes: See the notes on generic
aggregations.

Only the field specified is propagated, and only one field can be
specified. Also see LAST_TICK.

Examples: See the following:

See the LAST example in AGGREGATION_EXAMPLES.otq.


	"""
	class Parameters:
		bucket_interval = "BUCKET_INTERVAL"
		bucket_interval_units = "BUCKET_INTERVAL_UNITS"
		output_interval = "OUTPUT_INTERVAL"
		output_interval_units = "OUTPUT_INTERVAL_UNITS"
		is_running_aggr = "IS_RUNNING_AGGR"
		bucket_time = "BUCKET_TIME"
		bucket_end_criteria = "BUCKET_END_CRITERIA"
		boundary_tick_bucket = "BOUNDARY_TICK_BUCKET"
		all_fields_for_sliding = "ALL_FIELDS_FOR_SLIDING"
		partial_bucket_handling = "PARTIAL_BUCKET_HANDLING"
		output_field_name = "OUTPUT_FIELD_NAME"
		group_by = "GROUP_BY"
		groups_to_display = "GROUPS_TO_DISPLAY"
		bucket_end_per_group = "BUCKET_END_PER_GROUP"
		input_field_name = "INPUT_FIELD_NAME"
		time_series_type = "TIME_SERIES_TYPE"
		expect_large_ints = "EXPECT_LARGE_INTS"
		expect_decimals = "EXPECT_DECIMALS"
		null_int_val = "NULL_INT_VAL"
		fwd_fill_if = "FWD_FILL_IF"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["bucket_interval", "bucket_interval_units", "output_interval", "output_interval_units", "is_running_aggr", "bucket_time", "bucket_end_criteria", "boundary_tick_bucket", "all_fields_for_sliding", "partial_bucket_handling", "output_field_name", "group_by", "groups_to_display", "bucket_end_per_group", "input_field_name", "time_series_type", "expect_large_ints", "expect_decimals", "null_int_val", "fwd_fill_if"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["bucket_interval", "_default_bucket_interval", "bucket_interval_units", "_default_bucket_interval_units", "output_interval", "_default_output_interval", "output_interval_units", "_default_output_interval_units", "is_running_aggr", "_default_is_running_aggr", "bucket_time", "_default_bucket_time", "bucket_end_criteria", "_default_bucket_end_criteria", "boundary_tick_bucket", "_default_boundary_tick_bucket", "all_fields_for_sliding", "_default_all_fields_for_sliding", "partial_bucket_handling", "_default_partial_bucket_handling", "output_field_name", "_default_output_field_name", "group_by", "_default_group_by", "groups_to_display", "_default_groups_to_display", "bucket_end_per_group", "_default_bucket_end_per_group", "input_field_name", "_default_input_field_name", "time_series_type", "_default_time_series_type", "expect_large_ints", "_default_expect_large_ints", "expect_decimals", "_default_expect_decimals", "null_int_val", "_default_null_int_val", "fwd_fill_if", "_default_fwd_fill_if", "stack_info", "_used_strings"]

	class BucketIntervalUnits:
		DAYS = "DAYS"
		FLEXIBLE = "FLEXIBLE"
		MONTHS = "MONTHS"
		SECONDS = "SECONDS"
		TICKS = "TICKS"

	class OutputIntervalUnits:
		SECONDS = "SECONDS"
		TICKS = "TICKS"

	class BucketTime:
		BUCKET_END = "BUCKET_END"
		BUCKET_START = "BUCKET_START"

	class BoundaryTickBucket:
		NEW = "NEW"
		PREVIOUS = "PREVIOUS"

	class AllFieldsForSliding:
		WHEN_TICKS_EXIT_WINDOW = "WHEN_TICKS_EXIT_WINDOW"
		FALSE = "false"
		TRUE = "true"

	class PartialBucketHandling:
		AS_SEPARATE_BUCKET = "AS_SEPARATE_BUCKET"
		DISCARD = "DISCARD"
		MERGE_WITH_PREVIOUS = "MERGE_WITH_PREVIOUS"

	class GroupsToDisplay:
		ALL = "ALL"
		EVENT_IN_LAST_BUCKET = "EVENT_IN_LAST_BUCKET"

	class TimeSeriesType:
		EVENT_TS = "EVENT_TS"
		STATE_TS = "STATE_TS"

	class ExpectLargeInts:
		IF_INPUT_VAL_IS_LONG_INTEGER = "IF_INPUT_VAL_IS_LONG_INTEGER"
		FALSE = "false"
		TRUE = "true"

	class ExpectDecimals:
		IF_INPUT_VAL_IS_DECIMAL = "IF_INPUT_VAL_IS_DECIMAL"
		FALSE = "false"
		TRUE = "true"

	def __init__(self, bucket_interval=0, bucket_interval_units=BucketIntervalUnits.SECONDS, output_interval="", output_interval_units=OutputIntervalUnits.SECONDS, is_running_aggr=False, bucket_time=BucketTime.BUCKET_END, bucket_end_criteria="", boundary_tick_bucket=BoundaryTickBucket.NEW, all_fields_for_sliding=AllFieldsForSliding.FALSE, partial_bucket_handling=PartialBucketHandling.AS_SEPARATE_BUCKET, output_field_name="VALUE", group_by="", groups_to_display=GroupsToDisplay.ALL, bucket_end_per_group=False, input_field_name="", time_series_type=TimeSeriesType.EVENT_TS, expect_large_ints=ExpectLargeInts.FALSE, expect_decimals=ExpectDecimals.FALSE, null_int_val=0, fwd_fill_if="", In = "", Out = ""):
		_graph_components.EpBase.__init__(self, "LAST")
		self._default_bucket_interval = 0
		self.bucket_interval = bucket_interval
		self._default_bucket_interval_units = type(self).BucketIntervalUnits.SECONDS
		self.bucket_interval_units = bucket_interval_units
		self._default_output_interval = ""
		self.output_interval = output_interval
		self._default_output_interval_units = type(self).OutputIntervalUnits.SECONDS
		self.output_interval_units = output_interval_units
		self._default_is_running_aggr = False
		self.is_running_aggr = is_running_aggr
		self._default_bucket_time = type(self).BucketTime.BUCKET_END
		self.bucket_time = bucket_time
		self._default_bucket_end_criteria = ""
		self.bucket_end_criteria = bucket_end_criteria
		self._default_boundary_tick_bucket = type(self).BoundaryTickBucket.NEW
		self.boundary_tick_bucket = boundary_tick_bucket
		self._default_all_fields_for_sliding = type(self).AllFieldsForSliding.FALSE
		self.all_fields_for_sliding = all_fields_for_sliding
		self._default_partial_bucket_handling = type(self).PartialBucketHandling.AS_SEPARATE_BUCKET
		self.partial_bucket_handling = partial_bucket_handling
		self._default_output_field_name = "VALUE"
		self.output_field_name = output_field_name
		self._default_group_by = ""
		self.group_by = group_by
		self._default_groups_to_display = type(self).GroupsToDisplay.ALL
		self.groups_to_display = groups_to_display
		self._default_bucket_end_per_group = False
		self.bucket_end_per_group = bucket_end_per_group
		self._default_input_field_name = ""
		self.input_field_name = input_field_name
		self._default_time_series_type = type(self).TimeSeriesType.EVENT_TS
		self.time_series_type = time_series_type
		self._default_expect_large_ints = type(self).ExpectLargeInts.FALSE
		self.expect_large_ints = expect_large_ints
		self._default_expect_decimals = type(self).ExpectDecimals.FALSE
		self.expect_decimals = expect_decimals
		self._default_null_int_val = 0
		self.null_int_val = null_int_val
		self._default_fwd_fill_if = ""
		self.fwd_fill_if = fwd_fill_if
		if In != "":
			self.input_field_name=In
		if Out != "":
			self.output_field_name=Out
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_bucket_interval(self, value):
		self.bucket_interval = value
		return self

	def set_bucket_interval_units(self, value):
		self.bucket_interval_units = value
		return self

	def set_output_interval(self, value):
		self.output_interval = value
		return self

	def set_output_interval_units(self, value):
		self.output_interval_units = value
		return self

	def set_is_running_aggr(self, value):
		self.is_running_aggr = value
		return self

	def set_bucket_time(self, value):
		self.bucket_time = value
		return self

	def set_bucket_end_criteria(self, value):
		self.bucket_end_criteria = value
		return self

	def set_boundary_tick_bucket(self, value):
		self.boundary_tick_bucket = value
		return self

	def set_all_fields_for_sliding(self, value):
		self.all_fields_for_sliding = value
		return self

	def set_partial_bucket_handling(self, value):
		self.partial_bucket_handling = value
		return self

	def set_output_field_name(self, value):
		self.output_field_name = value
		return self

	def set_group_by(self, value):
		self.group_by = value
		return self

	def set_groups_to_display(self, value):
		self.groups_to_display = value
		return self

	def set_bucket_end_per_group(self, value):
		self.bucket_end_per_group = value
		return self

	def set_input_field_name(self, value):
		self.input_field_name = value
		return self

	def set_time_series_type(self, value):
		self.time_series_type = value
		return self

	def set_expect_large_ints(self, value):
		self.expect_large_ints = value
		return self

	def set_expect_decimals(self, value):
		self.expect_decimals = value
		return self

	def set_null_int_val(self, value):
		self.null_int_val = value
		return self

	def set_fwd_fill_if(self, value):
		self.fwd_fill_if = value
		return self

	@staticmethod
	def _get_name():
		return "LAST"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.bucket_interval != 0: 
			desc += "BUCKET_INTERVAL=" + py_to_str(self.bucket_interval) + ","
		if self.bucket_interval_units != self.BucketIntervalUnits.SECONDS: 
			desc += "BUCKET_INTERVAL_UNITS=" + py_to_str(self.bucket_interval_units) + ","
		if self.output_interval != "": 
			desc += "OUTPUT_INTERVAL=" + py_to_str(self.output_interval) + ","
		if self.output_interval_units != self.OutputIntervalUnits.SECONDS: 
			desc += "OUTPUT_INTERVAL_UNITS=" + py_to_str(self.output_interval_units) + ","
		if self.is_running_aggr != False: 
			desc += "IS_RUNNING_AGGR=" + py_to_str(self.is_running_aggr) + ","
		if self.bucket_time != self.BucketTime.BUCKET_END: 
			desc += "BUCKET_TIME=" + py_to_str(self.bucket_time) + ","
		if self.bucket_end_criteria != "": 
			desc += "BUCKET_END_CRITERIA=" + py_to_str(self.bucket_end_criteria) + ","
		if self.boundary_tick_bucket != self.BoundaryTickBucket.NEW: 
			desc += "BOUNDARY_TICK_BUCKET=" + py_to_str(self.boundary_tick_bucket) + ","
		if self.all_fields_for_sliding != self.AllFieldsForSliding.FALSE: 
			desc += "ALL_FIELDS_FOR_SLIDING=" + py_to_str(self.all_fields_for_sliding) + ","
		if self.partial_bucket_handling != self.PartialBucketHandling.AS_SEPARATE_BUCKET: 
			desc += "PARTIAL_BUCKET_HANDLING=" + py_to_str(self.partial_bucket_handling) + ","
		if self.output_field_name != "VALUE": 
			desc += "OUTPUT_FIELD_NAME=" + py_to_str(self.output_field_name) + ","
		if self.group_by != "": 
			desc += "GROUP_BY=" + py_to_str(self.group_by) + ","
		if self.groups_to_display != self.GroupsToDisplay.ALL: 
			desc += "GROUPS_TO_DISPLAY=" + py_to_str(self.groups_to_display) + ","
		if self.bucket_end_per_group != False: 
			desc += "BUCKET_END_PER_GROUP=" + py_to_str(self.bucket_end_per_group) + ","
		if self.input_field_name != "": 
			desc += "INPUT_FIELD_NAME=" + py_to_str(self.input_field_name) + ","
		if self.time_series_type != self.TimeSeriesType.EVENT_TS: 
			desc += "TIME_SERIES_TYPE=" + py_to_str(self.time_series_type) + ","
		if self.expect_large_ints != self.ExpectLargeInts.FALSE: 
			desc += "EXPECT_LARGE_INTS=" + py_to_str(self.expect_large_ints) + ","
		if self.expect_decimals != self.ExpectDecimals.FALSE: 
			desc += "EXPECT_DECIMALS=" + py_to_str(self.expect_decimals) + ","
		if self.null_int_val != 0: 
			desc += "NULL_INT_VAL=" + py_to_str(self.null_int_val) + ","
		if self.fwd_fill_if != "": 
			desc += "FWD_FILL_IF=" + py_to_str(self.fwd_fill_if) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class FirstTime(_graph_components.EpBase):
	"""
		

FIRST_TIME

Type: Aggregation

Description: For each bucket, gets the timestamp of the first
tick in it.

Python
class name:
FirstTime

Input: A time series of ticks

Output: A time series of ticks, one tick for each bucket
interval.

Parameters: See parameters
common to generic aggregations.


  BUCKET_INTERVAL
(seconds/ticks)
  BUCKET_INTERVAL_UNITS
(enumerated type)
  OUTPUT_INTERVAL
(seconds)
  OUTPUT_INTERVAL_UNITS
(SECONDS/TICKS)
  IS_RUNNING_AGGR
(Boolean)
  BUCKET_TIME
(Boolean)
  BUCKET_END_CRITERIA
(expression)
  BOUNDARY_TICK_BUCKET
(NEW/PREVIOUS)
  ALL_FIELDS_FOR_SLIDING
(Boolean)
  PARTIAL_BUCKET_HANDLING
(enumerated type)
  OUTPUT_FIELD_NAME
(string)
  GROUP_BY
(string)
  GROUPS_TO_DISPLAY
(enumerated)
  BUCKET_END_PER_GROUP
(Boolean)
  TIME_SERIES_TYPE (enumerated type)
    If set to STATE_TS, the latest tick before the current bucket
will be used as initial tick for the current bucket (unless the current
bucket has a tick at its start time) in computation of FIRST_TIME. If
set to EVENT_TS, only ticks from the current bucket will be used in
computation of FIRST_TIME.
Default: EVENT_TS

  

Notes: See the notes on generic
aggregations.

Examples: See the following:

See the FIRST_TIME example in AGGREGATION_EXAMPLES.otq

See the FIRST_LAST_TIME example
in AGGREGATION_FIRST_LAST_EXAMPLES.otq


	"""
	class Parameters:
		bucket_interval = "BUCKET_INTERVAL"
		bucket_interval_units = "BUCKET_INTERVAL_UNITS"
		output_interval = "OUTPUT_INTERVAL"
		output_interval_units = "OUTPUT_INTERVAL_UNITS"
		is_running_aggr = "IS_RUNNING_AGGR"
		bucket_time = "BUCKET_TIME"
		bucket_end_criteria = "BUCKET_END_CRITERIA"
		boundary_tick_bucket = "BOUNDARY_TICK_BUCKET"
		all_fields_for_sliding = "ALL_FIELDS_FOR_SLIDING"
		partial_bucket_handling = "PARTIAL_BUCKET_HANDLING"
		output_field_name = "OUTPUT_FIELD_NAME"
		group_by = "GROUP_BY"
		groups_to_display = "GROUPS_TO_DISPLAY"
		bucket_end_per_group = "BUCKET_END_PER_GROUP"
		time_series_type = "TIME_SERIES_TYPE"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["bucket_interval", "bucket_interval_units", "output_interval", "output_interval_units", "is_running_aggr", "bucket_time", "bucket_end_criteria", "boundary_tick_bucket", "all_fields_for_sliding", "partial_bucket_handling", "output_field_name", "group_by", "groups_to_display", "bucket_end_per_group", "time_series_type"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["bucket_interval", "_default_bucket_interval", "bucket_interval_units", "_default_bucket_interval_units", "output_interval", "_default_output_interval", "output_interval_units", "_default_output_interval_units", "is_running_aggr", "_default_is_running_aggr", "bucket_time", "_default_bucket_time", "bucket_end_criteria", "_default_bucket_end_criteria", "boundary_tick_bucket", "_default_boundary_tick_bucket", "all_fields_for_sliding", "_default_all_fields_for_sliding", "partial_bucket_handling", "_default_partial_bucket_handling", "output_field_name", "_default_output_field_name", "group_by", "_default_group_by", "groups_to_display", "_default_groups_to_display", "bucket_end_per_group", "_default_bucket_end_per_group", "time_series_type", "_default_time_series_type", "stack_info", "_used_strings"]

	class BucketIntervalUnits:
		DAYS = "DAYS"
		FLEXIBLE = "FLEXIBLE"
		MONTHS = "MONTHS"
		SECONDS = "SECONDS"
		TICKS = "TICKS"

	class OutputIntervalUnits:
		SECONDS = "SECONDS"
		TICKS = "TICKS"

	class BucketTime:
		BUCKET_END = "BUCKET_END"
		BUCKET_START = "BUCKET_START"

	class BoundaryTickBucket:
		NEW = "NEW"
		PREVIOUS = "PREVIOUS"

	class AllFieldsForSliding:
		WHEN_TICKS_EXIT_WINDOW = "WHEN_TICKS_EXIT_WINDOW"
		FALSE = "false"
		TRUE = "true"

	class PartialBucketHandling:
		AS_SEPARATE_BUCKET = "AS_SEPARATE_BUCKET"
		DISCARD = "DISCARD"
		MERGE_WITH_PREVIOUS = "MERGE_WITH_PREVIOUS"

	class GroupsToDisplay:
		ALL = "ALL"
		EVENT_IN_LAST_BUCKET = "EVENT_IN_LAST_BUCKET"

	class TimeSeriesType:
		EVENT_TS = "EVENT_TS"
		STATE_TS = "STATE_TS"

	def __init__(self, bucket_interval=0, bucket_interval_units=BucketIntervalUnits.SECONDS, output_interval="", output_interval_units=OutputIntervalUnits.SECONDS, is_running_aggr=False, bucket_time=BucketTime.BUCKET_END, bucket_end_criteria="", boundary_tick_bucket=BoundaryTickBucket.NEW, all_fields_for_sliding=AllFieldsForSliding.FALSE, partial_bucket_handling=PartialBucketHandling.AS_SEPARATE_BUCKET, output_field_name="VALUE", group_by="", groups_to_display=GroupsToDisplay.ALL, bucket_end_per_group=False, time_series_type=TimeSeriesType.EVENT_TS, Out = ""):
		_graph_components.EpBase.__init__(self, "FIRST_TIME")
		self._default_bucket_interval = 0
		self.bucket_interval = bucket_interval
		self._default_bucket_interval_units = type(self).BucketIntervalUnits.SECONDS
		self.bucket_interval_units = bucket_interval_units
		self._default_output_interval = ""
		self.output_interval = output_interval
		self._default_output_interval_units = type(self).OutputIntervalUnits.SECONDS
		self.output_interval_units = output_interval_units
		self._default_is_running_aggr = False
		self.is_running_aggr = is_running_aggr
		self._default_bucket_time = type(self).BucketTime.BUCKET_END
		self.bucket_time = bucket_time
		self._default_bucket_end_criteria = ""
		self.bucket_end_criteria = bucket_end_criteria
		self._default_boundary_tick_bucket = type(self).BoundaryTickBucket.NEW
		self.boundary_tick_bucket = boundary_tick_bucket
		self._default_all_fields_for_sliding = type(self).AllFieldsForSliding.FALSE
		self.all_fields_for_sliding = all_fields_for_sliding
		self._default_partial_bucket_handling = type(self).PartialBucketHandling.AS_SEPARATE_BUCKET
		self.partial_bucket_handling = partial_bucket_handling
		self._default_output_field_name = "VALUE"
		self.output_field_name = output_field_name
		self._default_group_by = ""
		self.group_by = group_by
		self._default_groups_to_display = type(self).GroupsToDisplay.ALL
		self.groups_to_display = groups_to_display
		self._default_bucket_end_per_group = False
		self.bucket_end_per_group = bucket_end_per_group
		self._default_time_series_type = type(self).TimeSeriesType.EVENT_TS
		self.time_series_type = time_series_type
		if Out != "":
			self.output_field_name=Out
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_bucket_interval(self, value):
		self.bucket_interval = value
		return self

	def set_bucket_interval_units(self, value):
		self.bucket_interval_units = value
		return self

	def set_output_interval(self, value):
		self.output_interval = value
		return self

	def set_output_interval_units(self, value):
		self.output_interval_units = value
		return self

	def set_is_running_aggr(self, value):
		self.is_running_aggr = value
		return self

	def set_bucket_time(self, value):
		self.bucket_time = value
		return self

	def set_bucket_end_criteria(self, value):
		self.bucket_end_criteria = value
		return self

	def set_boundary_tick_bucket(self, value):
		self.boundary_tick_bucket = value
		return self

	def set_all_fields_for_sliding(self, value):
		self.all_fields_for_sliding = value
		return self

	def set_partial_bucket_handling(self, value):
		self.partial_bucket_handling = value
		return self

	def set_output_field_name(self, value):
		self.output_field_name = value
		return self

	def set_group_by(self, value):
		self.group_by = value
		return self

	def set_groups_to_display(self, value):
		self.groups_to_display = value
		return self

	def set_bucket_end_per_group(self, value):
		self.bucket_end_per_group = value
		return self

	def set_time_series_type(self, value):
		self.time_series_type = value
		return self

	@staticmethod
	def _get_name():
		return "FIRST_TIME"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.bucket_interval != 0: 
			desc += "BUCKET_INTERVAL=" + py_to_str(self.bucket_interval) + ","
		if self.bucket_interval_units != self.BucketIntervalUnits.SECONDS: 
			desc += "BUCKET_INTERVAL_UNITS=" + py_to_str(self.bucket_interval_units) + ","
		if self.output_interval != "": 
			desc += "OUTPUT_INTERVAL=" + py_to_str(self.output_interval) + ","
		if self.output_interval_units != self.OutputIntervalUnits.SECONDS: 
			desc += "OUTPUT_INTERVAL_UNITS=" + py_to_str(self.output_interval_units) + ","
		if self.is_running_aggr != False: 
			desc += "IS_RUNNING_AGGR=" + py_to_str(self.is_running_aggr) + ","
		if self.bucket_time != self.BucketTime.BUCKET_END: 
			desc += "BUCKET_TIME=" + py_to_str(self.bucket_time) + ","
		if self.bucket_end_criteria != "": 
			desc += "BUCKET_END_CRITERIA=" + py_to_str(self.bucket_end_criteria) + ","
		if self.boundary_tick_bucket != self.BoundaryTickBucket.NEW: 
			desc += "BOUNDARY_TICK_BUCKET=" + py_to_str(self.boundary_tick_bucket) + ","
		if self.all_fields_for_sliding != self.AllFieldsForSliding.FALSE: 
			desc += "ALL_FIELDS_FOR_SLIDING=" + py_to_str(self.all_fields_for_sliding) + ","
		if self.partial_bucket_handling != self.PartialBucketHandling.AS_SEPARATE_BUCKET: 
			desc += "PARTIAL_BUCKET_HANDLING=" + py_to_str(self.partial_bucket_handling) + ","
		if self.output_field_name != "VALUE": 
			desc += "OUTPUT_FIELD_NAME=" + py_to_str(self.output_field_name) + ","
		if self.group_by != "": 
			desc += "GROUP_BY=" + py_to_str(self.group_by) + ","
		if self.groups_to_display != self.GroupsToDisplay.ALL: 
			desc += "GROUPS_TO_DISPLAY=" + py_to_str(self.groups_to_display) + ","
		if self.bucket_end_per_group != False: 
			desc += "BUCKET_END_PER_GROUP=" + py_to_str(self.bucket_end_per_group) + ","
		if self.time_series_type != self.TimeSeriesType.EVENT_TS: 
			desc += "TIME_SERIES_TYPE=" + py_to_str(self.time_series_type) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class LastTime(_graph_components.EpBase):
	"""
		

LAST_TIME

Type: Aggregation

Description: For each bucket, gets the timestamp of the last
tick in it.

Python
class name:
LastTime

Input: A time series of ticks.

Output: A time series of ticks, one tick for each bucket
interval.

Parameters: See parameters
common to generic aggregations.


  BUCKET_INTERVAL
(seconds/ticks)
  BUCKET_INTERVAL_UNITS
(enumerated type)
  OUTPUT_INTERVAL
(seconds)
  OUTPUT_INTERVAL_UNITS
(SECONDS/TICKS)
  IS_RUNNING_AGGR
(Boolean)
  BUCKET_TIME
(Boolean)
  BUCKET_END_CRITERIA
(expression)
  BOUNDARY_TICK_BUCKET
(NEW/PREVIOUS)
  ALL_FIELDS_FOR_SLIDING
(Boolean)
  PARTIAL_BUCKET_HANDLING
(enumerated type)
  OUTPUT_FIELD_NAME
(string)
  GROUP_BY
(string)
  GROUPS_TO_DISPLAY
(enumerated)
  BUCKET_END_PER_GROUP
(Boolean)
  TIME_SERIES_TYPE (enumerated type)
    If set to STATE_TS, the latest tick before the current bucket
will be used as an initial tick for the current bucket (unless the
current bucket has a tick at its start time) in the computation of
LAST_TIME. If set to EVENT_TS, only ticks from the current bucket will
be used in the computation of LAST_TIME.
Default: EVENT_TS

  

Notes: See the notes on generic
aggregations.

Examples: See the following:

See the LAST_TIME example in AGGREGATION_EXAMPLES.otq
and FIRST_LAST_TIME in AGGREGATION_FIRST_LAST_EXAMPLES.otq


	"""
	class Parameters:
		bucket_interval = "BUCKET_INTERVAL"
		bucket_interval_units = "BUCKET_INTERVAL_UNITS"
		output_interval = "OUTPUT_INTERVAL"
		output_interval_units = "OUTPUT_INTERVAL_UNITS"
		is_running_aggr = "IS_RUNNING_AGGR"
		bucket_time = "BUCKET_TIME"
		bucket_end_criteria = "BUCKET_END_CRITERIA"
		boundary_tick_bucket = "BOUNDARY_TICK_BUCKET"
		all_fields_for_sliding = "ALL_FIELDS_FOR_SLIDING"
		partial_bucket_handling = "PARTIAL_BUCKET_HANDLING"
		output_field_name = "OUTPUT_FIELD_NAME"
		group_by = "GROUP_BY"
		groups_to_display = "GROUPS_TO_DISPLAY"
		bucket_end_per_group = "BUCKET_END_PER_GROUP"
		time_series_type = "TIME_SERIES_TYPE"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["bucket_interval", "bucket_interval_units", "output_interval", "output_interval_units", "is_running_aggr", "bucket_time", "bucket_end_criteria", "boundary_tick_bucket", "all_fields_for_sliding", "partial_bucket_handling", "output_field_name", "group_by", "groups_to_display", "bucket_end_per_group", "time_series_type"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["bucket_interval", "_default_bucket_interval", "bucket_interval_units", "_default_bucket_interval_units", "output_interval", "_default_output_interval", "output_interval_units", "_default_output_interval_units", "is_running_aggr", "_default_is_running_aggr", "bucket_time", "_default_bucket_time", "bucket_end_criteria", "_default_bucket_end_criteria", "boundary_tick_bucket", "_default_boundary_tick_bucket", "all_fields_for_sliding", "_default_all_fields_for_sliding", "partial_bucket_handling", "_default_partial_bucket_handling", "output_field_name", "_default_output_field_name", "group_by", "_default_group_by", "groups_to_display", "_default_groups_to_display", "bucket_end_per_group", "_default_bucket_end_per_group", "time_series_type", "_default_time_series_type", "stack_info", "_used_strings"]

	class BucketIntervalUnits:
		DAYS = "DAYS"
		FLEXIBLE = "FLEXIBLE"
		MONTHS = "MONTHS"
		SECONDS = "SECONDS"
		TICKS = "TICKS"

	class OutputIntervalUnits:
		SECONDS = "SECONDS"
		TICKS = "TICKS"

	class BucketTime:
		BUCKET_END = "BUCKET_END"
		BUCKET_START = "BUCKET_START"

	class BoundaryTickBucket:
		NEW = "NEW"
		PREVIOUS = "PREVIOUS"

	class AllFieldsForSliding:
		WHEN_TICKS_EXIT_WINDOW = "WHEN_TICKS_EXIT_WINDOW"
		FALSE = "false"
		TRUE = "true"

	class PartialBucketHandling:
		AS_SEPARATE_BUCKET = "AS_SEPARATE_BUCKET"
		DISCARD = "DISCARD"
		MERGE_WITH_PREVIOUS = "MERGE_WITH_PREVIOUS"

	class GroupsToDisplay:
		ALL = "ALL"
		EVENT_IN_LAST_BUCKET = "EVENT_IN_LAST_BUCKET"

	class TimeSeriesType:
		EVENT_TS = "EVENT_TS"
		STATE_TS = "STATE_TS"

	def __init__(self, bucket_interval=0, bucket_interval_units=BucketIntervalUnits.SECONDS, output_interval="", output_interval_units=OutputIntervalUnits.SECONDS, is_running_aggr=False, bucket_time=BucketTime.BUCKET_END, bucket_end_criteria="", boundary_tick_bucket=BoundaryTickBucket.NEW, all_fields_for_sliding=AllFieldsForSliding.FALSE, partial_bucket_handling=PartialBucketHandling.AS_SEPARATE_BUCKET, output_field_name="VALUE", group_by="", groups_to_display=GroupsToDisplay.ALL, bucket_end_per_group=False, time_series_type=TimeSeriesType.EVENT_TS, Out = ""):
		_graph_components.EpBase.__init__(self, "LAST_TIME")
		self._default_bucket_interval = 0
		self.bucket_interval = bucket_interval
		self._default_bucket_interval_units = type(self).BucketIntervalUnits.SECONDS
		self.bucket_interval_units = bucket_interval_units
		self._default_output_interval = ""
		self.output_interval = output_interval
		self._default_output_interval_units = type(self).OutputIntervalUnits.SECONDS
		self.output_interval_units = output_interval_units
		self._default_is_running_aggr = False
		self.is_running_aggr = is_running_aggr
		self._default_bucket_time = type(self).BucketTime.BUCKET_END
		self.bucket_time = bucket_time
		self._default_bucket_end_criteria = ""
		self.bucket_end_criteria = bucket_end_criteria
		self._default_boundary_tick_bucket = type(self).BoundaryTickBucket.NEW
		self.boundary_tick_bucket = boundary_tick_bucket
		self._default_all_fields_for_sliding = type(self).AllFieldsForSliding.FALSE
		self.all_fields_for_sliding = all_fields_for_sliding
		self._default_partial_bucket_handling = type(self).PartialBucketHandling.AS_SEPARATE_BUCKET
		self.partial_bucket_handling = partial_bucket_handling
		self._default_output_field_name = "VALUE"
		self.output_field_name = output_field_name
		self._default_group_by = ""
		self.group_by = group_by
		self._default_groups_to_display = type(self).GroupsToDisplay.ALL
		self.groups_to_display = groups_to_display
		self._default_bucket_end_per_group = False
		self.bucket_end_per_group = bucket_end_per_group
		self._default_time_series_type = type(self).TimeSeriesType.EVENT_TS
		self.time_series_type = time_series_type
		if Out != "":
			self.output_field_name=Out
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_bucket_interval(self, value):
		self.bucket_interval = value
		return self

	def set_bucket_interval_units(self, value):
		self.bucket_interval_units = value
		return self

	def set_output_interval(self, value):
		self.output_interval = value
		return self

	def set_output_interval_units(self, value):
		self.output_interval_units = value
		return self

	def set_is_running_aggr(self, value):
		self.is_running_aggr = value
		return self

	def set_bucket_time(self, value):
		self.bucket_time = value
		return self

	def set_bucket_end_criteria(self, value):
		self.bucket_end_criteria = value
		return self

	def set_boundary_tick_bucket(self, value):
		self.boundary_tick_bucket = value
		return self

	def set_all_fields_for_sliding(self, value):
		self.all_fields_for_sliding = value
		return self

	def set_partial_bucket_handling(self, value):
		self.partial_bucket_handling = value
		return self

	def set_output_field_name(self, value):
		self.output_field_name = value
		return self

	def set_group_by(self, value):
		self.group_by = value
		return self

	def set_groups_to_display(self, value):
		self.groups_to_display = value
		return self

	def set_bucket_end_per_group(self, value):
		self.bucket_end_per_group = value
		return self

	def set_time_series_type(self, value):
		self.time_series_type = value
		return self

	@staticmethod
	def _get_name():
		return "LAST_TIME"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.bucket_interval != 0: 
			desc += "BUCKET_INTERVAL=" + py_to_str(self.bucket_interval) + ","
		if self.bucket_interval_units != self.BucketIntervalUnits.SECONDS: 
			desc += "BUCKET_INTERVAL_UNITS=" + py_to_str(self.bucket_interval_units) + ","
		if self.output_interval != "": 
			desc += "OUTPUT_INTERVAL=" + py_to_str(self.output_interval) + ","
		if self.output_interval_units != self.OutputIntervalUnits.SECONDS: 
			desc += "OUTPUT_INTERVAL_UNITS=" + py_to_str(self.output_interval_units) + ","
		if self.is_running_aggr != False: 
			desc += "IS_RUNNING_AGGR=" + py_to_str(self.is_running_aggr) + ","
		if self.bucket_time != self.BucketTime.BUCKET_END: 
			desc += "BUCKET_TIME=" + py_to_str(self.bucket_time) + ","
		if self.bucket_end_criteria != "": 
			desc += "BUCKET_END_CRITERIA=" + py_to_str(self.bucket_end_criteria) + ","
		if self.boundary_tick_bucket != self.BoundaryTickBucket.NEW: 
			desc += "BOUNDARY_TICK_BUCKET=" + py_to_str(self.boundary_tick_bucket) + ","
		if self.all_fields_for_sliding != self.AllFieldsForSliding.FALSE: 
			desc += "ALL_FIELDS_FOR_SLIDING=" + py_to_str(self.all_fields_for_sliding) + ","
		if self.partial_bucket_handling != self.PartialBucketHandling.AS_SEPARATE_BUCKET: 
			desc += "PARTIAL_BUCKET_HANDLING=" + py_to_str(self.partial_bucket_handling) + ","
		if self.output_field_name != "VALUE": 
			desc += "OUTPUT_FIELD_NAME=" + py_to_str(self.output_field_name) + ","
		if self.group_by != "": 
			desc += "GROUP_BY=" + py_to_str(self.group_by) + ","
		if self.groups_to_display != self.GroupsToDisplay.ALL: 
			desc += "GROUPS_TO_DISPLAY=" + py_to_str(self.groups_to_display) + ","
		if self.bucket_end_per_group != False: 
			desc += "BUCKET_END_PER_GROUP=" + py_to_str(self.bucket_end_per_group) + ","
		if self.time_series_type != self.TimeSeriesType.EVENT_TS: 
			desc += "TIME_SERIES_TYPE=" + py_to_str(self.time_series_type) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class LastTick(_graph_components.EpBase):
	"""
		

LAST_TICK

Type: Aggregation

Description: For each bucket, propagates only its last tick
(or last n ticks), adding the field with the timestamp of the
tick. If a bucket is empty, no tick is sent to the output stream.

Python
class name:&nbsp;LastTick

Input: A time series of ticks.

Output: A time series of ticks, n ticks for each
bucket.

Parameters: See parameters
common to generic aggregations.


  BUCKET_INTERVAL
(seconds/ticks)
  BUCKET_INTERVAL_UNITS
(enumerated type)
  BUCKET_TIME
(Boolean)
  BUCKET_END_CRITERIA
(expression)
  BOUNDARY_TICK_BUCKET
(NEW/PREVIOUS)
  PARTIAL_BUCKET_HANDLING
(enumerated type)
  IS_RUNNING_AGGR
(Boolean)
  NUM_TICKS (integer)
    Specifies the number (n) of ticks (from the last tick) to
propagate.

    Setting it to 1 will propagate only the last tick; 2 will
propagate the last 2 ticks.

  
  DEFAULT_TICK (field_name type
specification [,field_name type specification] )
    A comma-separated list of fields that should match to schema of
the input stream.
When DEFAULT_TICK is set, exactly one tick with default values will be
created for each empty bucket.
If no default value is specified for a missing field in the list, then double/decimal
fields will be initialized to NaN, integer
fields (of all sizes) and timestamp fields will be initialized
to 0, and string fields will
be initialized to an empty string.
Supported types are listed here.
Default: empty

  
  TIME_SERIES_TYPE (enumerated type)
    If set to STATE_TS, the latest ticks before the current bucket
will be used as initial ticks for the current bucket in the computation
of LAST_TICK. If set to EVENT_TS, only ticks from the current bucket
will be used in the computation of LAST_TICK.
Default: EVENT_TS

  
  KEEP_INITIAL_SCHEMA (Boolean)
    If set to true, and if the input schema changes, all input ticks
after that change will be converted to have the initial input schema.
Default: false

  
  GROUP_BY
(string)
  GROUPS_TO_DISPLAY
(enumerated)
  BUCKET_END_PER_GROUP
(Boolean)
  ORDER_BY_TICK_TIME (Boolean)
    If set to true, order output ticks chronologically, according to
the values of TICK_TIME field, across all GROUP_BY keys. The value of
parameter GROUP_BY must be non-empty for this parameter to be set.
Default: false

  

Note: Running PASSTHROUGH along with LAST_TICK would produce
the same desired results ordinarily obtained from the IS_RUNNING_AGGR
parameter.

Note: Presence of DEFAULT_TICK is mandatory when LAST_TICK is
used inside COMPUTE EP.

Examples: See the LAST_TICK
example in AGGREGATION_EXAMPLES.otq
and FIRST_LAST_TICK in AGGREGATION_FIRST_LAST_EXAMPLES.otq.


	"""
	class Parameters:
		bucket_interval = "BUCKET_INTERVAL"
		bucket_interval_units = "BUCKET_INTERVAL_UNITS"
		bucket_time = "BUCKET_TIME"
		bucket_end_criteria = "BUCKET_END_CRITERIA"
		boundary_tick_bucket = "BOUNDARY_TICK_BUCKET"
		partial_bucket_handling = "PARTIAL_BUCKET_HANDLING"
		is_running_aggr = "IS_RUNNING_AGGR"
		num_ticks = "NUM_TICKS"
		default_tick = "DEFAULT_TICK"
		time_series_type = "TIME_SERIES_TYPE"
		group_by = "GROUP_BY"
		groups_to_display = "GROUPS_TO_DISPLAY"
		bucket_end_per_group = "BUCKET_END_PER_GROUP"
		keep_initial_schema = "KEEP_INITIAL_SCHEMA"
		order_by_tick_time = "ORDER_BY_TICK_TIME"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["bucket_interval", "bucket_interval_units", "bucket_time", "bucket_end_criteria", "boundary_tick_bucket", "partial_bucket_handling", "is_running_aggr", "num_ticks", "default_tick", "time_series_type", "group_by", "groups_to_display", "bucket_end_per_group", "keep_initial_schema", "order_by_tick_time"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["bucket_interval", "_default_bucket_interval", "bucket_interval_units", "_default_bucket_interval_units", "bucket_time", "_default_bucket_time", "bucket_end_criteria", "_default_bucket_end_criteria", "boundary_tick_bucket", "_default_boundary_tick_bucket", "partial_bucket_handling", "_default_partial_bucket_handling", "is_running_aggr", "_default_is_running_aggr", "num_ticks", "_default_num_ticks", "default_tick", "_default_default_tick", "time_series_type", "_default_time_series_type", "group_by", "_default_group_by", "groups_to_display", "_default_groups_to_display", "bucket_end_per_group", "_default_bucket_end_per_group", "keep_initial_schema", "_default_keep_initial_schema", "order_by_tick_time", "_default_order_by_tick_time", "stack_info", "_used_strings"]

	class BucketIntervalUnits:
		DAYS = "DAYS"
		FLEXIBLE = "FLEXIBLE"
		MONTHS = "MONTHS"
		SECONDS = "SECONDS"
		TICKS = "TICKS"

	class BucketTime:
		BUCKET_END = "BUCKET_END"
		BUCKET_START = "BUCKET_START"

	class BoundaryTickBucket:
		NEW = "NEW"
		PREVIOUS = "PREVIOUS"

	class PartialBucketHandling:
		AS_SEPARATE_BUCKET = "AS_SEPARATE_BUCKET"
		DISCARD = "DISCARD"
		MERGE_WITH_PREVIOUS = "MERGE_WITH_PREVIOUS"

	class TimeSeriesType:
		EVENT_TS = "EVENT_TS"
		STATE_TS = "STATE_TS"

	class GroupsToDisplay:
		ALL = "ALL"
		EVENT_IN_LAST_BUCKET = "EVENT_IN_LAST_BUCKET"

	def __init__(self, bucket_interval=0, bucket_interval_units=BucketIntervalUnits.SECONDS, bucket_time=BucketTime.BUCKET_END, bucket_end_criteria="", boundary_tick_bucket=BoundaryTickBucket.NEW, partial_bucket_handling=PartialBucketHandling.AS_SEPARATE_BUCKET, is_running_aggr=False, num_ticks=1, default_tick="", time_series_type=TimeSeriesType.EVENT_TS, group_by="", groups_to_display=GroupsToDisplay.ALL, bucket_end_per_group=False, keep_initial_schema=False, order_by_tick_time=False):
		_graph_components.EpBase.__init__(self, "LAST_TICK")
		self._default_bucket_interval = 0
		self.bucket_interval = bucket_interval
		self._default_bucket_interval_units = type(self).BucketIntervalUnits.SECONDS
		self.bucket_interval_units = bucket_interval_units
		self._default_bucket_time = type(self).BucketTime.BUCKET_END
		self.bucket_time = bucket_time
		self._default_bucket_end_criteria = ""
		self.bucket_end_criteria = bucket_end_criteria
		self._default_boundary_tick_bucket = type(self).BoundaryTickBucket.NEW
		self.boundary_tick_bucket = boundary_tick_bucket
		self._default_partial_bucket_handling = type(self).PartialBucketHandling.AS_SEPARATE_BUCKET
		self.partial_bucket_handling = partial_bucket_handling
		self._default_is_running_aggr = False
		self.is_running_aggr = is_running_aggr
		self._default_num_ticks = 1
		self.num_ticks = num_ticks
		self._default_default_tick = ""
		self.default_tick = default_tick
		self._default_time_series_type = type(self).TimeSeriesType.EVENT_TS
		self.time_series_type = time_series_type
		self._default_group_by = ""
		self.group_by = group_by
		self._default_groups_to_display = type(self).GroupsToDisplay.ALL
		self.groups_to_display = groups_to_display
		self._default_bucket_end_per_group = False
		self.bucket_end_per_group = bucket_end_per_group
		self._default_keep_initial_schema = False
		self.keep_initial_schema = keep_initial_schema
		self._default_order_by_tick_time = False
		self.order_by_tick_time = order_by_tick_time
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_bucket_interval(self, value):
		self.bucket_interval = value
		return self

	def set_bucket_interval_units(self, value):
		self.bucket_interval_units = value
		return self

	def set_bucket_time(self, value):
		self.bucket_time = value
		return self

	def set_bucket_end_criteria(self, value):
		self.bucket_end_criteria = value
		return self

	def set_boundary_tick_bucket(self, value):
		self.boundary_tick_bucket = value
		return self

	def set_partial_bucket_handling(self, value):
		self.partial_bucket_handling = value
		return self

	def set_is_running_aggr(self, value):
		self.is_running_aggr = value
		return self

	def set_num_ticks(self, value):
		self.num_ticks = value
		return self

	def set_default_tick(self, value):
		self.default_tick = value
		return self

	def set_time_series_type(self, value):
		self.time_series_type = value
		return self

	def set_group_by(self, value):
		self.group_by = value
		return self

	def set_groups_to_display(self, value):
		self.groups_to_display = value
		return self

	def set_bucket_end_per_group(self, value):
		self.bucket_end_per_group = value
		return self

	def set_keep_initial_schema(self, value):
		self.keep_initial_schema = value
		return self

	def set_order_by_tick_time(self, value):
		self.order_by_tick_time = value
		return self

	@staticmethod
	def _get_name():
		return "LAST_TICK"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.bucket_interval != 0: 
			desc += "BUCKET_INTERVAL=" + py_to_str(self.bucket_interval) + ","
		if self.bucket_interval_units != self.BucketIntervalUnits.SECONDS: 
			desc += "BUCKET_INTERVAL_UNITS=" + py_to_str(self.bucket_interval_units) + ","
		if self.bucket_time != self.BucketTime.BUCKET_END: 
			desc += "BUCKET_TIME=" + py_to_str(self.bucket_time) + ","
		if self.bucket_end_criteria != "": 
			desc += "BUCKET_END_CRITERIA=" + py_to_str(self.bucket_end_criteria) + ","
		if self.boundary_tick_bucket != self.BoundaryTickBucket.NEW: 
			desc += "BOUNDARY_TICK_BUCKET=" + py_to_str(self.boundary_tick_bucket) + ","
		if self.partial_bucket_handling != self.PartialBucketHandling.AS_SEPARATE_BUCKET: 
			desc += "PARTIAL_BUCKET_HANDLING=" + py_to_str(self.partial_bucket_handling) + ","
		if self.is_running_aggr != False: 
			desc += "IS_RUNNING_AGGR=" + py_to_str(self.is_running_aggr) + ","
		if self.num_ticks != 1: 
			desc += "NUM_TICKS=" + py_to_str(self.num_ticks) + ","
		if self.default_tick != "": 
			desc += "DEFAULT_TICK=" + py_to_str(self.default_tick) + ","
		if self.time_series_type != self.TimeSeriesType.EVENT_TS: 
			desc += "TIME_SERIES_TYPE=" + py_to_str(self.time_series_type) + ","
		if self.group_by != "": 
			desc += "GROUP_BY=" + py_to_str(self.group_by) + ","
		if self.groups_to_display != self.GroupsToDisplay.ALL: 
			desc += "GROUPS_TO_DISPLAY=" + py_to_str(self.groups_to_display) + ","
		if self.bucket_end_per_group != False: 
			desc += "BUCKET_END_PER_GROUP=" + py_to_str(self.bucket_end_per_group) + ","
		if self.keep_initial_schema != False: 
			desc += "KEEP_INITIAL_SCHEMA=" + py_to_str(self.keep_initial_schema) + ","
		if self.order_by_tick_time != False: 
			desc += "ORDER_BY_TICK_TIME=" + py_to_str(self.order_by_tick_time) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class HighTime(_graph_components.EpBase):
	"""
		

HIGH_TIME

Type: Aggregation

Description: For each bucket, gets the timestamp of
first/last tick in it with the highest value of the input field.

Python
class name:
HighTime

Input: A time series of ticks.

Output: A time series of ticks, one tick for each bucket
interval.

Parameters: See parameters
common to generic aggregations.


  BUCKET_INTERVAL
(seconds/ticks)
  BUCKET_INTERVAL_UNITS
(enumerated type)
  OUTPUT_INTERVAL
(seconds)
  OUTPUT_INTERVAL_UNITS
(SECONDS/TICKS)
  IS_RUNNING_AGGR
(Boolean)
  BUCKET_TIME
(Boolean)
  BUCKET_END_CRITERIA
(expression)
  BOUNDARY_TICK_BUCKET
(NEW/PREVIOUS)
  ALL_FIELDS_FOR_SLIDING
(Boolean)
  PARTIAL_BUCKET_HANDLING
(enumerated type)
  OUTPUT_FIELD_NAME
(string)
  GROUP_BY
(string)
  GROUPS_TO_DISPLAY
(enumerated)
  BUCKET_END_PER_GROUP
(Boolean)
  INPUT_FIELD_NAME
(string)
  TIME_SERIES_TYPE (enumerated type)
    If set to STATE_TS, the value of the input field from the latest
tick before the current bucket will be used as the initial input value
for the current bucket (unless the current bucket has a tick at its
start time) in the computation of HIGH_TIME. If set to EVENT_TS, only
ticks from the current bucket will be used in the computation of
HIGH_TIME.
Default: EVENT_TS

  
  SELECTION (enumerated type)
    Two allowed values, FIRST and LAST, of this parameter control
the timestamp selection of the respective first and last tick in the
bucket, having the input field's highest value.
Default: FIRST

  

&nbsp;

Example: See the HIGH_TIME
example in AGGREGATION_EXAMPLES.otq.


	"""
	class Parameters:
		bucket_interval = "BUCKET_INTERVAL"
		bucket_interval_units = "BUCKET_INTERVAL_UNITS"
		output_interval = "OUTPUT_INTERVAL"
		output_interval_units = "OUTPUT_INTERVAL_UNITS"
		is_running_aggr = "IS_RUNNING_AGGR"
		bucket_time = "BUCKET_TIME"
		bucket_end_criteria = "BUCKET_END_CRITERIA"
		boundary_tick_bucket = "BOUNDARY_TICK_BUCKET"
		all_fields_for_sliding = "ALL_FIELDS_FOR_SLIDING"
		partial_bucket_handling = "PARTIAL_BUCKET_HANDLING"
		output_field_name = "OUTPUT_FIELD_NAME"
		group_by = "GROUP_BY"
		groups_to_display = "GROUPS_TO_DISPLAY"
		bucket_end_per_group = "BUCKET_END_PER_GROUP"
		input_field_name = "INPUT_FIELD_NAME"
		time_series_type = "TIME_SERIES_TYPE"
		selection = "SELECTION"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["bucket_interval", "bucket_interval_units", "output_interval", "output_interval_units", "is_running_aggr", "bucket_time", "bucket_end_criteria", "boundary_tick_bucket", "all_fields_for_sliding", "partial_bucket_handling", "output_field_name", "group_by", "groups_to_display", "bucket_end_per_group", "input_field_name", "time_series_type", "selection"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["bucket_interval", "_default_bucket_interval", "bucket_interval_units", "_default_bucket_interval_units", "output_interval", "_default_output_interval", "output_interval_units", "_default_output_interval_units", "is_running_aggr", "_default_is_running_aggr", "bucket_time", "_default_bucket_time", "bucket_end_criteria", "_default_bucket_end_criteria", "boundary_tick_bucket", "_default_boundary_tick_bucket", "all_fields_for_sliding", "_default_all_fields_for_sliding", "partial_bucket_handling", "_default_partial_bucket_handling", "output_field_name", "_default_output_field_name", "group_by", "_default_group_by", "groups_to_display", "_default_groups_to_display", "bucket_end_per_group", "_default_bucket_end_per_group", "input_field_name", "_default_input_field_name", "time_series_type", "_default_time_series_type", "selection", "_default_selection", "stack_info", "_used_strings"]

	class BucketIntervalUnits:
		DAYS = "DAYS"
		FLEXIBLE = "FLEXIBLE"
		MONTHS = "MONTHS"
		SECONDS = "SECONDS"
		TICKS = "TICKS"

	class OutputIntervalUnits:
		SECONDS = "SECONDS"
		TICKS = "TICKS"

	class BucketTime:
		BUCKET_END = "BUCKET_END"
		BUCKET_START = "BUCKET_START"

	class BoundaryTickBucket:
		NEW = "NEW"
		PREVIOUS = "PREVIOUS"

	class AllFieldsForSliding:
		WHEN_TICKS_EXIT_WINDOW = "WHEN_TICKS_EXIT_WINDOW"
		FALSE = "false"
		TRUE = "true"

	class PartialBucketHandling:
		AS_SEPARATE_BUCKET = "AS_SEPARATE_BUCKET"
		DISCARD = "DISCARD"
		MERGE_WITH_PREVIOUS = "MERGE_WITH_PREVIOUS"

	class GroupsToDisplay:
		ALL = "ALL"
		EVENT_IN_LAST_BUCKET = "EVENT_IN_LAST_BUCKET"

	class TimeSeriesType:
		EVENT_TS = "EVENT_TS"
		STATE_TS = "STATE_TS"

	class Selection:
		FIRST = "FIRST"
		LAST = "LAST"

	def __init__(self, bucket_interval=0, bucket_interval_units=BucketIntervalUnits.SECONDS, output_interval="", output_interval_units=OutputIntervalUnits.SECONDS, is_running_aggr=False, bucket_time=BucketTime.BUCKET_END, bucket_end_criteria="", boundary_tick_bucket=BoundaryTickBucket.NEW, all_fields_for_sliding=AllFieldsForSliding.FALSE, partial_bucket_handling=PartialBucketHandling.AS_SEPARATE_BUCKET, output_field_name="VALUE", group_by="", groups_to_display=GroupsToDisplay.ALL, bucket_end_per_group=False, input_field_name="", time_series_type=TimeSeriesType.EVENT_TS, selection=Selection.FIRST, In = "", Out = ""):
		_graph_components.EpBase.__init__(self, "HIGH_TIME")
		self._default_bucket_interval = 0
		self.bucket_interval = bucket_interval
		self._default_bucket_interval_units = type(self).BucketIntervalUnits.SECONDS
		self.bucket_interval_units = bucket_interval_units
		self._default_output_interval = ""
		self.output_interval = output_interval
		self._default_output_interval_units = type(self).OutputIntervalUnits.SECONDS
		self.output_interval_units = output_interval_units
		self._default_is_running_aggr = False
		self.is_running_aggr = is_running_aggr
		self._default_bucket_time = type(self).BucketTime.BUCKET_END
		self.bucket_time = bucket_time
		self._default_bucket_end_criteria = ""
		self.bucket_end_criteria = bucket_end_criteria
		self._default_boundary_tick_bucket = type(self).BoundaryTickBucket.NEW
		self.boundary_tick_bucket = boundary_tick_bucket
		self._default_all_fields_for_sliding = type(self).AllFieldsForSliding.FALSE
		self.all_fields_for_sliding = all_fields_for_sliding
		self._default_partial_bucket_handling = type(self).PartialBucketHandling.AS_SEPARATE_BUCKET
		self.partial_bucket_handling = partial_bucket_handling
		self._default_output_field_name = "VALUE"
		self.output_field_name = output_field_name
		self._default_group_by = ""
		self.group_by = group_by
		self._default_groups_to_display = type(self).GroupsToDisplay.ALL
		self.groups_to_display = groups_to_display
		self._default_bucket_end_per_group = False
		self.bucket_end_per_group = bucket_end_per_group
		self._default_input_field_name = ""
		self.input_field_name = input_field_name
		self._default_time_series_type = type(self).TimeSeriesType.EVENT_TS
		self.time_series_type = time_series_type
		self._default_selection = type(self).Selection.FIRST
		self.selection = selection
		if In != "":
			self.input_field_name=In
		if Out != "":
			self.output_field_name=Out
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_bucket_interval(self, value):
		self.bucket_interval = value
		return self

	def set_bucket_interval_units(self, value):
		self.bucket_interval_units = value
		return self

	def set_output_interval(self, value):
		self.output_interval = value
		return self

	def set_output_interval_units(self, value):
		self.output_interval_units = value
		return self

	def set_is_running_aggr(self, value):
		self.is_running_aggr = value
		return self

	def set_bucket_time(self, value):
		self.bucket_time = value
		return self

	def set_bucket_end_criteria(self, value):
		self.bucket_end_criteria = value
		return self

	def set_boundary_tick_bucket(self, value):
		self.boundary_tick_bucket = value
		return self

	def set_all_fields_for_sliding(self, value):
		self.all_fields_for_sliding = value
		return self

	def set_partial_bucket_handling(self, value):
		self.partial_bucket_handling = value
		return self

	def set_output_field_name(self, value):
		self.output_field_name = value
		return self

	def set_group_by(self, value):
		self.group_by = value
		return self

	def set_groups_to_display(self, value):
		self.groups_to_display = value
		return self

	def set_bucket_end_per_group(self, value):
		self.bucket_end_per_group = value
		return self

	def set_input_field_name(self, value):
		self.input_field_name = value
		return self

	def set_time_series_type(self, value):
		self.time_series_type = value
		return self

	def set_selection(self, value):
		self.selection = value
		return self

	@staticmethod
	def _get_name():
		return "HIGH_TIME"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.bucket_interval != 0: 
			desc += "BUCKET_INTERVAL=" + py_to_str(self.bucket_interval) + ","
		if self.bucket_interval_units != self.BucketIntervalUnits.SECONDS: 
			desc += "BUCKET_INTERVAL_UNITS=" + py_to_str(self.bucket_interval_units) + ","
		if self.output_interval != "": 
			desc += "OUTPUT_INTERVAL=" + py_to_str(self.output_interval) + ","
		if self.output_interval_units != self.OutputIntervalUnits.SECONDS: 
			desc += "OUTPUT_INTERVAL_UNITS=" + py_to_str(self.output_interval_units) + ","
		if self.is_running_aggr != False: 
			desc += "IS_RUNNING_AGGR=" + py_to_str(self.is_running_aggr) + ","
		if self.bucket_time != self.BucketTime.BUCKET_END: 
			desc += "BUCKET_TIME=" + py_to_str(self.bucket_time) + ","
		if self.bucket_end_criteria != "": 
			desc += "BUCKET_END_CRITERIA=" + py_to_str(self.bucket_end_criteria) + ","
		if self.boundary_tick_bucket != self.BoundaryTickBucket.NEW: 
			desc += "BOUNDARY_TICK_BUCKET=" + py_to_str(self.boundary_tick_bucket) + ","
		if self.all_fields_for_sliding != self.AllFieldsForSliding.FALSE: 
			desc += "ALL_FIELDS_FOR_SLIDING=" + py_to_str(self.all_fields_for_sliding) + ","
		if self.partial_bucket_handling != self.PartialBucketHandling.AS_SEPARATE_BUCKET: 
			desc += "PARTIAL_BUCKET_HANDLING=" + py_to_str(self.partial_bucket_handling) + ","
		if self.output_field_name != "VALUE": 
			desc += "OUTPUT_FIELD_NAME=" + py_to_str(self.output_field_name) + ","
		if self.group_by != "": 
			desc += "GROUP_BY=" + py_to_str(self.group_by) + ","
		if self.groups_to_display != self.GroupsToDisplay.ALL: 
			desc += "GROUPS_TO_DISPLAY=" + py_to_str(self.groups_to_display) + ","
		if self.bucket_end_per_group != False: 
			desc += "BUCKET_END_PER_GROUP=" + py_to_str(self.bucket_end_per_group) + ","
		if self.input_field_name != "": 
			desc += "INPUT_FIELD_NAME=" + py_to_str(self.input_field_name) + ","
		if self.time_series_type != self.TimeSeriesType.EVENT_TS: 
			desc += "TIME_SERIES_TYPE=" + py_to_str(self.time_series_type) + ","
		if self.selection != self.Selection.FIRST: 
			desc += "SELECTION=" + py_to_str(self.selection) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class LowTime(_graph_components.EpBase):
	"""
		

LOW_TIME

Type: Aggregation

Description: For each bucket, gets the timestamp of
first/last tick in it with the lowest value of the input field.

Python
class name:
LowTime

Input: A time series of ticks.

Output: A time series of ticks, one tick for each bucket
interval.

Parameters: See parameters
common to generic aggregations.


  BUCKET_INTERVAL
(seconds/ticks)
  BUCKET_INTERVAL_UNITS
(enumerated type)
  OUTPUT_INTERVAL
(seconds)
  OUTPUT_INTERVAL_UNITS
(SECONDS/TICKS)
  IS_RUNNING_AGGR
(Boolean)
  BUCKET_TIME
(Boolean)
  BUCKET_END_CRITERIA
(expression)
  BOUNDARY_TICK_BUCKET
(NEW/PREVIOUS)
  ALL_FIELDS_FOR_SLIDING
(Boolean)
  PARTIAL_BUCKET_HANDLING
(enumerated type)
  OUTPUT_FIELD_NAME
(string)
  GROUP_BY
(string)
  GROUPS_TO_DISPLAY
(enumerated)
  BUCKET_END_PER_GROUP
(Boolean)
  INPUT_FIELD_NAME
(string)
  TIME_SERIES_TYPE (enumerated type)
    If set to STATE_TS, the value of the input field from the latest
tick before the current bucket will be used as the initial input value
for the current bucket (unless the current bucket has a tick at its
start time) in the computation of LOW_TIME. If set to EVENT_TS, only
ticks from the current bucket will be used in computation of LOW_TIME.
Default: EVENT_TS

  
  SELECTION (enumerated type)
    The two allowed values, FIRST and LAST, of this parameter
control the timestamp selection of the respective first and last tick
in the bucket, having a lowest value of the input field.
Default: FIRST

  

Notes: See the notes on generic
aggregations.

Example: See the LOW_TIME
example in AGGREGATION_EXAMPLES.otq.


	"""
	class Parameters:
		bucket_interval = "BUCKET_INTERVAL"
		bucket_interval_units = "BUCKET_INTERVAL_UNITS"
		output_interval = "OUTPUT_INTERVAL"
		output_interval_units = "OUTPUT_INTERVAL_UNITS"
		is_running_aggr = "IS_RUNNING_AGGR"
		bucket_time = "BUCKET_TIME"
		bucket_end_criteria = "BUCKET_END_CRITERIA"
		boundary_tick_bucket = "BOUNDARY_TICK_BUCKET"
		all_fields_for_sliding = "ALL_FIELDS_FOR_SLIDING"
		partial_bucket_handling = "PARTIAL_BUCKET_HANDLING"
		output_field_name = "OUTPUT_FIELD_NAME"
		group_by = "GROUP_BY"
		groups_to_display = "GROUPS_TO_DISPLAY"
		bucket_end_per_group = "BUCKET_END_PER_GROUP"
		input_field_name = "INPUT_FIELD_NAME"
		time_series_type = "TIME_SERIES_TYPE"
		selection = "SELECTION"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["bucket_interval", "bucket_interval_units", "output_interval", "output_interval_units", "is_running_aggr", "bucket_time", "bucket_end_criteria", "boundary_tick_bucket", "all_fields_for_sliding", "partial_bucket_handling", "output_field_name", "group_by", "groups_to_display", "bucket_end_per_group", "input_field_name", "time_series_type", "selection"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["bucket_interval", "_default_bucket_interval", "bucket_interval_units", "_default_bucket_interval_units", "output_interval", "_default_output_interval", "output_interval_units", "_default_output_interval_units", "is_running_aggr", "_default_is_running_aggr", "bucket_time", "_default_bucket_time", "bucket_end_criteria", "_default_bucket_end_criteria", "boundary_tick_bucket", "_default_boundary_tick_bucket", "all_fields_for_sliding", "_default_all_fields_for_sliding", "partial_bucket_handling", "_default_partial_bucket_handling", "output_field_name", "_default_output_field_name", "group_by", "_default_group_by", "groups_to_display", "_default_groups_to_display", "bucket_end_per_group", "_default_bucket_end_per_group", "input_field_name", "_default_input_field_name", "time_series_type", "_default_time_series_type", "selection", "_default_selection", "stack_info", "_used_strings"]

	class BucketIntervalUnits:
		DAYS = "DAYS"
		FLEXIBLE = "FLEXIBLE"
		MONTHS = "MONTHS"
		SECONDS = "SECONDS"
		TICKS = "TICKS"

	class OutputIntervalUnits:
		SECONDS = "SECONDS"
		TICKS = "TICKS"

	class BucketTime:
		BUCKET_END = "BUCKET_END"
		BUCKET_START = "BUCKET_START"

	class BoundaryTickBucket:
		NEW = "NEW"
		PREVIOUS = "PREVIOUS"

	class AllFieldsForSliding:
		WHEN_TICKS_EXIT_WINDOW = "WHEN_TICKS_EXIT_WINDOW"
		FALSE = "false"
		TRUE = "true"

	class PartialBucketHandling:
		AS_SEPARATE_BUCKET = "AS_SEPARATE_BUCKET"
		DISCARD = "DISCARD"
		MERGE_WITH_PREVIOUS = "MERGE_WITH_PREVIOUS"

	class GroupsToDisplay:
		ALL = "ALL"
		EVENT_IN_LAST_BUCKET = "EVENT_IN_LAST_BUCKET"

	class TimeSeriesType:
		EVENT_TS = "EVENT_TS"
		STATE_TS = "STATE_TS"

	class Selection:
		FIRST = "FIRST"
		LAST = "LAST"

	def __init__(self, bucket_interval=0, bucket_interval_units=BucketIntervalUnits.SECONDS, output_interval="", output_interval_units=OutputIntervalUnits.SECONDS, is_running_aggr=False, bucket_time=BucketTime.BUCKET_END, bucket_end_criteria="", boundary_tick_bucket=BoundaryTickBucket.NEW, all_fields_for_sliding=AllFieldsForSliding.FALSE, partial_bucket_handling=PartialBucketHandling.AS_SEPARATE_BUCKET, output_field_name="VALUE", group_by="", groups_to_display=GroupsToDisplay.ALL, bucket_end_per_group=False, input_field_name="", time_series_type=TimeSeriesType.EVENT_TS, selection=Selection.FIRST, In = "", Out = ""):
		_graph_components.EpBase.__init__(self, "LOW_TIME")
		self._default_bucket_interval = 0
		self.bucket_interval = bucket_interval
		self._default_bucket_interval_units = type(self).BucketIntervalUnits.SECONDS
		self.bucket_interval_units = bucket_interval_units
		self._default_output_interval = ""
		self.output_interval = output_interval
		self._default_output_interval_units = type(self).OutputIntervalUnits.SECONDS
		self.output_interval_units = output_interval_units
		self._default_is_running_aggr = False
		self.is_running_aggr = is_running_aggr
		self._default_bucket_time = type(self).BucketTime.BUCKET_END
		self.bucket_time = bucket_time
		self._default_bucket_end_criteria = ""
		self.bucket_end_criteria = bucket_end_criteria
		self._default_boundary_tick_bucket = type(self).BoundaryTickBucket.NEW
		self.boundary_tick_bucket = boundary_tick_bucket
		self._default_all_fields_for_sliding = type(self).AllFieldsForSliding.FALSE
		self.all_fields_for_sliding = all_fields_for_sliding
		self._default_partial_bucket_handling = type(self).PartialBucketHandling.AS_SEPARATE_BUCKET
		self.partial_bucket_handling = partial_bucket_handling
		self._default_output_field_name = "VALUE"
		self.output_field_name = output_field_name
		self._default_group_by = ""
		self.group_by = group_by
		self._default_groups_to_display = type(self).GroupsToDisplay.ALL
		self.groups_to_display = groups_to_display
		self._default_bucket_end_per_group = False
		self.bucket_end_per_group = bucket_end_per_group
		self._default_input_field_name = ""
		self.input_field_name = input_field_name
		self._default_time_series_type = type(self).TimeSeriesType.EVENT_TS
		self.time_series_type = time_series_type
		self._default_selection = type(self).Selection.FIRST
		self.selection = selection
		if In != "":
			self.input_field_name=In
		if Out != "":
			self.output_field_name=Out
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_bucket_interval(self, value):
		self.bucket_interval = value
		return self

	def set_bucket_interval_units(self, value):
		self.bucket_interval_units = value
		return self

	def set_output_interval(self, value):
		self.output_interval = value
		return self

	def set_output_interval_units(self, value):
		self.output_interval_units = value
		return self

	def set_is_running_aggr(self, value):
		self.is_running_aggr = value
		return self

	def set_bucket_time(self, value):
		self.bucket_time = value
		return self

	def set_bucket_end_criteria(self, value):
		self.bucket_end_criteria = value
		return self

	def set_boundary_tick_bucket(self, value):
		self.boundary_tick_bucket = value
		return self

	def set_all_fields_for_sliding(self, value):
		self.all_fields_for_sliding = value
		return self

	def set_partial_bucket_handling(self, value):
		self.partial_bucket_handling = value
		return self

	def set_output_field_name(self, value):
		self.output_field_name = value
		return self

	def set_group_by(self, value):
		self.group_by = value
		return self

	def set_groups_to_display(self, value):
		self.groups_to_display = value
		return self

	def set_bucket_end_per_group(self, value):
		self.bucket_end_per_group = value
		return self

	def set_input_field_name(self, value):
		self.input_field_name = value
		return self

	def set_time_series_type(self, value):
		self.time_series_type = value
		return self

	def set_selection(self, value):
		self.selection = value
		return self

	@staticmethod
	def _get_name():
		return "LOW_TIME"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.bucket_interval != 0: 
			desc += "BUCKET_INTERVAL=" + py_to_str(self.bucket_interval) + ","
		if self.bucket_interval_units != self.BucketIntervalUnits.SECONDS: 
			desc += "BUCKET_INTERVAL_UNITS=" + py_to_str(self.bucket_interval_units) + ","
		if self.output_interval != "": 
			desc += "OUTPUT_INTERVAL=" + py_to_str(self.output_interval) + ","
		if self.output_interval_units != self.OutputIntervalUnits.SECONDS: 
			desc += "OUTPUT_INTERVAL_UNITS=" + py_to_str(self.output_interval_units) + ","
		if self.is_running_aggr != False: 
			desc += "IS_RUNNING_AGGR=" + py_to_str(self.is_running_aggr) + ","
		if self.bucket_time != self.BucketTime.BUCKET_END: 
			desc += "BUCKET_TIME=" + py_to_str(self.bucket_time) + ","
		if self.bucket_end_criteria != "": 
			desc += "BUCKET_END_CRITERIA=" + py_to_str(self.bucket_end_criteria) + ","
		if self.boundary_tick_bucket != self.BoundaryTickBucket.NEW: 
			desc += "BOUNDARY_TICK_BUCKET=" + py_to_str(self.boundary_tick_bucket) + ","
		if self.all_fields_for_sliding != self.AllFieldsForSliding.FALSE: 
			desc += "ALL_FIELDS_FOR_SLIDING=" + py_to_str(self.all_fields_for_sliding) + ","
		if self.partial_bucket_handling != self.PartialBucketHandling.AS_SEPARATE_BUCKET: 
			desc += "PARTIAL_BUCKET_HANDLING=" + py_to_str(self.partial_bucket_handling) + ","
		if self.output_field_name != "VALUE": 
			desc += "OUTPUT_FIELD_NAME=" + py_to_str(self.output_field_name) + ","
		if self.group_by != "": 
			desc += "GROUP_BY=" + py_to_str(self.group_by) + ","
		if self.groups_to_display != self.GroupsToDisplay.ALL: 
			desc += "GROUPS_TO_DISPLAY=" + py_to_str(self.groups_to_display) + ","
		if self.bucket_end_per_group != False: 
			desc += "BUCKET_END_PER_GROUP=" + py_to_str(self.bucket_end_per_group) + ","
		if self.input_field_name != "": 
			desc += "INPUT_FIELD_NAME=" + py_to_str(self.input_field_name) + ","
		if self.time_series_type != self.TimeSeriesType.EVENT_TS: 
			desc += "TIME_SERIES_TYPE=" + py_to_str(self.time_series_type) + ","
		if self.selection != self.Selection.FIRST: 
			desc += "SELECTION=" + py_to_str(self.selection) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class HighTick(_graph_components.EpBase):
	"""
		

HIGH_TICK

Type: Aggregation

Description: For each bucket propagates at most n
ticks in it, with the top highest values of the input field, adding
original timestamps of ticks in the field HIGH_TIME. Thus, bucket
output generally contains all ticks
with up to the k-th highest value of the input field and a part
of ticks with the k+1-th highest value of the input field for
some k &gt; 0.&nbsp;The ticks with NaN as an input field's
value are skipped by this event processor. &nbsp;

When used outside of COMPUTE EP, HIGH_TICK EP produces no ticks if
its input contains no ticks or only ticks with&nbsp;NaN input field's
vallues.&nbsp;

When used inside of COMPUTE EP, &nbsp;when there are no input ticks
and the input schema is not known, HIGH_TICK EP contributes two fields
to the output ticks: VALUE and HIGH_TIME, both set to their default
values.

When used inside of COMPUTE EP, &nbsp;when an input schema is known
but there are no input ticks that have non-NaN input field's values,
all output fields contributed by HIGH_TICK EP to the output ticks&nbsp;
have their default values. 

Python
class name:
HighTick

Input: A time series of ticks.

Output: A time series of ticks, n ticks for each
bucket.

Parameters: See parameters
common to generic aggregations.


  BUCKET_INTERVAL
(seconds/ticks)
  BUCKET_INTERVAL_UNITS
(enumerated type)
  IS_RUNNING_AGGR
(Boolean)
  BUCKET_TIME
(enumerated type)
  BUCKET_END_CRITERIA
(string)
  BOUNDARY_TICK_BUCKET
(NEW/PREVIOUS)
  PARTIAL_BUCKET_HANDLING
(enumerated type)
  NUM_TICKS (integer)
    The number of input ticks with top highest values of the input
field that are to be propagated for each bucket.
Default: 1

  
  SELECTION (enumerated type)
    The two allowed values, FIRST
and LAST, of this parameter control
the selection of the respective beginning or trailing part of ticks
with the k+1-th highest value (see above) of the input field.
Default: FIRST

  
  TIME_SERIES_TYPE (enumerated type)
    If set to STATE_TS, the value
of
the input field from the latest tick before the current bucket will be
used as the initial input value for the current bucket (unless the
current bucket has a tick at its start time) in the computation of
HIGH_TICK. If set to EVENT_TS, only
ticks from the current bucket will be used in the computation of
HIGH_TICK.
Default: EVENT_TS

  
  GROUP_BY
(string)
  GROUPS_TO_DISPLAY
(enumerated)
  BUCKET_END_PER_GROUP
(Boolean)
  INPUT_FIELD_NAME
(string)
  KEEP_INITIAL_SCHEMA (Boolean)
    If set to true, and if the input schema changes, all input ticks
after that change will be converted to have the initial input schema .
Default: false

  

Example: See the HIGH_TICK
example in AGGREGATION_EXAMPLES.otq.


	"""
	class Parameters:
		bucket_interval = "BUCKET_INTERVAL"
		bucket_interval_units = "BUCKET_INTERVAL_UNITS"
		is_running_aggr = "IS_RUNNING_AGGR"
		bucket_time = "BUCKET_TIME"
		bucket_end_criteria = "BUCKET_END_CRITERIA"
		boundary_tick_bucket = "BOUNDARY_TICK_BUCKET"
		partial_bucket_handling = "PARTIAL_BUCKET_HANDLING"
		num_ticks = "NUM_TICKS"
		selection = "SELECTION"
		time_series_type = "TIME_SERIES_TYPE"
		group_by = "GROUP_BY"
		groups_to_display = "GROUPS_TO_DISPLAY"
		bucket_end_per_group = "BUCKET_END_PER_GROUP"
		input_field_name = "INPUT_FIELD_NAME"
		keep_initial_schema = "KEEP_INITIAL_SCHEMA"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["bucket_interval", "bucket_interval_units", "is_running_aggr", "bucket_time", "bucket_end_criteria", "boundary_tick_bucket", "partial_bucket_handling", "num_ticks", "selection", "time_series_type", "group_by", "groups_to_display", "bucket_end_per_group", "input_field_name", "keep_initial_schema"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["bucket_interval", "_default_bucket_interval", "bucket_interval_units", "_default_bucket_interval_units", "is_running_aggr", "_default_is_running_aggr", "bucket_time", "_default_bucket_time", "bucket_end_criteria", "_default_bucket_end_criteria", "boundary_tick_bucket", "_default_boundary_tick_bucket", "partial_bucket_handling", "_default_partial_bucket_handling", "num_ticks", "_default_num_ticks", "selection", "_default_selection", "time_series_type", "_default_time_series_type", "group_by", "_default_group_by", "groups_to_display", "_default_groups_to_display", "bucket_end_per_group", "_default_bucket_end_per_group", "input_field_name", "_default_input_field_name", "keep_initial_schema", "_default_keep_initial_schema", "stack_info", "_used_strings"]

	class BucketIntervalUnits:
		DAYS = "DAYS"
		FLEXIBLE = "FLEXIBLE"
		MONTHS = "MONTHS"
		SECONDS = "SECONDS"
		TICKS = "TICKS"

	class BucketTime:
		BUCKET_END = "BUCKET_END"
		BUCKET_START = "BUCKET_START"

	class BoundaryTickBucket:
		NEW = "NEW"
		PREVIOUS = "PREVIOUS"

	class PartialBucketHandling:
		AS_SEPARATE_BUCKET = "AS_SEPARATE_BUCKET"
		DISCARD = "DISCARD"
		MERGE_WITH_PREVIOUS = "MERGE_WITH_PREVIOUS"

	class Selection:
		FIRST = "FIRST"
		LAST = "LAST"

	class TimeSeriesType:
		EVENT_TS = "EVENT_TS"
		STATE_TS = "STATE_TS"

	class GroupsToDisplay:
		ALL = "ALL"
		EVENT_IN_LAST_BUCKET = "EVENT_IN_LAST_BUCKET"

	def __init__(self, bucket_interval=0, bucket_interval_units=BucketIntervalUnits.SECONDS, is_running_aggr=False, bucket_time=BucketTime.BUCKET_END, bucket_end_criteria="", boundary_tick_bucket=BoundaryTickBucket.NEW, partial_bucket_handling=PartialBucketHandling.AS_SEPARATE_BUCKET, num_ticks=1, selection=Selection.FIRST, time_series_type=TimeSeriesType.EVENT_TS, group_by="", groups_to_display=GroupsToDisplay.ALL, bucket_end_per_group=False, input_field_name="", keep_initial_schema=False, In = ""):
		_graph_components.EpBase.__init__(self, "HIGH_TICK")
		self._default_bucket_interval = 0
		self.bucket_interval = bucket_interval
		self._default_bucket_interval_units = type(self).BucketIntervalUnits.SECONDS
		self.bucket_interval_units = bucket_interval_units
		self._default_is_running_aggr = False
		self.is_running_aggr = is_running_aggr
		self._default_bucket_time = type(self).BucketTime.BUCKET_END
		self.bucket_time = bucket_time
		self._default_bucket_end_criteria = ""
		self.bucket_end_criteria = bucket_end_criteria
		self._default_boundary_tick_bucket = type(self).BoundaryTickBucket.NEW
		self.boundary_tick_bucket = boundary_tick_bucket
		self._default_partial_bucket_handling = type(self).PartialBucketHandling.AS_SEPARATE_BUCKET
		self.partial_bucket_handling = partial_bucket_handling
		self._default_num_ticks = 1
		self.num_ticks = num_ticks
		self._default_selection = type(self).Selection.FIRST
		self.selection = selection
		self._default_time_series_type = type(self).TimeSeriesType.EVENT_TS
		self.time_series_type = time_series_type
		self._default_group_by = ""
		self.group_by = group_by
		self._default_groups_to_display = type(self).GroupsToDisplay.ALL
		self.groups_to_display = groups_to_display
		self._default_bucket_end_per_group = False
		self.bucket_end_per_group = bucket_end_per_group
		self._default_input_field_name = ""
		self.input_field_name = input_field_name
		self._default_keep_initial_schema = False
		self.keep_initial_schema = keep_initial_schema
		if In != "":
			self.input_field_name=In
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_bucket_interval(self, value):
		self.bucket_interval = value
		return self

	def set_bucket_interval_units(self, value):
		self.bucket_interval_units = value
		return self

	def set_is_running_aggr(self, value):
		self.is_running_aggr = value
		return self

	def set_bucket_time(self, value):
		self.bucket_time = value
		return self

	def set_bucket_end_criteria(self, value):
		self.bucket_end_criteria = value
		return self

	def set_boundary_tick_bucket(self, value):
		self.boundary_tick_bucket = value
		return self

	def set_partial_bucket_handling(self, value):
		self.partial_bucket_handling = value
		return self

	def set_num_ticks(self, value):
		self.num_ticks = value
		return self

	def set_selection(self, value):
		self.selection = value
		return self

	def set_time_series_type(self, value):
		self.time_series_type = value
		return self

	def set_group_by(self, value):
		self.group_by = value
		return self

	def set_groups_to_display(self, value):
		self.groups_to_display = value
		return self

	def set_bucket_end_per_group(self, value):
		self.bucket_end_per_group = value
		return self

	def set_input_field_name(self, value):
		self.input_field_name = value
		return self

	def set_keep_initial_schema(self, value):
		self.keep_initial_schema = value
		return self

	@staticmethod
	def _get_name():
		return "HIGH_TICK"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.bucket_interval != 0: 
			desc += "BUCKET_INTERVAL=" + py_to_str(self.bucket_interval) + ","
		if self.bucket_interval_units != self.BucketIntervalUnits.SECONDS: 
			desc += "BUCKET_INTERVAL_UNITS=" + py_to_str(self.bucket_interval_units) + ","
		if self.is_running_aggr != False: 
			desc += "IS_RUNNING_AGGR=" + py_to_str(self.is_running_aggr) + ","
		if self.bucket_time != self.BucketTime.BUCKET_END: 
			desc += "BUCKET_TIME=" + py_to_str(self.bucket_time) + ","
		if self.bucket_end_criteria != "": 
			desc += "BUCKET_END_CRITERIA=" + py_to_str(self.bucket_end_criteria) + ","
		if self.boundary_tick_bucket != self.BoundaryTickBucket.NEW: 
			desc += "BOUNDARY_TICK_BUCKET=" + py_to_str(self.boundary_tick_bucket) + ","
		if self.partial_bucket_handling != self.PartialBucketHandling.AS_SEPARATE_BUCKET: 
			desc += "PARTIAL_BUCKET_HANDLING=" + py_to_str(self.partial_bucket_handling) + ","
		if self.num_ticks != 1: 
			desc += "NUM_TICKS=" + py_to_str(self.num_ticks) + ","
		if self.selection != self.Selection.FIRST: 
			desc += "SELECTION=" + py_to_str(self.selection) + ","
		if self.time_series_type != self.TimeSeriesType.EVENT_TS: 
			desc += "TIME_SERIES_TYPE=" + py_to_str(self.time_series_type) + ","
		if self.group_by != "": 
			desc += "GROUP_BY=" + py_to_str(self.group_by) + ","
		if self.groups_to_display != self.GroupsToDisplay.ALL: 
			desc += "GROUPS_TO_DISPLAY=" + py_to_str(self.groups_to_display) + ","
		if self.bucket_end_per_group != False: 
			desc += "BUCKET_END_PER_GROUP=" + py_to_str(self.bucket_end_per_group) + ","
		if self.input_field_name != "": 
			desc += "INPUT_FIELD_NAME=" + py_to_str(self.input_field_name) + ","
		if self.keep_initial_schema != False: 
			desc += "KEEP_INITIAL_SCHEMA=" + py_to_str(self.keep_initial_schema) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class LowTick(_graph_components.EpBase):
	"""
		

LOW_TICK

Type: Aggregation

Description: For each bucket, propagates at the most n
ticks in it with top lowest values of the input field, adding original
timestamps of ticks in the field LOW_TIME. Thus, the bucket output
generally contains all
ticks with up to the k-th lowest value of the input field and
part of the ticks with the k+1-th lowest value of the input
field for some k &gt; 0. The ticks with NaN as an input field's
value are skipped by this event processor.&nbsp;

When used outside of COMPUTE EP, LOW_TICK EP produces no ticks if
its input contains no ticks or only ticks with&nbsp;NaN input field's
vallues.&nbsp;

When used inside of COMPUTE EP, &nbsp;when there are no input ticks
and
the input schema is not known, LOW_TICK EP contributes two fields to
the output ticks: VALUE and LOW_TIME, both set to their default values.

When used inside of COMPUTE EP, &nbsp;when an input schema is known
but
there are no input ticks that have non-NaN input field's values, all
output fields contributed by LOW_TICK EP to the output ticks&nbsp; have
their default values. 

Python
class name:
LowTick

Input: A time series of ticks.

Output: A time series of ticks, n ticks for each
bucket.

Parameters: See section parameters
common to generic aggregations.


  BUCKET_INTERVAL
(seconds/ticks)
  BUCKET_INTERVAL_UNITS
(enumerated type)
  IS_RUNNING_AGGR
(Boolean)
  BUCKET_TIME
(Boolean)
  BUCKET_END_CRITERIA
(expression)
  BOUNDARY_TICK_BUCKET
(NEW/PREVIOUS)
  PARTIAL_BUCKET_HANDLING
(enumerated type)
  NUM_TICKS (integer)
    The number of input ticks with the top lowest values of the
input field that are to be propagated for each bucket.
Default: 1

  
  SELECTION (enumerated type)
    The two allowed values, FIRST and LAST, of this parameter
control
the selection of the respective beginning or trailing part of ticks
with a k+1-th lowest value (see above) of the input field.
Default: FIRST

  
  TIME_SERIES_TYPE (enumerated type)
    If set to STATE_TS, the value of the input field from the latest
tick before the current bucket will be used as the initial input value
the for current bucket (unless the current bucket has a tick at its
start time) in the computation of LOW_TICK. If set to EVENT_TS, only
ticks from current bucket will be used in the computation of LOW_TICK.
Default: EVENT_TS

  
  GROUP_BY
(string)
  GROUPS_TO_DISPLAY
(enumerated)
  BUCKET_END_PER_GROUP
(Boolean)
  INPUT_FIELD_NAME
(string)
  KEEP_INITIAL_SCHEMA (Boolean)
    If set to true, and if the input schema changes, all input ticks
after that change will be converted to have the initial input schema .
Default: false

  

Example: For compute 5 lowest tick in 5 minutes buckets:

LOW_TICK(BUCKET_INTERVAL=300,NUM_TICKS=5)

See the LOW_TICK example in AGGREGATION_BASIC_EXAMPLES.otq.


	"""
	class Parameters:
		bucket_interval = "BUCKET_INTERVAL"
		bucket_interval_units = "BUCKET_INTERVAL_UNITS"
		is_running_aggr = "IS_RUNNING_AGGR"
		bucket_time = "BUCKET_TIME"
		bucket_end_criteria = "BUCKET_END_CRITERIA"
		boundary_tick_bucket = "BOUNDARY_TICK_BUCKET"
		partial_bucket_handling = "PARTIAL_BUCKET_HANDLING"
		num_ticks = "NUM_TICKS"
		selection = "SELECTION"
		time_series_type = "TIME_SERIES_TYPE"
		group_by = "GROUP_BY"
		groups_to_display = "GROUPS_TO_DISPLAY"
		bucket_end_per_group = "BUCKET_END_PER_GROUP"
		input_field_name = "INPUT_FIELD_NAME"
		keep_initial_schema = "KEEP_INITIAL_SCHEMA"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["bucket_interval", "bucket_interval_units", "is_running_aggr", "bucket_time", "bucket_end_criteria", "boundary_tick_bucket", "partial_bucket_handling", "num_ticks", "selection", "time_series_type", "group_by", "groups_to_display", "bucket_end_per_group", "input_field_name", "keep_initial_schema"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["bucket_interval", "_default_bucket_interval", "bucket_interval_units", "_default_bucket_interval_units", "is_running_aggr", "_default_is_running_aggr", "bucket_time", "_default_bucket_time", "bucket_end_criteria", "_default_bucket_end_criteria", "boundary_tick_bucket", "_default_boundary_tick_bucket", "partial_bucket_handling", "_default_partial_bucket_handling", "num_ticks", "_default_num_ticks", "selection", "_default_selection", "time_series_type", "_default_time_series_type", "group_by", "_default_group_by", "groups_to_display", "_default_groups_to_display", "bucket_end_per_group", "_default_bucket_end_per_group", "input_field_name", "_default_input_field_name", "keep_initial_schema", "_default_keep_initial_schema", "stack_info", "_used_strings"]

	class BucketIntervalUnits:
		DAYS = "DAYS"
		FLEXIBLE = "FLEXIBLE"
		MONTHS = "MONTHS"
		SECONDS = "SECONDS"
		TICKS = "TICKS"

	class BucketTime:
		BUCKET_END = "BUCKET_END"
		BUCKET_START = "BUCKET_START"

	class BoundaryTickBucket:
		NEW = "NEW"
		PREVIOUS = "PREVIOUS"

	class PartialBucketHandling:
		AS_SEPARATE_BUCKET = "AS_SEPARATE_BUCKET"
		DISCARD = "DISCARD"
		MERGE_WITH_PREVIOUS = "MERGE_WITH_PREVIOUS"

	class Selection:
		FIRST = "FIRST"
		LAST = "LAST"

	class TimeSeriesType:
		EVENT_TS = "EVENT_TS"
		STATE_TS = "STATE_TS"

	class GroupsToDisplay:
		ALL = "ALL"
		EVENT_IN_LAST_BUCKET = "EVENT_IN_LAST_BUCKET"

	def __init__(self, bucket_interval=0, bucket_interval_units=BucketIntervalUnits.SECONDS, is_running_aggr=False, bucket_time=BucketTime.BUCKET_END, bucket_end_criteria="", boundary_tick_bucket=BoundaryTickBucket.NEW, partial_bucket_handling=PartialBucketHandling.AS_SEPARATE_BUCKET, num_ticks=1, selection=Selection.FIRST, time_series_type=TimeSeriesType.EVENT_TS, group_by="", groups_to_display=GroupsToDisplay.ALL, bucket_end_per_group=False, input_field_name="", keep_initial_schema=False, In = ""):
		_graph_components.EpBase.__init__(self, "LOW_TICK")
		self._default_bucket_interval = 0
		self.bucket_interval = bucket_interval
		self._default_bucket_interval_units = type(self).BucketIntervalUnits.SECONDS
		self.bucket_interval_units = bucket_interval_units
		self._default_is_running_aggr = False
		self.is_running_aggr = is_running_aggr
		self._default_bucket_time = type(self).BucketTime.BUCKET_END
		self.bucket_time = bucket_time
		self._default_bucket_end_criteria = ""
		self.bucket_end_criteria = bucket_end_criteria
		self._default_boundary_tick_bucket = type(self).BoundaryTickBucket.NEW
		self.boundary_tick_bucket = boundary_tick_bucket
		self._default_partial_bucket_handling = type(self).PartialBucketHandling.AS_SEPARATE_BUCKET
		self.partial_bucket_handling = partial_bucket_handling
		self._default_num_ticks = 1
		self.num_ticks = num_ticks
		self._default_selection = type(self).Selection.FIRST
		self.selection = selection
		self._default_time_series_type = type(self).TimeSeriesType.EVENT_TS
		self.time_series_type = time_series_type
		self._default_group_by = ""
		self.group_by = group_by
		self._default_groups_to_display = type(self).GroupsToDisplay.ALL
		self.groups_to_display = groups_to_display
		self._default_bucket_end_per_group = False
		self.bucket_end_per_group = bucket_end_per_group
		self._default_input_field_name = ""
		self.input_field_name = input_field_name
		self._default_keep_initial_schema = False
		self.keep_initial_schema = keep_initial_schema
		if In != "":
			self.input_field_name=In
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_bucket_interval(self, value):
		self.bucket_interval = value
		return self

	def set_bucket_interval_units(self, value):
		self.bucket_interval_units = value
		return self

	def set_is_running_aggr(self, value):
		self.is_running_aggr = value
		return self

	def set_bucket_time(self, value):
		self.bucket_time = value
		return self

	def set_bucket_end_criteria(self, value):
		self.bucket_end_criteria = value
		return self

	def set_boundary_tick_bucket(self, value):
		self.boundary_tick_bucket = value
		return self

	def set_partial_bucket_handling(self, value):
		self.partial_bucket_handling = value
		return self

	def set_num_ticks(self, value):
		self.num_ticks = value
		return self

	def set_selection(self, value):
		self.selection = value
		return self

	def set_time_series_type(self, value):
		self.time_series_type = value
		return self

	def set_group_by(self, value):
		self.group_by = value
		return self

	def set_groups_to_display(self, value):
		self.groups_to_display = value
		return self

	def set_bucket_end_per_group(self, value):
		self.bucket_end_per_group = value
		return self

	def set_input_field_name(self, value):
		self.input_field_name = value
		return self

	def set_keep_initial_schema(self, value):
		self.keep_initial_schema = value
		return self

	@staticmethod
	def _get_name():
		return "LOW_TICK"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.bucket_interval != 0: 
			desc += "BUCKET_INTERVAL=" + py_to_str(self.bucket_interval) + ","
		if self.bucket_interval_units != self.BucketIntervalUnits.SECONDS: 
			desc += "BUCKET_INTERVAL_UNITS=" + py_to_str(self.bucket_interval_units) + ","
		if self.is_running_aggr != False: 
			desc += "IS_RUNNING_AGGR=" + py_to_str(self.is_running_aggr) + ","
		if self.bucket_time != self.BucketTime.BUCKET_END: 
			desc += "BUCKET_TIME=" + py_to_str(self.bucket_time) + ","
		if self.bucket_end_criteria != "": 
			desc += "BUCKET_END_CRITERIA=" + py_to_str(self.bucket_end_criteria) + ","
		if self.boundary_tick_bucket != self.BoundaryTickBucket.NEW: 
			desc += "BOUNDARY_TICK_BUCKET=" + py_to_str(self.boundary_tick_bucket) + ","
		if self.partial_bucket_handling != self.PartialBucketHandling.AS_SEPARATE_BUCKET: 
			desc += "PARTIAL_BUCKET_HANDLING=" + py_to_str(self.partial_bucket_handling) + ","
		if self.num_ticks != 1: 
			desc += "NUM_TICKS=" + py_to_str(self.num_ticks) + ","
		if self.selection != self.Selection.FIRST: 
			desc += "SELECTION=" + py_to_str(self.selection) + ","
		if self.time_series_type != self.TimeSeriesType.EVENT_TS: 
			desc += "TIME_SERIES_TYPE=" + py_to_str(self.time_series_type) + ","
		if self.group_by != "": 
			desc += "GROUP_BY=" + py_to_str(self.group_by) + ","
		if self.groups_to_display != self.GroupsToDisplay.ALL: 
			desc += "GROUPS_TO_DISPLAY=" + py_to_str(self.groups_to_display) + ","
		if self.bucket_end_per_group != False: 
			desc += "BUCKET_END_PER_GROUP=" + py_to_str(self.bucket_end_per_group) + ","
		if self.input_field_name != "": 
			desc += "INPUT_FIELD_NAME=" + py_to_str(self.input_field_name) + ","
		if self.keep_initial_schema != False: 
			desc += "KEEP_INITIAL_SCHEMA=" + py_to_str(self.keep_initial_schema) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class OrderByAggr(_graph_components.EpBase):
	"""
		

ORDER_BY_AGGR

Type: Aggregation

Description: For each bucket propagates at most N
sorted ticks, where sorting is done using the specified set of tick
fields.

Python
class name:
OrderByAggr

Input: A time series of ticks.

Output: A time series of ticks, N ticks for each
bucket.

Parameters: See parameters
common to generic aggregations.


  BUCKET_INTERVAL
(seconds/ticks)
  BUCKET_INTERVAL_UNITS
(enumerated type)
  IS_RUNNING_AGGR
(Boolean)
  BUCKET_TIME
(enumerated type)
  BUCKET_END_CRITERIA
(string)
  BOUNDARY_TICK_BUCKET
(NEW/PREVIOUS)
  PARTIAL_BUCKET_HANDLING
(enumerated type)
  NUM_TICKS (integer)
    The number of sorted ticks to be propagated for each bucket.

Special value ALL will propagate
every tick in each bucket.
Default: 1


  ORDER_BY (string [DESC|ASC], [string
[DESC|ASC]])
    A comma-separated list of field name/sorting direction pairs.
TIMESTAMP can be used as a field name. When some ticks have the same
values for all listed fields, they are propagated in the same order in
which they arrived.
Default: DESC

  
  GROUP_BY
(string)
  GROUPS_TO_DISPLAY
(enumerated)
  BUCKET_END_PER_GROUP
(Boolean)

&nbsp;

Example: See the ORDER_BY_AGGR
example in AGGREGATION_EXAMPLES.otq.


	"""
	class Parameters:
		bucket_interval = "BUCKET_INTERVAL"
		bucket_interval_units = "BUCKET_INTERVAL_UNITS"
		is_running_aggr = "IS_RUNNING_AGGR"
		bucket_time = "BUCKET_TIME"
		bucket_end_criteria = "BUCKET_END_CRITERIA"
		boundary_tick_bucket = "BOUNDARY_TICK_BUCKET"
		partial_bucket_handling = "PARTIAL_BUCKET_HANDLING"
		num_ticks = "NUM_TICKS"
		order_by = "ORDER_BY"
		group_by = "GROUP_BY"
		groups_to_display = "GROUPS_TO_DISPLAY"
		bucket_end_per_group = "BUCKET_END_PER_GROUP"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["bucket_interval", "bucket_interval_units", "is_running_aggr", "bucket_time", "bucket_end_criteria", "boundary_tick_bucket", "partial_bucket_handling", "num_ticks", "order_by", "group_by", "groups_to_display", "bucket_end_per_group"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["bucket_interval", "_default_bucket_interval", "bucket_interval_units", "_default_bucket_interval_units", "is_running_aggr", "_default_is_running_aggr", "bucket_time", "_default_bucket_time", "bucket_end_criteria", "_default_bucket_end_criteria", "boundary_tick_bucket", "_default_boundary_tick_bucket", "partial_bucket_handling", "_default_partial_bucket_handling", "num_ticks", "_default_num_ticks", "order_by", "_default_order_by", "group_by", "_default_group_by", "groups_to_display", "_default_groups_to_display", "bucket_end_per_group", "_default_bucket_end_per_group", "stack_info", "_used_strings"]

	class BucketIntervalUnits:
		DAYS = "DAYS"
		FLEXIBLE = "FLEXIBLE"
		MONTHS = "MONTHS"
		SECONDS = "SECONDS"
		TICKS = "TICKS"

	class BucketTime:
		BUCKET_END = "BUCKET_END"
		BUCKET_START = "BUCKET_START"

	class BoundaryTickBucket:
		NEW = "NEW"
		PREVIOUS = "PREVIOUS"

	class PartialBucketHandling:
		AS_SEPARATE_BUCKET = "AS_SEPARATE_BUCKET"
		DISCARD = "DISCARD"
		MERGE_WITH_PREVIOUS = "MERGE_WITH_PREVIOUS"

	class GroupsToDisplay:
		ALL = "ALL"
		EVENT_IN_LAST_BUCKET = "EVENT_IN_LAST_BUCKET"

	def __init__(self, bucket_interval=0, bucket_interval_units=BucketIntervalUnits.SECONDS, is_running_aggr=False, bucket_time=BucketTime.BUCKET_END, bucket_end_criteria="", boundary_tick_bucket=BoundaryTickBucket.NEW, partial_bucket_handling=PartialBucketHandling.AS_SEPARATE_BUCKET, num_ticks="1", order_by="", group_by="", groups_to_display=GroupsToDisplay.ALL, bucket_end_per_group=False):
		_graph_components.EpBase.__init__(self, "ORDER_BY_AGGR")
		self._default_bucket_interval = 0
		self.bucket_interval = bucket_interval
		self._default_bucket_interval_units = type(self).BucketIntervalUnits.SECONDS
		self.bucket_interval_units = bucket_interval_units
		self._default_is_running_aggr = False
		self.is_running_aggr = is_running_aggr
		self._default_bucket_time = type(self).BucketTime.BUCKET_END
		self.bucket_time = bucket_time
		self._default_bucket_end_criteria = ""
		self.bucket_end_criteria = bucket_end_criteria
		self._default_boundary_tick_bucket = type(self).BoundaryTickBucket.NEW
		self.boundary_tick_bucket = boundary_tick_bucket
		self._default_partial_bucket_handling = type(self).PartialBucketHandling.AS_SEPARATE_BUCKET
		self.partial_bucket_handling = partial_bucket_handling
		self._default_num_ticks = "1"
		self.num_ticks = num_ticks
		self._default_order_by = ""
		self.order_by = order_by
		self._default_group_by = ""
		self.group_by = group_by
		self._default_groups_to_display = type(self).GroupsToDisplay.ALL
		self.groups_to_display = groups_to_display
		self._default_bucket_end_per_group = False
		self.bucket_end_per_group = bucket_end_per_group
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_bucket_interval(self, value):
		self.bucket_interval = value
		return self

	def set_bucket_interval_units(self, value):
		self.bucket_interval_units = value
		return self

	def set_is_running_aggr(self, value):
		self.is_running_aggr = value
		return self

	def set_bucket_time(self, value):
		self.bucket_time = value
		return self

	def set_bucket_end_criteria(self, value):
		self.bucket_end_criteria = value
		return self

	def set_boundary_tick_bucket(self, value):
		self.boundary_tick_bucket = value
		return self

	def set_partial_bucket_handling(self, value):
		self.partial_bucket_handling = value
		return self

	def set_num_ticks(self, value):
		self.num_ticks = value
		return self

	def set_order_by(self, value):
		self.order_by = value
		return self

	def set_group_by(self, value):
		self.group_by = value
		return self

	def set_groups_to_display(self, value):
		self.groups_to_display = value
		return self

	def set_bucket_end_per_group(self, value):
		self.bucket_end_per_group = value
		return self

	@staticmethod
	def _get_name():
		return "ORDER_BY_AGGR"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.bucket_interval != 0: 
			desc += "BUCKET_INTERVAL=" + py_to_str(self.bucket_interval) + ","
		if self.bucket_interval_units != self.BucketIntervalUnits.SECONDS: 
			desc += "BUCKET_INTERVAL_UNITS=" + py_to_str(self.bucket_interval_units) + ","
		if self.is_running_aggr != False: 
			desc += "IS_RUNNING_AGGR=" + py_to_str(self.is_running_aggr) + ","
		if self.bucket_time != self.BucketTime.BUCKET_END: 
			desc += "BUCKET_TIME=" + py_to_str(self.bucket_time) + ","
		if self.bucket_end_criteria != "": 
			desc += "BUCKET_END_CRITERIA=" + py_to_str(self.bucket_end_criteria) + ","
		if self.boundary_tick_bucket != self.BoundaryTickBucket.NEW: 
			desc += "BOUNDARY_TICK_BUCKET=" + py_to_str(self.boundary_tick_bucket) + ","
		if self.partial_bucket_handling != self.PartialBucketHandling.AS_SEPARATE_BUCKET: 
			desc += "PARTIAL_BUCKET_HANDLING=" + py_to_str(self.partial_bucket_handling) + ","
		if self.num_ticks != "1": 
			desc += "NUM_TICKS=" + py_to_str(self.num_ticks) + ","
		if self.order_by != "": 
			desc += "ORDER_BY=" + py_to_str(self.order_by) + ","
		if self.group_by != "": 
			desc += "GROUP_BY=" + py_to_str(self.group_by) + ","
		if self.groups_to_display != self.GroupsToDisplay.ALL: 
			desc += "GROUPS_TO_DISPLAY=" + py_to_str(self.groups_to_display) + ","
		if self.bucket_end_per_group != False: 
			desc += "BUCKET_END_PER_GROUP=" + py_to_str(self.bucket_end_per_group) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class NumTicks(_graph_components.EpBase):
	"""
		

NUM_TICKS

Type: Aggregation

Description: For each bucket, counts the number of ticks.

Python
class name:
NumTicks

Input: A time series of ticks.

Output: A time series of ticks, one tick for each bucket.

Parameters: See parameters
common to generic aggregations.


  BUCKET_INTERVAL
(seconds/ticks)
  BUCKET_INTERVAL_UNITS
(enumerated type)
  OUTPUT_INTERVAL
(seconds)
  OUTPUT_INTERVAL_UNITS
(SECONDS/TICKS)
  IS_RUNNING_AGGR
(Boolean)
  BUCKET_TIME
(Boolean)
  BUCKET_END_CRITERIA
(expression)
  BOUNDARY_TICK_BUCKET
(NEW/PREVIOUS)
  ALL_FIELDS_FOR_SLIDING
(Boolean)
  PARTIAL_BUCKET_HANDLING
(enumerated type)
  OUTPUT_FIELD_NAME
(string)
  GROUP_BY
(string)
  GROUPS_TO_DISPLAY
(enumerated)
  BUCKET_END_PER_GROUP
(Boolean)

Notes: See the notes on generic
aggregations.

Examples: See the NUM_TICKS
example in AGGREGATION_EXAMPLES.otq.


	"""
	class Parameters:
		bucket_interval = "BUCKET_INTERVAL"
		bucket_interval_units = "BUCKET_INTERVAL_UNITS"
		output_interval = "OUTPUT_INTERVAL"
		output_interval_units = "OUTPUT_INTERVAL_UNITS"
		is_running_aggr = "IS_RUNNING_AGGR"
		bucket_time = "BUCKET_TIME"
		bucket_end_criteria = "BUCKET_END_CRITERIA"
		boundary_tick_bucket = "BOUNDARY_TICK_BUCKET"
		all_fields_for_sliding = "ALL_FIELDS_FOR_SLIDING"
		partial_bucket_handling = "PARTIAL_BUCKET_HANDLING"
		output_field_name = "OUTPUT_FIELD_NAME"
		group_by = "GROUP_BY"
		groups_to_display = "GROUPS_TO_DISPLAY"
		bucket_end_per_group = "BUCKET_END_PER_GROUP"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["bucket_interval", "bucket_interval_units", "output_interval", "output_interval_units", "is_running_aggr", "bucket_time", "bucket_end_criteria", "boundary_tick_bucket", "all_fields_for_sliding", "partial_bucket_handling", "output_field_name", "group_by", "groups_to_display", "bucket_end_per_group"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["bucket_interval", "_default_bucket_interval", "bucket_interval_units", "_default_bucket_interval_units", "output_interval", "_default_output_interval", "output_interval_units", "_default_output_interval_units", "is_running_aggr", "_default_is_running_aggr", "bucket_time", "_default_bucket_time", "bucket_end_criteria", "_default_bucket_end_criteria", "boundary_tick_bucket", "_default_boundary_tick_bucket", "all_fields_for_sliding", "_default_all_fields_for_sliding", "partial_bucket_handling", "_default_partial_bucket_handling", "output_field_name", "_default_output_field_name", "group_by", "_default_group_by", "groups_to_display", "_default_groups_to_display", "bucket_end_per_group", "_default_bucket_end_per_group", "stack_info", "_used_strings"]

	class BucketIntervalUnits:
		DAYS = "DAYS"
		FLEXIBLE = "FLEXIBLE"
		MONTHS = "MONTHS"
		SECONDS = "SECONDS"
		TICKS = "TICKS"

	class OutputIntervalUnits:
		SECONDS = "SECONDS"
		TICKS = "TICKS"

	class BucketTime:
		BUCKET_END = "BUCKET_END"
		BUCKET_START = "BUCKET_START"

	class BoundaryTickBucket:
		NEW = "NEW"
		PREVIOUS = "PREVIOUS"

	class AllFieldsForSliding:
		WHEN_TICKS_EXIT_WINDOW = "WHEN_TICKS_EXIT_WINDOW"
		FALSE = "false"
		TRUE = "true"

	class PartialBucketHandling:
		AS_SEPARATE_BUCKET = "AS_SEPARATE_BUCKET"
		DISCARD = "DISCARD"
		MERGE_WITH_PREVIOUS = "MERGE_WITH_PREVIOUS"

	class GroupsToDisplay:
		ALL = "ALL"
		EVENT_IN_LAST_BUCKET = "EVENT_IN_LAST_BUCKET"

	def __init__(self, bucket_interval=0, bucket_interval_units=BucketIntervalUnits.SECONDS, output_interval="", output_interval_units=OutputIntervalUnits.SECONDS, is_running_aggr=False, bucket_time=BucketTime.BUCKET_END, bucket_end_criteria="", boundary_tick_bucket=BoundaryTickBucket.NEW, all_fields_for_sliding=AllFieldsForSliding.FALSE, partial_bucket_handling=PartialBucketHandling.AS_SEPARATE_BUCKET, output_field_name="VALUE", group_by="", groups_to_display=GroupsToDisplay.ALL, bucket_end_per_group=False, Out = ""):
		_graph_components.EpBase.__init__(self, "NUM_TICKS")
		self._default_bucket_interval = 0
		self.bucket_interval = bucket_interval
		self._default_bucket_interval_units = type(self).BucketIntervalUnits.SECONDS
		self.bucket_interval_units = bucket_interval_units
		self._default_output_interval = ""
		self.output_interval = output_interval
		self._default_output_interval_units = type(self).OutputIntervalUnits.SECONDS
		self.output_interval_units = output_interval_units
		self._default_is_running_aggr = False
		self.is_running_aggr = is_running_aggr
		self._default_bucket_time = type(self).BucketTime.BUCKET_END
		self.bucket_time = bucket_time
		self._default_bucket_end_criteria = ""
		self.bucket_end_criteria = bucket_end_criteria
		self._default_boundary_tick_bucket = type(self).BoundaryTickBucket.NEW
		self.boundary_tick_bucket = boundary_tick_bucket
		self._default_all_fields_for_sliding = type(self).AllFieldsForSliding.FALSE
		self.all_fields_for_sliding = all_fields_for_sliding
		self._default_partial_bucket_handling = type(self).PartialBucketHandling.AS_SEPARATE_BUCKET
		self.partial_bucket_handling = partial_bucket_handling
		self._default_output_field_name = "VALUE"
		self.output_field_name = output_field_name
		self._default_group_by = ""
		self.group_by = group_by
		self._default_groups_to_display = type(self).GroupsToDisplay.ALL
		self.groups_to_display = groups_to_display
		self._default_bucket_end_per_group = False
		self.bucket_end_per_group = bucket_end_per_group
		if Out != "":
			self.output_field_name=Out
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_bucket_interval(self, value):
		self.bucket_interval = value
		return self

	def set_bucket_interval_units(self, value):
		self.bucket_interval_units = value
		return self

	def set_output_interval(self, value):
		self.output_interval = value
		return self

	def set_output_interval_units(self, value):
		self.output_interval_units = value
		return self

	def set_is_running_aggr(self, value):
		self.is_running_aggr = value
		return self

	def set_bucket_time(self, value):
		self.bucket_time = value
		return self

	def set_bucket_end_criteria(self, value):
		self.bucket_end_criteria = value
		return self

	def set_boundary_tick_bucket(self, value):
		self.boundary_tick_bucket = value
		return self

	def set_all_fields_for_sliding(self, value):
		self.all_fields_for_sliding = value
		return self

	def set_partial_bucket_handling(self, value):
		self.partial_bucket_handling = value
		return self

	def set_output_field_name(self, value):
		self.output_field_name = value
		return self

	def set_group_by(self, value):
		self.group_by = value
		return self

	def set_groups_to_display(self, value):
		self.groups_to_display = value
		return self

	def set_bucket_end_per_group(self, value):
		self.bucket_end_per_group = value
		return self

	@staticmethod
	def _get_name():
		return "NUM_TICKS"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.bucket_interval != 0: 
			desc += "BUCKET_INTERVAL=" + py_to_str(self.bucket_interval) + ","
		if self.bucket_interval_units != self.BucketIntervalUnits.SECONDS: 
			desc += "BUCKET_INTERVAL_UNITS=" + py_to_str(self.bucket_interval_units) + ","
		if self.output_interval != "": 
			desc += "OUTPUT_INTERVAL=" + py_to_str(self.output_interval) + ","
		if self.output_interval_units != self.OutputIntervalUnits.SECONDS: 
			desc += "OUTPUT_INTERVAL_UNITS=" + py_to_str(self.output_interval_units) + ","
		if self.is_running_aggr != False: 
			desc += "IS_RUNNING_AGGR=" + py_to_str(self.is_running_aggr) + ","
		if self.bucket_time != self.BucketTime.BUCKET_END: 
			desc += "BUCKET_TIME=" + py_to_str(self.bucket_time) + ","
		if self.bucket_end_criteria != "": 
			desc += "BUCKET_END_CRITERIA=" + py_to_str(self.bucket_end_criteria) + ","
		if self.boundary_tick_bucket != self.BoundaryTickBucket.NEW: 
			desc += "BOUNDARY_TICK_BUCKET=" + py_to_str(self.boundary_tick_bucket) + ","
		if self.all_fields_for_sliding != self.AllFieldsForSliding.FALSE: 
			desc += "ALL_FIELDS_FOR_SLIDING=" + py_to_str(self.all_fields_for_sliding) + ","
		if self.partial_bucket_handling != self.PartialBucketHandling.AS_SEPARATE_BUCKET: 
			desc += "PARTIAL_BUCKET_HANDLING=" + py_to_str(self.partial_bucket_handling) + ","
		if self.output_field_name != "VALUE": 
			desc += "OUTPUT_FIELD_NAME=" + py_to_str(self.output_field_name) + ","
		if self.group_by != "": 
			desc += "GROUP_BY=" + py_to_str(self.group_by) + ","
		if self.groups_to_display != self.GroupsToDisplay.ALL: 
			desc += "GROUPS_TO_DISPLAY=" + py_to_str(self.groups_to_display) + ","
		if self.bucket_end_per_group != False: 
			desc += "BUCKET_END_PER_GROUP=" + py_to_str(self.bucket_end_per_group) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


Count = NumTicks


class DumpTickSet(_graph_components.EpBase):
	"""
		

DUMP_TICK_SET

Type: Other

Description: Propagates all ticks from a given tick set upon
the arrival of each input tick (the input tick itself doesn't get
propagated). TIMESTAMPs of all propagated ticks are equal to the input
tick's TIMESTAMP.

Python
class name:
DumpTickSet

Input: A time series of ticks.

Output: A time series of ticks.

Parameters:


  TICK_SET (string)
    Specifies the tick set
name, whose ticks are propagated, upon receiving each input tick.

  
  DELIMITER (enumerated)
    This parameter specifies the policy for adding the delimiter
field. Possible options are:

    
      NONE
        No additional field is added to propagated ticks.

      
      TICK_AT_END
        An extra tick is created after the last tick. Also, an
additional column is added to output ticks. The extra tick has values
of all fields set to the defaults (0,NaN,""), except the delimiter
field, which is set to "D." All other ticks have this field's value set
to empty string.

      
      FLAG_AT_END
        The delimiter field is appended to each output tick. The
field's value is empty for all ticks except the last one, which is "D."

      
      
        The name of the additional field is "DELIMITER"+NAME_SUFFIX,
where NAME_SUFFIX is specified by the ADDED_FIELD_NAME_SUFFIX parameter.

      
    
    Default: NONE

  
  ADDED_FIELD_NAME_SUFFIX
(string)
    The suffix to add to the name of the additional field (i.e.,
DELIMITER). This facilitates having DUMP_TICK_SET nodes feed other
DUMP_TICK_SET nodes.
    Default: empty

  
  PROPAGATE_INPUT_TICKS (Boolean)
    If true propagates input ticks.
    Default: false

  
  WHEN_TO_DUMP (enumerated)
    This parameter specifies the policy for propagating tick set's
content:

    
      FIRST_TICK
        Propagate content only after first tick.

      
      EVERY_TICK
        For every input tick propagates content of the list.

      
      BEFORE_TICK
        Propagates before input ticks, content will be propagated
even if there are no input ticks.

      
    
    Default: EVERY_TICK

  

Examples:

DUMP_TICK_SET(TICK_SET="STATE::S1",DELIMITER=TICK_AT_END)

See DUMP_TICK_SET examples in TICK_SETS_AND_LISTS.otq.


	"""
	class Parameters:
		tick_set = "TICK_SET"
		delimiter = "DELIMITER"
		added_field_name_suffix = "ADDED_FIELD_NAME_SUFFIX"
		propagate_input_ticks = "PROPAGATE_INPUT_TICKS"
		when_to_dump = "WHEN_TO_DUMP"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["tick_set", "delimiter", "added_field_name_suffix", "propagate_input_ticks", "when_to_dump"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["tick_set", "_default_tick_set", "delimiter", "_default_delimiter", "added_field_name_suffix", "_default_added_field_name_suffix", "propagate_input_ticks", "_default_propagate_input_ticks", "when_to_dump", "_default_when_to_dump", "stack_info", "_used_strings"]

	class Delimiter:
		FLAG_AT_END = "FLAG_AT_END"
		NONE = "NONE"
		TICK_AT_END = "TICK_AT_END"

	class WhenToDump:
		BEFORE_TICK = "BEFORE_TICK"
		EVERY_TICK = "EVERY_TICK"
		FIRST_TICK = "FIRST_TICK"

	def __init__(self, tick_set="", delimiter=Delimiter.NONE, added_field_name_suffix="", propagate_input_ticks=False, when_to_dump=WhenToDump.EVERY_TICK):
		_graph_components.EpBase.__init__(self, "DUMP_TICK_SET")
		self._default_tick_set = ""
		self.tick_set = tick_set
		self._default_delimiter = type(self).Delimiter.NONE
		self.delimiter = delimiter
		self._default_added_field_name_suffix = ""
		self.added_field_name_suffix = added_field_name_suffix
		self._default_propagate_input_ticks = False
		self.propagate_input_ticks = propagate_input_ticks
		self._default_when_to_dump = type(self).WhenToDump.EVERY_TICK
		self.when_to_dump = when_to_dump
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_tick_set(self, value):
		self.tick_set = value
		return self

	def set_delimiter(self, value):
		self.delimiter = value
		return self

	def set_added_field_name_suffix(self, value):
		self.added_field_name_suffix = value
		return self

	def set_propagate_input_ticks(self, value):
		self.propagate_input_ticks = value
		return self

	def set_when_to_dump(self, value):
		self.when_to_dump = value
		return self

	@staticmethod
	def _get_name():
		return "DUMP_TICK_SET"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.tick_set != "": 
			desc += "TICK_SET=" + py_to_str(self.tick_set) + ","
		if self.delimiter != self.Delimiter.NONE: 
			desc += "DELIMITER=" + py_to_str(self.delimiter) + ","
		if self.added_field_name_suffix != "": 
			desc += "ADDED_FIELD_NAME_SUFFIX=" + py_to_str(self.added_field_name_suffix) + ","
		if self.propagate_input_ticks != False: 
			desc += "PROPAGATE_INPUT_TICKS=" + py_to_str(self.propagate_input_ticks) + ","
		if self.when_to_dump != self.WhenToDump.EVERY_TICK: 
			desc += "WHEN_TO_DUMP=" + py_to_str(self.when_to_dump) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class DumpTickList(_graph_components.EpBase):
	"""
		

DUMP_TICK_LIST

Type: Other

Description: Propagates all ticks from a given tick list upon
the arrival of input tick (the input tick itself doesn't get
propagated). Preserves original TIMESTAMPs from tick list, if they fall
into query start/end time range, and for ticks before start time and
after end time, timestamps are changed to start time and end time
correspondingly.

Python
class name:
DumpTickList

Input: A time series of ticks (EP expects only one input
tick).

Output: A time series of ticks.

Parameters:


  TICK_LIST (string)
    Specifies the tick list
name, whose ticks are propagated, upon receiving each input tick.

  
  DELIMITER (enumerated)
    This parameter specifies the policy for adding the delimiter
field. Possible options are:

    
      NONE
        No additional field is added to propagated ticks.

      
      TICK_AT_END
        An extra tick is created after the last tick. Also, an
additional column is added to output ticks. The extra tick has values
of all fields set to the defaults (0,NaN,""), except the delimiter
field, which is set to "D." All other ticks have this field's value set
to empty string.

      
      FLAG_AT_END
        The delimiter field is appended to each output tick. The
field's value is empty for all ticks except the last one, which is "D."

      
      
        The name of the additional field is "DELIMITER"+NAME_SUFFIX,
where NAME_SUFFIX is specified by the ADDED_FIELD_NAME_SUFFIX parameter.

      
    
    Default: NONE

  
  ADDED_FIELD_NAME_SUFFIX
(string)
    The suffix to add to the name of the additional field (i.e.,
DELIMITER).
    Default: empty

  
  PROPAGATE_INPUT_TICKS (Boolean)
    If true propagates input ticks.
    Default: false

  
  WHEN_TO_DUMP (enumerated)
    This parameter specifies the policy for propagating tick list's
content:

    
      FIRST_TICK
        Propagate content only after first tick.

      
      BEFORE_TICK
        Propagates before input ticks, content will be propagated
even if there are no input ticks.

      
    
    Default: FIRST_TICK

  

Examples:

DUMP_TICK_LIST(TICK_LIST="STATE::S1",DELIMITER=TICK_AT_END)

See the DUMP_TICK_LIST examples
in TICK_SETS_AND_LISTS.otq.


	"""
	class Parameters:
		tick_list = "TICK_LIST"
		delimiter = "DELIMITER"
		added_field_name_suffix = "ADDED_FIELD_NAME_SUFFIX"
		propagate_input_ticks = "PROPAGATE_INPUT_TICKS"
		when_to_dump = "WHEN_TO_DUMP"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["tick_list", "delimiter", "added_field_name_suffix", "propagate_input_ticks", "when_to_dump"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["tick_list", "_default_tick_list", "delimiter", "_default_delimiter", "added_field_name_suffix", "_default_added_field_name_suffix", "propagate_input_ticks", "_default_propagate_input_ticks", "when_to_dump", "_default_when_to_dump", "stack_info", "_used_strings"]

	class Delimiter:
		FLAG_AT_END = "FLAG_AT_END"
		NONE = "NONE"
		TICK_AT_END = "TICK_AT_END"

	class WhenToDump:
		BEFORE_TICK = "BEFORE_TICK"
		FIRST_TICK = "FIRST_TICK"

	def __init__(self, tick_list="", delimiter=Delimiter.NONE, added_field_name_suffix="", propagate_input_ticks=False, when_to_dump=WhenToDump.FIRST_TICK):
		_graph_components.EpBase.__init__(self, "DUMP_TICK_LIST")
		self._default_tick_list = ""
		self.tick_list = tick_list
		self._default_delimiter = type(self).Delimiter.NONE
		self.delimiter = delimiter
		self._default_added_field_name_suffix = ""
		self.added_field_name_suffix = added_field_name_suffix
		self._default_propagate_input_ticks = False
		self.propagate_input_ticks = propagate_input_ticks
		self._default_when_to_dump = type(self).WhenToDump.FIRST_TICK
		self.when_to_dump = when_to_dump
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_tick_list(self, value):
		self.tick_list = value
		return self

	def set_delimiter(self, value):
		self.delimiter = value
		return self

	def set_added_field_name_suffix(self, value):
		self.added_field_name_suffix = value
		return self

	def set_propagate_input_ticks(self, value):
		self.propagate_input_ticks = value
		return self

	def set_when_to_dump(self, value):
		self.when_to_dump = value
		return self

	@staticmethod
	def _get_name():
		return "DUMP_TICK_LIST"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.tick_list != "": 
			desc += "TICK_LIST=" + py_to_str(self.tick_list) + ","
		if self.delimiter != self.Delimiter.NONE: 
			desc += "DELIMITER=" + py_to_str(self.delimiter) + ","
		if self.added_field_name_suffix != "": 
			desc += "ADDED_FIELD_NAME_SUFFIX=" + py_to_str(self.added_field_name_suffix) + ","
		if self.propagate_input_ticks != False: 
			desc += "PROPAGATE_INPUT_TICKS=" + py_to_str(self.propagate_input_ticks) + ","
		if self.when_to_dump != self.WhenToDump.FIRST_TICK: 
			desc += "WHEN_TO_DUMP=" + py_to_str(self.when_to_dump) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class DumpTickDeque(_graph_components.EpBase):
	"""
		

DUMP_TICK_DEQUE

Type: Other

Description: Propagates all ticks from a given tick list upon
the arrival of input tick (the input tick itself doesn't get
propagated). Preserves original TIMESTAMPs from tick list, if they fall
into query start/end time range, and for ticks before start time and
after end time, timestamps are changed to start time and end time
correspondingly.

Python
class name:&nbsp;DumpTickDeque

Input: A time series of ticks (EP expects only one input
tick).

Output: A time series of ticks.

Parameters:


  TICK_DEQUE (string)
    Specifies the tick deque name,
whose ticks are propagated, upon receiving each input tick.

  
  DELIMITER (enumerated)
    This parameter specifies the policy for adding the delimiter
field. Possible options are:

    
      NONE
        No additional field is added to propagated ticks.

      
      TICK_AT_END
        An extra tick is created after the last tick. Also, an
additional column is added to output ticks. The extra tick has values
of all fields set to the defaults (0,NaN,""), except the delimiter
field, which is set to "D." All other ticks have this field's value set
to empty string.

      
      FLAG_AT_END
        The delimiter field is appended to each output tick. The
field's value is empty for all ticks except the last one, which is "D."

      
      
        The name of the additional field is "DELIMITER"+NAME_SUFFIX,
where NAME_SUFFIX is specified by the ADDED_FIELD_NAME_SUFFIX parameter.

      
    
    Default: NONE

  
  ADDED_FIELD_NAME_SUFFIX
(string)
    The suffix to add to the name of the additional field (i.e.,
DELIMITER).
    Default: empty

  
  PROPAGATE_INPUT_TICKS (Boolean)
    If true propagates input ticks.
    Default: false

  
  WHEN_TO_DUMP (enumerated)
    This parameter specifies the policy for propagating tick list's
content:

    
      FIRST_TICK
        Propagate content only after first tick.

      
      BEFORE_TICK
        Propagates before input ticks, content will be propagated
even if there are no input ticks.

      
    
    Default: FIRST_TICK

  

Examples:

DUMP_TICK_DEQUE(TICK_DEQUE="STATE::S1",DELIMITER=TICK_AT_END)

See the DUMP_TICK_DEQUE examples
in TICK_SETS_AND_LISTS.otq.


	"""
	class Parameters:
		tick_deque = "TICK_DEQUE"
		delimiter = "DELIMITER"
		added_field_name_suffix = "ADDED_FIELD_NAME_SUFFIX"
		propagate_input_ticks = "PROPAGATE_INPUT_TICKS"
		when_to_dump = "WHEN_TO_DUMP"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["tick_deque", "delimiter", "added_field_name_suffix", "propagate_input_ticks", "when_to_dump"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["tick_deque", "_default_tick_deque", "delimiter", "_default_delimiter", "added_field_name_suffix", "_default_added_field_name_suffix", "propagate_input_ticks", "_default_propagate_input_ticks", "when_to_dump", "_default_when_to_dump", "stack_info", "_used_strings"]

	class Delimiter:
		FLAG_AT_END = "FLAG_AT_END"
		NONE = "NONE"
		TICK_AT_END = "TICK_AT_END"

	class WhenToDump:
		BEFORE_TICK = "BEFORE_TICK"
		FIRST_TICK = "FIRST_TICK"

	def __init__(self, tick_deque="", delimiter=Delimiter.NONE, added_field_name_suffix="", propagate_input_ticks=False, when_to_dump=WhenToDump.FIRST_TICK):
		_graph_components.EpBase.__init__(self, "DUMP_TICK_DEQUE")
		self._default_tick_deque = ""
		self.tick_deque = tick_deque
		self._default_delimiter = type(self).Delimiter.NONE
		self.delimiter = delimiter
		self._default_added_field_name_suffix = ""
		self.added_field_name_suffix = added_field_name_suffix
		self._default_propagate_input_ticks = False
		self.propagate_input_ticks = propagate_input_ticks
		self._default_when_to_dump = type(self).WhenToDump.FIRST_TICK
		self.when_to_dump = when_to_dump
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_tick_deque(self, value):
		self.tick_deque = value
		return self

	def set_delimiter(self, value):
		self.delimiter = value
		return self

	def set_added_field_name_suffix(self, value):
		self.added_field_name_suffix = value
		return self

	def set_propagate_input_ticks(self, value):
		self.propagate_input_ticks = value
		return self

	def set_when_to_dump(self, value):
		self.when_to_dump = value
		return self

	@staticmethod
	def _get_name():
		return "DUMP_TICK_DEQUE"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.tick_deque != "": 
			desc += "TICK_DEQUE=" + py_to_str(self.tick_deque) + ","
		if self.delimiter != self.Delimiter.NONE: 
			desc += "DELIMITER=" + py_to_str(self.delimiter) + ","
		if self.added_field_name_suffix != "": 
			desc += "ADDED_FIELD_NAME_SUFFIX=" + py_to_str(self.added_field_name_suffix) + ","
		if self.propagate_input_ticks != False: 
			desc += "PROPAGATE_INPUT_TICKS=" + py_to_str(self.propagate_input_ticks) + ","
		if self.when_to_dump != self.WhenToDump.FIRST_TICK: 
			desc += "WHEN_TO_DUMP=" + py_to_str(self.when_to_dump) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class EstimateTsDelay(_graph_components.EpBase):
	"""
		

ESTIMATE_TS_DELAY

Type: Aggregation

Description: Given two time series of ticks, computes how much delay the first input series has in relation to the second.
A positive delay indicates that the first series' ticks happen later than the second series' ticks.
A negative delay indicates that the second series' ticks happen later instead.


The two series do not necessarily have to be identical with respect to the delay, nor do they have to represent
  the same quantity (i.e. be of the same magnitude). The only requirement to get meaningful results is that
  the two series be (linearly) correlated.



  See also: script/verify_computation_estimate_ts_delay.py in the OneTick distribution,
  which can be used to see exactly what is being computed and verify that OneTick gives the same results as the script.



  Output ticks always have 2 fields: 

  DELAY_MS, which is the computed delay in milliseconds, and
  CORRELATION, which is the Zero-Normalized Cross-Correlation of the two
  time series after that delay is applied.

Python
class name:
EstimateTsDelay

Input: Two time series of ticks.

Output: A time series of ticks, one for each bucket.
Note that the output tick for the bucket is created
MAX_TS_DELAY_MSEC milliseconds after the bucket end.

Parameters: See parameters
common to generic aggregations.


  SMALLEST_TIME_GRANULARITY_MSEC (milliseconds)
    This EP works by first sampling the source tick series with a constant rate. This is the sampling interval (1 / rate).
    As a consequence, any computed delay will be divisible by this value.
    It is important to carefully choose this parameter, as this EP has a computational cost of
      O(N * log(N)) per bucket,
      where N = (duration_of_bucket_in_msec + MAX_TS_DELAY_MSEC) / SMALLEST_TIME_GRANULARITY_MSEC.
     Default: 1.

    
  MAX_TS_DELAY_MSEC (milliseconds)
    The known upper bound on the delay's magnitude. The computed delay will never be greater than this value.
     Default: 1000.

    
  BUCKET_INTERVAL
    (seconds/ticks)
  BUCKET_INTERVAL_UNITS
    (enumerated type)
  BUCKET_TIME
    (Boolean)
  BUCKET_END_CRITERIA
    (expression)
  BOUNDARY_TICK_BUCKET
    (NEW/PREVIOUS)
  PARTIAL_BUCKET_HANDLING
    (enumerated type)
  INPUT_FIELD1_NAME
    (string)
  INPUT_FIELD2_NAME
    (string)
  GROUP_BY
    (string)
  GROUPS_TO_DISPLAY
    (enumerated)
  BUCKET_END_PER_GROUP
    (Boolean)

Examples: Calculate the delay between two symbols' price for each 1-hour bucket.

ESTIMATE_TS_DELAY(BUCKET_INTERVAL=3600,INPUT_FIELD1_NAME=PRICE,INPUT_FIELD2_NAME=PRICE,MAX_TS_DELAY_MSEC=100000)

See the ESTIMATE_TS_DELAY
example in ESTIMATE_TS_DELAY.otq.


	"""
	class Parameters:
		bucket_interval = "BUCKET_INTERVAL"
		bucket_interval_units = "BUCKET_INTERVAL_UNITS"
		bucket_time = "BUCKET_TIME"
		bucket_end_criteria = "BUCKET_END_CRITERIA"
		boundary_tick_bucket = "BOUNDARY_TICK_BUCKET"
		partial_bucket_handling = "PARTIAL_BUCKET_HANDLING"
		max_ts_delay_msec = "MAX_TS_DELAY_MSEC"
		smallest_time_granularity_msec = "SMALLEST_TIME_GRANULARITY_MSEC"
		input_field1_name = "INPUT_FIELD1_NAME"
		input_field2_name = "INPUT_FIELD2_NAME"
		group_by = "GROUP_BY"
		groups_to_display = "GROUPS_TO_DISPLAY"
		bucket_end_per_group = "BUCKET_END_PER_GROUP"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["bucket_interval", "bucket_interval_units", "bucket_time", "bucket_end_criteria", "boundary_tick_bucket", "partial_bucket_handling", "max_ts_delay_msec", "smallest_time_granularity_msec", "input_field1_name", "input_field2_name", "group_by", "groups_to_display", "bucket_end_per_group"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["bucket_interval", "_default_bucket_interval", "bucket_interval_units", "_default_bucket_interval_units", "bucket_time", "_default_bucket_time", "bucket_end_criteria", "_default_bucket_end_criteria", "boundary_tick_bucket", "_default_boundary_tick_bucket", "partial_bucket_handling", "_default_partial_bucket_handling", "max_ts_delay_msec", "_default_max_ts_delay_msec", "smallest_time_granularity_msec", "_default_smallest_time_granularity_msec", "input_field1_name", "_default_input_field1_name", "input_field2_name", "_default_input_field2_name", "group_by", "_default_group_by", "groups_to_display", "_default_groups_to_display", "bucket_end_per_group", "_default_bucket_end_per_group", "stack_info", "_used_strings"]

	class BucketIntervalUnits:
		DAYS = "DAYS"
		FLEXIBLE = "FLEXIBLE"
		MONTHS = "MONTHS"
		SECONDS = "SECONDS"
		TICKS = "TICKS"

	class BucketTime:
		BUCKET_END = "BUCKET_END"
		BUCKET_START = "BUCKET_START"

	class BoundaryTickBucket:
		NEW = "NEW"
		PREVIOUS = "PREVIOUS"

	class PartialBucketHandling:
		AS_SEPARATE_BUCKET = "AS_SEPARATE_BUCKET"
		DISCARD = "DISCARD"
		MERGE_WITH_PREVIOUS = "MERGE_WITH_PREVIOUS"

	class GroupsToDisplay:
		ALL = "ALL"
		EVENT_IN_LAST_BUCKET = "EVENT_IN_LAST_BUCKET"

	def __init__(self, bucket_interval=0, bucket_interval_units=BucketIntervalUnits.SECONDS, bucket_time=BucketTime.BUCKET_END, bucket_end_criteria="", boundary_tick_bucket=BoundaryTickBucket.NEW, partial_bucket_handling=PartialBucketHandling.AS_SEPARATE_BUCKET, max_ts_delay_msec=1000, smallest_time_granularity_msec=1, input_field1_name="", input_field2_name="", group_by="", groups_to_display=GroupsToDisplay.ALL, bucket_end_per_group=False):
		_graph_components.EpBase.__init__(self, "ESTIMATE_TS_DELAY")
		self._default_bucket_interval = 0
		self.bucket_interval = bucket_interval
		self._default_bucket_interval_units = type(self).BucketIntervalUnits.SECONDS
		self.bucket_interval_units = bucket_interval_units
		self._default_bucket_time = type(self).BucketTime.BUCKET_END
		self.bucket_time = bucket_time
		self._default_bucket_end_criteria = ""
		self.bucket_end_criteria = bucket_end_criteria
		self._default_boundary_tick_bucket = type(self).BoundaryTickBucket.NEW
		self.boundary_tick_bucket = boundary_tick_bucket
		self._default_partial_bucket_handling = type(self).PartialBucketHandling.AS_SEPARATE_BUCKET
		self.partial_bucket_handling = partial_bucket_handling
		self._default_max_ts_delay_msec = 1000
		self.max_ts_delay_msec = max_ts_delay_msec
		self._default_smallest_time_granularity_msec = 1
		self.smallest_time_granularity_msec = smallest_time_granularity_msec
		self._default_input_field1_name = ""
		self.input_field1_name = input_field1_name
		self._default_input_field2_name = ""
		self.input_field2_name = input_field2_name
		self._default_group_by = ""
		self.group_by = group_by
		self._default_groups_to_display = type(self).GroupsToDisplay.ALL
		self.groups_to_display = groups_to_display
		self._default_bucket_end_per_group = False
		self.bucket_end_per_group = bucket_end_per_group
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_bucket_interval(self, value):
		self.bucket_interval = value
		return self

	def set_bucket_interval_units(self, value):
		self.bucket_interval_units = value
		return self

	def set_bucket_time(self, value):
		self.bucket_time = value
		return self

	def set_bucket_end_criteria(self, value):
		self.bucket_end_criteria = value
		return self

	def set_boundary_tick_bucket(self, value):
		self.boundary_tick_bucket = value
		return self

	def set_partial_bucket_handling(self, value):
		self.partial_bucket_handling = value
		return self

	def set_max_ts_delay_msec(self, value):
		self.max_ts_delay_msec = value
		return self

	def set_smallest_time_granularity_msec(self, value):
		self.smallest_time_granularity_msec = value
		return self

	def set_input_field1_name(self, value):
		self.input_field1_name = value
		return self

	def set_input_field2_name(self, value):
		self.input_field2_name = value
		return self

	def set_group_by(self, value):
		self.group_by = value
		return self

	def set_groups_to_display(self, value):
		self.groups_to_display = value
		return self

	def set_bucket_end_per_group(self, value):
		self.bucket_end_per_group = value
		return self

	@staticmethod
	def _get_name():
		return "ESTIMATE_TS_DELAY"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.bucket_interval != 0: 
			desc += "BUCKET_INTERVAL=" + py_to_str(self.bucket_interval) + ","
		if self.bucket_interval_units != self.BucketIntervalUnits.SECONDS: 
			desc += "BUCKET_INTERVAL_UNITS=" + py_to_str(self.bucket_interval_units) + ","
		if self.bucket_time != self.BucketTime.BUCKET_END: 
			desc += "BUCKET_TIME=" + py_to_str(self.bucket_time) + ","
		if self.bucket_end_criteria != "": 
			desc += "BUCKET_END_CRITERIA=" + py_to_str(self.bucket_end_criteria) + ","
		if self.boundary_tick_bucket != self.BoundaryTickBucket.NEW: 
			desc += "BOUNDARY_TICK_BUCKET=" + py_to_str(self.boundary_tick_bucket) + ","
		if self.partial_bucket_handling != self.PartialBucketHandling.AS_SEPARATE_BUCKET: 
			desc += "PARTIAL_BUCKET_HANDLING=" + py_to_str(self.partial_bucket_handling) + ","
		if self.max_ts_delay_msec != 1000: 
			desc += "MAX_TS_DELAY_MSEC=" + py_to_str(self.max_ts_delay_msec) + ","
		if self.smallest_time_granularity_msec != 1: 
			desc += "SMALLEST_TIME_GRANULARITY_MSEC=" + py_to_str(self.smallest_time_granularity_msec) + ","
		if self.input_field1_name != "": 
			desc += "INPUT_FIELD1_NAME=" + py_to_str(self.input_field1_name) + ","
		if self.input_field2_name != "": 
			desc += "INPUT_FIELD2_NAME=" + py_to_str(self.input_field2_name) + ","
		if self.group_by != "": 
			desc += "GROUP_BY=" + py_to_str(self.group_by) + ","
		if self.groups_to_display != self.GroupsToDisplay.ALL: 
			desc += "GROUPS_TO_DISPLAY=" + py_to_str(self.groups_to_display) + ","
		if self.bucket_end_per_group != False: 
			desc += "BUCKET_END_PER_GROUP=" + py_to_str(self.bucket_end_per_group) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class Average(_graph_components.EpBase):
	"""
		

AVERAGE

Type: Aggregation

Description: For each bucket, computes the average value of a
specified numeric field and adds a new field with the calculated value
to the output tick.

Python
class name:&nbsp;Average

Input: A time series of ticks

Output: A new time series of ticks, one output tick for each
bucket period

Parameters: See parameters
common to generic aggregations.


  BUCKET_INTERVAL
(seconds/ticks)
  BUCKET_INTERVAL_UNITS
(enumerated type)
  OUTPUT_INTERVAL
(seconds)
  OUTPUT_INTERVAL_UNITS
(enumerated type)
  IS_RUNNING_AGGR
(Boolean)
  BUCKET_TIME
(Boolean)
  BUCKET_END_CRITERIA
(expression)
  BOUNDARY_TICK_BUCKET
(NEW/PREVIOUS)
  ALL_FIELDS_FOR_SLIDING
(Boolean)
  PARTIAL_BUCKET_HANDLING
(enumerated type)
  OUTPUT_FIELD_NAME
(string)
  GROUP_BY
(string)
  GROUPS_TO_DISPLAY
(enumerated)
  BUCKET_END_PER_GROUP
(Boolean)
  INPUT_FIELD_NAME
(string)

Examples: See the following examples:


  AVERAGE in AGGREGATION_EXAMPLES.otq
  SIMPLE_STAT in AGGREGATION_STATISTICS_EXAMPLES.otq
  STAT1 in AGGREGATION_BASIC_EXAMPLES.otq


	"""
	class Parameters:
		bucket_interval = "BUCKET_INTERVAL"
		bucket_interval_units = "BUCKET_INTERVAL_UNITS"
		output_interval = "OUTPUT_INTERVAL"
		output_interval_units = "OUTPUT_INTERVAL_UNITS"
		is_running_aggr = "IS_RUNNING_AGGR"
		bucket_time = "BUCKET_TIME"
		bucket_end_criteria = "BUCKET_END_CRITERIA"
		boundary_tick_bucket = "BOUNDARY_TICK_BUCKET"
		all_fields_for_sliding = "ALL_FIELDS_FOR_SLIDING"
		partial_bucket_handling = "PARTIAL_BUCKET_HANDLING"
		output_field_name = "OUTPUT_FIELD_NAME"
		group_by = "GROUP_BY"
		groups_to_display = "GROUPS_TO_DISPLAY"
		bucket_end_per_group = "BUCKET_END_PER_GROUP"
		input_field_name = "INPUT_FIELD_NAME"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["bucket_interval", "bucket_interval_units", "output_interval", "output_interval_units", "is_running_aggr", "bucket_time", "bucket_end_criteria", "boundary_tick_bucket", "all_fields_for_sliding", "partial_bucket_handling", "output_field_name", "group_by", "groups_to_display", "bucket_end_per_group", "input_field_name"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["bucket_interval", "_default_bucket_interval", "bucket_interval_units", "_default_bucket_interval_units", "output_interval", "_default_output_interval", "output_interval_units", "_default_output_interval_units", "is_running_aggr", "_default_is_running_aggr", "bucket_time", "_default_bucket_time", "bucket_end_criteria", "_default_bucket_end_criteria", "boundary_tick_bucket", "_default_boundary_tick_bucket", "all_fields_for_sliding", "_default_all_fields_for_sliding", "partial_bucket_handling", "_default_partial_bucket_handling", "output_field_name", "_default_output_field_name", "group_by", "_default_group_by", "groups_to_display", "_default_groups_to_display", "bucket_end_per_group", "_default_bucket_end_per_group", "input_field_name", "_default_input_field_name", "stack_info", "_used_strings"]

	class BucketIntervalUnits:
		DAYS = "DAYS"
		FLEXIBLE = "FLEXIBLE"
		MONTHS = "MONTHS"
		SECONDS = "SECONDS"
		TICKS = "TICKS"

	class OutputIntervalUnits:
		SECONDS = "SECONDS"
		TICKS = "TICKS"

	class BucketTime:
		BUCKET_END = "BUCKET_END"
		BUCKET_START = "BUCKET_START"

	class BoundaryTickBucket:
		NEW = "NEW"
		PREVIOUS = "PREVIOUS"

	class AllFieldsForSliding:
		WHEN_TICKS_EXIT_WINDOW = "WHEN_TICKS_EXIT_WINDOW"
		FALSE = "false"
		TRUE = "true"

	class PartialBucketHandling:
		AS_SEPARATE_BUCKET = "AS_SEPARATE_BUCKET"
		DISCARD = "DISCARD"
		MERGE_WITH_PREVIOUS = "MERGE_WITH_PREVIOUS"

	class GroupsToDisplay:
		ALL = "ALL"
		EVENT_IN_LAST_BUCKET = "EVENT_IN_LAST_BUCKET"

	def __init__(self, bucket_interval=0, bucket_interval_units=BucketIntervalUnits.SECONDS, output_interval="", output_interval_units=OutputIntervalUnits.SECONDS, is_running_aggr=False, bucket_time=BucketTime.BUCKET_END, bucket_end_criteria="", boundary_tick_bucket=BoundaryTickBucket.NEW, all_fields_for_sliding=AllFieldsForSliding.FALSE, partial_bucket_handling=PartialBucketHandling.AS_SEPARATE_BUCKET, output_field_name="VALUE", group_by="", groups_to_display=GroupsToDisplay.ALL, bucket_end_per_group=False, input_field_name="", In = "", Out = ""):
		_graph_components.EpBase.__init__(self, "AVERAGE")
		self._default_bucket_interval = 0
		self.bucket_interval = bucket_interval
		self._default_bucket_interval_units = type(self).BucketIntervalUnits.SECONDS
		self.bucket_interval_units = bucket_interval_units
		self._default_output_interval = ""
		self.output_interval = output_interval
		self._default_output_interval_units = type(self).OutputIntervalUnits.SECONDS
		self.output_interval_units = output_interval_units
		self._default_is_running_aggr = False
		self.is_running_aggr = is_running_aggr
		self._default_bucket_time = type(self).BucketTime.BUCKET_END
		self.bucket_time = bucket_time
		self._default_bucket_end_criteria = ""
		self.bucket_end_criteria = bucket_end_criteria
		self._default_boundary_tick_bucket = type(self).BoundaryTickBucket.NEW
		self.boundary_tick_bucket = boundary_tick_bucket
		self._default_all_fields_for_sliding = type(self).AllFieldsForSliding.FALSE
		self.all_fields_for_sliding = all_fields_for_sliding
		self._default_partial_bucket_handling = type(self).PartialBucketHandling.AS_SEPARATE_BUCKET
		self.partial_bucket_handling = partial_bucket_handling
		self._default_output_field_name = "VALUE"
		self.output_field_name = output_field_name
		self._default_group_by = ""
		self.group_by = group_by
		self._default_groups_to_display = type(self).GroupsToDisplay.ALL
		self.groups_to_display = groups_to_display
		self._default_bucket_end_per_group = False
		self.bucket_end_per_group = bucket_end_per_group
		self._default_input_field_name = ""
		self.input_field_name = input_field_name
		if In != "":
			self.input_field_name=In
		if Out != "":
			self.output_field_name=Out
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_bucket_interval(self, value):
		self.bucket_interval = value
		return self

	def set_bucket_interval_units(self, value):
		self.bucket_interval_units = value
		return self

	def set_output_interval(self, value):
		self.output_interval = value
		return self

	def set_output_interval_units(self, value):
		self.output_interval_units = value
		return self

	def set_is_running_aggr(self, value):
		self.is_running_aggr = value
		return self

	def set_bucket_time(self, value):
		self.bucket_time = value
		return self

	def set_bucket_end_criteria(self, value):
		self.bucket_end_criteria = value
		return self

	def set_boundary_tick_bucket(self, value):
		self.boundary_tick_bucket = value
		return self

	def set_all_fields_for_sliding(self, value):
		self.all_fields_for_sliding = value
		return self

	def set_partial_bucket_handling(self, value):
		self.partial_bucket_handling = value
		return self

	def set_output_field_name(self, value):
		self.output_field_name = value
		return self

	def set_group_by(self, value):
		self.group_by = value
		return self

	def set_groups_to_display(self, value):
		self.groups_to_display = value
		return self

	def set_bucket_end_per_group(self, value):
		self.bucket_end_per_group = value
		return self

	def set_input_field_name(self, value):
		self.input_field_name = value
		return self

	@staticmethod
	def _get_name():
		return "AVERAGE"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.bucket_interval != 0: 
			desc += "BUCKET_INTERVAL=" + py_to_str(self.bucket_interval) + ","
		if self.bucket_interval_units != self.BucketIntervalUnits.SECONDS: 
			desc += "BUCKET_INTERVAL_UNITS=" + py_to_str(self.bucket_interval_units) + ","
		if self.output_interval != "": 
			desc += "OUTPUT_INTERVAL=" + py_to_str(self.output_interval) + ","
		if self.output_interval_units != self.OutputIntervalUnits.SECONDS: 
			desc += "OUTPUT_INTERVAL_UNITS=" + py_to_str(self.output_interval_units) + ","
		if self.is_running_aggr != False: 
			desc += "IS_RUNNING_AGGR=" + py_to_str(self.is_running_aggr) + ","
		if self.bucket_time != self.BucketTime.BUCKET_END: 
			desc += "BUCKET_TIME=" + py_to_str(self.bucket_time) + ","
		if self.bucket_end_criteria != "": 
			desc += "BUCKET_END_CRITERIA=" + py_to_str(self.bucket_end_criteria) + ","
		if self.boundary_tick_bucket != self.BoundaryTickBucket.NEW: 
			desc += "BOUNDARY_TICK_BUCKET=" + py_to_str(self.boundary_tick_bucket) + ","
		if self.all_fields_for_sliding != self.AllFieldsForSliding.FALSE: 
			desc += "ALL_FIELDS_FOR_SLIDING=" + py_to_str(self.all_fields_for_sliding) + ","
		if self.partial_bucket_handling != self.PartialBucketHandling.AS_SEPARATE_BUCKET: 
			desc += "PARTIAL_BUCKET_HANDLING=" + py_to_str(self.partial_bucket_handling) + ","
		if self.output_field_name != "VALUE": 
			desc += "OUTPUT_FIELD_NAME=" + py_to_str(self.output_field_name) + ","
		if self.group_by != "": 
			desc += "GROUP_BY=" + py_to_str(self.group_by) + ","
		if self.groups_to_display != self.GroupsToDisplay.ALL: 
			desc += "GROUPS_TO_DISPLAY=" + py_to_str(self.groups_to_display) + ","
		if self.bucket_end_per_group != False: 
			desc += "BUCKET_END_PER_GROUP=" + py_to_str(self.bucket_end_per_group) + ","
		if self.input_field_name != "": 
			desc += "INPUT_FIELD_NAME=" + py_to_str(self.input_field_name) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


Avg = Average


class NumDistinct(_graph_components.EpBase):
	"""
		

NUM_DISTINCT

Type: Aggregation

Description: For each bucket, counts the number of distinct
ticks.

SHOW_EPS_FOR_OT_SOLUTIONS config
parameter should be set in order to be able to use this EP.

Python
class name:
NumDistinct

Input: A time series of ticks.

Output: A time series of ticks, one tick for each bucket.

Parameters: See parameters
common to generic aggregations.


  BUCKET_INTERVAL
(seconds/ticks)
  BUCKET_INTERVAL_UNITS
(enumerated type)
  OUTPUT_INTERVAL
(seconds)
  OUTPUT_INTERVAL_UNITS
(SECONDS/TICKS)
  IS_RUNNING_AGGR
(Boolean)
  BUCKET_TIME
(Boolean)
  BUCKET_END_CRITERIA
(expression)
  BOUNDARY_TICK_BUCKET
(NEW/PREVIOUS)
  PARTIAL_BUCKET_HANDLING
(enumerated type)
  KEYS (string)
    Specifies a list of tick attributes for which unique values are
found. The ticks in the input time series must contain those attributes.

  
  GROUP_BY
(string)
  OUTPUT_FIELD_NAME
(string)
  ALL_FIELDS_FOR_SLIDING
(Boolean)

Notes: See the notes on generic
aggregations.


	"""
	class Parameters:
		bucket_interval = "BUCKET_INTERVAL"
		bucket_interval_units = "BUCKET_INTERVAL_UNITS"
		output_interval = "OUTPUT_INTERVAL"
		output_interval_units = "OUTPUT_INTERVAL_UNITS"
		is_running_aggr = "IS_RUNNING_AGGR"
		bucket_time = "BUCKET_TIME"
		bucket_end_criteria = "BUCKET_END_CRITERIA"
		boundary_tick_bucket = "BOUNDARY_TICK_BUCKET"
		partial_bucket_handling = "PARTIAL_BUCKET_HANDLING"
		keys = "KEYS"
		group_by = "GROUP_BY"
		output_field_name = "OUTPUT_FIELD_NAME"
		all_fields_for_sliding = "ALL_FIELDS_FOR_SLIDING"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["bucket_interval", "bucket_interval_units", "output_interval", "output_interval_units", "is_running_aggr", "bucket_time", "bucket_end_criteria", "boundary_tick_bucket", "partial_bucket_handling", "keys", "group_by", "output_field_name", "all_fields_for_sliding"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["bucket_interval", "_default_bucket_interval", "bucket_interval_units", "_default_bucket_interval_units", "output_interval", "_default_output_interval", "output_interval_units", "_default_output_interval_units", "is_running_aggr", "_default_is_running_aggr", "bucket_time", "_default_bucket_time", "bucket_end_criteria", "_default_bucket_end_criteria", "boundary_tick_bucket", "_default_boundary_tick_bucket", "partial_bucket_handling", "_default_partial_bucket_handling", "keys", "_default_keys", "group_by", "_default_group_by", "output_field_name", "_default_output_field_name", "all_fields_for_sliding", "_default_all_fields_for_sliding", "stack_info", "_used_strings"]

	class BucketIntervalUnits:
		DAYS = "DAYS"
		FLEXIBLE = "FLEXIBLE"
		MONTHS = "MONTHS"
		SECONDS = "SECONDS"
		TICKS = "TICKS"

	class OutputIntervalUnits:
		SECONDS = "SECONDS"
		TICKS = "TICKS"

	class BucketTime:
		BUCKET_END = "BUCKET_END"
		BUCKET_START = "BUCKET_START"

	class BoundaryTickBucket:
		NEW = "NEW"
		PREVIOUS = "PREVIOUS"

	class PartialBucketHandling:
		AS_SEPARATE_BUCKET = "AS_SEPARATE_BUCKET"
		DISCARD = "DISCARD"
		MERGE_WITH_PREVIOUS = "MERGE_WITH_PREVIOUS"

	class AllFieldsForSliding:
		WHEN_TICKS_EXIT_WINDOW = "WHEN_TICKS_EXIT_WINDOW"
		FALSE = "false"
		TRUE = "true"

	def __init__(self, bucket_interval=0, bucket_interval_units=BucketIntervalUnits.SECONDS, output_interval="", output_interval_units=OutputIntervalUnits.SECONDS, is_running_aggr=False, bucket_time=BucketTime.BUCKET_END, bucket_end_criteria="", boundary_tick_bucket=BoundaryTickBucket.NEW, partial_bucket_handling=PartialBucketHandling.AS_SEPARATE_BUCKET, keys="", group_by="", output_field_name="VALUE", all_fields_for_sliding=AllFieldsForSliding.FALSE, Out = ""):
		_graph_components.EpBase.__init__(self, "NUM_DISTINCT")
		self._default_bucket_interval = 0
		self.bucket_interval = bucket_interval
		self._default_bucket_interval_units = type(self).BucketIntervalUnits.SECONDS
		self.bucket_interval_units = bucket_interval_units
		self._default_output_interval = ""
		self.output_interval = output_interval
		self._default_output_interval_units = type(self).OutputIntervalUnits.SECONDS
		self.output_interval_units = output_interval_units
		self._default_is_running_aggr = False
		self.is_running_aggr = is_running_aggr
		self._default_bucket_time = type(self).BucketTime.BUCKET_END
		self.bucket_time = bucket_time
		self._default_bucket_end_criteria = ""
		self.bucket_end_criteria = bucket_end_criteria
		self._default_boundary_tick_bucket = type(self).BoundaryTickBucket.NEW
		self.boundary_tick_bucket = boundary_tick_bucket
		self._default_partial_bucket_handling = type(self).PartialBucketHandling.AS_SEPARATE_BUCKET
		self.partial_bucket_handling = partial_bucket_handling
		self._default_keys = ""
		self.keys = keys
		self._default_group_by = ""
		self.group_by = group_by
		self._default_output_field_name = "VALUE"
		self.output_field_name = output_field_name
		self._default_all_fields_for_sliding = type(self).AllFieldsForSliding.FALSE
		self.all_fields_for_sliding = all_fields_for_sliding
		if Out != "":
			self.output_field_name=Out
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_bucket_interval(self, value):
		self.bucket_interval = value
		return self

	def set_bucket_interval_units(self, value):
		self.bucket_interval_units = value
		return self

	def set_output_interval(self, value):
		self.output_interval = value
		return self

	def set_output_interval_units(self, value):
		self.output_interval_units = value
		return self

	def set_is_running_aggr(self, value):
		self.is_running_aggr = value
		return self

	def set_bucket_time(self, value):
		self.bucket_time = value
		return self

	def set_bucket_end_criteria(self, value):
		self.bucket_end_criteria = value
		return self

	def set_boundary_tick_bucket(self, value):
		self.boundary_tick_bucket = value
		return self

	def set_partial_bucket_handling(self, value):
		self.partial_bucket_handling = value
		return self

	def set_keys(self, value):
		self.keys = value
		return self

	def set_group_by(self, value):
		self.group_by = value
		return self

	def set_output_field_name(self, value):
		self.output_field_name = value
		return self

	def set_all_fields_for_sliding(self, value):
		self.all_fields_for_sliding = value
		return self

	@staticmethod
	def _get_name():
		return "NUM_DISTINCT"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.bucket_interval != 0: 
			desc += "BUCKET_INTERVAL=" + py_to_str(self.bucket_interval) + ","
		if self.bucket_interval_units != self.BucketIntervalUnits.SECONDS: 
			desc += "BUCKET_INTERVAL_UNITS=" + py_to_str(self.bucket_interval_units) + ","
		if self.output_interval != "": 
			desc += "OUTPUT_INTERVAL=" + py_to_str(self.output_interval) + ","
		if self.output_interval_units != self.OutputIntervalUnits.SECONDS: 
			desc += "OUTPUT_INTERVAL_UNITS=" + py_to_str(self.output_interval_units) + ","
		if self.is_running_aggr != False: 
			desc += "IS_RUNNING_AGGR=" + py_to_str(self.is_running_aggr) + ","
		if self.bucket_time != self.BucketTime.BUCKET_END: 
			desc += "BUCKET_TIME=" + py_to_str(self.bucket_time) + ","
		if self.bucket_end_criteria != "": 
			desc += "BUCKET_END_CRITERIA=" + py_to_str(self.bucket_end_criteria) + ","
		if self.boundary_tick_bucket != self.BoundaryTickBucket.NEW: 
			desc += "BOUNDARY_TICK_BUCKET=" + py_to_str(self.boundary_tick_bucket) + ","
		if self.partial_bucket_handling != self.PartialBucketHandling.AS_SEPARATE_BUCKET: 
			desc += "PARTIAL_BUCKET_HANDLING=" + py_to_str(self.partial_bucket_handling) + ","
		if self.keys != "": 
			desc += "KEYS=" + py_to_str(self.keys) + ","
		if self.group_by != "": 
			desc += "GROUP_BY=" + py_to_str(self.group_by) + ","
		if self.output_field_name != "VALUE": 
			desc += "OUTPUT_FIELD_NAME=" + py_to_str(self.output_field_name) + ","
		if self.all_fields_for_sliding != self.AllFieldsForSliding.FALSE: 
			desc += "ALL_FIELDS_FOR_SLIDING=" + py_to_str(self.all_fields_for_sliding) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class Median(_graph_components.EpBase):
	"""
		

MEDIAN

Type: Aggregation

Description: For each bucket computes the median value of a
specified numeric field.

Python
class name:
Median

Input: A time series of ticks.

Output: A time series of ticks, one for each bucket.

Parameters: See parameters
common to generic aggregations.


  BUCKET_INTERVAL
(seconds/ticks)
  BUCKET_INTERVAL_UNITS
(enumerated type)
  OUTPUT_INTERVAL
(seconds)
  OUTPUT_INTERVAL_UNITS
(SECONDS/TICKS)
  IS_RUNNING_AGGR
(Boolean)
  BUCKET_TIME
(Boolean)
  BUCKET_END_CRITERIA
(expression)
  BOUNDARY_TICK_BUCKET
(NEW/PREVIOUS)
  ALL_FIELDS_FOR_SLIDING
(Boolean)
  PARTIAL_BUCKET_HANDLING
(enumerated type)
  OUTPUT_FIELD_NAME
(string)
  GROUP_BY
(string)
  GROUPS_TO_DISPLAY
(enumerated)
  BUCKET_END_PER_GROUP
(Boolean)
  INPUT_FIELD_NAME
(string)
EXPECT_DECIMALS (enumerated type)
This parameter should be set to true
if the input field of this EP may contain high-precision decimal values.
Such values require special handling and are represented using the
DECIMAL128 type. If set to true,
the output field will have the DECIMAL128 type.
When this parameter is set to
IF_INPUT_VAL_IS_DECIMAL, the EP behaves the same way as when this
parameter is set to true when the input
field is of type DECIMAL128, and the same way as when it is set to
false when the input field is not of type
DECIMAL128.
When set to IF_INPUT_VAL_IS_DECIMAL and the input field type
changes during execution (e.g., from DOUBLE to DECIMAL128 or from DECIMAL128
to DOUBLE), the output field type is switched accordingly. Switching from
DECIMAL128 to DOUBLE may result in loss of precision.
Default: false



Example: See MEDIAN in AGGREGATION_EXAMPLES.otq.


	"""
	class Parameters:
		bucket_interval = "BUCKET_INTERVAL"
		bucket_interval_units = "BUCKET_INTERVAL_UNITS"
		output_interval = "OUTPUT_INTERVAL"
		output_interval_units = "OUTPUT_INTERVAL_UNITS"
		is_running_aggr = "IS_RUNNING_AGGR"
		bucket_time = "BUCKET_TIME"
		bucket_end_criteria = "BUCKET_END_CRITERIA"
		boundary_tick_bucket = "BOUNDARY_TICK_BUCKET"
		all_fields_for_sliding = "ALL_FIELDS_FOR_SLIDING"
		partial_bucket_handling = "PARTIAL_BUCKET_HANDLING"
		output_field_name = "OUTPUT_FIELD_NAME"
		group_by = "GROUP_BY"
		groups_to_display = "GROUPS_TO_DISPLAY"
		bucket_end_per_group = "BUCKET_END_PER_GROUP"
		input_field_name = "INPUT_FIELD_NAME"
		expect_decimals = "EXPECT_DECIMALS"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["bucket_interval", "bucket_interval_units", "output_interval", "output_interval_units", "is_running_aggr", "bucket_time", "bucket_end_criteria", "boundary_tick_bucket", "all_fields_for_sliding", "partial_bucket_handling", "output_field_name", "group_by", "groups_to_display", "bucket_end_per_group", "input_field_name", "expect_decimals"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["bucket_interval", "_default_bucket_interval", "bucket_interval_units", "_default_bucket_interval_units", "output_interval", "_default_output_interval", "output_interval_units", "_default_output_interval_units", "is_running_aggr", "_default_is_running_aggr", "bucket_time", "_default_bucket_time", "bucket_end_criteria", "_default_bucket_end_criteria", "boundary_tick_bucket", "_default_boundary_tick_bucket", "all_fields_for_sliding", "_default_all_fields_for_sliding", "partial_bucket_handling", "_default_partial_bucket_handling", "output_field_name", "_default_output_field_name", "group_by", "_default_group_by", "groups_to_display", "_default_groups_to_display", "bucket_end_per_group", "_default_bucket_end_per_group", "input_field_name", "_default_input_field_name", "expect_decimals", "_default_expect_decimals", "stack_info", "_used_strings"]

	class BucketIntervalUnits:
		DAYS = "DAYS"
		FLEXIBLE = "FLEXIBLE"
		MONTHS = "MONTHS"
		SECONDS = "SECONDS"
		TICKS = "TICKS"

	class OutputIntervalUnits:
		SECONDS = "SECONDS"
		TICKS = "TICKS"

	class BucketTime:
		BUCKET_END = "BUCKET_END"
		BUCKET_START = "BUCKET_START"

	class BoundaryTickBucket:
		NEW = "NEW"
		PREVIOUS = "PREVIOUS"

	class AllFieldsForSliding:
		WHEN_TICKS_EXIT_WINDOW = "WHEN_TICKS_EXIT_WINDOW"
		FALSE = "false"
		TRUE = "true"

	class PartialBucketHandling:
		AS_SEPARATE_BUCKET = "AS_SEPARATE_BUCKET"
		DISCARD = "DISCARD"
		MERGE_WITH_PREVIOUS = "MERGE_WITH_PREVIOUS"

	class GroupsToDisplay:
		ALL = "ALL"
		EVENT_IN_LAST_BUCKET = "EVENT_IN_LAST_BUCKET"

	class ExpectDecimals:
		IF_INPUT_VAL_IS_DECIMAL = "IF_INPUT_VAL_IS_DECIMAL"
		FALSE = "false"
		TRUE = "true"

	def __init__(self, bucket_interval=0, bucket_interval_units=BucketIntervalUnits.SECONDS, output_interval="", output_interval_units=OutputIntervalUnits.SECONDS, is_running_aggr=False, bucket_time=BucketTime.BUCKET_END, bucket_end_criteria="", boundary_tick_bucket=BoundaryTickBucket.NEW, all_fields_for_sliding=AllFieldsForSliding.FALSE, partial_bucket_handling=PartialBucketHandling.AS_SEPARATE_BUCKET, output_field_name="VALUE", group_by="", groups_to_display=GroupsToDisplay.ALL, bucket_end_per_group=False, input_field_name="", expect_decimals=ExpectDecimals.FALSE, In = "", Out = ""):
		_graph_components.EpBase.__init__(self, "MEDIAN")
		self._default_bucket_interval = 0
		self.bucket_interval = bucket_interval
		self._default_bucket_interval_units = type(self).BucketIntervalUnits.SECONDS
		self.bucket_interval_units = bucket_interval_units
		self._default_output_interval = ""
		self.output_interval = output_interval
		self._default_output_interval_units = type(self).OutputIntervalUnits.SECONDS
		self.output_interval_units = output_interval_units
		self._default_is_running_aggr = False
		self.is_running_aggr = is_running_aggr
		self._default_bucket_time = type(self).BucketTime.BUCKET_END
		self.bucket_time = bucket_time
		self._default_bucket_end_criteria = ""
		self.bucket_end_criteria = bucket_end_criteria
		self._default_boundary_tick_bucket = type(self).BoundaryTickBucket.NEW
		self.boundary_tick_bucket = boundary_tick_bucket
		self._default_all_fields_for_sliding = type(self).AllFieldsForSliding.FALSE
		self.all_fields_for_sliding = all_fields_for_sliding
		self._default_partial_bucket_handling = type(self).PartialBucketHandling.AS_SEPARATE_BUCKET
		self.partial_bucket_handling = partial_bucket_handling
		self._default_output_field_name = "VALUE"
		self.output_field_name = output_field_name
		self._default_group_by = ""
		self.group_by = group_by
		self._default_groups_to_display = type(self).GroupsToDisplay.ALL
		self.groups_to_display = groups_to_display
		self._default_bucket_end_per_group = False
		self.bucket_end_per_group = bucket_end_per_group
		self._default_input_field_name = ""
		self.input_field_name = input_field_name
		self._default_expect_decimals = type(self).ExpectDecimals.FALSE
		self.expect_decimals = expect_decimals
		if In != "":
			self.input_field_name=In
		if Out != "":
			self.output_field_name=Out
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_bucket_interval(self, value):
		self.bucket_interval = value
		return self

	def set_bucket_interval_units(self, value):
		self.bucket_interval_units = value
		return self

	def set_output_interval(self, value):
		self.output_interval = value
		return self

	def set_output_interval_units(self, value):
		self.output_interval_units = value
		return self

	def set_is_running_aggr(self, value):
		self.is_running_aggr = value
		return self

	def set_bucket_time(self, value):
		self.bucket_time = value
		return self

	def set_bucket_end_criteria(self, value):
		self.bucket_end_criteria = value
		return self

	def set_boundary_tick_bucket(self, value):
		self.boundary_tick_bucket = value
		return self

	def set_all_fields_for_sliding(self, value):
		self.all_fields_for_sliding = value
		return self

	def set_partial_bucket_handling(self, value):
		self.partial_bucket_handling = value
		return self

	def set_output_field_name(self, value):
		self.output_field_name = value
		return self

	def set_group_by(self, value):
		self.group_by = value
		return self

	def set_groups_to_display(self, value):
		self.groups_to_display = value
		return self

	def set_bucket_end_per_group(self, value):
		self.bucket_end_per_group = value
		return self

	def set_input_field_name(self, value):
		self.input_field_name = value
		return self

	def set_expect_decimals(self, value):
		self.expect_decimals = value
		return self

	@staticmethod
	def _get_name():
		return "MEDIAN"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.bucket_interval != 0: 
			desc += "BUCKET_INTERVAL=" + py_to_str(self.bucket_interval) + ","
		if self.bucket_interval_units != self.BucketIntervalUnits.SECONDS: 
			desc += "BUCKET_INTERVAL_UNITS=" + py_to_str(self.bucket_interval_units) + ","
		if self.output_interval != "": 
			desc += "OUTPUT_INTERVAL=" + py_to_str(self.output_interval) + ","
		if self.output_interval_units != self.OutputIntervalUnits.SECONDS: 
			desc += "OUTPUT_INTERVAL_UNITS=" + py_to_str(self.output_interval_units) + ","
		if self.is_running_aggr != False: 
			desc += "IS_RUNNING_AGGR=" + py_to_str(self.is_running_aggr) + ","
		if self.bucket_time != self.BucketTime.BUCKET_END: 
			desc += "BUCKET_TIME=" + py_to_str(self.bucket_time) + ","
		if self.bucket_end_criteria != "": 
			desc += "BUCKET_END_CRITERIA=" + py_to_str(self.bucket_end_criteria) + ","
		if self.boundary_tick_bucket != self.BoundaryTickBucket.NEW: 
			desc += "BOUNDARY_TICK_BUCKET=" + py_to_str(self.boundary_tick_bucket) + ","
		if self.all_fields_for_sliding != self.AllFieldsForSliding.FALSE: 
			desc += "ALL_FIELDS_FOR_SLIDING=" + py_to_str(self.all_fields_for_sliding) + ","
		if self.partial_bucket_handling != self.PartialBucketHandling.AS_SEPARATE_BUCKET: 
			desc += "PARTIAL_BUCKET_HANDLING=" + py_to_str(self.partial_bucket_handling) + ","
		if self.output_field_name != "VALUE": 
			desc += "OUTPUT_FIELD_NAME=" + py_to_str(self.output_field_name) + ","
		if self.group_by != "": 
			desc += "GROUP_BY=" + py_to_str(self.group_by) + ","
		if self.groups_to_display != self.GroupsToDisplay.ALL: 
			desc += "GROUPS_TO_DISPLAY=" + py_to_str(self.groups_to_display) + ","
		if self.bucket_end_per_group != False: 
			desc += "BUCKET_END_PER_GROUP=" + py_to_str(self.bucket_end_per_group) + ","
		if self.input_field_name != "": 
			desc += "INPUT_FIELD_NAME=" + py_to_str(self.input_field_name) + ","
		if self.expect_decimals != self.ExpectDecimals.FALSE: 
			desc += "EXPECT_DECIMALS=" + py_to_str(self.expect_decimals) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class MultiPortfolioPrice(_graph_components.EpBase):
	"""
		

MULTI_PORTFOLIO_PRICE

Type: Aggregation

Description: For each bucket, computes weighted portfolio
price for multiple portfolios.

Python
class name:&nbsp;MultiPortfolioPrice

Input: Multiple time series of ticks, each having the
attribute PRICE.
Each time series has an associated set of portfolios and weight in each
associated portfolio. Portfolios for each input time series are
provided using a server-side query that is specified in the
PORTFOLIOS_QUERY parameter.

Output: A time series of ticks, one tick per portfolio for
each bucket interval.

Parameters: See the parameters
common to generic aggregations.


  BUCKET_INTERVAL
(seconds)
  BUCKET_INTERVAL_UNITS
(enumerated type)
  OUTPUT_INTERVAL
(seconds)
  OUTPUT_INTERVAL_UNITS
(SECONDS/TICKS)
  IS_RUNNING_AGGR
(Boolean)
  BUCKET_TIME
(Boolean)
  BUCKET_END_CRITERIA
(expression)
  BOUNDARY_TICK_BUCKET
(NEW/PREVIOUS)
  PARTIAL_BUCKET_HANDLING
(enumerated type)
  INPUT_FIELD_NAME
    A comma-separated list of the names of the input fields for
which portfolio value is computed.
Default: PRICE. If there is only one
available field in the input, the default is the name of the available
field.

  
  WEIGHT_FIELD_NAME
    The name of the field in the input ticks that contains current
value
of weight for a member of the portfolio that contributed the tick.
    
    

  
  WEIGHT_MULTIPLIER_FIELD_NAME
(string)
  SIDE (enumerated type)
    When set to LONG, the price
of the portfolio is computed only for the input time series with weight
&gt; 0. When set to SHORT, the price
of the portfolio is computed only for the input time series with weight
&lt; 0.
When set to BOTH, the price of the
portfolio is computed for all input time series.
Default: BOTH

  
  WEIGHT_TYPE (enumerated type)
    When set to ABSOLUTE,
portfolio price is computed as the sum of input_field_value*weight
across all members of the portfolio.
When set to RELATIVE, portfolio
price is computed as the sum of input_field_value*weight/sum_of_all_weights
across all members of the portfolio.
Default: ABSOLUTE

  
  PORTFOLIOS_QUERY
    A mandatory parameter that the specifies server-side .otq file
that is expected to return mandatory columns PORTFOLIO_NAME
and SYMBOL_NAME, as well as an
optional columns WEIGHT, FX_SYMBOL_NAMEand FX_MULTIPLY

  
  PORTFOLIOS_QUERY_PARAMS
    An optional parameter that specifies parameters of the query
specified in PORTFOLIOS_QUERY.

  
  PORTFOLIO_VALUE_FIELD_NAME
    A comma-separated list of the names of the output fields which
contain computed values of the portfolio. The number of the field names
must match the number of the field names listed in the INPUT_FIELD_NAME parameter.
Default: VALUE

  

Notes: See the notes on generic
aggregations.

If WEIGHT_FIELD_NAME is not set,
weights for the computation of portfolio price are provided by the WEIGHT column produced by PORTFOLIOS_QUERY.

If WEIGHT_FIELD_NAME is not set and PORTFOLIOS_QUERY does not return
the WEIGHT column, the weights take the default value 1 for the purposes of portfolio price
calculation.

This event processor should have both portfolio constituent time
series and FX symbol time series, if FX_SYMBOL_NAME field has non-empty
values, as its inputs.

If PORTFOLIOS_QUERY
produces field FX_MULTIPLY, its value 1 (default) means that the value
of an input field for a portfolio constituent is multiplied by the
current PRICE of the FX symbol (specified via field FX_SYMBOL_NAME)
when added to the portfolio price. Its value 0 means that the value of
an input field for a portfolio constituent is divided by the current
PRICE of the FX symbol.

Examples: See MULTIPLE_PORTFOLIO_EXAMPLES.otq.


	"""
	class Parameters:
		bucket_interval = "BUCKET_INTERVAL"
		bucket_interval_units = "BUCKET_INTERVAL_UNITS"
		output_interval = "OUTPUT_INTERVAL"
		output_interval_units = "OUTPUT_INTERVAL_UNITS"
		is_running_aggr = "IS_RUNNING_AGGR"
		bucket_time = "BUCKET_TIME"
		bucket_end_criteria = "BUCKET_END_CRITERIA"
		boundary_tick_bucket = "BOUNDARY_TICK_BUCKET"
		partial_bucket_handling = "PARTIAL_BUCKET_HANDLING"
		input_field_name = "INPUT_FIELD_NAME"
		weight_field_name = "WEIGHT_FIELD_NAME"
		weight_multiplier_field_name = "WEIGHT_MULTIPLIER_FIELD_NAME"
		side = "SIDE"
		weight_type = "WEIGHT_TYPE"
		portfolios_query = "PORTFOLIOS_QUERY"
		portfolios_query_params = "PORTFOLIOS_QUERY_PARAMS"
		portfolio_value_field_name = "PORTFOLIO_VALUE_FIELD_NAME"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["bucket_interval", "bucket_interval_units", "output_interval", "output_interval_units", "is_running_aggr", "bucket_time", "bucket_end_criteria", "boundary_tick_bucket", "partial_bucket_handling", "input_field_name", "weight_field_name", "weight_multiplier_field_name", "side", "weight_type", "portfolios_query", "portfolios_query_params", "portfolio_value_field_name"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["bucket_interval", "_default_bucket_interval", "bucket_interval_units", "_default_bucket_interval_units", "output_interval", "_default_output_interval", "output_interval_units", "_default_output_interval_units", "is_running_aggr", "_default_is_running_aggr", "bucket_time", "_default_bucket_time", "bucket_end_criteria", "_default_bucket_end_criteria", "boundary_tick_bucket", "_default_boundary_tick_bucket", "partial_bucket_handling", "_default_partial_bucket_handling", "input_field_name", "_default_input_field_name", "weight_field_name", "_default_weight_field_name", "weight_multiplier_field_name", "_default_weight_multiplier_field_name", "side", "_default_side", "weight_type", "_default_weight_type", "portfolios_query", "_default_portfolios_query", "portfolios_query_params", "_default_portfolios_query_params", "portfolio_value_field_name", "_default_portfolio_value_field_name", "stack_info", "_used_strings"]

	class BucketIntervalUnits:
		DAYS = "DAYS"
		FLEXIBLE = "FLEXIBLE"
		MONTHS = "MONTHS"
		SECONDS = "SECONDS"
		TICKS = "TICKS"

	class OutputIntervalUnits:
		SECONDS = "SECONDS"
		TICKS = "TICKS"

	class BucketTime:
		BUCKET_END = "BUCKET_END"
		BUCKET_START = "BUCKET_START"

	class BoundaryTickBucket:
		NEW = "NEW"
		PREVIOUS = "PREVIOUS"

	class PartialBucketHandling:
		AS_SEPARATE_BUCKET = "AS_SEPARATE_BUCKET"
		DISCARD = "DISCARD"
		MERGE_WITH_PREVIOUS = "MERGE_WITH_PREVIOUS"

	class Side:
		BOTH = "BOTH"
		LONG = "LONG"
		SHORT = "SHORT"

	class WeightType:
		ABSOLUTE = "ABSOLUTE"
		RELATIVE = "RELATIVE"

	def __init__(self, bucket_interval=0, bucket_interval_units=BucketIntervalUnits.SECONDS, output_interval="", output_interval_units=OutputIntervalUnits.SECONDS, is_running_aggr=False, bucket_time=BucketTime.BUCKET_END, bucket_end_criteria="", boundary_tick_bucket=BoundaryTickBucket.NEW, partial_bucket_handling=PartialBucketHandling.AS_SEPARATE_BUCKET, input_field_name="", weight_field_name="", weight_multiplier_field_name="", side=Side.BOTH, weight_type=WeightType.ABSOLUTE, portfolios_query="", portfolios_query_params="", portfolio_value_field_name="", In = ""):
		_graph_components.EpBase.__init__(self, "MULTI_PORTFOLIO_PRICE")
		self._default_bucket_interval = 0
		self.bucket_interval = bucket_interval
		self._default_bucket_interval_units = type(self).BucketIntervalUnits.SECONDS
		self.bucket_interval_units = bucket_interval_units
		self._default_output_interval = ""
		self.output_interval = output_interval
		self._default_output_interval_units = type(self).OutputIntervalUnits.SECONDS
		self.output_interval_units = output_interval_units
		self._default_is_running_aggr = False
		self.is_running_aggr = is_running_aggr
		self._default_bucket_time = type(self).BucketTime.BUCKET_END
		self.bucket_time = bucket_time
		self._default_bucket_end_criteria = ""
		self.bucket_end_criteria = bucket_end_criteria
		self._default_boundary_tick_bucket = type(self).BoundaryTickBucket.NEW
		self.boundary_tick_bucket = boundary_tick_bucket
		self._default_partial_bucket_handling = type(self).PartialBucketHandling.AS_SEPARATE_BUCKET
		self.partial_bucket_handling = partial_bucket_handling
		self._default_input_field_name = ""
		self.input_field_name = input_field_name
		self._default_weight_field_name = ""
		self.weight_field_name = weight_field_name
		self._default_weight_multiplier_field_name = ""
		self.weight_multiplier_field_name = weight_multiplier_field_name
		self._default_side = type(self).Side.BOTH
		self.side = side
		self._default_weight_type = type(self).WeightType.ABSOLUTE
		self.weight_type = weight_type
		self._default_portfolios_query = ""
		self.portfolios_query = portfolios_query
		self._default_portfolios_query_params = ""
		self.portfolios_query_params = portfolios_query_params
		self._default_portfolio_value_field_name = ""
		self.portfolio_value_field_name = portfolio_value_field_name
		if In != "":
			self.input_field_name=In
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_bucket_interval(self, value):
		self.bucket_interval = value
		return self

	def set_bucket_interval_units(self, value):
		self.bucket_interval_units = value
		return self

	def set_output_interval(self, value):
		self.output_interval = value
		return self

	def set_output_interval_units(self, value):
		self.output_interval_units = value
		return self

	def set_is_running_aggr(self, value):
		self.is_running_aggr = value
		return self

	def set_bucket_time(self, value):
		self.bucket_time = value
		return self

	def set_bucket_end_criteria(self, value):
		self.bucket_end_criteria = value
		return self

	def set_boundary_tick_bucket(self, value):
		self.boundary_tick_bucket = value
		return self

	def set_partial_bucket_handling(self, value):
		self.partial_bucket_handling = value
		return self

	def set_input_field_name(self, value):
		self.input_field_name = value
		return self

	def set_weight_field_name(self, value):
		self.weight_field_name = value
		return self

	def set_weight_multiplier_field_name(self, value):
		self.weight_multiplier_field_name = value
		return self

	def set_side(self, value):
		self.side = value
		return self

	def set_weight_type(self, value):
		self.weight_type = value
		return self

	def set_portfolios_query(self, value):
		self.portfolios_query = value
		return self

	def set_portfolios_query_params(self, value):
		self.portfolios_query_params = value
		return self

	def set_portfolio_value_field_name(self, value):
		self.portfolio_value_field_name = value
		return self

	@staticmethod
	def _get_name():
		return "MULTI_PORTFOLIO_PRICE"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.bucket_interval != 0: 
			desc += "BUCKET_INTERVAL=" + py_to_str(self.bucket_interval) + ","
		if self.bucket_interval_units != self.BucketIntervalUnits.SECONDS: 
			desc += "BUCKET_INTERVAL_UNITS=" + py_to_str(self.bucket_interval_units) + ","
		if self.output_interval != "": 
			desc += "OUTPUT_INTERVAL=" + py_to_str(self.output_interval) + ","
		if self.output_interval_units != self.OutputIntervalUnits.SECONDS: 
			desc += "OUTPUT_INTERVAL_UNITS=" + py_to_str(self.output_interval_units) + ","
		if self.is_running_aggr != False: 
			desc += "IS_RUNNING_AGGR=" + py_to_str(self.is_running_aggr) + ","
		if self.bucket_time != self.BucketTime.BUCKET_END: 
			desc += "BUCKET_TIME=" + py_to_str(self.bucket_time) + ","
		if self.bucket_end_criteria != "": 
			desc += "BUCKET_END_CRITERIA=" + py_to_str(self.bucket_end_criteria) + ","
		if self.boundary_tick_bucket != self.BoundaryTickBucket.NEW: 
			desc += "BOUNDARY_TICK_BUCKET=" + py_to_str(self.boundary_tick_bucket) + ","
		if self.partial_bucket_handling != self.PartialBucketHandling.AS_SEPARATE_BUCKET: 
			desc += "PARTIAL_BUCKET_HANDLING=" + py_to_str(self.partial_bucket_handling) + ","
		if self.input_field_name != "": 
			desc += "INPUT_FIELD_NAME=" + py_to_str(self.input_field_name) + ","
		if self.weight_field_name != "": 
			desc += "WEIGHT_FIELD_NAME=" + py_to_str(self.weight_field_name) + ","
		if self.weight_multiplier_field_name != "": 
			desc += "WEIGHT_MULTIPLIER_FIELD_NAME=" + py_to_str(self.weight_multiplier_field_name) + ","
		if self.side != self.Side.BOTH: 
			desc += "SIDE=" + py_to_str(self.side) + ","
		if self.weight_type != self.WeightType.ABSOLUTE: 
			desc += "WEIGHT_TYPE=" + py_to_str(self.weight_type) + ","
		if self.portfolios_query != "": 
			desc += "PORTFOLIOS_QUERY=" + py_to_str(self.portfolios_query) + ","
		if self.portfolios_query_params != "": 
			desc += "PORTFOLIOS_QUERY_PARAMS=" + py_to_str(self.portfolios_query_params) + ","
		if self.portfolio_value_field_name != "": 
			desc += "PORTFOLIO_VALUE_FIELD_NAME=" + py_to_str(self.portfolio_value_field_name) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class Variance(_graph_components.EpBase):
	"""
		

VARIANCE

Type: Aggregation

Description: For each bucket, computes the variance of a
specified numeric attribute.

Python
class name:
Variance

Input: A time series of ticks.

Output: A time series of ticks.

Parameters: See parameters
common to generic aggregations.


  BUCKET_INTERVAL
(seconds/ticks)
  BUCKET_INTERVAL_UNITS
(enumerated type)
  OUTPUT_INTERVAL
(seconds)
  OUTPUT_INTERVAL_UNITS
(enumerated type)
  IS_RUNNING_AGGR
(Boolean)
  BUCKET_TIME
(Boolean)
  BUCKET_END_CRITERIA
(expression)
  BOUNDARY_TICK_BUCKET
(NEW/PREVIOUS)
  ALL_FIELDS_FOR_SLIDING
(Boolean)
  PARTIAL_BUCKET_HANDLING
(enumerated type)
  OUTPUT_FIELD_NAME
(string)
  GROUP_BY
(string)
  GROUPS_TO_DISPLAY
(enumerated)
  BUCKET_END_PER_GROUP
(Boolean)
  INPUT_FIELD_NAME
(string)
  BIASED (Boolean)
    Switches between biased/unbiased variance calculation.

  

Examples: See

See the VARIANCE_on_PRICE example
in AGGREGATION_EXAMPLES.otq
and VARIANCE in AGGREGATION_STATISTICS_EXAMPLES.otq.


	"""
	class Parameters:
		bucket_interval = "BUCKET_INTERVAL"
		bucket_interval_units = "BUCKET_INTERVAL_UNITS"
		output_interval = "OUTPUT_INTERVAL"
		output_interval_units = "OUTPUT_INTERVAL_UNITS"
		is_running_aggr = "IS_RUNNING_AGGR"
		bucket_time = "BUCKET_TIME"
		bucket_end_criteria = "BUCKET_END_CRITERIA"
		boundary_tick_bucket = "BOUNDARY_TICK_BUCKET"
		all_fields_for_sliding = "ALL_FIELDS_FOR_SLIDING"
		partial_bucket_handling = "PARTIAL_BUCKET_HANDLING"
		output_field_name = "OUTPUT_FIELD_NAME"
		group_by = "GROUP_BY"
		groups_to_display = "GROUPS_TO_DISPLAY"
		bucket_end_per_group = "BUCKET_END_PER_GROUP"
		input_field_name = "INPUT_FIELD_NAME"
		biased = "BIASED"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["bucket_interval", "bucket_interval_units", "output_interval", "output_interval_units", "is_running_aggr", "bucket_time", "bucket_end_criteria", "boundary_tick_bucket", "all_fields_for_sliding", "partial_bucket_handling", "output_field_name", "group_by", "groups_to_display", "bucket_end_per_group", "input_field_name", "biased"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["bucket_interval", "_default_bucket_interval", "bucket_interval_units", "_default_bucket_interval_units", "output_interval", "_default_output_interval", "output_interval_units", "_default_output_interval_units", "is_running_aggr", "_default_is_running_aggr", "bucket_time", "_default_bucket_time", "bucket_end_criteria", "_default_bucket_end_criteria", "boundary_tick_bucket", "_default_boundary_tick_bucket", "all_fields_for_sliding", "_default_all_fields_for_sliding", "partial_bucket_handling", "_default_partial_bucket_handling", "output_field_name", "_default_output_field_name", "group_by", "_default_group_by", "groups_to_display", "_default_groups_to_display", "bucket_end_per_group", "_default_bucket_end_per_group", "input_field_name", "_default_input_field_name", "biased", "_default_biased", "stack_info", "_used_strings"]

	class BucketIntervalUnits:
		DAYS = "DAYS"
		FLEXIBLE = "FLEXIBLE"
		MONTHS = "MONTHS"
		SECONDS = "SECONDS"
		TICKS = "TICKS"

	class OutputIntervalUnits:
		SECONDS = "SECONDS"
		TICKS = "TICKS"

	class BucketTime:
		BUCKET_END = "BUCKET_END"
		BUCKET_START = "BUCKET_START"

	class BoundaryTickBucket:
		NEW = "NEW"
		PREVIOUS = "PREVIOUS"

	class AllFieldsForSliding:
		WHEN_TICKS_EXIT_WINDOW = "WHEN_TICKS_EXIT_WINDOW"
		FALSE = "false"
		TRUE = "true"

	class PartialBucketHandling:
		AS_SEPARATE_BUCKET = "AS_SEPARATE_BUCKET"
		DISCARD = "DISCARD"
		MERGE_WITH_PREVIOUS = "MERGE_WITH_PREVIOUS"

	class GroupsToDisplay:
		ALL = "ALL"
		EVENT_IN_LAST_BUCKET = "EVENT_IN_LAST_BUCKET"

	def __init__(self, bucket_interval=0, bucket_interval_units=BucketIntervalUnits.SECONDS, output_interval="", output_interval_units=OutputIntervalUnits.SECONDS, is_running_aggr=False, bucket_time=BucketTime.BUCKET_END, bucket_end_criteria="", boundary_tick_bucket=BoundaryTickBucket.NEW, all_fields_for_sliding=AllFieldsForSliding.FALSE, partial_bucket_handling=PartialBucketHandling.AS_SEPARATE_BUCKET, output_field_name="VALUE", group_by="", groups_to_display=GroupsToDisplay.ALL, bucket_end_per_group=False, input_field_name="", biased=True, In = "", Out = ""):
		_graph_components.EpBase.__init__(self, "VARIANCE")
		self._default_bucket_interval = 0
		self.bucket_interval = bucket_interval
		self._default_bucket_interval_units = type(self).BucketIntervalUnits.SECONDS
		self.bucket_interval_units = bucket_interval_units
		self._default_output_interval = ""
		self.output_interval = output_interval
		self._default_output_interval_units = type(self).OutputIntervalUnits.SECONDS
		self.output_interval_units = output_interval_units
		self._default_is_running_aggr = False
		self.is_running_aggr = is_running_aggr
		self._default_bucket_time = type(self).BucketTime.BUCKET_END
		self.bucket_time = bucket_time
		self._default_bucket_end_criteria = ""
		self.bucket_end_criteria = bucket_end_criteria
		self._default_boundary_tick_bucket = type(self).BoundaryTickBucket.NEW
		self.boundary_tick_bucket = boundary_tick_bucket
		self._default_all_fields_for_sliding = type(self).AllFieldsForSliding.FALSE
		self.all_fields_for_sliding = all_fields_for_sliding
		self._default_partial_bucket_handling = type(self).PartialBucketHandling.AS_SEPARATE_BUCKET
		self.partial_bucket_handling = partial_bucket_handling
		self._default_output_field_name = "VALUE"
		self.output_field_name = output_field_name
		self._default_group_by = ""
		self.group_by = group_by
		self._default_groups_to_display = type(self).GroupsToDisplay.ALL
		self.groups_to_display = groups_to_display
		self._default_bucket_end_per_group = False
		self.bucket_end_per_group = bucket_end_per_group
		self._default_input_field_name = ""
		self.input_field_name = input_field_name
		self._default_biased = True
		self.biased = biased
		if In != "":
			self.input_field_name=In
		if Out != "":
			self.output_field_name=Out
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_bucket_interval(self, value):
		self.bucket_interval = value
		return self

	def set_bucket_interval_units(self, value):
		self.bucket_interval_units = value
		return self

	def set_output_interval(self, value):
		self.output_interval = value
		return self

	def set_output_interval_units(self, value):
		self.output_interval_units = value
		return self

	def set_is_running_aggr(self, value):
		self.is_running_aggr = value
		return self

	def set_bucket_time(self, value):
		self.bucket_time = value
		return self

	def set_bucket_end_criteria(self, value):
		self.bucket_end_criteria = value
		return self

	def set_boundary_tick_bucket(self, value):
		self.boundary_tick_bucket = value
		return self

	def set_all_fields_for_sliding(self, value):
		self.all_fields_for_sliding = value
		return self

	def set_partial_bucket_handling(self, value):
		self.partial_bucket_handling = value
		return self

	def set_output_field_name(self, value):
		self.output_field_name = value
		return self

	def set_group_by(self, value):
		self.group_by = value
		return self

	def set_groups_to_display(self, value):
		self.groups_to_display = value
		return self

	def set_bucket_end_per_group(self, value):
		self.bucket_end_per_group = value
		return self

	def set_input_field_name(self, value):
		self.input_field_name = value
		return self

	def set_biased(self, value):
		self.biased = value
		return self

	@staticmethod
	def _get_name():
		return "VARIANCE"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.bucket_interval != 0: 
			desc += "BUCKET_INTERVAL=" + py_to_str(self.bucket_interval) + ","
		if self.bucket_interval_units != self.BucketIntervalUnits.SECONDS: 
			desc += "BUCKET_INTERVAL_UNITS=" + py_to_str(self.bucket_interval_units) + ","
		if self.output_interval != "": 
			desc += "OUTPUT_INTERVAL=" + py_to_str(self.output_interval) + ","
		if self.output_interval_units != self.OutputIntervalUnits.SECONDS: 
			desc += "OUTPUT_INTERVAL_UNITS=" + py_to_str(self.output_interval_units) + ","
		if self.is_running_aggr != False: 
			desc += "IS_RUNNING_AGGR=" + py_to_str(self.is_running_aggr) + ","
		if self.bucket_time != self.BucketTime.BUCKET_END: 
			desc += "BUCKET_TIME=" + py_to_str(self.bucket_time) + ","
		if self.bucket_end_criteria != "": 
			desc += "BUCKET_END_CRITERIA=" + py_to_str(self.bucket_end_criteria) + ","
		if self.boundary_tick_bucket != self.BoundaryTickBucket.NEW: 
			desc += "BOUNDARY_TICK_BUCKET=" + py_to_str(self.boundary_tick_bucket) + ","
		if self.all_fields_for_sliding != self.AllFieldsForSliding.FALSE: 
			desc += "ALL_FIELDS_FOR_SLIDING=" + py_to_str(self.all_fields_for_sliding) + ","
		if self.partial_bucket_handling != self.PartialBucketHandling.AS_SEPARATE_BUCKET: 
			desc += "PARTIAL_BUCKET_HANDLING=" + py_to_str(self.partial_bucket_handling) + ","
		if self.output_field_name != "VALUE": 
			desc += "OUTPUT_FIELD_NAME=" + py_to_str(self.output_field_name) + ","
		if self.group_by != "": 
			desc += "GROUP_BY=" + py_to_str(self.group_by) + ","
		if self.groups_to_display != self.GroupsToDisplay.ALL: 
			desc += "GROUPS_TO_DISPLAY=" + py_to_str(self.groups_to_display) + ","
		if self.bucket_end_per_group != False: 
			desc += "BUCKET_END_PER_GROUP=" + py_to_str(self.bucket_end_per_group) + ","
		if self.input_field_name != "": 
			desc += "INPUT_FIELD_NAME=" + py_to_str(self.input_field_name) + ","
		if self.biased != True: 
			desc += "BIASED=" + py_to_str(self.biased) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class Stddev(_graph_components.EpBase):
	"""
		

STDDEV

Type: Aggregation

Description: Computes the standard deviation of the
INPUT_FIELD over the specified bucket interval.

Python
class name:
Stddev

Input: A time series of ticks.

Output: A time series of ticks, one for each bucket.

Parameters: See parameters
common to generic aggregations.


  BUCKET_INTERVAL
(seconds/ticks)
  BUCKET_INTERVAL_UNITS
(enumerated type)
  OUTPUT_INTERVAL
(seconds)
  OUTPUT_INTERVAL_UNITS
(enumerated type)
  IS_RUNNING_AGGR
(Boolean)
  BUCKET_TIME
(Boolean)
  BUCKET_END_CRITERIA
(expression)
  BOUNDARY_TICK_BUCKET
(NEW/PREVIOUS)
  ALL_FIELDS_FOR_SLIDING
(Boolean)
  PARTIAL_BUCKET_HANDLING
(enumerated type)
  OUTPUT_FIELD_NAME
(string)
  GROUP_BY
(string)
  GROUPS_TO_DISPLAY
(enumerated)
  BUCKET_END_PER_GROUP
(Boolean)
  INPUT_FIELD_NAME
(string)
  BIASED (Boolean)
    Switches between biased and unbiased standard deviation
calculation.
Default: true

  

Examples: Calculate the unbiased standard deviation of PRICE
over 1 hour buckets.

STDDEV(3600,
SECONDS,,false,false,AS_SEPERATE_BUCKET,STDDEV_PRICE,,PRICE,false)

See the STDDEV example in AGGREGATION_EXAMPLES.otq.


	"""
	class Parameters:
		bucket_interval = "BUCKET_INTERVAL"
		bucket_interval_units = "BUCKET_INTERVAL_UNITS"
		output_interval = "OUTPUT_INTERVAL"
		output_interval_units = "OUTPUT_INTERVAL_UNITS"
		is_running_aggr = "IS_RUNNING_AGGR"
		bucket_time = "BUCKET_TIME"
		bucket_end_criteria = "BUCKET_END_CRITERIA"
		boundary_tick_bucket = "BOUNDARY_TICK_BUCKET"
		all_fields_for_sliding = "ALL_FIELDS_FOR_SLIDING"
		partial_bucket_handling = "PARTIAL_BUCKET_HANDLING"
		output_field_name = "OUTPUT_FIELD_NAME"
		group_by = "GROUP_BY"
		groups_to_display = "GROUPS_TO_DISPLAY"
		bucket_end_per_group = "BUCKET_END_PER_GROUP"
		input_field_name = "INPUT_FIELD_NAME"
		biased = "BIASED"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["bucket_interval", "bucket_interval_units", "output_interval", "output_interval_units", "is_running_aggr", "bucket_time", "bucket_end_criteria", "boundary_tick_bucket", "all_fields_for_sliding", "partial_bucket_handling", "output_field_name", "group_by", "groups_to_display", "bucket_end_per_group", "input_field_name", "biased"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["bucket_interval", "_default_bucket_interval", "bucket_interval_units", "_default_bucket_interval_units", "output_interval", "_default_output_interval", "output_interval_units", "_default_output_interval_units", "is_running_aggr", "_default_is_running_aggr", "bucket_time", "_default_bucket_time", "bucket_end_criteria", "_default_bucket_end_criteria", "boundary_tick_bucket", "_default_boundary_tick_bucket", "all_fields_for_sliding", "_default_all_fields_for_sliding", "partial_bucket_handling", "_default_partial_bucket_handling", "output_field_name", "_default_output_field_name", "group_by", "_default_group_by", "groups_to_display", "_default_groups_to_display", "bucket_end_per_group", "_default_bucket_end_per_group", "input_field_name", "_default_input_field_name", "biased", "_default_biased", "stack_info", "_used_strings"]

	class BucketIntervalUnits:
		DAYS = "DAYS"
		FLEXIBLE = "FLEXIBLE"
		MONTHS = "MONTHS"
		SECONDS = "SECONDS"
		TICKS = "TICKS"

	class OutputIntervalUnits:
		SECONDS = "SECONDS"
		TICKS = "TICKS"

	class BucketTime:
		BUCKET_END = "BUCKET_END"
		BUCKET_START = "BUCKET_START"

	class BoundaryTickBucket:
		NEW = "NEW"
		PREVIOUS = "PREVIOUS"

	class AllFieldsForSliding:
		WHEN_TICKS_EXIT_WINDOW = "WHEN_TICKS_EXIT_WINDOW"
		FALSE = "false"
		TRUE = "true"

	class PartialBucketHandling:
		AS_SEPARATE_BUCKET = "AS_SEPARATE_BUCKET"
		DISCARD = "DISCARD"
		MERGE_WITH_PREVIOUS = "MERGE_WITH_PREVIOUS"

	class GroupsToDisplay:
		ALL = "ALL"
		EVENT_IN_LAST_BUCKET = "EVENT_IN_LAST_BUCKET"

	def __init__(self, bucket_interval=0, bucket_interval_units=BucketIntervalUnits.SECONDS, output_interval="", output_interval_units=OutputIntervalUnits.SECONDS, is_running_aggr=False, bucket_time=BucketTime.BUCKET_END, bucket_end_criteria="", boundary_tick_bucket=BoundaryTickBucket.NEW, all_fields_for_sliding=AllFieldsForSliding.FALSE, partial_bucket_handling=PartialBucketHandling.AS_SEPARATE_BUCKET, output_field_name="VALUE", group_by="", groups_to_display=GroupsToDisplay.ALL, bucket_end_per_group=False, input_field_name="", biased=True, In = "", Out = ""):
		_graph_components.EpBase.__init__(self, "STDDEV")
		self._default_bucket_interval = 0
		self.bucket_interval = bucket_interval
		self._default_bucket_interval_units = type(self).BucketIntervalUnits.SECONDS
		self.bucket_interval_units = bucket_interval_units
		self._default_output_interval = ""
		self.output_interval = output_interval
		self._default_output_interval_units = type(self).OutputIntervalUnits.SECONDS
		self.output_interval_units = output_interval_units
		self._default_is_running_aggr = False
		self.is_running_aggr = is_running_aggr
		self._default_bucket_time = type(self).BucketTime.BUCKET_END
		self.bucket_time = bucket_time
		self._default_bucket_end_criteria = ""
		self.bucket_end_criteria = bucket_end_criteria
		self._default_boundary_tick_bucket = type(self).BoundaryTickBucket.NEW
		self.boundary_tick_bucket = boundary_tick_bucket
		self._default_all_fields_for_sliding = type(self).AllFieldsForSliding.FALSE
		self.all_fields_for_sliding = all_fields_for_sliding
		self._default_partial_bucket_handling = type(self).PartialBucketHandling.AS_SEPARATE_BUCKET
		self.partial_bucket_handling = partial_bucket_handling
		self._default_output_field_name = "VALUE"
		self.output_field_name = output_field_name
		self._default_group_by = ""
		self.group_by = group_by
		self._default_groups_to_display = type(self).GroupsToDisplay.ALL
		self.groups_to_display = groups_to_display
		self._default_bucket_end_per_group = False
		self.bucket_end_per_group = bucket_end_per_group
		self._default_input_field_name = ""
		self.input_field_name = input_field_name
		self._default_biased = True
		self.biased = biased
		if In != "":
			self.input_field_name=In
		if Out != "":
			self.output_field_name=Out
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_bucket_interval(self, value):
		self.bucket_interval = value
		return self

	def set_bucket_interval_units(self, value):
		self.bucket_interval_units = value
		return self

	def set_output_interval(self, value):
		self.output_interval = value
		return self

	def set_output_interval_units(self, value):
		self.output_interval_units = value
		return self

	def set_is_running_aggr(self, value):
		self.is_running_aggr = value
		return self

	def set_bucket_time(self, value):
		self.bucket_time = value
		return self

	def set_bucket_end_criteria(self, value):
		self.bucket_end_criteria = value
		return self

	def set_boundary_tick_bucket(self, value):
		self.boundary_tick_bucket = value
		return self

	def set_all_fields_for_sliding(self, value):
		self.all_fields_for_sliding = value
		return self

	def set_partial_bucket_handling(self, value):
		self.partial_bucket_handling = value
		return self

	def set_output_field_name(self, value):
		self.output_field_name = value
		return self

	def set_group_by(self, value):
		self.group_by = value
		return self

	def set_groups_to_display(self, value):
		self.groups_to_display = value
		return self

	def set_bucket_end_per_group(self, value):
		self.bucket_end_per_group = value
		return self

	def set_input_field_name(self, value):
		self.input_field_name = value
		return self

	def set_biased(self, value):
		self.biased = value
		return self

	@staticmethod
	def _get_name():
		return "STDDEV"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.bucket_interval != 0: 
			desc += "BUCKET_INTERVAL=" + py_to_str(self.bucket_interval) + ","
		if self.bucket_interval_units != self.BucketIntervalUnits.SECONDS: 
			desc += "BUCKET_INTERVAL_UNITS=" + py_to_str(self.bucket_interval_units) + ","
		if self.output_interval != "": 
			desc += "OUTPUT_INTERVAL=" + py_to_str(self.output_interval) + ","
		if self.output_interval_units != self.OutputIntervalUnits.SECONDS: 
			desc += "OUTPUT_INTERVAL_UNITS=" + py_to_str(self.output_interval_units) + ","
		if self.is_running_aggr != False: 
			desc += "IS_RUNNING_AGGR=" + py_to_str(self.is_running_aggr) + ","
		if self.bucket_time != self.BucketTime.BUCKET_END: 
			desc += "BUCKET_TIME=" + py_to_str(self.bucket_time) + ","
		if self.bucket_end_criteria != "": 
			desc += "BUCKET_END_CRITERIA=" + py_to_str(self.bucket_end_criteria) + ","
		if self.boundary_tick_bucket != self.BoundaryTickBucket.NEW: 
			desc += "BOUNDARY_TICK_BUCKET=" + py_to_str(self.boundary_tick_bucket) + ","
		if self.all_fields_for_sliding != self.AllFieldsForSliding.FALSE: 
			desc += "ALL_FIELDS_FOR_SLIDING=" + py_to_str(self.all_fields_for_sliding) + ","
		if self.partial_bucket_handling != self.PartialBucketHandling.AS_SEPARATE_BUCKET: 
			desc += "PARTIAL_BUCKET_HANDLING=" + py_to_str(self.partial_bucket_handling) + ","
		if self.output_field_name != "VALUE": 
			desc += "OUTPUT_FIELD_NAME=" + py_to_str(self.output_field_name) + ","
		if self.group_by != "": 
			desc += "GROUP_BY=" + py_to_str(self.group_by) + ","
		if self.groups_to_display != self.GroupsToDisplay.ALL: 
			desc += "GROUPS_TO_DISPLAY=" + py_to_str(self.groups_to_display) + ","
		if self.bucket_end_per_group != False: 
			desc += "BUCKET_END_PER_GROUP=" + py_to_str(self.bucket_end_per_group) + ","
		if self.input_field_name != "": 
			desc += "INPUT_FIELD_NAME=" + py_to_str(self.input_field_name) + ","
		if self.biased != True: 
			desc += "BIASED=" + py_to_str(self.biased) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class StandardizedMoment(_graph_components.EpBase):
	"""
		

STANDARDIZED_MOMENT

Type: Aggregation

Description: Computes the standardized statistical moment of
degree k of the INPUT_FIELD over the specified bucket interval.
The standardized moment of degree k is defined as the expected value of
the expression ((X - mean) / stddev)^k.

Python
class name:
StandardizedMoment

Input: A time series of ticks.

Output: A time series of ticks, one for each bucket.

Parameters: See parameters
common to generic aggregations.


  BUCKET_INTERVAL
(seconds/ticks)
  BUCKET_INTERVAL_UNITS
(enumerated type)
  OUTPUT_INTERVAL
(seconds)
  OUTPUT_INTERVAL_UNITS
(enumerated type)
  IS_RUNNING_AGGR
(Boolean)
  BUCKET_TIME
(Boolean)
  BUCKET_END_CRITERIA
(expression)
  BOUNDARY_TICK_BUCKET
(NEW/PREVIOUS)
  ALL_FIELDS_FOR_SLIDING
(Boolean)
  PARTIAL_BUCKET_HANDLING
(enumerated type)
  OUTPUT_FIELD_NAME
(string)
  GROUP_BY
(string)
  GROUPS_TO_DISPLAY
(enumerated)
  BUCKET_END_PER_GROUP
(Boolean)
  INPUT_FIELD_NAME
(string)
  DEGREE (Integer)
    The order (degree) of the standardized moment to compute,
denoted as k in the description above.
Default: 3

  

Examples: Calculate the 3rd standardized moment (skewness) of
PRICE over 1 hour buckets.

STANDARDIZED_MOMENT(3600,
SECONDS,,false,false,AS_SEPARATE_BUCKET,STANDARDIZED_MOMENT_PRICE,,PRICE,3)

See the STANDARDIZED_MOMENT
example in AGGREGATION_EXAMPLES.otq.


	"""
	class Parameters:
		bucket_interval = "BUCKET_INTERVAL"
		bucket_interval_units = "BUCKET_INTERVAL_UNITS"
		output_interval = "OUTPUT_INTERVAL"
		output_interval_units = "OUTPUT_INTERVAL_UNITS"
		is_running_aggr = "IS_RUNNING_AGGR"
		bucket_time = "BUCKET_TIME"
		bucket_end_criteria = "BUCKET_END_CRITERIA"
		boundary_tick_bucket = "BOUNDARY_TICK_BUCKET"
		all_fields_for_sliding = "ALL_FIELDS_FOR_SLIDING"
		partial_bucket_handling = "PARTIAL_BUCKET_HANDLING"
		output_field_name = "OUTPUT_FIELD_NAME"
		group_by = "GROUP_BY"
		groups_to_display = "GROUPS_TO_DISPLAY"
		bucket_end_per_group = "BUCKET_END_PER_GROUP"
		input_field_name = "INPUT_FIELD_NAME"
		degree = "DEGREE"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["bucket_interval", "bucket_interval_units", "output_interval", "output_interval_units", "is_running_aggr", "bucket_time", "bucket_end_criteria", "boundary_tick_bucket", "all_fields_for_sliding", "partial_bucket_handling", "output_field_name", "group_by", "groups_to_display", "bucket_end_per_group", "input_field_name", "degree"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["bucket_interval", "_default_bucket_interval", "bucket_interval_units", "_default_bucket_interval_units", "output_interval", "_default_output_interval", "output_interval_units", "_default_output_interval_units", "is_running_aggr", "_default_is_running_aggr", "bucket_time", "_default_bucket_time", "bucket_end_criteria", "_default_bucket_end_criteria", "boundary_tick_bucket", "_default_boundary_tick_bucket", "all_fields_for_sliding", "_default_all_fields_for_sliding", "partial_bucket_handling", "_default_partial_bucket_handling", "output_field_name", "_default_output_field_name", "group_by", "_default_group_by", "groups_to_display", "_default_groups_to_display", "bucket_end_per_group", "_default_bucket_end_per_group", "input_field_name", "_default_input_field_name", "degree", "_default_degree", "stack_info", "_used_strings"]

	class BucketIntervalUnits:
		DAYS = "DAYS"
		FLEXIBLE = "FLEXIBLE"
		MONTHS = "MONTHS"
		SECONDS = "SECONDS"
		TICKS = "TICKS"

	class OutputIntervalUnits:
		SECONDS = "SECONDS"
		TICKS = "TICKS"

	class BucketTime:
		BUCKET_END = "BUCKET_END"
		BUCKET_START = "BUCKET_START"

	class BoundaryTickBucket:
		NEW = "NEW"
		PREVIOUS = "PREVIOUS"

	class AllFieldsForSliding:
		WHEN_TICKS_EXIT_WINDOW = "WHEN_TICKS_EXIT_WINDOW"
		FALSE = "false"
		TRUE = "true"

	class PartialBucketHandling:
		AS_SEPARATE_BUCKET = "AS_SEPARATE_BUCKET"
		DISCARD = "DISCARD"
		MERGE_WITH_PREVIOUS = "MERGE_WITH_PREVIOUS"

	class GroupsToDisplay:
		ALL = "ALL"
		EVENT_IN_LAST_BUCKET = "EVENT_IN_LAST_BUCKET"

	def __init__(self, bucket_interval=0, bucket_interval_units=BucketIntervalUnits.SECONDS, output_interval="", output_interval_units=OutputIntervalUnits.SECONDS, is_running_aggr=False, bucket_time=BucketTime.BUCKET_END, bucket_end_criteria="", boundary_tick_bucket=BoundaryTickBucket.NEW, all_fields_for_sliding=AllFieldsForSliding.FALSE, partial_bucket_handling=PartialBucketHandling.AS_SEPARATE_BUCKET, output_field_name="VALUE", group_by="", groups_to_display=GroupsToDisplay.ALL, bucket_end_per_group=False, input_field_name="", degree=3, In = "", Out = ""):
		_graph_components.EpBase.__init__(self, "STANDARDIZED_MOMENT")
		self._default_bucket_interval = 0
		self.bucket_interval = bucket_interval
		self._default_bucket_interval_units = type(self).BucketIntervalUnits.SECONDS
		self.bucket_interval_units = bucket_interval_units
		self._default_output_interval = ""
		self.output_interval = output_interval
		self._default_output_interval_units = type(self).OutputIntervalUnits.SECONDS
		self.output_interval_units = output_interval_units
		self._default_is_running_aggr = False
		self.is_running_aggr = is_running_aggr
		self._default_bucket_time = type(self).BucketTime.BUCKET_END
		self.bucket_time = bucket_time
		self._default_bucket_end_criteria = ""
		self.bucket_end_criteria = bucket_end_criteria
		self._default_boundary_tick_bucket = type(self).BoundaryTickBucket.NEW
		self.boundary_tick_bucket = boundary_tick_bucket
		self._default_all_fields_for_sliding = type(self).AllFieldsForSliding.FALSE
		self.all_fields_for_sliding = all_fields_for_sliding
		self._default_partial_bucket_handling = type(self).PartialBucketHandling.AS_SEPARATE_BUCKET
		self.partial_bucket_handling = partial_bucket_handling
		self._default_output_field_name = "VALUE"
		self.output_field_name = output_field_name
		self._default_group_by = ""
		self.group_by = group_by
		self._default_groups_to_display = type(self).GroupsToDisplay.ALL
		self.groups_to_display = groups_to_display
		self._default_bucket_end_per_group = False
		self.bucket_end_per_group = bucket_end_per_group
		self._default_input_field_name = ""
		self.input_field_name = input_field_name
		self._default_degree = 3
		self.degree = degree
		if In != "":
			self.input_field_name=In
		if Out != "":
			self.output_field_name=Out
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_bucket_interval(self, value):
		self.bucket_interval = value
		return self

	def set_bucket_interval_units(self, value):
		self.bucket_interval_units = value
		return self

	def set_output_interval(self, value):
		self.output_interval = value
		return self

	def set_output_interval_units(self, value):
		self.output_interval_units = value
		return self

	def set_is_running_aggr(self, value):
		self.is_running_aggr = value
		return self

	def set_bucket_time(self, value):
		self.bucket_time = value
		return self

	def set_bucket_end_criteria(self, value):
		self.bucket_end_criteria = value
		return self

	def set_boundary_tick_bucket(self, value):
		self.boundary_tick_bucket = value
		return self

	def set_all_fields_for_sliding(self, value):
		self.all_fields_for_sliding = value
		return self

	def set_partial_bucket_handling(self, value):
		self.partial_bucket_handling = value
		return self

	def set_output_field_name(self, value):
		self.output_field_name = value
		return self

	def set_group_by(self, value):
		self.group_by = value
		return self

	def set_groups_to_display(self, value):
		self.groups_to_display = value
		return self

	def set_bucket_end_per_group(self, value):
		self.bucket_end_per_group = value
		return self

	def set_input_field_name(self, value):
		self.input_field_name = value
		return self

	def set_degree(self, value):
		self.degree = value
		return self

	@staticmethod
	def _get_name():
		return "STANDARDIZED_MOMENT"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.bucket_interval != 0: 
			desc += "BUCKET_INTERVAL=" + py_to_str(self.bucket_interval) + ","
		if self.bucket_interval_units != self.BucketIntervalUnits.SECONDS: 
			desc += "BUCKET_INTERVAL_UNITS=" + py_to_str(self.bucket_interval_units) + ","
		if self.output_interval != "": 
			desc += "OUTPUT_INTERVAL=" + py_to_str(self.output_interval) + ","
		if self.output_interval_units != self.OutputIntervalUnits.SECONDS: 
			desc += "OUTPUT_INTERVAL_UNITS=" + py_to_str(self.output_interval_units) + ","
		if self.is_running_aggr != False: 
			desc += "IS_RUNNING_AGGR=" + py_to_str(self.is_running_aggr) + ","
		if self.bucket_time != self.BucketTime.BUCKET_END: 
			desc += "BUCKET_TIME=" + py_to_str(self.bucket_time) + ","
		if self.bucket_end_criteria != "": 
			desc += "BUCKET_END_CRITERIA=" + py_to_str(self.bucket_end_criteria) + ","
		if self.boundary_tick_bucket != self.BoundaryTickBucket.NEW: 
			desc += "BOUNDARY_TICK_BUCKET=" + py_to_str(self.boundary_tick_bucket) + ","
		if self.all_fields_for_sliding != self.AllFieldsForSliding.FALSE: 
			desc += "ALL_FIELDS_FOR_SLIDING=" + py_to_str(self.all_fields_for_sliding) + ","
		if self.partial_bucket_handling != self.PartialBucketHandling.AS_SEPARATE_BUCKET: 
			desc += "PARTIAL_BUCKET_HANDLING=" + py_to_str(self.partial_bucket_handling) + ","
		if self.output_field_name != "VALUE": 
			desc += "OUTPUT_FIELD_NAME=" + py_to_str(self.output_field_name) + ","
		if self.group_by != "": 
			desc += "GROUP_BY=" + py_to_str(self.group_by) + ","
		if self.groups_to_display != self.GroupsToDisplay.ALL: 
			desc += "GROUPS_TO_DISPLAY=" + py_to_str(self.groups_to_display) + ","
		if self.bucket_end_per_group != False: 
			desc += "BUCKET_END_PER_GROUP=" + py_to_str(self.bucket_end_per_group) + ","
		if self.input_field_name != "": 
			desc += "INPUT_FIELD_NAME=" + py_to_str(self.input_field_name) + ","
		if self.degree != 3: 
			desc += "DEGREE=" + py_to_str(self.degree) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class Return(_graph_components.EpBase):
	"""
		

RETURN

Type: Aggregation

Description: Computes the ratio between the value of the
input field at the end of a bucket interval and the value of this field
at the start of a bucket interval. If IS_RUNNING_AGGR is set to true,
then RETURN is calculated as the ratio between latest tick and the
first tick from the start time. Below, the input field is referred to
as 'price'.

Return for an interval = ending price for the
interval / starting price for the interval

where


  ending price for the interval
is taken from the last tick observed in the stream up to the end of the
interval.
  starting price for the interval
is taken from the first tick in the interval for the first bucket
interval that has any ticks. For all other intervals, it is the price
of the first tick with timestamp of the start of interval or, if no
such tick is present, the ending price of the previous interval. The
ending price for the interval is taken from the last tick observed in
the stream up to the end of the interval.

Python
class name:&nbsp;Return

Input: A time series of ticks.

Output: A time series of ticks.

Parameters: See parameters
common to generic aggregations.


  BUCKET_INTERVAL
(seconds/ticks)
  BUCKET_INTERVAL_UNITS
(enumerated type)
  OUTPUT_INTERVAL
(seconds)
  OUTPUT_INTERVAL_UNITS
(enumerated type)
  IS_RUNNING_AGGR
(Boolean)
  BUCKET_TIME
(Boolean)
  BUCKET_END_CRITERIA
(expression)
  BOUNDARY_TICK_BUCKET
(NEW/PREVIOUS)
  ALL_FIELDS_FOR_SLIDING
(Boolean)
  PARTIAL_BUCKET_HANDLING
(enumerated type)
  OUTPUT_FIELD_NAME
(string)
  GROUP_BY
(string)
  GROUPS_TO_DISPLAY
(enumerated)
  BUCKET_END_PER_GROUP
(Boolean)
  INPUT_FIELD_NAME
(string)

Notes: See the notes on generic
aggregations.

Examples: Breaks ticks into 10-minute buckets and computes
the return in the end of each bucket:

RETURN (600, SECONDS, , false, false,
AS_SEPARATE_BUCKET, RETURN, PRICE, , )

See the RETURN example in AGGREGATION_EXAMPLES.otq.


	"""
	class Parameters:
		bucket_interval = "BUCKET_INTERVAL"
		bucket_interval_units = "BUCKET_INTERVAL_UNITS"
		output_interval = "OUTPUT_INTERVAL"
		output_interval_units = "OUTPUT_INTERVAL_UNITS"
		is_running_aggr = "IS_RUNNING_AGGR"
		bucket_time = "BUCKET_TIME"
		bucket_end_criteria = "BUCKET_END_CRITERIA"
		boundary_tick_bucket = "BOUNDARY_TICK_BUCKET"
		all_fields_for_sliding = "ALL_FIELDS_FOR_SLIDING"
		partial_bucket_handling = "PARTIAL_BUCKET_HANDLING"
		output_field_name = "OUTPUT_FIELD_NAME"
		group_by = "GROUP_BY"
		groups_to_display = "GROUPS_TO_DISPLAY"
		bucket_end_per_group = "BUCKET_END_PER_GROUP"
		input_field_name = "INPUT_FIELD_NAME"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["bucket_interval", "bucket_interval_units", "output_interval", "output_interval_units", "is_running_aggr", "bucket_time", "bucket_end_criteria", "boundary_tick_bucket", "all_fields_for_sliding", "partial_bucket_handling", "output_field_name", "group_by", "groups_to_display", "bucket_end_per_group", "input_field_name"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["bucket_interval", "_default_bucket_interval", "bucket_interval_units", "_default_bucket_interval_units", "output_interval", "_default_output_interval", "output_interval_units", "_default_output_interval_units", "is_running_aggr", "_default_is_running_aggr", "bucket_time", "_default_bucket_time", "bucket_end_criteria", "_default_bucket_end_criteria", "boundary_tick_bucket", "_default_boundary_tick_bucket", "all_fields_for_sliding", "_default_all_fields_for_sliding", "partial_bucket_handling", "_default_partial_bucket_handling", "output_field_name", "_default_output_field_name", "group_by", "_default_group_by", "groups_to_display", "_default_groups_to_display", "bucket_end_per_group", "_default_bucket_end_per_group", "input_field_name", "_default_input_field_name", "stack_info", "_used_strings"]

	class BucketIntervalUnits:
		DAYS = "DAYS"
		FLEXIBLE = "FLEXIBLE"
		MONTHS = "MONTHS"
		SECONDS = "SECONDS"
		TICKS = "TICKS"

	class OutputIntervalUnits:
		SECONDS = "SECONDS"
		TICKS = "TICKS"

	class BucketTime:
		BUCKET_END = "BUCKET_END"
		BUCKET_START = "BUCKET_START"

	class BoundaryTickBucket:
		NEW = "NEW"
		PREVIOUS = "PREVIOUS"

	class AllFieldsForSliding:
		WHEN_TICKS_EXIT_WINDOW = "WHEN_TICKS_EXIT_WINDOW"
		FALSE = "false"
		TRUE = "true"

	class PartialBucketHandling:
		AS_SEPARATE_BUCKET = "AS_SEPARATE_BUCKET"
		DISCARD = "DISCARD"
		MERGE_WITH_PREVIOUS = "MERGE_WITH_PREVIOUS"

	class GroupsToDisplay:
		ALL = "ALL"
		EVENT_IN_LAST_BUCKET = "EVENT_IN_LAST_BUCKET"

	def __init__(self, bucket_interval=0, bucket_interval_units=BucketIntervalUnits.SECONDS, output_interval="", output_interval_units=OutputIntervalUnits.SECONDS, is_running_aggr=False, bucket_time=BucketTime.BUCKET_END, bucket_end_criteria="", boundary_tick_bucket=BoundaryTickBucket.NEW, all_fields_for_sliding=AllFieldsForSliding.FALSE, partial_bucket_handling=PartialBucketHandling.AS_SEPARATE_BUCKET, output_field_name="VALUE", group_by="", groups_to_display=GroupsToDisplay.ALL, bucket_end_per_group=False, input_field_name="", In = "", Out = ""):
		_graph_components.EpBase.__init__(self, "RETURN")
		self._default_bucket_interval = 0
		self.bucket_interval = bucket_interval
		self._default_bucket_interval_units = type(self).BucketIntervalUnits.SECONDS
		self.bucket_interval_units = bucket_interval_units
		self._default_output_interval = ""
		self.output_interval = output_interval
		self._default_output_interval_units = type(self).OutputIntervalUnits.SECONDS
		self.output_interval_units = output_interval_units
		self._default_is_running_aggr = False
		self.is_running_aggr = is_running_aggr
		self._default_bucket_time = type(self).BucketTime.BUCKET_END
		self.bucket_time = bucket_time
		self._default_bucket_end_criteria = ""
		self.bucket_end_criteria = bucket_end_criteria
		self._default_boundary_tick_bucket = type(self).BoundaryTickBucket.NEW
		self.boundary_tick_bucket = boundary_tick_bucket
		self._default_all_fields_for_sliding = type(self).AllFieldsForSliding.FALSE
		self.all_fields_for_sliding = all_fields_for_sliding
		self._default_partial_bucket_handling = type(self).PartialBucketHandling.AS_SEPARATE_BUCKET
		self.partial_bucket_handling = partial_bucket_handling
		self._default_output_field_name = "VALUE"
		self.output_field_name = output_field_name
		self._default_group_by = ""
		self.group_by = group_by
		self._default_groups_to_display = type(self).GroupsToDisplay.ALL
		self.groups_to_display = groups_to_display
		self._default_bucket_end_per_group = False
		self.bucket_end_per_group = bucket_end_per_group
		self._default_input_field_name = ""
		self.input_field_name = input_field_name
		if In != "":
			self.input_field_name=In
		if Out != "":
			self.output_field_name=Out
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_bucket_interval(self, value):
		self.bucket_interval = value
		return self

	def set_bucket_interval_units(self, value):
		self.bucket_interval_units = value
		return self

	def set_output_interval(self, value):
		self.output_interval = value
		return self

	def set_output_interval_units(self, value):
		self.output_interval_units = value
		return self

	def set_is_running_aggr(self, value):
		self.is_running_aggr = value
		return self

	def set_bucket_time(self, value):
		self.bucket_time = value
		return self

	def set_bucket_end_criteria(self, value):
		self.bucket_end_criteria = value
		return self

	def set_boundary_tick_bucket(self, value):
		self.boundary_tick_bucket = value
		return self

	def set_all_fields_for_sliding(self, value):
		self.all_fields_for_sliding = value
		return self

	def set_partial_bucket_handling(self, value):
		self.partial_bucket_handling = value
		return self

	def set_output_field_name(self, value):
		self.output_field_name = value
		return self

	def set_group_by(self, value):
		self.group_by = value
		return self

	def set_groups_to_display(self, value):
		self.groups_to_display = value
		return self

	def set_bucket_end_per_group(self, value):
		self.bucket_end_per_group = value
		return self

	def set_input_field_name(self, value):
		self.input_field_name = value
		return self

	@staticmethod
	def _get_name():
		return "RETURN"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.bucket_interval != 0: 
			desc += "BUCKET_INTERVAL=" + py_to_str(self.bucket_interval) + ","
		if self.bucket_interval_units != self.BucketIntervalUnits.SECONDS: 
			desc += "BUCKET_INTERVAL_UNITS=" + py_to_str(self.bucket_interval_units) + ","
		if self.output_interval != "": 
			desc += "OUTPUT_INTERVAL=" + py_to_str(self.output_interval) + ","
		if self.output_interval_units != self.OutputIntervalUnits.SECONDS: 
			desc += "OUTPUT_INTERVAL_UNITS=" + py_to_str(self.output_interval_units) + ","
		if self.is_running_aggr != False: 
			desc += "IS_RUNNING_AGGR=" + py_to_str(self.is_running_aggr) + ","
		if self.bucket_time != self.BucketTime.BUCKET_END: 
			desc += "BUCKET_TIME=" + py_to_str(self.bucket_time) + ","
		if self.bucket_end_criteria != "": 
			desc += "BUCKET_END_CRITERIA=" + py_to_str(self.bucket_end_criteria) + ","
		if self.boundary_tick_bucket != self.BoundaryTickBucket.NEW: 
			desc += "BOUNDARY_TICK_BUCKET=" + py_to_str(self.boundary_tick_bucket) + ","
		if self.all_fields_for_sliding != self.AllFieldsForSliding.FALSE: 
			desc += "ALL_FIELDS_FOR_SLIDING=" + py_to_str(self.all_fields_for_sliding) + ","
		if self.partial_bucket_handling != self.PartialBucketHandling.AS_SEPARATE_BUCKET: 
			desc += "PARTIAL_BUCKET_HANDLING=" + py_to_str(self.partial_bucket_handling) + ","
		if self.output_field_name != "VALUE": 
			desc += "OUTPUT_FIELD_NAME=" + py_to_str(self.output_field_name) + ","
		if self.group_by != "": 
			desc += "GROUP_BY=" + py_to_str(self.group_by) + ","
		if self.groups_to_display != self.GroupsToDisplay.ALL: 
			desc += "GROUPS_TO_DISPLAY=" + py_to_str(self.groups_to_display) + ","
		if self.bucket_end_per_group != False: 
			desc += "BUCKET_END_PER_GROUP=" + py_to_str(self.bucket_end_per_group) + ","
		if self.input_field_name != "": 
			desc += "INPUT_FIELD_NAME=" + py_to_str(self.input_field_name) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class PartitionEvenlyIntoGroups(_graph_components.EpBase):
	"""
		

PARTITION_EVENLY_INTO_GROUPS

Type: Aggregation

Description: For each bucket, this EP breaks ticks into the
specified number of groups (NUMBER_OF_GROUPS) by the specified field
(FIELD_TO_PARTITION) in a way that the sums of the specified weight
fields (WEIGHT_FIELD) in each group are as close as possible.

Python
class name:
PartitionEvenlyIntoGroups

Input: A time series of ticks

Output: A time series of ticks

Parameters: See parameters
common to generic aggregations.


  BUCKET_INTERVAL
(seconds/ticks)
  BUCKET_INTERVAL_UNITS
(enumerated type)
  OUTPUT_INTERVAL
(seconds)
  OUTPUT_INTERVAL_UNITS
(enumerated type)
  IS_RUNNING_AGGR
(Boolean)
  BUCKET_TIME
(Boolean)
  BUCKET_END_CRITERIA
(expression)
  BOUNDARY_TICK_BUCKET
(NEW/PREVIOUS)
  PARTIAL_BUCKET_HANDLING
(enumerated type)
  FIELD_TO_PARTITION (string)
    Specifies the tick field that will be partitioned.

  
  WEIGHT_FIELD (string)
    Specifies the tick field, the values of which are evaluated as
weight; and then, the partitioning is be applied, so that the total
weight of the groups are as close as possible.

  
  NUMBER_OF_GROUPS (integer)
    Specifies the target number of partitions to which the tick
should be divided.

  

Example:

Assuming that we are receiving the following feed:

SYMBOL1, SIZE=10, TYPE="A", OTHER_FIELDS...SYMBOL1, SIZE=20, TYPE="B", OTHER_FIELDS...SYMBOL1, SIZE=30, TYPE="C", OTHER_FIELDS...SYMBOL1, SIZE=35, TYPE="D", OTHER_FIELDS...SYMBOL1, SIZE=4, TYPE="E", OTHER_FIELDS...
When we execute the following:

PARTITION_EVENLY_INTO_GROUPS(WEIGHT_FIELD=SIZE,FIELD_TO_PARTITION=TYPE,NUMBER_OF_GROUPS=2)

we get the following two groups:


  The first group (with the calculated weight of 50):
    FIELD_TO_PARTITION="B", GROUP_ID=0FIELD_TO_PARTITION="C", GROUP_ID=0
  
  And the second group (with the calculated weight of 49):
    FIELD_TO_PARTITION="A", GROUP_ID=1FIELD_TO_PARTITION="D", GROUP_ID=1FIELD_TO_PARTITION="E", GROUP_ID=1
  


	"""
	class Parameters:
		bucket_interval = "BUCKET_INTERVAL"
		bucket_interval_units = "BUCKET_INTERVAL_UNITS"
		output_interval = "OUTPUT_INTERVAL"
		output_interval_units = "OUTPUT_INTERVAL_UNITS"
		is_running_aggr = "IS_RUNNING_AGGR"
		bucket_time = "BUCKET_TIME"
		bucket_end_criteria = "BUCKET_END_CRITERIA"
		boundary_tick_bucket = "BOUNDARY_TICK_BUCKET"
		partial_bucket_handling = "PARTIAL_BUCKET_HANDLING"
		field_to_partition = "FIELD_TO_PARTITION"
		weight_field = "WEIGHT_FIELD"
		number_of_groups = "NUMBER_OF_GROUPS"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["bucket_interval", "bucket_interval_units", "output_interval", "output_interval_units", "is_running_aggr", "bucket_time", "bucket_end_criteria", "boundary_tick_bucket", "partial_bucket_handling", "field_to_partition", "weight_field", "number_of_groups"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["bucket_interval", "_default_bucket_interval", "bucket_interval_units", "_default_bucket_interval_units", "output_interval", "_default_output_interval", "output_interval_units", "_default_output_interval_units", "is_running_aggr", "_default_is_running_aggr", "bucket_time", "_default_bucket_time", "bucket_end_criteria", "_default_bucket_end_criteria", "boundary_tick_bucket", "_default_boundary_tick_bucket", "partial_bucket_handling", "_default_partial_bucket_handling", "field_to_partition", "_default_field_to_partition", "weight_field", "_default_weight_field", "number_of_groups", "_default_number_of_groups", "stack_info", "_used_strings"]

	class BucketIntervalUnits:
		DAYS = "DAYS"
		FLEXIBLE = "FLEXIBLE"
		MONTHS = "MONTHS"
		SECONDS = "SECONDS"
		TICKS = "TICKS"

	class OutputIntervalUnits:
		SECONDS = "SECONDS"
		TICKS = "TICKS"

	class BucketTime:
		BUCKET_END = "BUCKET_END"
		BUCKET_START = "BUCKET_START"

	class BoundaryTickBucket:
		NEW = "NEW"
		PREVIOUS = "PREVIOUS"

	class PartialBucketHandling:
		AS_SEPARATE_BUCKET = "AS_SEPARATE_BUCKET"
		DISCARD = "DISCARD"
		MERGE_WITH_PREVIOUS = "MERGE_WITH_PREVIOUS"

	def __init__(self, bucket_interval=0, bucket_interval_units=BucketIntervalUnits.SECONDS, output_interval="", output_interval_units=OutputIntervalUnits.SECONDS, is_running_aggr=False, bucket_time=BucketTime.BUCKET_END, bucket_end_criteria="", boundary_tick_bucket=BoundaryTickBucket.NEW, partial_bucket_handling=PartialBucketHandling.AS_SEPARATE_BUCKET, field_to_partition="", weight_field="", number_of_groups=""):
		_graph_components.EpBase.__init__(self, "PARTITION_EVENLY_INTO_GROUPS")
		self._default_bucket_interval = 0
		self.bucket_interval = bucket_interval
		self._default_bucket_interval_units = type(self).BucketIntervalUnits.SECONDS
		self.bucket_interval_units = bucket_interval_units
		self._default_output_interval = ""
		self.output_interval = output_interval
		self._default_output_interval_units = type(self).OutputIntervalUnits.SECONDS
		self.output_interval_units = output_interval_units
		self._default_is_running_aggr = False
		self.is_running_aggr = is_running_aggr
		self._default_bucket_time = type(self).BucketTime.BUCKET_END
		self.bucket_time = bucket_time
		self._default_bucket_end_criteria = ""
		self.bucket_end_criteria = bucket_end_criteria
		self._default_boundary_tick_bucket = type(self).BoundaryTickBucket.NEW
		self.boundary_tick_bucket = boundary_tick_bucket
		self._default_partial_bucket_handling = type(self).PartialBucketHandling.AS_SEPARATE_BUCKET
		self.partial_bucket_handling = partial_bucket_handling
		self._default_field_to_partition = ""
		self.field_to_partition = field_to_partition
		self._default_weight_field = ""
		self.weight_field = weight_field
		self._default_number_of_groups = ""
		self.number_of_groups = number_of_groups
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_bucket_interval(self, value):
		self.bucket_interval = value
		return self

	def set_bucket_interval_units(self, value):
		self.bucket_interval_units = value
		return self

	def set_output_interval(self, value):
		self.output_interval = value
		return self

	def set_output_interval_units(self, value):
		self.output_interval_units = value
		return self

	def set_is_running_aggr(self, value):
		self.is_running_aggr = value
		return self

	def set_bucket_time(self, value):
		self.bucket_time = value
		return self

	def set_bucket_end_criteria(self, value):
		self.bucket_end_criteria = value
		return self

	def set_boundary_tick_bucket(self, value):
		self.boundary_tick_bucket = value
		return self

	def set_partial_bucket_handling(self, value):
		self.partial_bucket_handling = value
		return self

	def set_field_to_partition(self, value):
		self.field_to_partition = value
		return self

	def set_weight_field(self, value):
		self.weight_field = value
		return self

	def set_number_of_groups(self, value):
		self.number_of_groups = value
		return self

	@staticmethod
	def _get_name():
		return "PARTITION_EVENLY_INTO_GROUPS"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.bucket_interval != 0: 
			desc += "BUCKET_INTERVAL=" + py_to_str(self.bucket_interval) + ","
		if self.bucket_interval_units != self.BucketIntervalUnits.SECONDS: 
			desc += "BUCKET_INTERVAL_UNITS=" + py_to_str(self.bucket_interval_units) + ","
		if self.output_interval != "": 
			desc += "OUTPUT_INTERVAL=" + py_to_str(self.output_interval) + ","
		if self.output_interval_units != self.OutputIntervalUnits.SECONDS: 
			desc += "OUTPUT_INTERVAL_UNITS=" + py_to_str(self.output_interval_units) + ","
		if self.is_running_aggr != False: 
			desc += "IS_RUNNING_AGGR=" + py_to_str(self.is_running_aggr) + ","
		if self.bucket_time != self.BucketTime.BUCKET_END: 
			desc += "BUCKET_TIME=" + py_to_str(self.bucket_time) + ","
		if self.bucket_end_criteria != "": 
			desc += "BUCKET_END_CRITERIA=" + py_to_str(self.bucket_end_criteria) + ","
		if self.boundary_tick_bucket != self.BoundaryTickBucket.NEW: 
			desc += "BOUNDARY_TICK_BUCKET=" + py_to_str(self.boundary_tick_bucket) + ","
		if self.partial_bucket_handling != self.PartialBucketHandling.AS_SEPARATE_BUCKET: 
			desc += "PARTIAL_BUCKET_HANDLING=" + py_to_str(self.partial_bucket_handling) + ","
		if self.field_to_partition != "": 
			desc += "FIELD_TO_PARTITION=" + py_to_str(self.field_to_partition) + ","
		if self.weight_field != "": 
			desc += "WEIGHT_FIELD=" + py_to_str(self.weight_field) + ","
		if self.number_of_groups != "": 
			desc += "NUMBER_OF_GROUPS=" + py_to_str(self.number_of_groups) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class PrevailingPrice(_graph_components.EpBase):
	"""
		

PREVAILING_PRICE

Type: Aggregation

Description: Computes prevailing security price using closing
price from the prior business day, if provided, as well as current
trade and quote prices.

Closing price is expected to come from a symbol parameter CLOSING_PRICE. Note that in order to be
valid, this price must be adjusted for corporate actions that happened
between corresponding close and start time of the query.

If the previous day's closing price is set, it is considered to be
the prevailing price at the start time of the query.

This EP allows one or two time series as its input. If two time
series are at its input, one of them must be of tick type QTE and must
contain fields ASK_PRICE and BID_PRICE. The other (or the only) time
series must contain field PRICE and is considered to be a trade time
series.

If a time series of trades is the only input to PREVAILING_PRICE,
then the latest trade price is considered to be the prevailing price.

If a time series of quotes is the input to the event processor, then
the latest mid-quote price is considered to be the prevailing price.

If both trade and quote time series are the inputs of the event
processor, then the latest mid-quote price is considered the prevailing
price until the first trade tick arrives. After the trade tick arrives,
the price of the latest trade tick is considered the prevailing price.

Python
class name:
PrevailingPrice

Input: Time series of trade ticks and/or time series of quote
ticks.

Output: A time series of ticks.

Parameters: See parameters
common to generic aggregations.


  BUCKET_INTERVAL
(seconds/ticks)
  BUCKET_INTERVAL_UNITS
(enumerated type)
  OUTPUT_INTERVAL
(seconds)
  OUTPUT_INTERVAL_UNITS
(SECONDS/TICKS)
  IS_RUNNING_AGGR
(Boolean)
  BUCKET_TIME
(Boolean)
  BUCKET_END_CRITERIA
(expression)
  BOUNDARY_TICK_BUCKET
(NEW/PREVIOUS)
  PARTIAL_BUCKET_HANDLING
(enumerated type)
  OUTPUT_FIELD_NAME
(string)
  INPUT_FIELD_NAME
(string)
  BACKFILL_FLAG (Boolean)
    If set to true, the prevailing price at the query start time is
assumed to be the price of the very first tick in this input time
series. For example, if the first tick for a given security arrived at
9:45:00 am, and the query is done between 9:30:00 am and 4:00:00 pm,
the price of the security between 9:30:00 am and 9:45:00 am is assumed
to be equal to the price of the tick that arrived at 9:45 am.
Default: false

  

Examples: Compute the portfolio price with 1 minute buckets
for the portfolio files specified as input symbols and symbol date for
all the symbols in the portfolio files set to 20050103. Use PREVAILING_PRICE
to supply the value of price before the arrival of the first tick for
each input time series.

CSV_FILE_LISTING()QUERY_SYMBOLS (20050103)PREVAILING_PRICE(0, SECONDS, , true, AS_SEPARATE_BUCKET, VALUE, , true)PORTFOLIO_PRICE(60,false,AS_SEPARATE_BUCKET,BOTH
See also CSV_FILE_LISTING and QUERY_SYMBOLS.

See the PREVAILING_PRICE example
in AGGREGATION_COMPUTE_EXAMPLES.otq.


	"""
	class Parameters:
		bucket_interval = "BUCKET_INTERVAL"
		bucket_interval_units = "BUCKET_INTERVAL_UNITS"
		output_interval = "OUTPUT_INTERVAL"
		output_interval_units = "OUTPUT_INTERVAL_UNITS"
		is_running_aggr = "IS_RUNNING_AGGR"
		bucket_time = "BUCKET_TIME"
		bucket_end_criteria = "BUCKET_END_CRITERIA"
		boundary_tick_bucket = "BOUNDARY_TICK_BUCKET"
		partial_bucket_handling = "PARTIAL_BUCKET_HANDLING"
		output_field_name = "OUTPUT_FIELD_NAME"
		input_field_name = "INPUT_FIELD_NAME"
		backfill_flag = "BACKFILL_FLAG"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["bucket_interval", "bucket_interval_units", "output_interval", "output_interval_units", "is_running_aggr", "bucket_time", "bucket_end_criteria", "boundary_tick_bucket", "partial_bucket_handling", "output_field_name", "input_field_name", "backfill_flag"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["bucket_interval", "_default_bucket_interval", "bucket_interval_units", "_default_bucket_interval_units", "output_interval", "_default_output_interval", "output_interval_units", "_default_output_interval_units", "is_running_aggr", "_default_is_running_aggr", "bucket_time", "_default_bucket_time", "bucket_end_criteria", "_default_bucket_end_criteria", "boundary_tick_bucket", "_default_boundary_tick_bucket", "partial_bucket_handling", "_default_partial_bucket_handling", "output_field_name", "_default_output_field_name", "input_field_name", "_default_input_field_name", "backfill_flag", "_default_backfill_flag", "stack_info", "_used_strings"]

	class BucketIntervalUnits:
		DAYS = "DAYS"
		FLEXIBLE = "FLEXIBLE"
		MONTHS = "MONTHS"
		SECONDS = "SECONDS"
		TICKS = "TICKS"

	class OutputIntervalUnits:
		SECONDS = "SECONDS"
		TICKS = "TICKS"

	class BucketTime:
		BUCKET_END = "BUCKET_END"
		BUCKET_START = "BUCKET_START"

	class BoundaryTickBucket:
		NEW = "NEW"
		PREVIOUS = "PREVIOUS"

	class PartialBucketHandling:
		AS_SEPARATE_BUCKET = "AS_SEPARATE_BUCKET"
		DISCARD = "DISCARD"
		MERGE_WITH_PREVIOUS = "MERGE_WITH_PREVIOUS"

	def __init__(self, bucket_interval=0, bucket_interval_units=BucketIntervalUnits.SECONDS, output_interval="", output_interval_units=OutputIntervalUnits.SECONDS, is_running_aggr=False, bucket_time=BucketTime.BUCKET_END, bucket_end_criteria="", boundary_tick_bucket=BoundaryTickBucket.NEW, partial_bucket_handling=PartialBucketHandling.AS_SEPARATE_BUCKET, output_field_name="VALUE", input_field_name="", backfill_flag=True, In = "", Out = ""):
		_graph_components.EpBase.__init__(self, "PREVAILING_PRICE")
		self._default_bucket_interval = 0
		self.bucket_interval = bucket_interval
		self._default_bucket_interval_units = type(self).BucketIntervalUnits.SECONDS
		self.bucket_interval_units = bucket_interval_units
		self._default_output_interval = ""
		self.output_interval = output_interval
		self._default_output_interval_units = type(self).OutputIntervalUnits.SECONDS
		self.output_interval_units = output_interval_units
		self._default_is_running_aggr = False
		self.is_running_aggr = is_running_aggr
		self._default_bucket_time = type(self).BucketTime.BUCKET_END
		self.bucket_time = bucket_time
		self._default_bucket_end_criteria = ""
		self.bucket_end_criteria = bucket_end_criteria
		self._default_boundary_tick_bucket = type(self).BoundaryTickBucket.NEW
		self.boundary_tick_bucket = boundary_tick_bucket
		self._default_partial_bucket_handling = type(self).PartialBucketHandling.AS_SEPARATE_BUCKET
		self.partial_bucket_handling = partial_bucket_handling
		self._default_output_field_name = "VALUE"
		self.output_field_name = output_field_name
		self._default_input_field_name = ""
		self.input_field_name = input_field_name
		self._default_backfill_flag = True
		self.backfill_flag = backfill_flag
		if In != "":
			self.input_field_name=In
		if Out != "":
			self.output_field_name=Out
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_bucket_interval(self, value):
		self.bucket_interval = value
		return self

	def set_bucket_interval_units(self, value):
		self.bucket_interval_units = value
		return self

	def set_output_interval(self, value):
		self.output_interval = value
		return self

	def set_output_interval_units(self, value):
		self.output_interval_units = value
		return self

	def set_is_running_aggr(self, value):
		self.is_running_aggr = value
		return self

	def set_bucket_time(self, value):
		self.bucket_time = value
		return self

	def set_bucket_end_criteria(self, value):
		self.bucket_end_criteria = value
		return self

	def set_boundary_tick_bucket(self, value):
		self.boundary_tick_bucket = value
		return self

	def set_partial_bucket_handling(self, value):
		self.partial_bucket_handling = value
		return self

	def set_output_field_name(self, value):
		self.output_field_name = value
		return self

	def set_input_field_name(self, value):
		self.input_field_name = value
		return self

	def set_backfill_flag(self, value):
		self.backfill_flag = value
		return self

	@staticmethod
	def _get_name():
		return "PREVAILING_PRICE"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.bucket_interval != 0: 
			desc += "BUCKET_INTERVAL=" + py_to_str(self.bucket_interval) + ","
		if self.bucket_interval_units != self.BucketIntervalUnits.SECONDS: 
			desc += "BUCKET_INTERVAL_UNITS=" + py_to_str(self.bucket_interval_units) + ","
		if self.output_interval != "": 
			desc += "OUTPUT_INTERVAL=" + py_to_str(self.output_interval) + ","
		if self.output_interval_units != self.OutputIntervalUnits.SECONDS: 
			desc += "OUTPUT_INTERVAL_UNITS=" + py_to_str(self.output_interval_units) + ","
		if self.is_running_aggr != False: 
			desc += "IS_RUNNING_AGGR=" + py_to_str(self.is_running_aggr) + ","
		if self.bucket_time != self.BucketTime.BUCKET_END: 
			desc += "BUCKET_TIME=" + py_to_str(self.bucket_time) + ","
		if self.bucket_end_criteria != "": 
			desc += "BUCKET_END_CRITERIA=" + py_to_str(self.bucket_end_criteria) + ","
		if self.boundary_tick_bucket != self.BoundaryTickBucket.NEW: 
			desc += "BOUNDARY_TICK_BUCKET=" + py_to_str(self.boundary_tick_bucket) + ","
		if self.partial_bucket_handling != self.PartialBucketHandling.AS_SEPARATE_BUCKET: 
			desc += "PARTIAL_BUCKET_HANDLING=" + py_to_str(self.partial_bucket_handling) + ","
		if self.output_field_name != "VALUE": 
			desc += "OUTPUT_FIELD_NAME=" + py_to_str(self.output_field_name) + ","
		if self.input_field_name != "": 
			desc += "INPUT_FIELD_NAME=" + py_to_str(self.input_field_name) + ","
		if self.backfill_flag != True: 
			desc += "BACKFILL_FLAG=" + py_to_str(self.backfill_flag) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class TwAverage(_graph_components.EpBase):
	"""
		

TW_AVERAGE

Type: Aggregation

Description: For each bucket, computes the time-weighted
average value of a specified numeric attribute.

Use the following formula to compute TW_AVERAGE:

sum( value * (next_timestamp - current_timestamp)) /
sum(next_timestamp - current_timestamp)

where:


value = the specific numeric attribute

current_timestamp = the timestamp of the tick carrying
non-NaN value

next_timestamp = the timestamp of the next tick or the end of
interval


In other words, each value is weighted by the time it was "active"
before being overtaken by the next value.

Only non-NaN values are used in the computation. If value NaN
persists during full bucket, computed value of time-weighted average
will be NaN

Python
class name:&nbsp;TwAverage

Input: A time series of ticks.

Output: A time series of ticks.

Parameters: See section parameters
common to generic aggregations.


  BUCKET_INTERVAL
(seconds/ticks)
  BUCKET_INTERVAL_UNITS
(enumerated type)
  OUTPUT_INTERVAL
(seconds)
  OUTPUT_INTERVAL_UNITS
(enumerated type)
  IS_RUNNING_AGGR
(Boolean)
  BUCKET_TIME
(Boolean)
  BUCKET_END_CRITERIA
(expression)
  BOUNDARY_TICK_BUCKET
(NEW/PREVIOUS)
  ALL_FIELDS_FOR_SLIDING
(Boolean)
  PARTIAL_BUCKET_HANDLING
(enumerated type)
  OUTPUT_FIELD_NAME
(string)
  GROUP_BY
(string)
  GROUPS_TO_DISPLAY
(enumerated)
  BUCKET_END_PER_GROUP
(Boolean)
  INPUT_FIELD_NAME
(string)
  TIME_SERIES_TYPE (enumerated type)
    
      If set to STATE_TS, the value from the input field
from the latest tick before the current bucket will be used as the
initial value of the current bucket (unless the current bucket has a
tick at its start time) in the computation of time-weighted average.
      If set to EVENT_TS, only ticks from the current
bucket will be used in the computation of time-weighted average.
    
    Default: STATE_TS

  

Notes: See the notes on generic
aggregations.

Examples:

See the TW_AVERAGE example in AGGREGATION_EXAMPLES.otq.


	"""
	class Parameters:
		bucket_interval = "BUCKET_INTERVAL"
		bucket_interval_units = "BUCKET_INTERVAL_UNITS"
		output_interval = "OUTPUT_INTERVAL"
		output_interval_units = "OUTPUT_INTERVAL_UNITS"
		is_running_aggr = "IS_RUNNING_AGGR"
		bucket_time = "BUCKET_TIME"
		bucket_end_criteria = "BUCKET_END_CRITERIA"
		boundary_tick_bucket = "BOUNDARY_TICK_BUCKET"
		all_fields_for_sliding = "ALL_FIELDS_FOR_SLIDING"
		partial_bucket_handling = "PARTIAL_BUCKET_HANDLING"
		output_field_name = "OUTPUT_FIELD_NAME"
		group_by = "GROUP_BY"
		groups_to_display = "GROUPS_TO_DISPLAY"
		bucket_end_per_group = "BUCKET_END_PER_GROUP"
		input_field_name = "INPUT_FIELD_NAME"
		time_series_type = "TIME_SERIES_TYPE"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["bucket_interval", "bucket_interval_units", "output_interval", "output_interval_units", "is_running_aggr", "bucket_time", "bucket_end_criteria", "boundary_tick_bucket", "all_fields_for_sliding", "partial_bucket_handling", "output_field_name", "group_by", "groups_to_display", "bucket_end_per_group", "input_field_name", "time_series_type"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["bucket_interval", "_default_bucket_interval", "bucket_interval_units", "_default_bucket_interval_units", "output_interval", "_default_output_interval", "output_interval_units", "_default_output_interval_units", "is_running_aggr", "_default_is_running_aggr", "bucket_time", "_default_bucket_time", "bucket_end_criteria", "_default_bucket_end_criteria", "boundary_tick_bucket", "_default_boundary_tick_bucket", "all_fields_for_sliding", "_default_all_fields_for_sliding", "partial_bucket_handling", "_default_partial_bucket_handling", "output_field_name", "_default_output_field_name", "group_by", "_default_group_by", "groups_to_display", "_default_groups_to_display", "bucket_end_per_group", "_default_bucket_end_per_group", "input_field_name", "_default_input_field_name", "time_series_type", "_default_time_series_type", "stack_info", "_used_strings"]

	class BucketIntervalUnits:
		DAYS = "DAYS"
		FLEXIBLE = "FLEXIBLE"
		MONTHS = "MONTHS"
		SECONDS = "SECONDS"
		TICKS = "TICKS"

	class OutputIntervalUnits:
		SECONDS = "SECONDS"
		TICKS = "TICKS"

	class BucketTime:
		BUCKET_END = "BUCKET_END"
		BUCKET_START = "BUCKET_START"

	class BoundaryTickBucket:
		NEW = "NEW"
		PREVIOUS = "PREVIOUS"

	class AllFieldsForSliding:
		WHEN_TICKS_EXIT_WINDOW = "WHEN_TICKS_EXIT_WINDOW"
		FALSE = "false"
		TRUE = "true"

	class PartialBucketHandling:
		AS_SEPARATE_BUCKET = "AS_SEPARATE_BUCKET"
		DISCARD = "DISCARD"
		MERGE_WITH_PREVIOUS = "MERGE_WITH_PREVIOUS"

	class GroupsToDisplay:
		ALL = "ALL"
		EVENT_IN_LAST_BUCKET = "EVENT_IN_LAST_BUCKET"

	class TimeSeriesType:
		EVENT_TS = "EVENT_TS"
		STATE_TS = "STATE_TS"

	def __init__(self, bucket_interval=0, bucket_interval_units=BucketIntervalUnits.SECONDS, output_interval="", output_interval_units=OutputIntervalUnits.SECONDS, is_running_aggr=False, bucket_time=BucketTime.BUCKET_END, bucket_end_criteria="", boundary_tick_bucket=BoundaryTickBucket.NEW, all_fields_for_sliding=AllFieldsForSliding.FALSE, partial_bucket_handling=PartialBucketHandling.AS_SEPARATE_BUCKET, output_field_name="VALUE", group_by="", groups_to_display=GroupsToDisplay.ALL, bucket_end_per_group=False, input_field_name="", time_series_type=TimeSeriesType.STATE_TS, In = "", Out = ""):
		_graph_components.EpBase.__init__(self, "TW_AVERAGE")
		self._default_bucket_interval = 0
		self.bucket_interval = bucket_interval
		self._default_bucket_interval_units = type(self).BucketIntervalUnits.SECONDS
		self.bucket_interval_units = bucket_interval_units
		self._default_output_interval = ""
		self.output_interval = output_interval
		self._default_output_interval_units = type(self).OutputIntervalUnits.SECONDS
		self.output_interval_units = output_interval_units
		self._default_is_running_aggr = False
		self.is_running_aggr = is_running_aggr
		self._default_bucket_time = type(self).BucketTime.BUCKET_END
		self.bucket_time = bucket_time
		self._default_bucket_end_criteria = ""
		self.bucket_end_criteria = bucket_end_criteria
		self._default_boundary_tick_bucket = type(self).BoundaryTickBucket.NEW
		self.boundary_tick_bucket = boundary_tick_bucket
		self._default_all_fields_for_sliding = type(self).AllFieldsForSliding.FALSE
		self.all_fields_for_sliding = all_fields_for_sliding
		self._default_partial_bucket_handling = type(self).PartialBucketHandling.AS_SEPARATE_BUCKET
		self.partial_bucket_handling = partial_bucket_handling
		self._default_output_field_name = "VALUE"
		self.output_field_name = output_field_name
		self._default_group_by = ""
		self.group_by = group_by
		self._default_groups_to_display = type(self).GroupsToDisplay.ALL
		self.groups_to_display = groups_to_display
		self._default_bucket_end_per_group = False
		self.bucket_end_per_group = bucket_end_per_group
		self._default_input_field_name = ""
		self.input_field_name = input_field_name
		self._default_time_series_type = type(self).TimeSeriesType.STATE_TS
		self.time_series_type = time_series_type
		if In != "":
			self.input_field_name=In
		if Out != "":
			self.output_field_name=Out
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_bucket_interval(self, value):
		self.bucket_interval = value
		return self

	def set_bucket_interval_units(self, value):
		self.bucket_interval_units = value
		return self

	def set_output_interval(self, value):
		self.output_interval = value
		return self

	def set_output_interval_units(self, value):
		self.output_interval_units = value
		return self

	def set_is_running_aggr(self, value):
		self.is_running_aggr = value
		return self

	def set_bucket_time(self, value):
		self.bucket_time = value
		return self

	def set_bucket_end_criteria(self, value):
		self.bucket_end_criteria = value
		return self

	def set_boundary_tick_bucket(self, value):
		self.boundary_tick_bucket = value
		return self

	def set_all_fields_for_sliding(self, value):
		self.all_fields_for_sliding = value
		return self

	def set_partial_bucket_handling(self, value):
		self.partial_bucket_handling = value
		return self

	def set_output_field_name(self, value):
		self.output_field_name = value
		return self

	def set_group_by(self, value):
		self.group_by = value
		return self

	def set_groups_to_display(self, value):
		self.groups_to_display = value
		return self

	def set_bucket_end_per_group(self, value):
		self.bucket_end_per_group = value
		return self

	def set_input_field_name(self, value):
		self.input_field_name = value
		return self

	def set_time_series_type(self, value):
		self.time_series_type = value
		return self

	@staticmethod
	def _get_name():
		return "TW_AVERAGE"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.bucket_interval != 0: 
			desc += "BUCKET_INTERVAL=" + py_to_str(self.bucket_interval) + ","
		if self.bucket_interval_units != self.BucketIntervalUnits.SECONDS: 
			desc += "BUCKET_INTERVAL_UNITS=" + py_to_str(self.bucket_interval_units) + ","
		if self.output_interval != "": 
			desc += "OUTPUT_INTERVAL=" + py_to_str(self.output_interval) + ","
		if self.output_interval_units != self.OutputIntervalUnits.SECONDS: 
			desc += "OUTPUT_INTERVAL_UNITS=" + py_to_str(self.output_interval_units) + ","
		if self.is_running_aggr != False: 
			desc += "IS_RUNNING_AGGR=" + py_to_str(self.is_running_aggr) + ","
		if self.bucket_time != self.BucketTime.BUCKET_END: 
			desc += "BUCKET_TIME=" + py_to_str(self.bucket_time) + ","
		if self.bucket_end_criteria != "": 
			desc += "BUCKET_END_CRITERIA=" + py_to_str(self.bucket_end_criteria) + ","
		if self.boundary_tick_bucket != self.BoundaryTickBucket.NEW: 
			desc += "BOUNDARY_TICK_BUCKET=" + py_to_str(self.boundary_tick_bucket) + ","
		if self.all_fields_for_sliding != self.AllFieldsForSliding.FALSE: 
			desc += "ALL_FIELDS_FOR_SLIDING=" + py_to_str(self.all_fields_for_sliding) + ","
		if self.partial_bucket_handling != self.PartialBucketHandling.AS_SEPARATE_BUCKET: 
			desc += "PARTIAL_BUCKET_HANDLING=" + py_to_str(self.partial_bucket_handling) + ","
		if self.output_field_name != "VALUE": 
			desc += "OUTPUT_FIELD_NAME=" + py_to_str(self.output_field_name) + ","
		if self.group_by != "": 
			desc += "GROUP_BY=" + py_to_str(self.group_by) + ","
		if self.groups_to_display != self.GroupsToDisplay.ALL: 
			desc += "GROUPS_TO_DISPLAY=" + py_to_str(self.groups_to_display) + ","
		if self.bucket_end_per_group != False: 
			desc += "BUCKET_END_PER_GROUP=" + py_to_str(self.bucket_end_per_group) + ","
		if self.input_field_name != "": 
			desc += "INPUT_FIELD_NAME=" + py_to_str(self.input_field_name) + ","
		if self.time_series_type != self.TimeSeriesType.STATE_TS: 
			desc += "TIME_SERIES_TYPE=" + py_to_str(self.time_series_type) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class ExpTwAverage(_graph_components.EpBase):
	"""
		

EXP_TW_AVERAGE

Type: Aggregation

Description: For each bucket, computes the exponentially
time-weighted average value of a specified numeric field. The
weight of each point in the time series is computed relative to the end
time of the bucket, so that the value which is in effect during some
infinitely small time interval delta t has weight (delta t)*exp(-Lambda*(end_time
- t)), where Lambda is a constant, end_time
represents end time of the bucket, and t represents the
timestamp of that infinitely small time interval.

Python
class name:
ExpTwAverage

Input: A time series of ticks.

Output: A time series of ticks, one for each bucket, with the
field containing averages.

Parameters: See parameters
common to generic aggregations.


  BUCKET_INTERVAL
(seconds/ticks)
  BUCKET_INTERVAL_UNITS
(enumerated type)
  OUTPUT_INTERVAL
(seconds)
  OUTPUT_INTERVAL_UNITS
(SECONDS/TICKS)
  IS_RUNNING_AGGR
(Boolean)
  BUCKET_TIME
(Boolean)
  BUCKET_END_CRITERIA
(expression)
  BOUNDARY_TICK_BUCKET
(NEW/PREVIOUS)
  ALL_FIELDS_FOR_SLIDING
(Boolean)
  PARTIAL_BUCKET_HANDLING
(enumerated type)
  OUTPUT_FIELD_NAME
(string)
  GROUP_BY
(string)
  GROUPS_TO_DISPLAY
(enumerated)
  BUCKET_END_PER_GROUP
(Boolean)
  INPUT_FIELD_NAME
(string)
  DECAY (double)
    The value must be provided. If DECAY_VALUE_TYPE is
HALF_LIFE_SECONDS, DECAY specifies the duration of time interval
between T(N-1) and T(N), where the weight of data point T(N-1) is two
times less than the weight of data point T(N). If DECAY_TYPE is LAMBDA,
DECAY provides the value of the LAMBDA variable in the above-mentioned
formula for weight decay.

  
  DECAY_VALUE_TYPE (enumerated type)
    Possible values are HALF_LIFE_SECONDS and LAMBDA.
Default: HALF_LIFE_SECONDS

  

Notes: See the notes on generic
aggregations.

Example: See the EXP_TW_AVERAGE
example in AGGREGATION_EXAMPLES.otq.


	"""
	class Parameters:
		bucket_interval = "BUCKET_INTERVAL"
		bucket_interval_units = "BUCKET_INTERVAL_UNITS"
		output_interval = "OUTPUT_INTERVAL"
		output_interval_units = "OUTPUT_INTERVAL_UNITS"
		is_running_aggr = "IS_RUNNING_AGGR"
		bucket_time = "BUCKET_TIME"
		bucket_end_criteria = "BUCKET_END_CRITERIA"
		boundary_tick_bucket = "BOUNDARY_TICK_BUCKET"
		all_fields_for_sliding = "ALL_FIELDS_FOR_SLIDING"
		partial_bucket_handling = "PARTIAL_BUCKET_HANDLING"
		output_field_name = "OUTPUT_FIELD_NAME"
		group_by = "GROUP_BY"
		groups_to_display = "GROUPS_TO_DISPLAY"
		bucket_end_per_group = "BUCKET_END_PER_GROUP"
		input_field_name = "INPUT_FIELD_NAME"
		decay = "DECAY"
		decay_value_type = "DECAY_VALUE_TYPE"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["bucket_interval", "bucket_interval_units", "output_interval", "output_interval_units", "is_running_aggr", "bucket_time", "bucket_end_criteria", "boundary_tick_bucket", "all_fields_for_sliding", "partial_bucket_handling", "output_field_name", "group_by", "groups_to_display", "bucket_end_per_group", "input_field_name", "decay", "decay_value_type"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["bucket_interval", "_default_bucket_interval", "bucket_interval_units", "_default_bucket_interval_units", "output_interval", "_default_output_interval", "output_interval_units", "_default_output_interval_units", "is_running_aggr", "_default_is_running_aggr", "bucket_time", "_default_bucket_time", "bucket_end_criteria", "_default_bucket_end_criteria", "boundary_tick_bucket", "_default_boundary_tick_bucket", "all_fields_for_sliding", "_default_all_fields_for_sliding", "partial_bucket_handling", "_default_partial_bucket_handling", "output_field_name", "_default_output_field_name", "group_by", "_default_group_by", "groups_to_display", "_default_groups_to_display", "bucket_end_per_group", "_default_bucket_end_per_group", "input_field_name", "_default_input_field_name", "decay", "_default_decay", "decay_value_type", "_default_decay_value_type", "stack_info", "_used_strings"]

	class BucketIntervalUnits:
		DAYS = "DAYS"
		FLEXIBLE = "FLEXIBLE"
		MONTHS = "MONTHS"
		SECONDS = "SECONDS"
		TICKS = "TICKS"

	class OutputIntervalUnits:
		SECONDS = "SECONDS"
		TICKS = "TICKS"

	class BucketTime:
		BUCKET_END = "BUCKET_END"
		BUCKET_START = "BUCKET_START"

	class BoundaryTickBucket:
		NEW = "NEW"
		PREVIOUS = "PREVIOUS"

	class AllFieldsForSliding:
		WHEN_TICKS_EXIT_WINDOW = "WHEN_TICKS_EXIT_WINDOW"
		FALSE = "false"
		TRUE = "true"

	class PartialBucketHandling:
		AS_SEPARATE_BUCKET = "AS_SEPARATE_BUCKET"
		DISCARD = "DISCARD"
		MERGE_WITH_PREVIOUS = "MERGE_WITH_PREVIOUS"

	class GroupsToDisplay:
		ALL = "ALL"
		EVENT_IN_LAST_BUCKET = "EVENT_IN_LAST_BUCKET"

	class DecayValueType:
		HALF_LIFE_SECONDS = "HALF_LIFE_SECONDS"
		LAMBDA = "LAMBDA"

	def __init__(self, bucket_interval=0, bucket_interval_units=BucketIntervalUnits.SECONDS, output_interval="", output_interval_units=OutputIntervalUnits.SECONDS, is_running_aggr=False, bucket_time=BucketTime.BUCKET_END, bucket_end_criteria="", boundary_tick_bucket=BoundaryTickBucket.NEW, all_fields_for_sliding=AllFieldsForSliding.FALSE, partial_bucket_handling=PartialBucketHandling.AS_SEPARATE_BUCKET, output_field_name="VALUE", group_by="", groups_to_display=GroupsToDisplay.ALL, bucket_end_per_group=False, input_field_name="", decay="", decay_value_type=DecayValueType.HALF_LIFE_SECONDS, In = "", Out = ""):
		_graph_components.EpBase.__init__(self, "EXP_TW_AVERAGE")
		self._default_bucket_interval = 0
		self.bucket_interval = bucket_interval
		self._default_bucket_interval_units = type(self).BucketIntervalUnits.SECONDS
		self.bucket_interval_units = bucket_interval_units
		self._default_output_interval = ""
		self.output_interval = output_interval
		self._default_output_interval_units = type(self).OutputIntervalUnits.SECONDS
		self.output_interval_units = output_interval_units
		self._default_is_running_aggr = False
		self.is_running_aggr = is_running_aggr
		self._default_bucket_time = type(self).BucketTime.BUCKET_END
		self.bucket_time = bucket_time
		self._default_bucket_end_criteria = ""
		self.bucket_end_criteria = bucket_end_criteria
		self._default_boundary_tick_bucket = type(self).BoundaryTickBucket.NEW
		self.boundary_tick_bucket = boundary_tick_bucket
		self._default_all_fields_for_sliding = type(self).AllFieldsForSliding.FALSE
		self.all_fields_for_sliding = all_fields_for_sliding
		self._default_partial_bucket_handling = type(self).PartialBucketHandling.AS_SEPARATE_BUCKET
		self.partial_bucket_handling = partial_bucket_handling
		self._default_output_field_name = "VALUE"
		self.output_field_name = output_field_name
		self._default_group_by = ""
		self.group_by = group_by
		self._default_groups_to_display = type(self).GroupsToDisplay.ALL
		self.groups_to_display = groups_to_display
		self._default_bucket_end_per_group = False
		self.bucket_end_per_group = bucket_end_per_group
		self._default_input_field_name = ""
		self.input_field_name = input_field_name
		self._default_decay = ""
		self.decay = decay
		self._default_decay_value_type = type(self).DecayValueType.HALF_LIFE_SECONDS
		self.decay_value_type = decay_value_type
		if In != "":
			self.input_field_name=In
		if Out != "":
			self.output_field_name=Out
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_bucket_interval(self, value):
		self.bucket_interval = value
		return self

	def set_bucket_interval_units(self, value):
		self.bucket_interval_units = value
		return self

	def set_output_interval(self, value):
		self.output_interval = value
		return self

	def set_output_interval_units(self, value):
		self.output_interval_units = value
		return self

	def set_is_running_aggr(self, value):
		self.is_running_aggr = value
		return self

	def set_bucket_time(self, value):
		self.bucket_time = value
		return self

	def set_bucket_end_criteria(self, value):
		self.bucket_end_criteria = value
		return self

	def set_boundary_tick_bucket(self, value):
		self.boundary_tick_bucket = value
		return self

	def set_all_fields_for_sliding(self, value):
		self.all_fields_for_sliding = value
		return self

	def set_partial_bucket_handling(self, value):
		self.partial_bucket_handling = value
		return self

	def set_output_field_name(self, value):
		self.output_field_name = value
		return self

	def set_group_by(self, value):
		self.group_by = value
		return self

	def set_groups_to_display(self, value):
		self.groups_to_display = value
		return self

	def set_bucket_end_per_group(self, value):
		self.bucket_end_per_group = value
		return self

	def set_input_field_name(self, value):
		self.input_field_name = value
		return self

	def set_decay(self, value):
		self.decay = value
		return self

	def set_decay_value_type(self, value):
		self.decay_value_type = value
		return self

	@staticmethod
	def _get_name():
		return "EXP_TW_AVERAGE"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.bucket_interval != 0: 
			desc += "BUCKET_INTERVAL=" + py_to_str(self.bucket_interval) + ","
		if self.bucket_interval_units != self.BucketIntervalUnits.SECONDS: 
			desc += "BUCKET_INTERVAL_UNITS=" + py_to_str(self.bucket_interval_units) + ","
		if self.output_interval != "": 
			desc += "OUTPUT_INTERVAL=" + py_to_str(self.output_interval) + ","
		if self.output_interval_units != self.OutputIntervalUnits.SECONDS: 
			desc += "OUTPUT_INTERVAL_UNITS=" + py_to_str(self.output_interval_units) + ","
		if self.is_running_aggr != False: 
			desc += "IS_RUNNING_AGGR=" + py_to_str(self.is_running_aggr) + ","
		if self.bucket_time != self.BucketTime.BUCKET_END: 
			desc += "BUCKET_TIME=" + py_to_str(self.bucket_time) + ","
		if self.bucket_end_criteria != "": 
			desc += "BUCKET_END_CRITERIA=" + py_to_str(self.bucket_end_criteria) + ","
		if self.boundary_tick_bucket != self.BoundaryTickBucket.NEW: 
			desc += "BOUNDARY_TICK_BUCKET=" + py_to_str(self.boundary_tick_bucket) + ","
		if self.all_fields_for_sliding != self.AllFieldsForSliding.FALSE: 
			desc += "ALL_FIELDS_FOR_SLIDING=" + py_to_str(self.all_fields_for_sliding) + ","
		if self.partial_bucket_handling != self.PartialBucketHandling.AS_SEPARATE_BUCKET: 
			desc += "PARTIAL_BUCKET_HANDLING=" + py_to_str(self.partial_bucket_handling) + ","
		if self.output_field_name != "VALUE": 
			desc += "OUTPUT_FIELD_NAME=" + py_to_str(self.output_field_name) + ","
		if self.group_by != "": 
			desc += "GROUP_BY=" + py_to_str(self.group_by) + ","
		if self.groups_to_display != self.GroupsToDisplay.ALL: 
			desc += "GROUPS_TO_DISPLAY=" + py_to_str(self.groups_to_display) + ","
		if self.bucket_end_per_group != False: 
			desc += "BUCKET_END_PER_GROUP=" + py_to_str(self.bucket_end_per_group) + ","
		if self.input_field_name != "": 
			desc += "INPUT_FIELD_NAME=" + py_to_str(self.input_field_name) + ","
		if self.decay != "": 
			desc += "DECAY=" + py_to_str(self.decay) + ","
		if self.decay_value_type != self.DecayValueType.HALF_LIFE_SECONDS: 
			desc += "DECAY_VALUE_TYPE=" + py_to_str(self.decay_value_type) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class ExpWAverage(_graph_components.EpBase):
	"""
		

EXP_W_AVERAGE

Type: Aggregation

Description: For each bucket, computes the exponentially
weighted average value of the specified numeric attribute. Weights
of data points in a bucket decrease exponentially in the direction from
the most recent tick to the most aged one, being equal to exp(-Lambda * N) for a fixed weight decay
value Lambda, where N ranges over 0, 1, 2, &hellip;
as ticks in reverse order of their arrival are treated. Once the
weights are known, the average is found using the formula sum(weight*value)/sum(weight), where the
sum is computed across all data points.

Python
class name:
ExpWAverage

Input: A time series of ticks.

Output: A time series of ticks, one for each bucket, with the
field containing averages.

Parameters: See parameters
common to generic aggregations.


  BUCKET_INTERVAL
(seconds/ticks)
  BUCKET_INTERVAL_UNITS
(enumerated type)
  OUTPUT_INTERVAL
(seconds)
  OUTPUT_INTERVAL_UNITS
(SECONDS/TICKS)
  IS_RUNNING_AGGR
(Boolean)
  BUCKET_TIME
(Boolean)
  BUCKET_END_CRITERIA
(expression)
  BOUNDARY_TICK_BUCKET
(NEW/PREVIOUS)
  ALL_FIELDS_FOR_SLIDING
(Boolean)
  PARTIAL_BUCKET_HANDLING
(enumerated type)
  OUTPUT_FIELD_NAME
(string)
  GROUP_BY
(string)
  GROUPS_TO_DISPLAY
(enumerated)
  BUCKET_END_PER_GROUP
(Boolean)
  INPUT_FIELD_NAME
(string)
  TIME_SERIES_TYPE (enumerated type)
If set to STATE_TS, the value from
the input field from the latest tick before the current bucket will be
used as the initial value of the current bucket (unless the current
bucket has a tick at its start time) in the computation of the average
value. If set to EVENT_TS, only
ticks from the current bucket will be used in the computation of the
average value.
Default: STATE_TS
  DECAY (double)
This is a mandatory parameter, specifying weight decay. If DECAY_VALUE_TYPE
is set to LAMBDA, DECAY provides the value of the Lambda
variable in the aforementioned formula. Otherwise, if DECAY_VALUE_TYPE
is set to HALF_LIFE_INDEX, DECAY specifies the
necessary number of consecutive ticks, the first one of which would
have twice less the weight of the last one. The Lambda value is
then calculated using this number.
  DECAY_VALUE_TYPE (enumerated type)
The decay value can specified either directly or indirectly, controlled
respectively by LAMBDA and HALF_LIFE_INDEX values of
this parameter.
Default: LAMBDA

Notes: See the notes on generic
aggregations.

Examples: See EXP_W_AVERAGE
in AGGREGATION_EXAMPLES.otq.


	"""
	class Parameters:
		bucket_interval = "BUCKET_INTERVAL"
		bucket_interval_units = "BUCKET_INTERVAL_UNITS"
		output_interval = "OUTPUT_INTERVAL"
		output_interval_units = "OUTPUT_INTERVAL_UNITS"
		is_running_aggr = "IS_RUNNING_AGGR"
		bucket_time = "BUCKET_TIME"
		bucket_end_criteria = "BUCKET_END_CRITERIA"
		boundary_tick_bucket = "BOUNDARY_TICK_BUCKET"
		all_fields_for_sliding = "ALL_FIELDS_FOR_SLIDING"
		partial_bucket_handling = "PARTIAL_BUCKET_HANDLING"
		output_field_name = "OUTPUT_FIELD_NAME"
		group_by = "GROUP_BY"
		groups_to_display = "GROUPS_TO_DISPLAY"
		bucket_end_per_group = "BUCKET_END_PER_GROUP"
		input_field_name = "INPUT_FIELD_NAME"
		time_series_type = "TIME_SERIES_TYPE"
		decay = "DECAY"
		decay_value_type = "DECAY_VALUE_TYPE"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["bucket_interval", "bucket_interval_units", "output_interval", "output_interval_units", "is_running_aggr", "bucket_time", "bucket_end_criteria", "boundary_tick_bucket", "all_fields_for_sliding", "partial_bucket_handling", "output_field_name", "group_by", "groups_to_display", "bucket_end_per_group", "input_field_name", "time_series_type", "decay", "decay_value_type"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["bucket_interval", "_default_bucket_interval", "bucket_interval_units", "_default_bucket_interval_units", "output_interval", "_default_output_interval", "output_interval_units", "_default_output_interval_units", "is_running_aggr", "_default_is_running_aggr", "bucket_time", "_default_bucket_time", "bucket_end_criteria", "_default_bucket_end_criteria", "boundary_tick_bucket", "_default_boundary_tick_bucket", "all_fields_for_sliding", "_default_all_fields_for_sliding", "partial_bucket_handling", "_default_partial_bucket_handling", "output_field_name", "_default_output_field_name", "group_by", "_default_group_by", "groups_to_display", "_default_groups_to_display", "bucket_end_per_group", "_default_bucket_end_per_group", "input_field_name", "_default_input_field_name", "time_series_type", "_default_time_series_type", "decay", "_default_decay", "decay_value_type", "_default_decay_value_type", "stack_info", "_used_strings"]

	class BucketIntervalUnits:
		DAYS = "DAYS"
		FLEXIBLE = "FLEXIBLE"
		MONTHS = "MONTHS"
		SECONDS = "SECONDS"
		TICKS = "TICKS"

	class OutputIntervalUnits:
		SECONDS = "SECONDS"
		TICKS = "TICKS"

	class BucketTime:
		BUCKET_END = "BUCKET_END"
		BUCKET_START = "BUCKET_START"

	class BoundaryTickBucket:
		NEW = "NEW"
		PREVIOUS = "PREVIOUS"

	class AllFieldsForSliding:
		WHEN_TICKS_EXIT_WINDOW = "WHEN_TICKS_EXIT_WINDOW"
		FALSE = "false"
		TRUE = "true"

	class PartialBucketHandling:
		AS_SEPARATE_BUCKET = "AS_SEPARATE_BUCKET"
		DISCARD = "DISCARD"
		MERGE_WITH_PREVIOUS = "MERGE_WITH_PREVIOUS"

	class GroupsToDisplay:
		ALL = "ALL"
		EVENT_IN_LAST_BUCKET = "EVENT_IN_LAST_BUCKET"

	class TimeSeriesType:
		EVENT_TS = "EVENT_TS"
		STATE_TS = "STATE_TS"

	class DecayValueType:
		HALF_LIFE_INDEX = "HALF_LIFE_INDEX"
		LAMBDA = "LAMBDA"

	def __init__(self, bucket_interval=0, bucket_interval_units=BucketIntervalUnits.SECONDS, output_interval="", output_interval_units=OutputIntervalUnits.SECONDS, is_running_aggr=False, bucket_time=BucketTime.BUCKET_END, bucket_end_criteria="", boundary_tick_bucket=BoundaryTickBucket.NEW, all_fields_for_sliding=AllFieldsForSliding.FALSE, partial_bucket_handling=PartialBucketHandling.AS_SEPARATE_BUCKET, output_field_name="VALUE", group_by="", groups_to_display=GroupsToDisplay.ALL, bucket_end_per_group=False, input_field_name="", time_series_type=TimeSeriesType.STATE_TS, decay="", decay_value_type=DecayValueType.LAMBDA, In = "", Out = ""):
		_graph_components.EpBase.__init__(self, "EXP_W_AVERAGE")
		self._default_bucket_interval = 0
		self.bucket_interval = bucket_interval
		self._default_bucket_interval_units = type(self).BucketIntervalUnits.SECONDS
		self.bucket_interval_units = bucket_interval_units
		self._default_output_interval = ""
		self.output_interval = output_interval
		self._default_output_interval_units = type(self).OutputIntervalUnits.SECONDS
		self.output_interval_units = output_interval_units
		self._default_is_running_aggr = False
		self.is_running_aggr = is_running_aggr
		self._default_bucket_time = type(self).BucketTime.BUCKET_END
		self.bucket_time = bucket_time
		self._default_bucket_end_criteria = ""
		self.bucket_end_criteria = bucket_end_criteria
		self._default_boundary_tick_bucket = type(self).BoundaryTickBucket.NEW
		self.boundary_tick_bucket = boundary_tick_bucket
		self._default_all_fields_for_sliding = type(self).AllFieldsForSliding.FALSE
		self.all_fields_for_sliding = all_fields_for_sliding
		self._default_partial_bucket_handling = type(self).PartialBucketHandling.AS_SEPARATE_BUCKET
		self.partial_bucket_handling = partial_bucket_handling
		self._default_output_field_name = "VALUE"
		self.output_field_name = output_field_name
		self._default_group_by = ""
		self.group_by = group_by
		self._default_groups_to_display = type(self).GroupsToDisplay.ALL
		self.groups_to_display = groups_to_display
		self._default_bucket_end_per_group = False
		self.bucket_end_per_group = bucket_end_per_group
		self._default_input_field_name = ""
		self.input_field_name = input_field_name
		self._default_time_series_type = type(self).TimeSeriesType.STATE_TS
		self.time_series_type = time_series_type
		self._default_decay = ""
		self.decay = decay
		self._default_decay_value_type = type(self).DecayValueType.LAMBDA
		self.decay_value_type = decay_value_type
		if In != "":
			self.input_field_name=In
		if Out != "":
			self.output_field_name=Out
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_bucket_interval(self, value):
		self.bucket_interval = value
		return self

	def set_bucket_interval_units(self, value):
		self.bucket_interval_units = value
		return self

	def set_output_interval(self, value):
		self.output_interval = value
		return self

	def set_output_interval_units(self, value):
		self.output_interval_units = value
		return self

	def set_is_running_aggr(self, value):
		self.is_running_aggr = value
		return self

	def set_bucket_time(self, value):
		self.bucket_time = value
		return self

	def set_bucket_end_criteria(self, value):
		self.bucket_end_criteria = value
		return self

	def set_boundary_tick_bucket(self, value):
		self.boundary_tick_bucket = value
		return self

	def set_all_fields_for_sliding(self, value):
		self.all_fields_for_sliding = value
		return self

	def set_partial_bucket_handling(self, value):
		self.partial_bucket_handling = value
		return self

	def set_output_field_name(self, value):
		self.output_field_name = value
		return self

	def set_group_by(self, value):
		self.group_by = value
		return self

	def set_groups_to_display(self, value):
		self.groups_to_display = value
		return self

	def set_bucket_end_per_group(self, value):
		self.bucket_end_per_group = value
		return self

	def set_input_field_name(self, value):
		self.input_field_name = value
		return self

	def set_time_series_type(self, value):
		self.time_series_type = value
		return self

	def set_decay(self, value):
		self.decay = value
		return self

	def set_decay_value_type(self, value):
		self.decay_value_type = value
		return self

	@staticmethod
	def _get_name():
		return "EXP_W_AVERAGE"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.bucket_interval != 0: 
			desc += "BUCKET_INTERVAL=" + py_to_str(self.bucket_interval) + ","
		if self.bucket_interval_units != self.BucketIntervalUnits.SECONDS: 
			desc += "BUCKET_INTERVAL_UNITS=" + py_to_str(self.bucket_interval_units) + ","
		if self.output_interval != "": 
			desc += "OUTPUT_INTERVAL=" + py_to_str(self.output_interval) + ","
		if self.output_interval_units != self.OutputIntervalUnits.SECONDS: 
			desc += "OUTPUT_INTERVAL_UNITS=" + py_to_str(self.output_interval_units) + ","
		if self.is_running_aggr != False: 
			desc += "IS_RUNNING_AGGR=" + py_to_str(self.is_running_aggr) + ","
		if self.bucket_time != self.BucketTime.BUCKET_END: 
			desc += "BUCKET_TIME=" + py_to_str(self.bucket_time) + ","
		if self.bucket_end_criteria != "": 
			desc += "BUCKET_END_CRITERIA=" + py_to_str(self.bucket_end_criteria) + ","
		if self.boundary_tick_bucket != self.BoundaryTickBucket.NEW: 
			desc += "BOUNDARY_TICK_BUCKET=" + py_to_str(self.boundary_tick_bucket) + ","
		if self.all_fields_for_sliding != self.AllFieldsForSliding.FALSE: 
			desc += "ALL_FIELDS_FOR_SLIDING=" + py_to_str(self.all_fields_for_sliding) + ","
		if self.partial_bucket_handling != self.PartialBucketHandling.AS_SEPARATE_BUCKET: 
			desc += "PARTIAL_BUCKET_HANDLING=" + py_to_str(self.partial_bucket_handling) + ","
		if self.output_field_name != "VALUE": 
			desc += "OUTPUT_FIELD_NAME=" + py_to_str(self.output_field_name) + ","
		if self.group_by != "": 
			desc += "GROUP_BY=" + py_to_str(self.group_by) + ","
		if self.groups_to_display != self.GroupsToDisplay.ALL: 
			desc += "GROUPS_TO_DISPLAY=" + py_to_str(self.groups_to_display) + ","
		if self.bucket_end_per_group != False: 
			desc += "BUCKET_END_PER_GROUP=" + py_to_str(self.bucket_end_per_group) + ","
		if self.input_field_name != "": 
			desc += "INPUT_FIELD_NAME=" + py_to_str(self.input_field_name) + ","
		if self.time_series_type != self.TimeSeriesType.STATE_TS: 
			desc += "TIME_SERIES_TYPE=" + py_to_str(self.time_series_type) + ","
		if self.decay != "": 
			desc += "DECAY=" + py_to_str(self.decay) + ","
		if self.decay_value_type != self.DecayValueType.LAMBDA: 
			desc += "DECAY_VALUE_TYPE=" + py_to_str(self.decay_value_type) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class TwSpread(_graph_components.EpBase):
	"""
		

TW_SPREAD

Type: Aggregation

Description: For each bucket, computes the time-weighted
spread between the bid price and the ask price.

Essentially, TW_SPREAD is TW_AVERAGE
as applied to the (ask - bid) value (which changes every time either
bid or ask change), when IN_BASIS_POINTS
is false (otherwise it is the TW_AVERAGE applied to the spread computed
as a percentage of mid-quote).

However, if the time series contains ticks with non-positive spread
and the POSITIVE_SPREAD_ONLY is
set to true (which is the default), TW_SPREAD will behave as follows.

When the POSITIVE_SPREAD_ONLY parameter is set to true, not only
does it exclude the ticks with zero or negative spread from
contributing to the numerator of the TW_SPREAD formula, but also from
the denominator.

For example, given the following input time series:

Index        Symbol          Time                    ASK_PRICE       BID_PRICE       SPREAD
1       DEMO_L1::AAPL   2003/12/01 16:04:52.000 22.280000       20.740000       1.540000
2       DEMO_L1::AAPL   2003/12/01 16:04:52.000 21.830000       20.740000       1.090000
3       DEMO_L1::AAPL   2003/12/01 16:04:52.000 21.800000       21.270000       0.530000
4       DEMO_L1::AAPL   2003/12/01 16:04:53.000 21.800000       21.270000       0.530000
5       DEMO_L1::AAPL   2003/12/01 16:04:58.000 21.820000       21.690000       0.130000

and executing the query from 16:04:50 till 16:05:09 (excluding the
end time, as we always do in OneTick!), the TW_SPREAD with
POSITIVE_SPREAD_ONLY=true and IN_BASIS_POINTS=false will be computed
according to the following formula:

(1.54*0 + 1.09*0 + 0.53*1 + 0.53*5 + 0.13*11 ) /
(query end time 16:05:09 - timestamp of the first tick 16:04:52) = 4.61
/ 17 sec = 0.271176

However, if we extend the end time of the query by one more second,
and assume that this would make one more tick visible to the query:

6  DEMO_L1::AAPL   2003/12/01 16:05:09.000 0.000000        0.000000        0.000000
the calculation will change.

The numerator will not include spread*duration for the very last
tick because it's a zero spread tick. The duration of tick #5 will
remain the same as before (because at time 16:05:09 a new, invalid,
tick arrives which doesn't make the previous spread, 0.13, valid any
longer than before). The denominator will be the full duration of the
query, as before starting from the timestamp of the first arriving tick
at 16:04:52 and ending at the new query end time of 16:05:10; however,
we will have to exclude from the denominator the length of time that an
invalid tick with spread zero was active, namely 1 second. In this
example there is just one tick with non-positive spread, but if there
are more ticks like that, all their durations need to be subtracted
from the denominator. So the final calculation will look like this:

(1.54*0 + 1.09*0 + 0.53*1 + 0.53*5 + 0.13*11 ) /
(duration minus the time that the tick with the invalid spread was
active) = 4.61 / 17 sec = 0.271176

Python
class name:&nbsp;TwSpread

Input: A time series of ticks containing numeric fields named
BID_PRICE and ASK_PRICE.

Output: A time series of ticks.

Parameters: See parameters
common to generic aggregations.


  BUCKET_INTERVAL
(seconds/ticks)
  BUCKET_INTERVAL_UNITS
(enumerated type)
  OUTPUT_INTERVAL
(seconds)
  OUTPUT_INTERVAL_UNITS
(enumerated type)
  IS_RUNNING_AGGR
(Boolean)
  BUCKET_TIME
(Boolean)
  BUCKET_END_CRITERIA
(expression)
  BOUNDARY_TICK_BUCKET
(NEW/PREVIOUS)
  ALL_FIELDS_FOR_SLIDING
(Boolean)
  PARTIAL_BUCKET_HANDLING
(enumerated type)
  OUTPUT_FIELD_NAME
(string)
  GROUP_BY
(string)
  GROUPS_TO_DISPLAY
(enumerated)
  BUCKET_END_PER_GROUP
(Boolean)
  IN_BASIS_POINTS (Boolean)
    Determines if the spread is to be converted to basis points
(percentage of mid-quote) or absolute monetary units.
Default: true

  
  POSITIVE_SPREAD_ONLY (Boolean)
    When set to true, only ticks where ASK &gt; BID
will be used in the computation.
Default: true

  

Note: See the notes on generic
aggregations.

Examples: See the TW_SPREAD
example in AGGREGATION_EXAMPLES.otq.


	"""
	class Parameters:
		bucket_interval = "BUCKET_INTERVAL"
		bucket_interval_units = "BUCKET_INTERVAL_UNITS"
		output_interval = "OUTPUT_INTERVAL"
		output_interval_units = "OUTPUT_INTERVAL_UNITS"
		is_running_aggr = "IS_RUNNING_AGGR"
		bucket_time = "BUCKET_TIME"
		bucket_end_criteria = "BUCKET_END_CRITERIA"
		boundary_tick_bucket = "BOUNDARY_TICK_BUCKET"
		all_fields_for_sliding = "ALL_FIELDS_FOR_SLIDING"
		partial_bucket_handling = "PARTIAL_BUCKET_HANDLING"
		output_field_name = "OUTPUT_FIELD_NAME"
		group_by = "GROUP_BY"
		groups_to_display = "GROUPS_TO_DISPLAY"
		bucket_end_per_group = "BUCKET_END_PER_GROUP"
		in_basis_points = "IN_BASIS_POINTS"
		positive_spread_only = "POSITIVE_SPREAD_ONLY"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["bucket_interval", "bucket_interval_units", "output_interval", "output_interval_units", "is_running_aggr", "bucket_time", "bucket_end_criteria", "boundary_tick_bucket", "all_fields_for_sliding", "partial_bucket_handling", "output_field_name", "group_by", "groups_to_display", "bucket_end_per_group", "in_basis_points", "positive_spread_only"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["bucket_interval", "_default_bucket_interval", "bucket_interval_units", "_default_bucket_interval_units", "output_interval", "_default_output_interval", "output_interval_units", "_default_output_interval_units", "is_running_aggr", "_default_is_running_aggr", "bucket_time", "_default_bucket_time", "bucket_end_criteria", "_default_bucket_end_criteria", "boundary_tick_bucket", "_default_boundary_tick_bucket", "all_fields_for_sliding", "_default_all_fields_for_sliding", "partial_bucket_handling", "_default_partial_bucket_handling", "output_field_name", "_default_output_field_name", "group_by", "_default_group_by", "groups_to_display", "_default_groups_to_display", "bucket_end_per_group", "_default_bucket_end_per_group", "in_basis_points", "_default_in_basis_points", "positive_spread_only", "_default_positive_spread_only", "stack_info", "_used_strings"]

	class BucketIntervalUnits:
		DAYS = "DAYS"
		FLEXIBLE = "FLEXIBLE"
		MONTHS = "MONTHS"
		SECONDS = "SECONDS"
		TICKS = "TICKS"

	class OutputIntervalUnits:
		SECONDS = "SECONDS"
		TICKS = "TICKS"

	class BucketTime:
		BUCKET_END = "BUCKET_END"
		BUCKET_START = "BUCKET_START"

	class BoundaryTickBucket:
		NEW = "NEW"
		PREVIOUS = "PREVIOUS"

	class AllFieldsForSliding:
		WHEN_TICKS_EXIT_WINDOW = "WHEN_TICKS_EXIT_WINDOW"
		FALSE = "false"
		TRUE = "true"

	class PartialBucketHandling:
		AS_SEPARATE_BUCKET = "AS_SEPARATE_BUCKET"
		DISCARD = "DISCARD"
		MERGE_WITH_PREVIOUS = "MERGE_WITH_PREVIOUS"

	class GroupsToDisplay:
		ALL = "ALL"
		EVENT_IN_LAST_BUCKET = "EVENT_IN_LAST_BUCKET"

	def __init__(self, bucket_interval=0, bucket_interval_units=BucketIntervalUnits.SECONDS, output_interval="", output_interval_units=OutputIntervalUnits.SECONDS, is_running_aggr=False, bucket_time=BucketTime.BUCKET_END, bucket_end_criteria="", boundary_tick_bucket=BoundaryTickBucket.NEW, all_fields_for_sliding=AllFieldsForSliding.FALSE, partial_bucket_handling=PartialBucketHandling.AS_SEPARATE_BUCKET, output_field_name="VALUE", group_by="", groups_to_display=GroupsToDisplay.ALL, bucket_end_per_group=False, in_basis_points=True, positive_spread_only=True, Out = ""):
		_graph_components.EpBase.__init__(self, "TW_SPREAD")
		self._default_bucket_interval = 0
		self.bucket_interval = bucket_interval
		self._default_bucket_interval_units = type(self).BucketIntervalUnits.SECONDS
		self.bucket_interval_units = bucket_interval_units
		self._default_output_interval = ""
		self.output_interval = output_interval
		self._default_output_interval_units = type(self).OutputIntervalUnits.SECONDS
		self.output_interval_units = output_interval_units
		self._default_is_running_aggr = False
		self.is_running_aggr = is_running_aggr
		self._default_bucket_time = type(self).BucketTime.BUCKET_END
		self.bucket_time = bucket_time
		self._default_bucket_end_criteria = ""
		self.bucket_end_criteria = bucket_end_criteria
		self._default_boundary_tick_bucket = type(self).BoundaryTickBucket.NEW
		self.boundary_tick_bucket = boundary_tick_bucket
		self._default_all_fields_for_sliding = type(self).AllFieldsForSliding.FALSE
		self.all_fields_for_sliding = all_fields_for_sliding
		self._default_partial_bucket_handling = type(self).PartialBucketHandling.AS_SEPARATE_BUCKET
		self.partial_bucket_handling = partial_bucket_handling
		self._default_output_field_name = "VALUE"
		self.output_field_name = output_field_name
		self._default_group_by = ""
		self.group_by = group_by
		self._default_groups_to_display = type(self).GroupsToDisplay.ALL
		self.groups_to_display = groups_to_display
		self._default_bucket_end_per_group = False
		self.bucket_end_per_group = bucket_end_per_group
		self._default_in_basis_points = True
		self.in_basis_points = in_basis_points
		self._default_positive_spread_only = True
		self.positive_spread_only = positive_spread_only
		if Out != "":
			self.output_field_name=Out
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_bucket_interval(self, value):
		self.bucket_interval = value
		return self

	def set_bucket_interval_units(self, value):
		self.bucket_interval_units = value
		return self

	def set_output_interval(self, value):
		self.output_interval = value
		return self

	def set_output_interval_units(self, value):
		self.output_interval_units = value
		return self

	def set_is_running_aggr(self, value):
		self.is_running_aggr = value
		return self

	def set_bucket_time(self, value):
		self.bucket_time = value
		return self

	def set_bucket_end_criteria(self, value):
		self.bucket_end_criteria = value
		return self

	def set_boundary_tick_bucket(self, value):
		self.boundary_tick_bucket = value
		return self

	def set_all_fields_for_sliding(self, value):
		self.all_fields_for_sliding = value
		return self

	def set_partial_bucket_handling(self, value):
		self.partial_bucket_handling = value
		return self

	def set_output_field_name(self, value):
		self.output_field_name = value
		return self

	def set_group_by(self, value):
		self.group_by = value
		return self

	def set_groups_to_display(self, value):
		self.groups_to_display = value
		return self

	def set_bucket_end_per_group(self, value):
		self.bucket_end_per_group = value
		return self

	def set_in_basis_points(self, value):
		self.in_basis_points = value
		return self

	def set_positive_spread_only(self, value):
		self.positive_spread_only = value
		return self

	@staticmethod
	def _get_name():
		return "TW_SPREAD"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.bucket_interval != 0: 
			desc += "BUCKET_INTERVAL=" + py_to_str(self.bucket_interval) + ","
		if self.bucket_interval_units != self.BucketIntervalUnits.SECONDS: 
			desc += "BUCKET_INTERVAL_UNITS=" + py_to_str(self.bucket_interval_units) + ","
		if self.output_interval != "": 
			desc += "OUTPUT_INTERVAL=" + py_to_str(self.output_interval) + ","
		if self.output_interval_units != self.OutputIntervalUnits.SECONDS: 
			desc += "OUTPUT_INTERVAL_UNITS=" + py_to_str(self.output_interval_units) + ","
		if self.is_running_aggr != False: 
			desc += "IS_RUNNING_AGGR=" + py_to_str(self.is_running_aggr) + ","
		if self.bucket_time != self.BucketTime.BUCKET_END: 
			desc += "BUCKET_TIME=" + py_to_str(self.bucket_time) + ","
		if self.bucket_end_criteria != "": 
			desc += "BUCKET_END_CRITERIA=" + py_to_str(self.bucket_end_criteria) + ","
		if self.boundary_tick_bucket != self.BoundaryTickBucket.NEW: 
			desc += "BOUNDARY_TICK_BUCKET=" + py_to_str(self.boundary_tick_bucket) + ","
		if self.all_fields_for_sliding != self.AllFieldsForSliding.FALSE: 
			desc += "ALL_FIELDS_FOR_SLIDING=" + py_to_str(self.all_fields_for_sliding) + ","
		if self.partial_bucket_handling != self.PartialBucketHandling.AS_SEPARATE_BUCKET: 
			desc += "PARTIAL_BUCKET_HANDLING=" + py_to_str(self.partial_bucket_handling) + ","
		if self.output_field_name != "VALUE": 
			desc += "OUTPUT_FIELD_NAME=" + py_to_str(self.output_field_name) + ","
		if self.group_by != "": 
			desc += "GROUP_BY=" + py_to_str(self.group_by) + ","
		if self.groups_to_display != self.GroupsToDisplay.ALL: 
			desc += "GROUPS_TO_DISPLAY=" + py_to_str(self.groups_to_display) + ","
		if self.bucket_end_per_group != False: 
			desc += "BUCKET_END_PER_GROUP=" + py_to_str(self.bucket_end_per_group) + ","
		if self.in_basis_points != True: 
			desc += "IN_BASIS_POINTS=" + py_to_str(self.in_basis_points) + ","
		if self.positive_spread_only != True: 
			desc += "POSITIVE_SPREAD_ONLY=" + py_to_str(self.positive_spread_only) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class ObLock(_graph_components.EpBase):
	"""
		

OB_LOCK

Type: Filter

Description: Causes price level ticks from a given source to
be excluded from the aggregate order book when the LOCK_FIELD value is
positive. This node requires two input streams: the stream of PRL ticks
or output ticks of OB_SNAPSHOT and a stream of lock indications.

Python
class name:&nbsp;ObLock

Input: A time series of book ticks (PRL ticks or output ticks
of OB_SNAPSHOT) and a time series of lock indications.

Output: A time series of book ticks. A tick with RECORD_TYPE='X' and all other fields set
to default values is inserted when the LOCK_FIELD value changes to
positive. A tick with RECORD_TYPE='O'
and all other fields set to default values is inserted when the
LOCK_FIELD value changes to zero or negative.

Parameters:


  LOCK_FIELD_NAME - The name of the
FIELD that serves as the lock flag.
Default: None
  LOCK_SOURCE_NAME - The name of the
lock input of the OB_LOCK node.
Default: None
  DETECT_LOCK_AT_START_TIME -
If set to true, the query attempts to determine the value of
the lock field at the start time of the query, by going back in time
from the query start time. If set to false, the value of the
lock field is assumed to be 0 at the start time of the query.
Default: true

Note: OB_LOCK requires that the RECORD_TYPE field be present
in the input book tick stream. The output of OB_SNAPSHOT may not have
this field. In this case, use the ADD_FIELD node, which adds the
RECORD_TYPE field and sets it to 'A'.
This node would be added between OB_SNAPSHOT and the book input of
OB_LOCK.


	"""
	class Parameters:
		lock_field_name = "LOCK_FIELD_NAME"
		lock_source_name = "LOCK_SOURCE_NAME"
		detect_lock_at_start_time = "DETECT_LOCK_AT_START_TIME"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["lock_field_name", "lock_source_name", "detect_lock_at_start_time"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["lock_field_name", "_default_lock_field_name", "lock_source_name", "_default_lock_source_name", "detect_lock_at_start_time", "_default_detect_lock_at_start_time", "stack_info", "_used_strings"]

	def __init__(self, lock_field_name="", lock_source_name="", detect_lock_at_start_time=True):
		_graph_components.EpBase.__init__(self, "OB_LOCK")
		self._default_lock_field_name = ""
		self.lock_field_name = lock_field_name
		self._default_lock_source_name = ""
		self.lock_source_name = lock_source_name
		self._default_detect_lock_at_start_time = True
		self.detect_lock_at_start_time = detect_lock_at_start_time
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_lock_field_name(self, value):
		self.lock_field_name = value
		return self

	def set_lock_source_name(self, value):
		self.lock_source_name = value
		return self

	def set_detect_lock_at_start_time(self, value):
		self.detect_lock_at_start_time = value
		return self

	@staticmethod
	def _get_name():
		return "OB_LOCK"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.lock_field_name != "": 
			desc += "LOCK_FIELD_NAME=" + py_to_str(self.lock_field_name) + ","
		if self.lock_source_name != "": 
			desc += "LOCK_SOURCE_NAME=" + py_to_str(self.lock_source_name) + ","
		if self.detect_lock_at_start_time != True: 
			desc += "DETECT_LOCK_AT_START_TIME=" + py_to_str(self.detect_lock_at_start_time) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class ObSnapshot(_graph_components.EpBase):
	"""
		

OB_SNAPSHOT

Type: Aggregation

Description: Returns the order book state at the end of each
bucket interval: the price, the size, the side, and the time of the
last update for a specified number of order book levels.

Python
class name:
ObSnapshot

Input: A time series of order book ticks.

Output: A time series of ticks, one for each bucket, with
fields named PRICE, UPDATE_TIME, SIZE, LEVEL, and BUY_SELL_FLAG.

Parameters: See Parameters
common to many order book aggregations and Parameters common to generic aggregations.


  BUCKET_INTERVAL
(seconds)
  BUCKET_INTERVAL_UNITS
(enumerated type)
  BUCKET_TIME
(Boolean)
  BUCKET_END_CRITERIA
(expression)
  IS_RUNNING_AGGR
(Boolean)
  SIDE
(ASK|BID)
  MAX_LEVELS
(integer)
  PARTIAL_BUCKET_HANDLING
(Boolean)
  GROUP_BY
(string)
  GROUPS_TO_DISPLAY
(enumerated)
  BUCKET_END_PER_GROUP
(Boolean)
  MAX_DEPTH_SHARES
(integer)
  MAX_DEPTH_FOR_PRICE
(double)
  MAX_SPREAD
(double)
  MAX_INITIALIZATION_DAYS
(integer)
  SIZE_MAX_FRACTIONAL_DIGITS
(integer)
  STATE_KEY_MAX_INACTIVITY_SEC
(integer)
  BOOK_UNCROSS_METHOD
(enumerated)
  DQ_EVENTS_THAT_CLEAR_BOOK
(string)
  INCLUDE_MARKET_ORDER_TICKS&nbsp;(Boolean)
  IDENTIFY_SOURCE (Boolean)
    When this parameter is set to&nbsp;true,
input ticks must have field SOURCE,
and the EP will produce a separate output tick for each value of SOURCE field. 
Default: false

  
  SHOW_FULL_DETAIL (Boolean)
    When set to "true" and if the state key of the input ticks
consists of some fields besides PRICE, output ticks will contain all
fields from the input ticks for each price level. When set to "false,"
only PRICE, UPDATE_TIME, SIZE, LEVEL, and BUY_SELL_FLAG fields will be
populated.
Note: setting this flag to "true" has no effect on a time series that
does not have a state key
Default: false

  
  SHOW_ONLY_CHANGES (Boolean)
    When set to true, the output stream carries only changes to the
book. The representation is as follows:

    
      Changed and added levels are represented by themselves.
      Deleted levels are shown with a size and level of zero.
    
    As with other modes, correct detection of update boundaries may
require setting the BOOK_DELIMITERS option.
Default: false

  
  BOOK_DELIMITERS (enumerated)
    When set to "D," an extra tick is created after each book. Also,
an additional column, called DELIMITER, is added to output ticks. The
extra tick has values of all fields set to the defaults (0,NaN,""),
except the delimiter field, which is set to "D." All other ticks have
the DELIMITER set to zero (0).
Default: none

  
  LEVEL_ASSIGNMENT_METHOD
(enumerated)
    Possible values of this parameter are PRICE_LEVEL and
BOOK_POSITION. When set to PRICE_LEVEL (default), book entries with the
same price are assigned the same level number. When set to
BOOK_POSITION, each entry in the book has its own level number.
Default: PRICE_LEVEL

  


  SHOW_LAST_UPDATE_TIME (Boolean)
    If set, keep the timestamp of the last event that affected the
book in UPDATE_TIME field of a delimiter tick. Notice that this could
be the timestamp of an event that did not affect displayed book levels,
and it could be the timestamp of an event that caused a deletion of a
level from the book.
Default: false

  

See also,

notes on order book aggregations
.

Examples: Every 10 minutes output a complete snapshot for the
bid side.

OB_SNAPSHOT(600, false, BID, AS_SEPARATE_BUCKET,
false, false)

See the OB_SNAPSHOT example in OB_AGGREGATION_EXAMPLES.otq.


	"""
	class Parameters:
		bucket_interval = "BUCKET_INTERVAL"
		bucket_interval_units = "BUCKET_INTERVAL_UNITS"
		bucket_time = "BUCKET_TIME"
		bucket_end_criteria = "BUCKET_END_CRITERIA"
		boundary_tick_bucket = "BOUNDARY_TICK_BUCKET"
		is_running_aggr = "IS_RUNNING_AGGR"
		side = "SIDE"
		max_levels = "MAX_LEVELS"
		partial_bucket_handling = "PARTIAL_BUCKET_HANDLING"
		group_by = "GROUP_BY"
		groups_to_display = "GROUPS_TO_DISPLAY"
		bucket_end_per_group = "BUCKET_END_PER_GROUP"
		max_depth_shares = "MAX_DEPTH_SHARES"
		max_depth_for_price = "MAX_DEPTH_FOR_PRICE"
		max_spread = "MAX_SPREAD"
		identify_source = "IDENTIFY_SOURCE"
		show_full_detail = "SHOW_FULL_DETAIL"
		show_only_changes = "SHOW_ONLY_CHANGES"
		book_delimiters = "BOOK_DELIMITERS"
		max_initialization_days = "MAX_INITIALIZATION_DAYS"
		size_max_fractional_digits = "SIZE_MAX_FRACTIONAL_DIGITS"
		book_uncross_method = "BOOK_UNCROSS_METHOD"
		state_key_max_inactivity_sec = "STATE_KEY_MAX_INACTIVITY_SEC"
		level_assignment_method = "LEVEL_ASSIGNMENT_METHOD"
		dq_events_that_clear_book = "DQ_EVENTS_THAT_CLEAR_BOOK"
		show_last_update_time = "SHOW_LAST_UPDATE_TIME"
		include_market_order_ticks = "INCLUDE_MARKET_ORDER_TICKS"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["bucket_interval", "bucket_interval_units", "bucket_time", "bucket_end_criteria", "boundary_tick_bucket", "is_running_aggr", "side", "max_levels", "partial_bucket_handling", "group_by", "groups_to_display", "bucket_end_per_group", "max_depth_shares", "max_depth_for_price", "max_spread", "identify_source", "show_full_detail", "show_only_changes", "book_delimiters", "max_initialization_days", "size_max_fractional_digits", "book_uncross_method", "state_key_max_inactivity_sec", "level_assignment_method", "dq_events_that_clear_book", "show_last_update_time", "include_market_order_ticks"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["bucket_interval", "_default_bucket_interval", "bucket_interval_units", "_default_bucket_interval_units", "bucket_time", "_default_bucket_time", "bucket_end_criteria", "_default_bucket_end_criteria", "boundary_tick_bucket", "_default_boundary_tick_bucket", "is_running_aggr", "_default_is_running_aggr", "side", "_default_side", "max_levels", "_default_max_levels", "partial_bucket_handling", "_default_partial_bucket_handling", "group_by", "_default_group_by", "groups_to_display", "_default_groups_to_display", "bucket_end_per_group", "_default_bucket_end_per_group", "max_depth_shares", "_default_max_depth_shares", "max_depth_for_price", "_default_max_depth_for_price", "max_spread", "_default_max_spread", "identify_source", "_default_identify_source", "show_full_detail", "_default_show_full_detail", "show_only_changes", "_default_show_only_changes", "book_delimiters", "_default_book_delimiters", "max_initialization_days", "_default_max_initialization_days", "size_max_fractional_digits", "_default_size_max_fractional_digits", "book_uncross_method", "_default_book_uncross_method", "state_key_max_inactivity_sec", "_default_state_key_max_inactivity_sec", "level_assignment_method", "_default_level_assignment_method", "dq_events_that_clear_book", "_default_dq_events_that_clear_book", "show_last_update_time", "_default_show_last_update_time", "include_market_order_ticks", "_default_include_market_order_ticks", "stack_info", "_used_strings"]

	class BucketIntervalUnits:
		DAYS = "DAYS"
		FLEXIBLE = "FLEXIBLE"
		MONTHS = "MONTHS"
		SECONDS = "SECONDS"

	class BucketTime:
		BUCKET_END = "BUCKET_END"
		BUCKET_START = "BUCKET_START"

	class BoundaryTickBucket:
		NEW = "NEW"
		PREVIOUS = "PREVIOUS"

	class Side:
		EMPTY = ""
		ASK = "ASK"
		BID = "BID"

	class PartialBucketHandling:
		AS_SEPARATE_BUCKET = "AS_SEPARATE_BUCKET"
		DISCARD = "DISCARD"
		MERGE_WITH_PREVIOUS = "MERGE_WITH_PREVIOUS"

	class GroupsToDisplay:
		ALL = "ALL"
		EVENT_IN_LAST_BUCKET = "EVENT_IN_LAST_BUCKET"

	class BookDelimiters:
		D = "D"
		NONE = "NONE"

	class LevelAssignmentMethod:
		BOOK_POSITION = "BOOK_POSITION"
		PRICE_LEVEL = "PRICE_LEVEL"

	def __init__(self, bucket_interval=0, bucket_interval_units=BucketIntervalUnits.SECONDS, bucket_time=BucketTime.BUCKET_END, bucket_end_criteria="", boundary_tick_bucket=BoundaryTickBucket.NEW, is_running_aggr=False, side=Side.EMPTY, max_levels="", partial_bucket_handling=PartialBucketHandling.AS_SEPARATE_BUCKET, group_by="", groups_to_display=GroupsToDisplay.ALL, bucket_end_per_group=False, max_depth_shares="", max_depth_for_price="", max_spread="", identify_source=False, show_full_detail=False, show_only_changes=False, book_delimiters=BookDelimiters.NONE, max_initialization_days="", size_max_fractional_digits=0, book_uncross_method="", state_key_max_inactivity_sec="", level_assignment_method=LevelAssignmentMethod.PRICE_LEVEL, dq_events_that_clear_book="", show_last_update_time=False, include_market_order_ticks=False):
		_graph_components.EpBase.__init__(self, "OB_SNAPSHOT")
		self._default_bucket_interval = 0
		self.bucket_interval = bucket_interval
		self._default_bucket_interval_units = type(self).BucketIntervalUnits.SECONDS
		self.bucket_interval_units = bucket_interval_units
		self._default_bucket_time = type(self).BucketTime.BUCKET_END
		self.bucket_time = bucket_time
		self._default_bucket_end_criteria = ""
		self.bucket_end_criteria = bucket_end_criteria
		self._default_boundary_tick_bucket = type(self).BoundaryTickBucket.NEW
		self.boundary_tick_bucket = boundary_tick_bucket
		self._default_is_running_aggr = False
		self.is_running_aggr = is_running_aggr
		self._default_side = type(self).Side.EMPTY
		self.side = side
		self._default_max_levels = ""
		self.max_levels = max_levels
		self._default_partial_bucket_handling = type(self).PartialBucketHandling.AS_SEPARATE_BUCKET
		self.partial_bucket_handling = partial_bucket_handling
		self._default_group_by = ""
		self.group_by = group_by
		self._default_groups_to_display = type(self).GroupsToDisplay.ALL
		self.groups_to_display = groups_to_display
		self._default_bucket_end_per_group = False
		self.bucket_end_per_group = bucket_end_per_group
		self._default_max_depth_shares = ""
		self.max_depth_shares = max_depth_shares
		self._default_max_depth_for_price = ""
		self.max_depth_for_price = max_depth_for_price
		self._default_max_spread = ""
		self.max_spread = max_spread
		self._default_identify_source = False
		self.identify_source = identify_source
		self._default_show_full_detail = False
		self.show_full_detail = show_full_detail
		self._default_show_only_changes = False
		self.show_only_changes = show_only_changes
		self._default_book_delimiters = type(self).BookDelimiters.NONE
		self.book_delimiters = book_delimiters
		self._default_max_initialization_days = ""
		self.max_initialization_days = max_initialization_days
		self._default_size_max_fractional_digits = 0
		self.size_max_fractional_digits = size_max_fractional_digits
		self._default_book_uncross_method = ""
		self.book_uncross_method = book_uncross_method
		self._default_state_key_max_inactivity_sec = ""
		self.state_key_max_inactivity_sec = state_key_max_inactivity_sec
		self._default_level_assignment_method = type(self).LevelAssignmentMethod.PRICE_LEVEL
		self.level_assignment_method = level_assignment_method
		self._default_dq_events_that_clear_book = ""
		self.dq_events_that_clear_book = dq_events_that_clear_book
		self._default_show_last_update_time = False
		self.show_last_update_time = show_last_update_time
		self._default_include_market_order_ticks = False
		self.include_market_order_ticks = include_market_order_ticks
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_bucket_interval(self, value):
		self.bucket_interval = value
		return self

	def set_bucket_interval_units(self, value):
		self.bucket_interval_units = value
		return self

	def set_bucket_time(self, value):
		self.bucket_time = value
		return self

	def set_bucket_end_criteria(self, value):
		self.bucket_end_criteria = value
		return self

	def set_boundary_tick_bucket(self, value):
		self.boundary_tick_bucket = value
		return self

	def set_is_running_aggr(self, value):
		self.is_running_aggr = value
		return self

	def set_side(self, value):
		self.side = value
		return self

	def set_max_levels(self, value):
		self.max_levels = value
		return self

	def set_partial_bucket_handling(self, value):
		self.partial_bucket_handling = value
		return self

	def set_group_by(self, value):
		self.group_by = value
		return self

	def set_groups_to_display(self, value):
		self.groups_to_display = value
		return self

	def set_bucket_end_per_group(self, value):
		self.bucket_end_per_group = value
		return self

	def set_max_depth_shares(self, value):
		self.max_depth_shares = value
		return self

	def set_max_depth_for_price(self, value):
		self.max_depth_for_price = value
		return self

	def set_max_spread(self, value):
		self.max_spread = value
		return self

	def set_identify_source(self, value):
		self.identify_source = value
		return self

	def set_show_full_detail(self, value):
		self.show_full_detail = value
		return self

	def set_show_only_changes(self, value):
		self.show_only_changes = value
		return self

	def set_book_delimiters(self, value):
		self.book_delimiters = value
		return self

	def set_max_initialization_days(self, value):
		self.max_initialization_days = value
		return self

	def set_size_max_fractional_digits(self, value):
		self.size_max_fractional_digits = value
		return self

	def set_book_uncross_method(self, value):
		self.book_uncross_method = value
		return self

	def set_state_key_max_inactivity_sec(self, value):
		self.state_key_max_inactivity_sec = value
		return self

	def set_level_assignment_method(self, value):
		self.level_assignment_method = value
		return self

	def set_dq_events_that_clear_book(self, value):
		self.dq_events_that_clear_book = value
		return self

	def set_show_last_update_time(self, value):
		self.show_last_update_time = value
		return self

	def set_include_market_order_ticks(self, value):
		self.include_market_order_ticks = value
		return self

	@staticmethod
	def _get_name():
		return "OB_SNAPSHOT"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.bucket_interval != 0: 
			desc += "BUCKET_INTERVAL=" + py_to_str(self.bucket_interval) + ","
		if self.bucket_interval_units != self.BucketIntervalUnits.SECONDS: 
			desc += "BUCKET_INTERVAL_UNITS=" + py_to_str(self.bucket_interval_units) + ","
		if self.bucket_time != self.BucketTime.BUCKET_END: 
			desc += "BUCKET_TIME=" + py_to_str(self.bucket_time) + ","
		if self.bucket_end_criteria != "": 
			desc += "BUCKET_END_CRITERIA=" + py_to_str(self.bucket_end_criteria) + ","
		if self.boundary_tick_bucket != self.BoundaryTickBucket.NEW: 
			desc += "BOUNDARY_TICK_BUCKET=" + py_to_str(self.boundary_tick_bucket) + ","
		if self.is_running_aggr != False: 
			desc += "IS_RUNNING_AGGR=" + py_to_str(self.is_running_aggr) + ","
		if self.side != self.Side.EMPTY: 
			desc += "SIDE=" + py_to_str(self.side) + ","
		if self.max_levels != "": 
			desc += "MAX_LEVELS=" + py_to_str(self.max_levels) + ","
		if self.partial_bucket_handling != self.PartialBucketHandling.AS_SEPARATE_BUCKET: 
			desc += "PARTIAL_BUCKET_HANDLING=" + py_to_str(self.partial_bucket_handling) + ","
		if self.group_by != "": 
			desc += "GROUP_BY=" + py_to_str(self.group_by) + ","
		if self.groups_to_display != self.GroupsToDisplay.ALL: 
			desc += "GROUPS_TO_DISPLAY=" + py_to_str(self.groups_to_display) + ","
		if self.bucket_end_per_group != False: 
			desc += "BUCKET_END_PER_GROUP=" + py_to_str(self.bucket_end_per_group) + ","
		if self.max_depth_shares != "": 
			desc += "MAX_DEPTH_SHARES=" + py_to_str(self.max_depth_shares) + ","
		if self.max_depth_for_price != "": 
			desc += "MAX_DEPTH_FOR_PRICE=" + py_to_str(self.max_depth_for_price) + ","
		if self.max_spread != "": 
			desc += "MAX_SPREAD=" + py_to_str(self.max_spread) + ","
		if self.identify_source != False: 
			desc += "IDENTIFY_SOURCE=" + py_to_str(self.identify_source) + ","
		if self.show_full_detail != False: 
			desc += "SHOW_FULL_DETAIL=" + py_to_str(self.show_full_detail) + ","
		if self.show_only_changes != False: 
			desc += "SHOW_ONLY_CHANGES=" + py_to_str(self.show_only_changes) + ","
		if self.book_delimiters != self.BookDelimiters.NONE: 
			desc += "BOOK_DELIMITERS=" + py_to_str(self.book_delimiters) + ","
		if self.max_initialization_days != "": 
			desc += "MAX_INITIALIZATION_DAYS=" + py_to_str(self.max_initialization_days) + ","
		if self.size_max_fractional_digits != 0: 
			desc += "SIZE_MAX_FRACTIONAL_DIGITS=" + py_to_str(self.size_max_fractional_digits) + ","
		if self.book_uncross_method != "": 
			desc += "BOOK_UNCROSS_METHOD=" + py_to_str(self.book_uncross_method) + ","
		if self.state_key_max_inactivity_sec != "": 
			desc += "STATE_KEY_MAX_INACTIVITY_SEC=" + py_to_str(self.state_key_max_inactivity_sec) + ","
		if self.level_assignment_method != self.LevelAssignmentMethod.PRICE_LEVEL: 
			desc += "LEVEL_ASSIGNMENT_METHOD=" + py_to_str(self.level_assignment_method) + ","
		if self.dq_events_that_clear_book != "": 
			desc += "DQ_EVENTS_THAT_CLEAR_BOOK=" + py_to_str(self.dq_events_that_clear_book) + ","
		if self.show_last_update_time != False: 
			desc += "SHOW_LAST_UPDATE_TIME=" + py_to_str(self.show_last_update_time) + ","
		if self.include_market_order_ticks != False: 
			desc += "INCLUDE_MARKET_ORDER_TICKS=" + py_to_str(self.include_market_order_ticks) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class ObSnapshotWide(_graph_components.EpBase):
	"""
		

OB_SNAPSHOT_WIDE

Type: Aggregation

Description: Returns a side by side order book state at the
end of each interval: the price, the size, and the time of the last
update for a specified number of order book levels.

Python
class name:
ObSnapshotWide

Input: A time series of order book ticks.

Output: A time series of ticks with fields named ASK_PRICE,
ASK_SIZE, ASK_UPDATE_TIME, BID_PRICE, BID_SIZE, BID_UPDATE_TIME, LEVEL.

Parameters: See parameters
common to many order book aggregations and parameters common to generic aggregations.


  BUCKET_INTERVAL
(seconds)
  BUCKET_INTERVAL_UNITS
(enumerated type)
  BUCKET_TIME
(Boolean)
  BUCKET_END_CRITERIA
(expression)
  IS_RUNNING_AGGR
(Boolean)
  MAX_LEVELS
(integer)
  PARTIAL_BUCKET_HANDLING
(Boolean)
  GROUP_BY
(string)
  GROUPS_TO_DISPLAY
(enumerated)
  BUCKET_END_PER_GROUP
(Boolean)
  MAX_DEPTH_SHARES
(integer)
  MAX_DEPTH_FOR_PRICE
(double)
  MAX_SPREAD
(double)
  MAX_INITIALIZATION_DAYS
(integer)
  SIZE_MAX_FRACTIONAL_DIGITS
(integer)
  STATE_KEY_MAX_INACTIVITY_SEC
(integer)
  BOOK_UNCROSS_METHOD
(enumerated)
  DQ_EVENTS_THAT_CLEAR_BOOK
(string)
  INCLUDE_MARKET_ORDER_TICKS&nbsp;(Boolean)
  SHOW_FULL_DETAIL
(Boolean)
    When set to true, output ticks will contain all fields from the
input ticks.

  
  BOOK_DELIMITERS (enumerated)
    When set to "D", an extra tick is created after each book. Also,
an additional column, called DELIMITER, is added to output ticks. The
extra tick has values of all fields set to the defaults (0,NaN,""),
except the delimiter field, which is set to "D". All other ticks have
DELIMITER set to zero (0).
Default: NONE

  

See also, notes on order book
aggregations.

Examples: On every tick recompute the snapshot for the top 10
levels on each side. For running queries, such as this one, the output
will only be generated for a tick that changes one of the levels
requested by the user.

OB_SNAPSHOT_WIDE (0, true, 10, , AS_SEPARATE_BUCKET,)

&nbsp;

See the OB_SNAPSHOT_WIDE example
in OB_AGGREGATION_EXAMPLES.otq.


	"""
	class Parameters:
		bucket_interval = "BUCKET_INTERVAL"
		bucket_interval_units = "BUCKET_INTERVAL_UNITS"
		bucket_time = "BUCKET_TIME"
		bucket_end_criteria = "BUCKET_END_CRITERIA"
		boundary_tick_bucket = "BOUNDARY_TICK_BUCKET"
		is_running_aggr = "IS_RUNNING_AGGR"
		show_full_detail = "SHOW_FULL_DETAIL"
		max_levels = "MAX_LEVELS"
		max_depth_shares = "MAX_DEPTH_SHARES"
		max_depth_for_price = "MAX_DEPTH_FOR_PRICE"
		max_spread = "MAX_SPREAD"
		partial_bucket_handling = "PARTIAL_BUCKET_HANDLING"
		group_by = "GROUP_BY"
		groups_to_display = "GROUPS_TO_DISPLAY"
		bucket_end_per_group = "BUCKET_END_PER_GROUP"
		book_delimiters = "BOOK_DELIMITERS"
		max_initialization_days = "MAX_INITIALIZATION_DAYS"
		size_max_fractional_digits = "SIZE_MAX_FRACTIONAL_DIGITS"
		book_uncross_method = "BOOK_UNCROSS_METHOD"
		state_key_max_inactivity_sec = "STATE_KEY_MAX_INACTIVITY_SEC"
		dq_events_that_clear_book = "DQ_EVENTS_THAT_CLEAR_BOOK"
		include_market_order_ticks = "INCLUDE_MARKET_ORDER_TICKS"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["bucket_interval", "bucket_interval_units", "bucket_time", "bucket_end_criteria", "boundary_tick_bucket", "is_running_aggr", "show_full_detail", "max_levels", "max_depth_shares", "max_depth_for_price", "max_spread", "partial_bucket_handling", "group_by", "groups_to_display", "bucket_end_per_group", "book_delimiters", "max_initialization_days", "size_max_fractional_digits", "book_uncross_method", "state_key_max_inactivity_sec", "dq_events_that_clear_book", "include_market_order_ticks"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["bucket_interval", "_default_bucket_interval", "bucket_interval_units", "_default_bucket_interval_units", "bucket_time", "_default_bucket_time", "bucket_end_criteria", "_default_bucket_end_criteria", "boundary_tick_bucket", "_default_boundary_tick_bucket", "is_running_aggr", "_default_is_running_aggr", "show_full_detail", "_default_show_full_detail", "max_levels", "_default_max_levels", "max_depth_shares", "_default_max_depth_shares", "max_depth_for_price", "_default_max_depth_for_price", "max_spread", "_default_max_spread", "partial_bucket_handling", "_default_partial_bucket_handling", "group_by", "_default_group_by", "groups_to_display", "_default_groups_to_display", "bucket_end_per_group", "_default_bucket_end_per_group", "book_delimiters", "_default_book_delimiters", "max_initialization_days", "_default_max_initialization_days", "size_max_fractional_digits", "_default_size_max_fractional_digits", "book_uncross_method", "_default_book_uncross_method", "state_key_max_inactivity_sec", "_default_state_key_max_inactivity_sec", "dq_events_that_clear_book", "_default_dq_events_that_clear_book", "include_market_order_ticks", "_default_include_market_order_ticks", "stack_info", "_used_strings"]

	class BucketIntervalUnits:
		DAYS = "DAYS"
		FLEXIBLE = "FLEXIBLE"
		MONTHS = "MONTHS"
		SECONDS = "SECONDS"

	class BucketTime:
		BUCKET_END = "BUCKET_END"
		BUCKET_START = "BUCKET_START"

	class BoundaryTickBucket:
		NEW = "NEW"
		PREVIOUS = "PREVIOUS"

	class PartialBucketHandling:
		AS_SEPARATE_BUCKET = "AS_SEPARATE_BUCKET"
		DISCARD = "DISCARD"
		MERGE_WITH_PREVIOUS = "MERGE_WITH_PREVIOUS"

	class GroupsToDisplay:
		ALL = "ALL"
		EVENT_IN_LAST_BUCKET = "EVENT_IN_LAST_BUCKET"

	class BookDelimiters:
		D = "D"
		NONE = "NONE"

	def __init__(self, bucket_interval=0, bucket_interval_units=BucketIntervalUnits.SECONDS, bucket_time=BucketTime.BUCKET_END, bucket_end_criteria="", boundary_tick_bucket=BoundaryTickBucket.NEW, is_running_aggr=False, show_full_detail=False, max_levels="", max_depth_shares="", max_depth_for_price="", max_spread="", partial_bucket_handling=PartialBucketHandling.AS_SEPARATE_BUCKET, group_by="", groups_to_display=GroupsToDisplay.ALL, bucket_end_per_group=False, book_delimiters=BookDelimiters.NONE, max_initialization_days="", size_max_fractional_digits=0, book_uncross_method="", state_key_max_inactivity_sec="", dq_events_that_clear_book="", include_market_order_ticks=False):
		_graph_components.EpBase.__init__(self, "OB_SNAPSHOT_WIDE")
		self._default_bucket_interval = 0
		self.bucket_interval = bucket_interval
		self._default_bucket_interval_units = type(self).BucketIntervalUnits.SECONDS
		self.bucket_interval_units = bucket_interval_units
		self._default_bucket_time = type(self).BucketTime.BUCKET_END
		self.bucket_time = bucket_time
		self._default_bucket_end_criteria = ""
		self.bucket_end_criteria = bucket_end_criteria
		self._default_boundary_tick_bucket = type(self).BoundaryTickBucket.NEW
		self.boundary_tick_bucket = boundary_tick_bucket
		self._default_is_running_aggr = False
		self.is_running_aggr = is_running_aggr
		self._default_show_full_detail = False
		self.show_full_detail = show_full_detail
		self._default_max_levels = ""
		self.max_levels = max_levels
		self._default_max_depth_shares = ""
		self.max_depth_shares = max_depth_shares
		self._default_max_depth_for_price = ""
		self.max_depth_for_price = max_depth_for_price
		self._default_max_spread = ""
		self.max_spread = max_spread
		self._default_partial_bucket_handling = type(self).PartialBucketHandling.AS_SEPARATE_BUCKET
		self.partial_bucket_handling = partial_bucket_handling
		self._default_group_by = ""
		self.group_by = group_by
		self._default_groups_to_display = type(self).GroupsToDisplay.ALL
		self.groups_to_display = groups_to_display
		self._default_bucket_end_per_group = False
		self.bucket_end_per_group = bucket_end_per_group
		self._default_book_delimiters = type(self).BookDelimiters.NONE
		self.book_delimiters = book_delimiters
		self._default_max_initialization_days = ""
		self.max_initialization_days = max_initialization_days
		self._default_size_max_fractional_digits = 0
		self.size_max_fractional_digits = size_max_fractional_digits
		self._default_book_uncross_method = ""
		self.book_uncross_method = book_uncross_method
		self._default_state_key_max_inactivity_sec = ""
		self.state_key_max_inactivity_sec = state_key_max_inactivity_sec
		self._default_dq_events_that_clear_book = ""
		self.dq_events_that_clear_book = dq_events_that_clear_book
		self._default_include_market_order_ticks = False
		self.include_market_order_ticks = include_market_order_ticks
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_bucket_interval(self, value):
		self.bucket_interval = value
		return self

	def set_bucket_interval_units(self, value):
		self.bucket_interval_units = value
		return self

	def set_bucket_time(self, value):
		self.bucket_time = value
		return self

	def set_bucket_end_criteria(self, value):
		self.bucket_end_criteria = value
		return self

	def set_boundary_tick_bucket(self, value):
		self.boundary_tick_bucket = value
		return self

	def set_is_running_aggr(self, value):
		self.is_running_aggr = value
		return self

	def set_show_full_detail(self, value):
		self.show_full_detail = value
		return self

	def set_max_levels(self, value):
		self.max_levels = value
		return self

	def set_max_depth_shares(self, value):
		self.max_depth_shares = value
		return self

	def set_max_depth_for_price(self, value):
		self.max_depth_for_price = value
		return self

	def set_max_spread(self, value):
		self.max_spread = value
		return self

	def set_partial_bucket_handling(self, value):
		self.partial_bucket_handling = value
		return self

	def set_group_by(self, value):
		self.group_by = value
		return self

	def set_groups_to_display(self, value):
		self.groups_to_display = value
		return self

	def set_bucket_end_per_group(self, value):
		self.bucket_end_per_group = value
		return self

	def set_book_delimiters(self, value):
		self.book_delimiters = value
		return self

	def set_max_initialization_days(self, value):
		self.max_initialization_days = value
		return self

	def set_size_max_fractional_digits(self, value):
		self.size_max_fractional_digits = value
		return self

	def set_book_uncross_method(self, value):
		self.book_uncross_method = value
		return self

	def set_state_key_max_inactivity_sec(self, value):
		self.state_key_max_inactivity_sec = value
		return self

	def set_dq_events_that_clear_book(self, value):
		self.dq_events_that_clear_book = value
		return self

	def set_include_market_order_ticks(self, value):
		self.include_market_order_ticks = value
		return self

	@staticmethod
	def _get_name():
		return "OB_SNAPSHOT_WIDE"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.bucket_interval != 0: 
			desc += "BUCKET_INTERVAL=" + py_to_str(self.bucket_interval) + ","
		if self.bucket_interval_units != self.BucketIntervalUnits.SECONDS: 
			desc += "BUCKET_INTERVAL_UNITS=" + py_to_str(self.bucket_interval_units) + ","
		if self.bucket_time != self.BucketTime.BUCKET_END: 
			desc += "BUCKET_TIME=" + py_to_str(self.bucket_time) + ","
		if self.bucket_end_criteria != "": 
			desc += "BUCKET_END_CRITERIA=" + py_to_str(self.bucket_end_criteria) + ","
		if self.boundary_tick_bucket != self.BoundaryTickBucket.NEW: 
			desc += "BOUNDARY_TICK_BUCKET=" + py_to_str(self.boundary_tick_bucket) + ","
		if self.is_running_aggr != False: 
			desc += "IS_RUNNING_AGGR=" + py_to_str(self.is_running_aggr) + ","
		if self.show_full_detail != False: 
			desc += "SHOW_FULL_DETAIL=" + py_to_str(self.show_full_detail) + ","
		if self.max_levels != "": 
			desc += "MAX_LEVELS=" + py_to_str(self.max_levels) + ","
		if self.max_depth_shares != "": 
			desc += "MAX_DEPTH_SHARES=" + py_to_str(self.max_depth_shares) + ","
		if self.max_depth_for_price != "": 
			desc += "MAX_DEPTH_FOR_PRICE=" + py_to_str(self.max_depth_for_price) + ","
		if self.max_spread != "": 
			desc += "MAX_SPREAD=" + py_to_str(self.max_spread) + ","
		if self.partial_bucket_handling != self.PartialBucketHandling.AS_SEPARATE_BUCKET: 
			desc += "PARTIAL_BUCKET_HANDLING=" + py_to_str(self.partial_bucket_handling) + ","
		if self.group_by != "": 
			desc += "GROUP_BY=" + py_to_str(self.group_by) + ","
		if self.groups_to_display != self.GroupsToDisplay.ALL: 
			desc += "GROUPS_TO_DISPLAY=" + py_to_str(self.groups_to_display) + ","
		if self.bucket_end_per_group != False: 
			desc += "BUCKET_END_PER_GROUP=" + py_to_str(self.bucket_end_per_group) + ","
		if self.book_delimiters != self.BookDelimiters.NONE: 
			desc += "BOOK_DELIMITERS=" + py_to_str(self.book_delimiters) + ","
		if self.max_initialization_days != "": 
			desc += "MAX_INITIALIZATION_DAYS=" + py_to_str(self.max_initialization_days) + ","
		if self.size_max_fractional_digits != 0: 
			desc += "SIZE_MAX_FRACTIONAL_DIGITS=" + py_to_str(self.size_max_fractional_digits) + ","
		if self.book_uncross_method != "": 
			desc += "BOOK_UNCROSS_METHOD=" + py_to_str(self.book_uncross_method) + ","
		if self.state_key_max_inactivity_sec != "": 
			desc += "STATE_KEY_MAX_INACTIVITY_SEC=" + py_to_str(self.state_key_max_inactivity_sec) + ","
		if self.dq_events_that_clear_book != "": 
			desc += "DQ_EVENTS_THAT_CLEAR_BOOK=" + py_to_str(self.dq_events_that_clear_book) + ","
		if self.include_market_order_ticks != False: 
			desc += "INCLUDE_MARKET_ORDER_TICKS=" + py_to_str(self.include_market_order_ticks) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class ObSnapshotFlat(_graph_components.EpBase):
	"""
		

OB_SNAPSHOT_FLAT

Type: Aggregation

Description: Returns the snapshot for a specified number of
book levels as a single tick with multiple field groups corresponding
to book levels.

Python
class name:&nbsp;ObSnapshotFlat

Input: A time series of order book ticks.

Output: A time series of ticks, one for each bucket, with
fields named:


BID_PRICE1, BID_SIZE1, BID_UPDATE_TIME1,

ASK_PRICE1, ASK_SIZE1, ASK_UPDATE_TIME1,

BID_PRICEN, BID_SIZEN, BID_UPDATE_TIMEN,

ASK_PRICEN, ASK_SIZEN, ASK_UPDATE_TIMEN


Parameters: See Parameters
common to many order book aggregations and Parameters common to generic aggregations.


  BUCKET_INTERVAL
(seconds)
  BUCKET_INTERVAL_UNITS
(enumerated type)
  BUCKET_TIME
(Boolean)
  BUCKET_END_CRITERIA
(expression)
  IS_RUNNING_AGGR
(Boolean)
  MAX_LEVELS
(integer)
  PARTIAL_BUCKET_HANDLING
(Boolean)
  GROUP_BY
(string)
  GROUPS_TO_DISPLAY
(enumerated)
  BUCKET_END_PER_GROUP
(Boolean)
  BOOK_UNCROSS_METHOD
(enumerated)
  DQ_EVENTS_THAT_CLEAR_BOOK
(string)
  INCLUDE_MARKET_ORDER_TICKS&nbsp;(Boolean)
  SHOW_FULL_DETAIL (Boolean)
    When set to true, output ticks will contain all fields, except
BUY_SELL_FLAG, from the input ticks, for each price level.
Note: setting this flag to "true" has no effect on a time series that
does not have a state key
Default: false

  
  
    MAX_INITIALIZATION_DAYS (integer)
    
Specifies up to how many days to go back in order to find initial book
state. If this value is greater than 0, one additional day is
automatically added. The query will not go back resulting number of
days if it finds initial book state earlier.
Default: 1

  
  
    STATE_KEY_MAX_INACTIVITY_SEC (integer)
    
If set, specifies in how many seconds after it was added a given state
key should be automatically removed from the book
Default: None

  
  
    SIZE_MAX_FRACTIONAL_DIGITS (integer)
    
Specifies maximum number of digits after dot in SIZE, if SIZE can be
fractional,
Default: 0

  

See also, notes on order book
aggregations.

Examples: On every tick recompute the snapshot for the top 10
levels on each side. For running queries, such as this one, the output
will only be generated for a tick that changes one of the levels
requested by the user.

OB_SNAPSHOT_FLAT (0, true, 10,
AS_SEPARATE_BUCKET,,false)

See the OB_SNAPSHOT_FLAT example
in OB_AGGREGATION_EXAMPLES.otq.


	"""
	class Parameters:
		bucket_interval = "BUCKET_INTERVAL"
		bucket_interval_units = "BUCKET_INTERVAL_UNITS"
		bucket_time = "BUCKET_TIME"
		bucket_end_criteria = "BUCKET_END_CRITERIA"
		boundary_tick_bucket = "BOUNDARY_TICK_BUCKET"
		is_running_aggr = "IS_RUNNING_AGGR"
		max_levels = "MAX_LEVELS"
		partial_bucket_handling = "PARTIAL_BUCKET_HANDLING"
		group_by = "GROUP_BY"
		groups_to_display = "GROUPS_TO_DISPLAY"
		bucket_end_per_group = "BUCKET_END_PER_GROUP"
		show_full_detail = "SHOW_FULL_DETAIL"
		max_initialization_days = "MAX_INITIALIZATION_DAYS"
		size_max_fractional_digits = "SIZE_MAX_FRACTIONAL_DIGITS"
		book_uncross_method = "BOOK_UNCROSS_METHOD"
		state_key_max_inactivity_sec = "STATE_KEY_MAX_INACTIVITY_SEC"
		dq_events_that_clear_book = "DQ_EVENTS_THAT_CLEAR_BOOK"
		include_market_order_ticks = "INCLUDE_MARKET_ORDER_TICKS"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["bucket_interval", "bucket_interval_units", "bucket_time", "bucket_end_criteria", "boundary_tick_bucket", "is_running_aggr", "max_levels", "partial_bucket_handling", "group_by", "groups_to_display", "bucket_end_per_group", "show_full_detail", "max_initialization_days", "size_max_fractional_digits", "book_uncross_method", "state_key_max_inactivity_sec", "dq_events_that_clear_book", "include_market_order_ticks"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["bucket_interval", "_default_bucket_interval", "bucket_interval_units", "_default_bucket_interval_units", "bucket_time", "_default_bucket_time", "bucket_end_criteria", "_default_bucket_end_criteria", "boundary_tick_bucket", "_default_boundary_tick_bucket", "is_running_aggr", "_default_is_running_aggr", "max_levels", "_default_max_levels", "partial_bucket_handling", "_default_partial_bucket_handling", "group_by", "_default_group_by", "groups_to_display", "_default_groups_to_display", "bucket_end_per_group", "_default_bucket_end_per_group", "show_full_detail", "_default_show_full_detail", "max_initialization_days", "_default_max_initialization_days", "size_max_fractional_digits", "_default_size_max_fractional_digits", "book_uncross_method", "_default_book_uncross_method", "state_key_max_inactivity_sec", "_default_state_key_max_inactivity_sec", "dq_events_that_clear_book", "_default_dq_events_that_clear_book", "include_market_order_ticks", "_default_include_market_order_ticks", "stack_info", "_used_strings"]

	class BucketIntervalUnits:
		DAYS = "DAYS"
		FLEXIBLE = "FLEXIBLE"
		MONTHS = "MONTHS"
		SECONDS = "SECONDS"

	class BucketTime:
		BUCKET_END = "BUCKET_END"
		BUCKET_START = "BUCKET_START"

	class BoundaryTickBucket:
		NEW = "NEW"
		PREVIOUS = "PREVIOUS"

	class PartialBucketHandling:
		AS_SEPARATE_BUCKET = "AS_SEPARATE_BUCKET"
		DISCARD = "DISCARD"
		MERGE_WITH_PREVIOUS = "MERGE_WITH_PREVIOUS"

	class GroupsToDisplay:
		ALL = "ALL"
		EVENT_IN_LAST_BUCKET = "EVENT_IN_LAST_BUCKET"

	def __init__(self, bucket_interval=0, bucket_interval_units=BucketIntervalUnits.SECONDS, bucket_time=BucketTime.BUCKET_END, bucket_end_criteria="", boundary_tick_bucket=BoundaryTickBucket.NEW, is_running_aggr=False, max_levels="", partial_bucket_handling=PartialBucketHandling.AS_SEPARATE_BUCKET, group_by="", groups_to_display=GroupsToDisplay.ALL, bucket_end_per_group=False, show_full_detail=False, max_initialization_days="", size_max_fractional_digits=0, book_uncross_method="", state_key_max_inactivity_sec="", dq_events_that_clear_book="", include_market_order_ticks=False):
		_graph_components.EpBase.__init__(self, "OB_SNAPSHOT_FLAT")
		self._default_bucket_interval = 0
		self.bucket_interval = bucket_interval
		self._default_bucket_interval_units = type(self).BucketIntervalUnits.SECONDS
		self.bucket_interval_units = bucket_interval_units
		self._default_bucket_time = type(self).BucketTime.BUCKET_END
		self.bucket_time = bucket_time
		self._default_bucket_end_criteria = ""
		self.bucket_end_criteria = bucket_end_criteria
		self._default_boundary_tick_bucket = type(self).BoundaryTickBucket.NEW
		self.boundary_tick_bucket = boundary_tick_bucket
		self._default_is_running_aggr = False
		self.is_running_aggr = is_running_aggr
		self._default_max_levels = ""
		self.max_levels = max_levels
		self._default_partial_bucket_handling = type(self).PartialBucketHandling.AS_SEPARATE_BUCKET
		self.partial_bucket_handling = partial_bucket_handling
		self._default_group_by = ""
		self.group_by = group_by
		self._default_groups_to_display = type(self).GroupsToDisplay.ALL
		self.groups_to_display = groups_to_display
		self._default_bucket_end_per_group = False
		self.bucket_end_per_group = bucket_end_per_group
		self._default_show_full_detail = False
		self.show_full_detail = show_full_detail
		self._default_max_initialization_days = ""
		self.max_initialization_days = max_initialization_days
		self._default_size_max_fractional_digits = 0
		self.size_max_fractional_digits = size_max_fractional_digits
		self._default_book_uncross_method = ""
		self.book_uncross_method = book_uncross_method
		self._default_state_key_max_inactivity_sec = ""
		self.state_key_max_inactivity_sec = state_key_max_inactivity_sec
		self._default_dq_events_that_clear_book = ""
		self.dq_events_that_clear_book = dq_events_that_clear_book
		self._default_include_market_order_ticks = False
		self.include_market_order_ticks = include_market_order_ticks
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_bucket_interval(self, value):
		self.bucket_interval = value
		return self

	def set_bucket_interval_units(self, value):
		self.bucket_interval_units = value
		return self

	def set_bucket_time(self, value):
		self.bucket_time = value
		return self

	def set_bucket_end_criteria(self, value):
		self.bucket_end_criteria = value
		return self

	def set_boundary_tick_bucket(self, value):
		self.boundary_tick_bucket = value
		return self

	def set_is_running_aggr(self, value):
		self.is_running_aggr = value
		return self

	def set_max_levels(self, value):
		self.max_levels = value
		return self

	def set_partial_bucket_handling(self, value):
		self.partial_bucket_handling = value
		return self

	def set_group_by(self, value):
		self.group_by = value
		return self

	def set_groups_to_display(self, value):
		self.groups_to_display = value
		return self

	def set_bucket_end_per_group(self, value):
		self.bucket_end_per_group = value
		return self

	def set_show_full_detail(self, value):
		self.show_full_detail = value
		return self

	def set_max_initialization_days(self, value):
		self.max_initialization_days = value
		return self

	def set_size_max_fractional_digits(self, value):
		self.size_max_fractional_digits = value
		return self

	def set_book_uncross_method(self, value):
		self.book_uncross_method = value
		return self

	def set_state_key_max_inactivity_sec(self, value):
		self.state_key_max_inactivity_sec = value
		return self

	def set_dq_events_that_clear_book(self, value):
		self.dq_events_that_clear_book = value
		return self

	def set_include_market_order_ticks(self, value):
		self.include_market_order_ticks = value
		return self

	@staticmethod
	def _get_name():
		return "OB_SNAPSHOT_FLAT"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.bucket_interval != 0: 
			desc += "BUCKET_INTERVAL=" + py_to_str(self.bucket_interval) + ","
		if self.bucket_interval_units != self.BucketIntervalUnits.SECONDS: 
			desc += "BUCKET_INTERVAL_UNITS=" + py_to_str(self.bucket_interval_units) + ","
		if self.bucket_time != self.BucketTime.BUCKET_END: 
			desc += "BUCKET_TIME=" + py_to_str(self.bucket_time) + ","
		if self.bucket_end_criteria != "": 
			desc += "BUCKET_END_CRITERIA=" + py_to_str(self.bucket_end_criteria) + ","
		if self.boundary_tick_bucket != self.BoundaryTickBucket.NEW: 
			desc += "BOUNDARY_TICK_BUCKET=" + py_to_str(self.boundary_tick_bucket) + ","
		if self.is_running_aggr != False: 
			desc += "IS_RUNNING_AGGR=" + py_to_str(self.is_running_aggr) + ","
		if self.max_levels != "": 
			desc += "MAX_LEVELS=" + py_to_str(self.max_levels) + ","
		if self.partial_bucket_handling != self.PartialBucketHandling.AS_SEPARATE_BUCKET: 
			desc += "PARTIAL_BUCKET_HANDLING=" + py_to_str(self.partial_bucket_handling) + ","
		if self.group_by != "": 
			desc += "GROUP_BY=" + py_to_str(self.group_by) + ","
		if self.groups_to_display != self.GroupsToDisplay.ALL: 
			desc += "GROUPS_TO_DISPLAY=" + py_to_str(self.groups_to_display) + ","
		if self.bucket_end_per_group != False: 
			desc += "BUCKET_END_PER_GROUP=" + py_to_str(self.bucket_end_per_group) + ","
		if self.show_full_detail != False: 
			desc += "SHOW_FULL_DETAIL=" + py_to_str(self.show_full_detail) + ","
		if self.max_initialization_days != "": 
			desc += "MAX_INITIALIZATION_DAYS=" + py_to_str(self.max_initialization_days) + ","
		if self.size_max_fractional_digits != 0: 
			desc += "SIZE_MAX_FRACTIONAL_DIGITS=" + py_to_str(self.size_max_fractional_digits) + ","
		if self.book_uncross_method != "": 
			desc += "BOOK_UNCROSS_METHOD=" + py_to_str(self.book_uncross_method) + ","
		if self.state_key_max_inactivity_sec != "": 
			desc += "STATE_KEY_MAX_INACTIVITY_SEC=" + py_to_str(self.state_key_max_inactivity_sec) + ","
		if self.dq_events_that_clear_book != "": 
			desc += "DQ_EVENTS_THAT_CLEAR_BOOK=" + py_to_str(self.dq_events_that_clear_book) + ","
		if self.include_market_order_ticks != False: 
			desc += "INCLUDE_MARKET_ORDER_TICKS=" + py_to_str(self.include_market_order_ticks) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class DataSnapshot(_graph_components.EpBase):
	"""
		

DATA_SNAPSHOT

Type: Aggregation

Description: Returns time series state at the end of each
bucket interval. Based on a specified set of fields (referred below as
state key fields), this EP outputs the latest tick in effect at the end
of the bucket interval, for each unique combination of values of those
fields. This EP does not search back from the start time for the
initial state, and thus is expected to be used after the PREPEND_INITIAL_STATE EP.

Python
class name:&nbsp;DataSnapshot

Input: A time series of ticks.

Output: A time series of time series states, a time series
state for each bucket.

Parameters: See Parameters
common to generic aggregations.


  BUCKET_INTERVAL
(seconds)
  BUCKET_INTERVAL_UNITS
(enumerated type)
  IS_RUNNING_AGGR
(Boolean)
  BUCKET_TIME
(Boolean)
  BUCKET_END_CRITERIA
(expression)
  BOUNDARY_TICK_BUCKET
(NEW/PREVIOUS)
  PARTIAL_BUCKET_HANDLING
(Boolean)
  SHOW_ONLY_CHANGES (Boolean)
    When set to true, the output
stream carries only changes to the state of the time series. The
representation is as follows:

    
      Changed and added levels are represented by themselves.
      Deleted levels are shown with the value of field specified by
DELETE_STATE_KEY_INDICATOR_FIELD_NAME set to the value specified by
DELETE_STATE_KEY_INDICATOR_FIELD_VALUE.
    
    Correct detection of update boundaries may require setting the
BOOK_DELIMITERS option.
Default: false

  
  BOOK_DELIMITERS (enumerated)
    When set to "D," an extra tick is created at the end of each
time series state. Also, an additional column, called DELIMITER, is
added to output ticks. The extra tick has values of all fields set to
the defaults (0,NaN,""), except the delimiter field, which is set to
"D." All other ticks have the DELIMITER set to zero (0).
Default: NONE

  
  EMPTY_STATE_INDICATOR_FIELD_NAME
(string)
    A field that contains an indicator that the state of a time
series is empty.
Default: TICK_STATUS

  
  EMPTY_STATE_INDICATOR_FIELD_VALUE
(string or number)
    The value of the field specified by parameter EMPTY_STATE_INDICATOR_FIELD_NAME
that indicates that the state of a time series is empty.
Default: 31

  
  DELETE_STATE_KEY_INDICATOR_FIELD_NAME
(string)
    A field that contains an indicator that the state key of a given
tick is no longer a part of the state of a time series.

  
  DELETE_STATE_KEY_INDICATOR_FIELD_VALUE
(string or number)
    The value of the field specified by the DELETE_STATE_KEY_INDICATOR_FIELD_NAME
parameter, which indicates that the state key of a given tick is no
longer a part of the state of a time series.

  

Examples: See the book_snapshot_show_only_changes,
book_snapshot_running,
and book_snapshot_running_show_only_changes
examples in data_snapshot.otq.


	"""
	class Parameters:
		bucket_interval = "BUCKET_INTERVAL"
		bucket_interval_units = "BUCKET_INTERVAL_UNITS"
		is_running_aggr = "IS_RUNNING_AGGR"
		bucket_time = "BUCKET_TIME"
		bucket_end_criteria = "BUCKET_END_CRITERIA"
		boundary_tick_bucket = "BOUNDARY_TICK_BUCKET"
		partial_bucket_handling = "PARTIAL_BUCKET_HANDLING"
		show_only_changes = "SHOW_ONLY_CHANGES"
		book_delimiters = "BOOK_DELIMITERS"
		empty_state_indicator_field_name = "EMPTY_STATE_INDICATOR_FIELD_NAME"
		empty_state_indicator_field_value = "EMPTY_STATE_INDICATOR_FIELD_VALUE"
		delete_state_key_indicator_field_name = "DELETE_STATE_KEY_INDICATOR_FIELD_NAME"
		delete_state_key_indicator_field_value = "DELETE_STATE_KEY_INDICATOR_FIELD_VALUE"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["bucket_interval", "bucket_interval_units", "is_running_aggr", "bucket_time", "bucket_end_criteria", "boundary_tick_bucket", "partial_bucket_handling", "show_only_changes", "book_delimiters", "empty_state_indicator_field_name", "empty_state_indicator_field_value", "delete_state_key_indicator_field_name", "delete_state_key_indicator_field_value"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["bucket_interval", "_default_bucket_interval", "bucket_interval_units", "_default_bucket_interval_units", "is_running_aggr", "_default_is_running_aggr", "bucket_time", "_default_bucket_time", "bucket_end_criteria", "_default_bucket_end_criteria", "boundary_tick_bucket", "_default_boundary_tick_bucket", "partial_bucket_handling", "_default_partial_bucket_handling", "show_only_changes", "_default_show_only_changes", "book_delimiters", "_default_book_delimiters", "empty_state_indicator_field_name", "_default_empty_state_indicator_field_name", "empty_state_indicator_field_value", "_default_empty_state_indicator_field_value", "delete_state_key_indicator_field_name", "_default_delete_state_key_indicator_field_name", "delete_state_key_indicator_field_value", "_default_delete_state_key_indicator_field_value", "stack_info", "_used_strings"]

	class BucketIntervalUnits:
		DAYS = "DAYS"
		FLEXIBLE = "FLEXIBLE"
		MONTHS = "MONTHS"
		SECONDS = "SECONDS"
		TICKS = "TICKS"

	class BucketTime:
		BUCKET_END = "BUCKET_END"
		BUCKET_START = "BUCKET_START"

	class BoundaryTickBucket:
		NEW = "NEW"
		PREVIOUS = "PREVIOUS"

	class PartialBucketHandling:
		AS_SEPARATE_BUCKET = "AS_SEPARATE_BUCKET"
		DISCARD = "DISCARD"
		MERGE_WITH_PREVIOUS = "MERGE_WITH_PREVIOUS"

	class BookDelimiters:
		D = "D"
		NONE = "NONE"

	def __init__(self, bucket_interval=0, bucket_interval_units=BucketIntervalUnits.SECONDS, is_running_aggr=False, bucket_time=BucketTime.BUCKET_END, bucket_end_criteria="", boundary_tick_bucket=BoundaryTickBucket.NEW, partial_bucket_handling=PartialBucketHandling.AS_SEPARATE_BUCKET, show_only_changes=False, book_delimiters=BookDelimiters.NONE, empty_state_indicator_field_name="TICK_STATUS", empty_state_indicator_field_value=31, delete_state_key_indicator_field_name="", delete_state_key_indicator_field_value=""):
		_graph_components.EpBase.__init__(self, "DATA_SNAPSHOT")
		self._default_bucket_interval = 0
		self.bucket_interval = bucket_interval
		self._default_bucket_interval_units = type(self).BucketIntervalUnits.SECONDS
		self.bucket_interval_units = bucket_interval_units
		self._default_is_running_aggr = False
		self.is_running_aggr = is_running_aggr
		self._default_bucket_time = type(self).BucketTime.BUCKET_END
		self.bucket_time = bucket_time
		self._default_bucket_end_criteria = ""
		self.bucket_end_criteria = bucket_end_criteria
		self._default_boundary_tick_bucket = type(self).BoundaryTickBucket.NEW
		self.boundary_tick_bucket = boundary_tick_bucket
		self._default_partial_bucket_handling = type(self).PartialBucketHandling.AS_SEPARATE_BUCKET
		self.partial_bucket_handling = partial_bucket_handling
		self._default_show_only_changes = False
		self.show_only_changes = show_only_changes
		self._default_book_delimiters = type(self).BookDelimiters.NONE
		self.book_delimiters = book_delimiters
		self._default_empty_state_indicator_field_name = "TICK_STATUS"
		self.empty_state_indicator_field_name = empty_state_indicator_field_name
		self._default_empty_state_indicator_field_value = 31
		self.empty_state_indicator_field_value = empty_state_indicator_field_value
		self._default_delete_state_key_indicator_field_name = ""
		self.delete_state_key_indicator_field_name = delete_state_key_indicator_field_name
		self._default_delete_state_key_indicator_field_value = ""
		self.delete_state_key_indicator_field_value = delete_state_key_indicator_field_value
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_bucket_interval(self, value):
		self.bucket_interval = value
		return self

	def set_bucket_interval_units(self, value):
		self.bucket_interval_units = value
		return self

	def set_is_running_aggr(self, value):
		self.is_running_aggr = value
		return self

	def set_bucket_time(self, value):
		self.bucket_time = value
		return self

	def set_bucket_end_criteria(self, value):
		self.bucket_end_criteria = value
		return self

	def set_boundary_tick_bucket(self, value):
		self.boundary_tick_bucket = value
		return self

	def set_partial_bucket_handling(self, value):
		self.partial_bucket_handling = value
		return self

	def set_show_only_changes(self, value):
		self.show_only_changes = value
		return self

	def set_book_delimiters(self, value):
		self.book_delimiters = value
		return self

	def set_empty_state_indicator_field_name(self, value):
		self.empty_state_indicator_field_name = value
		return self

	def set_empty_state_indicator_field_value(self, value):
		self.empty_state_indicator_field_value = value
		return self

	def set_delete_state_key_indicator_field_name(self, value):
		self.delete_state_key_indicator_field_name = value
		return self

	def set_delete_state_key_indicator_field_value(self, value):
		self.delete_state_key_indicator_field_value = value
		return self

	@staticmethod
	def _get_name():
		return "DATA_SNAPSHOT"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.bucket_interval != 0: 
			desc += "BUCKET_INTERVAL=" + py_to_str(self.bucket_interval) + ","
		if self.bucket_interval_units != self.BucketIntervalUnits.SECONDS: 
			desc += "BUCKET_INTERVAL_UNITS=" + py_to_str(self.bucket_interval_units) + ","
		if self.is_running_aggr != False: 
			desc += "IS_RUNNING_AGGR=" + py_to_str(self.is_running_aggr) + ","
		if self.bucket_time != self.BucketTime.BUCKET_END: 
			desc += "BUCKET_TIME=" + py_to_str(self.bucket_time) + ","
		if self.bucket_end_criteria != "": 
			desc += "BUCKET_END_CRITERIA=" + py_to_str(self.bucket_end_criteria) + ","
		if self.boundary_tick_bucket != self.BoundaryTickBucket.NEW: 
			desc += "BOUNDARY_TICK_BUCKET=" + py_to_str(self.boundary_tick_bucket) + ","
		if self.partial_bucket_handling != self.PartialBucketHandling.AS_SEPARATE_BUCKET: 
			desc += "PARTIAL_BUCKET_HANDLING=" + py_to_str(self.partial_bucket_handling) + ","
		if self.show_only_changes != False: 
			desc += "SHOW_ONLY_CHANGES=" + py_to_str(self.show_only_changes) + ","
		if self.book_delimiters != self.BookDelimiters.NONE: 
			desc += "BOOK_DELIMITERS=" + py_to_str(self.book_delimiters) + ","
		if self.empty_state_indicator_field_name != "TICK_STATUS": 
			desc += "EMPTY_STATE_INDICATOR_FIELD_NAME=" + py_to_str(self.empty_state_indicator_field_name) + ","
		if self.empty_state_indicator_field_value != 31: 
			desc += "EMPTY_STATE_INDICATOR_FIELD_VALUE=" + py_to_str(self.empty_state_indicator_field_value) + ","
		if self.delete_state_key_indicator_field_name != "": 
			desc += "DELETE_STATE_KEY_INDICATOR_FIELD_NAME=" + py_to_str(self.delete_state_key_indicator_field_name) + ","
		if self.delete_state_key_indicator_field_value != "": 
			desc += "DELETE_STATE_KEY_INDICATOR_FIELD_VALUE=" + py_to_str(self.delete_state_key_indicator_field_value) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class ObSize(_graph_components.EpBase):
	"""
		

OB_SIZE

Type: Aggregation

Description: Returns the total size for a specified number of
order book levels at the end of each bucket interval.

Python
class name:&nbsp;ObSize

Input: A time series of order book ticks.

Output: A time series of ticks with a field named VALUE, if
the SIDE parameter is specified, or fields named ASK_VALUE and
BID_VALUE otherwise.

Parameters: See parameters
common to many order book aggregations and parameters common to generic aggregations.


  BUCKET_INTERVAL
(seconds)
  BUCKET_INTERVAL_UNITS
(enumerated type)
  BUCKET_TIME
(Boolean)
  BUCKET_END_CRITERIA
(expression)
  IS_RUNNING_AGGR
(Boolean)
  SIDE
(ASK|BID)
  MAX_LEVELS
(integer)
  MAX_DEPTH_FOR_PRICE
(double)
  MAX_SPREAD
(double)
  MIN_LEVELS(integer)
  PARTIAL_BUCKET_HANDLING
(Boolean)
  GROUP_BY
(string)
  GROUPS_TO_DISPLAY
(enumerated)
  BUCKET_END_PER_GROUP
(Boolean)
  MAX_INITIALIZATION_DAYS
(integer)
  BOOK_UNCROSS_METHOD
(enumerated)
  DQ_EVENTS_THAT_CLEAR_BOOK
(string)
  BEST_ASK_PRICE_FIELD (string)
    If specified, this parameter represents the name of the field
value of which represents the lowest ask price starting from which the
book ask size is to be computed. This value would also be used as the
top price, relative to which MAX_DEPTH_FOR_PRICE would be computed.

  
  
    BEST_BID_PRICE_FIELD (string)
If specified, this parameter represents the name of the field value of
which represents the highest bid price starting from which the book bid
size is to be computed. This value would also be used as the top price,
relative to which MAX_DEPTH_FOR_PRICE would be computed.

  

Notes: See the notes on order
book aggregations.

Examples: Output size of ASK side of the Order Book, computed
to 10 levels, in 5 minute buckets.

OB_SIZE (300, false, ASK, 10)

See the OB_SIZE example in OB_AGGREGATION_EXAMPLES.otq.


	"""
	class Parameters:
		bucket_interval = "BUCKET_INTERVAL"
		bucket_interval_units = "BUCKET_INTERVAL_UNITS"
		bucket_time = "BUCKET_TIME"
		bucket_end_criteria = "BUCKET_END_CRITERIA"
		boundary_tick_bucket = "BOUNDARY_TICK_BUCKET"
		is_running_aggr = "IS_RUNNING_AGGR"
		side = "SIDE"
		max_levels = "MAX_LEVELS"
		partial_bucket_handling = "PARTIAL_BUCKET_HANDLING"
		group_by = "GROUP_BY"
		groups_to_display = "GROUPS_TO_DISPLAY"
		bucket_end_per_group = "BUCKET_END_PER_GROUP"
		max_initialization_days = "MAX_INITIALIZATION_DAYS"
		size_max_fractional_digits = "SIZE_MAX_FRACTIONAL_DIGITS"
		book_uncross_method = "BOOK_UNCROSS_METHOD"
		max_depth_for_price = "MAX_DEPTH_FOR_PRICE"
		max_spread = "MAX_SPREAD"
		min_levels = "MIN_LEVELS"
		state_key_max_inactivity_sec = "STATE_KEY_MAX_INACTIVITY_SEC"
		dq_events_that_clear_book = "DQ_EVENTS_THAT_CLEAR_BOOK"
		best_ask_price_field = "BEST_ASK_PRICE_FIELD"
		best_bid_price_field = "BEST_BID_PRICE_FIELD"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["bucket_interval", "bucket_interval_units", "bucket_time", "bucket_end_criteria", "boundary_tick_bucket", "is_running_aggr", "side", "max_levels", "partial_bucket_handling", "group_by", "groups_to_display", "bucket_end_per_group", "max_initialization_days", "size_max_fractional_digits", "book_uncross_method", "max_depth_for_price", "max_spread", "min_levels", "state_key_max_inactivity_sec", "dq_events_that_clear_book", "best_ask_price_field", "best_bid_price_field"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["bucket_interval", "_default_bucket_interval", "bucket_interval_units", "_default_bucket_interval_units", "bucket_time", "_default_bucket_time", "bucket_end_criteria", "_default_bucket_end_criteria", "boundary_tick_bucket", "_default_boundary_tick_bucket", "is_running_aggr", "_default_is_running_aggr", "side", "_default_side", "max_levels", "_default_max_levels", "partial_bucket_handling", "_default_partial_bucket_handling", "group_by", "_default_group_by", "groups_to_display", "_default_groups_to_display", "bucket_end_per_group", "_default_bucket_end_per_group", "max_initialization_days", "_default_max_initialization_days", "size_max_fractional_digits", "_default_size_max_fractional_digits", "book_uncross_method", "_default_book_uncross_method", "max_depth_for_price", "_default_max_depth_for_price", "max_spread", "_default_max_spread", "min_levels", "_default_min_levels", "state_key_max_inactivity_sec", "_default_state_key_max_inactivity_sec", "dq_events_that_clear_book", "_default_dq_events_that_clear_book", "best_ask_price_field", "_default_best_ask_price_field", "best_bid_price_field", "_default_best_bid_price_field", "stack_info", "_used_strings"]

	class BucketIntervalUnits:
		DAYS = "DAYS"
		FLEXIBLE = "FLEXIBLE"
		MONTHS = "MONTHS"
		SECONDS = "SECONDS"

	class BucketTime:
		BUCKET_END = "BUCKET_END"
		BUCKET_START = "BUCKET_START"

	class BoundaryTickBucket:
		NEW = "NEW"
		PREVIOUS = "PREVIOUS"

	class Side:
		EMPTY = ""
		ASK = "ASK"
		BID = "BID"

	class PartialBucketHandling:
		AS_SEPARATE_BUCKET = "AS_SEPARATE_BUCKET"
		DISCARD = "DISCARD"
		MERGE_WITH_PREVIOUS = "MERGE_WITH_PREVIOUS"

	class GroupsToDisplay:
		ALL = "ALL"
		EVENT_IN_LAST_BUCKET = "EVENT_IN_LAST_BUCKET"

	def __init__(self, bucket_interval=0, bucket_interval_units=BucketIntervalUnits.SECONDS, bucket_time=BucketTime.BUCKET_END, bucket_end_criteria="", boundary_tick_bucket=BoundaryTickBucket.NEW, is_running_aggr=False, side=Side.EMPTY, max_levels="", partial_bucket_handling=PartialBucketHandling.AS_SEPARATE_BUCKET, group_by="", groups_to_display=GroupsToDisplay.ALL, bucket_end_per_group=False, max_initialization_days="", size_max_fractional_digits=0, book_uncross_method="", max_depth_for_price="", max_spread="", min_levels="", state_key_max_inactivity_sec="", dq_events_that_clear_book="", best_ask_price_field="", best_bid_price_field=""):
		_graph_components.EpBase.__init__(self, "OB_SIZE")
		self._default_bucket_interval = 0
		self.bucket_interval = bucket_interval
		self._default_bucket_interval_units = type(self).BucketIntervalUnits.SECONDS
		self.bucket_interval_units = bucket_interval_units
		self._default_bucket_time = type(self).BucketTime.BUCKET_END
		self.bucket_time = bucket_time
		self._default_bucket_end_criteria = ""
		self.bucket_end_criteria = bucket_end_criteria
		self._default_boundary_tick_bucket = type(self).BoundaryTickBucket.NEW
		self.boundary_tick_bucket = boundary_tick_bucket
		self._default_is_running_aggr = False
		self.is_running_aggr = is_running_aggr
		self._default_side = type(self).Side.EMPTY
		self.side = side
		self._default_max_levels = ""
		self.max_levels = max_levels
		self._default_partial_bucket_handling = type(self).PartialBucketHandling.AS_SEPARATE_BUCKET
		self.partial_bucket_handling = partial_bucket_handling
		self._default_group_by = ""
		self.group_by = group_by
		self._default_groups_to_display = type(self).GroupsToDisplay.ALL
		self.groups_to_display = groups_to_display
		self._default_bucket_end_per_group = False
		self.bucket_end_per_group = bucket_end_per_group
		self._default_max_initialization_days = ""
		self.max_initialization_days = max_initialization_days
		self._default_size_max_fractional_digits = 0
		self.size_max_fractional_digits = size_max_fractional_digits
		self._default_book_uncross_method = ""
		self.book_uncross_method = book_uncross_method
		self._default_max_depth_for_price = ""
		self.max_depth_for_price = max_depth_for_price
		self._default_max_spread = ""
		self.max_spread = max_spread
		self._default_min_levels = ""
		self.min_levels = min_levels
		self._default_state_key_max_inactivity_sec = ""
		self.state_key_max_inactivity_sec = state_key_max_inactivity_sec
		self._default_dq_events_that_clear_book = ""
		self.dq_events_that_clear_book = dq_events_that_clear_book
		self._default_best_ask_price_field = ""
		self.best_ask_price_field = best_ask_price_field
		self._default_best_bid_price_field = ""
		self.best_bid_price_field = best_bid_price_field
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_bucket_interval(self, value):
		self.bucket_interval = value
		return self

	def set_bucket_interval_units(self, value):
		self.bucket_interval_units = value
		return self

	def set_bucket_time(self, value):
		self.bucket_time = value
		return self

	def set_bucket_end_criteria(self, value):
		self.bucket_end_criteria = value
		return self

	def set_boundary_tick_bucket(self, value):
		self.boundary_tick_bucket = value
		return self

	def set_is_running_aggr(self, value):
		self.is_running_aggr = value
		return self

	def set_side(self, value):
		self.side = value
		return self

	def set_max_levels(self, value):
		self.max_levels = value
		return self

	def set_partial_bucket_handling(self, value):
		self.partial_bucket_handling = value
		return self

	def set_group_by(self, value):
		self.group_by = value
		return self

	def set_groups_to_display(self, value):
		self.groups_to_display = value
		return self

	def set_bucket_end_per_group(self, value):
		self.bucket_end_per_group = value
		return self

	def set_max_initialization_days(self, value):
		self.max_initialization_days = value
		return self

	def set_size_max_fractional_digits(self, value):
		self.size_max_fractional_digits = value
		return self

	def set_book_uncross_method(self, value):
		self.book_uncross_method = value
		return self

	def set_max_depth_for_price(self, value):
		self.max_depth_for_price = value
		return self

	def set_max_spread(self, value):
		self.max_spread = value
		return self

	def set_min_levels(self, value):
		self.min_levels = value
		return self

	def set_state_key_max_inactivity_sec(self, value):
		self.state_key_max_inactivity_sec = value
		return self

	def set_dq_events_that_clear_book(self, value):
		self.dq_events_that_clear_book = value
		return self

	def set_best_ask_price_field(self, value):
		self.best_ask_price_field = value
		return self

	def set_best_bid_price_field(self, value):
		self.best_bid_price_field = value
		return self

	@staticmethod
	def _get_name():
		return "OB_SIZE"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.bucket_interval != 0: 
			desc += "BUCKET_INTERVAL=" + py_to_str(self.bucket_interval) + ","
		if self.bucket_interval_units != self.BucketIntervalUnits.SECONDS: 
			desc += "BUCKET_INTERVAL_UNITS=" + py_to_str(self.bucket_interval_units) + ","
		if self.bucket_time != self.BucketTime.BUCKET_END: 
			desc += "BUCKET_TIME=" + py_to_str(self.bucket_time) + ","
		if self.bucket_end_criteria != "": 
			desc += "BUCKET_END_CRITERIA=" + py_to_str(self.bucket_end_criteria) + ","
		if self.boundary_tick_bucket != self.BoundaryTickBucket.NEW: 
			desc += "BOUNDARY_TICK_BUCKET=" + py_to_str(self.boundary_tick_bucket) + ","
		if self.is_running_aggr != False: 
			desc += "IS_RUNNING_AGGR=" + py_to_str(self.is_running_aggr) + ","
		if self.side != self.Side.EMPTY: 
			desc += "SIDE=" + py_to_str(self.side) + ","
		if self.max_levels != "": 
			desc += "MAX_LEVELS=" + py_to_str(self.max_levels) + ","
		if self.partial_bucket_handling != self.PartialBucketHandling.AS_SEPARATE_BUCKET: 
			desc += "PARTIAL_BUCKET_HANDLING=" + py_to_str(self.partial_bucket_handling) + ","
		if self.group_by != "": 
			desc += "GROUP_BY=" + py_to_str(self.group_by) + ","
		if self.groups_to_display != self.GroupsToDisplay.ALL: 
			desc += "GROUPS_TO_DISPLAY=" + py_to_str(self.groups_to_display) + ","
		if self.bucket_end_per_group != False: 
			desc += "BUCKET_END_PER_GROUP=" + py_to_str(self.bucket_end_per_group) + ","
		if self.max_initialization_days != "": 
			desc += "MAX_INITIALIZATION_DAYS=" + py_to_str(self.max_initialization_days) + ","
		if self.size_max_fractional_digits != 0: 
			desc += "SIZE_MAX_FRACTIONAL_DIGITS=" + py_to_str(self.size_max_fractional_digits) + ","
		if self.book_uncross_method != "": 
			desc += "BOOK_UNCROSS_METHOD=" + py_to_str(self.book_uncross_method) + ","
		if self.max_depth_for_price != "": 
			desc += "MAX_DEPTH_FOR_PRICE=" + py_to_str(self.max_depth_for_price) + ","
		if self.max_spread != "": 
			desc += "MAX_SPREAD=" + py_to_str(self.max_spread) + ","
		if self.min_levels != "": 
			desc += "MIN_LEVELS=" + py_to_str(self.min_levels) + ","
		if self.state_key_max_inactivity_sec != "": 
			desc += "STATE_KEY_MAX_INACTIVITY_SEC=" + py_to_str(self.state_key_max_inactivity_sec) + ","
		if self.dq_events_that_clear_book != "": 
			desc += "DQ_EVENTS_THAT_CLEAR_BOOK=" + py_to_str(self.dq_events_that_clear_book) + ","
		if self.best_ask_price_field != "": 
			desc += "BEST_ASK_PRICE_FIELD=" + py_to_str(self.best_ask_price_field) + ","
		if self.best_bid_price_field != "": 
			desc += "BEST_BID_PRICE_FIELD=" + py_to_str(self.best_bid_price_field) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class ObVwap(_graph_components.EpBase):
	"""
		

OB_VWAP

Type: Aggregation

Description: Returns the size-weighted price computed over a
specified number of order book levels at the end of each interval.

Python
class name:&nbsp;ObVwap

Input: A time series of order book ticks.

Output: A time series of ticks with a field named VALUE, if
the SIDE parameter is specified, or fields named ASK_VALUE and
BID_VALUE otherwise.

Parameters: See parameters
common to many order book aggregations and parameters common to generic aggregations.


  BUCKET_INTERVAL
(seconds)
  BUCKET_INTERVAL_UNITS
(enumerated type)
  BUCKET_TIME
(Boolean)
  BUCKET_END_CRITERIA
(expression)
  IS_RUNNING_AGGR
(Boolean)
  SIDE
(ASK|BID)
  MAX_LEVELS
(integer)
  PARTIAL_BUCKET_HANDLING
(Boolean)
  GROUP_BY
(string)
  GROUPS_TO_DISPLAY
(enumerated)
  BUCKET_END_PER_GROUP
(Boolean)
  MAX_DEPTH_SHARES
(integer)
  MAX_INITIALIZATION_DAYS
(integer)
  BOOK_UNCROSS_METHOD
(enumerated)
  DQ_EVENTS_THAT_CLEAR_BOOK
(string)

Notes: See the notes on order
book aggregations.

Examples: Every 10 minutes output a VWAP for the top 5 levels
on each side.

OB_VWAP (300, false, ,5, false)

&nbsp;

See the OB_VWAP example in OB_AGGREGATION_EXAMPLES.otq.


	"""
	class Parameters:
		bucket_interval = "BUCKET_INTERVAL"
		bucket_interval_units = "BUCKET_INTERVAL_UNITS"
		bucket_time = "BUCKET_TIME"
		bucket_end_criteria = "BUCKET_END_CRITERIA"
		boundary_tick_bucket = "BOUNDARY_TICK_BUCKET"
		is_running_aggr = "IS_RUNNING_AGGR"
		side = "SIDE"
		max_levels = "MAX_LEVELS"
		partial_bucket_handling = "PARTIAL_BUCKET_HANDLING"
		group_by = "GROUP_BY"
		groups_to_display = "GROUPS_TO_DISPLAY"
		bucket_end_per_group = "BUCKET_END_PER_GROUP"
		max_depth_shares = "MAX_DEPTH_SHARES"
		max_initialization_days = "MAX_INITIALIZATION_DAYS"
		size_max_fractional_digits = "SIZE_MAX_FRACTIONAL_DIGITS"
		book_uncross_method = "BOOK_UNCROSS_METHOD"
		state_key_max_inactivity_sec = "STATE_KEY_MAX_INACTIVITY_SEC"
		dq_events_that_clear_book = "DQ_EVENTS_THAT_CLEAR_BOOK"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["bucket_interval", "bucket_interval_units", "bucket_time", "bucket_end_criteria", "boundary_tick_bucket", "is_running_aggr", "side", "max_levels", "partial_bucket_handling", "group_by", "groups_to_display", "bucket_end_per_group", "max_depth_shares", "max_initialization_days", "size_max_fractional_digits", "book_uncross_method", "state_key_max_inactivity_sec", "dq_events_that_clear_book"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["bucket_interval", "_default_bucket_interval", "bucket_interval_units", "_default_bucket_interval_units", "bucket_time", "_default_bucket_time", "bucket_end_criteria", "_default_bucket_end_criteria", "boundary_tick_bucket", "_default_boundary_tick_bucket", "is_running_aggr", "_default_is_running_aggr", "side", "_default_side", "max_levels", "_default_max_levels", "partial_bucket_handling", "_default_partial_bucket_handling", "group_by", "_default_group_by", "groups_to_display", "_default_groups_to_display", "bucket_end_per_group", "_default_bucket_end_per_group", "max_depth_shares", "_default_max_depth_shares", "max_initialization_days", "_default_max_initialization_days", "size_max_fractional_digits", "_default_size_max_fractional_digits", "book_uncross_method", "_default_book_uncross_method", "state_key_max_inactivity_sec", "_default_state_key_max_inactivity_sec", "dq_events_that_clear_book", "_default_dq_events_that_clear_book", "stack_info", "_used_strings"]

	class BucketIntervalUnits:
		DAYS = "DAYS"
		FLEXIBLE = "FLEXIBLE"
		MONTHS = "MONTHS"
		SECONDS = "SECONDS"

	class BucketTime:
		BUCKET_END = "BUCKET_END"
		BUCKET_START = "BUCKET_START"

	class BoundaryTickBucket:
		NEW = "NEW"
		PREVIOUS = "PREVIOUS"

	class Side:
		EMPTY = ""
		ASK = "ASK"
		BID = "BID"

	class PartialBucketHandling:
		AS_SEPARATE_BUCKET = "AS_SEPARATE_BUCKET"
		DISCARD = "DISCARD"
		MERGE_WITH_PREVIOUS = "MERGE_WITH_PREVIOUS"

	class GroupsToDisplay:
		ALL = "ALL"
		EVENT_IN_LAST_BUCKET = "EVENT_IN_LAST_BUCKET"

	def __init__(self, bucket_interval=0, bucket_interval_units=BucketIntervalUnits.SECONDS, bucket_time=BucketTime.BUCKET_END, bucket_end_criteria="", boundary_tick_bucket=BoundaryTickBucket.NEW, is_running_aggr=False, side=Side.EMPTY, max_levels="", partial_bucket_handling=PartialBucketHandling.AS_SEPARATE_BUCKET, group_by="", groups_to_display=GroupsToDisplay.ALL, bucket_end_per_group=False, max_depth_shares="", max_initialization_days="", size_max_fractional_digits=0, book_uncross_method="", state_key_max_inactivity_sec="", dq_events_that_clear_book=""):
		_graph_components.EpBase.__init__(self, "OB_VWAP")
		self._default_bucket_interval = 0
		self.bucket_interval = bucket_interval
		self._default_bucket_interval_units = type(self).BucketIntervalUnits.SECONDS
		self.bucket_interval_units = bucket_interval_units
		self._default_bucket_time = type(self).BucketTime.BUCKET_END
		self.bucket_time = bucket_time
		self._default_bucket_end_criteria = ""
		self.bucket_end_criteria = bucket_end_criteria
		self._default_boundary_tick_bucket = type(self).BoundaryTickBucket.NEW
		self.boundary_tick_bucket = boundary_tick_bucket
		self._default_is_running_aggr = False
		self.is_running_aggr = is_running_aggr
		self._default_side = type(self).Side.EMPTY
		self.side = side
		self._default_max_levels = ""
		self.max_levels = max_levels
		self._default_partial_bucket_handling = type(self).PartialBucketHandling.AS_SEPARATE_BUCKET
		self.partial_bucket_handling = partial_bucket_handling
		self._default_group_by = ""
		self.group_by = group_by
		self._default_groups_to_display = type(self).GroupsToDisplay.ALL
		self.groups_to_display = groups_to_display
		self._default_bucket_end_per_group = False
		self.bucket_end_per_group = bucket_end_per_group
		self._default_max_depth_shares = ""
		self.max_depth_shares = max_depth_shares
		self._default_max_initialization_days = ""
		self.max_initialization_days = max_initialization_days
		self._default_size_max_fractional_digits = 0
		self.size_max_fractional_digits = size_max_fractional_digits
		self._default_book_uncross_method = ""
		self.book_uncross_method = book_uncross_method
		self._default_state_key_max_inactivity_sec = ""
		self.state_key_max_inactivity_sec = state_key_max_inactivity_sec
		self._default_dq_events_that_clear_book = ""
		self.dq_events_that_clear_book = dq_events_that_clear_book
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_bucket_interval(self, value):
		self.bucket_interval = value
		return self

	def set_bucket_interval_units(self, value):
		self.bucket_interval_units = value
		return self

	def set_bucket_time(self, value):
		self.bucket_time = value
		return self

	def set_bucket_end_criteria(self, value):
		self.bucket_end_criteria = value
		return self

	def set_boundary_tick_bucket(self, value):
		self.boundary_tick_bucket = value
		return self

	def set_is_running_aggr(self, value):
		self.is_running_aggr = value
		return self

	def set_side(self, value):
		self.side = value
		return self

	def set_max_levels(self, value):
		self.max_levels = value
		return self

	def set_partial_bucket_handling(self, value):
		self.partial_bucket_handling = value
		return self

	def set_group_by(self, value):
		self.group_by = value
		return self

	def set_groups_to_display(self, value):
		self.groups_to_display = value
		return self

	def set_bucket_end_per_group(self, value):
		self.bucket_end_per_group = value
		return self

	def set_max_depth_shares(self, value):
		self.max_depth_shares = value
		return self

	def set_max_initialization_days(self, value):
		self.max_initialization_days = value
		return self

	def set_size_max_fractional_digits(self, value):
		self.size_max_fractional_digits = value
		return self

	def set_book_uncross_method(self, value):
		self.book_uncross_method = value
		return self

	def set_state_key_max_inactivity_sec(self, value):
		self.state_key_max_inactivity_sec = value
		return self

	def set_dq_events_that_clear_book(self, value):
		self.dq_events_that_clear_book = value
		return self

	@staticmethod
	def _get_name():
		return "OB_VWAP"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.bucket_interval != 0: 
			desc += "BUCKET_INTERVAL=" + py_to_str(self.bucket_interval) + ","
		if self.bucket_interval_units != self.BucketIntervalUnits.SECONDS: 
			desc += "BUCKET_INTERVAL_UNITS=" + py_to_str(self.bucket_interval_units) + ","
		if self.bucket_time != self.BucketTime.BUCKET_END: 
			desc += "BUCKET_TIME=" + py_to_str(self.bucket_time) + ","
		if self.bucket_end_criteria != "": 
			desc += "BUCKET_END_CRITERIA=" + py_to_str(self.bucket_end_criteria) + ","
		if self.boundary_tick_bucket != self.BoundaryTickBucket.NEW: 
			desc += "BOUNDARY_TICK_BUCKET=" + py_to_str(self.boundary_tick_bucket) + ","
		if self.is_running_aggr != False: 
			desc += "IS_RUNNING_AGGR=" + py_to_str(self.is_running_aggr) + ","
		if self.side != self.Side.EMPTY: 
			desc += "SIDE=" + py_to_str(self.side) + ","
		if self.max_levels != "": 
			desc += "MAX_LEVELS=" + py_to_str(self.max_levels) + ","
		if self.partial_bucket_handling != self.PartialBucketHandling.AS_SEPARATE_BUCKET: 
			desc += "PARTIAL_BUCKET_HANDLING=" + py_to_str(self.partial_bucket_handling) + ","
		if self.group_by != "": 
			desc += "GROUP_BY=" + py_to_str(self.group_by) + ","
		if self.groups_to_display != self.GroupsToDisplay.ALL: 
			desc += "GROUPS_TO_DISPLAY=" + py_to_str(self.groups_to_display) + ","
		if self.bucket_end_per_group != False: 
			desc += "BUCKET_END_PER_GROUP=" + py_to_str(self.bucket_end_per_group) + ","
		if self.max_depth_shares != "": 
			desc += "MAX_DEPTH_SHARES=" + py_to_str(self.max_depth_shares) + ","
		if self.max_initialization_days != "": 
			desc += "MAX_INITIALIZATION_DAYS=" + py_to_str(self.max_initialization_days) + ","
		if self.size_max_fractional_digits != 0: 
			desc += "SIZE_MAX_FRACTIONAL_DIGITS=" + py_to_str(self.size_max_fractional_digits) + ","
		if self.book_uncross_method != "": 
			desc += "BOOK_UNCROSS_METHOD=" + py_to_str(self.book_uncross_method) + ","
		if self.state_key_max_inactivity_sec != "": 
			desc += "STATE_KEY_MAX_INACTIVITY_SEC=" + py_to_str(self.state_key_max_inactivity_sec) + ","
		if self.dq_events_that_clear_book != "": 
			desc += "DQ_EVENTS_THAT_CLEAR_BOOK=" + py_to_str(self.dq_events_that_clear_book) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class AccessInfo(_graph_components.EpBase):
	"""
		

ACCESS_INFO

Type: Other

Description: Shows access information of different OneTick
components. Different options suggest different components. All output
ticks have the timestamp of the query start time.

Python class name: AccessInfo

Input: No input (this EP is a data source and should be the
root of the graph).

Output: A tick for each entry of each component.

Parameters:


  INFO_TYPE (enumerated)
    This parameter specifies the component to get access information
about. Possible options are DATABASES, DATABASES_INCLUDING_DERIVED, FEATURES, EVENT_PROCESSORS, RESOURCES, VARIABLES, PATHS,
and ROLES:

    
      DATABASES
        If the SHOW_FOR_ALL_USERS option is not
specified, then EP shows the list of databases to which the user has
access. In this case, if the DEEP_SCAN is also set, the output ticks
have the following fields to indicate the access level for each
database on each host, for each applicable time interval:

        
          DB_NAME
          READ_ACCESS
          WRITE_ACCESS
          MIN_AGE_SET
          MIN_AGE_MSEC
          MAX_AGE_SET
          MAX_AGE_MSEC
          MIN_START_DATE_SET
          MIN_START_DATE_MSEC
          MAX_END_DATE_SET
          MAX_END_DATE
          MAX_QUERY_DURATION_SET
          MAX_QUERY_DURATION
          MIN_AGE_DB_DAYS
          MAX_AGE_DB_DAYS_SET
          CEP_ACCESS
          MEMDB_ACCESS  
          SERVER_ADDRESS
          INTERVAL_START
          INTERVAL_END
          GUI_VISIBLE
        
        Output example:

        DB_NAME,READ_ACCESS,WRITE_ACCESS,MIN_AGE_SET,MIN_AGE_MSEC,MAX_AGE_SET,MAX_AGE_MSEC,MIN_START_DATE_SET,MIN_START_DATE_MSEC,MAX_END_DATE_SET,MAX_END_DATE,MAX_QUERY_DURATION_SET,MAX_QUERY_DURATION,MIN_AGE_DB_DAYS,MAX_AGE_DB_DAYS_SET,CEP_ACCESS,MEMDB_ACCESS,SERVER_ADDRESS,INTERVAL_START,INTERVAL_END,GUI_VISIBLE
            CTF,1,0,0,0,0,0,0,0,0,0,1,000000003000,,0,1,0,Milkyway2:36142,315532800000,1577836800000,1
            A,1,1,0,0,0,0,1,978307200000,0,0,0,000000000000,,1,0,0,Milkyway2:45013,315532800000,946684800000,1
            D,0,0,0,0,0,0,0,0,1,1577836800000,1,000100000000,,0,0,0,Milkyway2:43393,1262304000000,1577836800000,0
                    When DEEP_SCAN
is not set, the output ticks have the following fields to indicate the
access level of each database in the host where the query will run.

        
            DB_NAME
            READ_ACCESS
            WRITE_ACCESS
            MIN_AGE_SET
            MIN_AGE_MSEC
            MAX_AGE_SET
            MAX_AGE_MSEC
            MIN_START_DATE_SET
            MIN_START_DATE_MSEC
            MAX_END_DATE_SET
            MAX_END_DATE
            MAX_QUERY_DURATION_SET
            MAX_QUERY_DURATION
            MIN_AGE_DB_DAYS
            MAX_AGE_DB_DAYS_SET
            CEP_ACCESS
            MEMDB_ACCESS  
        
        Output example:

        DB_NAME,READ_ACCESS,WRITE_ACCESS,MIN_AGE_SET,MIN_AGE_MSEC,MAX_AGE_SET,MAX_AGE_MSEC,MIN_START_DATE_SET,MIN_START_DATE_MSEC,MAX_END_DATE_SET,MAX_END_DATE,MAX_QUERY_DURATION_SET,MAX_QUERY_DURATION,MIN_AGE_DB_DAYS,MAX_AGE_DB_DAYS_SET,CEP_ACCESS,MEMDB_ACCESS
            A,1,1,0,0,1,2147483647000,0,0,0,0,1,000000003000,-10,1,0,0
            B,0,0,0,0,1,2147483647000,0,0,0,0,1,000000003000,-10,1,0,0
        If the SHOW_FOR_ALL_USERS option is
specified, then the EP shows database information from the access
control file (for all databases from access control file, and not from
the locator). The DEEP_SCAN
option is not allowed in this case. The output ticks will have the
following fields to indicate the access level of each database for each
role in the host where the query will run:

        
            DB_NAME
            READ_ACCESS
            WRITE_ACCESS
            MIN_AGE_SET
            MIN_AGE_MSEC
            MAX_AGE_SET
            MAX_AGE_MSEC
            MIN_START_DATE_SET
            MIN_START_DATE_MSEC
            MAX_END_DATE_SET
            MAX_END_DATE
            MAX_QUERY_DURATION_SET
            MAX_QUERY_DURATION
            MIN_AGE_DB_DAYS
            MAX_AGE_DB_DAYS_SET
            CEP_ACCESS
            MEMDB_ACCESS  
            ROLE
        
        Output example:

        DB_NAME,READ_ACCESS,WRITE_ACCESS,MIN_AGE_SET,MIN_AGE_MSEC,MAX_AGE_SET,MAX_AGE_MSEC,MIN_START_DATE_SET,MIN_START_DATE_MSEC,MAX_END_DATE_SET,MAX_END_DATE,MAX_QUERY_DURATION_SET,MAX_QUERY_DURATION,MIN_AGE_DB_DAYS,MAX_AGE_DB_DAYS_SET,CEP_ACCESS,MEMDB_ACCESS,ROLE
            A,1,1,0,0,1,2147483647000,0,0,0,0,1,000000003000,-10,1,0,0,admin
            B,1,1,0,0,1,2147483647000,0,0,0,0,1,000000003000,-10,1,0,0,admin
            A,1,1,0,0,1,2147483647000,0,0,0,0,1,000000003000,-10,1,0,0,moderator
            A,1,0,0,0,1,2147483647000,0,0,0,0,1,000000003000,-10,1,0,0,dev            
      
      DATABASES_INCLUDING_DERIVED
        Same as DATABASES, except that
output produced for this option also includes information for derived
databases.

      
      FEATURES/EVENT_PROCESSORS
        If the SHOW_FOR_ALL_USERS option is not
set, the EP shows the list of allowed features/event_processors for the
current user. The following output tick fields indicate the properties
and values of an item along with the role with which access is granted:

        
          FEATURE_NAME/EP_NAME
          PROPERTY
          VALUE
          ROLE
        
The EP also shows item level default parameters with the OTHER role name. In addition, the EP also
adds a tick per feature/event_processor with PROPERTY=ACCESS,
        VALUE=allowed/denied, and ROLE="".
        Output example:

        TIMESTAMP,EP_NAME,PROPERTY,VALUE,ROLE,LOCAL::,ACCESS_INFO1070289000000,CODE,ACCESS,allowed,,LOCAL::,ACCESS_INFO1070289000000,CODE,LANGUAGES,C++,OTHER,LOCAL::,ACCESS_INFO1070289000000,CODE,LANGUAGES,Perl,dev,LOCAL::,ACCESS_INFO......TIMESTAMP,FEATURES_NAME,PROPERTY,VALUE,ROLE,LOCAL::,ACCESS_INFO1070289000000,CONFIG_CHANGE,ACCESS,allowed,,LOCAL::,ACCESS_INFO1070289000000,CONFIG_CHANGE,PARAM1,value1,OTHER,LOCAL::,ACCESS_INFO1070289000000,CONFIG_READ,ACCESS,allowed,,LOCAL::,ACCESS_INFO1070289000000,CONFIG_READ,ALLOWED_DIRS,D:/moderator,dev,LOCAL::,ACCESS_INFO
        If SHOW_FOR_ALL_USERS
is set, the EP shows the FEATURES/EVENT_PROCESSORS
specified in the access control file for all roles. The following
output tick fields indicate the category of the parameters for each
item along with the role with which access is granted:

        
          FEATURE_NAME/EP_NAME
          PARAMETERS
          ROLE
        
For each feature and event processor listed in the access control file,
the EP also shows the default item
level parameters with the OTHER role
name. In addition, for each event processor that is represented in the
access control file only through its item-level entry that does not
carry any item-level parameters, and with no
role-level entries, ACCESS_INFO EP outputs a tick with the event
processor's name, empty value of parameters, and role NONE.
        Output example:

        TIMESTAMP,EP_NAME,PARAMETERS,ROLE,LOCAL::,ACCESS_INFO1070289000000,CODE,"LANGUAGES=C++",OTHER,LOCAL::,ACCESS_INFO1070289000000,CODE,"LANGUAGES=Java,C++,Perl",admin,LOCAL::,ACCESS_INFO1070289000000,CODE,"LANGUAGES=Perl",dev,LOCAL::,ACCESS_INFO......TIMESTAMP,FEATURES_NAME,PARAMETERS,ROLE,LOCAL::,ACCESS_INFO1070289000000,CONFIG_CHANGE,"PARAM1=value1",OTHER,LOCAL::,ACCESS_INFO1070289000000,CONFIG_CHANGE,"PARAM2=value2,PARAM3=value3",admin,LOCAL::,ACCESS_INFO1070289000000,CONFIG_CHANGE,,moderator,LOCAL::,ACCESS_INFO1070289000000,CONFIG_READ,"ALLOWED_DIRS=D:/dev,D:/moderator",admin,LOCAL::,ACCESS_INFO
      
      RESOURCES/VARIABLES
        If the SHOW_FOR_ALL_USERS option is not
set, the EP shows the RESOURCES/VARIABLES
specified in the access control file for the current user. The
following output tick fields indicate the category of an item and its
properties and values along with the role with which access is granted:

        
          RESOURCE_NAME/VARIABLE_NAME
          PROPERTY
          VALUE
          ROLE
        
        The EP also shows item level default parameters with the
role name OTHER.

        Output example:

        TIMESTAMP,RESOURCE_NAME,PROPERTY,VALUE,ROLE,LOCAL::,ACCESS_INFO1070289000000,SOCKET_READ,PER_QUERY_LIMIT,"10000",moderator,LOCAL::,ACCESS_INFO1070289000000,SOCKET_WRITE,PER_QUERY_LIMIT,"150000",OTHER,LOCAL::,ACCESS_INFO1070289000000,SOCKET_WRITE,PER_QUERY_LIMIT,"unlimited",moderator,LOCAL::,ACCESS_INFO
        If the SHOW_FOR_ALL_USERS option is
set, the EP shows the RESOURCES/VARIABLES
specified in the access control file for all roles. The following
output tick fields indicate the category of an item and its parameter
along with the role with which access is granted:

        
          FEATURE_NAME/EP_NAME
          PARAMETERS
          ROLE
        
        For each resource, the EP also shows the default item level
parameters with the role name OTHER.

        Output example:

        TIMESTAMP,RESOURCE_NAME,PARAMETERS,ROLE,LOCAL::,ACCESS_INFO1070289000000,CPU_CORES,"PER_QUERY_LIMIT=4",OTHER,LOCAL::,ACCESS_INFO1070289000000,CPU_CORES,"PER_QUERY_LIMIT=8",admin,LOCAL::,ACCESS_INFO1070289000000,CPU_CORES,"PER_QUERY_LIMIT=unlimited",dev,LOCAL::,ACCESS_INFO
      
      PATHS
        Shows the directories from the CSV_FILE_PATH,
        OTQ_FILE_PATH and DATA_FILE_PATH config variables (including
        extra_search_path for the
user, if configured in access control). Output tick fields are:

        
          TYPE - possible values CSV,
            OTQ and DATA
          PATH - path of the directory
        
        Output example:

        1070289000000,CSV,/home/developer/dev/testruns/access_info_test/access_info_test,LOCAL::,ACCESS_INFO1070289000000,CSV,/home/developer/dev/tick/qatests/central/access_info_test,LOCAL::,ACCESS_INFO1070289000000,OTQ,/home/developer/dev/testruns/access_info_test/access_info_test,LOCAL::,ACCESS_INFO
      
      ROLES
        If the SHOW_FOR_ALL_USERS option is not
set, the EP shows the ROLES
specified in the access control file for the current user. Output tick
fields are:

        
          USER
          ROLE
        
        Output example:

        TIMESTAMP,USER,ROLE,LOCAL::,ACCESS_INFO1070289000000,user1,dev,LOCAL::,ACCESS_INFO1070289000000,user1,moderator,LOCAL::,ACCESS_INFO
        If the SHOW_FOR_ALL_USERS option is
set, the EP shows the ROLES
specified in the access control file for all users. Output tick fields
are:

        
          USER
          ROLE
        
        Output example:

        TIMESTAMP,USER,ROLE,LOCAL::,ACCESS_INFO1070289000000,admin,admin,LOCAL::,ACCESS_INFO1070289000000,user1,dev,LOCAL::,ACCESS_INFO1070289000000,user1,moderator,LOCAL::,ACCESS_INFO1070289000000,user2,moderator,LOCAL::,ACCESS_INFO
      
    
  
  DEEP_SCAN (Boolean)
    If true, the EP will process a deeper scan to bring access
information from each accessible host. It can be true only when INFO_TYPE is set to DATABASES.

    Default: false

  
  SHOW_FOR_ALL_USERS (Boolean)
    If true, the EP will show information for all roles instead of
the current user. For more details, see its meaning in different types.

    Default: false

  

Access control:

By default, all users are allowed to use SHOW_FOR_ALL_USERS=true
option. In order to prevent some roles from use of this option, while
still allowing other uses of ACCESS_INFO event processor to everyone,
the following entry should be added to the access control file (using
appropriate role names ):

&lt;event_processors&gt;  &lt;ep ID="ACCESS_INFO" EXECUTE_ACCESS="true"&gt;    &lt;allow role="role_X" SHOW_FOR_ALL_USERS_PERMISSION="false"/&gt;    &lt;allow role="role_Y" SHOW_FOR_ALL_USERS_PERMISSION="false"/&gt;  &lt;/ep&gt;&lt;/event_processors&gt;
Examples:

See the examples in access_info_examples.otq.


	"""
	class Parameters:
		info_type = "INFO_TYPE"
		deep_scan = "DEEP_SCAN"
		show_for_all_users = "SHOW_FOR_ALL_USERS"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["info_type", "deep_scan", "show_for_all_users"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["info_type", "_default_info_type", "deep_scan", "_default_deep_scan", "show_for_all_users", "_default_show_for_all_users", "stack_info", "_used_strings"]

	class InfoType:
		EMPTY = ""
		DATABASES = "DATABASES"
		DATABASES_INCLUDING_DERIVED = "DATABASES_INCLUDING_DERIVED"
		EVENT_PROCESSORS = "EVENT_PROCESSORS"
		FEATURES = "FEATURES"
		GRAPH_TICK_FUNCTIONS = "GRAPH_TICK_FUNCTIONS"
		PATHS = "PATHS"
		RESOURCES = "RESOURCES"
		ROLES = "ROLES"
		VARIABLES = "VARIABLES"

	def __init__(self, info_type=InfoType.EMPTY, deep_scan=False, show_for_all_users=False):
		_graph_components.EpBase.__init__(self, "ACCESS_INFO")
		self._default_info_type = type(self).InfoType.EMPTY
		self.info_type = info_type
		self._default_deep_scan = False
		self.deep_scan = deep_scan
		self._default_show_for_all_users = False
		self.show_for_all_users = show_for_all_users
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_info_type(self, value):
		self.info_type = value
		return self

	def set_deep_scan(self, value):
		self.deep_scan = value
		return self

	def set_show_for_all_users(self, value):
		self.show_for_all_users = value
		return self

	@staticmethod
	def _get_name():
		return "ACCESS_INFO"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.info_type != self.InfoType.EMPTY: 
			desc += "INFO_TYPE=" + py_to_str(self.info_type) + ","
		if self.deep_scan != False: 
			desc += "DEEP_SCAN=" + py_to_str(self.deep_scan) + ","
		if self.show_for_all_users != False: 
			desc += "SHOW_FOR_ALL_USERS=" + py_to_str(self.show_for_all_users) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class ObNumLevels(_graph_components.EpBase):
	"""
		

OB_NUM_LEVELS

Type: Aggregation

Description: Returns the number of levels in the order book
at the end of each bucket.

Python
class name:
ObNumLevels

Input: A time series of order book ticks.

Output: A time series of ticks with a field named VALUE if
the SIDE parameter is specified, or fields named ASK_VALUE and
BID_VALUE otherwise.

Parameters: See parameters
common to many order book aggregations and parameters common to generic aggregations.


  BUCKET_INTERVAL
(seconds)
  IS_RUNNING_AGGR
(Boolean)
  SIDE
(ASK|BID)
  MAX_INITIALIZATION_DAYS
(integer)
  BOOK_UNCROSS_METHOD
(enumerated)
  DQ_EVENTS_THAT_CLEAR_BOOK
(string)

Notes: See the notes on order
book aggregations.

Examples: Every 10 minutes output a tick with the total
number of ask levels at that time.

OB_NUM_LEVELS (600, false, ASK, false)

See the OB_NUM_LEVELS example in OB_AGGREGATION_EXAMPLES.otq.


	"""
	class Parameters:
		bucket_interval = "BUCKET_INTERVAL"
		is_running_aggr = "IS_RUNNING_AGGR"
		side = "SIDE"
		partial_bucket_handling = "PARTIAL_BUCKET_HANDLING"
		max_initialization_days = "MAX_INITIALIZATION_DAYS"
		size_max_fractional_digits = "SIZE_MAX_FRACTIONAL_DIGITS"
		book_uncross_method = "BOOK_UNCROSS_METHOD"
		state_key_max_inactivity_sec = "STATE_KEY_MAX_INACTIVITY_SEC"
		dq_events_that_clear_book = "DQ_EVENTS_THAT_CLEAR_BOOK"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["bucket_interval", "is_running_aggr", "side", "partial_bucket_handling", "max_initialization_days", "size_max_fractional_digits", "book_uncross_method", "state_key_max_inactivity_sec", "dq_events_that_clear_book"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["bucket_interval", "_default_bucket_interval", "is_running_aggr", "_default_is_running_aggr", "side", "_default_side", "partial_bucket_handling", "_default_partial_bucket_handling", "max_initialization_days", "_default_max_initialization_days", "size_max_fractional_digits", "_default_size_max_fractional_digits", "book_uncross_method", "_default_book_uncross_method", "state_key_max_inactivity_sec", "_default_state_key_max_inactivity_sec", "dq_events_that_clear_book", "_default_dq_events_that_clear_book", "stack_info", "_used_strings"]

	class Side:
		EMPTY = ""
		ASK = "ASK"
		BID = "BID"

	class PartialBucketHandling:
		AS_SEPARATE_BUCKET = "AS_SEPARATE_BUCKET"
		DISCARD = "DISCARD"
		MERGE_WITH_PREVIOUS = "MERGE_WITH_PREVIOUS"

	def __init__(self, bucket_interval=0, is_running_aggr=False, side=Side.EMPTY, partial_bucket_handling=PartialBucketHandling.AS_SEPARATE_BUCKET, max_initialization_days="", size_max_fractional_digits=0, book_uncross_method="", state_key_max_inactivity_sec="", dq_events_that_clear_book=""):
		_graph_components.EpBase.__init__(self, "OB_NUM_LEVELS")
		self._default_bucket_interval = 0
		self.bucket_interval = bucket_interval
		self._default_is_running_aggr = False
		self.is_running_aggr = is_running_aggr
		self._default_side = type(self).Side.EMPTY
		self.side = side
		self._default_partial_bucket_handling = type(self).PartialBucketHandling.AS_SEPARATE_BUCKET
		self.partial_bucket_handling = partial_bucket_handling
		self._default_max_initialization_days = ""
		self.max_initialization_days = max_initialization_days
		self._default_size_max_fractional_digits = 0
		self.size_max_fractional_digits = size_max_fractional_digits
		self._default_book_uncross_method = ""
		self.book_uncross_method = book_uncross_method
		self._default_state_key_max_inactivity_sec = ""
		self.state_key_max_inactivity_sec = state_key_max_inactivity_sec
		self._default_dq_events_that_clear_book = ""
		self.dq_events_that_clear_book = dq_events_that_clear_book
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_bucket_interval(self, value):
		self.bucket_interval = value
		return self

	def set_is_running_aggr(self, value):
		self.is_running_aggr = value
		return self

	def set_side(self, value):
		self.side = value
		return self

	def set_partial_bucket_handling(self, value):
		self.partial_bucket_handling = value
		return self

	def set_max_initialization_days(self, value):
		self.max_initialization_days = value
		return self

	def set_size_max_fractional_digits(self, value):
		self.size_max_fractional_digits = value
		return self

	def set_book_uncross_method(self, value):
		self.book_uncross_method = value
		return self

	def set_state_key_max_inactivity_sec(self, value):
		self.state_key_max_inactivity_sec = value
		return self

	def set_dq_events_that_clear_book(self, value):
		self.dq_events_that_clear_book = value
		return self

	@staticmethod
	def _get_name():
		return "OB_NUM_LEVELS"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.bucket_interval != 0: 
			desc += "BUCKET_INTERVAL=" + py_to_str(self.bucket_interval) + ","
		if self.is_running_aggr != False: 
			desc += "IS_RUNNING_AGGR=" + py_to_str(self.is_running_aggr) + ","
		if self.side != self.Side.EMPTY: 
			desc += "SIDE=" + py_to_str(self.side) + ","
		if self.partial_bucket_handling != self.PartialBucketHandling.AS_SEPARATE_BUCKET: 
			desc += "PARTIAL_BUCKET_HANDLING=" + py_to_str(self.partial_bucket_handling) + ","
		if self.max_initialization_days != "": 
			desc += "MAX_INITIALIZATION_DAYS=" + py_to_str(self.max_initialization_days) + ","
		if self.size_max_fractional_digits != 0: 
			desc += "SIZE_MAX_FRACTIONAL_DIGITS=" + py_to_str(self.size_max_fractional_digits) + ","
		if self.book_uncross_method != "": 
			desc += "BOOK_UNCROSS_METHOD=" + py_to_str(self.book_uncross_method) + ","
		if self.state_key_max_inactivity_sec != "": 
			desc += "STATE_KEY_MAX_INACTIVITY_SEC=" + py_to_str(self.state_key_max_inactivity_sec) + ","
		if self.dq_events_that_clear_book != "": 
			desc += "DQ_EVENTS_THAT_CLEAR_BOOK=" + py_to_str(self.dq_events_that_clear_book) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class ObSummary(_graph_components.EpBase):
	"""
		

OB_SUMMARY

Type: Aggregation

Description: Computes summary statistics, such as VWAP, best
and worst price, total size, and number of levels, for one or both
sides of an order book. The number of levels used in computation of the
summary can be controlled by setting maximum depth in terms of both
size and price, as well as through explicit setting of the maximum and
minimum number of levels.

Python
class name:
ObSummary

Input: A time series of order book ticks.

Output: A time series of ticks with fields
ASK_SIZE,BID_SIZE,ASK_VWAP,BID_VWAP,BEST_ASK_PRICE,BEST_BID_PRICE,WORST_ASK_PRICE,WORST_BID_SIZE,NUM_ASK_LEVELS,NUM_BID_LEVELS

Parameters: See parameters
common to many order book aggregations and parameters common to generic aggregations.


  BUCKET_INTERVAL
(seconds)
  BUCKET_INTERVAL_UNITS
(enumerated type)
  BUCKET_TIME
(Boolean)
  BUCKET_END_CRITERIA
(expression)
  IS_RUNNING_AGGR
(Boolean)
  SIDE
(ASK|BID)
  PARTIAL_BUCKET_HANDLING
(Boolean)
  GROUP_BY
(string)
  GROUPS_TO_DISPLAY
(enumerated)
  BUCKET_END_PER_GROUP
(Boolean)
  MAX_DEPTH_SHARES
(integer)
  MAX_DEPTH_FOR_PRICE
(double)
  MAX_SPREAD
(double)
  MAX_INITIALIZATION_DAYS
(integer)
  MAX_LEVELS
(integer)
  MIN_LEVELS
(integer)
  BOOK_UNCROSS_METHOD
(enumerated)
  DQ_EVENTS_THAT_CLEAR_BOOK
(string)
  INCLUDE_MARKET_ORDER_TICKS&nbsp;(Boolean)
  SIZE_MAX_FRACTIONAL_DIGITS
(integer)
  STATE_KEY_MAX_INACTIVITY_SEC
(integer)

Notes: See the notes on order
book aggregations.


	"""
	class Parameters:
		bucket_interval = "BUCKET_INTERVAL"
		bucket_interval_units = "BUCKET_INTERVAL_UNITS"
		bucket_time = "BUCKET_TIME"
		bucket_end_criteria = "BUCKET_END_CRITERIA"
		boundary_tick_bucket = "BOUNDARY_TICK_BUCKET"
		is_running_aggr = "IS_RUNNING_AGGR"
		side = "SIDE"
		max_levels = "MAX_LEVELS"
		partial_bucket_handling = "PARTIAL_BUCKET_HANDLING"
		group_by = "GROUP_BY"
		groups_to_display = "GROUPS_TO_DISPLAY"
		bucket_end_per_group = "BUCKET_END_PER_GROUP"
		max_initialization_days = "MAX_INITIALIZATION_DAYS"
		size_max_fractional_digits = "SIZE_MAX_FRACTIONAL_DIGITS"
		book_uncross_method = "BOOK_UNCROSS_METHOD"
		max_depth_shares = "MAX_DEPTH_SHARES"
		max_depth_for_price = "MAX_DEPTH_FOR_PRICE"
		max_spread = "MAX_SPREAD"
		min_levels = "MIN_LEVELS"
		state_key_max_inactivity_sec = "STATE_KEY_MAX_INACTIVITY_SEC"
		dq_events_that_clear_book = "DQ_EVENTS_THAT_CLEAR_BOOK"
		include_market_order_ticks = "INCLUDE_MARKET_ORDER_TICKS"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["bucket_interval", "bucket_interval_units", "bucket_time", "bucket_end_criteria", "boundary_tick_bucket", "is_running_aggr", "side", "max_levels", "partial_bucket_handling", "group_by", "groups_to_display", "bucket_end_per_group", "max_initialization_days", "size_max_fractional_digits", "book_uncross_method", "max_depth_shares", "max_depth_for_price", "max_spread", "min_levels", "state_key_max_inactivity_sec", "dq_events_that_clear_book", "include_market_order_ticks"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["bucket_interval", "_default_bucket_interval", "bucket_interval_units", "_default_bucket_interval_units", "bucket_time", "_default_bucket_time", "bucket_end_criteria", "_default_bucket_end_criteria", "boundary_tick_bucket", "_default_boundary_tick_bucket", "is_running_aggr", "_default_is_running_aggr", "side", "_default_side", "max_levels", "_default_max_levels", "partial_bucket_handling", "_default_partial_bucket_handling", "group_by", "_default_group_by", "groups_to_display", "_default_groups_to_display", "bucket_end_per_group", "_default_bucket_end_per_group", "max_initialization_days", "_default_max_initialization_days", "size_max_fractional_digits", "_default_size_max_fractional_digits", "book_uncross_method", "_default_book_uncross_method", "max_depth_shares", "_default_max_depth_shares", "max_depth_for_price", "_default_max_depth_for_price", "max_spread", "_default_max_spread", "min_levels", "_default_min_levels", "state_key_max_inactivity_sec", "_default_state_key_max_inactivity_sec", "dq_events_that_clear_book", "_default_dq_events_that_clear_book", "include_market_order_ticks", "_default_include_market_order_ticks", "stack_info", "_used_strings"]

	class BucketIntervalUnits:
		DAYS = "DAYS"
		FLEXIBLE = "FLEXIBLE"
		MONTHS = "MONTHS"
		SECONDS = "SECONDS"

	class BucketTime:
		BUCKET_END = "BUCKET_END"
		BUCKET_START = "BUCKET_START"

	class BoundaryTickBucket:
		NEW = "NEW"
		PREVIOUS = "PREVIOUS"

	class Side:
		EMPTY = ""
		ASK = "ASK"
		BID = "BID"

	class PartialBucketHandling:
		AS_SEPARATE_BUCKET = "AS_SEPARATE_BUCKET"
		DISCARD = "DISCARD"
		MERGE_WITH_PREVIOUS = "MERGE_WITH_PREVIOUS"

	class GroupsToDisplay:
		ALL = "ALL"
		EVENT_IN_LAST_BUCKET = "EVENT_IN_LAST_BUCKET"

	def __init__(self, bucket_interval=0, bucket_interval_units=BucketIntervalUnits.SECONDS, bucket_time=BucketTime.BUCKET_END, bucket_end_criteria="", boundary_tick_bucket=BoundaryTickBucket.NEW, is_running_aggr=False, side=Side.EMPTY, max_levels="", partial_bucket_handling=PartialBucketHandling.AS_SEPARATE_BUCKET, group_by="", groups_to_display=GroupsToDisplay.ALL, bucket_end_per_group=False, max_initialization_days="", size_max_fractional_digits=0, book_uncross_method="", max_depth_shares="", max_depth_for_price="", max_spread="", min_levels="", state_key_max_inactivity_sec="", dq_events_that_clear_book="", include_market_order_ticks=False):
		_graph_components.EpBase.__init__(self, "OB_SUMMARY")
		self._default_bucket_interval = 0
		self.bucket_interval = bucket_interval
		self._default_bucket_interval_units = type(self).BucketIntervalUnits.SECONDS
		self.bucket_interval_units = bucket_interval_units
		self._default_bucket_time = type(self).BucketTime.BUCKET_END
		self.bucket_time = bucket_time
		self._default_bucket_end_criteria = ""
		self.bucket_end_criteria = bucket_end_criteria
		self._default_boundary_tick_bucket = type(self).BoundaryTickBucket.NEW
		self.boundary_tick_bucket = boundary_tick_bucket
		self._default_is_running_aggr = False
		self.is_running_aggr = is_running_aggr
		self._default_side = type(self).Side.EMPTY
		self.side = side
		self._default_max_levels = ""
		self.max_levels = max_levels
		self._default_partial_bucket_handling = type(self).PartialBucketHandling.AS_SEPARATE_BUCKET
		self.partial_bucket_handling = partial_bucket_handling
		self._default_group_by = ""
		self.group_by = group_by
		self._default_groups_to_display = type(self).GroupsToDisplay.ALL
		self.groups_to_display = groups_to_display
		self._default_bucket_end_per_group = False
		self.bucket_end_per_group = bucket_end_per_group
		self._default_max_initialization_days = ""
		self.max_initialization_days = max_initialization_days
		self._default_size_max_fractional_digits = 0
		self.size_max_fractional_digits = size_max_fractional_digits
		self._default_book_uncross_method = ""
		self.book_uncross_method = book_uncross_method
		self._default_max_depth_shares = ""
		self.max_depth_shares = max_depth_shares
		self._default_max_depth_for_price = ""
		self.max_depth_for_price = max_depth_for_price
		self._default_max_spread = ""
		self.max_spread = max_spread
		self._default_min_levels = ""
		self.min_levels = min_levels
		self._default_state_key_max_inactivity_sec = ""
		self.state_key_max_inactivity_sec = state_key_max_inactivity_sec
		self._default_dq_events_that_clear_book = ""
		self.dq_events_that_clear_book = dq_events_that_clear_book
		self._default_include_market_order_ticks = False
		self.include_market_order_ticks = include_market_order_ticks
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_bucket_interval(self, value):
		self.bucket_interval = value
		return self

	def set_bucket_interval_units(self, value):
		self.bucket_interval_units = value
		return self

	def set_bucket_time(self, value):
		self.bucket_time = value
		return self

	def set_bucket_end_criteria(self, value):
		self.bucket_end_criteria = value
		return self

	def set_boundary_tick_bucket(self, value):
		self.boundary_tick_bucket = value
		return self

	def set_is_running_aggr(self, value):
		self.is_running_aggr = value
		return self

	def set_side(self, value):
		self.side = value
		return self

	def set_max_levels(self, value):
		self.max_levels = value
		return self

	def set_partial_bucket_handling(self, value):
		self.partial_bucket_handling = value
		return self

	def set_group_by(self, value):
		self.group_by = value
		return self

	def set_groups_to_display(self, value):
		self.groups_to_display = value
		return self

	def set_bucket_end_per_group(self, value):
		self.bucket_end_per_group = value
		return self

	def set_max_initialization_days(self, value):
		self.max_initialization_days = value
		return self

	def set_size_max_fractional_digits(self, value):
		self.size_max_fractional_digits = value
		return self

	def set_book_uncross_method(self, value):
		self.book_uncross_method = value
		return self

	def set_max_depth_shares(self, value):
		self.max_depth_shares = value
		return self

	def set_max_depth_for_price(self, value):
		self.max_depth_for_price = value
		return self

	def set_max_spread(self, value):
		self.max_spread = value
		return self

	def set_min_levels(self, value):
		self.min_levels = value
		return self

	def set_state_key_max_inactivity_sec(self, value):
		self.state_key_max_inactivity_sec = value
		return self

	def set_dq_events_that_clear_book(self, value):
		self.dq_events_that_clear_book = value
		return self

	def set_include_market_order_ticks(self, value):
		self.include_market_order_ticks = value
		return self

	@staticmethod
	def _get_name():
		return "OB_SUMMARY"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.bucket_interval != 0: 
			desc += "BUCKET_INTERVAL=" + py_to_str(self.bucket_interval) + ","
		if self.bucket_interval_units != self.BucketIntervalUnits.SECONDS: 
			desc += "BUCKET_INTERVAL_UNITS=" + py_to_str(self.bucket_interval_units) + ","
		if self.bucket_time != self.BucketTime.BUCKET_END: 
			desc += "BUCKET_TIME=" + py_to_str(self.bucket_time) + ","
		if self.bucket_end_criteria != "": 
			desc += "BUCKET_END_CRITERIA=" + py_to_str(self.bucket_end_criteria) + ","
		if self.boundary_tick_bucket != self.BoundaryTickBucket.NEW: 
			desc += "BOUNDARY_TICK_BUCKET=" + py_to_str(self.boundary_tick_bucket) + ","
		if self.is_running_aggr != False: 
			desc += "IS_RUNNING_AGGR=" + py_to_str(self.is_running_aggr) + ","
		if self.side != self.Side.EMPTY: 
			desc += "SIDE=" + py_to_str(self.side) + ","
		if self.max_levels != "": 
			desc += "MAX_LEVELS=" + py_to_str(self.max_levels) + ","
		if self.partial_bucket_handling != self.PartialBucketHandling.AS_SEPARATE_BUCKET: 
			desc += "PARTIAL_BUCKET_HANDLING=" + py_to_str(self.partial_bucket_handling) + ","
		if self.group_by != "": 
			desc += "GROUP_BY=" + py_to_str(self.group_by) + ","
		if self.groups_to_display != self.GroupsToDisplay.ALL: 
			desc += "GROUPS_TO_DISPLAY=" + py_to_str(self.groups_to_display) + ","
		if self.bucket_end_per_group != False: 
			desc += "BUCKET_END_PER_GROUP=" + py_to_str(self.bucket_end_per_group) + ","
		if self.max_initialization_days != "": 
			desc += "MAX_INITIALIZATION_DAYS=" + py_to_str(self.max_initialization_days) + ","
		if self.size_max_fractional_digits != 0: 
			desc += "SIZE_MAX_FRACTIONAL_DIGITS=" + py_to_str(self.size_max_fractional_digits) + ","
		if self.book_uncross_method != "": 
			desc += "BOOK_UNCROSS_METHOD=" + py_to_str(self.book_uncross_method) + ","
		if self.max_depth_shares != "": 
			desc += "MAX_DEPTH_SHARES=" + py_to_str(self.max_depth_shares) + ","
		if self.max_depth_for_price != "": 
			desc += "MAX_DEPTH_FOR_PRICE=" + py_to_str(self.max_depth_for_price) + ","
		if self.max_spread != "": 
			desc += "MAX_SPREAD=" + py_to_str(self.max_spread) + ","
		if self.min_levels != "": 
			desc += "MIN_LEVELS=" + py_to_str(self.min_levels) + ","
		if self.state_key_max_inactivity_sec != "": 
			desc += "STATE_KEY_MAX_INACTIVITY_SEC=" + py_to_str(self.state_key_max_inactivity_sec) + ","
		if self.dq_events_that_clear_book != "": 
			desc += "DQ_EVENTS_THAT_CLEAR_BOOK=" + py_to_str(self.dq_events_that_clear_book) + ","
		if self.include_market_order_ticks != False: 
			desc += "INCLUDE_MARKET_ORDER_TICKS=" + py_to_str(self.include_market_order_ticks) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class PortfolioPrice(_graph_components.EpBase):
	"""
		

PORTFOLIO_PRICE

Type: Aggregation

Description: For each bucket, computes weighted portfolio
price.

Python
class name:
PortfolioPrice

Input: Multiple time series of ticks, each time series having
an attribute specified in INPUT_FIELD_NAME parameter. Each tick has an
associated weight property, value of which is either carried in the
field specified in WEIGHT_FIELD_NAME parameter or in symbol parameter
WEIGHT.

Output: A time series of ticks, one tick for each bucket
interval.

Parameters: See parameters
common to generic aggregations.


  BUCKET_INTERVAL
(seconds/ticks)
  BUCKET_INTERVAL_UNITS
(enumerated type)
  OUTPUT_INTERVAL
(seconds)
  OUTPUT_INTERVAL_UNITS
(SECONDS/TICKS)
  IS_RUNNING_AGGR
(Boolean)
  BUCKET_TIME
(Boolean)
  BUCKET_END_CRITERIA
(expression)
  BOUNDARY_TICK_BUCKET
(NEW/PREVIOUS)
  PARTIAL_BUCKET_HANDLING
(enumerated type)
  GROUP_BY
(string)
  GROUPS_TO_DISPLAY
(enumerated)
  WEIGHT_FIELD_NAME
    The name of the field that contains the current value of weight
for a member of the portfolio that contributed the tick. If
WEIGHT_FIELD_NAME is not specified, the symbol should have the WEIGHT
parameter and the value of this parameter is used as the weight. If
WEIGHT_FIELD_NAME is specified, all ticks should have the field pointed
by this parameter and the value of this field is used as the weight.

  
  SIDE (enumerated type)
    When set to LONG, the price of the portfolio is computed only
for the input time series with weight &gt; 0. When set to SHORT, the
price of the portfolio is computed only for the input time series with
weight &lt; 0. When set to BOTH, the price of the portfolio is computed
for all input time series.
Default: BOTH

  
  WEIGHT_TYPE (enumerated type)
    When set to ABSOLUTE, the
portfolio price is computed as the sum of input_field_value*weight
across all members of the portfolio. When set to RELATIVE, the portfolio price is computed
as the sum of input_field_value*weight/sum_of_all_weights
across all members of the portfolio.
Default: ABSOLUTE

  

Notes:

If WEIGHT_FIELD_NAME is not set, weights for the computation of
portfolio price are provided by the previous stage of a multistage
query. The previous stage is expected to produce ticks that would
include columns SYMBOL_NAME and WEIGHT.

If WEIGHT_FIELD_NAME is not set and PORTFOLIO_PRICE is used in a
single-stage query, the weights take the default value 1 for the purposes of calculating
PORTFOLIO_PRICE.

See also the notes on generic
aggregations.

Examples: Compute the portfolio price with 1-minute buckets
for the portfolio files specified as input symbols and symbol date for
all the symbols in the portfolio files set to 20050103:

CSV_FILE_LISTING()QUERY_SYMBOLS(20050103)PORTFOLIO_PRICE(60, SECONDS, , false, AS_SEPARATE_BUCKET, , BOTH)
See the examples in PORTFOLIO_EXAMPLES.otq.

See also CSV_FILE_LISTING and QUERY_SYMBOLS.


	"""
	class Parameters:
		bucket_interval = "BUCKET_INTERVAL"
		bucket_interval_units = "BUCKET_INTERVAL_UNITS"
		output_interval = "OUTPUT_INTERVAL"
		output_interval_units = "OUTPUT_INTERVAL_UNITS"
		is_running_aggr = "IS_RUNNING_AGGR"
		bucket_time = "BUCKET_TIME"
		bucket_end_criteria = "BUCKET_END_CRITERIA"
		boundary_tick_bucket = "BOUNDARY_TICK_BUCKET"
		partial_bucket_handling = "PARTIAL_BUCKET_HANDLING"
		input_field_name = "INPUT_FIELD_NAME"
		group_by = "GROUP_BY"
		groups_to_display = "GROUPS_TO_DISPLAY"
		weight_field_name = "WEIGHT_FIELD_NAME"
		side = "SIDE"
		weight_type = "WEIGHT_TYPE"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["bucket_interval", "bucket_interval_units", "output_interval", "output_interval_units", "is_running_aggr", "bucket_time", "bucket_end_criteria", "boundary_tick_bucket", "partial_bucket_handling", "input_field_name", "group_by", "groups_to_display", "weight_field_name", "side", "weight_type"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["bucket_interval", "_default_bucket_interval", "bucket_interval_units", "_default_bucket_interval_units", "output_interval", "_default_output_interval", "output_interval_units", "_default_output_interval_units", "is_running_aggr", "_default_is_running_aggr", "bucket_time", "_default_bucket_time", "bucket_end_criteria", "_default_bucket_end_criteria", "boundary_tick_bucket", "_default_boundary_tick_bucket", "partial_bucket_handling", "_default_partial_bucket_handling", "input_field_name", "_default_input_field_name", "group_by", "_default_group_by", "groups_to_display", "_default_groups_to_display", "weight_field_name", "_default_weight_field_name", "side", "_default_side", "weight_type", "_default_weight_type", "stack_info", "_used_strings"]

	class BucketIntervalUnits:
		DAYS = "DAYS"
		FLEXIBLE = "FLEXIBLE"
		MONTHS = "MONTHS"
		SECONDS = "SECONDS"
		TICKS = "TICKS"

	class OutputIntervalUnits:
		SECONDS = "SECONDS"
		TICKS = "TICKS"

	class BucketTime:
		BUCKET_END = "BUCKET_END"
		BUCKET_START = "BUCKET_START"

	class BoundaryTickBucket:
		NEW = "NEW"
		PREVIOUS = "PREVIOUS"

	class PartialBucketHandling:
		AS_SEPARATE_BUCKET = "AS_SEPARATE_BUCKET"
		DISCARD = "DISCARD"
		MERGE_WITH_PREVIOUS = "MERGE_WITH_PREVIOUS"

	class GroupsToDisplay:
		ALL = "ALL"
		EVENT_IN_LAST_BUCKET = "EVENT_IN_LAST_BUCKET"

	class Side:
		BOTH = "BOTH"
		LONG = "LONG"
		SHORT = "SHORT"

	class WeightType:
		ABSOLUTE = "ABSOLUTE"
		RELATIVE = "RELATIVE"

	def __init__(self, bucket_interval=0, bucket_interval_units=BucketIntervalUnits.SECONDS, output_interval="", output_interval_units=OutputIntervalUnits.SECONDS, is_running_aggr=False, bucket_time=BucketTime.BUCKET_END, bucket_end_criteria="", boundary_tick_bucket=BoundaryTickBucket.NEW, partial_bucket_handling=PartialBucketHandling.AS_SEPARATE_BUCKET, input_field_name="", group_by="", groups_to_display=GroupsToDisplay.ALL, weight_field_name="", side=Side.BOTH, weight_type=WeightType.ABSOLUTE, In = ""):
		_graph_components.EpBase.__init__(self, "PORTFOLIO_PRICE")
		self._default_bucket_interval = 0
		self.bucket_interval = bucket_interval
		self._default_bucket_interval_units = type(self).BucketIntervalUnits.SECONDS
		self.bucket_interval_units = bucket_interval_units
		self._default_output_interval = ""
		self.output_interval = output_interval
		self._default_output_interval_units = type(self).OutputIntervalUnits.SECONDS
		self.output_interval_units = output_interval_units
		self._default_is_running_aggr = False
		self.is_running_aggr = is_running_aggr
		self._default_bucket_time = type(self).BucketTime.BUCKET_END
		self.bucket_time = bucket_time
		self._default_bucket_end_criteria = ""
		self.bucket_end_criteria = bucket_end_criteria
		self._default_boundary_tick_bucket = type(self).BoundaryTickBucket.NEW
		self.boundary_tick_bucket = boundary_tick_bucket
		self._default_partial_bucket_handling = type(self).PartialBucketHandling.AS_SEPARATE_BUCKET
		self.partial_bucket_handling = partial_bucket_handling
		self._default_input_field_name = ""
		self.input_field_name = input_field_name
		self._default_group_by = ""
		self.group_by = group_by
		self._default_groups_to_display = type(self).GroupsToDisplay.ALL
		self.groups_to_display = groups_to_display
		self._default_weight_field_name = ""
		self.weight_field_name = weight_field_name
		self._default_side = type(self).Side.BOTH
		self.side = side
		self._default_weight_type = type(self).WeightType.ABSOLUTE
		self.weight_type = weight_type
		if In != "":
			self.input_field_name=In
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_bucket_interval(self, value):
		self.bucket_interval = value
		return self

	def set_bucket_interval_units(self, value):
		self.bucket_interval_units = value
		return self

	def set_output_interval(self, value):
		self.output_interval = value
		return self

	def set_output_interval_units(self, value):
		self.output_interval_units = value
		return self

	def set_is_running_aggr(self, value):
		self.is_running_aggr = value
		return self

	def set_bucket_time(self, value):
		self.bucket_time = value
		return self

	def set_bucket_end_criteria(self, value):
		self.bucket_end_criteria = value
		return self

	def set_boundary_tick_bucket(self, value):
		self.boundary_tick_bucket = value
		return self

	def set_partial_bucket_handling(self, value):
		self.partial_bucket_handling = value
		return self

	def set_input_field_name(self, value):
		self.input_field_name = value
		return self

	def set_group_by(self, value):
		self.group_by = value
		return self

	def set_groups_to_display(self, value):
		self.groups_to_display = value
		return self

	def set_weight_field_name(self, value):
		self.weight_field_name = value
		return self

	def set_side(self, value):
		self.side = value
		return self

	def set_weight_type(self, value):
		self.weight_type = value
		return self

	@staticmethod
	def _get_name():
		return "PORTFOLIO_PRICE"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.bucket_interval != 0: 
			desc += "BUCKET_INTERVAL=" + py_to_str(self.bucket_interval) + ","
		if self.bucket_interval_units != self.BucketIntervalUnits.SECONDS: 
			desc += "BUCKET_INTERVAL_UNITS=" + py_to_str(self.bucket_interval_units) + ","
		if self.output_interval != "": 
			desc += "OUTPUT_INTERVAL=" + py_to_str(self.output_interval) + ","
		if self.output_interval_units != self.OutputIntervalUnits.SECONDS: 
			desc += "OUTPUT_INTERVAL_UNITS=" + py_to_str(self.output_interval_units) + ","
		if self.is_running_aggr != False: 
			desc += "IS_RUNNING_AGGR=" + py_to_str(self.is_running_aggr) + ","
		if self.bucket_time != self.BucketTime.BUCKET_END: 
			desc += "BUCKET_TIME=" + py_to_str(self.bucket_time) + ","
		if self.bucket_end_criteria != "": 
			desc += "BUCKET_END_CRITERIA=" + py_to_str(self.bucket_end_criteria) + ","
		if self.boundary_tick_bucket != self.BoundaryTickBucket.NEW: 
			desc += "BOUNDARY_TICK_BUCKET=" + py_to_str(self.boundary_tick_bucket) + ","
		if self.partial_bucket_handling != self.PartialBucketHandling.AS_SEPARATE_BUCKET: 
			desc += "PARTIAL_BUCKET_HANDLING=" + py_to_str(self.partial_bucket_handling) + ","
		if self.input_field_name != "": 
			desc += "INPUT_FIELD_NAME=" + py_to_str(self.input_field_name) + ","
		if self.group_by != "": 
			desc += "GROUP_BY=" + py_to_str(self.group_by) + ","
		if self.groups_to_display != self.GroupsToDisplay.ALL: 
			desc += "GROUPS_TO_DISPLAY=" + py_to_str(self.groups_to_display) + ","
		if self.weight_field_name != "": 
			desc += "WEIGHT_FIELD_NAME=" + py_to_str(self.weight_field_name) + ","
		if self.side != self.Side.BOTH: 
			desc += "SIDE=" + py_to_str(self.side) + ","
		if self.weight_type != self.WeightType.ABSOLUTE: 
			desc += "WEIGHT_TYPE=" + py_to_str(self.weight_type) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class OptionPrice(_graph_components.EpBase):
	"""
		

OPTION_PRICE

Type: Aggregation

Description: For each bucket, computes call/put option price
and (optionally) related Greeks for the last tick in the bucket.

Python
class name:&nbsp;OptionPrice

Input: A time series of ticks having the PRICE attribute.

This EP requires several parameters to compute the option price.
Those are, OPTION_TYPE, STRIKE_PRICE, EXPIRATION_DATE
or DAYS_TILL_EXPIRATION, VOLATILITY, and INTEREST_RATE.
Each parameter can be specified, either via a symbol parameter with the
same name or via a tick field, by specifying the name of that field as
an EP parameter, as follows. Besides, VOLATIITY and INTEREST_RATE
can also be specified on EP basis. In either case, the OPTION_TYPE
value must be set to either CALL or PUT. EXPIRATION_DATE
is in YYYYMMDD format, a string in case of a symbol parameter and an
integer in case of a tick attribute. Additionally, NUMBER_OF_STEPS
should be specified on EP basis in case of Cox-Ross-Rubinstein method.

Output: A time series of ticks, one tick for each bucket
interval.

Parameters: See parameters
common to generic aggregations.


  BUCKET_INTERVAL
(seconds/ticks)
  BUCKET_INTERVAL_UNITS
(enumerated type)
  OUTPUT_INTERVAL
(seconds)
  OUTPUT_INTERVAL_UNITS
(SECONDS/TICKS)
  IS_RUNNING_AGGR
(Boolean)
  BUCKET_TIME
(Boolean)
  BUCKET_END_CRITERIA
(expression)
  BOUNDARY_TICK_BUCKET
(NEW/PREVIOUS)
  PARTIAL_BUCKET_HANDLING
(enumerated type)
  ALL_FIELDS_FOR_RUNNING (Boolean)
    Specifies whether all input tick fields should be present in the
output ticks of this EP&nbsp; when IS_RUNNING_AGGR=true
Default: false

  
  VOLATILITY (numeric)
    The historical volatility of the assets returns.

    
  INTEREST_RATE (numeric)
    The risk-free interest rate.

  
  COMPUTE_MODEL (enumeration)
    
Allowed values are BS and CRR. Choose between BlackScholes (BS) and Cox-Ross-Rubinstein (CRR) models for computing call/put option
price.
Default: BS

  
  NUMBER_OF_STEPS (integer)
    Specifies the number of time steps between the valuation and
expiration dates. This is a mandatory parameter for CRR model.
    

  
  COMPUTE_DELTA (Boolean)
    Specifies whether Delta is to be computed or not. This parameter
is used only in case of BS model.
Default: false

  
  COMPUTE_GAMMA (Boolean)
    Specifies whether Gamma is to be computed or not. This parameter
is used only in case of BS model.
Default: false

  
  COMPUTE_THETA (Boolean)
    Specifies whether Theta is to be computed or not. This parameter
is used only in case of BS model.
Default: false

  
  COMPUTE_VEGA (Boolean)
    Specifies whether Vega is to be computed or not. This parameter
is used only in case of BS model.
Default value: false

  
  COMPUTE_RHO (Boolean)
    Specifies whether Rho is to be computed or not. This parameter
is used only in case of BS model.
Default: false

  
  VOLATILITY_FIELD_NAME (string)
Specifies name of the field, which carries the historical volatility of
the assets returns.
Default: empty
  INTEREST_RATE_FIELD_NAME
(string)
Specifies name of the field, which carries the risk-free interest rate.
Default: empty
  OPTION_TYPE_FIELD_NAME (string)
Specifies name of the field, which carries the option type (either CALL
or PUT).
Default: empty
UNDERLYING_PRICE_FIELD_NAME
(string)
Specifies name of the field, which carries the underlying price of the option.
Default: PRICE
  STRIKE_PRICE_FIELD_NAME
(string)
Specifies name of the field, which carries the strike price of the
option.
Default: empty
  DAYS_IN_YEAR (numeric)
Specifies number of days in a year (say, 365 or 252 (business days,
etc.). Used with DAYS_TILL_EXPIRATION parameter to compute the
fractional years till expiration.
Default: 365
  DAYS_TILL_EXPIRATION_FIELD_NAME
(string)
Specifies name of the field, which carries number of days (it can be
fractional, represented by the double type) till
expiration of the option. If the number of days till expiration is
negative, computed options price is returned as NaN. 
Default: empty
  EXPIRATION_DATE_FIELD_NAME
(string)
Specifies name of the field, which carries the expiration date of the
option, in YYYYMMDD format.
Default: empty

Notes: See the notes on
generic aggregations.

Examples:

Compute option price with 1 minute buckets for the option files
specified as input symbols and symbol date for all the symbols in the
option file set to 20050103.

CSV_FILE_LISTING()

QUERY_SYMBOLS(20050103)

OPTION_PRICE(60, SECONDS, ,false, AS_SEPARATE_BUCKET,
"0.2","0.05", false, false, false, false, false)

See also CSV_FILE_LISTING.


	"""
	class Parameters:
		bucket_interval = "BUCKET_INTERVAL"
		bucket_interval_units = "BUCKET_INTERVAL_UNITS"
		output_interval = "OUTPUT_INTERVAL"
		output_interval_units = "OUTPUT_INTERVAL_UNITS"
		is_running_aggr = "IS_RUNNING_AGGR"
		bucket_time = "BUCKET_TIME"
		bucket_end_criteria = "BUCKET_END_CRITERIA"
		boundary_tick_bucket = "BOUNDARY_TICK_BUCKET"
		partial_bucket_handling = "PARTIAL_BUCKET_HANDLING"
		volatility = "VOLATILITY"
		interest_rate = "INTEREST_RATE"
		compute_model = "COMPUTE_MODEL"
		number_of_steps = "NUMBER_OF_STEPS"
		compute_delta = "COMPUTE_DELTA"
		compute_gamma = "COMPUTE_GAMMA"
		compute_theta = "COMPUTE_THETA"
		compute_vega = "COMPUTE_VEGA"
		compute_rho = "COMPUTE_RHO"
		all_fields_for_running = "ALL_FIELDS_FOR_RUNNING"
		volatility_field_name = "VOLATILITY_FIELD_NAME"
		interest_rate_field_name = "INTEREST_RATE_FIELD_NAME"
		option_type_field_name = "OPTION_TYPE_FIELD_NAME"
		underlying_price_field_name = "UNDERLYING_PRICE_FIELD_NAME"
		strike_price_field_name = "STRIKE_PRICE_FIELD_NAME"
		days_in_year = "DAYS_IN_YEAR"
		days_till_expiration_field_name = "DAYS_TILL_EXPIRATION_FIELD_NAME"
		expiration_date_field_name = "EXPIRATION_DATE_FIELD_NAME"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["bucket_interval", "bucket_interval_units", "output_interval", "output_interval_units", "is_running_aggr", "bucket_time", "bucket_end_criteria", "boundary_tick_bucket", "partial_bucket_handling", "volatility", "interest_rate", "compute_model", "number_of_steps", "compute_delta", "compute_gamma", "compute_theta", "compute_vega", "compute_rho", "all_fields_for_running", "volatility_field_name", "interest_rate_field_name", "option_type_field_name", "underlying_price_field_name", "strike_price_field_name", "days_in_year", "days_till_expiration_field_name", "expiration_date_field_name"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["bucket_interval", "_default_bucket_interval", "bucket_interval_units", "_default_bucket_interval_units", "output_interval", "_default_output_interval", "output_interval_units", "_default_output_interval_units", "is_running_aggr", "_default_is_running_aggr", "bucket_time", "_default_bucket_time", "bucket_end_criteria", "_default_bucket_end_criteria", "boundary_tick_bucket", "_default_boundary_tick_bucket", "partial_bucket_handling", "_default_partial_bucket_handling", "volatility", "_default_volatility", "interest_rate", "_default_interest_rate", "compute_model", "_default_compute_model", "number_of_steps", "_default_number_of_steps", "compute_delta", "_default_compute_delta", "compute_gamma", "_default_compute_gamma", "compute_theta", "_default_compute_theta", "compute_vega", "_default_compute_vega", "compute_rho", "_default_compute_rho", "all_fields_for_running", "_default_all_fields_for_running", "volatility_field_name", "_default_volatility_field_name", "interest_rate_field_name", "_default_interest_rate_field_name", "option_type_field_name", "_default_option_type_field_name", "underlying_price_field_name", "_default_underlying_price_field_name", "strike_price_field_name", "_default_strike_price_field_name", "days_in_year", "_default_days_in_year", "days_till_expiration_field_name", "_default_days_till_expiration_field_name", "expiration_date_field_name", "_default_expiration_date_field_name", "stack_info", "_used_strings"]

	class BucketIntervalUnits:
		DAYS = "DAYS"
		FLEXIBLE = "FLEXIBLE"
		MONTHS = "MONTHS"
		SECONDS = "SECONDS"
		TICKS = "TICKS"

	class OutputIntervalUnits:
		SECONDS = "SECONDS"
		TICKS = "TICKS"

	class BucketTime:
		BUCKET_END = "BUCKET_END"
		BUCKET_START = "BUCKET_START"

	class BoundaryTickBucket:
		NEW = "NEW"
		PREVIOUS = "PREVIOUS"

	class PartialBucketHandling:
		AS_SEPARATE_BUCKET = "AS_SEPARATE_BUCKET"
		DISCARD = "DISCARD"
		MERGE_WITH_PREVIOUS = "MERGE_WITH_PREVIOUS"

	class ComputeModel:
		BS = "BS"
		CRR = "CRR"

	class AllFieldsForRunning:
		WHEN_TICKS_EXIT_WINDOW = "WHEN_TICKS_EXIT_WINDOW"
		FALSE = "false"
		TRUE = "true"

	def __init__(self, bucket_interval=0, bucket_interval_units=BucketIntervalUnits.SECONDS, output_interval="", output_interval_units=OutputIntervalUnits.SECONDS, is_running_aggr=False, bucket_time=BucketTime.BUCKET_END, bucket_end_criteria="", boundary_tick_bucket=BoundaryTickBucket.NEW, partial_bucket_handling=PartialBucketHandling.AS_SEPARATE_BUCKET, volatility="", interest_rate="", compute_model=ComputeModel.BS, number_of_steps="", compute_delta=False, compute_gamma=False, compute_theta=False, compute_vega=False, compute_rho=False, all_fields_for_running=AllFieldsForRunning.FALSE, volatility_field_name="", interest_rate_field_name="", option_type_field_name="", underlying_price_field_name="PRICE", strike_price_field_name="", days_in_year=365, days_till_expiration_field_name="", expiration_date_field_name=""):
		_graph_components.EpBase.__init__(self, "OPTION_PRICE")
		self._default_bucket_interval = 0
		self.bucket_interval = bucket_interval
		self._default_bucket_interval_units = type(self).BucketIntervalUnits.SECONDS
		self.bucket_interval_units = bucket_interval_units
		self._default_output_interval = ""
		self.output_interval = output_interval
		self._default_output_interval_units = type(self).OutputIntervalUnits.SECONDS
		self.output_interval_units = output_interval_units
		self._default_is_running_aggr = False
		self.is_running_aggr = is_running_aggr
		self._default_bucket_time = type(self).BucketTime.BUCKET_END
		self.bucket_time = bucket_time
		self._default_bucket_end_criteria = ""
		self.bucket_end_criteria = bucket_end_criteria
		self._default_boundary_tick_bucket = type(self).BoundaryTickBucket.NEW
		self.boundary_tick_bucket = boundary_tick_bucket
		self._default_partial_bucket_handling = type(self).PartialBucketHandling.AS_SEPARATE_BUCKET
		self.partial_bucket_handling = partial_bucket_handling
		self._default_volatility = ""
		self.volatility = volatility
		self._default_interest_rate = ""
		self.interest_rate = interest_rate
		self._default_compute_model = type(self).ComputeModel.BS
		self.compute_model = compute_model
		self._default_number_of_steps = ""
		self.number_of_steps = number_of_steps
		self._default_compute_delta = False
		self.compute_delta = compute_delta
		self._default_compute_gamma = False
		self.compute_gamma = compute_gamma
		self._default_compute_theta = False
		self.compute_theta = compute_theta
		self._default_compute_vega = False
		self.compute_vega = compute_vega
		self._default_compute_rho = False
		self.compute_rho = compute_rho
		self._default_all_fields_for_running = type(self).AllFieldsForRunning.FALSE
		self.all_fields_for_running = all_fields_for_running
		self._default_volatility_field_name = ""
		self.volatility_field_name = volatility_field_name
		self._default_interest_rate_field_name = ""
		self.interest_rate_field_name = interest_rate_field_name
		self._default_option_type_field_name = ""
		self.option_type_field_name = option_type_field_name
		self._default_underlying_price_field_name = "PRICE"
		self.underlying_price_field_name = underlying_price_field_name
		self._default_strike_price_field_name = ""
		self.strike_price_field_name = strike_price_field_name
		self._default_days_in_year = 365
		self.days_in_year = days_in_year
		self._default_days_till_expiration_field_name = ""
		self.days_till_expiration_field_name = days_till_expiration_field_name
		self._default_expiration_date_field_name = ""
		self.expiration_date_field_name = expiration_date_field_name
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_bucket_interval(self, value):
		self.bucket_interval = value
		return self

	def set_bucket_interval_units(self, value):
		self.bucket_interval_units = value
		return self

	def set_output_interval(self, value):
		self.output_interval = value
		return self

	def set_output_interval_units(self, value):
		self.output_interval_units = value
		return self

	def set_is_running_aggr(self, value):
		self.is_running_aggr = value
		return self

	def set_bucket_time(self, value):
		self.bucket_time = value
		return self

	def set_bucket_end_criteria(self, value):
		self.bucket_end_criteria = value
		return self

	def set_boundary_tick_bucket(self, value):
		self.boundary_tick_bucket = value
		return self

	def set_partial_bucket_handling(self, value):
		self.partial_bucket_handling = value
		return self

	def set_volatility(self, value):
		self.volatility = value
		return self

	def set_interest_rate(self, value):
		self.interest_rate = value
		return self

	def set_compute_model(self, value):
		self.compute_model = value
		return self

	def set_number_of_steps(self, value):
		self.number_of_steps = value
		return self

	def set_compute_delta(self, value):
		self.compute_delta = value
		return self

	def set_compute_gamma(self, value):
		self.compute_gamma = value
		return self

	def set_compute_theta(self, value):
		self.compute_theta = value
		return self

	def set_compute_vega(self, value):
		self.compute_vega = value
		return self

	def set_compute_rho(self, value):
		self.compute_rho = value
		return self

	def set_all_fields_for_running(self, value):
		self.all_fields_for_running = value
		return self

	def set_volatility_field_name(self, value):
		self.volatility_field_name = value
		return self

	def set_interest_rate_field_name(self, value):
		self.interest_rate_field_name = value
		return self

	def set_option_type_field_name(self, value):
		self.option_type_field_name = value
		return self

	def set_underlying_price_field_name(self, value):
		self.underlying_price_field_name = value
		return self

	def set_strike_price_field_name(self, value):
		self.strike_price_field_name = value
		return self

	def set_days_in_year(self, value):
		self.days_in_year = value
		return self

	def set_days_till_expiration_field_name(self, value):
		self.days_till_expiration_field_name = value
		return self

	def set_expiration_date_field_name(self, value):
		self.expiration_date_field_name = value
		return self

	@staticmethod
	def _get_name():
		return "OPTION_PRICE"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.bucket_interval != 0: 
			desc += "BUCKET_INTERVAL=" + py_to_str(self.bucket_interval) + ","
		if self.bucket_interval_units != self.BucketIntervalUnits.SECONDS: 
			desc += "BUCKET_INTERVAL_UNITS=" + py_to_str(self.bucket_interval_units) + ","
		if self.output_interval != "": 
			desc += "OUTPUT_INTERVAL=" + py_to_str(self.output_interval) + ","
		if self.output_interval_units != self.OutputIntervalUnits.SECONDS: 
			desc += "OUTPUT_INTERVAL_UNITS=" + py_to_str(self.output_interval_units) + ","
		if self.is_running_aggr != False: 
			desc += "IS_RUNNING_AGGR=" + py_to_str(self.is_running_aggr) + ","
		if self.bucket_time != self.BucketTime.BUCKET_END: 
			desc += "BUCKET_TIME=" + py_to_str(self.bucket_time) + ","
		if self.bucket_end_criteria != "": 
			desc += "BUCKET_END_CRITERIA=" + py_to_str(self.bucket_end_criteria) + ","
		if self.boundary_tick_bucket != self.BoundaryTickBucket.NEW: 
			desc += "BOUNDARY_TICK_BUCKET=" + py_to_str(self.boundary_tick_bucket) + ","
		if self.partial_bucket_handling != self.PartialBucketHandling.AS_SEPARATE_BUCKET: 
			desc += "PARTIAL_BUCKET_HANDLING=" + py_to_str(self.partial_bucket_handling) + ","
		if self.volatility != "": 
			desc += "VOLATILITY=" + py_to_str(self.volatility) + ","
		if self.interest_rate != "": 
			desc += "INTEREST_RATE=" + py_to_str(self.interest_rate) + ","
		if self.compute_model != self.ComputeModel.BS: 
			desc += "COMPUTE_MODEL=" + py_to_str(self.compute_model) + ","
		if self.number_of_steps != "": 
			desc += "NUMBER_OF_STEPS=" + py_to_str(self.number_of_steps) + ","
		if self.compute_delta != False: 
			desc += "COMPUTE_DELTA=" + py_to_str(self.compute_delta) + ","
		if self.compute_gamma != False: 
			desc += "COMPUTE_GAMMA=" + py_to_str(self.compute_gamma) + ","
		if self.compute_theta != False: 
			desc += "COMPUTE_THETA=" + py_to_str(self.compute_theta) + ","
		if self.compute_vega != False: 
			desc += "COMPUTE_VEGA=" + py_to_str(self.compute_vega) + ","
		if self.compute_rho != False: 
			desc += "COMPUTE_RHO=" + py_to_str(self.compute_rho) + ","
		if self.all_fields_for_running != self.AllFieldsForRunning.FALSE: 
			desc += "ALL_FIELDS_FOR_RUNNING=" + py_to_str(self.all_fields_for_running) + ","
		if self.volatility_field_name != "": 
			desc += "VOLATILITY_FIELD_NAME=" + py_to_str(self.volatility_field_name) + ","
		if self.interest_rate_field_name != "": 
			desc += "INTEREST_RATE_FIELD_NAME=" + py_to_str(self.interest_rate_field_name) + ","
		if self.option_type_field_name != "": 
			desc += "OPTION_TYPE_FIELD_NAME=" + py_to_str(self.option_type_field_name) + ","
		if self.underlying_price_field_name != "PRICE": 
			desc += "UNDERLYING_PRICE_FIELD_NAME=" + py_to_str(self.underlying_price_field_name) + ","
		if self.strike_price_field_name != "": 
			desc += "STRIKE_PRICE_FIELD_NAME=" + py_to_str(self.strike_price_field_name) + ","
		if self.days_in_year != 365: 
			desc += "DAYS_IN_YEAR=" + py_to_str(self.days_in_year) + ","
		if self.days_till_expiration_field_name != "": 
			desc += "DAYS_TILL_EXPIRATION_FIELD_NAME=" + py_to_str(self.days_till_expiration_field_name) + ","
		if self.expiration_date_field_name != "": 
			desc += "EXPIRATION_DATE_FIELD_NAME=" + py_to_str(self.expiration_date_field_name) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class ImpliedVol(_graph_components.EpBase):
	"""
		

IMPLIED_VOL

Type: Aggregation

Description: For each bucket, computes implied volatility
value for the last tick in the bucket, based on the Black-Scholes
option pricing model.

Python
class name:
ImpliedVol

Input: A time series of ticks, having the PRICE and
OPTION_PRICE attributes.

This EP requires several parameters to compute the implied
volatility. Those are, OPTION_TYPE, STRIKE_PRICE, EXPIRATION_DATE
or DAYS_TILL_EXPIRATION and INTEREST_RATE.
Each parameter can be specified either via a symbol parameter with the
same name, or via a tick field, by specifying name of that field as an
EP parameter (see below). Besides, INTEREST_RATE can also be
specified on EP basis. In either case OPTION_TYPE must have
either CALL value, or PUT. EXPIRATION_DATE is
in YYYYMMDD format, a string in case
of a symbol parameter and an integer in case of a tick attribute.

Output: A time series of ticks, one tick for each bucket
interval.

Parameters: See parameters
common to generic aggregations.


  BUCKET_INTERVAL
(seconds)
  BUCKET_INTERVAL_UNITS
(enumerated type)
  OUTPUT_INTERVAL
(seconds)
  OUTPUT_INTERVAL_UNITS
(SECONDS/TICKS)
  IS_RUNNING_AGGR
(Boolean)
  BUCKET_TIME
(Boolean)
  BUCKET_END_CRITERIA
(expression)
  BOUNDARY_TICK_BUCKET
(NEW/PREVIOUS)
  ALL_FIELDS_FOR_SLIDING
(Boolean)
  PARTIAL_BUCKET_HANDLING
(enumerated type)
  OUTPUT_FIELD_NAME
(string)
  GROUP_BY
(string)
  GROUPS_TO_DISPLAY
(enumerated)
  BUCKET_END_PER_GROUP
(Boolean)
  INTEREST_RATE (numeric)
    The risk-free interest rate.

  
  PRICE_FIELD_NAME (string)
    The name of the field carrying the price value.
Default: PRICE

  
  OPTION_PRICE_FIELD_NAME
(Boolean)
    The name of the field carrying the option price value.
Default: OPTION_PRICE

  
  METHOD (enumeration)
    Allowed values are NEWTON, NEWTON_WITH_FALLBACK and BISECTIONS. Choose between NEWTON and BISECTIONS
for finding successively better approximations to the implied
volatility value. Choose NEWTON_WITH_FALLBACK
to automatically fall back to BISECTIONS method when NEWTON fails to
converge.
Default: NEWTON

  
  PRECISION (numeric)
Precision of the implied volatility value.
Default: 1.0e-5
  VALUE_FOR_NON_CONVERGE 
(enumeration)
    Allowed values are NAN_VAL
and CLOSEST_FOUND_VAL, where CLOSEST_FOUND_VAL stands for the
volatility value for which the difference between calculated option
price and input option price is minimal. Choose between NAN_VAL and CLOSEST_FOUND_VAL
as implied volatility value, when the root-finding method does not
converge within the specified precision.
Default: NAN_VAL

  
  INTEREST_RATE_FIELD_NAME
(string)
Specifies name of the field, which carries the risk-free interest rate.
    It
is the annualized rate, continuously compounded. The value is
0-based, i.e. 0 stands for 0% interest rate, 0.03 - for 3% interest rate
Default: empty
  OPTION_TYPE_FIELD_NAME (string)
Specifies name of the field, which carries the option type (either CALL
or PUT).
Default: empty
  STRIKE_PRICE_FIELD_NAME
(string)
Specifies name of the field, which carries the strike price of the
option.
Default: empty
  DAYS_IN_YEAR (numeric)
Specifies number of days in a year (say, 365 or 252 (business days,
etc.). Used with DAYS_TILL_EXPIRATION
parameter to compute the fractional years till expiration.
Default: 365
  DAYS_TILL_EXPIRATION_FIELD_NAME
(string)
Specifies name of the field, which carries number of days till
expiration of the option.
Default: empty
  EXPIRATION_DATE_FIELD_NAME
(string)
Specifies name of the field, which carries the expiration date of the
option, in YYYYMMDD format.
Default: empty

Notes: See notes on
generic aggregations.


	"""
	class Parameters:
		bucket_interval = "BUCKET_INTERVAL"
		bucket_interval_units = "BUCKET_INTERVAL_UNITS"
		output_interval = "OUTPUT_INTERVAL"
		output_interval_units = "OUTPUT_INTERVAL_UNITS"
		is_running_aggr = "IS_RUNNING_AGGR"
		bucket_time = "BUCKET_TIME"
		bucket_end_criteria = "BUCKET_END_CRITERIA"
		boundary_tick_bucket = "BOUNDARY_TICK_BUCKET"
		all_fields_for_sliding = "ALL_FIELDS_FOR_SLIDING"
		partial_bucket_handling = "PARTIAL_BUCKET_HANDLING"
		output_field_name = "OUTPUT_FIELD_NAME"
		group_by = "GROUP_BY"
		groups_to_display = "GROUPS_TO_DISPLAY"
		bucket_end_per_group = "BUCKET_END_PER_GROUP"
		interest_rate = "INTEREST_RATE"
		price_field_name = "PRICE_FIELD_NAME"
		option_price_field_name = "OPTION_PRICE_FIELD_NAME"
		method = "METHOD"
		value_for_non_converge = "VALUE_FOR_NON_CONVERGE"
		precision = "PRECISION"
		interest_rate_field_name = "INTEREST_RATE_FIELD_NAME"
		option_type_field_name = "OPTION_TYPE_FIELD_NAME"
		strike_price_field_name = "STRIKE_PRICE_FIELD_NAME"
		days_in_year = "DAYS_IN_YEAR"
		days_till_expiration_field_name = "DAYS_TILL_EXPIRATION_FIELD_NAME"
		expiration_date_field_name = "EXPIRATION_DATE_FIELD_NAME"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["bucket_interval", "bucket_interval_units", "output_interval", "output_interval_units", "is_running_aggr", "bucket_time", "bucket_end_criteria", "boundary_tick_bucket", "all_fields_for_sliding", "partial_bucket_handling", "output_field_name", "group_by", "groups_to_display", "bucket_end_per_group", "interest_rate", "price_field_name", "option_price_field_name", "method", "value_for_non_converge", "precision", "interest_rate_field_name", "option_type_field_name", "strike_price_field_name", "days_in_year", "days_till_expiration_field_name", "expiration_date_field_name"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["bucket_interval", "_default_bucket_interval", "bucket_interval_units", "_default_bucket_interval_units", "output_interval", "_default_output_interval", "output_interval_units", "_default_output_interval_units", "is_running_aggr", "_default_is_running_aggr", "bucket_time", "_default_bucket_time", "bucket_end_criteria", "_default_bucket_end_criteria", "boundary_tick_bucket", "_default_boundary_tick_bucket", "all_fields_for_sliding", "_default_all_fields_for_sliding", "partial_bucket_handling", "_default_partial_bucket_handling", "output_field_name", "_default_output_field_name", "group_by", "_default_group_by", "groups_to_display", "_default_groups_to_display", "bucket_end_per_group", "_default_bucket_end_per_group", "interest_rate", "_default_interest_rate", "price_field_name", "_default_price_field_name", "option_price_field_name", "_default_option_price_field_name", "method", "_default_method", "value_for_non_converge", "_default_value_for_non_converge", "precision", "_default_precision", "interest_rate_field_name", "_default_interest_rate_field_name", "option_type_field_name", "_default_option_type_field_name", "strike_price_field_name", "_default_strike_price_field_name", "days_in_year", "_default_days_in_year", "days_till_expiration_field_name", "_default_days_till_expiration_field_name", "expiration_date_field_name", "_default_expiration_date_field_name", "stack_info", "_used_strings"]

	class BucketIntervalUnits:
		DAYS = "DAYS"
		FLEXIBLE = "FLEXIBLE"
		MONTHS = "MONTHS"
		SECONDS = "SECONDS"
		TICKS = "TICKS"

	class OutputIntervalUnits:
		SECONDS = "SECONDS"
		TICKS = "TICKS"

	class BucketTime:
		BUCKET_END = "BUCKET_END"
		BUCKET_START = "BUCKET_START"

	class BoundaryTickBucket:
		NEW = "NEW"
		PREVIOUS = "PREVIOUS"

	class AllFieldsForSliding:
		WHEN_TICKS_EXIT_WINDOW = "WHEN_TICKS_EXIT_WINDOW"
		FALSE = "false"
		TRUE = "true"

	class PartialBucketHandling:
		AS_SEPARATE_BUCKET = "AS_SEPARATE_BUCKET"
		DISCARD = "DISCARD"
		MERGE_WITH_PREVIOUS = "MERGE_WITH_PREVIOUS"

	class GroupsToDisplay:
		ALL = "ALL"
		EVENT_IN_LAST_BUCKET = "EVENT_IN_LAST_BUCKET"

	class Method:
		BISECTIONS = "BISECTIONS"
		NEWTON = "NEWTON"
		NEWTON_WITH_FALLBACK = "NEWTON_WITH_FALLBACK"

	class ValueForNonConverge:
		CLOSEST_FOUND_VAL = "CLOSEST_FOUND_VAL"
		NAN_VAL = "NAN_VAL"

	def __init__(self, bucket_interval=0, bucket_interval_units=BucketIntervalUnits.SECONDS, output_interval="", output_interval_units=OutputIntervalUnits.SECONDS, is_running_aggr=False, bucket_time=BucketTime.BUCKET_END, bucket_end_criteria="", boundary_tick_bucket=BoundaryTickBucket.NEW, all_fields_for_sliding=AllFieldsForSliding.FALSE, partial_bucket_handling=PartialBucketHandling.AS_SEPARATE_BUCKET, output_field_name="VALUE", group_by="", groups_to_display=GroupsToDisplay.ALL, bucket_end_per_group=False, interest_rate="", price_field_name="PRICE", option_price_field_name="OPTION_PRICE", method=Method.NEWTON, value_for_non_converge=ValueForNonConverge.NAN_VAL, precision="1.0e-5", interest_rate_field_name="", option_type_field_name="", strike_price_field_name="", days_in_year=365, days_till_expiration_field_name="", expiration_date_field_name="", Out = ""):
		_graph_components.EpBase.__init__(self, "IMPLIED_VOL")
		self._default_bucket_interval = 0
		self.bucket_interval = bucket_interval
		self._default_bucket_interval_units = type(self).BucketIntervalUnits.SECONDS
		self.bucket_interval_units = bucket_interval_units
		self._default_output_interval = ""
		self.output_interval = output_interval
		self._default_output_interval_units = type(self).OutputIntervalUnits.SECONDS
		self.output_interval_units = output_interval_units
		self._default_is_running_aggr = False
		self.is_running_aggr = is_running_aggr
		self._default_bucket_time = type(self).BucketTime.BUCKET_END
		self.bucket_time = bucket_time
		self._default_bucket_end_criteria = ""
		self.bucket_end_criteria = bucket_end_criteria
		self._default_boundary_tick_bucket = type(self).BoundaryTickBucket.NEW
		self.boundary_tick_bucket = boundary_tick_bucket
		self._default_all_fields_for_sliding = type(self).AllFieldsForSliding.FALSE
		self.all_fields_for_sliding = all_fields_for_sliding
		self._default_partial_bucket_handling = type(self).PartialBucketHandling.AS_SEPARATE_BUCKET
		self.partial_bucket_handling = partial_bucket_handling
		self._default_output_field_name = "VALUE"
		self.output_field_name = output_field_name
		self._default_group_by = ""
		self.group_by = group_by
		self._default_groups_to_display = type(self).GroupsToDisplay.ALL
		self.groups_to_display = groups_to_display
		self._default_bucket_end_per_group = False
		self.bucket_end_per_group = bucket_end_per_group
		self._default_interest_rate = ""
		self.interest_rate = interest_rate
		self._default_price_field_name = "PRICE"
		self.price_field_name = price_field_name
		self._default_option_price_field_name = "OPTION_PRICE"
		self.option_price_field_name = option_price_field_name
		self._default_method = type(self).Method.NEWTON
		self.method = method
		self._default_value_for_non_converge = type(self).ValueForNonConverge.NAN_VAL
		self.value_for_non_converge = value_for_non_converge
		self._default_precision = "1.0e-5"
		self.precision = precision
		self._default_interest_rate_field_name = ""
		self.interest_rate_field_name = interest_rate_field_name
		self._default_option_type_field_name = ""
		self.option_type_field_name = option_type_field_name
		self._default_strike_price_field_name = ""
		self.strike_price_field_name = strike_price_field_name
		self._default_days_in_year = 365
		self.days_in_year = days_in_year
		self._default_days_till_expiration_field_name = ""
		self.days_till_expiration_field_name = days_till_expiration_field_name
		self._default_expiration_date_field_name = ""
		self.expiration_date_field_name = expiration_date_field_name
		if Out != "":
			self.output_field_name=Out
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_bucket_interval(self, value):
		self.bucket_interval = value
		return self

	def set_bucket_interval_units(self, value):
		self.bucket_interval_units = value
		return self

	def set_output_interval(self, value):
		self.output_interval = value
		return self

	def set_output_interval_units(self, value):
		self.output_interval_units = value
		return self

	def set_is_running_aggr(self, value):
		self.is_running_aggr = value
		return self

	def set_bucket_time(self, value):
		self.bucket_time = value
		return self

	def set_bucket_end_criteria(self, value):
		self.bucket_end_criteria = value
		return self

	def set_boundary_tick_bucket(self, value):
		self.boundary_tick_bucket = value
		return self

	def set_all_fields_for_sliding(self, value):
		self.all_fields_for_sliding = value
		return self

	def set_partial_bucket_handling(self, value):
		self.partial_bucket_handling = value
		return self

	def set_output_field_name(self, value):
		self.output_field_name = value
		return self

	def set_group_by(self, value):
		self.group_by = value
		return self

	def set_groups_to_display(self, value):
		self.groups_to_display = value
		return self

	def set_bucket_end_per_group(self, value):
		self.bucket_end_per_group = value
		return self

	def set_interest_rate(self, value):
		self.interest_rate = value
		return self

	def set_price_field_name(self, value):
		self.price_field_name = value
		return self

	def set_option_price_field_name(self, value):
		self.option_price_field_name = value
		return self

	def set_method(self, value):
		self.method = value
		return self

	def set_value_for_non_converge(self, value):
		self.value_for_non_converge = value
		return self

	def set_precision(self, value):
		self.precision = value
		return self

	def set_interest_rate_field_name(self, value):
		self.interest_rate_field_name = value
		return self

	def set_option_type_field_name(self, value):
		self.option_type_field_name = value
		return self

	def set_strike_price_field_name(self, value):
		self.strike_price_field_name = value
		return self

	def set_days_in_year(self, value):
		self.days_in_year = value
		return self

	def set_days_till_expiration_field_name(self, value):
		self.days_till_expiration_field_name = value
		return self

	def set_expiration_date_field_name(self, value):
		self.expiration_date_field_name = value
		return self

	@staticmethod
	def _get_name():
		return "IMPLIED_VOL"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.bucket_interval != 0: 
			desc += "BUCKET_INTERVAL=" + py_to_str(self.bucket_interval) + ","
		if self.bucket_interval_units != self.BucketIntervalUnits.SECONDS: 
			desc += "BUCKET_INTERVAL_UNITS=" + py_to_str(self.bucket_interval_units) + ","
		if self.output_interval != "": 
			desc += "OUTPUT_INTERVAL=" + py_to_str(self.output_interval) + ","
		if self.output_interval_units != self.OutputIntervalUnits.SECONDS: 
			desc += "OUTPUT_INTERVAL_UNITS=" + py_to_str(self.output_interval_units) + ","
		if self.is_running_aggr != False: 
			desc += "IS_RUNNING_AGGR=" + py_to_str(self.is_running_aggr) + ","
		if self.bucket_time != self.BucketTime.BUCKET_END: 
			desc += "BUCKET_TIME=" + py_to_str(self.bucket_time) + ","
		if self.bucket_end_criteria != "": 
			desc += "BUCKET_END_CRITERIA=" + py_to_str(self.bucket_end_criteria) + ","
		if self.boundary_tick_bucket != self.BoundaryTickBucket.NEW: 
			desc += "BOUNDARY_TICK_BUCKET=" + py_to_str(self.boundary_tick_bucket) + ","
		if self.all_fields_for_sliding != self.AllFieldsForSliding.FALSE: 
			desc += "ALL_FIELDS_FOR_SLIDING=" + py_to_str(self.all_fields_for_sliding) + ","
		if self.partial_bucket_handling != self.PartialBucketHandling.AS_SEPARATE_BUCKET: 
			desc += "PARTIAL_BUCKET_HANDLING=" + py_to_str(self.partial_bucket_handling) + ","
		if self.output_field_name != "VALUE": 
			desc += "OUTPUT_FIELD_NAME=" + py_to_str(self.output_field_name) + ","
		if self.group_by != "": 
			desc += "GROUP_BY=" + py_to_str(self.group_by) + ","
		if self.groups_to_display != self.GroupsToDisplay.ALL: 
			desc += "GROUPS_TO_DISPLAY=" + py_to_str(self.groups_to_display) + ","
		if self.bucket_end_per_group != False: 
			desc += "BUCKET_END_PER_GROUP=" + py_to_str(self.bucket_end_per_group) + ","
		if self.interest_rate != "": 
			desc += "INTEREST_RATE=" + py_to_str(self.interest_rate) + ","
		if self.price_field_name != "PRICE": 
			desc += "PRICE_FIELD_NAME=" + py_to_str(self.price_field_name) + ","
		if self.option_price_field_name != "OPTION_PRICE": 
			desc += "OPTION_PRICE_FIELD_NAME=" + py_to_str(self.option_price_field_name) + ","
		if self.method != self.Method.NEWTON: 
			desc += "METHOD=" + py_to_str(self.method) + ","
		if self.value_for_non_converge != self.ValueForNonConverge.NAN_VAL: 
			desc += "VALUE_FOR_NON_CONVERGE=" + py_to_str(self.value_for_non_converge) + ","
		if self.precision != "1.0e-5": 
			desc += "PRECISION=" + py_to_str(self.precision) + ","
		if self.interest_rate_field_name != "": 
			desc += "INTEREST_RATE_FIELD_NAME=" + py_to_str(self.interest_rate_field_name) + ","
		if self.option_type_field_name != "": 
			desc += "OPTION_TYPE_FIELD_NAME=" + py_to_str(self.option_type_field_name) + ","
		if self.strike_price_field_name != "": 
			desc += "STRIKE_PRICE_FIELD_NAME=" + py_to_str(self.strike_price_field_name) + ","
		if self.days_in_year != 365: 
			desc += "DAYS_IN_YEAR=" + py_to_str(self.days_in_year) + ","
		if self.days_till_expiration_field_name != "": 
			desc += "DAYS_TILL_EXPIRATION_FIELD_NAME=" + py_to_str(self.days_till_expiration_field_name) + ","
		if self.expiration_date_field_name != "": 
			desc += "EXPIRATION_DATE_FIELD_NAME=" + py_to_str(self.expiration_date_field_name) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class Merge(_graph_components.EpBase):
	"""
		

MERGE

Type: Union

Description: Merges ticks from multiple input streams into a
single output stream ordered by the timestamp. Different tick types can
mix freely. The merged time series will have additional fields
identifying which input time series the output tick belongs to, namely TICK_TYPE, SYMBOL_NAME
(which can optionally be separated into two fields: SYMBOL_NAME and DB_NAME,
with the former carrying a pure symbol name and the latter carrying the
database name), or (optionally) SYMBOL_INDEX.

Python
class name:
Merge

Input: Multiple time series of ticks.

Output: A single, merged time series of ticks.

Parameters:


  ADD_SYMBOL_INDEX (Boolean)
    If set to TRUE, this event
processor adds a field SYMBOL_INDEX
to each tick, with a numeric index corresponding to the symbol the tick
is for.
Default: FALSE

  
  SEPARATE_DB_NAME (Boolean)
    If set to TRUE, the security
name of the input time series is separated into a pure symbol name and
the database name parts are propagated in the SYMBOL_NAME and DB_NAME
fields, respectively. Otherwise, the full symbol name is propagated in
a single field called SYMBOL_NAME.
Default: FALSE

  
  ADDED_FIELD_NAME_SUFFIX
(string)
    The suffix to add to the names of additional fields (that is, SYMBOL_NAME, TICK_TYPE,
    DB_NAME, or SYMBOL_INDEX). This facilitates having
MERGE nodes feed other MERGE nodes.

  
  IDENTIFY_INPUT_TS (Boolean)
    If set to FALSE, the fields SYMBOL_NAME, DB_NAME,
and TICK_TYPE are not appended to
the output ticks.&nbsp;
If set to TRUE, the fields SYMBOL_NAME, DB_NAME,
and TICK_TYPE are&nbsp;appended to
the output ticks, to identify the time series to which a given merged
tick belongs. The exception is the case when MERGE
EP is applied to a single symbol, with a single tick type, and
SYMBOL_NAME or/and TICK_TYPE field is present in the input ticks. In
such case, the input ticks are propagated unchanged.&nbsp; 
If set to IF_NO_CONFLICT,
the values of fields SYMBOL_NAME are
    DB_NAME are propagated from the
input ticks if SYMBOL_NAME field is
present in the input ticks, otherwise these fields are appended to the
output ticks. Also, if set to IF_NO_CONFLICT,&nbsp;
the values of field&nbsp;TICK_TYPE
field are propagated from the input ticks if TICK_TYPE
field is present in the input ticks,&nbsp;otherwise this field is
appended to the output ticks.
Default: TRUE

  
  ALWAYS_ADD_TICK_TYPE_FLAG
(Boolean)
    By default, when&nbsp;MERGE EP
is applied to a single symbol,&nbsp;with a single tick type, and IDENTIFY_INPUT_TS parameter is set to TRUE, while SYMBOL_NAME
field is present in the input ticks, and TICK_TYPEfield
is not present in the input ticks, the output ticks do not have TICK_TYPE field. When&nbsp;ALWAYS_ADD_TICK_TYPE_FLAG EP parameter is
set to TRUE,&nbsp; TICK_TYPE field is appended to the output
ticks, carrying the value of the tick type at the MERGE EP, for the above-mentioned
scenario.
Default: FALSE

  
  STABILIZE_SCHEMA (Boolean)
    If set to TRUE, any fields
that were present on any tick in the input time series will be present
in the ticks of the output time series. New fields will be added to the
output tick at the point they are first seen in the input time series.
If any field already present in the input is not present on a given
input tick, its type will be determined by the widest encountered type
under that field name.
Incompatible types (for example, int and string) under the same field
name will result in an exception.
    
Numeric fields are compatible with each other with the following
priority : integers &lt; real numbers &lt; time types
When a time type is present with another type, the output field will be
msec.
Default: FALSE

  

Input ticks do not need to have the same
structure-they are allowed to have different fields.

Examples: Merges ticks for all securities in the input list
into a single timestamp-sorted output stream. When tick type is set to TRD+QTE, it will merge trades and quotes
into the same stream.

MERGE ()

See the merge_trades_and_quotes
example in UNION_EXAMPLES.otq.


	"""
	class Parameters:
		add_symbol_index = "ADD_SYMBOL_INDEX"
		separate_db_name = "SEPARATE_DB_NAME"
		added_field_name_suffix = "ADDED_FIELD_NAME_SUFFIX"
		identify_input_ts = "IDENTIFY_INPUT_TS"
		always_add_tick_type_flag = "ALWAYS_ADD_TICK_TYPE_FLAG"
		stabilize_schema = "STABILIZE_SCHEMA"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["add_symbol_index", "separate_db_name", "added_field_name_suffix", "identify_input_ts", "always_add_tick_type_flag", "stabilize_schema"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["add_symbol_index", "_default_add_symbol_index", "separate_db_name", "_default_separate_db_name", "added_field_name_suffix", "_default_added_field_name_suffix", "identify_input_ts", "_default_identify_input_ts", "always_add_tick_type_flag", "_default_always_add_tick_type_flag", "stabilize_schema", "_default_stabilize_schema", "stack_info", "_used_strings"]

	class IdentifyInputTs:
		IF_NO_CONFLICT = "IF_NO_CONFLICT"
		FALSE = "false"
		TRUE = "true"

	def __init__(self, add_symbol_index=False, separate_db_name=False, added_field_name_suffix="", identify_input_ts=IdentifyInputTs.TRUE, always_add_tick_type_flag=False, stabilize_schema=False):
		_graph_components.EpBase.__init__(self, "MERGE")
		self._default_add_symbol_index = False
		self.add_symbol_index = add_symbol_index
		self._default_separate_db_name = False
		self.separate_db_name = separate_db_name
		self._default_added_field_name_suffix = ""
		self.added_field_name_suffix = added_field_name_suffix
		self._default_identify_input_ts = type(self).IdentifyInputTs.TRUE
		self.identify_input_ts = identify_input_ts
		self._default_always_add_tick_type_flag = False
		self.always_add_tick_type_flag = always_add_tick_type_flag
		self._default_stabilize_schema = False
		self.stabilize_schema = stabilize_schema
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_add_symbol_index(self, value):
		self.add_symbol_index = value
		return self

	def set_separate_db_name(self, value):
		self.separate_db_name = value
		return self

	def set_added_field_name_suffix(self, value):
		self.added_field_name_suffix = value
		return self

	def set_identify_input_ts(self, value):
		self.identify_input_ts = value
		return self

	def set_always_add_tick_type_flag(self, value):
		self.always_add_tick_type_flag = value
		return self

	def set_stabilize_schema(self, value):
		self.stabilize_schema = value
		return self

	@staticmethod
	def _get_name():
		return "MERGE"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.add_symbol_index != False: 
			desc += "ADD_SYMBOL_INDEX=" + py_to_str(self.add_symbol_index) + ","
		if self.separate_db_name != False: 
			desc += "SEPARATE_DB_NAME=" + py_to_str(self.separate_db_name) + ","
		if self.added_field_name_suffix != "": 
			desc += "ADDED_FIELD_NAME_SUFFIX=" + py_to_str(self.added_field_name_suffix) + ","
		if self.identify_input_ts != self.IdentifyInputTs.TRUE: 
			desc += "IDENTIFY_INPUT_TS=" + py_to_str(self.identify_input_ts) + ","
		if self.always_add_tick_type_flag != False: 
			desc += "ALWAYS_ADD_TICK_TYPE_FLAG=" + py_to_str(self.always_add_tick_type_flag) + ","
		if self.stabilize_schema != False: 
			desc += "STABILIZE_SCHEMA=" + py_to_str(self.stabilize_schema) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class TsAdd(_graph_components.EpBase):
	"""
		

TS_ADD

Type: Aggregation

Description: Computes the sum of fields' latest values across
multiple series.

Python
class name:
TsAdd

Input: N time series of ticks, where N &gt; 1.

Output: A time series of ticks with the sum field and the
field named NUM_SYMBOLS containing the number of values that went into
computing the sum.

Parameters: See parameters
common to generic aggregations.


  BUCKET_INTERVAL
(seconds/ticks)
  BUCKET_INTERVAL_UNITS
(enumerated type)
  OUTPUT_INTERVAL
(seconds)
  OUTPUT_INTERVAL_UNITS
(enumerated type)
  IS_RUNNING_AGGR
(Boolean)
  BUCKET_TIME
(Boolean)
  BUCKET_END_CRITERIA
(expression)
  BOUNDARY_TICK_BUCKET
(NEW/PREVIOUS)
  ALL_FIELDS_FOR_SLIDING
(Boolean)
  PARTIAL_BUCKET_HANDLING
(enumerated type)
  INPUT_FIELD_NAME
(string)

Examples: Compute 5 minute buckets of total trading volumes
across multiple securities (see AGGREGATION_EXAMPLES.otq):

SUM (600, SECONDS, , false, false, AS_SEPARATE_BUCKET, VOLUME, , )TS_ADD(600, SECONDS, , true, AS_SEPARATE_BUCKET,)

	"""
	class Parameters:
		bucket_interval = "BUCKET_INTERVAL"
		bucket_interval_units = "BUCKET_INTERVAL_UNITS"
		output_interval = "OUTPUT_INTERVAL"
		output_interval_units = "OUTPUT_INTERVAL_UNITS"
		is_running_aggr = "IS_RUNNING_AGGR"
		bucket_time = "BUCKET_TIME"
		bucket_end_criteria = "BUCKET_END_CRITERIA"
		boundary_tick_bucket = "BOUNDARY_TICK_BUCKET"
		partial_bucket_handling = "PARTIAL_BUCKET_HANDLING"
		input_field_name = "INPUT_FIELD_NAME"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["bucket_interval", "bucket_interval_units", "output_interval", "output_interval_units", "is_running_aggr", "bucket_time", "bucket_end_criteria", "boundary_tick_bucket", "partial_bucket_handling", "input_field_name"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["bucket_interval", "_default_bucket_interval", "bucket_interval_units", "_default_bucket_interval_units", "output_interval", "_default_output_interval", "output_interval_units", "_default_output_interval_units", "is_running_aggr", "_default_is_running_aggr", "bucket_time", "_default_bucket_time", "bucket_end_criteria", "_default_bucket_end_criteria", "boundary_tick_bucket", "_default_boundary_tick_bucket", "partial_bucket_handling", "_default_partial_bucket_handling", "input_field_name", "_default_input_field_name", "stack_info", "_used_strings"]

	class BucketIntervalUnits:
		DAYS = "DAYS"
		FLEXIBLE = "FLEXIBLE"
		MONTHS = "MONTHS"
		SECONDS = "SECONDS"
		TICKS = "TICKS"

	class OutputIntervalUnits:
		SECONDS = "SECONDS"
		TICKS = "TICKS"

	class BucketTime:
		BUCKET_END = "BUCKET_END"
		BUCKET_START = "BUCKET_START"

	class BoundaryTickBucket:
		NEW = "NEW"
		PREVIOUS = "PREVIOUS"

	class PartialBucketHandling:
		AS_SEPARATE_BUCKET = "AS_SEPARATE_BUCKET"
		DISCARD = "DISCARD"
		MERGE_WITH_PREVIOUS = "MERGE_WITH_PREVIOUS"

	def __init__(self, bucket_interval=0, bucket_interval_units=BucketIntervalUnits.SECONDS, output_interval="", output_interval_units=OutputIntervalUnits.SECONDS, is_running_aggr=False, bucket_time=BucketTime.BUCKET_END, bucket_end_criteria="", boundary_tick_bucket=BoundaryTickBucket.NEW, partial_bucket_handling=PartialBucketHandling.AS_SEPARATE_BUCKET, input_field_name="", In = ""):
		_graph_components.EpBase.__init__(self, "TS_ADD")
		self._default_bucket_interval = 0
		self.bucket_interval = bucket_interval
		self._default_bucket_interval_units = type(self).BucketIntervalUnits.SECONDS
		self.bucket_interval_units = bucket_interval_units
		self._default_output_interval = ""
		self.output_interval = output_interval
		self._default_output_interval_units = type(self).OutputIntervalUnits.SECONDS
		self.output_interval_units = output_interval_units
		self._default_is_running_aggr = False
		self.is_running_aggr = is_running_aggr
		self._default_bucket_time = type(self).BucketTime.BUCKET_END
		self.bucket_time = bucket_time
		self._default_bucket_end_criteria = ""
		self.bucket_end_criteria = bucket_end_criteria
		self._default_boundary_tick_bucket = type(self).BoundaryTickBucket.NEW
		self.boundary_tick_bucket = boundary_tick_bucket
		self._default_partial_bucket_handling = type(self).PartialBucketHandling.AS_SEPARATE_BUCKET
		self.partial_bucket_handling = partial_bucket_handling
		self._default_input_field_name = ""
		self.input_field_name = input_field_name
		if In != "":
			self.input_field_name=In
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_bucket_interval(self, value):
		self.bucket_interval = value
		return self

	def set_bucket_interval_units(self, value):
		self.bucket_interval_units = value
		return self

	def set_output_interval(self, value):
		self.output_interval = value
		return self

	def set_output_interval_units(self, value):
		self.output_interval_units = value
		return self

	def set_is_running_aggr(self, value):
		self.is_running_aggr = value
		return self

	def set_bucket_time(self, value):
		self.bucket_time = value
		return self

	def set_bucket_end_criteria(self, value):
		self.bucket_end_criteria = value
		return self

	def set_boundary_tick_bucket(self, value):
		self.boundary_tick_bucket = value
		return self

	def set_partial_bucket_handling(self, value):
		self.partial_bucket_handling = value
		return self

	def set_input_field_name(self, value):
		self.input_field_name = value
		return self

	@staticmethod
	def _get_name():
		return "TS_ADD"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.bucket_interval != 0: 
			desc += "BUCKET_INTERVAL=" + py_to_str(self.bucket_interval) + ","
		if self.bucket_interval_units != self.BucketIntervalUnits.SECONDS: 
			desc += "BUCKET_INTERVAL_UNITS=" + py_to_str(self.bucket_interval_units) + ","
		if self.output_interval != "": 
			desc += "OUTPUT_INTERVAL=" + py_to_str(self.output_interval) + ","
		if self.output_interval_units != self.OutputIntervalUnits.SECONDS: 
			desc += "OUTPUT_INTERVAL_UNITS=" + py_to_str(self.output_interval_units) + ","
		if self.is_running_aggr != False: 
			desc += "IS_RUNNING_AGGR=" + py_to_str(self.is_running_aggr) + ","
		if self.bucket_time != self.BucketTime.BUCKET_END: 
			desc += "BUCKET_TIME=" + py_to_str(self.bucket_time) + ","
		if self.bucket_end_criteria != "": 
			desc += "BUCKET_END_CRITERIA=" + py_to_str(self.bucket_end_criteria) + ","
		if self.boundary_tick_bucket != self.BoundaryTickBucket.NEW: 
			desc += "BOUNDARY_TICK_BUCKET=" + py_to_str(self.boundary_tick_bucket) + ","
		if self.partial_bucket_handling != self.PartialBucketHandling.AS_SEPARATE_BUCKET: 
			desc += "PARTIAL_BUCKET_HANDLING=" + py_to_str(self.partial_bucket_handling) + ","
		if self.input_field_name != "": 
			desc += "INPUT_FIELD_NAME=" + py_to_str(self.input_field_name) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class DbShowCorrectionStatus(_graph_components.EpBase):
	"""
		

DB/SHOW_CORRECTION_STATUS

Type: Other

Description: When correction EP's (DB/UPDATE, DB/DELETE,
DB/INSERT_ROW, DB/INSERT_DATA_QUALITY_EVENT) are executed against an
in-memory database and do not return an immediate outcome (because
memory loader is either not running or does not get to apply the
correction within the specified time frame), this event processor can
later be used to check the result.

Python
class name:
DbShowCorrectionStatus

Input: None

Output: A single tick with the following fields:


  NUM_AFFECTED_ROWS
    The number of rows affected by the correction

  
  STATUS
    The status of the operation. Possible values are SUCCESS,
    FAILURE and IN_PROGRESS

  
  OPERATION_ID
    The operation ID (basically the same as the input param)

  
  ERROR_MESSAGE
    The error message, in case the correction has failed

  
  MESSAGE_TIMESTAMP
    The time at which the correction operation was completed

  
  PROCESS_TYPE
    The type of the process that has applied the correction. At the
moment the only possible value is MEMORY LOADER

  
  PROCESS_HOST
    Host name of the process that has applied the correction.

  
  EXECUTABLE_PATH
    Path of the executable that has applied the correction.

  

Parameters:


  OPERATION_ID
    The operation ID, returned by the corresponding correction EP.

  


	"""
	class Parameters:
		operation_id = "OPERATION_ID"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["operation_id"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["operation_id", "_default_operation_id", "stack_info", "_used_strings"]

	def __init__(self, operation_id=""):
		_graph_components.EpBase.__init__(self, "DB/SHOW_CORRECTION_STATUS")
		self._default_operation_id = ""
		self.operation_id = operation_id
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_operation_id(self, value):
		self.operation_id = value
		return self

	@staticmethod
	def _get_name():
		return "DB/SHOW_CORRECTION_STATUS"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.operation_id != "": 
			desc += "OPERATION_ID=" + py_to_str(self.operation_id) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class TsSubtract(_graph_components.EpBase):
	"""
		

TS_SUBTRACT

Type: Aggregation

Description: Computes the difference of the latest values of
some attribute between two input time series.

Python
class name:&nbsp;TsSubtract

Input: Two time series of ticks.

Output: A time series of ticks.

Parameters: See parameters
common to generic aggregations.


  BUCKET_INTERVAL
(seconds/ticks)
  BUCKET_INTERVAL_UNITS
(enumerated type)
  OUTPUT_INTERVAL
(seconds)
  OUTPUT_INTERVAL_UNITS
(enumerated type)
  IS_RUNNING_AGGR
(Boolean)
  BUCKET_TIME
(Boolean)
  BUCKET_END_CRITERIA
(expression)
  BOUNDARY_TICK_BUCKET
(NEW/PREVIOUS)
  ALL_FIELDS_FOR_SLIDING
(Boolean)
  PARTIAL_BUCKET_HANDLING
(enumerated type)
  INPUT_FIELD1_NAME (string)
    The name of the field of the first time series (from which we
are subtracting) in the format &lt;ts_name&gt;.FIELD_NAME.

  
  INPUT_FIELD2_NAME (string)
    The name of the field of the second time series (which we are
subtracting) in the format &lt;ts_name&gt;.FIELD_NAME.

  
  OUTPUT_FIELD_NAME
(string)
  GROUP_BY
(string)
  GROUPS_TO_DISPLAY
(enumerated)
  BUCKET_END_PER_GROUP
(Boolean)
  KEEP_ZERO_DURATION_TICKS
(Boolean)
    When set to false, output ticks that last 0 time are discarded
from the output.

    Such ticks can result when some ticks in the first time series
have the same timestamps as the ticks in the other time series. When
the tick from the first time series arrives, the tick with the same
timestamp from the other time series may not have arrived yet, so the
computed output tick would contain the difference between the new tick
from the first time series and the oldest tick from the second time
series. But when the new tick from the second time series, with the
same timestamp, arrives, the computed output tick will contain the
difference between new ticks from the first and second time series.
Timestamps of the output ticks would be the same, and the first
difference would be undesirable when computing the difference between
two same-sized vectors.
Default: false

  

Notes: See the notes on generic
aggregations.

Examples: Compute the difference of returns between two
portfolios, where relative names of two portfolio files are specified
as input symbols:

CSV_FILE_LISTING()

QUERY_SYMBOLS(20050103)

PREVAILING_PRICE(0,true,AS_SEPARATE_BUCKET,VALUE,,true)

PORTFOLIO_PRICE(60,false,AS_SEPARATE_BUCKET,BOTH)

RETURN (0, true, false,
AS_SEPARATE_BUCKET, RETURN,VALUE)

TS_SUBTRACT(0,SECONDS,,true,AS_SEPARATE_BUCKET,RETURN,RETURN,,,false)

See the TS_SUBTRACT example in AGGREGATION_EXAMPLES.otq.


	"""
	class Parameters:
		bucket_interval = "BUCKET_INTERVAL"
		bucket_interval_units = "BUCKET_INTERVAL_UNITS"
		output_interval = "OUTPUT_INTERVAL"
		output_interval_units = "OUTPUT_INTERVAL_UNITS"
		is_running_aggr = "IS_RUNNING_AGGR"
		bucket_time = "BUCKET_TIME"
		bucket_end_criteria = "BUCKET_END_CRITERIA"
		boundary_tick_bucket = "BOUNDARY_TICK_BUCKET"
		partial_bucket_handling = "PARTIAL_BUCKET_HANDLING"
		input_field1_name = "INPUT_FIELD1_NAME"
		input_field2_name = "INPUT_FIELD2_NAME"
		output_field_name = "OUTPUT_FIELD_NAME"
		group_by = "GROUP_BY"
		groups_to_display = "GROUPS_TO_DISPLAY"
		bucket_end_per_group = "BUCKET_END_PER_GROUP"
		keep_zero_duration_ticks = "KEEP_ZERO_DURATION_TICKS"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["bucket_interval", "bucket_interval_units", "output_interval", "output_interval_units", "is_running_aggr", "bucket_time", "bucket_end_criteria", "boundary_tick_bucket", "partial_bucket_handling", "input_field1_name", "input_field2_name", "output_field_name", "group_by", "groups_to_display", "bucket_end_per_group", "keep_zero_duration_ticks"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["bucket_interval", "_default_bucket_interval", "bucket_interval_units", "_default_bucket_interval_units", "output_interval", "_default_output_interval", "output_interval_units", "_default_output_interval_units", "is_running_aggr", "_default_is_running_aggr", "bucket_time", "_default_bucket_time", "bucket_end_criteria", "_default_bucket_end_criteria", "boundary_tick_bucket", "_default_boundary_tick_bucket", "partial_bucket_handling", "_default_partial_bucket_handling", "input_field1_name", "_default_input_field1_name", "input_field2_name", "_default_input_field2_name", "output_field_name", "_default_output_field_name", "group_by", "_default_group_by", "groups_to_display", "_default_groups_to_display", "bucket_end_per_group", "_default_bucket_end_per_group", "keep_zero_duration_ticks", "_default_keep_zero_duration_ticks", "stack_info", "_used_strings"]

	class BucketIntervalUnits:
		DAYS = "DAYS"
		FLEXIBLE = "FLEXIBLE"
		MONTHS = "MONTHS"
		SECONDS = "SECONDS"
		TICKS = "TICKS"

	class OutputIntervalUnits:
		SECONDS = "SECONDS"
		TICKS = "TICKS"

	class BucketTime:
		BUCKET_END = "BUCKET_END"
		BUCKET_START = "BUCKET_START"

	class BoundaryTickBucket:
		NEW = "NEW"
		PREVIOUS = "PREVIOUS"

	class PartialBucketHandling:
		AS_SEPARATE_BUCKET = "AS_SEPARATE_BUCKET"
		DISCARD = "DISCARD"
		MERGE_WITH_PREVIOUS = "MERGE_WITH_PREVIOUS"

	class GroupsToDisplay:
		ALL = "ALL"
		EVENT_IN_LAST_BUCKET = "EVENT_IN_LAST_BUCKET"

	def __init__(self, bucket_interval=0, bucket_interval_units=BucketIntervalUnits.SECONDS, output_interval="", output_interval_units=OutputIntervalUnits.SECONDS, is_running_aggr=True, bucket_time=BucketTime.BUCKET_END, bucket_end_criteria="", boundary_tick_bucket=BoundaryTickBucket.NEW, partial_bucket_handling=PartialBucketHandling.AS_SEPARATE_BUCKET, input_field1_name="", input_field2_name="", output_field_name="VALUE", group_by="", groups_to_display=GroupsToDisplay.ALL, bucket_end_per_group=False, keep_zero_duration_ticks=False, Out = ""):
		_graph_components.EpBase.__init__(self, "TS_SUBTRACT")
		self._default_bucket_interval = 0
		self.bucket_interval = bucket_interval
		self._default_bucket_interval_units = type(self).BucketIntervalUnits.SECONDS
		self.bucket_interval_units = bucket_interval_units
		self._default_output_interval = ""
		self.output_interval = output_interval
		self._default_output_interval_units = type(self).OutputIntervalUnits.SECONDS
		self.output_interval_units = output_interval_units
		self._default_is_running_aggr = True
		self.is_running_aggr = is_running_aggr
		self._default_bucket_time = type(self).BucketTime.BUCKET_END
		self.bucket_time = bucket_time
		self._default_bucket_end_criteria = ""
		self.bucket_end_criteria = bucket_end_criteria
		self._default_boundary_tick_bucket = type(self).BoundaryTickBucket.NEW
		self.boundary_tick_bucket = boundary_tick_bucket
		self._default_partial_bucket_handling = type(self).PartialBucketHandling.AS_SEPARATE_BUCKET
		self.partial_bucket_handling = partial_bucket_handling
		self._default_input_field1_name = ""
		self.input_field1_name = input_field1_name
		self._default_input_field2_name = ""
		self.input_field2_name = input_field2_name
		self._default_output_field_name = "VALUE"
		self.output_field_name = output_field_name
		self._default_group_by = ""
		self.group_by = group_by
		self._default_groups_to_display = type(self).GroupsToDisplay.ALL
		self.groups_to_display = groups_to_display
		self._default_bucket_end_per_group = False
		self.bucket_end_per_group = bucket_end_per_group
		self._default_keep_zero_duration_ticks = False
		self.keep_zero_duration_ticks = keep_zero_duration_ticks
		if Out != "":
			self.output_field_name=Out
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_bucket_interval(self, value):
		self.bucket_interval = value
		return self

	def set_bucket_interval_units(self, value):
		self.bucket_interval_units = value
		return self

	def set_output_interval(self, value):
		self.output_interval = value
		return self

	def set_output_interval_units(self, value):
		self.output_interval_units = value
		return self

	def set_is_running_aggr(self, value):
		self.is_running_aggr = value
		return self

	def set_bucket_time(self, value):
		self.bucket_time = value
		return self

	def set_bucket_end_criteria(self, value):
		self.bucket_end_criteria = value
		return self

	def set_boundary_tick_bucket(self, value):
		self.boundary_tick_bucket = value
		return self

	def set_partial_bucket_handling(self, value):
		self.partial_bucket_handling = value
		return self

	def set_input_field1_name(self, value):
		self.input_field1_name = value
		return self

	def set_input_field2_name(self, value):
		self.input_field2_name = value
		return self

	def set_output_field_name(self, value):
		self.output_field_name = value
		return self

	def set_group_by(self, value):
		self.group_by = value
		return self

	def set_groups_to_display(self, value):
		self.groups_to_display = value
		return self

	def set_bucket_end_per_group(self, value):
		self.bucket_end_per_group = value
		return self

	def set_keep_zero_duration_ticks(self, value):
		self.keep_zero_duration_ticks = value
		return self

	@staticmethod
	def _get_name():
		return "TS_SUBTRACT"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.bucket_interval != 0: 
			desc += "BUCKET_INTERVAL=" + py_to_str(self.bucket_interval) + ","
		if self.bucket_interval_units != self.BucketIntervalUnits.SECONDS: 
			desc += "BUCKET_INTERVAL_UNITS=" + py_to_str(self.bucket_interval_units) + ","
		if self.output_interval != "": 
			desc += "OUTPUT_INTERVAL=" + py_to_str(self.output_interval) + ","
		if self.output_interval_units != self.OutputIntervalUnits.SECONDS: 
			desc += "OUTPUT_INTERVAL_UNITS=" + py_to_str(self.output_interval_units) + ","
		if self.is_running_aggr != True: 
			desc += "IS_RUNNING_AGGR=" + py_to_str(self.is_running_aggr) + ","
		if self.bucket_time != self.BucketTime.BUCKET_END: 
			desc += "BUCKET_TIME=" + py_to_str(self.bucket_time) + ","
		if self.bucket_end_criteria != "": 
			desc += "BUCKET_END_CRITERIA=" + py_to_str(self.bucket_end_criteria) + ","
		if self.boundary_tick_bucket != self.BoundaryTickBucket.NEW: 
			desc += "BOUNDARY_TICK_BUCKET=" + py_to_str(self.boundary_tick_bucket) + ","
		if self.partial_bucket_handling != self.PartialBucketHandling.AS_SEPARATE_BUCKET: 
			desc += "PARTIAL_BUCKET_HANDLING=" + py_to_str(self.partial_bucket_handling) + ","
		if self.input_field1_name != "": 
			desc += "INPUT_FIELD1_NAME=" + py_to_str(self.input_field1_name) + ","
		if self.input_field2_name != "": 
			desc += "INPUT_FIELD2_NAME=" + py_to_str(self.input_field2_name) + ","
		if self.output_field_name != "VALUE": 
			desc += "OUTPUT_FIELD_NAME=" + py_to_str(self.output_field_name) + ","
		if self.group_by != "": 
			desc += "GROUP_BY=" + py_to_str(self.group_by) + ","
		if self.groups_to_display != self.GroupsToDisplay.ALL: 
			desc += "GROUPS_TO_DISPLAY=" + py_to_str(self.groups_to_display) + ","
		if self.bucket_end_per_group != False: 
			desc += "BUCKET_END_PER_GROUP=" + py_to_str(self.bucket_end_per_group) + ","
		if self.keep_zero_duration_ticks != False: 
			desc += "KEEP_ZERO_DURATION_TICKS=" + py_to_str(self.keep_zero_duration_ticks) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class ShowCompressedFileContent(_graph_components.EpBase):
	"""
		

SHOW_COMPRESSED_FILE_CONTENT

Type: Other

Description: Shows the content of the specified compressed
file/archive. Timestamps of the ticks created by
SHOW_COMPRESSED_FILE_CONTENT are set to the start time of the query.

When this function is invoked on the remote server, symbol names for
the graph must name the files (absolute path) on the remote file
system. To see the file content of the client's local box, the database
of the input symbols should be specified as LOCAL.

Python
class name:&nbsp;ShowCompressedFileContent

Input: None

Output: A time series of ticks.

Parameters:


  RECURSIVE_LISTING (Boolean)
    If set, the EP returns all available paths inside compressed
files.
Default value: false

  

Examples:

Shows the content of the compressed file or archive, the path to
which is specified in symbols:

SHOW_COMPRESSED_FILE_CONTENT ()


	"""
	class Parameters:
		recursive_listing = "RECURSIVE_LISTING"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["recursive_listing"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["recursive_listing", "_default_recursive_listing", "stack_info", "_used_strings"]

	def __init__(self, recursive_listing=False):
		_graph_components.EpBase.__init__(self, "SHOW_COMPRESSED_FILE_CONTENT")
		self._default_recursive_listing = False
		self.recursive_listing = recursive_listing
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_recursive_listing(self, value):
		self.recursive_listing = value
		return self

	@staticmethod
	def _get_name():
		return "SHOW_COMPRESSED_FILE_CONTENT"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.recursive_listing != False: 
			desc += "RECURSIVE_LISTING=" + py_to_str(self.recursive_listing) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class CsvFileListing(_graph_components.EpBase):
	"""
		

CSV_FILE_LISTING

Type: Other

Description: Converts each line of a delimiter-separated
ASCII file into a tick. If the first line of the file begins with #, the fields of this line are considered
field names for the generated ticks. Otherwise, field names are named COLUMN1, COLUMN2,
etc. A field name can be preceded by a type name, for supported type
names see here.
The only exception is the matrix type declaration which for this EP
should have the following format: matrix(&lt;subtype&amp;gt:&lt;num_dimensions&gt;).
White spaces within matrix type declaration are not allowed. Each row
of the file must contain the same number of columns. Column
descriptions are separated by a comma or a whitespace (that is, space
or tab).

Files read by CSV_FILE_LISTING are determined by the list of symbols
for the graph or by both the list of symbols and FILE_NAME_REGEX
parameter. Each symbol contains an absolute or relative path of the
file or directory of files to be read. Compressed/archive files are
also supported, such as GZIP, ZIP and other file types for which custom
compression module is specified in the OneTick configuration file (for
example, EXTRA_COMPRESSION_MODULES=OMD_LIBARCHIVE
for reading TAR files).

URLs (like http://csvfiles.com/data.csv)
are also supported. For compressed/archive files specified via URL only
GZIP format is supported currently.

Files specified via HDFS location (like java_com/omd/fs/hdfs/HDFSFileSystemCreator:/&lt;namenode&gt;:&lt;port&gt;/&lt;hdfs
path&gt;) are also supported. For this the OMD HDFS driver is
used, which allows reading data without using additional programs, like
FUSE and NFS. The OMD HDFS driver uses HDFS native API to retrieve data
from HDFS without the need of the filesystem to be mounted. For OMD
HDFS configuration see hdfs_support.html

Files stored in Amazon S3 (s3://),
Azure Blob Storage (azblob://), and
Google Cloud Storage (gsc://) are
also supported.

The CSV_FILE_PATH parameter in the OneTick configuration file
(one_tick_conf.txt) specifies the set of directories where
CSV_FILE_LISTING looks for the file if an input symbol contains a
relative path to this file or its directory.

Under certain circumstances, namely, when running on a tick server
with the TICK_SERVER_CSV_CACHE_DIR OneTick configuration variable
pointing to a local directory, CSV_FILE_LISTING EP will make an attempt
to fetch the file from the client host, in case if the file is not
found locally. Fetched files are cached in the directory, pointed by
TICK_SERVER_CSV_CACHE_DIR variable. In case if the file is fetched from
the client, FILE_READ_INTERVAL, ALLOW_FILE_OVERWITE, and
FILE_NAME_REGEX parameters are not expected to be specified. Note that
the client might itself be a tick server, too (proxy server), which,
after a failed attempt to find the file locally on its own host, will
then try to delegate the request to its client, and so on until the
file is found somewhere or not found at all, resulting in an exception.
Each client in such a chain will use its own CSV_FILE_PATH
configuration variable to look for files specified by relative paths.

When this function is invoked on the remote server, symbol names for
the graph must name the files/directories on the remote file system,
and the CSV_FILE_PATH specified in the configuration file of the remote
server will be used.

To convert the contents of the client's local file into ticks, the
database of the input symbols should be specified as LOCAL.

Non-first lines of the file that begin with&nbsp;# &nbsp;are considered to be commented out.

Python
class name:&nbsp;CsvFileListing

Input: None

Output: A time series of ticks.

Parameters:


  DISCARD_TIMESTAMP_COLUMN
(Boolean)
    Switches propagation of the column named TIMESTAMP as a tick
field, if any. If TIME_ASSIGNMENT
parameter is set to a field name, this option applies to that field,
rather than to TIMESTAMP field.
Default value: false

  
  TIME_ASSIGNMENT
    Possible values are _START_TIME,
    _END_TIME and
    &lt;field name&gt;. In case of
    _START_TIME and_END_TIME,
the timestamps of the ticks are set to the start and end time of the
query
respectively. If it's set to a field name, the timestamps of the ticks
are extracted
from that field. Possible types for timestamp fields are long, ulong,
msectime, nsectime, double, decimal, string and varstring. For string
and varstring types, both YYYYMMDDhhmmss[.qqqqqq]
and an offset in milliseconds from 19700101 00:00:00 GMT are supported.
The timezone of the field is
always assumed to be GMT. Only the
lines with timestamp represented by this field being greater than or
equal to the start time of the query and less than the end time of the
query will be propagated to the output of this EP.&nbsp;
    Note that when timestamp field is used, the CSV file
needs to be sorted by that field in ascending order.
Default value: _END_TIME.

  
  FILE_READ_INTERVAL (real)
    A nonnegative real number (time in seconds) which, if not 0,
allows reading the file continuously. Once in every specified duration
of time, the EP would check whether the file has been updated and then
propagate the newly added ticks. (The file can be updated only by
adding new lines starting at the end of the file. Even if the file was
overwritten, its previous content would not be changed). To make this
option work, the "continuous query" flag should be switched on. Also,
the last character of the file should be a newline (CRLF, CR or LF).
Otherwise, characters after the last newline would be interpreted as a
partial line and wouldn't be processed.
Default value: 0 (for non-continuous reading)

  
  ALLOW_FILE_OVERWRITE (Boolean)
    If set to true, overwriting or replacing the file during the
query is allowed. As a result, the EP works a bit slower because it has
to reopen the file after every file read interval. If
ALLOW_FILE_OVERWRITE is set to false, the file is expected to be only
appended to.

    Notice that editors replace the file when it is saved. When
using an editor to make changes to the file, ALLOW_FILE_OVERWRITE must
be set to true.
Default value: false

  
  FIELD_DELIMITERS (string)
    A list of characters enclosed in single or double quotes (for
instance "=, :" equal sign, comma, space, colon or '.@;' dot, at and
semicolon). These characters are used to tokenize each line of the CSV
file. For a tab character \\t (back-slash followed by t) should
be specified. The same character cannot be marked both as the quote
character and as the field delimiter.
Default value: ", " (comma, space)

  
  QUOTE_CHARS (string)
    A list of characters enclosed in single or double quotes that
are used to mark quotation in the input CSV file[s] (for instance " " '
" double quotes and single quote or '|:/' vertical bar, colon, and
slash). A piece of text in the .cvs file enclosed between the same
quote chars does not get tokenized no matter whether it contains field
delimiters or not. The same character cannot be marked both as the
quote character and as the field delimiter. Besides, space characters
cannot be used as quotes so they are ignored when specified.
Use "" in order for QUOTE_CHARS to be empty.
Default value: " " ' " (double quotes, single quote)

  
  STORE_FILE_IN_ONE_FIELD
(Boolean)
    If set, the EP creates a single tick with the field COLUMN1 from
the entire contents of the file and the field FILE_LOCATION that will
contain the full path of the file on the host.
Default value: false

  
  FILE_NAME_REGEX (string)
    If set, then it is assumed that each symbol of the graph is an
absolute or relative directory path where the CSV files should be
looked for. And that all files from that directory that match the
specified regex pattern should be processed by the EP in lexicographic
order of their names.
In the case of continuous file listing, at first all-matching files
will be listed and only the file with the highest lexicographic name
will be tracked for further changes. Apart from that, if the new file
with a higher lexicographic name appears in the tracking directory
during the continuous file listing, it will start to be listed by the
EP and the previous file will be released.
Default value: &lt;empty&gt;

  
  FILE_CONTENTS (string)
    If not empty, the EP treats its value as a content of the file
from which it should read and construct ticks. The file name can be
omitted as it is not used. CSV_FILE_LISTING will use the contents of
the local file rather than open a file on the server. In order to fill
the FILE_CONTENTS parameter, this parameter can be set to the value of
eval(...) where eval will call CSV_FILE_LISTING for
LOCAL::&lt;filename&gt; and STORE_FILE_IN_ONE_FIELD=true.
Default value: &lt;empty&gt;

  
  USE_FIELD_DELIMITERS_FOR_TITLE
(Boolean)
    If set, the EP will use field delimiters while processing the
first line starting with # (title). Formerly only ", " (comma, space)
were used as title delimiters.
Default value: false

  
  HANDLE_ESCAPED_CHARS (Boolean)
    If set, the backslash char '\\' gets a special meaning
and everywhere in the input text the combinations \\', \\"
and \\\\ are changed correspondingly by ', " and \\,
which are processed then as regular chars. Besides, combinations like \\x??,
where ?-s are hexadecimal digits (0-9, a-f or A-F), are changed
by the chars with the specified ASCII code. For example, \\x0A
will be replaced by a newline character, \\x09 will be replaced
by tab, and so on.
Default value: false

  
  FIRST_LINE_IS_TITLE (Boolean)
    If set to true, the first line should be treated as the title
even if it does not start by #. If set to false, the first line should
be treated as the title only if it starts by #.
Default value: false

  
  FIELDS (string)
    Lists field names for the generated ticks. A field name can be
preceded by a type name, for supported type names see here. If a
CSV file does not have a title line at all, it can be taken directly
from this parameter (in this case, this parameter must list as many
field names as there are columns in the file). If a CSV file does have
a title line (with or without the initial hash character) and it
mismatches with FIELD parameter, an exception would be thrown.
Default value: &lt;empty&gt;

  
  WHITESPACE_DELIM_HANDLING
(enum)
    Sets handling mode of adjacent whitespace delimiters. Supported
values are TREAT_ADJACENT_AS_ONE and IGNORE_ADJASCENCE. If
WHITESPACE_DELIM_HANDLING is set to TREAT_ADJACENT_AS_ONE, a continuous
sequence of whitespace characters is considered a single delimiter, to
ignore this use IGNORE_ADJASCENCE.

  

Access control:

By default, all users are allowed to use this event processor for
reading from any directory. Limitations can be applied using access
control. If CSV_FILE_LISTING is present in an access control event
processor list, then only the specified roles can use this event
processor. Additionally, for each role allowed directories can be
specified. If some directory is allowed, all of its subdirectories are
allowed. Omitting this parameter means that all directories are
allowed. If a user has several roles, she has access to a directory, if
allowed, for at least one of her roles. Extra search paths for CSV
files (in addition to CSV_FILE_PATH) can be specified for roles and in
event processor level as well. One can also specify allowed directories
list in the event processor level (not only role level). This means
that directories listed in the event processor level are allowed for
all users (whose roles are not mentioned under the section for this
event processor, or the user does not have any role in the access
control file). Role level permissions override event processor level
permissions for the users whose roles are mentioned in access control
for this EP.

Directories are specified using any separator symbol (comma is the
default). Here is an example on how to specify role level permissions:

&lt;allow role="developers"
directories="/home/developer;/work"
extra_search_path="/home/developer/CSV" path_separator=";"/&gt;

Here is an example on how to specify item level permissions:

&lt;ep id="CSV_FILE_LISTING"
directories="/home/developer;/work;" path_separator=";"&gt;

Examples:

Convert the contents of files specified as input symbols into ticks:

CSV_FILE_LISTING ()

See CSV_FILE_LISTING in OTHER_EXAMPLES.otq.


	"""
	class Parameters:
		discard_timestamp_column = "DISCARD_TIMESTAMP_COLUMN"
		time_assignment = "TIME_ASSIGNMENT"
		file_read_interval = "FILE_READ_INTERVAL"
		allow_file_overwrite = "ALLOW_FILE_OVERWRITE"
		field_delimiters = "FIELD_DELIMITERS"
		quote_chars = "QUOTE_CHARS"
		store_file_in_one_field = "STORE_FILE_IN_ONE_FIELD"
		file_name_regex = "FILE_NAME_REGEX"
		file_contents = "FILE_CONTENTS"
		use_field_delimiters_for_title = "USE_FIELD_DELIMITERS_FOR_TITLE"
		handle_escaped_chars = "HANDLE_ESCAPED_CHARS"
		first_line_is_title = "FIRST_LINE_IS_TITLE"
		fields = "FIELDS"
		whitespace_delim_handling = "WHITESPACE_DELIM_HANDLING"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["discard_timestamp_column", "time_assignment", "file_read_interval", "allow_file_overwrite", "field_delimiters", "quote_chars", "store_file_in_one_field", "file_name_regex", "file_contents", "use_field_delimiters_for_title", "handle_escaped_chars", "first_line_is_title", "fields", "whitespace_delim_handling"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["discard_timestamp_column", "_default_discard_timestamp_column", "time_assignment", "_default_time_assignment", "file_read_interval", "_default_file_read_interval", "allow_file_overwrite", "_default_allow_file_overwrite", "field_delimiters", "_default_field_delimiters", "quote_chars", "_default_quote_chars", "store_file_in_one_field", "_default_store_file_in_one_field", "file_name_regex", "_default_file_name_regex", "file_contents", "_default_file_contents", "use_field_delimiters_for_title", "_default_use_field_delimiters_for_title", "handle_escaped_chars", "_default_handle_escaped_chars", "first_line_is_title", "_default_first_line_is_title", "fields", "_default_fields", "whitespace_delim_handling", "_default_whitespace_delim_handling", "stack_info", "_used_strings"]

	class WhitespaceDelimHandling:
		IGNORE_ADJASCENCE = "IGNORE_ADJASCENCE"
		TREAT_ADJACENT_AS_ONE = "TREAT_ADJACENT_AS_ONE"

	def __init__(self, discard_timestamp_column=False, time_assignment="_END_TIME", file_read_interval=0, allow_file_overwrite=False, field_delimiters="\", \"", quote_chars="\" \" ' \"", store_file_in_one_field=False, file_name_regex="", file_contents="", use_field_delimiters_for_title=False, handle_escaped_chars=False, first_line_is_title=False, fields="", whitespace_delim_handling=WhitespaceDelimHandling.TREAT_ADJACENT_AS_ONE):
		_graph_components.EpBase.__init__(self, "CSV_FILE_LISTING")
		self._default_discard_timestamp_column = False
		self.discard_timestamp_column = discard_timestamp_column
		self._default_time_assignment = "_END_TIME"
		self.time_assignment = time_assignment
		self._default_file_read_interval = 0
		self.file_read_interval = file_read_interval
		self._default_allow_file_overwrite = False
		self.allow_file_overwrite = allow_file_overwrite
		self._default_field_delimiters = "\", \""
		self.field_delimiters = field_delimiters
		self._default_quote_chars = "\" \" ' \""
		self.quote_chars = quote_chars
		self._default_store_file_in_one_field = False
		self.store_file_in_one_field = store_file_in_one_field
		self._default_file_name_regex = ""
		self.file_name_regex = file_name_regex
		self._default_file_contents = ""
		self.file_contents = file_contents
		self._default_use_field_delimiters_for_title = False
		self.use_field_delimiters_for_title = use_field_delimiters_for_title
		self._default_handle_escaped_chars = False
		self.handle_escaped_chars = handle_escaped_chars
		self._default_first_line_is_title = False
		self.first_line_is_title = first_line_is_title
		self._default_fields = ""
		self.fields = fields
		self._default_whitespace_delim_handling = type(self).WhitespaceDelimHandling.TREAT_ADJACENT_AS_ONE
		self.whitespace_delim_handling = whitespace_delim_handling
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_discard_timestamp_column(self, value):
		self.discard_timestamp_column = value
		return self

	def set_time_assignment(self, value):
		self.time_assignment = value
		return self

	def set_file_read_interval(self, value):
		self.file_read_interval = value
		return self

	def set_allow_file_overwrite(self, value):
		self.allow_file_overwrite = value
		return self

	def set_field_delimiters(self, value):
		self.field_delimiters = value
		return self

	def set_quote_chars(self, value):
		self.quote_chars = value
		return self

	def set_store_file_in_one_field(self, value):
		self.store_file_in_one_field = value
		return self

	def set_file_name_regex(self, value):
		self.file_name_regex = value
		return self

	def set_file_contents(self, value):
		self.file_contents = value
		return self

	def set_use_field_delimiters_for_title(self, value):
		self.use_field_delimiters_for_title = value
		return self

	def set_handle_escaped_chars(self, value):
		self.handle_escaped_chars = value
		return self

	def set_first_line_is_title(self, value):
		self.first_line_is_title = value
		return self

	def set_fields(self, value):
		self.fields = value
		return self

	def set_whitespace_delim_handling(self, value):
		self.whitespace_delim_handling = value
		return self

	@staticmethod
	def _get_name():
		return "CSV_FILE_LISTING"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.discard_timestamp_column != False: 
			desc += "DISCARD_TIMESTAMP_COLUMN=" + py_to_str(self.discard_timestamp_column) + ","
		if self.time_assignment != "_END_TIME": 
			desc += "TIME_ASSIGNMENT=" + py_to_str(self.time_assignment) + ","
		if self.file_read_interval != 0: 
			desc += "FILE_READ_INTERVAL=" + py_to_str(self.file_read_interval) + ","
		if self.allow_file_overwrite != False: 
			desc += "ALLOW_FILE_OVERWRITE=" + py_to_str(self.allow_file_overwrite) + ","
		if self.field_delimiters != "\", \"": 
			desc += "FIELD_DELIMITERS=" + py_to_str(self.field_delimiters) + ","
		if self.quote_chars != "\" \" ' \"": 
			desc += "QUOTE_CHARS=" + py_to_str(self.quote_chars) + ","
		if self.store_file_in_one_field != False: 
			desc += "STORE_FILE_IN_ONE_FIELD=" + py_to_str(self.store_file_in_one_field) + ","
		if self.file_name_regex != "": 
			desc += "FILE_NAME_REGEX=" + py_to_str(self.file_name_regex) + ","
		if self.file_contents != "": 
			desc += "FILE_CONTENTS=" + py_to_str(self.file_contents) + ","
		if self.use_field_delimiters_for_title != False: 
			desc += "USE_FIELD_DELIMITERS_FOR_TITLE=" + py_to_str(self.use_field_delimiters_for_title) + ","
		if self.handle_escaped_chars != False: 
			desc += "HANDLE_ESCAPED_CHARS=" + py_to_str(self.handle_escaped_chars) + ","
		if self.first_line_is_title != False: 
			desc += "FIRST_LINE_IS_TITLE=" + py_to_str(self.first_line_is_title) + ","
		if self.fields != "": 
			desc += "FIELDS=" + py_to_str(self.fields) + ","
		if self.whitespace_delim_handling != self.WhitespaceDelimHandling.TREAT_ADJACENT_AS_ONE: 
			desc += "WHITESPACE_DELIM_HANDLING=" + py_to_str(self.whitespace_delim_handling) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class CsvFileQuery(_graph_components.EpBase):
	"""
		

CSV_FILE_QUERY

Type: Other

Description: Converts each line of a delimiter-separated
ASCII file into a tick, and queries the result against a list of
symbols. Each row of the file must contain the same number of columns.
The first line of the file must begin with #. Fields on this line are
considered to be field names for generated ticks. A field name can be
preceded by type name, for supported type names see here.
The only exception is the matrix type declaration which for this EP
should have the following format: matrix(&lt;subtype&amp;gt:&lt;num_dimensions&gt;).
White spaces within matrix type declaration are not allowed. Column
descriptions are separated by comma or white space. When a white space
character (i.e., space or tab) is chosen as a delimiter, a continuous
sequence of white space characters is considered a single delimiter.

The string/varstring field SYMBOL_NAME must always be present among
fields for values of one of the columns to be interpreted as symbol
names.

The CSV_FILE_QUERY will run on the computer to which the queried
symbols belong, which is determined by the database of the queried
symbols. Use of LOCAL:: for symbol names will trigger local operation.

Python
class name:&nbsp;CsvFileQuery

Input: None

Output: A time series of ticks.

Parameters:


  FILE (string)
    The path to a comma-separated or white space-separated ASCII
file. The parameter CSV_FILE_PATH in the OneTick configuration file
specifies the set of directories where CSV_FILE_QUERY looks for the
file if the value of this parameter is a relative path to this file.
When this function is invoked on a remote server, the value of this
parameter must name a file on the remote file system, and CSV_FILE_PATH
specified in the configuration file of the remote server will be used.

    Under certain circumstances, namely, when run on a tick server
with TICK_SERVER_CSV_CACHE_DIR OneTick configuration variable pointing
to some local directory, CSV_FILE_QUERY EP will make an attempt to
fetch the file from the client host, in case if the file is not found
locally. Fetched files are cached in the directory, pointed by
TICK_SERVER_CSV_CACHE_DIR variable. In case if the file is fetched from
the client, FILE_READ_INTERVAL and ALLOW_FILE_OVERWRITE parameters are
not expected to be specified. Note, that the client might itself be a
tick server, too (proxy server), which, after a failed attempt to find
the file locally on its own host, will then try to delegate the request
to its client, and so on until the file is found somewhere or not found
at all, resulting in an exception. Each client in such a chain will use
its own CSV_FILE_PATH configuration variable to look for files
specified by relative paths.

    Compressed/archive files are also supported, such as gzip, zip
and other file types for which custom compression module is specified
in the OneTick configuration file (for example: EXTRA_COMPRESSION_MODULES=OMD_LIBARCHIVE
for reading tar files).

    URLs (like: http://csvfiles.com/data.csv) are also supported in
this parameter. For compressed/archive files specified via URL only
gzip format is supported currently.

    Files stored in Amazon S3 (s3://),
Azure Blob Storage (azblob://), and
Google Cloud Storage (gsc://) are
also supported.

  
  SYMBOLOGY (string)
    SYMBOLOGY of symbol names (values of SYMBOL_NAME field) of the
csv file. If specified, in order to find correct time series in the
file for query symbol the EP will first find synonym[s] of this symbols
in the specified symbology using reference database. Then it will find
the history of this new symbol for the query interval and finally will
query the csv file using the symbol names from history.

  
  DISCARD_TIMESTAMP_COLUMN
(Boolean)
    Switches propagation of the column named TIMESTAMP as a tick
field, if one exists. If TIME_ASSIGNMENT
parameter is set to a field name, this option applies to that field,
rather than to TIMESTAMP field.
Default value: false.

  
  TIME_ASSIGNMENT
    Possible values are _START_TIME,
    _END_TIME and
    &lt;field name&gt;. In case of
    _START_TIME and_END_TIME,
the timestamps of the ticks are set to the start and end time of the
query
respectively. If it's set to a field name, the timestamps of the ticks
are extracted
from that field. Possible types for timestamp fields are long, ulong,
msectime, nsectime, double, decimal, string and varstring. For string
and varstring types, both YYYYMMDDhhmmss[.qqqqqq]
and an offset in milliseconds from 19700101 00:00:00 GMT are supported.
The timezone of the field is
always assumed to be GMT.&nbsp;Only
the lines with&nbsp;timestamp represented by this field being greater
than or equal to the start time of the query and less than the end time
of the query will be propagated to the output of this EP. If
    TIME_ASSIGNMENT parameter is not
set to a field name, but TIMESTAMP
column
exists, it'll be used as timestamp field.
    Note that when timestamp field is used, the CSV file
needs to be sorted by that field in ascending order.
Default value: _END_TIME.

  
  FILE_READ_INTERVAL (real)
    A nonnegative real number (time in seconds) which, if not 0,
allows reading the file continuously. Once in every specified duration
of time the EP would check whether the file has been updated and then
propagate new added ticks. (The file can be updated only by adding new
lines starting at the end of the file. Even if the file was
overwritten, its previous content wouldn't be changed). To make this
option work, the "continuous query" flag should be switched on. Also,
the last character of the file should be a newline (CRLF, CR or LF),
otherwise characters after the last newline would be interpreted as a
partial line and wouldn't be processed. Default value: 0 (for
non-continuous reading).

  
  ALLOW_FILE_OVERWRITE (Boolean)
    If set to true, overwriting or replacing the file during the
query is allowed. As a result, the EP works a bit slower because it has
to reopen the file after every file read interval. If
ALLOW_FILE_OVERWRITE is set to false, the file is expected to be only
appended to.

    Notice that editors replace the file when it is saved. When
using an editor to make changes to the file, ALLOW_FILE_OVERWRITE must
be set to true.

    Default value: false.

  
  FIELD_DELIMITERS (string)
    A list of characters enclosed in single or double quotes (for
instance "=, :" equal sign, comma, space, colon or '.@;' dot, at and
semicolon). These characters are used to tokenize each line of the .csv
file. For a tab character \\t should be specified. The same
character cannot be marked both as quote char and as field delimiter.
Default value: ", " (comma, space)

  
  QUOTE_CHARS (string)
    A list of characters enclosed in single or double quotes that
are used to mark quotation in the input .csv file[s] (for instance, " "
' " double quotes and single quote or '|:/' vertical bar, colon and
slash). A piece of text in the .cvs file enclosed between the same
quote chars does not get tokenized no matter whether it contains field
delimiters or not. The same character cannot be marked both as quote
char and as field delimiter, besides that space characters cannot be
used as quotes so they are ignored when specified.
Use "" in order for QUOTE_CHARS to be empty.
Default value: " " ' " (double quotes, single quote)

  
  FILE_CONTENTS (string)
    If not empty, the EP treats its value as a content of the file
from which it should read and construct ticks. The file name can be
omitted as it is not used.

    Default value: &lt;empty&gt;

  
  HANDLE_ESCAPED_CHARS (Boolean)
    If set, the backslash char '\\' gets a special meaning
and everywhere in the input text the combinations \\', \\"
and \\\\ are changed correspondingly by ', " and \\,
which are processed then as regular chars. Besides that combinations
like \\x??, where ?-s are hexadecimal digits (0-9, a-f
or A-F), are changed by the chars with the specified ASCII code. For
example \\x0A will be replaced by a newline character, \\x09
will be replaced by tab, etc..

    Default value: false

  
  FIRST_LINE_IS_TITLE (Boolean)
    If set to true, the first line should be treated as title even
if it does not start from #. If set to false, the first line should be
treated as title only if it starts from #
Default value: false

  
  FIELDS (string)
    Lists field names for the generated ticks. A field name can be
preceded by a type name, for supported type names see here. If a
CSV file does not have a title line at all, it can be taken directly
from this parameter (in this case, this parameter must list as many
field names as there are columns in the file). If a CSV file does have
a title line (with or without initial hash character) and it mismatches
with FIELD parameter, an exception would be thrown.
Default value: &lt;empty&gt;

  

Access control: By default, all users are allowed to use this
event processor for reading from any directory. Limitations can be
applied using access control. If CSV_FILE_QUERY is present in the
access control event processor list, only the specified roles can use
this event processor. Additionally, for each role allowed directories
can be specified. If some directory is allowed, all of its
subdirectories are allowed. Omitting this parameter means that all
directories are allowed. If a user has several roles, she has access to
a directory, if allowed, for at least one of her roles. Extra search
paths for .csv files (in addition to CSV_FILE_PATH) can be specified
for roles and in event processor level as well. One can also specify
allowed directories list in the event processor level (not only role
level). This means that directories listed in the event processor level
are allowed for all users (whose roles aren't mentioned under the
section for this event processor, or user does not have any role in the
access control file). Role level permissions override event processor
level permissions for the users whose roles are mentioned in access
control for this ep.

Directories are specified using any separator symbol (comma is the
default). Here is an example how to specify role level permissions:

&lt;allow role="developers"
directories="/home/developer;/work"
extra_search_path="/home/developer/csv" path_separator=";"/&gt;

Here is an example how to specify event processor level permissions:

&lt;ep id="CSV_FILE_QUERY"
directories="/home/developer;/work" path_separator=";"&gt;


	"""
	class Parameters:
		file = "FILE"
		symbology = "SYMBOLOGY"
		discard_timestamp_column = "DISCARD_TIMESTAMP_COLUMN"
		time_assignment = "TIME_ASSIGNMENT"
		file_read_interval = "FILE_READ_INTERVAL"
		allow_file_overwrite = "ALLOW_FILE_OVERWRITE"
		field_delimiters = "FIELD_DELIMITERS"
		quote_chars = "QUOTE_CHARS"
		file_contents = "FILE_CONTENTS"
		handle_escaped_chars = "HANDLE_ESCAPED_CHARS"
		first_line_is_title = "FIRST_LINE_IS_TITLE"
		fields = "FIELDS"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["file", "symbology", "discard_timestamp_column", "time_assignment", "file_read_interval", "allow_file_overwrite", "field_delimiters", "quote_chars", "file_contents", "handle_escaped_chars", "first_line_is_title", "fields"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["file", "_default_file", "symbology", "_default_symbology", "discard_timestamp_column", "_default_discard_timestamp_column", "time_assignment", "_default_time_assignment", "file_read_interval", "_default_file_read_interval", "allow_file_overwrite", "_default_allow_file_overwrite", "field_delimiters", "_default_field_delimiters", "quote_chars", "_default_quote_chars", "file_contents", "_default_file_contents", "handle_escaped_chars", "_default_handle_escaped_chars", "first_line_is_title", "_default_first_line_is_title", "fields", "_default_fields", "stack_info", "_used_strings"]

	def __init__(self, file="", symbology="", discard_timestamp_column=False, time_assignment="_END_TIME", file_read_interval=0, allow_file_overwrite=False, field_delimiters="\", \"", quote_chars="\" \" ' \"", file_contents="", handle_escaped_chars=False, first_line_is_title=False, fields=""):
		_graph_components.EpBase.__init__(self, "CSV_FILE_QUERY")
		self._default_file = ""
		self.file = file
		self._default_symbology = ""
		self.symbology = symbology
		self._default_discard_timestamp_column = False
		self.discard_timestamp_column = discard_timestamp_column
		self._default_time_assignment = "_END_TIME"
		self.time_assignment = time_assignment
		self._default_file_read_interval = 0
		self.file_read_interval = file_read_interval
		self._default_allow_file_overwrite = False
		self.allow_file_overwrite = allow_file_overwrite
		self._default_field_delimiters = "\", \""
		self.field_delimiters = field_delimiters
		self._default_quote_chars = "\" \" ' \""
		self.quote_chars = quote_chars
		self._default_file_contents = ""
		self.file_contents = file_contents
		self._default_handle_escaped_chars = False
		self.handle_escaped_chars = handle_escaped_chars
		self._default_first_line_is_title = False
		self.first_line_is_title = first_line_is_title
		self._default_fields = ""
		self.fields = fields
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_file(self, value):
		self.file = value
		return self

	def set_symbology(self, value):
		self.symbology = value
		return self

	def set_discard_timestamp_column(self, value):
		self.discard_timestamp_column = value
		return self

	def set_time_assignment(self, value):
		self.time_assignment = value
		return self

	def set_file_read_interval(self, value):
		self.file_read_interval = value
		return self

	def set_allow_file_overwrite(self, value):
		self.allow_file_overwrite = value
		return self

	def set_field_delimiters(self, value):
		self.field_delimiters = value
		return self

	def set_quote_chars(self, value):
		self.quote_chars = value
		return self

	def set_file_contents(self, value):
		self.file_contents = value
		return self

	def set_handle_escaped_chars(self, value):
		self.handle_escaped_chars = value
		return self

	def set_first_line_is_title(self, value):
		self.first_line_is_title = value
		return self

	def set_fields(self, value):
		self.fields = value
		return self

	@staticmethod
	def _get_name():
		return "CSV_FILE_QUERY"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.file != "": 
			desc += "FILE=" + py_to_str(self.file) + ","
		if self.symbology != "": 
			desc += "SYMBOLOGY=" + py_to_str(self.symbology) + ","
		if self.discard_timestamp_column != False: 
			desc += "DISCARD_TIMESTAMP_COLUMN=" + py_to_str(self.discard_timestamp_column) + ","
		if self.time_assignment != "_END_TIME": 
			desc += "TIME_ASSIGNMENT=" + py_to_str(self.time_assignment) + ","
		if self.file_read_interval != 0: 
			desc += "FILE_READ_INTERVAL=" + py_to_str(self.file_read_interval) + ","
		if self.allow_file_overwrite != False: 
			desc += "ALLOW_FILE_OVERWRITE=" + py_to_str(self.allow_file_overwrite) + ","
		if self.field_delimiters != "\", \"": 
			desc += "FIELD_DELIMITERS=" + py_to_str(self.field_delimiters) + ","
		if self.quote_chars != "\" \" ' \"": 
			desc += "QUOTE_CHARS=" + py_to_str(self.quote_chars) + ","
		if self.file_contents != "": 
			desc += "FILE_CONTENTS=" + py_to_str(self.file_contents) + ","
		if self.handle_escaped_chars != False: 
			desc += "HANDLE_ESCAPED_CHARS=" + py_to_str(self.handle_escaped_chars) + ","
		if self.first_line_is_title != False: 
			desc += "FIRST_LINE_IS_TITLE=" + py_to_str(self.first_line_is_title) + ","
		if self.fields != "": 
			desc += "FIELDS=" + py_to_str(self.fields) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class LinearRegression(_graph_components.EpBase):
	"""
		

LINEAR_REGRESSION

Type: Aggregation

Description: For each bucket, computes the linear regression parameters slope and intercept of specified input fields DEPENDENT_VARIABLE_FIELD_NAME and 
INDEPENDENT_VARIABLE_FIELD_NAME. Adds computed parameters as SLOPE and INTERCEPT fields in output time series.
The relationship between the dependent variable (Y) and the independent variable (X) is defined by the formula: Y = SLOPE * X + INTERCEPT, 
where SLOPE and INTERCEPT are the calculated output parameters.


	"""
	class Parameters:
		bucket_interval = "BUCKET_INTERVAL"
		bucket_interval_units = "BUCKET_INTERVAL_UNITS"
		output_interval = "OUTPUT_INTERVAL"
		output_interval_units = "OUTPUT_INTERVAL_UNITS"
		is_running_aggr = "IS_RUNNING_AGGR"
		bucket_time = "BUCKET_TIME"
		bucket_end_criteria = "BUCKET_END_CRITERIA"
		boundary_tick_bucket = "BOUNDARY_TICK_BUCKET"
		all_fields_for_sliding = "ALL_FIELDS_FOR_SLIDING"
		partial_bucket_handling = "PARTIAL_BUCKET_HANDLING"
		dependent_variable_field_name = "DEPENDENT_VARIABLE_FIELD_NAME"
		independent_variable_field_name = "INDEPENDENT_VARIABLE_FIELD_NAME"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["bucket_interval", "bucket_interval_units", "output_interval", "output_interval_units", "is_running_aggr", "bucket_time", "bucket_end_criteria", "boundary_tick_bucket", "all_fields_for_sliding", "partial_bucket_handling", "dependent_variable_field_name", "independent_variable_field_name"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["bucket_interval", "_default_bucket_interval", "bucket_interval_units", "_default_bucket_interval_units", "output_interval", "_default_output_interval", "output_interval_units", "_default_output_interval_units", "is_running_aggr", "_default_is_running_aggr", "bucket_time", "_default_bucket_time", "bucket_end_criteria", "_default_bucket_end_criteria", "boundary_tick_bucket", "_default_boundary_tick_bucket", "all_fields_for_sliding", "_default_all_fields_for_sliding", "partial_bucket_handling", "_default_partial_bucket_handling", "dependent_variable_field_name", "_default_dependent_variable_field_name", "independent_variable_field_name", "_default_independent_variable_field_name", "stack_info", "_used_strings"]

	class BucketIntervalUnits:
		DAYS = "DAYS"
		FLEXIBLE = "FLEXIBLE"
		MONTHS = "MONTHS"
		SECONDS = "SECONDS"
		TICKS = "TICKS"

	class OutputIntervalUnits:
		SECONDS = "SECONDS"
		TICKS = "TICKS"

	class BucketTime:
		BUCKET_END = "BUCKET_END"
		BUCKET_START = "BUCKET_START"

	class BoundaryTickBucket:
		NEW = "NEW"
		PREVIOUS = "PREVIOUS"

	class AllFieldsForSliding:
		WHEN_TICKS_EXIT_WINDOW = "WHEN_TICKS_EXIT_WINDOW"
		FALSE = "false"
		TRUE = "true"

	class PartialBucketHandling:
		AS_SEPARATE_BUCKET = "AS_SEPARATE_BUCKET"
		DISCARD = "DISCARD"
		MERGE_WITH_PREVIOUS = "MERGE_WITH_PREVIOUS"

	def __init__(self, bucket_interval=0, bucket_interval_units=BucketIntervalUnits.SECONDS, output_interval="", output_interval_units=OutputIntervalUnits.SECONDS, is_running_aggr=False, bucket_time=BucketTime.BUCKET_END, bucket_end_criteria="", boundary_tick_bucket=BoundaryTickBucket.NEW, all_fields_for_sliding=AllFieldsForSliding.FALSE, partial_bucket_handling=PartialBucketHandling.AS_SEPARATE_BUCKET, dependent_variable_field_name="", independent_variable_field_name=""):
		_graph_components.EpBase.__init__(self, "LINEAR_REGRESSION")
		self._default_bucket_interval = 0
		self.bucket_interval = bucket_interval
		self._default_bucket_interval_units = type(self).BucketIntervalUnits.SECONDS
		self.bucket_interval_units = bucket_interval_units
		self._default_output_interval = ""
		self.output_interval = output_interval
		self._default_output_interval_units = type(self).OutputIntervalUnits.SECONDS
		self.output_interval_units = output_interval_units
		self._default_is_running_aggr = False
		self.is_running_aggr = is_running_aggr
		self._default_bucket_time = type(self).BucketTime.BUCKET_END
		self.bucket_time = bucket_time
		self._default_bucket_end_criteria = ""
		self.bucket_end_criteria = bucket_end_criteria
		self._default_boundary_tick_bucket = type(self).BoundaryTickBucket.NEW
		self.boundary_tick_bucket = boundary_tick_bucket
		self._default_all_fields_for_sliding = type(self).AllFieldsForSliding.FALSE
		self.all_fields_for_sliding = all_fields_for_sliding
		self._default_partial_bucket_handling = type(self).PartialBucketHandling.AS_SEPARATE_BUCKET
		self.partial_bucket_handling = partial_bucket_handling
		self._default_dependent_variable_field_name = ""
		self.dependent_variable_field_name = dependent_variable_field_name
		self._default_independent_variable_field_name = ""
		self.independent_variable_field_name = independent_variable_field_name
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_bucket_interval(self, value):
		self.bucket_interval = value
		return self

	def set_bucket_interval_units(self, value):
		self.bucket_interval_units = value
		return self

	def set_output_interval(self, value):
		self.output_interval = value
		return self

	def set_output_interval_units(self, value):
		self.output_interval_units = value
		return self

	def set_is_running_aggr(self, value):
		self.is_running_aggr = value
		return self

	def set_bucket_time(self, value):
		self.bucket_time = value
		return self

	def set_bucket_end_criteria(self, value):
		self.bucket_end_criteria = value
		return self

	def set_boundary_tick_bucket(self, value):
		self.boundary_tick_bucket = value
		return self

	def set_all_fields_for_sliding(self, value):
		self.all_fields_for_sliding = value
		return self

	def set_partial_bucket_handling(self, value):
		self.partial_bucket_handling = value
		return self

	def set_dependent_variable_field_name(self, value):
		self.dependent_variable_field_name = value
		return self

	def set_independent_variable_field_name(self, value):
		self.independent_variable_field_name = value
		return self

	@staticmethod
	def _get_name():
		return "LINEAR_REGRESSION"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.bucket_interval != 0: 
			desc += "BUCKET_INTERVAL=" + py_to_str(self.bucket_interval) + ","
		if self.bucket_interval_units != self.BucketIntervalUnits.SECONDS: 
			desc += "BUCKET_INTERVAL_UNITS=" + py_to_str(self.bucket_interval_units) + ","
		if self.output_interval != "": 
			desc += "OUTPUT_INTERVAL=" + py_to_str(self.output_interval) + ","
		if self.output_interval_units != self.OutputIntervalUnits.SECONDS: 
			desc += "OUTPUT_INTERVAL_UNITS=" + py_to_str(self.output_interval_units) + ","
		if self.is_running_aggr != False: 
			desc += "IS_RUNNING_AGGR=" + py_to_str(self.is_running_aggr) + ","
		if self.bucket_time != self.BucketTime.BUCKET_END: 
			desc += "BUCKET_TIME=" + py_to_str(self.bucket_time) + ","
		if self.bucket_end_criteria != "": 
			desc += "BUCKET_END_CRITERIA=" + py_to_str(self.bucket_end_criteria) + ","
		if self.boundary_tick_bucket != self.BoundaryTickBucket.NEW: 
			desc += "BOUNDARY_TICK_BUCKET=" + py_to_str(self.boundary_tick_bucket) + ","
		if self.all_fields_for_sliding != self.AllFieldsForSliding.FALSE: 
			desc += "ALL_FIELDS_FOR_SLIDING=" + py_to_str(self.all_fields_for_sliding) + ","
		if self.partial_bucket_handling != self.PartialBucketHandling.AS_SEPARATE_BUCKET: 
			desc += "PARTIAL_BUCKET_HANDLING=" + py_to_str(self.partial_bucket_handling) + ","
		if self.dependent_variable_field_name != "": 
			desc += "DEPENDENT_VARIABLE_FIELD_NAME=" + py_to_str(self.dependent_variable_field_name) + ","
		if self.independent_variable_field_name != "": 
			desc += "INDEPENDENT_VARIABLE_FIELD_NAME=" + py_to_str(self.independent_variable_field_name) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class DataFileQuery(_graph_components.EpBase):
	"""
		

DATA_FILE_QUERY

Type: Other

Description: Reads data streams, in supported formats, from
file or file content, processes these streams to generate ticks, and
queries the result against a list of symbols.

Continuous processing (CEP) of the data file currently is not supported.

Note: The EP is supported only on 64-bit Windows/Linux
platforms. It is not supported in VS2013 Windows builds.

Python
class name:&nbsp;DataFileQuery

Input: None

Output: A time series of ticks

Parameters:


  MSG_FORMAT
(string)
    Specifies the input format: ARROW,
    JSON

    The JSON format supports
various types of compression provided by libarchive and allows reading
from cloud storage services like S3, Azure Blob Storage (AZBLOB), and
Google Cloud Storage (GCS).

  
  FILE (string)
    Specifies the path of the file to process.
The parameter DATA_FILE_PATH in the OneTick configuration file
specifies the set of directories where DATA_FILE_QUERY looks for the
file if the value of this parameter is a relative path to this file.
    
When run on a tick server with TICK_SERVER_DATA_CACHE_DIR OneTick
configuration variable pointing to some local directory,
DATA_FILE_QUERY EP will make an attempt to fetch the file from the
client host, in case if the file is not found locally. Fetched files
are cached in the directory, pointed by TICK_SERVER_DATA_CACHE_DIR
variable.
    
To use AWS S3 file system paths, the
    AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY environment
variables must be set. 
Note: Currently, AWS S3 is supported only for the ARROW format. 

  
  SYMBOL_NAME_FIELD (string)
    Defines the field expected to contain the symbol name. Its data
type
should be either string or varstring. When one or more symbols are
associated with the query or this EP, only rows
corresponding to those symbols will be transmitted.
Default value: SYMBOL_NAME. 

  
  SYMBOLOGY (string)
    SYMBOLOGY of symbol names (values of SYMBOL_NAME_FIELD)
of the data file. If specified, in order to find correct time series in
the file for query symbol the EP will first find synonym[s] of this
symbols in the specified symbology using reference database. Then it
will find the history of this new symbol for the query interval and
finally will query the data file using the symbol names from history.

  
  TIMESTAMP_COLUMN (string)
    Specifies the field that is expected to hold the timestamp. In
case of
MSG_FORMAT=ARROW, Arrow type of this field must be DATE64 type, TIMESTAMP type or INT64 type with
metadata "TIMESTAMP_TYPE=NANO/MILLI". If provided,
the value of this field will be used to set the timestamps for the
ticks.
    

  
  TIME_ASSIGNMENT (enum)
    Specifies that the timestamps for the ticks are set to either
the start_time or end_time of the query.
Note: This functionality works only if the TIMESTAMP_COLUMN parameter
is not specified.
Default value: _END_TIME.

  
  FILE_CONTENTS (string)
    If not empty, the EP treats its value as a stream from which it
should read and construct ticks. The FILE
parameter should be omitted as it is not used.

    Default value: &lt;empty&gt;

  
  FORMAT_FILE (string)
    Specifies the absolute path of the message format file. This
parameter applies only to JSON msg format.

  
  CONFIG_DIR (string)
    Specifies the directory of the normalization file. This
parameter applies only to JSON msg format.

  

Examples:

DATA_FILE_QUERY(MSG_FORMAT=JSON,FILE=/path/of/the/data_file,FORMAT_FILE=path/of/the/format_file,CONFIG_DIR
=path/of/the/config_dir)


DATA_FILE_QUERY(TIMESTAMP_COLUMN="_TIMESTAMP",
FILE_CONTENTS=eval("$otq_dir/get_file_content.otq","filename=$data_dir/arrow_file.arrow"))


	"""
	class Parameters:
		msg_format = "MSG_FORMAT"
		file = "FILE"
		symbology = "SYMBOLOGY"
		symbol_name_field = "SYMBOL_NAME_FIELD"
		timestamp_column = "TIMESTAMP_COLUMN"
		time_assignment = "TIME_ASSIGNMENT"
		file_contents = "FILE_CONTENTS"
		format_file = "FORMAT_FILE"
		config_dir = "CONFIG_DIR"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["msg_format", "file", "symbology", "symbol_name_field", "timestamp_column", "time_assignment", "file_contents", "format_file", "config_dir"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["msg_format", "_default_msg_format", "file", "_default_file", "symbology", "_default_symbology", "symbol_name_field", "_default_symbol_name_field", "timestamp_column", "_default_timestamp_column", "time_assignment", "_default_time_assignment", "file_contents", "_default_file_contents", "format_file", "_default_format_file", "config_dir", "_default_config_dir", "stack_info", "_used_strings"]

	class MsgFormat:
		ARROW = "ARROW"
		JSON = "JSON"

	class TimeAssignment:
		_END_TIME = "_END_TIME"
		_START_TIME = "_START_TIME"

	def __init__(self, msg_format=MsgFormat.ARROW, file="", symbology="", symbol_name_field="SYMBOL_NAME", timestamp_column="", time_assignment=TimeAssignment._END_TIME, file_contents="", format_file="", config_dir=""):
		_graph_components.EpBase.__init__(self, "DATA_FILE_QUERY")
		self._default_msg_format = type(self).MsgFormat.ARROW
		self.msg_format = msg_format
		self._default_file = ""
		self.file = file
		self._default_symbology = ""
		self.symbology = symbology
		self._default_symbol_name_field = "SYMBOL_NAME"
		self.symbol_name_field = symbol_name_field
		self._default_timestamp_column = ""
		self.timestamp_column = timestamp_column
		self._default_time_assignment = type(self).TimeAssignment._END_TIME
		self.time_assignment = time_assignment
		self._default_file_contents = ""
		self.file_contents = file_contents
		self._default_format_file = ""
		self.format_file = format_file
		self._default_config_dir = ""
		self.config_dir = config_dir
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_msg_format(self, value):
		self.msg_format = value
		return self

	def set_file(self, value):
		self.file = value
		return self

	def set_symbology(self, value):
		self.symbology = value
		return self

	def set_symbol_name_field(self, value):
		self.symbol_name_field = value
		return self

	def set_timestamp_column(self, value):
		self.timestamp_column = value
		return self

	def set_time_assignment(self, value):
		self.time_assignment = value
		return self

	def set_file_contents(self, value):
		self.file_contents = value
		return self

	def set_format_file(self, value):
		self.format_file = value
		return self

	def set_config_dir(self, value):
		self.config_dir = value
		return self

	@staticmethod
	def _get_name():
		return "DATA_FILE_QUERY"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.msg_format != self.MsgFormat.ARROW: 
			desc += "MSG_FORMAT=" + py_to_str(self.msg_format) + ","
		if self.file != "": 
			desc += "FILE=" + py_to_str(self.file) + ","
		if self.symbology != "": 
			desc += "SYMBOLOGY=" + py_to_str(self.symbology) + ","
		if self.symbol_name_field != "SYMBOL_NAME": 
			desc += "SYMBOL_NAME_FIELD=" + py_to_str(self.symbol_name_field) + ","
		if self.timestamp_column != "": 
			desc += "TIMESTAMP_COLUMN=" + py_to_str(self.timestamp_column) + ","
		if self.time_assignment != self.TimeAssignment._END_TIME: 
			desc += "TIME_ASSIGNMENT=" + py_to_str(self.time_assignment) + ","
		if self.file_contents != "": 
			desc += "FILE_CONTENTS=" + py_to_str(self.file_contents) + ","
		if self.format_file != "": 
			desc += "FORMAT_FILE=" + py_to_str(self.format_file) + ","
		if self.config_dir != "": 
			desc += "CONFIG_DIR=" + py_to_str(self.config_dir) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class ReadFromParquet(_graph_components.EpBase):
	"""
		

READ_FROM_PARQUET

Type: Other

Description: Converts each row of the given Parquet file into
a tick and propagates only those that are satisfying the WHERE
criterion. Column names of the Parquet file are used to name the tick
fields.

In case the given Parquet file has a row group statistics, it may be
used to skip reading entire row groups, when the row group statistics
makes it clear that no row in the group satisfies the WHERE
clause. This EP can read only flat Parquet files; nested structures are
not supported.
Fields with reserved names may be
dropped or renamed..
For type conversions, refer to parquet_archives.html

Note: The EP is supported only on 64-bit Windows/Linux
platforms.

Input: None

Output: A time series of ticks

Parameters:


  PARQUET_FILE_PATH (string,
mandatory)
    Specifies the path of the Parquet file to read.

    Following file paths are supported:
    
    	local file path
    	URLs (like http://parquetfiles.com/data.parquet). Please note that during execution the whole content
of parquet file will be read into memory. 
    	HDFS locations (like hdfs://&lt;namenode&gt;:&lt;port&gt;/&lt;hdfs path&gt;)
    	OneTick supports object cloud storage paths like Amazon S3, Google Cloud Storage, Azure Blob Storage.  For more information about feature support and limitations please see Parquet Archive on cloud object storage document.
    
	

    Directories can also be specified, in
which case that directory will be interpreted as a partitioned parquet
file. All files and folders within the specified directory,
 except those starting with '.' will be considered part of the parquet dataset.

  
  WHERE (expression)
    Specifies a criterion for selecting the ticks to propagate. It
is an expression built with Boolean operators AND, OR,
and NOT, which accepts arithmetic
operators: +, -, *, / and comparison operators =, !=, &lt;, &lt;=,
    &gt;, &gt;=.
Field names are represented without quotes, are case-sensitive, and
have to be present in the tick. String literals must be wrapped by
single or double quotes.

    The WHERE expression accepts OneTick
built-in functions. You can also use OneTick
event processors that act as a filter. Fields of preceding ticks
(those corresponding to the Parquet file rows that were already
considered and maybe converted/propagated) can also be involved in
computations field names are followed by a negative integer index in
square brackets, specifying how far to look back (for example, PRICE[-1] or SIZE[-3]).
The expression can involve state variables.
You can also use the TIMESTAMP, _START_TIME, and _END_TIME (described in detail in the Pseudo-fields document).

  
  TIME_ASSIGNMENT (string)
    Timestamps of the ticks created by READ_FROM_PARQUET are
set to the start/end of the query or to the given tick field depending
on the TIME_ASSIGNMENT parameter. Possible values are _START_TIME, _END_TIME
or a field name.
When a field name is selected, reading will be filtered by query start
and end times. If the selected field does not possess timestamp metadata, it will be treated as a nanosecond timestamp
If set to a field name, that field's values must be in non-descending order.
Default value: _END_TIME.

  
  DISCARD_FIELDS (string)
    A comma-separated list of fields to be discarded from the output
ticks.

  
  FIELDS (string)
    A comma-separated list of fields to be picked from the output
ticks. The opposite to DISCARD_FIELDS.

  
  SYMBOL_NAME_FIELD (string)
    Field that is expected to contain the symbol name. When this
parameter is set and one or more symbols containing time series (i.e.
[dbname]::[time series name] and not just [dbname]::) are bound to the
query or to this EP, only rows belonging to those symbols will be
propagated.
When more than one symbols are present, rows will be sent to the appropriate subquery
 depending on the value of the field specified by this parameter.
Default value: SYMBOL_NAME OR _SYMBOL_NAME

  
  SYMBOLOGY (string)
    SYMBOLOGY of symbol names (values of SYMBOL_NAME_FIELD) of the
parquet file. If specified, in order to find correct time series in the
file for query symbol the EP will first find synonym[s] of this symbols
in the specified symbology using reference database. Then it will find
the history of this new symbol for the query interval and finally will
query the parquet file using the symbol names from history.

  

Examples:

Convert the rows of the given Parquet file into ticks and propagate
all of them:

READ_FROM_PARQUET(PARQUET_FILE_PATH="/path/of/the/parquet/file")

Convert the rows of the given Parquet file into ticks and propagate
only those with _TIMESTAMP greater than 20060515 16:50:50.374
(EST5EDT). Use the field _TIMESTAMP to set the timestamps of the ticks
and discard the field BID_PRICE:

READ_FROM_PARQUET(PARQUET_FILE_PATH="/path/of/the/parquet/file",
TIME_ASSIGNMENT="_TIMESTAMP", WHERE="_TIMESTAMP &gt; PARSE_TIME('%Y%m%d
%H:%M:%S.%q', '20060515 16:50:50.374', 'EST5EDT')",
DISCARD_FIELDS="BID_PRICE")


	"""
	class Parameters:
		parquet_file_path = "PARQUET_FILE_PATH"
		time_assignment = "TIME_ASSIGNMENT"
		where = "WHERE"
		discard_fields = "DISCARD_FIELDS"
		fields = "FIELDS"
		symbol_name_field = "SYMBOL_NAME_FIELD"
		symbology = "SYMBOLOGY"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["parquet_file_path", "time_assignment", "where", "discard_fields", "fields", "symbol_name_field", "symbology"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["parquet_file_path", "_default_parquet_file_path", "time_assignment", "_default_time_assignment", "where", "_default_where", "discard_fields", "_default_discard_fields", "fields", "_default_fields", "symbol_name_field", "_default_symbol_name_field", "symbology", "_default_symbology", "stack_info", "_used_strings"]

	class TimeAssignment:
		_END_TIME = "_END_TIME"
		_START_TIME = "_START_TIME"

	def __init__(self, parquet_file_path="", time_assignment=TimeAssignment._END_TIME, where="", discard_fields="", fields="", symbol_name_field="SYMBOL_NAME OR _SYMBOL_NAME", symbology=""):
		_graph_components.EpBase.__init__(self, "READ_FROM_PARQUET")
		self._default_parquet_file_path = ""
		self.parquet_file_path = parquet_file_path
		self._default_time_assignment = type(self).TimeAssignment._END_TIME
		self.time_assignment = time_assignment
		self._default_where = ""
		self.where = where
		self._default_discard_fields = ""
		self.discard_fields = discard_fields
		self._default_fields = ""
		self.fields = fields
		self._default_symbol_name_field = "SYMBOL_NAME OR _SYMBOL_NAME"
		self.symbol_name_field = symbol_name_field
		self._default_symbology = ""
		self.symbology = symbology
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_parquet_file_path(self, value):
		self.parquet_file_path = value
		return self

	def set_time_assignment(self, value):
		self.time_assignment = value
		return self

	def set_where(self, value):
		self.where = value
		return self

	def set_discard_fields(self, value):
		self.discard_fields = value
		return self

	def set_fields(self, value):
		self.fields = value
		return self

	def set_symbol_name_field(self, value):
		self.symbol_name_field = value
		return self

	def set_symbology(self, value):
		self.symbology = value
		return self

	@staticmethod
	def _get_name():
		return "READ_FROM_PARQUET"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.parquet_file_path != "": 
			desc += "PARQUET_FILE_PATH=" + py_to_str(self.parquet_file_path) + ","
		if self.time_assignment != self.TimeAssignment._END_TIME: 
			desc += "TIME_ASSIGNMENT=" + py_to_str(self.time_assignment) + ","
		if self.where != "": 
			desc += "WHERE=" + py_to_str(self.where) + ","
		if self.discard_fields != "": 
			desc += "DISCARD_FIELDS=" + py_to_str(self.discard_fields) + ","
		if self.fields != "": 
			desc += "FIELDS=" + py_to_str(self.fields) + ","
		if self.symbol_name_field != "SYMBOL_NAME OR _SYMBOL_NAME": 
			desc += "SYMBOL_NAME_FIELD=" + py_to_str(self.symbol_name_field) + ","
		if self.symbology != "": 
			desc += "SYMBOLOGY=" + py_to_str(self.symbology) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class WriteToParquet(_graph_components.EpBase):
	"""
		

WRITE_TO_PARQUET

Type: OutputAdapter

Description: Writes the input tick series to parquet data file. This event processor (EP) also propagates input ticks if the PROPAGATE_TICKS parameter is set to TRUE.
Input must not have field 'time' as that field will also be added by the EP in the resulting file(s)
Setting PARTITIONING_KEYS parmeter will switch this EP to partitioned mode.
In non-partitioned mode, if the path points to a file that already exists, it will be overriden.
When partitioning is active : If any of the partitions are already present in the target directory, their contents will be overwritten; Key fields and their string values will be automatically uri-encoded to avoid conflicts with filesystem naming rules.Pseudofields '_SYMBOL_NAME' and '_TICK_TYPE' may be used as PARTITIONING_KEYS and will be added to the schema automatically.
Since WRITE_TO_PARQUET cannot guarantee that the order in which ticks arrive will be the same as the order in which data will be submitted to the files, in queries with more than one symbol, this EP must either partition by '_SYMBOL_NAME' or have its input merged.
For type conversions, refer to parquet_archives.html

Input: A time series of ticks.

Output: A time series of ticks or none.

Parameters:


OUTPUT_PATH (String)
    Partitioned : Path to the root directory of the parquet files.
	Non-partitioned : Path to the parquet file.
    Following paths are supported:
        
            local path
            OneTick supports object cloud storage paths like Amazon S3, Google Cloud Storage, Azure Blob Storage.  For more information about feature support and limitations please see Parquet Archive on cloud object storage document.
        
        

    Cannot be empty


COMPRESSION_TYPE (String)
    Compression algorithm used when writing the parquet files.
    Supported values: NONE, SNAPPY, GZIP, LZ4, ZSTD
    Default: SNAPPY


NUM_TICKS_PER_ROW_GROUP (Integer)
Number of rows per row group. Refer to PARQUET_NUM_TICKS_PER_ROW_GROUP locator variable.
	Default: 1000


MEMORY_THRESHOLD (Float)
    How much, in Megabytes, WRITE_TO_PARQUET will store per thread before passing ticks to be written.
    Default: 50


PARTITIONING_KEYS (String)
    Comma-separated list of fields to be used as keys for partitioning.


PROPAGATE_INPUT_TICKS (Boolean)
    Switches propagation of the ticks. If set to true, ticks will be propagated.
    Default: false


FIELDS_FOR_DISABLED_DICTIONARY_ENCODING (String)
    Comma-separated list of fields for which dictionary encoding should be disabled.
    By default, dictionary encoding is enabled for all fields.

    Note: This parameter can only be used if PARTITIONING_KEYS is specified.





	"""
	class Parameters:
		output_path = "OUTPUT_PATH"
		compression_type = "COMPRESSION_TYPE"
		num_ticks_per_row_group = "NUM_TICKS_PER_ROW_GROUP"
		memory_threshold = "MEMORY_THRESHOLD"
		partitioning_keys = "PARTITIONING_KEYS"
		propagate_input_ticks = "PROPAGATE_INPUT_TICKS"
		fields_for_disabled_dictionary_encoding = "FIELDS_FOR_DISABLED_DICTIONARY_ENCODING"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["output_path", "compression_type", "num_ticks_per_row_group", "memory_threshold", "partitioning_keys", "propagate_input_ticks", "fields_for_disabled_dictionary_encoding"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["output_path", "_default_output_path", "compression_type", "_default_compression_type", "num_ticks_per_row_group", "_default_num_ticks_per_row_group", "memory_threshold", "_default_memory_threshold", "partitioning_keys", "_default_partitioning_keys", "propagate_input_ticks", "_default_propagate_input_ticks", "fields_for_disabled_dictionary_encoding", "_default_fields_for_disabled_dictionary_encoding", "stack_info", "_used_strings"]

	class CompressionType:
		GZIP = "GZIP"
		LZ4 = "LZ4"
		NONE = "NONE"
		SNAPPY = "SNAPPY"
		ZSTD = "ZSTD"

	def __init__(self, output_path="", compression_type=CompressionType.NONE, num_ticks_per_row_group=1000, memory_threshold=50, partitioning_keys="", propagate_input_ticks=False, fields_for_disabled_dictionary_encoding=""):
		_graph_components.EpBase.__init__(self, "WRITE_TO_PARQUET")
		self._default_output_path = ""
		self.output_path = output_path
		self._default_compression_type = type(self).CompressionType.NONE
		self.compression_type = compression_type
		self._default_num_ticks_per_row_group = 1000
		self.num_ticks_per_row_group = num_ticks_per_row_group
		self._default_memory_threshold = 50
		self.memory_threshold = memory_threshold
		self._default_partitioning_keys = ""
		self.partitioning_keys = partitioning_keys
		self._default_propagate_input_ticks = False
		self.propagate_input_ticks = propagate_input_ticks
		self._default_fields_for_disabled_dictionary_encoding = ""
		self.fields_for_disabled_dictionary_encoding = fields_for_disabled_dictionary_encoding
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_output_path(self, value):
		self.output_path = value
		return self

	def set_compression_type(self, value):
		self.compression_type = value
		return self

	def set_num_ticks_per_row_group(self, value):
		self.num_ticks_per_row_group = value
		return self

	def set_memory_threshold(self, value):
		self.memory_threshold = value
		return self

	def set_partitioning_keys(self, value):
		self.partitioning_keys = value
		return self

	def set_propagate_input_ticks(self, value):
		self.propagate_input_ticks = value
		return self

	def set_fields_for_disabled_dictionary_encoding(self, value):
		self.fields_for_disabled_dictionary_encoding = value
		return self

	@staticmethod
	def _get_name():
		return "WRITE_TO_PARQUET"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.output_path != "": 
			desc += "OUTPUT_PATH=" + py_to_str(self.output_path) + ","
		if self.compression_type != self.CompressionType.NONE: 
			desc += "COMPRESSION_TYPE=" + py_to_str(self.compression_type) + ","
		if self.num_ticks_per_row_group != 1000: 
			desc += "NUM_TICKS_PER_ROW_GROUP=" + py_to_str(self.num_ticks_per_row_group) + ","
		if self.memory_threshold != 50: 
			desc += "MEMORY_THRESHOLD=" + py_to_str(self.memory_threshold) + ","
		if self.partitioning_keys != "": 
			desc += "PARTITIONING_KEYS=" + py_to_str(self.partitioning_keys) + ","
		if self.propagate_input_ticks != False: 
			desc += "PROPAGATE_INPUT_TICKS=" + py_to_str(self.propagate_input_ticks) + ","
		if self.fields_for_disabled_dictionary_encoding != "": 
			desc += "FIELDS_FOR_DISABLED_DICTIONARY_ENCODING=" + py_to_str(self.fields_for_disabled_dictionary_encoding) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class CommandExecute(_graph_components.EpBase):
	"""
		

COMMAND_EXECUTE

Type: Other

Description: Executes a command and processes its standard
output and standard error streams.

Converts each delimiter-separated line of the standard output into a
tick. If the first line of the standard output begins with #, the fields of this line are considered
field names for the generated ticks. Otherwise, field names are named COLUMN1, COLUMN2,
etc. A field name can be preceded by long, double, or string[&lt;size&gt;],
stating that the field has a long integer, a double precision floating
number, or a string of certain length, respectively. Each row of the
standard output must contain the same number of columns.

Converts each row of the standard error stream of the executed
command into a warning with omd::SymbolError::EXECUTED_COMMAND_STDERR
code. If executed command finishes with 0 exit status an info message
with omd::SymbolError::EXECUTED_COMMAND_DONE
code is returned; otherwise, a warning message with omd::SymbolError::EXECUTED_COMMAND_ERROR
code is returned. All info/warning messages are sent through omd::EventProcessor::OutputCallback::process_error
callback.

Python
class name:
CommandExecute

Input: None

Output: A time series of ticks

Parameters:


  COMMAND (string)
    The name of the executable.

  
  COMMAND_ARGS (string)
    A space-separated list of arguments that may have the executable
specified in COMMAND.

  
  ENV (string)
    A comma-separated list of
&lt;name&gt; = &lt;value&gt; pairs. Sets the process
environment that will be created for the command. If this parameter is
not specified, the environment of the parent process - in other
words, the process that runs the query (tick_server, am_server) -
will be used instead. If it is specified, new variables will be added
to the existing ones. If conflicting with existing ones, environment
variable values specified here have precedence.

  
  TIMEOUT (double)
    A nonnegative real number (time in seconds) that shows how long
the EP will wait for data from standard output and standard error
streams of the spawned process. If there is no data from both these
streams during the specified time, the spawned process will be
terminated (by SIGTERM signal in
case of UNIX-like systems) and a warning with omd::SymbolError::EXECUTED_COMMAND_TIMEOUT
code will be returned, alerting that there was no data during the last &lt;timeout&gt; seconds and the spawned
process was terminated. To disable this restriction (i.e., wait for
data until the process is running), 0
should be specified.
Default: 3.0

  
  TIME_ASSIGNMENT (enum)
    The possible values _START_TIME
and _END_TIME cause produced ticks
to have their timestamp set to the start time and end time of the
query, respectively. For a CEP, only _START_TIME
is allowed, and in that case each tick will have the heartbeat
timestamp when it is received.
Default: _END_TIME

  
  FIELD_DELIMITERS (string)
    A list of delimiters in quotes or in apostrophe-quotes (for
instance "=, :" equal sign, comma,
space, colon or '.@;' dot, at and
semicolon). They are used to tokenize each line of arriving data
(STDOUT of child process). For the tab character '\\t' should be specified. Quote and
apostrophe-quote cannot be used as a field delimiter.
Default: &lt;empty&gt;: entire line is considered as a single field

  
  WORKING_DIRECTORY (string)
    This is not a parameter of the EP, but a symbol parameter. It
specifies the directory path, where the command will be executed. It
can be given by an absolute value or relative to the current working
directory of the parent process.

  
  LINE_DELIMITERS (string)
    A list of delimiters surrounded by double or single quotes (for
instance "=, :" means equal
sign, comma, space, and colon ; '.@;'
means dot, at, and semicolon ). These characters, as
well as '\\n', are used to tokenize
arriving data (STDOUT of child process) into lines. For the tab
character, '\\t' should be specified.
Single and double quotes cannot be used as line delimiters.
Default: &lt;empty&gt;, but '\\n' is
treated as a line delimiter and cannot be canceled.

  
  PROCESS_LINE_WITH_COLUMN_NAMES
(Boolean)
    If set to true, the first line of the arriving data, if
possible, will be treated as a description of data in next lines. The
format of column lines description is the same as in CSV_FILE_LISTING and CSV_FILE_QUERY EPs.
Default: true

  
  KEEP_LINE_DELIMITERS (Boolean)
    If set to true, the line
delimiter of each line will be appended to the last field of that line.
Default: false

  
  FLUSH_AFTER_EACH_TICK (Boolean)
    If this flag is set in the historical query mode, then after
propagation of each tick or error message the EP flushes the
client-server connection socket; otherwise, the ticks and error
messages are buffered on server-side and are sent to the client when
the buffer is full or the query is ended. This flag has no effect in
CEP mode, in which the server-side buffering is disabled.
Default: false

  

Access control: The event processor cannot be used by
default. To enable it, access control should be used. If the
COMMAND_EXECUTE is present in the list of event processors in the
access control file, then the specified roles can use the EP.
Additionally, you can specify allowed directories for executable files
for each role using the COMMAND_PATH
parameter. If a directory is allowed, all of its subdirectories are
allowed, too. Omitting this parameter means that all executables from
all directories are allowed. A user with several roles has access to
executables of a directory, if the directory is allowed for at least
one of her roles.

Directories are specified using any separator symbol (colon is the
default). Here is an example of how to specify the role level
permissions:

&lt;allow role="developers"
command_path="/home/developer/bin;/usr/bin" path_separator=";"/&gt;

Examples:

See the COMMAND_EXECUTE examples in command_execute_examples.otq.


	"""
	class Parameters:
		command = "COMMAND"
		command_args = "COMMAND_ARGS"
		env = "ENV"
		timeout = "TIMEOUT"
		time_assignment = "TIME_ASSIGNMENT"
		field_delimiters = "FIELD_DELIMITERS"
		line_delimiters = "LINE_DELIMITERS"
		process_line_with_column_names = "PROCESS_LINE_WITH_COLUMN_NAMES"
		keep_line_delimiters = "KEEP_LINE_DELIMITERS"
		flush_after_each_tick = "FLUSH_AFTER_EACH_TICK"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["command", "command_args", "env", "timeout", "time_assignment", "field_delimiters", "line_delimiters", "process_line_with_column_names", "keep_line_delimiters", "flush_after_each_tick"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["command", "_default_command", "command_args", "_default_command_args", "env", "_default_env", "timeout", "_default_timeout", "time_assignment", "_default_time_assignment", "field_delimiters", "_default_field_delimiters", "line_delimiters", "_default_line_delimiters", "process_line_with_column_names", "_default_process_line_with_column_names", "keep_line_delimiters", "_default_keep_line_delimiters", "flush_after_each_tick", "_default_flush_after_each_tick", "stack_info", "_used_strings"]

	class TimeAssignment:
		_END_TIME = "_END_TIME"
		_START_TIME = "_START_TIME"

	def __init__(self, command="", command_args="", env="", timeout="3.0", time_assignment=TimeAssignment._END_TIME, field_delimiters="", line_delimiters="\"\"", process_line_with_column_names=True, keep_line_delimiters=False, flush_after_each_tick=False):
		_graph_components.EpBase.__init__(self, "COMMAND_EXECUTE")
		self._default_command = ""
		self.command = command
		self._default_command_args = ""
		self.command_args = command_args
		self._default_env = ""
		self.env = env
		self._default_timeout = "3.0"
		self.timeout = timeout
		self._default_time_assignment = type(self).TimeAssignment._END_TIME
		self.time_assignment = time_assignment
		self._default_field_delimiters = ""
		self.field_delimiters = field_delimiters
		self._default_line_delimiters = "\"\""
		self.line_delimiters = line_delimiters
		self._default_process_line_with_column_names = True
		self.process_line_with_column_names = process_line_with_column_names
		self._default_keep_line_delimiters = False
		self.keep_line_delimiters = keep_line_delimiters
		self._default_flush_after_each_tick = False
		self.flush_after_each_tick = flush_after_each_tick
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_command(self, value):
		self.command = value
		return self

	def set_command_args(self, value):
		self.command_args = value
		return self

	def set_env(self, value):
		self.env = value
		return self

	def set_timeout(self, value):
		self.timeout = value
		return self

	def set_time_assignment(self, value):
		self.time_assignment = value
		return self

	def set_field_delimiters(self, value):
		self.field_delimiters = value
		return self

	def set_line_delimiters(self, value):
		self.line_delimiters = value
		return self

	def set_process_line_with_column_names(self, value):
		self.process_line_with_column_names = value
		return self

	def set_keep_line_delimiters(self, value):
		self.keep_line_delimiters = value
		return self

	def set_flush_after_each_tick(self, value):
		self.flush_after_each_tick = value
		return self

	@staticmethod
	def _get_name():
		return "COMMAND_EXECUTE"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.command != "": 
			desc += "COMMAND=" + py_to_str(self.command) + ","
		if self.command_args != "": 
			desc += "COMMAND_ARGS=" + py_to_str(self.command_args) + ","
		if self.env != "": 
			desc += "ENV=" + py_to_str(self.env) + ","
		if self.timeout != "3.0": 
			desc += "TIMEOUT=" + py_to_str(self.timeout) + ","
		if self.time_assignment != self.TimeAssignment._END_TIME: 
			desc += "TIME_ASSIGNMENT=" + py_to_str(self.time_assignment) + ","
		if self.field_delimiters != "": 
			desc += "FIELD_DELIMITERS=" + py_to_str(self.field_delimiters) + ","
		if self.line_delimiters != "\"\"": 
			desc += "LINE_DELIMITERS=" + py_to_str(self.line_delimiters) + ","
		if self.process_line_with_column_names != True: 
			desc += "PROCESS_LINE_WITH_COLUMN_NAMES=" + py_to_str(self.process_line_with_column_names) + ","
		if self.keep_line_delimiters != False: 
			desc += "KEEP_LINE_DELIMITERS=" + py_to_str(self.keep_line_delimiters) + ","
		if self.flush_after_each_tick != False: 
			desc += "FLUSH_AFTER_EACH_TICK=" + py_to_str(self.flush_after_each_tick) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class DirectoryListing(_graph_components.EpBase):
	"""
		

DIRECTORY_LISTING

Type: Other

Description: Shows the hierarchy of the server's file system.
Timestamps of the ticks created by DIRECTORY_LISTING are set to the
start time of the query.

When this function is invoked on the remote server, symbol names for
the graph must name the directories (absolute path) on the remote file
system. In order to see the file system hierarchy of the client's local
box, the database of the input symbols should be specified as LOCAL.

Python
class name:&nbsp;DirectoryListing

Input: None

Output: A time series of ticks.

Parameters:


  RECURSIVE_LISTING (Boolean)
    If set the EP returns all available paths in the OS file system
with a prefix specified in the symbol names path.
Default value: false

  
  GET_ABSOLUTE_PATH (Boolean)
    If set, the output of the EP carries the absolute paths of
files. Otherwise, it carries file paths starting with those specified
in the symbol names path.
Default value: false

  

Access control: By default, all users are allowed to use this
event processor for listing any directories. Limitations can be applied
using access control. If DIRECTORY_LISTING is present in the access
control event processor list, then only the specified roles can use
this event processor. Additionally, for each role allowed directories
can be specified. If some directory is allowed, all of its
subdirectories are allowed. Omitting this parameter means that all
directories are allowed. If a user has several roles, she has access to
a directory, if allowed, for at least one of her roles. One can also
specify allowed directories list in the event processor level (not only
role level). This means that directories listed in the event processor
level are allowed for all users (whose roles aren't mentioned under the
section for this event processor, or user does not have any role in the
access control file). Role level permissions override event processor
level permissions for the users whose roles are mentioned in access
control for this ep.

Directories are specified using any separator symbol (colon is the
default). Here is an example how to specify role level permissions:

&lt;allow role="developers"
directories="/home/developer;/work" path_separator=";"/&gt;

Here is an example how to specify item level permissions:

&lt;ep id="DIRECTORY_LISTING"
directories="/home/developer;/work" path_separator=";"&gt;

Examples:

Shows the content of the directory, the path to which is specified
in symbols:

DIRECTORY_LISTING ()

See the DIRECTORY_LISTING in OTHER_EXAMPLES.otq.


	"""
	class Parameters:
		recursive_listing = "RECURSIVE_LISTING"
		get_absolute_path = "GET_ABSOLUTE_PATH"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["recursive_listing", "get_absolute_path"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["recursive_listing", "_default_recursive_listing", "get_absolute_path", "_default_get_absolute_path", "stack_info", "_used_strings"]

	def __init__(self, recursive_listing=False, get_absolute_path=False):
		_graph_components.EpBase.__init__(self, "DIRECTORY_LISTING")
		self._default_recursive_listing = False
		self.recursive_listing = recursive_listing
		self._default_get_absolute_path = False
		self.get_absolute_path = get_absolute_path
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_recursive_listing(self, value):
		self.recursive_listing = value
		return self

	def set_get_absolute_path(self, value):
		self.get_absolute_path = value
		return self

	@staticmethod
	def _get_name():
		return "DIRECTORY_LISTING"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.recursive_listing != False: 
			desc += "RECURSIVE_LISTING=" + py_to_str(self.recursive_listing) + ","
		if self.get_absolute_path != False: 
			desc += "GET_ABSOLUTE_PATH=" + py_to_str(self.get_absolute_path) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class OtqQuery(_graph_components.EpBase):
	"""
		

OTQ_QUERY
obsolete


Type: Other

Description: Returns a time series of ticks resulted after
the execution of an OTQ query. While the inner query, when compared to
the hosting query, can generally have a different start/end times,
symbol date, and "apply times daily" flag value; its symbols are still
dependent on those of the outer query in the following way:
The inner query can have at most 1 (non-bound) symbol, which, if it
exists, can be overridden either by that of the hosting query, or by
the symbols, bound to some destination node of that OTQ_QUERY node (1
for each replica OTQ_QUERY node). If the OTQ_QUERY node itself has
bound symbols, they will be used to override the bound symbol lists of
the inner query, assuming all these lists are equal.

Note, that being a replacement for the CEP simulation feature of the
OTQ_QUERY EP, CEP_SIMULATOR EP
replaced by itself (in a backward-incompatible manner) that logic in
OTQ_QUERY EP. However, the old (and deprecated, due to scalability
issues) behavior can be restored by specifying OTQ_QUERY_CEP_SIMULATION_OLD_MODE=true in
the OneTick main configuration file.


While designing the query, please take into consideration that if
the inner query has a wider range, this EP extends the range of the
outer query.

This EP is OBSOLETE! Although this EP was obviated by the advent of
CEP_SIMULATOR and MODIFY_QUERY_TIMES EPs, you might still consider
using it to handle cases where the inner query has apply_times_daily=true and the outer query
has apply_times_daily=false.


Python
class name:&nbsp;OtqQuery

Input: None.

Output: A time series of ticks.

Parameters:


  OTQ_FILE_PATH (string)
    The path to an .otq file, possibly followed by the query name (&lt;otq_file_path&gt;::&lt;query_name&gt;),
the latter being needed if the specified .otq file contains multiple
queries. The parameter OTQ_FILE_PATH in the OneTick configuration file
specifies the set of directories where OTQ_QUERY looks for the file if
the value of this parameter is a relative path. The .otq file is
searched first on the client host using OTQ_FILE_PATH from the client
configuration, and if it's not found it searches the server host using
OTQ_FILE_PATH from the server configuration (a server can potentially
act as a client for another server, so multiple hops are possible). The
last host in such a chain (on which OTQ_QUERY is processed locally)
will break execution if the target .otq file is not found.

  
  START_DATE (date as YYYYMMDD)
    Overrides the query start date, if not empty. If empty, the
query start date defaults to that of the hosting query. Note, that in
GUI, start date must be specified in accordance with the date input
format, chosen in the settings (a hint is displayed next to this
parameter).
Default: EMPTY

  
  START_TIME (time as hhmmss.mmm)
    Overrides the query start time, if not empty. If empty, the
query start time defaults to that of the hosting query. It must be
specified only in conjunction with the START_DATE parameter. Note, that
in GUI, start time must be specified in accordance with the time input
format (a hint is displayed next to this parameter).
Default: EMPTY

  
  END_DATE (date as YYYYMMDD)
    Overrides the query end date, if not empty. If empty, the query
end date defaults to that of the hosting query. Note, that in GUI, end
date must be specified in accordance with the date input format, chosen
in the settings (a hint is displayed next to this parameter).
Default: EMPTY

  
  END_TIME (time as hhmmss.mmm)
    Overrides the query end time, if not empty. If empty, the query
end time defaults to that of the hosting query. It must be specified
only in conjunction with the END_DATE parameter. Note, that in GUI, end
time must be specified in accordance with the time input format (a hint
is displayed next to this parameter).
Default: EMPTY

  
  SYMBOL_DATE (date as YYYYMMDD)
    Overrides query symbol date if not empty. If empty, the query
symbol date defaults to either that of the specified .otq file if the
inner query external symbol is not overridden, or else to that of the
hosting query. Note, that in GUI, symbol date must be specified in
accordance with the date input format, chosen in the settings (a hint
is displayed next to this parameter).
Default: EMPTY

  
  TIMEZONE (string)
    The time zone used to form a query interval from non-default
start/end date times.
Default: Local time zone

  
  APPLY_TIMES_DAILY (Boolean)
    Overrides "apply times daily" query flag, if not empty. If
empty, "apply times daily" defaults to that of the hosting query.
Default: EMPTY

  
  OTQ_PARAMS (string)
    A comma-separated list of &lt;param_name&gt;=&lt;value&gt;
pairs, used to override default values of .otq parameters in a
specified .otq file.
Default: EMPTY

  
  RESULT_CACHE_NAME (string)
    A non-empty value of this parameter serves as an instruction to
cache an inner query execution result (output time series) under
specified name so that subsequent executions of the same inner query
make use of cached data as long as the inner query file remains
unchanged. Results of different inner queries cannot be cached under
the same name.
Default: EMPTY

  
  CEP_SIMULATION (Boolean)
    Causes OTQ_QUERY to simulate CEP processing by continuously
querying a database for new ticks (added by memory loaders).
Default: false

  
  CEP_SIMULATION_INTERVAL
(numeric)
    Specifies checking period in seconds for the CEP simulator.
Default: 0

  
  MAX_INTER_SYMBOL_DELAY
(numeric)
    Used only in case if CEP_SIMULATION
is set to true. Specifies maximum amount of time (in milliseconds) by
which timestamps of new ticks from a particular symbol can be less,
than those of already arrived ticks from another. Helps to generate
heartbeats for symbols, that do not tick, as long as there is some
other symbol, that does.
Default: 0

  
  MIN_DB_DELAY (numeric)
    Specifies minimum amount of time (in milliseconds) by which
ticks in the memory database may be delayed.
Default: 0

  
  GO_BACK_ONCE (Boolean)
    Specifies, whether only the first, if set to true, or every, if
set to false, attempt to query the memory database will have
initialization phase.
Default: false

  
  LOG_DETAILS_PER_ITERATION
(Boolean)
    If set to true, logs the time when each iteration begins,
together with the time up until which ticks are queried (query end
time), the time when each iteration ends, as well as latest tick
timestamp (and hence start time of the next iteration) for each queried
symbol.
Default: false

  

Access control: By default, all users are allowed to use this
event processor for executing queries from any directories. Limitations
can be applied using access control. If OTQ_QUERY is present in the
access control event processor list, then only the specified roles can
use this event processor. Additionally, for each role, allowed
directories can be specified. If some directory is allowed, all of its
subdirectories are allowed. Omitting this parameter means that all
directories are allowed. If a user has several roles, she has access to
a directory if it is allowed for at least one of her roles. Also, extra
search paths for .otq files (in addition to OTQ_FILE_PATH) can be
specified for roles and in event processor level. One can also specify
allowed directories list in the event processor level (not only role
level). This means that directories listed in the event processor level
are allowed for all users (whose roles aren't mentioned under the
section for this event processor, or user does not have any role in the
access control file). Role level permissions override event processor
level permissions for the users whose roles are mentioned in access
control for this ep.

Directories are specified using any separator symbol (comma is the
default). Here is an example how to specify role level permissions:

&lt;allow role="developers"
directories="/home/developer;/work"
extra_search_path="/home/developer/otqs" path_separator=";"/&gt;

Here is an example how to specify item level permissions:

&lt;ep id="OTQ_QUERY"
directories="/home/developer/otqs" path_separator=";"&gt;

Examples:

OTQ_QUERY(".\\AGGREGATION_COMPUTE_EXAMPLES.otq,,0,,0,,,,)

See the OTQ_QUERY example in OTHER_EXAMPLES.otq.


	"""
	class Parameters:
		otq_file_path = "OTQ_FILE_PATH"
		start_date = "START_DATE"
		start_time = "START_TIME"
		end_date = "END_DATE"
		end_time = "END_TIME"
		symbol_date = "SYMBOL_DATE"
		timezone = "TIMEZONE"
		apply_times_daily = "APPLY_TIMES_DAILY"
		otq_params = "OTQ_PARAMS"
		result_cache_name = "RESULT_CACHE_NAME"
		cep_simulation = "CEP_SIMULATION"
		cep_simulation_interval = "CEP_SIMULATION_INTERVAL"
		max_inter_symbol_delay = "MAX_INTER_SYMBOL_DELAY"
		min_db_delay = "MIN_DB_DELAY"
		go_back_once = "GO_BACK_ONCE"
		log_details_per_iteration = "LOG_DETAILS_PER_ITERATION"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["otq_file_path", "start_date", "start_time", "end_date", "end_time", "symbol_date", "timezone", "apply_times_daily", "otq_params", "result_cache_name", "cep_simulation", "cep_simulation_interval", "max_inter_symbol_delay", "min_db_delay", "go_back_once", "log_details_per_iteration"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["otq_file_path", "_default_otq_file_path", "start_date", "_default_start_date", "start_time", "_default_start_time", "end_date", "_default_end_date", "end_time", "_default_end_time", "symbol_date", "_default_symbol_date", "timezone", "_default_timezone", "apply_times_daily", "_default_apply_times_daily", "otq_params", "_default_otq_params", "result_cache_name", "_default_result_cache_name", "cep_simulation", "_default_cep_simulation", "cep_simulation_interval", "_default_cep_simulation_interval", "max_inter_symbol_delay", "_default_max_inter_symbol_delay", "min_db_delay", "_default_min_db_delay", "go_back_once", "_default_go_back_once", "log_details_per_iteration", "_default_log_details_per_iteration", "stack_info", "_used_strings"]

	def __init__(self, otq_file_path="", start_date="", start_time="", end_date="", end_time="", symbol_date="", timezone="", apply_times_daily=False, otq_params="", result_cache_name="", cep_simulation=False, cep_simulation_interval="", max_inter_symbol_delay=0, min_db_delay=0, go_back_once=False, log_details_per_iteration=False):
		_graph_components.EpBase.__init__(self, "OTQ_QUERY")
		self._default_otq_file_path = ""
		self.otq_file_path = otq_file_path
		self._default_start_date = ""
		self.start_date = start_date
		self._default_start_time = ""
		self.start_time = start_time
		self._default_end_date = ""
		self.end_date = end_date
		self._default_end_time = ""
		self.end_time = end_time
		self._default_symbol_date = ""
		self.symbol_date = symbol_date
		self._default_timezone = ""
		self.timezone = timezone
		self._default_apply_times_daily = False
		self.apply_times_daily = apply_times_daily
		self._default_otq_params = ""
		self.otq_params = otq_params
		self._default_result_cache_name = ""
		self.result_cache_name = result_cache_name
		self._default_cep_simulation = False
		self.cep_simulation = cep_simulation
		self._default_cep_simulation_interval = ""
		self.cep_simulation_interval = cep_simulation_interval
		self._default_max_inter_symbol_delay = 0
		self.max_inter_symbol_delay = max_inter_symbol_delay
		self._default_min_db_delay = 0
		self.min_db_delay = min_db_delay
		self._default_go_back_once = False
		self.go_back_once = go_back_once
		self._default_log_details_per_iteration = False
		self.log_details_per_iteration = log_details_per_iteration
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_otq_file_path(self, value):
		self.otq_file_path = value
		return self

	def set_start_date(self, value):
		self.start_date = value
		return self

	def set_start_time(self, value):
		self.start_time = value
		return self

	def set_end_date(self, value):
		self.end_date = value
		return self

	def set_end_time(self, value):
		self.end_time = value
		return self

	def set_symbol_date(self, value):
		self.symbol_date = value
		return self

	def set_timezone(self, value):
		self.timezone = value
		return self

	def set_apply_times_daily(self, value):
		self.apply_times_daily = value
		return self

	def set_otq_params(self, value):
		self.otq_params = value
		return self

	def set_result_cache_name(self, value):
		self.result_cache_name = value
		return self

	def set_cep_simulation(self, value):
		self.cep_simulation = value
		return self

	def set_cep_simulation_interval(self, value):
		self.cep_simulation_interval = value
		return self

	def set_max_inter_symbol_delay(self, value):
		self.max_inter_symbol_delay = value
		return self

	def set_min_db_delay(self, value):
		self.min_db_delay = value
		return self

	def set_go_back_once(self, value):
		self.go_back_once = value
		return self

	def set_log_details_per_iteration(self, value):
		self.log_details_per_iteration = value
		return self

	@staticmethod
	def _get_name():
		return "OTQ_QUERY"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.otq_file_path != "": 
			desc += "OTQ_FILE_PATH=" + py_to_str(self.otq_file_path) + ","
		if self.start_date != "": 
			desc += "START_DATE=" + py_to_str(self.start_date) + ","
		if self.start_time != "": 
			desc += "START_TIME=" + py_to_str(self.start_time) + ","
		if self.end_date != "": 
			desc += "END_DATE=" + py_to_str(self.end_date) + ","
		if self.end_time != "": 
			desc += "END_TIME=" + py_to_str(self.end_time) + ","
		if self.symbol_date != "": 
			desc += "SYMBOL_DATE=" + py_to_str(self.symbol_date) + ","
		if self.timezone != "": 
			desc += "TIMEZONE=" + py_to_str(self.timezone) + ","
		if self.apply_times_daily != False: 
			desc += "APPLY_TIMES_DAILY=" + py_to_str(self.apply_times_daily) + ","
		if self.otq_params != "": 
			desc += "OTQ_PARAMS=" + py_to_str(self.otq_params) + ","
		if self.result_cache_name != "": 
			desc += "RESULT_CACHE_NAME=" + py_to_str(self.result_cache_name) + ","
		if self.cep_simulation != False: 
			desc += "CEP_SIMULATION=" + py_to_str(self.cep_simulation) + ","
		if self.cep_simulation_interval != "": 
			desc += "CEP_SIMULATION_INTERVAL=" + py_to_str(self.cep_simulation_interval) + ","
		if self.max_inter_symbol_delay != 0: 
			desc += "MAX_INTER_SYMBOL_DELAY=" + py_to_str(self.max_inter_symbol_delay) + ","
		if self.min_db_delay != 0: 
			desc += "MIN_DB_DELAY=" + py_to_str(self.min_db_delay) + ","
		if self.go_back_once != False: 
			desc += "GO_BACK_ONCE=" + py_to_str(self.go_back_once) + ","
		if self.log_details_per_iteration != False: 
			desc += "LOG_DETAILS_PER_ITERATION=" + py_to_str(self.log_details_per_iteration) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class CepSimulator(_graph_components.EpBase):
	"""
		

CEP_SIMULATOR

Type: Other

Description: Imitates a CEP source by continuously polling a
memory database using the query specified via the OTQ_FILE_PATH parameter and fetching
newly-arrived portions of data to propagate further. In case if the
query, specified by the OTQ_FILE_PATH parameter, has unbound symbols,
those symbols are replaced by that of the CEP_SIMULATOR node. Bound
symbols and tick types are left intact.

CEP_SIMULATOR EP has been introduced as a standalone replacement for
the CEP simulation feature of OTQ_QUERY
EP. Moreover, OTQ_QUERY EP, used in CEP simulation mode is internally
replaced by CEP_SIMULATOR EP. The old (and deprecated, due to
scalability issues) CEP simulation in OTQ_QUERY EP can be enabled by
specifying OTQ_QUERY_CEP_SIMULATION_OLD_MODE=true
in the OneTick main configuration file.

The CEP_SIMULATOR EP requires the following configuration to be set:


  The server-side locator config file must have at least a minimal &lt;FEED&gt; entry. E.g.:
    &lt;FEED type=rawdb&gt;
&lt;/FEED&gt;

  
  The client-side locator config file must have a &lt;CEP_TICK_SERVERS&gt; entry. E.g.:
    &lt;CEP_TICK_SERVERS&gt;
&nbsp; &lt;location location=hostname:40001/&gt;
&lt;/CEP_TICK_SERVERS&gt;

  

Python
class name:&nbsp;CepSimulator

Input: None.

Output: A time series of ticks.

Parameters:


  OTQ_FILE_PATH (string)
    The path to an .otq file, possibly followed by the query name (&lt;otq_file_path&gt;::&lt;query_name&gt;),
the latter being needed if the specified .otq file contains multiple
queries. The parameter OTQ_FILE_PATH in the OneTick configuration file
specifies the set of directories where OTQ_QUERY looks for the file if
the value of this parameter is a relative path. The respective query

  
  START_DATE (date as YYYYMMDD)
    Overrides the query start date, if not empty. If empty, the
query start date defaults to that of the hosting query. Note, that in
GUI, start date must be specified in accordance with the date input
format, chosen in the settings (a hint is displayed next to this
parameter).
Default: EMPTY

  
  START_TIME (time as hhmmss.mmm)
    Overrides the query start time, if not empty. If empty, the
query start time defaults to that of the hosting query. It must be
specified only in conjunction with the START_DATE parameter. Note, that
in GUI, start time must be specified in accordance with the time input
format (a hint is displayed next to this parameter).
Default: EMPTY

  
  END_DATE (date as YYYYMMDD)
    Overrides the query end date, if not empty. If empty, the query
end date defaults to that of the hosting query. Note, that in GUI, end
date must be specified in accordance with the date input format, chosen
in the settings (a hint is displayed next to this parameter).
Default: EMPTY

  
  END_TIME (time as hhmmss.mmm)
    Overrides the query end time, if not empty. If empty, the query
end time defaults to that of the hosting query. It must be specified
only in conjunction with the END_DATE parameter. Note, that in GUI, end
time must be specified in accordance with the time input format (a hint
is displayed next to this parameter).
Default: EMPTY

  
  SYMBOL_DATE (date as YYYYMMDD)
    Overrides query symbol date if not empty. If empty, the query
symbol date defaults to either that of the specified .otq file if the
inner query external symbol is not overridden, or else to that of the
hosting query. Note, that in GUI, symbol date must be specified in
accordance with the date input format, chosen in the settings (a hint
is displayed next to this parameter).
Default: EMPTY

  
  TIMEZONE (string)
    The time zone used to form a query interval from non-default
start/end date times.
Default: Local time zone

  
  OTQ_PARAMS (string)
    A comma-separated list of &lt;param_name&gt;=&lt;value&gt;
pairs, used to override default values of .otq parameters in a
specified .otq file.
Default: EMPTY

  
  CEP_SIMULATION_INTERVAL
(numeric)
    Specifies checking period in seconds for the CEP simulator.
Default: 0

  
  MAX_INTER_SYMBOL_DELAY
(numeric)
    Specifies maximum amount of time (in milliseconds) by which
timestamps of new ticks from a particular symbol can be less, than
those of already arrived ticks from another. Helps to generate
heartbeats for symbols, that do not tick, as long as there is some
other symbol, that does.
Default: 0

  
  MIN_DB_DELAY (numeric)
    Specifies minimum amount of time (in milliseconds) by which
ticks in the memory database may be delayed.
Default: 0

  
  GO_BACK_ONCE (Boolean)
    Specifies, whether only the first, if set to true, or every, if
set to false, attempt to query the memory database will have
initialization phase.
Default: false

  
  LOG_DETAILS_PER_ITERATION
(Boolean)
    If set to true, logs the time when each iteration begins,
together with the time up until which ticks are queried (query end
time), the time when each iteration ends, as well as latest tick
timestamp (and hence start time of the next iteration) for each queried
symbol.
Default: false

  


	"""
	class Parameters:
		otq_file_path = "OTQ_FILE_PATH"
		start_date = "START_DATE"
		start_time = "START_TIME"
		end_date = "END_DATE"
		end_time = "END_TIME"
		symbol_date = "SYMBOL_DATE"
		timezone = "TIMEZONE"
		otq_params = "OTQ_PARAMS"
		cep_simulation_interval = "CEP_SIMULATION_INTERVAL"
		max_inter_symbol_delay = "MAX_INTER_SYMBOL_DELAY"
		min_db_delay = "MIN_DB_DELAY"
		go_back_once = "GO_BACK_ONCE"
		log_details_per_iteration = "LOG_DETAILS_PER_ITERATION"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["otq_file_path", "start_date", "start_time", "end_date", "end_time", "symbol_date", "timezone", "otq_params", "cep_simulation_interval", "max_inter_symbol_delay", "min_db_delay", "go_back_once", "log_details_per_iteration"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["otq_file_path", "_default_otq_file_path", "start_date", "_default_start_date", "start_time", "_default_start_time", "end_date", "_default_end_date", "end_time", "_default_end_time", "symbol_date", "_default_symbol_date", "timezone", "_default_timezone", "otq_params", "_default_otq_params", "cep_simulation_interval", "_default_cep_simulation_interval", "max_inter_symbol_delay", "_default_max_inter_symbol_delay", "min_db_delay", "_default_min_db_delay", "go_back_once", "_default_go_back_once", "log_details_per_iteration", "_default_log_details_per_iteration", "stack_info", "_used_strings"]

	def __init__(self, otq_file_path="", start_date="", start_time="", end_date="", end_time="", symbol_date="", timezone="", otq_params="", cep_simulation_interval="", max_inter_symbol_delay=0, min_db_delay=0, go_back_once=False, log_details_per_iteration=False):
		_graph_components.EpBase.__init__(self, "CEP_SIMULATOR")
		self._default_otq_file_path = ""
		self.otq_file_path = otq_file_path
		self._default_start_date = ""
		self.start_date = start_date
		self._default_start_time = ""
		self.start_time = start_time
		self._default_end_date = ""
		self.end_date = end_date
		self._default_end_time = ""
		self.end_time = end_time
		self._default_symbol_date = ""
		self.symbol_date = symbol_date
		self._default_timezone = ""
		self.timezone = timezone
		self._default_otq_params = ""
		self.otq_params = otq_params
		self._default_cep_simulation_interval = ""
		self.cep_simulation_interval = cep_simulation_interval
		self._default_max_inter_symbol_delay = 0
		self.max_inter_symbol_delay = max_inter_symbol_delay
		self._default_min_db_delay = 0
		self.min_db_delay = min_db_delay
		self._default_go_back_once = False
		self.go_back_once = go_back_once
		self._default_log_details_per_iteration = False
		self.log_details_per_iteration = log_details_per_iteration
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_otq_file_path(self, value):
		self.otq_file_path = value
		return self

	def set_start_date(self, value):
		self.start_date = value
		return self

	def set_start_time(self, value):
		self.start_time = value
		return self

	def set_end_date(self, value):
		self.end_date = value
		return self

	def set_end_time(self, value):
		self.end_time = value
		return self

	def set_symbol_date(self, value):
		self.symbol_date = value
		return self

	def set_timezone(self, value):
		self.timezone = value
		return self

	def set_otq_params(self, value):
		self.otq_params = value
		return self

	def set_cep_simulation_interval(self, value):
		self.cep_simulation_interval = value
		return self

	def set_max_inter_symbol_delay(self, value):
		self.max_inter_symbol_delay = value
		return self

	def set_min_db_delay(self, value):
		self.min_db_delay = value
		return self

	def set_go_back_once(self, value):
		self.go_back_once = value
		return self

	def set_log_details_per_iteration(self, value):
		self.log_details_per_iteration = value
		return self

	@staticmethod
	def _get_name():
		return "CEP_SIMULATOR"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.otq_file_path != "": 
			desc += "OTQ_FILE_PATH=" + py_to_str(self.otq_file_path) + ","
		if self.start_date != "": 
			desc += "START_DATE=" + py_to_str(self.start_date) + ","
		if self.start_time != "": 
			desc += "START_TIME=" + py_to_str(self.start_time) + ","
		if self.end_date != "": 
			desc += "END_DATE=" + py_to_str(self.end_date) + ","
		if self.end_time != "": 
			desc += "END_TIME=" + py_to_str(self.end_time) + ","
		if self.symbol_date != "": 
			desc += "SYMBOL_DATE=" + py_to_str(self.symbol_date) + ","
		if self.timezone != "": 
			desc += "TIMEZONE=" + py_to_str(self.timezone) + ","
		if self.otq_params != "": 
			desc += "OTQ_PARAMS=" + py_to_str(self.otq_params) + ","
		if self.cep_simulation_interval != "": 
			desc += "CEP_SIMULATION_INTERVAL=" + py_to_str(self.cep_simulation_interval) + ","
		if self.max_inter_symbol_delay != 0: 
			desc += "MAX_INTER_SYMBOL_DELAY=" + py_to_str(self.max_inter_symbol_delay) + ","
		if self.min_db_delay != 0: 
			desc += "MIN_DB_DELAY=" + py_to_str(self.min_db_delay) + ","
		if self.go_back_once != False: 
			desc += "GO_BACK_ONCE=" + py_to_str(self.go_back_once) + ","
		if self.log_details_per_iteration != False: 
			desc += "LOG_DETAILS_PER_ITERATION=" + py_to_str(self.log_details_per_iteration) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class SplitQueryOutputBySymbol(_graph_components.EpBase):
	"""
		

SPLIT_QUERY_OUTPUT_BY_SYMBOL

Type: Other

Description: A data source EP, used to dispatch output ticks,
resulting after execution of the specified query, according to the
values of the specified field in those ticks. Each replica of this EP,
corresponding to a particular bound or unbound symbol, thus, propagates
resulting ticks of the specified query, with values of the specified
field in those ticks equal to that symbol. Note, that db name part of
symbols is not taken into account.

For example, if SPLIT_QUERY_OUTPUT_BY_SYMBOL EP is executed
for symbols TAQ::CSCO, TAQ::IBM and TAQ::MSFT, then resulting ticks of
the underlying query with values of the specified field equal to "CSCO"
are propagated by the replicate of SPLIT_QUERY_OUTPUT_BY_SYMBOL
EP, corresponding to the symbol TAQ::CSCO, those with values of the
specified field equal to "IBM" are propagated by the replicate of SPLIT_QUERY_OUTPUT_BY_SYMBOL
EP, corresponding to the symbol TAQ::IBM, and the same for TAQ::MSFT.

The underlying query is executed for the time interval of the query,
containing the SPLIT_QUERY_OUTPUT_BY_SYMBOL EP. If the query,
containing the SPLIT_QUERY_OUTPUT_BY_SYMBOL EP, is a non-CEP
query, then the underlying query is expected to be non-CEP as well,
while in case of a CEP query the underlying query can both be a CEP
query or a non-CEP query, meaning that the underlying query does not
inherit the "Continuous query" flag of the query, containing the SPLIT_QUERY_OUTPUT_BY_SYMBOL
EP, but uses its own "Continuous query" flag.

Use Cases: You would consider using the
SPLIT_QUERY_OUTPUT_BY_SYMBOL EP:


  
    To join ODBC and OneTick data for each non-bound symbol,
where the SQL query that gives you data for each queried symbol brings
them all together (there is no index on symbol name field in RDBMS).
This case is equivalent to the news stream one, where each piece of
news could be for a different symbol.

    Without the SPLIT_QUERY_OUTPUT_BY_SYMBOL EP, you would have to
read this composite stream for every single non-bound symbol (and then
filter it for a given symbol prior to joining them with market data
stream), which is very inefficient.

    By using the SPLIT_QUERY_OUTPUT_BY_SYMBOL EP, the query that
reads ODBC db or news will be executed once, and the new EP will send
the output of the query to the right non-bound symbol (this is similar
to how CSV_FILE_QUERY works).

  
  
    To merge all symbols to add some aggregate (across all
symbols) stats to each tick, and after that, to process each symbol
individually.

    Without using the SPLIT_QUERY_OUTPUT_BY_SYMBOL EP, you could do
that using GROUP_BY EP.

    SPLIT_QUERY_OUTPUT_BY_SYMBOL EP provides another, a more memory
and performance-efficient, way to achieve this.

    While the GROUP_BY EP is used with queries that bind symbols to
some EP, like MERGE, as opposed to queries
with non-bound symbols, and objective of SPLIT_QUERY_OUPUT_BY_SYMBOL EP
is to support non-bound symbols, despite part of query logic being
shared among them all.

    The graph that involves SPLIT_QUERY_OUTPUT_BY_SYMBOL EP will
make the outer query look natural, as if SPLIT_QUERY_OUTPUT_BY_SYMBOL
EP was just a PASSTHROUGH EP, while in
reality the SPLIT_QUERY_OUTPUT_BY_SYMBOL EP propagates the results of
another query (which has symbol name as one of its output fields, and
these symbol names are used to dispatch the results of that inner query
to the right non-bound symbols).

  

Python
class name:&nbsp;SplitQueryOutputBySymbol

Input: None.

Output: A time series of ticks.

Parameters:


  OTQ_QUERY (string)
    Specifies the underlying query to execute as a path to an .otq
file, possibly followed by the query name (&lt;otq_file_path&gt;::&lt;query_name&gt;),
the latter being needed if the specified .otq file contains multiple
queries. The parameter OTQ_FILE_PATH in the OneTick configuration file
specifies the set of directories where SPLIT_QUERY_OUTPUT_BY_SYMBOL
looks for the file if the value of this parameter is a relative path.

  
  OTQ_QUERY_PARAMS (string)
    A comma-separated list of &lt;param_name&gt;=&lt;value&gt;
pairs, used to override default values of .otq parameters in a
specified .otq file.
Default: EMPTY

  
  SYMBOL_NAME_FIELD (string)
    Specifies the field in the resulting ticks of the underlying
query, according to values of which those ticks are dispatched.
Default: SYMBOL_NAME

  
  ALLOW_MULTIPLE_SYMBOL_NAMES
(Boolean)
    If set to true, allows the values of the
symbol name field to be comma-separated lists of symbol names, in
which case each tick is dispatched to every replica of SPLIT_QUERY_OUTPUT_BY_SYMBOL
EP, corresponding to a symbol name, equal to one of those in the
respective list.
Default: false

  
  ENSURE_SINGLE_INVOCATION
(Boolean)
    By default, the underlying query is executed once per symbol
batch and per execution thread (see Query Performance Tuning
section) of the containing query. If this parameter is set to true, the
underlying query is executed once regardless of the batch size and the
number of CPU cores utilized. The former option should be the preferred
(hence the default) one, as it reduces memory overhead, while the
latter one might be chosen to speed-up the overall execution. Note,
that if the underlying query is a CEP query, than this option has no
effect, as there is a single batch and a single thread anyway.
Default: false

  

Example: SPLIT_QUERY_OUTPUT_BY_SYMBOL EP is used to
dispatch output ticks resulting from the execution of the specified
query. Thus we create the SIZE_PRICE_TICKER
query (you can find it in SPLIT_QUERY_OUTPUT_BY_SYMBOL.otq)
that propagates the SIZE, PRICE, and TICKER
fields for ticks without changing their values.

PASSTHROUGH(FIELDS="SIZE, PRICE, TICKER")

As symbols used 7 different symbols.

SECURITY = DEMO_L1::A 0

SECURITY = DEMO_L1::AA 0

SECURITY = DEMO_L1::AAA 0

SECURITY = DEMO_L1::AABC 0

SECURITY = DEMO_L1::AAC 0

SECURITY = DEMO_L1::AACB 0

SECURITY = DEMO_L1::AACE 0

In this example SPLIT_QUERY_OUTPUT_BY_SYMBOL is used to
dispatch to output only ticks with A and AA symbols.

See the SPLIT_QUERY_OUTPUT_BY_SYMBOL
example in SPLIT_QUERY_OUTPUT_BY_SYMBOL.otq.


	"""
	class Parameters:
		otq_query = "OTQ_QUERY"
		otq_query_params = "OTQ_QUERY_PARAMS"
		symbol_field_name = "SYMBOL_FIELD_NAME"
		allow_multiple_symbol_names = "ALLOW_MULTIPLE_SYMBOL_NAMES"
		ensure_single_invocation = "ENSURE_SINGLE_INVOCATION"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["otq_query", "otq_query_params", "symbol_field_name", "allow_multiple_symbol_names", "ensure_single_invocation"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["otq_query", "_default_otq_query", "otq_query_params", "_default_otq_query_params", "symbol_field_name", "_default_symbol_field_name", "allow_multiple_symbol_names", "_default_allow_multiple_symbol_names", "ensure_single_invocation", "_default_ensure_single_invocation", "stack_info", "_used_strings"]

	def __init__(self, otq_query="", otq_query_params="", symbol_field_name="SYMBOL_NAME", allow_multiple_symbol_names=False, ensure_single_invocation=False):
		_graph_components.EpBase.__init__(self, "SPLIT_QUERY_OUTPUT_BY_SYMBOL")
		self._default_otq_query = ""
		self.otq_query = otq_query
		self._default_otq_query_params = ""
		self.otq_query_params = otq_query_params
		self._default_symbol_field_name = "SYMBOL_NAME"
		self.symbol_field_name = symbol_field_name
		self._default_allow_multiple_symbol_names = False
		self.allow_multiple_symbol_names = allow_multiple_symbol_names
		self._default_ensure_single_invocation = False
		self.ensure_single_invocation = ensure_single_invocation
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_otq_query(self, value):
		self.otq_query = value
		return self

	def set_otq_query_params(self, value):
		self.otq_query_params = value
		return self

	def set_symbol_field_name(self, value):
		self.symbol_field_name = value
		return self

	def set_allow_multiple_symbol_names(self, value):
		self.allow_multiple_symbol_names = value
		return self

	def set_ensure_single_invocation(self, value):
		self.ensure_single_invocation = value
		return self

	@staticmethod
	def _get_name():
		return "SPLIT_QUERY_OUTPUT_BY_SYMBOL"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.otq_query != "": 
			desc += "OTQ_QUERY=" + py_to_str(self.otq_query) + ","
		if self.otq_query_params != "": 
			desc += "OTQ_QUERY_PARAMS=" + py_to_str(self.otq_query_params) + ","
		if self.symbol_field_name != "SYMBOL_NAME": 
			desc += "SYMBOL_FIELD_NAME=" + py_to_str(self.symbol_field_name) + ","
		if self.allow_multiple_symbol_names != False: 
			desc += "ALLOW_MULTIPLE_SYMBOL_NAMES=" + py_to_str(self.allow_multiple_symbol_names) + ","
		if self.ensure_single_invocation != False: 
			desc += "ENSURE_SINGLE_INVOCATION=" + py_to_str(self.ensure_single_invocation) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class PointInTime(_graph_components.EpBase):
	"""
		

POINT_IN_TIME

Type: Other

Description: An EP used to execute the specified query and
discretely propagate certain ticks of the resulting time series, namely those
that are offset by the specified time intervals or by the specified
number of ticks relative to certain timestamps. These timestamps are
either timestamps of the input ticks or are specified via the TIMES
parameter of this event processor. In the latter case, the event
processor is a data source (i.e. has no input and is itself a source of
ticks).

More precisely, assume a set of timestamps T = {t1, t2, ..., tn}
and a set of numbers D = {d1,
d2, ..., dm} are given, first
considering the case when the numbers represent time intervals in
milliseconds. Then, for each of the timestamps ti at most m ticks of the queried time
series as of timestamps ti
+ d1, ti + d2, ..., ti + dm
are propagated. Ticks are propagated with timestamp ti and have additional
fields called TICK_TIME, carrying the original,
nanosecond-granularity timestamp of the tick in the queried time series
and OFFSET, carrying the respective offset dj. Clearly, nothing
is propagated for an offset dj,
if the queried time series does not have a tick as of timestamp ti + dj. Note, that offsets can be positive, negative and zero.

For example, assume T =
{2003-12-01 16:00:23, 2003-12-01 16:00:27} and D = {-1000, 1000}. If the queried
time series is as follows (for simplicity, only a simple index field is
present),


Symbol		Time				INDEXDEMO_L1::A	2003/12/01 16:00:21.000000000	1DEMO_L1::A	2003/12/01 16:00:21.000000000	2DEMO_L1::A	2003/12/01 16:00:23.000000000	3DEMO_L1::A	2003/12/01 16:00:24.000000000	4DEMO_L1::A	2003/12/01 16:00:25.000000000	5DEMO_L1::A	2003/12/01 16:00:25.000000000	6DEMO_L1::A	2003/12/01 16:00:26.000000000	7DEMO_L1::A	2003/12/01 16:00:26.000000000	8DEMO_L1::A	2003/12/01 16:00:27.000000000	9DEMO_L1::A	2003/12/01 16:00:27.000000000	10DEMO_L1::A	2003/12/01 16:00:28.000000000	11DEMO_L1::A	2003/12/01 16:00:29.000000000	12



then the output will be:

Symbol		Time				INDEX	POINT_TIME			OFFSETDEMO_L1::A	2003/12/01 16:00:23.000000000	2	2003/12/01 16:00:21.000000000	-1000DEMO_L1::A	2003/12/01 16:00:23.000000000	4	2003/12/01 16:00:24.000000000	1000DEMO_L1::A	2003/12/01 16:00:27.000000000	8	2003/12/01 16:00:26.000000000	-1000DEMO_L1::A	2003/12/01 16:00:27.000000000	11	2003/12/01 16:00:28.000000000	1000


The offsets may alternatively specify number of ticks, rather than
milliseconds, in which case the ticks from the queried time series
being dj
ticks before or after (depending on the offset sign) the tick as of
timestamp ti
are propagated.

In the above example, if D =
{-2, 1}, then the output will be as follows:

Symbol		Time				INDEX	POINT_TIME			OFFSETDEMO_L1::A	2003/12/01 16:00:23.000000000	1	2003/12/01 16:00:21.000000000	-2DEMO_L1::A	2003/12/01 16:00:23.000000000	4	2003/12/01 16:00:24.000000000	1DEMO_L1::A	2003/12/01 16:00:27.000000000	8	2003/12/01 16:00:26.000000000	-2DEMO_L1::A	2003/12/01 16:00:27.000000000	11	2003/12/01 16:00:28.000000000	1


Timestamp set T
is either specified by the TIMES parameter or just
has the timestamps of input ticks. Offsets and offset type are
specified by the OFFSETS and OFFSET_TYPE parameters,
respectively.

Note, that in order this EP to have reasonable
performance, the set T
of timestamps (regardless of where those timestamps come from: TIMES
parameter or input ticks) has to be relatively small. In other words,
the points in time, which the user is interested in, have to be quite
few in order usage of this EP to be justified.
The time
series from which ticks are propagated is obtained by applying the
current queried symbol and current start/end times to the query,
specified by OTQ_QUERY parameter. More precisely, current queried symbol replaces the unbound symbol list of the specified query in case it has unbound symbols, or replaces every bound symbol list of it. In the latter case it is required that bound symbol lists of all its symbol-bound EPs are the same. For millisecond
offsets, current start and end times, before being applied to the
specified OTQ query, may be offset by the minimal negative and maximal
positive offsets respectively in order every possible offset timestamp
to be included in the resulted query interval.

Python
class name:
PointInTime

Input: None or a time series of ticks.Output: A time series of ticks.

Parameters:


  OFFSET_TYPE (enumeration)
    
TIME_MSEC or NUM_TICKS.
Default: TIME_MSEC

    
  OFFSETS (string)
    
Either a comma-separated list of integers, or a special value in the form SP_&lt;NAME&gt;. Here, &lt;NAME&gt; refers to a parameter of the current queried symbol. The value of that parameter is expected to carry the comma-separated list of integer offsets, thus allowing for offsets to be symbol-specific. For example, if OFFSETS=SP_SYMBOL_OFFSETS, then queried symbols are expected to have a parameter with name SYMBOL_OFFSETS which specifies the offsets.
Default: EMPTY

  
  TIMES (string) 
Either a comma-separated list of timestamps, or a special value in the form SP_&lt;NAME&gt;.In the former case timestamps are expected to be in in one of the following formats: 
    
 
      YYYYMMDDhhmmss[.Q]: Full timestamp format with up to 9 digits after the decimal point, allowing nanosecond granularity. Timestamps are interpreted in the query time zone. 
      hhmmss[.Q]: Time-of-day format with optional millisecond precision. When this format is used, a point is generated for each day in the querys time interval at the specified time. 
     Note: All entries in the TIMES list must use the same format. Mixing formats within a single list is not allowed.In the case of SP_&lt;NAME&gt;, &lt;NAME&gt; refers to a parameter of the current queried symbol.  The value of that parameter is expected to carry the comma-separated list of timestamps, just like above. This allows timestamps to be specified on the symbol level. For example, if TIMES=SP_SYMBOL_TIMES, then queried symbols are expected to have a parameter with name SYMBOL_TIMES which specifies the timestamps. This parameter can be nonempty only if this event processor is a data source of ticks. Default: EMPTY
 
  
  INPUT_TS_FIELDS_TO_PROPAGATE (string)
    In case if TIMES parameter is empty and
timestamps come from input ticks, this parameter may optionally specify
a comma-separated of list of input fields to be propagated alongside
with ticks from the queried time series. In other words, when the
timestamp of a particular input tick is used to locate a tick in the
queried time series for a particular offset, that tick is propagated
joined not only with TICK_TIME and OFFSET
fields, but also with the specified fields of the input tick.
Default: EMPTY

  
  OTQ_QUERY (string)
    
The path to an .otq file, possibly followed by the query name (&lt;otq_file_path&gt;::&lt;query_name&gt;),
the latter being needed if the specified .otq file contains multiple
queries. The parameter OTQ_FILE_PATH in the OneTick configuration file
specifies the set of directories where POINT_IN_TIME looks for the file
if the value of this parameter is a relative path.
Default: EMPTY

  
  OTQ_QUERY_PARAMS (string)
    
A comma-separated list of &lt;param_name&gt;=&lt;value&gt;
pairs, used to override default values of .otq parameters in a
specified .otq file.
Default: EMPTY

GO_FORWARD_TO_LAST_TICK_MSEC (milliseconds)
Number of milliseconds to add to the query end datetime. Used primarily when OFFSET_TYPE is set to NUM_TICKS. Serves the purpose of supplying the query with sufficient number of data for POINT_IN_TIME to be able to receive ticks for positive offsets.

Default: 0

OTQ_QUERY_TIME_RANGE_SELECTION_METHOD (enumeration)Specifies the method to determine the time interval of the executed query. Possible values are EXTEND_PARENT_QUERY_RANGE, EXTEND_TIME_POINTS and EXTEND_TIME_POINTS_WITH_OVERLAP_OPTIMIZATION. In the first case the time interval is that of the parent query (the one to which POINT_IN_TIME belongs to), extended with the value of GO_FORWARD_TO_LAST_TICK_MSEC  parameter from the right side as well as by values of minimal negative and maximal
positive millisecond offsets, as described above. In the second case each time point is processed separately, a small interval around it obtained by applying the value of GO_FORWARD_TO_LAST_TICK_MSEC  parameter as well as by applying values of minimal negative and maximal positive millisecond offsets. Choosing between these two depends mainly on two factors. The first factor is whether the intervals around time points cover most of the query interval or not. If not, then voting for the EXTEND_TIME_POINTS option may result in a faster query. The second factor is whether the specified query carries some state or not. If it does, then its output may highly depend on whether it is executed for the whole time interval or just the small sub-intervals around the time points. Finally, the third option, EXTEND_TIME_POINTS_WITH_OVERLAP_OPTIMIZATION, is just like EXTEND_TIME_POINTS, with the exception that overlapping sub-intervals around different time points are merged into one. Note again, that POINT_IN_TIME may output different things depending on the value of this parameter.Default: EXTEND_PARENT_QUERY_RANGE


Examples: See the above examples as well as a few others in point_in_time_examples.otq.

	"""
	class Parameters:
		offset_type = "OFFSET_TYPE"
		offsets = "OFFSETS"
		times = "TIMES"
		input_ts_fields_to_propagate = "INPUT_TS_FIELDS_TO_PROPAGATE"
		otq_query = "OTQ_QUERY"
		otq_query_params = "OTQ_QUERY_PARAMS"
		go_forward_to_last_tick_msec = "GO_FORWARD_TO_LAST_TICK_MSEC"
		otq_query_time_range_selection_method = "OTQ_QUERY_TIME_RANGE_SELECTION_METHOD"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["offset_type", "offsets", "times", "input_ts_fields_to_propagate", "otq_query", "otq_query_params", "go_forward_to_last_tick_msec", "otq_query_time_range_selection_method"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["offset_type", "_default_offset_type", "offsets", "_default_offsets", "times", "_default_times", "input_ts_fields_to_propagate", "_default_input_ts_fields_to_propagate", "otq_query", "_default_otq_query", "otq_query_params", "_default_otq_query_params", "go_forward_to_last_tick_msec", "_default_go_forward_to_last_tick_msec", "otq_query_time_range_selection_method", "_default_otq_query_time_range_selection_method", "stack_info", "_used_strings"]

	class OffsetType:
		NUM_TICKS = "NUM_TICKS"
		TIME_MSEC = "TIME_MSEC"

	class OtqQueryTimeRangeSelectionMethod:
		EXTEND_PARENT_QUERY_RANGE = "EXTEND_PARENT_QUERY_RANGE"
		EXTEND_TIME_POINTS = "EXTEND_TIME_POINTS"
		EXTEND_TIME_POINTS_WITH_OVERLAP_OPTIMIZATION = "EXTEND_TIME_POINTS_WITH_OVERLAP_OPTIMIZATION"

	def __init__(self, offset_type=OffsetType.TIME_MSEC, offsets="", times="", input_ts_fields_to_propagate="", otq_query="", otq_query_params="", go_forward_to_last_tick_msec=0, otq_query_time_range_selection_method=OtqQueryTimeRangeSelectionMethod.EXTEND_PARENT_QUERY_RANGE):
		_graph_components.EpBase.__init__(self, "POINT_IN_TIME")
		self._default_offset_type = type(self).OffsetType.TIME_MSEC
		self.offset_type = offset_type
		self._default_offsets = ""
		self.offsets = offsets
		self._default_times = ""
		self.times = times
		self._default_input_ts_fields_to_propagate = ""
		self.input_ts_fields_to_propagate = input_ts_fields_to_propagate
		self._default_otq_query = ""
		self.otq_query = otq_query
		self._default_otq_query_params = ""
		self.otq_query_params = otq_query_params
		self._default_go_forward_to_last_tick_msec = 0
		self.go_forward_to_last_tick_msec = go_forward_to_last_tick_msec
		self._default_otq_query_time_range_selection_method = type(self).OtqQueryTimeRangeSelectionMethod.EXTEND_PARENT_QUERY_RANGE
		self.otq_query_time_range_selection_method = otq_query_time_range_selection_method
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_offset_type(self, value):
		self.offset_type = value
		return self

	def set_offsets(self, value):
		self.offsets = value
		return self

	def set_times(self, value):
		self.times = value
		return self

	def set_input_ts_fields_to_propagate(self, value):
		self.input_ts_fields_to_propagate = value
		return self

	def set_otq_query(self, value):
		self.otq_query = value
		return self

	def set_otq_query_params(self, value):
		self.otq_query_params = value
		return self

	def set_go_forward_to_last_tick_msec(self, value):
		self.go_forward_to_last_tick_msec = value
		return self

	def set_otq_query_time_range_selection_method(self, value):
		self.otq_query_time_range_selection_method = value
		return self

	@staticmethod
	def _get_name():
		return "POINT_IN_TIME"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.offset_type != self.OffsetType.TIME_MSEC: 
			desc += "OFFSET_TYPE=" + py_to_str(self.offset_type) + ","
		if self.offsets != "": 
			desc += "OFFSETS=" + py_to_str(self.offsets) + ","
		if self.times != "": 
			desc += "TIMES=" + py_to_str(self.times) + ","
		if self.input_ts_fields_to_propagate != "": 
			desc += "INPUT_TS_FIELDS_TO_PROPAGATE=" + py_to_str(self.input_ts_fields_to_propagate) + ","
		if self.otq_query != "": 
			desc += "OTQ_QUERY=" + py_to_str(self.otq_query) + ","
		if self.otq_query_params != "": 
			desc += "OTQ_QUERY_PARAMS=" + py_to_str(self.otq_query_params) + ","
		if self.go_forward_to_last_tick_msec != 0: 
			desc += "GO_FORWARD_TO_LAST_TICK_MSEC=" + py_to_str(self.go_forward_to_last_tick_msec) + ","
		if self.otq_query_time_range_selection_method != self.OtqQueryTimeRangeSelectionMethod.EXTEND_PARENT_QUERY_RANGE: 
			desc += "OTQ_QUERY_TIME_RANGE_SELECTION_METHOD=" + py_to_str(self.otq_query_time_range_selection_method) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class NestedOtq(_graph_components.EpBase):
	"""
		

NESTED_OTQ

Type: Other

Description: Allows query nesting in graph queries.

This EP can be used only in graph queries constructed by an API. It
cannot be inserted from the GUI.

To specify output and input pins in a nested query to which sinks or
sources of NESTED_OTQ EP is connected, output_name and input_name(if
not specified source_name will be used) can be used correspondingly
(when adding a sink or source using API functions).

Python
class name:
NestedOtq

Input: A time series of ticks.

Output: A time series of ticks.

Parameters:


  OTQ_NAME (string)
    The nested query name. It should contain the full query name (&lt;otq_file_path&gt;::&lt;query_name&gt;)
if the specified .otq file contains multiple queries.

    To specify remote queries the following syntax can be used (see Viewing and Executing
Remote Queries):

    remote://&lt;DBNAME&gt;::[full/or/relative/path/]myotq.otq::Graph_1

    Default: empty

  
  OTQ_PARAMETERS (string)
    A comma-separated list of OTQ parameters to be passed to the
nested query.
Default: empty

  
  SHARED_STATE_VARIABLES
(string)
    A comma separated list of state variables that refer to
variables from the outer (i.e., the graph that this EP belongs to)
graph (see State Variables).

  


	"""
	class Parameters:
		otq_name = "OTQ_NAME"
		otq_parameters = "OTQ_PARAMETERS"
		shared_state_variables = "SHARED_STATE_VARIABLES"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["otq_name", "otq_parameters", "shared_state_variables"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["otq_name", "_default_otq_name", "otq_parameters", "_default_otq_parameters", "shared_state_variables", "_default_shared_state_variables", "stack_info", "_used_strings"]

	def __init__(self, otq_name="", otq_parameters="", shared_state_variables=""):
		_graph_components.EpBase.__init__(self, "NESTED_OTQ")
		self._default_otq_name = ""
		self.otq_name = otq_name
		self._default_otq_parameters = ""
		self.otq_parameters = otq_parameters
		self._default_shared_state_variables = ""
		self.shared_state_variables = shared_state_variables
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_otq_name(self, value):
		self.otq_name = value
		return self

	def set_otq_parameters(self, value):
		self.otq_parameters = value
		return self

	def set_shared_state_variables(self, value):
		self.shared_state_variables = value
		return self

	@staticmethod
	def _get_name():
		return "NESTED_OTQ"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.otq_name != "": 
			desc += "OTQ_NAME=" + py_to_str(self.otq_name) + ","
		if self.otq_parameters != "": 
			desc += "OTQ_PARAMETERS=" + py_to_str(self.otq_parameters) + ","
		if self.shared_state_variables != "": 
			desc += "SHARED_STATE_VARIABLES=" + py_to_str(self.shared_state_variables) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class Omd_writeToKafka(_graph_components.EpBase):
	"""
		

OMD::WRITE_TO_KAFKA

Type: Output Adapter

Description: Publishes incoming ticks to a kafka broker.

To be able to run it, the following entry needs to be added to the
main configuration file:

LOAD_KAFKA=true

Python
class name:
Omd_writeToKafka

Input: A time series of ticks

Output: A time series of ticks or none.

Parameters:


  MSG_FORMAT (enum)
    Specifies the output format: NAME_VALUE_PAIRS,
    BINARY, STRING_VAL_OF_SINGLE_FIELD, PROTOBUF, or AVRO.
In case of NAME_VALUE_PAIRS, each
tick is converted into a sequence of name-value pairs
(FIELD_NAME=FIELD_VALUE), separated by pipe.
In case of BINARY,
ticks are sent in binary format with occasional tick descriptor
submissions. This format is required if the subscriber is OneTick
application. If another application is going to subscribe for the
published data, NAME_VALUE_PAIRS
should be used to receive self-descriptive ticks.
In case of STRING_VAL_OF_SINGLE_FIELD
the single field of the tick will be converted into string.EP throws an
exception when gets ticks that consist of more than one field.
In case of PROTOBUF, ticks are sent
in Google Protobuf format. 
In case of AVRO, ticks are sent in
Avro format.
Note, that for fields, representing OneTick nanosecond granularity
timestamps, only millisecond parts are converted to Avro long values
and sub-millisecond parts are truncated. In order to retain
sub-millisecond parts, nanosecond granularity timestamps must first be
converted to OneTick long values.

Note that BINARY format provides
better performance. Default: NAME_VALUE_PAIRS
  
  PROPAGATE_TICKS (Boolean)
    Switches on the propagation of ticks. If set to true, ticks are propagated.
Default: true

  
  BROKER_LIST (string)
    The addresses of the brokers, represented as host1:port1,host2:port2,.... This sets the
value of the kafka global configuration option metadata.broker.list.

  
  PRODUCE_KEY (expression)
    Specifies the value that should be used as a key. Possible
values are expression or empty field.
If field is not empty the result of the expression will be set as a key.
If field is empty SYMBOL_NAME will
be set as a key

  
  TOPIC (string)
    Specifies the value that should be used as a topic name.
Possible values are DB_NAME, SYMBOL_NAME, CUSTOM
and CUSTOM_TOPIC_NAME_EXPR.
Default: DB_NAME

  
  USER (string)
    The user name used to connect to kafka broker. This sets the
value of the kafka global configuration option sasl.username.

  
  PASSWORD (string)
    The password used to connect to kafka broker. This sets the
value of the kafka global configuration option sasl.password.

  
  CONNECTION_PROPERTIES (string)
    Specifies additional properties to be set for kafka global
configuration in format property1=value1,property2=value2..., for full
list of available values refer to this link 
https://kafka.apache.org/documentation/#producerconfigs .

  
  CUSTOM_TOPIC_NAME
    Sets a custom name as a topic if the CUSTOM
value is used for the TOPIC parameter. .
If the CUSTOM_TOPIC_NAME_EXPR value
is specified for the TOPIC parameter, then value of this
parameter must be an expression and result of this expression will be
set as a topic .

  
  PARTITION
    Specifies the partition to produce.

  
  TICK_DESCRIPTOR_RESEND_INTERVAL
    Sets an interval to resend tick descriptor for each time series.

  
  SCHEMA
    The file, containing schemas of protobuf or avro messages.
Required, if MSG_FORMAT is PROTOBUF
or AVRO.

  
  REPORT_KAFKA_LOG_EVENTS
    If specified, all log events received from Kafka library will be
logged.

  

Notes:


  Kafka message header TICK_TYPE
is added to all sent Kafka messages. with its value set to that of the
input field TICK_TYPE, if present, or to the value of the pseudo-column
_TICK_TYPE.
  Kafka message header SYMBOL_NAME
is added to all sent Kafka messages. with its value set to that of the
value of the pseudo-column _SYMBOL_NAME, if that value is not empty,
and to _EMPTY_ otherwise.


	"""
	class Parameters:
		propagate_ticks = "PROPAGATE_TICKS"
		broker_list = "BROKER_LIST"
		user = "USER"
		password = "PASSWORD"
		connection_properties = "CONNECTION_PROPERTIES"
		produce_key = "PRODUCE_KEY"
		topic = "TOPIC"
		custom_topic_name = "CUSTOM_TOPIC_NAME"
		partition = "PARTITION"
		msg_format = "MSG_FORMAT"
		schema_file = "SCHEMA_FILE"
		tick_descriptor_resend_interval = "TICK_DESCRIPTOR_RESEND_INTERVAL"
		report_kafka_log_events = "REPORT_KAFKA_LOG_EVENTS"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["propagate_ticks", "broker_list", "user", "password", "connection_properties", "produce_key", "topic", "custom_topic_name", "partition", "msg_format", "schema_file", "tick_descriptor_resend_interval", "report_kafka_log_events"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["propagate_ticks", "_default_propagate_ticks", "broker_list", "_default_broker_list", "user", "_default_user", "password", "_default_password", "connection_properties", "_default_connection_properties", "produce_key", "_default_produce_key", "topic", "_default_topic", "custom_topic_name", "_default_custom_topic_name", "partition", "_default_partition", "msg_format", "_default_msg_format", "schema_file", "_default_schema_file", "tick_descriptor_resend_interval", "_default_tick_descriptor_resend_interval", "report_kafka_log_events", "_default_report_kafka_log_events", "stack_info", "_used_strings"]

	class Topic:
		CUSTOM = "CUSTOM"
		CUSTOM_TOPIC_NAME_EXPR = "CUSTOM_TOPIC_NAME_EXPR"
		DB_NAME = "DB_NAME"
		SYMBOL_NAME = "SYMBOL_NAME"

	class MsgFormat:
		AVRO = "AVRO"
		BINARY = "BINARY"
		NAME_VALUE_PAIRS = "NAME_VALUE_PAIRS"
		PROTOBUF = "PROTOBUF"
		STRING_VAL_OF_SINGLE_FIELD = "STRING_VAL_OF_SINGLE_FIELD"

	def __init__(self, propagate_ticks=True, broker_list="", user="", password="", connection_properties="", produce_key="", topic=Topic.DB_NAME, custom_topic_name="", partition="", msg_format=MsgFormat.NAME_VALUE_PAIRS, schema_file="", tick_descriptor_resend_interval=5, report_kafka_log_events=False):
		_graph_components.EpBase.__init__(self, "OMD::WRITE_TO_KAFKA")
		self._default_propagate_ticks = True
		self.propagate_ticks = propagate_ticks
		self._default_broker_list = ""
		self.broker_list = broker_list
		self._default_user = ""
		self.user = user
		self._default_password = ""
		self.password = password
		self._default_connection_properties = ""
		self.connection_properties = connection_properties
		self._default_produce_key = ""
		self.produce_key = produce_key
		self._default_topic = type(self).Topic.DB_NAME
		self.topic = topic
		self._default_custom_topic_name = ""
		self.custom_topic_name = custom_topic_name
		self._default_partition = ""
		self.partition = partition
		self._default_msg_format = type(self).MsgFormat.NAME_VALUE_PAIRS
		self.msg_format = msg_format
		self._default_schema_file = ""
		self.schema_file = schema_file
		self._default_tick_descriptor_resend_interval = 5
		self.tick_descriptor_resend_interval = tick_descriptor_resend_interval
		self._default_report_kafka_log_events = False
		self.report_kafka_log_events = report_kafka_log_events
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_propagate_ticks(self, value):
		self.propagate_ticks = value
		return self

	def set_broker_list(self, value):
		self.broker_list = value
		return self

	def set_user(self, value):
		self.user = value
		return self

	def set_password(self, value):
		self.password = value
		return self

	def set_connection_properties(self, value):
		self.connection_properties = value
		return self

	def set_produce_key(self, value):
		self.produce_key = value
		return self

	def set_topic(self, value):
		self.topic = value
		return self

	def set_custom_topic_name(self, value):
		self.custom_topic_name = value
		return self

	def set_partition(self, value):
		self.partition = value
		return self

	def set_msg_format(self, value):
		self.msg_format = value
		return self

	def set_schema_file(self, value):
		self.schema_file = value
		return self

	def set_tick_descriptor_resend_interval(self, value):
		self.tick_descriptor_resend_interval = value
		return self

	def set_report_kafka_log_events(self, value):
		self.report_kafka_log_events = value
		return self

	@staticmethod
	def _get_name():
		return "OMD::WRITE_TO_KAFKA"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.propagate_ticks != True: 
			desc += "PROPAGATE_TICKS=" + py_to_str(self.propagate_ticks) + ","
		if self.broker_list != "": 
			desc += "BROKER_LIST=" + py_to_str(self.broker_list) + ","
		if self.user != "": 
			desc += "USER=" + py_to_str(self.user) + ","
		if self.password != "": 
			desc += "PASSWORD=" + py_to_str(self.password) + ","
		if self.connection_properties != "": 
			desc += "CONNECTION_PROPERTIES=" + py_to_str(self.connection_properties) + ","
		if self.produce_key != "": 
			desc += "PRODUCE_KEY=" + py_to_str(self.produce_key) + ","
		if self.topic != self.Topic.DB_NAME: 
			desc += "TOPIC=" + py_to_str(self.topic) + ","
		if self.custom_topic_name != "": 
			desc += "CUSTOM_TOPIC_NAME=" + py_to_str(self.custom_topic_name) + ","
		if self.partition != "": 
			desc += "PARTITION=" + py_to_str(self.partition) + ","
		if self.msg_format != self.MsgFormat.NAME_VALUE_PAIRS: 
			desc += "MSG_FORMAT=" + py_to_str(self.msg_format) + ","
		if self.schema_file != "": 
			desc += "SCHEMA_FILE=" + py_to_str(self.schema_file) + ","
		if self.tick_descriptor_resend_interval != 5: 
			desc += "TICK_DESCRIPTOR_RESEND_INTERVAL=" + py_to_str(self.tick_descriptor_resend_interval) + ","
		if self.report_kafka_log_events != False: 
			desc += "REPORT_KAFKA_LOG_EVENTS=" + py_to_str(self.report_kafka_log_events) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class OtqPlaceholder(_graph_components.EpBase):
	"""
		

  OTQ_PLACEHOLDER

Description: This EP can be used only in graph queries constructed by an API. Has the same functionality as _OTQ_PLACEHOLDER EP.

Parameters:



  OTQ_PATH
    Nested OTQ query name.
    Default: an OTQ parameter $OTQ_PATH

  
  OTQ_PARAMETERS
    Space-separated list of names of OTQ parameters of the nested OTQ.
    Default: $PARAM1 $PARAM2

  
  INPUT_PIN_NAMES
    Space-separated list of names of input pins of the nested OTQ query.
    Default: IN

  
  OUTPUT_PIN_NAMES
    Space-separated list of names of output pins of the nested OTQ query.
    Default: OUT

  
  COMMENT_OUT
    If set to true, yes, or 1, the OTQ_PLACEHOLDER is treated as commented out; that is, the nested OTQ query specified in the OTQ_PATH parameter is ignored. eval and expr are also supported, assuming the resulted value is numeric.

  




	"""
	class Parameters:
		otq_path = "OTQ_PATH"
		otq_parameters = "OTQ_PARAMETERS"
		input_pin_names = "INPUT_PIN_NAMES"
		output_pin_names = "OUTPUT_PIN_NAMES"
		comment_out = "COMMENT_OUT"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["otq_path", "otq_parameters", "input_pin_names", "output_pin_names", "comment_out"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["otq_path", "_default_otq_path", "otq_parameters", "_default_otq_parameters", "input_pin_names", "_default_input_pin_names", "output_pin_names", "_default_output_pin_names", "comment_out", "_default_comment_out", "stack_info", "_used_strings"]

	def __init__(self, otq_path="", otq_parameters="", input_pin_names="", output_pin_names="", comment_out=""):
		_graph_components.EpBase.__init__(self, "OTQ_PLACEHOLDER")
		self._default_otq_path = ""
		self.otq_path = otq_path
		self._default_otq_parameters = ""
		self.otq_parameters = otq_parameters
		self._default_input_pin_names = ""
		self.input_pin_names = input_pin_names
		self._default_output_pin_names = ""
		self.output_pin_names = output_pin_names
		self._default_comment_out = ""
		self.comment_out = comment_out
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_otq_path(self, value):
		self.otq_path = value
		return self

	def set_otq_parameters(self, value):
		self.otq_parameters = value
		return self

	def set_input_pin_names(self, value):
		self.input_pin_names = value
		return self

	def set_output_pin_names(self, value):
		self.output_pin_names = value
		return self

	def set_comment_out(self, value):
		self.comment_out = value
		return self

	@staticmethod
	def _get_name():
		return "OTQ_PLACEHOLDER"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.otq_path != "": 
			desc += "OTQ_PATH=" + py_to_str(self.otq_path) + ","
		if self.otq_parameters != "": 
			desc += "OTQ_PARAMETERS=" + py_to_str(self.otq_parameters) + ","
		if self.input_pin_names != "": 
			desc += "INPUT_PIN_NAMES=" + py_to_str(self.input_pin_names) + ","
		if self.output_pin_names != "": 
			desc += "OUTPUT_PIN_NAMES=" + py_to_str(self.output_pin_names) + ","
		if self.comment_out != "": 
			desc += "COMMENT_OUT=" + py_to_str(self.comment_out) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class GroupBy(_graph_components.EpBase):
	"""
		

GROUP_BY

Type: Other

Description: Runs every input tick through an instance of the
specified query graph, one for each unique combination of the values of
the specified key fields. Resulting ticks are propagated joined with
key field values. To avoid name collisions, suffix for key field names
can be specified via ADDED_FIELD_NAME_SUFFIX parameter (see
below).

Python
class name:&nbsp;GroupBy

Input: A time series of ticks.

Output: A single or multiple time series of ticks depending
on count of EP outputs (EP outputs are determined by outputs of called
query, and they should be specified in OUTPUTS EP parameter).

Parameters:


  QUERY_NAME (string)
    Specifies the query to be executed as a path to an .otq file,
possibly followed by the query name (&lt;otq_file_path&gt;::&lt;query_name&gt;).
Specified query may have multiple output nodes, but it should have one
input node.

  
  KEY_FIELDS (string)
    Comma-separated list of key field names. Each unique combination
of values of key fields results in association with them of an instance
of the specified query. Types of key fields can not be changed, for
string fields size of fields also should not change.

  
  OTQ_PARAMETERS (string)
    Comma separated list of &lt;name&gt;=&lt;value&gt; pairs
specifying OTQ parameters of the query to be executed.
Default: empty

  
  ADDED_FIELD_NAME_SUFFIX
(string)
    The suffix for the names of appended key field names.
Default: empty

  
  SYMBOL_NAME_FIELD (string)
    If the parameter value is not empty, the unbound symbol name of
the called query is overwritten by the value from the field specified
in this parameter. If the field does not contain a database name, it is
taken from the tick type (if possible) or the LOCAL:: database name is used. If this
parameter is not set, _SYMBOL_NAME
is used, in case if the latter is empty, the _EMPTY
symbol is used.
Default: empty

  
  OUTPUTS (string)
    Lists EP's output names, they should match with output node
names from the called query.
Default: empty

  
  OUT_OF_ORDER_OUTPUT_TICK_POLICY
(enumerated)
    Specifies policy for out of order ticks, produced by created
groups. Possible values are: THROW_EXCEPTION
and DISCARD_TICK.
Default: THROW_EXCEPTION

  
  NUM_THREADS (int)
    If non zero, turns on asynchronous processing mode and specifies
number of threads to be used for processing input ticks. If this
parameter is not specified or zero, then input ticks are processed
synchronously.
Default: empty

  

In asynchronous mode, GROUP_BY EP can be used to
dispatch ticks to its sources in a separate thread. If the inner query
is just a single PASSTHROUGH EP and the
specified key field's value is constant (that is, there is only one
group), GROUP_BY EP will queue ticks and dispatch them in a separate
thread, thus allowing simultaneous tick processing in GROUP_BY's
sources and destinations.

Examples:

GROUP_BY(QUERY_NAME="called_query.otq",KEY_FIELDS=SYMBOL_NAME)

See the GROUP_BY examples in GROUP_BY_EP.otq.


	"""
	class Parameters:
		query_name = "QUERY_NAME"
		key_fields = "KEY_FIELDS"
		otq_parameters = "OTQ_PARAMETERS"
		added_field_name_suffix = "ADDED_FIELD_NAME_SUFFIX"
		symbol_name_field = "SYMBOL_NAME_FIELD"
		outputs = "OUTPUTS"
		out_of_order_output_tick_policy = "OUT_OF_ORDER_OUTPUT_TICK_POLICY"
		num_threads = "NUM_THREADS"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["query_name", "key_fields", "otq_parameters", "added_field_name_suffix", "symbol_name_field", "outputs", "out_of_order_output_tick_policy", "num_threads"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["query_name", "_default_query_name", "key_fields", "_default_key_fields", "otq_parameters", "_default_otq_parameters", "added_field_name_suffix", "_default_added_field_name_suffix", "symbol_name_field", "_default_symbol_name_field", "outputs", "_default_outputs", "out_of_order_output_tick_policy", "_default_out_of_order_output_tick_policy", "num_threads", "_default_num_threads", "stack_info", "_used_strings"]

	class OutOfOrderOutputTickPolicy:
		DISCARD_TICK = "DISCARD_TICK"
		THROW_EXCEPTION = "THROW_EXCEPTION"

	def __init__(self, query_name="", key_fields="", otq_parameters="", added_field_name_suffix="", symbol_name_field="", outputs="", out_of_order_output_tick_policy=OutOfOrderOutputTickPolicy.THROW_EXCEPTION, num_threads=""):
		_graph_components.EpBase.__init__(self, "GROUP_BY")
		self._default_query_name = ""
		self.query_name = query_name
		self._default_key_fields = ""
		self.key_fields = key_fields
		self._default_otq_parameters = ""
		self.otq_parameters = otq_parameters
		self._default_added_field_name_suffix = ""
		self.added_field_name_suffix = added_field_name_suffix
		self._default_symbol_name_field = ""
		self.symbol_name_field = symbol_name_field
		self._default_outputs = ""
		self.outputs = outputs
		self._default_out_of_order_output_tick_policy = type(self).OutOfOrderOutputTickPolicy.THROW_EXCEPTION
		self.out_of_order_output_tick_policy = out_of_order_output_tick_policy
		self._default_num_threads = ""
		self.num_threads = num_threads
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_query_name(self, value):
		self.query_name = value
		return self

	def set_key_fields(self, value):
		self.key_fields = value
		return self

	def set_otq_parameters(self, value):
		self.otq_parameters = value
		return self

	def set_added_field_name_suffix(self, value):
		self.added_field_name_suffix = value
		return self

	def set_symbol_name_field(self, value):
		self.symbol_name_field = value
		return self

	def set_outputs(self, value):
		self.outputs = value
		return self

	def set_out_of_order_output_tick_policy(self, value):
		self.out_of_order_output_tick_policy = value
		return self

	def set_num_threads(self, value):
		self.num_threads = value
		return self

	@staticmethod
	def _get_name():
		return "GROUP_BY"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.query_name != "": 
			desc += "QUERY_NAME=" + py_to_str(self.query_name) + ","
		if self.key_fields != "": 
			desc += "KEY_FIELDS=" + py_to_str(self.key_fields) + ","
		if self.otq_parameters != "": 
			desc += "OTQ_PARAMETERS=" + py_to_str(self.otq_parameters) + ","
		if self.added_field_name_suffix != "": 
			desc += "ADDED_FIELD_NAME_SUFFIX=" + py_to_str(self.added_field_name_suffix) + ","
		if self.symbol_name_field != "": 
			desc += "SYMBOL_NAME_FIELD=" + py_to_str(self.symbol_name_field) + ","
		if self.outputs != "": 
			desc += "OUTPUTS=" + py_to_str(self.outputs) + ","
		if self.out_of_order_output_tick_policy != self.OutOfOrderOutputTickPolicy.THROW_EXCEPTION: 
			desc += "OUT_OF_ORDER_OUTPUT_TICK_POLICY=" + py_to_str(self.out_of_order_output_tick_policy) + ","
		if self.num_threads != "": 
			desc += "NUM_THREADS=" + py_to_str(self.num_threads) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class SwitchSymbolsAndThreads(_graph_components.EpBase):
	"""
		

SWITCH_SYMBOLS_AND_THREADS

Type: Other

Description: Runs every input tick of a CEP query through an
instance of the specified query graph in a different dispatch thread,
one for each value of the field specified in the OUTPUT_SYMBOL_FIELD
parameter (see below). Resulting ticks are not propagated back to EP's
destinations. By default, the dispatch thread for a given output symbol
is decided using a hash function. Alternatively, you can use the SWITCH_CONFIG_QUERY
and SWITCH_CONFIG_EXPR parameters to specify the mapping to
dispatch threads.

Python
class name:&nbsp;SwitchSymbolsAndThreads

Input: A time series of ticks

Output: EP produces no output

Parameters:


  QUERY_NAME (string)
    Specifies the query that ticks will be dispatched to (&lt;otq_file_path&gt;::&lt;query_name&gt;).
The specified query must have a single output and a single input.

  
  OTQ_PARAMETERS (string)
    Comma-separated list of &lt;name&gt;=&lt;value&gt; pairs
specifying OTQ parameters of the query to be executed.
Default: empty

  
  SWITCH_CONFIG_QUERY (string)
    Specifies the query that returns the dispatch thread mapping to
which output thread ticks with the given symbol ID (as specified in OUTPUT_SYMBOL_INDEX_FIELD)
are dispatched. Ticks returned by this query must have the SYMBOL_ID
and DISPATCH_THREAD_ID integer fields, where DISPATCH_THREAD_ID should
contain values less than NUM_DISPATCH_THREADS. If SWITCH_CONFIG_EXPR
is not specified and a symbol (specified in OUTPUT_SYMBOL_INDEX_FIELD)
is encountered that is not present in the provided mapping, an
exception is thrown (you can use the SWITCH_SYMBOLS_AND_THREADS.THROW_FOR_UNKNOWN_SYMBOLS
config variable to log a warning instead).

  
  SWITCH_CONFIG_OTQ_PARAMETERS
(string)
    Comma-separated list of &lt;name&gt;=&lt;value&gt; pairs
specifying OTQ parameters of the SWITCH_CONFIG_QUERY.
Default: empty

  
  SWITCH_CONFIG_EXPR (string)
    Expression that returns dispatch thread index for the tick (it
should be the same for any given output symbol). If SWITCH_CONFIG_QUERY
is specified, SWITCH_CONFIG_EXPR is used only for symbols that
are not present in SWITCH_CONFIG_QUERY results. In case if
neither SWITCH_CONFIG_QUERY nor SWITCH_CONFIG_EXPR are
specified, dispatch thread is decided using a hash function
Default: empty

  
  OUTPUT_SYMBOL_FIELD (string)
    Separate instance of the specified query is created for each
output symbol. The value from the specified field is also set as an
unbound symbol.

  
  OUTPUT_SYMBOL_INDEX_FIELD
(int)
    This parameter can optionally provide a unique 0-based index for
each symbol. Use of OUTPUT_SYMBOL_INDEX_FIELD facilitates much faster
dispatch of input ticks to their corresponding output threads and also
faster operation of those output threads.
Default: empty

  
  NUM_DISPATCH_THREADS (int)
    Number of dispatch threads.
Default: empty

  
  MAX_INPUT_THREAD_DELAY_SEC
(int)
    Time period to wait for input threads. Within this time period,
no output events are propagated if MAX_EXPECTED_INPUT_THREADS
is not set. If a new input thread is detected after this period of
time, an exception is thrown.
Default: 3

  
  SHARE_HTBT_ACROSS_OUT_THREAD_SYMBOLS
(boolean)
    Propagates heartbeats to one of symbols only in dispatch thread
if set to true; otherwise, all symbols in dispatch thread receive
heartbeats.
Default: false

  
  MAX_EXPECTED_INPUT_THREADS
(boolean)
    If specified, tick dispatching starts either if the
corresponding amount of input threads are detected or after the MAX_INPUT_THREAD_DELAY_SEC
time period has elapsed.
Default: false

  

This EP should be used only in CEP queries.


	"""
	class Parameters:
		query_name = "QUERY_NAME"
		otq_parameters = "OTQ_PARAMETERS"
		switch_config_query = "SWITCH_CONFIG_QUERY"
		switch_config_otq_parameters = "SWITCH_CONFIG_OTQ_PARAMETERS"
		switch_config_expr = "SWITCH_CONFIG_EXPR"
		output_symbol_field = "OUTPUT_SYMBOL_FIELD"
		output_symbol_index_field = "OUTPUT_SYMBOL_INDEX_FIELD"
		num_dispatch_threads = "NUM_DISPATCH_THREADS"
		max_input_thread_delay_sec = "MAX_INPUT_THREAD_DELAY_SEC"
		share_htbt_across_out_thread_symbols = "SHARE_HTBT_ACROSS_OUT_THREAD_SYMBOLS"
		max_expected_input_threads = "MAX_EXPECTED_INPUT_THREADS"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["query_name", "otq_parameters", "switch_config_query", "switch_config_otq_parameters", "switch_config_expr", "output_symbol_field", "output_symbol_index_field", "num_dispatch_threads", "max_input_thread_delay_sec", "share_htbt_across_out_thread_symbols", "max_expected_input_threads"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["query_name", "_default_query_name", "otq_parameters", "_default_otq_parameters", "switch_config_query", "_default_switch_config_query", "switch_config_otq_parameters", "_default_switch_config_otq_parameters", "switch_config_expr", "_default_switch_config_expr", "output_symbol_field", "_default_output_symbol_field", "output_symbol_index_field", "_default_output_symbol_index_field", "num_dispatch_threads", "_default_num_dispatch_threads", "max_input_thread_delay_sec", "_default_max_input_thread_delay_sec", "share_htbt_across_out_thread_symbols", "_default_share_htbt_across_out_thread_symbols", "max_expected_input_threads", "_default_max_expected_input_threads", "stack_info", "_used_strings"]

	def __init__(self, query_name="", otq_parameters="", switch_config_query="", switch_config_otq_parameters="", switch_config_expr="", output_symbol_field="", output_symbol_index_field="", num_dispatch_threads="", max_input_thread_delay_sec="", share_htbt_across_out_thread_symbols="", max_expected_input_threads=""):
		_graph_components.EpBase.__init__(self, "SWITCH_SYMBOLS_AND_THREADS")
		self._default_query_name = ""
		self.query_name = query_name
		self._default_otq_parameters = ""
		self.otq_parameters = otq_parameters
		self._default_switch_config_query = ""
		self.switch_config_query = switch_config_query
		self._default_switch_config_otq_parameters = ""
		self.switch_config_otq_parameters = switch_config_otq_parameters
		self._default_switch_config_expr = ""
		self.switch_config_expr = switch_config_expr
		self._default_output_symbol_field = ""
		self.output_symbol_field = output_symbol_field
		self._default_output_symbol_index_field = ""
		self.output_symbol_index_field = output_symbol_index_field
		self._default_num_dispatch_threads = ""
		self.num_dispatch_threads = num_dispatch_threads
		self._default_max_input_thread_delay_sec = ""
		self.max_input_thread_delay_sec = max_input_thread_delay_sec
		self._default_share_htbt_across_out_thread_symbols = ""
		self.share_htbt_across_out_thread_symbols = share_htbt_across_out_thread_symbols
		self._default_max_expected_input_threads = ""
		self.max_expected_input_threads = max_expected_input_threads
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_query_name(self, value):
		self.query_name = value
		return self

	def set_otq_parameters(self, value):
		self.otq_parameters = value
		return self

	def set_switch_config_query(self, value):
		self.switch_config_query = value
		return self

	def set_switch_config_otq_parameters(self, value):
		self.switch_config_otq_parameters = value
		return self

	def set_switch_config_expr(self, value):
		self.switch_config_expr = value
		return self

	def set_output_symbol_field(self, value):
		self.output_symbol_field = value
		return self

	def set_output_symbol_index_field(self, value):
		self.output_symbol_index_field = value
		return self

	def set_num_dispatch_threads(self, value):
		self.num_dispatch_threads = value
		return self

	def set_max_input_thread_delay_sec(self, value):
		self.max_input_thread_delay_sec = value
		return self

	def set_share_htbt_across_out_thread_symbols(self, value):
		self.share_htbt_across_out_thread_symbols = value
		return self

	def set_max_expected_input_threads(self, value):
		self.max_expected_input_threads = value
		return self

	@staticmethod
	def _get_name():
		return "SWITCH_SYMBOLS_AND_THREADS"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.query_name != "": 
			desc += "QUERY_NAME=" + py_to_str(self.query_name) + ","
		if self.otq_parameters != "": 
			desc += "OTQ_PARAMETERS=" + py_to_str(self.otq_parameters) + ","
		if self.switch_config_query != "": 
			desc += "SWITCH_CONFIG_QUERY=" + py_to_str(self.switch_config_query) + ","
		if self.switch_config_otq_parameters != "": 
			desc += "SWITCH_CONFIG_OTQ_PARAMETERS=" + py_to_str(self.switch_config_otq_parameters) + ","
		if self.switch_config_expr != "": 
			desc += "SWITCH_CONFIG_EXPR=" + py_to_str(self.switch_config_expr) + ","
		if self.output_symbol_field != "": 
			desc += "OUTPUT_SYMBOL_FIELD=" + py_to_str(self.output_symbol_field) + ","
		if self.output_symbol_index_field != "": 
			desc += "OUTPUT_SYMBOL_INDEX_FIELD=" + py_to_str(self.output_symbol_index_field) + ","
		if self.num_dispatch_threads != "": 
			desc += "NUM_DISPATCH_THREADS=" + py_to_str(self.num_dispatch_threads) + ","
		if self.max_input_thread_delay_sec != "": 
			desc += "MAX_INPUT_THREAD_DELAY_SEC=" + py_to_str(self.max_input_thread_delay_sec) + ","
		if self.share_htbt_across_out_thread_symbols != "": 
			desc += "SHARE_HTBT_ACROSS_OUT_THREAD_SYMBOLS=" + py_to_str(self.share_htbt_across_out_thread_symbols) + ","
		if self.max_expected_input_threads != "": 
			desc += "MAX_EXPECTED_INPUT_THREADS=" + py_to_str(self.max_expected_input_threads) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class ModifyTsFieldProperties(_graph_components.EpBase):
	"""
		

MODIFY_TS_FIELD_PROPERTIES

Type: Other

Description: Modifies a property for the specified tick
descriptor field (see&nbsp;td_field_properties.html).
Note that certain properties that pertain to fields are actually set on
tick descriptor level, MODIFY_TS_PROPERTIES
should be used for those.

Note that in most if not all cases, modifying these properties has
no relevance/effect on analytics. The main application of this EP is to
annotate a tick descriptor with properties for further processing by
loading logic. This is useful if the query is used for transformation
of ticks prior to loading (see docs/transforming_ticks_in_loader.html)
or when the query itself writes to a database or raw data using WRITE_TO_ONETICK_DB or WRITE_TO_RAW.

Python
class name:
ModifyTsFieldProperties

Input: A time series of ticks

Output: A time series of ticks

Parameters:


  FIELD_NAME (string)
    A field, whose properties will be modified.

  
  PROPERTY_NAME (string)
    Property to modify. A user-defined property or one of predefined
properties (see docs/datamodeling.html#td_field_properties).

  
  PROPERTY_VALUE (string)
    The value of the property.

  
  ALLOW_MISSING_FIELDS (boolean)
    If set to TRUE, then the EP
will not throw an exception, if FIELD_NAME is not specified or
doesn't exist in tick descriptor
Default: FALSE

  

Examples:

MODIFY_TS_FIELD_PROPERTIES(FIELD_NAME=PRICE,PROPERTY_NAME=DELTA_VALUE_SUPPORT,PROPERTY_VALUE=1)MODIFY_TS_FIELD_PROPERTIES(FIELD_NAME=FLD,PROPERTY_NAME=DELTA_VALUE_SUPPORT,PROPERTY_VALUE=a,ALLOW_MISSING_FIELDS=true)

	"""
	class Parameters:
		field_name = "FIELD_NAME"
		property_name = "PROPERTY_NAME"
		property_value = "PROPERTY_VALUE"
		allow_missing_fields = "ALLOW_MISSING_FIELDS"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["field_name", "property_name", "property_value", "allow_missing_fields"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["field_name", "_default_field_name", "property_name", "_default_property_name", "property_value", "_default_property_value", "allow_missing_fields", "_default_allow_missing_fields", "stack_info", "_used_strings"]

	class PropertyName:
		EMPTY = ""
		ATOMIC_GROUP_BEGIN_INDICATOR = "ATOMIC_GROUP_BEGIN_INDICATOR"
		ATOMIC_GROUP_END_INDICATOR = "ATOMIC_GROUP_END_INDICATOR"
		DELETE_STATE_KEY_INDICATOR = "DELETE_STATE_KEY_INDICATOR"
		DELTA_VALUE_SUPPORT = "DELTA_VALUE_SUPPORT"
		EMPTY_STATE_INDICATOR = "EMPTY_STATE_INDICATOR"
		OLD_ORDER_ID = "OLD_ORDER_ID"
		STATE_END_TICK_INDICATOR = "STATE_END_TICK_INDICATOR"
		STATE_KEY_UPDATE_TIME = "STATE_KEY_UPDATE_TIME"
		STATE_TICK_INDICATOR = "STATE_TICK_INDICATOR"
		UNICODE_CHAR_TYPE = "UNICODE_CHAR_TYPE"
		UNINITIALIZED_VALUE = "UNINITIALIZED_VALUE"

	def __init__(self, field_name="", property_name=PropertyName.EMPTY, property_value="", allow_missing_fields=False):
		_graph_components.EpBase.__init__(self, "MODIFY_TS_FIELD_PROPERTIES")
		self._default_field_name = ""
		self.field_name = field_name
		self._default_property_name = type(self).PropertyName.EMPTY
		self.property_name = property_name
		self._default_property_value = ""
		self.property_value = property_value
		self._default_allow_missing_fields = False
		self.allow_missing_fields = allow_missing_fields
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_field_name(self, value):
		self.field_name = value
		return self

	def set_property_name(self, value):
		self.property_name = value
		return self

	def set_property_value(self, value):
		self.property_value = value
		return self

	def set_allow_missing_fields(self, value):
		self.allow_missing_fields = value
		return self

	@staticmethod
	def _get_name():
		return "MODIFY_TS_FIELD_PROPERTIES"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.field_name != "": 
			desc += "FIELD_NAME=" + py_to_str(self.field_name) + ","
		if self.property_name != self.PropertyName.EMPTY: 
			desc += "PROPERTY_NAME=" + py_to_str(self.property_name) + ","
		if self.property_value != "": 
			desc += "PROPERTY_VALUE=" + py_to_str(self.property_value) + ","
		if self.allow_missing_fields != False: 
			desc += "ALLOW_MISSING_FIELDS=" + py_to_str(self.allow_missing_fields) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class QuerySymbols(_graph_components.EpBase):
	"""
		

QUERY_SYMBOLS

Type: Other

Description: This event processor takes in ticks which
contain a field called SYMBOL_NAME as well and other fields. The
portion of the graph after QUERY_SYMBOLS becomes the next query with
the symbol names, tick type, and per-symbol parameters obtained from
QUERY_SYMBOLS. Symbol names for the second query are the values of
SYMBOL_NAME fields in the input ticks.

It can be used with CSV_FILE_LISTING,
to read in symbols for binding to a graph from a file.

This event processor can only be used in chain queries. It cannot be
the first or the last event processor in the chain query.

Python
class name:&nbsp;QuerySymbols

Input: A time series of ticks with SYMBOL_NAME as one of the
attributes.

Output: N/A - the symbols found in SYMBOL_NAME will be used
as the symbol list for the next part of the graph.

Parameters:


  SYMBOL_DATE (date as YYYYMMDD)
    Specifies the symbol date for all the symbols to be passed to
the next query. Note, that in GUI, symbol date must be specified in
accordance with the date input format, chosen in the settings (a hint
is displayed next to this parameter).

  
  TICK_TYPE
    Specifies the tick type to be passed to the next query.

  
  PREV_STAGE_START_DATE (date as
YYMMDD)
Note, that in GUI, PREV_STAGE_START_DATE must be specified in
accordance with the date input format, chosen in the settings (a hint
is displayed next to this parameter).
Default: 0
  PREV_STAGE_START_TIME (time as
HHMMSSmmm)
Note, that in GUI, PREV_STAGE_START_TIME must be specified in
accordance with the time input format (a hint is displayed next to this
parameter).
Default: 0
  PREV_STAGE_END_DATE (date as
YYMMDD)
Note, that in GUI, PREV_STAGE_END_DATE must be specified in accordance
with the date input format, chosen in the settings (a hint is displayed
next to this parameter).
Default: 0
  PREV_STAGE_END_TIME (time as
HHMMSSmmm)
Note, that in GUI, PREV_STAGE_END_TIME must be specified in accordance
with the time input format (a hint is displayed next to this parameter).
Default: 0
  PREV_STAGE_TIMEZONE
    Start and end time of the query that precedes the QUERY_SYMBOLS
event processor.
Default: Local

  

Examples: Compute the portfolio price with 1 minute buckets
for the portfolio files specified as input symbols and symbol date for
all the symbols in the portfolio files set to 20050103:

CSV_FILE_LISTING()

QUERY_SYMBOLS(20050103)

PORTFOLIO_PRICE(60,false,AS_SEPARATE_BUCKET,BOTH)

&nbsp;

See the CSV_FILE_LISTING,
QUERY_SYMBOLS,
and RETURN examples
in OTHER_EXAMPLES.otq.


	"""
	class Parameters:
		symbol_date = "SYMBOL_DATE"
		tick_type_field = "TICK_TYPE"
		prev_stage_start_date = "PREV_STAGE_START_DATE"
		prev_stage_start_time = "PREV_STAGE_START_TIME"
		prev_stage_end_date = "PREV_STAGE_END_DATE"
		prev_stage_end_time = "PREV_STAGE_END_TIME"
		prev_stage_timezone = "PREV_STAGE_TIMEZONE"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["symbol_date", "tick_type_field", "prev_stage_start_date", "prev_stage_start_time", "prev_stage_end_date", "prev_stage_end_time", "prev_stage_timezone"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["symbol_date", "_default_symbol_date", "tick_type_field", "_default_tick_type_field", "prev_stage_start_date", "_default_prev_stage_start_date", "prev_stage_start_time", "_default_prev_stage_start_time", "prev_stage_end_date", "_default_prev_stage_end_date", "prev_stage_end_time", "_default_prev_stage_end_time", "prev_stage_timezone", "_default_prev_stage_timezone", "stack_info", "_used_strings"]

	def __init__(self, symbol_date="", tick_type_field="", prev_stage_start_date="", prev_stage_start_time="", prev_stage_end_date="", prev_stage_end_time="", prev_stage_timezone=""):
		_graph_components.EpBase.__init__(self, "QUERY_SYMBOLS")
		self._default_symbol_date = ""
		self.symbol_date = symbol_date
		self._default_tick_type_field = ""
		self.tick_type_field = tick_type_field
		self._default_prev_stage_start_date = ""
		self.prev_stage_start_date = prev_stage_start_date
		self._default_prev_stage_start_time = ""
		self.prev_stage_start_time = prev_stage_start_time
		self._default_prev_stage_end_date = ""
		self.prev_stage_end_date = prev_stage_end_date
		self._default_prev_stage_end_time = ""
		self.prev_stage_end_time = prev_stage_end_time
		self._default_prev_stage_timezone = ""
		self.prev_stage_timezone = prev_stage_timezone
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_symbol_date(self, value):
		self.symbol_date = value
		return self

	def set_tick_type_field(self, value):
		self.tick_type_field = value
		return self

	def set_prev_stage_start_date(self, value):
		self.prev_stage_start_date = value
		return self

	def set_prev_stage_start_time(self, value):
		self.prev_stage_start_time = value
		return self

	def set_prev_stage_end_date(self, value):
		self.prev_stage_end_date = value
		return self

	def set_prev_stage_end_time(self, value):
		self.prev_stage_end_time = value
		return self

	def set_prev_stage_timezone(self, value):
		self.prev_stage_timezone = value
		return self

	@staticmethod
	def _get_name():
		return "QUERY_SYMBOLS"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.symbol_date != "": 
			desc += "SYMBOL_DATE=" + py_to_str(self.symbol_date) + ","
		if self.tick_type_field != "": 
			desc += "TICK_TYPE_FIELD=" + py_to_str(self.tick_type_field) + ","
		if self.prev_stage_start_date != "": 
			desc += "PREV_STAGE_START_DATE=" + py_to_str(self.prev_stage_start_date) + ","
		if self.prev_stage_start_time != "": 
			desc += "PREV_STAGE_START_TIME=" + py_to_str(self.prev_stage_start_time) + ","
		if self.prev_stage_end_date != "": 
			desc += "PREV_STAGE_END_DATE=" + py_to_str(self.prev_stage_end_date) + ","
		if self.prev_stage_end_time != "": 
			desc += "PREV_STAGE_END_TIME=" + py_to_str(self.prev_stage_end_time) + ","
		if self.prev_stage_timezone != "": 
			desc += "PREV_STAGE_TIMEZONE=" + py_to_str(self.prev_stage_timezone) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class MemoryUsage(_graph_components.EpBase):
	"""
		

MEMORY_USAGE

Type: Other

Description: This EP is for tracking memory usage by queries.
It is now obsolete, and should not be
used. See tick_server.html#Tracking_resource_usage
for how use of process resources, including memory, should now be
tracked.&nbsp;&nbsp;

This EP shows information about cumulative memory usage of all
queries executed&nbsp;by the tick server process, since the tick server
process started or since the last time tick server's memory statistics
were reset, whichever is more recent. Memory usage information is
propagated as a tick with 3 numeric fields: TOTAL_ALLOCATED_MEMORY,
TOTAL_UNRELEASED_MEMORY, and PEAK_ALLOCATED_MEMORY. Unless
tick_server.exe was launched with the shared liballocation_interceptors
library&nbsp;(see the tick_server.exe section of the OneTick
Installation and Administration guide for details), all fields of the
output tick will contain 0.&nbsp;

This event processor must be an input node of the graph in which it
is used.

Python
class name:&nbsp;MemoryUsage

Input: None

Output: A single tick.

Parameters:


  RESET_MEMORY_STATS (Boolean)
    A true value resets accumulated memory usage information.
Default: false

  

Example:

MEMORY_USAGE(RESET_MEMORY_STATS='false')

See the MEMORY_USAGE example in OTHER_EXAMPLES.otq.


	"""
	class Parameters:
		reset_memory_stats = "RESET_MEMORY_STATS"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["reset_memory_stats"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["reset_memory_stats", "_default_reset_memory_stats", "stack_info", "_used_strings"]

	def __init__(self, reset_memory_stats=False):
		_graph_components.EpBase.__init__(self, "MEMORY_USAGE")
		self._default_reset_memory_stats = False
		self.reset_memory_stats = reset_memory_stats
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_reset_memory_stats(self, value):
		self.reset_memory_stats = value
		return self

	@staticmethod
	def _get_name():
		return "MEMORY_USAGE"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.reset_memory_stats != False: 
			desc += "RESET_MEMORY_STATS=" + py_to_str(self.reset_memory_stats) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class DeclareStateVariables(_graph_components.EpBase):
	"""
		

DECLARE_STATE_VARIABLES

Type: Other

Description: Declares a set of state
variables by specifying their scope, types and (optionally) initial
values. This event processor (EP) is transparent to the input time
series. What is important is its placement on the graph, which
determines the scope of declared state variables.

Python
class name:&nbsp;DeclareStateVariables

Input: A time series of ticks.

Output: A time series of ticks.

Parameters:


  SCOPE (enumerated type)
    Specifies the scope of the declared variables. Possible scopes
are:

    
      BRANCH - A branch-scope state variable is
visible to all event processors of the branch its declarator EP belongs
to (a branch is a maximal long chain of EPs, each node of which has at
most 1 source and at most 1 sink).
      ALL_INPUTS - An all-inputs-scope state
variable is visible to all event processors of the input subgraph of
its declarator EP (input subgraph of a particular node is the subset of
nodes directly or indirectly feeding it).
      ALL_OUTPUTS - An all-outputs-scope
state variable is visible to all event processors of the output
subgraph of its declarator EP (output subgraph of a particular node is
the subset of nodes directly or indirectly fed by it).
      QUERY - A query scope state variable is
visible to all event processors of the graph.
      CROSS_SYMBOL - A cross-symbol scope
state variable is visible to all event processors across all unbound
symbols. Cross-symbol scope state variables cannot be modified after
initialization.
    
  
  VARIABLES (string)
  
    A list of declarations, separated by ";", each declaration being
of the form

    &lt;TYPE_1&gt; &lt;VARIABLE_1&gt;
[=&lt;INITIAL_VALUE_1], &lt;VARIABLE_2&gt; [=&lt;INITIAL_VALUE_2];
&lt;TYPE_2&gt; &lt;VARIABLE_3&gt; [=&lt;INITIAL_VALUE_3], &hellip;
&lt;VARIABLE_N&gt; [=&lt;INITIAL_VALUE_N].

    &lt;TYPE&gt; can be one of int, long, double, decimal and
string, standing for 32-bit integer, 64-bit integer, double-precision
floating point, 128 bit base 10 floating point type and character
string built-in types, respectively, or it can specify a user-defined
custom type name(see User
defined types). String type state variables can hold character
strings of arbitrary length. For custom types, the full type name
including the package name should be specified.

    &lt;INITIAL_VALUE_1&gt; is a constant expression or an OTQ query
evaluation as in EP Parameters.
Constant expressions are built from boolean operators (AND, OR, NOT),
relationship operators (&lt;, &lt;=, &gt;, &gt;=, =, !=), and
arithmetic operators (+, -, *, /, and + also serves as string
concatenation operator) according to the usual rules for precedence,
with parentheses available to resolve ambiguities. Alongside constant
numeric and (both single and double-quoted) string literals, it is
possible to use OneTick functions like MIN, MAX, MOD, DIV (see
the catalog of built-in functions for the full list).

    You can also use pseudo-fields such as _SYMBOL_NAME,
_START_TIME, and _END_TIME (described in detail in the Pseudo-fields document).

    For declaring TICK_SETs the following syntax should be
used:

    TICK_SET
SET_1("INSERTION_POLICY","KEY_FIELD_1,KEY_FIELD_2,....KEY_FIELD_N")[= eval("file.otq|THIS[::myquery]")]

    Where possible values for INSERTION_POLICY are OLDEST_TICK,
which specifies not to overwrite ticks with the same keys,
LATEST_TICK, which makes the last inserted tick overwrite the one with
the same keys (if existing), and THROW_ON_DUPLICATE, which throws an 
exception if a tick with the same keys already exists.

    KEY_FIELD_1,KEY_FIELD_2,....KEY_FIELD_N are key field names for TICK_SET.

    Initial values for tick sets can specify the OTQ query (via the
eval expression), which will be executed to initialize the tick set.


    Tick sets can also be declared via the TICK_SET_UNORDERED
declarator:

    TICK_SET_UNORDERED
SET_1("INSERTION_POLICY",MAX_DISTINCT_KEYS,"KEY_FIELD_1,KEY_FIELD_2,....KEY_FIELD_N")[= eval("file.otq|THIS[::myquery]")]

    Where the only additional parameter MAX_DISTINCT_KEYS is the
maximum expected size of the set; or -1, if the number of ticks is not
known.

    The other parameters are the same as in TICK_SET.

    In unordered tick sets, the ticks are not ordered internally. As
a result, when iterating over the ticks of an unordered set, no
particular order should be expected. Unordered tick sets are usable in
all EPs or built-in functions wherever tick sets can be used, except
unordered tick sets can not be iterated backwards. For large number of
ticks, when MAX_DISTINCT_KEYS is set accurately, unordered tick sets
are expected to be faster than ordered tick sets.

    TICK_LISTs are declared in the following way:

    TICK_LIST TLIST=eval("file.otq|THIS[::myquery]")

  
  INIT_TICK_UPDATE_POLICY
(enumerated type)
    Specifies policy for updating declared state variables on
initialization ticks. Possible policies are:

    
      NEVER - Declared state variables are not
updated on initialization ticks.
      ALWAYS - State variables are updated on
initialization ticks and after init stage are not reset.
      RESET_AFTER_INIT - State
variables are updated on initialization ticks and the values are reset
after init stage.
    
  

In evals for initializing tick containers, _PROCESS_ON_CLIENT
special parameter can be used to execute an eval in the client process
from the query that is executing on the server.

Examples:

DECLARE_STATE_VARIABLES (SCOPE=BRANCH,VARIABLES="int
V1,V2; double V3")

DECLARE_STATE_VARIABLES (SCOPE=BRANCH,VARIABLES="int
V1,V2; TICK_SET S1("LATEST_TICK","PRICE,SIZE"),S2("LATEST_TICK","PRICE,SIZE")=eval(load_ticks.otq);
TICK_LIST TLIST=eval(load_ticks.otq)")

DECLARE_STATE_VARIABLES
(SCOPE=BRANCH,VARIABLES="TICK_SET_UNORDERED
S1("OLDEST_TICK",100,"PRICE,SIZE")=eval(load_ticks.otq)")

DECLARE_STATE_VARIABLES
(SCOPE=QUERY,VARIABLES="double V1=SQRT(2); int V2=(1+1!=2); string
V3="Current symbol: "+_SYMBOL_NAME");

DECLARE_STATE_VARIABLES
(SCOPE=BRANCH,VARIABLES="TICK_SET
S("OLDEST_TICK","PRICE,SIZE")=eval('helper.otq::read_from_odbc','SYMBOL_NAME=_SYMBOL_NAME,_PROCESS_ON_CLIENT=true')")


	"""
	class Parameters:
		scope = "SCOPE"
		variables = "VARIABLES"
		init_tick_update_policy = "INIT_TICK_UPDATE_POLICY"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["scope", "variables", "init_tick_update_policy"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["scope", "_default_scope", "variables", "_default_variables", "init_tick_update_policy", "_default_init_tick_update_policy", "stack_info", "_used_strings"]

	class Scope:
		ALL_INPUTS = "ALL_INPUTS"
		ALL_OUTPUTS = "ALL_OUTPUTS"
		BRANCH = "BRANCH"
		CROSS_SYMBOL = "CROSS_SYMBOL"
		QUERY = "QUERY"

	class InitTickUpdatePolicy:
		ALWAYS = "ALWAYS"
		NEVER = "NEVER"
		RESET_AFTER_INIT = "RESET_AFTER_INIT"

	def __init__(self, scope=Scope.BRANCH, variables="", init_tick_update_policy=InitTickUpdatePolicy.NEVER):
		_graph_components.EpBase.__init__(self, "DECLARE_STATE_VARIABLES")
		self._default_scope = type(self).Scope.BRANCH
		self.scope = scope
		self._default_variables = ""
		self.variables = variables
		self._default_init_tick_update_policy = type(self).InitTickUpdatePolicy.NEVER
		self.init_tick_update_policy = init_tick_update_policy
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_scope(self, value):
		self.scope = value
		return self

	def set_variables(self, value):
		self.variables = value
		return self

	def set_init_tick_update_policy(self, value):
		self.init_tick_update_policy = value
		return self

	@staticmethod
	def _get_name():
		return "DECLARE_STATE_VARIABLES"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.scope != self.Scope.BRANCH: 
			desc += "SCOPE=" + py_to_str(self.scope) + ","
		if self.variables != "": 
			desc += "VARIABLES=" + py_to_str(self.variables) + ","
		if self.init_tick_update_policy != self.InitTickUpdatePolicy.NEVER: 
			desc += "INIT_TICK_UPDATE_POLICY=" + py_to_str(self.init_tick_update_policy) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class TsExpression(_graph_components.EpBase):
	"""
		

TS_EXPRESSION

Type: Other

Description: Evaluates a numeric expression on ticks of
multiple time series.&nbsp;

It requires the input time series to have associated names. In graph
queries, you can accomplished this by assigning a source name to each
input of the TS_EXPRESSION event processor. In chain queries,
using a tick type of the form &lt;db_name1&gt;::&lt;tick_type1&gt;+&lt;db_name2&gt;::&lt;tick_type2&gt;
(for example, TAQ::TRD+NYSE_OB::OB)
will result in the input time series of TS_EXPRESSION to be named as &lt;tick_type1&gt; and &lt;tick_type2&gt;.

The input expression gets evaluated as soon as a tick from each
input time series is available, after which those ticks are dropped.
Hence, input time series must have an equal number of ticks. Output
ticks have the timestamp of the tick that arrived right before the
output value was evaluated.

Python
class name:
TsExpression

Input: N time series of ticks, where N is greater than or
equal to 1.

Output: A time series of ticks having a single numeric field
containing the output value.

Parameters:


  EXPRESSION (numeric expression)
    An expression built with Boolean operators AND, OR,
and NOT. Accepts arithmetic
operators: +, -, *, / and comparison operators =, !=, &lt;, &lt;=,
    &gt;, &gt;=.
Attribute names are represented without quotes, are case-sensitive,
have to be present in the input time series, and must be prefixed by
the name of the input time series to which they belong. String literals
must be wrapped by single or double quotes.

    In addition, EXPRESSION accepts functions power, sqrt,
    log, log10,
    exp, etc. (see Catalog of Built-in functions for more details).

    Attributes of preceding (those already arrived) ticks can also
be involved in computations, in which case the attribute name is
followed by a negative integer index in square brackets specifying how
far to look back (for example, TRD.PRICE[-1]
or TRD.SIZE[-3]).

  
  OUTPUT_FIELD_NAME (string)
    The name of the output field.
Default: VALUE

  

Examples: See TS_EXPRESSION_example
in TS_EXPRESSION_EXAMPLES.otq.


	"""
	class Parameters:
		expression = "EXPRESSION"
		output_field_name = "OUTPUT_FIELD_NAME"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["expression", "output_field_name"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["expression", "_default_expression", "output_field_name", "_default_output_field_name", "stack_info", "_used_strings"]

	def __init__(self, expression="", output_field_name="VALUE", Out = ""):
		_graph_components.EpBase.__init__(self, "TS_EXPRESSION")
		self._default_expression = ""
		self.expression = expression
		self._default_output_field_name = "VALUE"
		self.output_field_name = output_field_name
		if Out != "":
			self.output_field_name=Out
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_expression(self, value):
		self.expression = value
		return self

	def set_output_field_name(self, value):
		self.output_field_name = value
		return self

	@staticmethod
	def _get_name():
		return "TS_EXPRESSION"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.expression != "": 
			desc += "EXPRESSION=" + py_to_str(self.expression) + ","
		if self.output_field_name != "VALUE": 
			desc += "OUTPUT_FIELD_NAME=" + py_to_str(self.output_field_name) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class ModifyStateVarFromQuery(_graph_components.EpBase):
	"""
		

MODIFY_STATE_VAR_FROM_QUERY

Type: Other

Description: Modifies a state
variable by assigning it a value that was resulted from a query
evaluation. This event processor (EP) is transparent to the input time
series.

Python
class name:
ModifyStateVarFromQuery

Input: A time series of ticks

Output: A time series of ticks

Parameters:


  VARIABLE (string)
    The name of the state variable to be updated.
    

  
  OTQ_QUERY (expression)
    Specifies the query the results of which will be used to update
the state variable.
The expression should be in the &lt;otq_file_path&gt;::&lt;query_name&gt;
format. The query name part is needed if the specified OTQ file
contains multiple queries.
This parameter is reevaluated upon the arrival of each tick, generally
resulting in different queries to be executed for different input ticks.

  
  SYMBOL_NAME (expression)
    The symbol name, which is used to override the external symbol
list of the executed query. This parameter is reevaluated upon the
arrival of each tick.
Default: _SYMBOL_NAME

  
  START_TIMESTAMP (expression)
    The start time to override that of the executed query, in
milliseconds. This parameter is reevaluated upon the arrival of each
tick.
Default: _START_TIME

  
  END_TIMESTAMP (expression)
    The end time to override that of the executed query, in
milliseconds. This parameter is reevaluated upon the arrival of each
tick.
Default: _END_TIME

  
  OTQ_QUERY_PARAMS (expression)
    A string containing a comma-separated list of &lt;name&gt;=&lt;value&gt; pairs,
specifying OTQ parameters of the query to be executed. This parameter
is reevaluated upon the arrival of each tick.
Default: empty

  
  SYMBOL_PARAMS (expression)
    A string containing a comma-separated list of &lt;name&gt;=&lt;value&gt; pairs,
specifying symbol parameters of the security, which is used to override
an external symbol list of the executed query. This parameter is
reevaluated upon the arrival of each tick.
Default: empty

  
  ACTION (string)
    Specifies whether all ticks should be erased before the query
results are inserted into the tick set. Possible values are UPDATE and REPLACE.
For non-tick-sets, you can set the ACTION only to REPLACE; otherwise,
an error is thrown.
Default: REPLACE

  
  WHERE (expression)
    Specifies a condition for executing the given query and updating
the state variable.
Default: empty

  
  OUTPUT_FIELD_NAME (string)
    Specifies the output field name for state variables of
primitive types, in case if the query result contains multiple
fields.
Default: empty

  

Examples:

MODIFY_STATE_VAR_FROM_QUERY(STATE_VARIABLE="STATE::L1",OTQ_QUERY="\\"modifier_query.otq\\"",SYMBOL_NAME="\\"FULL_DEMO_L1::AA\\"")


	"""
	class Parameters:
		state_variable = "STATE_VARIABLE"
		otq_query = "OTQ_QUERY"
		symbol_name = "SYMBOL_NAME"
		start_timestamp = "START_TIMESTAMP"
		end_timestamp = "END_TIMESTAMP"
		otq_query_params = "OTQ_QUERY_PARAMS"
		symbol_params = "SYMBOL_PARAMS"
		action = "ACTION"
		where = "WHERE"
		output_field_name = "OUTPUT_FIELD_NAME"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["state_variable", "otq_query", "symbol_name", "start_timestamp", "end_timestamp", "otq_query_params", "symbol_params", "action", "where", "output_field_name"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["state_variable", "_default_state_variable", "otq_query", "_default_otq_query", "symbol_name", "_default_symbol_name", "start_timestamp", "_default_start_timestamp", "end_timestamp", "_default_end_timestamp", "otq_query_params", "_default_otq_query_params", "symbol_params", "_default_symbol_params", "action", "_default_action", "where", "_default_where", "output_field_name", "_default_output_field_name", "stack_info", "_used_strings"]

	class Action:
		REPLACE = "REPLACE"
		UPDATE = "UPDATE"

	def __init__(self, state_variable="", otq_query="", symbol_name="", start_timestamp="", end_timestamp="", otq_query_params="", symbol_params="", action=Action.REPLACE, where="", output_field_name="", Out = ""):
		_graph_components.EpBase.__init__(self, "MODIFY_STATE_VAR_FROM_QUERY")
		self._default_state_variable = ""
		self.state_variable = state_variable
		self._default_otq_query = ""
		self.otq_query = otq_query
		self._default_symbol_name = ""
		self.symbol_name = symbol_name
		self._default_start_timestamp = ""
		self.start_timestamp = start_timestamp
		self._default_end_timestamp = ""
		self.end_timestamp = end_timestamp
		self._default_otq_query_params = ""
		self.otq_query_params = otq_query_params
		self._default_symbol_params = ""
		self.symbol_params = symbol_params
		self._default_action = type(self).Action.REPLACE
		self.action = action
		self._default_where = ""
		self.where = where
		self._default_output_field_name = ""
		self.output_field_name = output_field_name
		if Out != "":
			self.output_field_name=Out
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_state_variable(self, value):
		self.state_variable = value
		return self

	def set_otq_query(self, value):
		self.otq_query = value
		return self

	def set_symbol_name(self, value):
		self.symbol_name = value
		return self

	def set_start_timestamp(self, value):
		self.start_timestamp = value
		return self

	def set_end_timestamp(self, value):
		self.end_timestamp = value
		return self

	def set_otq_query_params(self, value):
		self.otq_query_params = value
		return self

	def set_symbol_params(self, value):
		self.symbol_params = value
		return self

	def set_action(self, value):
		self.action = value
		return self

	def set_where(self, value):
		self.where = value
		return self

	def set_output_field_name(self, value):
		self.output_field_name = value
		return self

	@staticmethod
	def _get_name():
		return "MODIFY_STATE_VAR_FROM_QUERY"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.state_variable != "": 
			desc += "STATE_VARIABLE=" + py_to_str(self.state_variable) + ","
		if self.otq_query != "": 
			desc += "OTQ_QUERY=" + py_to_str(self.otq_query) + ","
		if self.symbol_name != "": 
			desc += "SYMBOL_NAME=" + py_to_str(self.symbol_name) + ","
		if self.start_timestamp != "": 
			desc += "START_TIMESTAMP=" + py_to_str(self.start_timestamp) + ","
		if self.end_timestamp != "": 
			desc += "END_TIMESTAMP=" + py_to_str(self.end_timestamp) + ","
		if self.otq_query_params != "": 
			desc += "OTQ_QUERY_PARAMS=" + py_to_str(self.otq_query_params) + ","
		if self.symbol_params != "": 
			desc += "SYMBOL_PARAMS=" + py_to_str(self.symbol_params) + ","
		if self.action != self.Action.REPLACE: 
			desc += "ACTION=" + py_to_str(self.action) + ","
		if self.where != "": 
			desc += "WHERE=" + py_to_str(self.where) + ","
		if self.output_field_name != "": 
			desc += "OUTPUT_FIELD_NAME=" + py_to_str(self.output_field_name) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class Passthrough(_graph_components.EpBase):
	"""
		

PASSTHROUGH

Type: Transformer

Description: Propagates some or all fields for ticks without
changing their values.

Python
class name:&nbsp;Passthrough

Input A time series of ticks.

Output: A time series of ticks, one for each input tick.

Parameters:


  FIELDS (string [, string])
    A comma-separated list of fields to be extracted from the input
ticks and propagated forward. If this value is not set, all fields are
propagated.
Default: empty

  
  GO_BACK_TO_FIRST_TICK (seconds)
    This parameter determines how far back to go looking for the
latest tick before start_time. If one is found, it is inserted into the
output time series with the timestamp set to start_time.
Default: 0

  
  MAX_BACK_TICKS_TO_PREPEND
(integer)
    When the GO_BACK_TO_FIRST_TICK
interval is specified, this parameter determines the maximum number of
the most recent ticks before start_time that will be prepended to the
output time series. None of those ticks can have the original timestamp
earlier than GO_BACK_TO_FIRST_TICK
seconds prior to the start time. These ticks will be inserted into the
output time series in the reverse order of their original timestamp.
Their timestamp will be changed to start_time.
Default: 1

  
  BACK_TICK_IF_NONE_AT_START_TIME
(Boolean)
    If true, the most recent tick
before start time, if found within GO_BACK_TO_FIRST_TICK
before start time, will be propagated only if the time series has no
ticks at exactly start time. MAX_BACK_TICKS_TO_PREPEND
must be set to 1, when this
parameter is set to true.
Default: false

  
  WHERE_CLAUSE_FOR_BACK_TICKS
(boolean logical expression)
    A logical expression (see LogicalExpressions.htm)
that is computed only for the ticks encountered when a query goes back
from the start time, in search of the ticks to prepend. If it returns false, a tick is ignored.
Default: empty

  
  USE_REGEX (Boolean)
    If true, then values in the FIELDS
parameter are treated as regular expressions. Notice that regular
expressions for field names are treated as if both their prefix and
their suffix are .*; that is, the
prefix and the suffix match any substring. As a result, the field name XX will match all of aXX, aXXB,
and XXb, when USE_REGEX=true. You can have a field name
begin from ^ to indicate that the .* prefix does not apply, and you can have
a field name end at $ to indicate
that the .* suffix does not apply.
Default: false

  
  DROP_FIELDS (Boolean)
    If set to true, the fields
listed in the FIELDS parameter will not be
propagated. However, the fields that are not listed in the FIELDS
parameter will be propagated.
Default: false

  
  CASE_INSENSITIVE (Boolean)
    Specifies whether matching of input tick fields and fields
listed in the FIELDS parameter is to be case-insensitive. If this
parameter is set to true, then
fields listed in the FIELDS
parameter, rather than those of the input tick matching them, will be
propagated as output tick fields. For performance reasons,
case-insensitive field matching must be avoided if the exact field
names are known, especially when PASSTHROUGH is an input node.
Default: false

  
  THROW_FOR_MISSING_FIELDS
(Boolean)
    If set to true and any field
specified in the FIELDS isn't present in the
input tick then an exception is thrown.
Default: true if DROP_FIELDS=false and false if DROP_FIELDS=true.

  

Example: Strips from the input stream of ticks all fields
other than PRICE and SIZE. Looks back up to 5 seconds
to find the first tick if one does not exist exactly at the query's
start_time.

PASSTHROUGH ("PRICE, SIZE", 5)

Strips from the input stream of ticks all fields other than the ones
that start with 'ASK_' prefix.

PASSTHROUGH ("^ASK_.*", 0 , 1 , true)

See the PASSTHROUGH example in TRANSFORMER_EXAMPLES.otq.


	"""
	class Parameters:
		fields = "FIELDS"
		go_back_to_first_tick = "GO_BACK_TO_FIRST_TICK"
		max_back_ticks_to_prepend = "MAX_BACK_TICKS_TO_PREPEND"
		where_clause_for_back_ticks = "WHERE_CLAUSE_FOR_BACK_TICKS"
		use_regex = "USE_REGEX"
		drop_fields = "DROP_FIELDS"
		case_insensitive = "CASE_INSENSITIVE"
		back_tick_if_none_at_start_time = "BACK_TICK_IF_NONE_AT_START_TIME"
		throw_for_missing_fields = "THROW_FOR_MISSING_FIELDS"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["fields", "go_back_to_first_tick", "max_back_ticks_to_prepend", "where_clause_for_back_ticks", "use_regex", "drop_fields", "case_insensitive", "back_tick_if_none_at_start_time", "throw_for_missing_fields"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["fields", "_default_fields", "go_back_to_first_tick", "_default_go_back_to_first_tick", "max_back_ticks_to_prepend", "_default_max_back_ticks_to_prepend", "where_clause_for_back_ticks", "_default_where_clause_for_back_ticks", "use_regex", "_default_use_regex", "drop_fields", "_default_drop_fields", "case_insensitive", "_default_case_insensitive", "back_tick_if_none_at_start_time", "_default_back_tick_if_none_at_start_time", "throw_for_missing_fields", "_default_throw_for_missing_fields", "stack_info", "_used_strings"]

	def __init__(self, fields="", go_back_to_first_tick=0, max_back_ticks_to_prepend=1, where_clause_for_back_ticks="", use_regex=False, drop_fields=False, case_insensitive=False, back_tick_if_none_at_start_time=False, throw_for_missing_fields=True):
		_graph_components.EpBase.__init__(self, "PASSTHROUGH")
		self._default_fields = ""
		self.fields = fields
		self._default_go_back_to_first_tick = 0
		self.go_back_to_first_tick = go_back_to_first_tick
		self._default_max_back_ticks_to_prepend = 1
		self.max_back_ticks_to_prepend = max_back_ticks_to_prepend
		self._default_where_clause_for_back_ticks = ""
		self.where_clause_for_back_ticks = where_clause_for_back_ticks
		self._default_use_regex = False
		self.use_regex = use_regex
		self._default_drop_fields = False
		self.drop_fields = drop_fields
		self._default_case_insensitive = False
		self.case_insensitive = case_insensitive
		self._default_back_tick_if_none_at_start_time = False
		self.back_tick_if_none_at_start_time = back_tick_if_none_at_start_time
		self._default_throw_for_missing_fields = True
		self.throw_for_missing_fields = throw_for_missing_fields
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_fields(self, value):
		self.fields = value
		return self

	def set_go_back_to_first_tick(self, value):
		self.go_back_to_first_tick = value
		return self

	def set_max_back_ticks_to_prepend(self, value):
		self.max_back_ticks_to_prepend = value
		return self

	def set_where_clause_for_back_ticks(self, value):
		self.where_clause_for_back_ticks = value
		return self

	def set_use_regex(self, value):
		self.use_regex = value
		return self

	def set_drop_fields(self, value):
		self.drop_fields = value
		return self

	def set_case_insensitive(self, value):
		self.case_insensitive = value
		return self

	def set_back_tick_if_none_at_start_time(self, value):
		self.back_tick_if_none_at_start_time = value
		return self

	def set_throw_for_missing_fields(self, value):
		self.throw_for_missing_fields = value
		return self

	@staticmethod
	def _get_name():
		return "PASSTHROUGH"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.fields != "": 
			desc += "FIELDS=" + py_to_str(self.fields) + ","
		if self.go_back_to_first_tick != 0: 
			desc += "GO_BACK_TO_FIRST_TICK=" + py_to_str(self.go_back_to_first_tick) + ","
		if self.max_back_ticks_to_prepend != 1: 
			desc += "MAX_BACK_TICKS_TO_PREPEND=" + py_to_str(self.max_back_ticks_to_prepend) + ","
		if self.where_clause_for_back_ticks != "": 
			desc += "WHERE_CLAUSE_FOR_BACK_TICKS=" + py_to_str(self.where_clause_for_back_ticks) + ","
		if self.use_regex != False: 
			desc += "USE_REGEX=" + py_to_str(self.use_regex) + ","
		if self.drop_fields != False: 
			desc += "DROP_FIELDS=" + py_to_str(self.drop_fields) + ","
		if self.case_insensitive != False: 
			desc += "CASE_INSENSITIVE=" + py_to_str(self.case_insensitive) + ","
		if self.back_tick_if_none_at_start_time != False: 
			desc += "BACK_TICK_IF_NONE_AT_START_TIME=" + py_to_str(self.back_tick_if_none_at_start_time) + ","
		if self.throw_for_missing_fields != True: 
			desc += "THROW_FOR_MISSING_FIELDS=" + py_to_str(self.throw_for_missing_fields) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class PrependInitialState(_graph_components.EpBase):
	"""
		

PREPEND_INITIAL_STATE

Type: Transformer

Description: Based on a specified set of fields (referred
below as state key fields), this EP outputs the latest tick before the
start time for each unique combination of values of those fields,
followed by the unchanged input time series. In other words, this EP
produces a stateful time series at the start time of the query,
followed by the time series itself.

While searching for all unique combinations of values of state key
fields, this EP inspects ticks from the start time back into the past,
until one of the following occurs:


  A group of ticks that represent the current state of a time
series (state ticks) are found. The EP stops at the last tick from this
group going backwards. State ticks are identified as follows:
    
      If the value of STATE_TICK_INDICATOR_FIELD_NAME
parameter is not empty, then the value of STATE_TICK_INDICATOR_FIELD_VALUE
parameter is used to identify state ticks.
      If a field in the input tick descriptor has a property with
name STATE_TICK_INDICATOR, then the value of that property will
serve as a marker for state ticks. That is, a tick is considered as a
state tick if and only if the above mentioned field in it has the value
of STATE_TICK_INDICATOR property. At most one field can have STATE_TICK_INDICATOR
property. If none has, but a field with name TICK_STATUS is
present in the input tick descriptor, then value 16 of it will
serve as a marker for state ticks.
    
    
      Which method, if both are applicable, is actually used is
controlled by PRIORITY_FOR_CONFLICTING_VALUES
parameter.
    
  
  A tick that indicated that the state is empty is found. Empty
state ticks are identified as follows:
    
      If the value of EMPTY_STATE_INDICATOR_FIELD_NAME
parameter is not empty, then the value of EMPTY_STATE_INDICATOR_FIELD_VALUE
parameter is used to identify empty state ticks.
      If a field in the input tick descriptor has a property with
name EMPTY_STATE_INDICATOR, then the value of that property
will serve as a marker for empty state ticks. That is, a tick is
considered as an empty state tick if and only if the above mentioned
field in it has the value of EMPTY_STATE_INDICATOR property. At
most one field can have EMPTY_STATE_INDICATOR property. If none
has, but a field with name TICK_STATUS is present in the input
tick descriptor, then value 31 of it will serve as a marker for
empty state ticks.
    
    
      Which method, if both are applicable, is actually used is
controlled by PRIORITY_FOR_CONFLICTING_VALUES
parameter.
    
  
  The EP goes back MAX_GO_BACK_SECONDS.
In this case, an exception is thrown because the initial state of the
time series was not found.

It is often undesirable to make ticks that represent the current
state of a time series visible. Besides making the analysis of a time
series more complex, visible state ticks can get out of sync with the
time series after some filter is applied. For this reason,
PREPEND_INITIAL_STATE EP, by default, relies on a special value (16) of the TICK_STATUS
field to recognize state ticks. When a filter is applied to a time
series, state ticks represented this way get automatically discarded.
This filtering effect would not apply when
STACK_TICK_INDICATOR_FIELD_NAME and STATE_TICK_INDICATOR_FIELD_VALUE
parameters are set to be different from their defaults or when
STATE_TICK_INDICATOR field property is set appropriately for a certain
field in the input tick descriptor.

Also, it is usually desirable to make a tick that signals an empty
state of a time series visible. However, visible ticks can be easily
filtered out, which can lead to loss of information about when a time
series was reset. For this reason, PREPEND_INITIAL_STATE EP, by
default, relies on a special value (31)
of the TICK_STATUS field to
recognize an empty state of a time series. In addition, a visible tick (TICK_STATUS=0) could follow with a field
set to a special value (for example, RECORD_TYPE='Z'),
to make the fact of an empty state apparent. An alternative field name
and value can be chosen, via EMPTY_STATE_INDICATOR_FIELD_NAME and
EMPTY_STATE_INDICATOR_FIELD_VALUE parameters or when
EMPTY_STATE_INDICATOR field property is set appropriately for a certain
field in the input tick descriptor.

Sometimes the boundary between state ticks and regular ticks in the
input time series is explicitly designated via another tick, called a
state end tick. In order not to consider such a tick as a regular tick,
PREPEND_INITIAL_STATE EP identifies it as follows:


  If the value of STATE_END_TICK_INDICATOR_FIELD_NAME
parameter is not empty, then the value of STATE_END_TICK_INDICATOR_FIELD_VALUE
parameter is used to identify state end ticks.
  If a field in the input tick descriptor has a property with name STATE_END_TICK_INDICATOR,
then the value of that property will serve as a marker for state end
ticks. That is, a tick is considered as a state end tick if and only if
the above mentioned field in it has the value of STATE_END_TICK_INDICATOR
property. At most one field can have STATE_END_TICK_INDICATOR
property. If none has, but a field with name TICK_STATUS is
present in the input tick descriptor, then value 28 of it will serve as
a marker for empty state ticks.
  Which method, if both are applicable, is actually used is
controlled by PRIORITY_FOR_CONFLICTING_VALUES
parameter.

A tick may indicate a deletion from the state for the respective
combination of the values of state key fields. Deletion tick are
identified as follows:



  If the value of DELETE_STATE_KEY_INDICATOR_FIELD_NAME
parameter is not empty, then the value ofDELETE_STATE_KEY_INDICATOR_FIELD_VALUE
parameter is used to identify state end ticks.
  If a field in the input tick descriptor has a property with name DELETE_STATE_KEY_INDICATOR,
then the value of that property will serve as a marker for deletion
ticks. That is, if the above mentioned field in a tick has the value of
    DELETE_STATE_KEY_INDICATOR property, then such a tick will
indicate a deletion from the mapping for the respective combination of
the values of state key fields. At most one field can have DELETE_STATE_KEY_INDICATOR
property.
  Which method, if both are applicable, is actually used is
controlled by PRIORITY_FOR_CONFLICTING_VALUES
parameter.

Sometimes a group of consecutive ticks
needs to be treated as an atomic group. To designate such an atomic
group, a pair of auxiliary ticks is used: atomic group begin tick and
atomic group end tick. As these ticks are not part of the state,
PREPEND_INITIAL_STATE EP needs to identify them. Atomic group begin
ticks are identified as follows:


  If the value of ATOMIC_GROUP_BEGIN_INDICATOR_FIELD_NAME
parameter is not empty, then the value of ATOMIC_GROUP_BEGIN_INDICATOR_FIELD_VALUE
parameter is used to identify atomic group begin ticks.
  If a field in the input tick descriptor has a property with name ATOMIC_GROUP_BEGIN_INDICATOR,
then the value of that property will serve as a marker for atomic group
begin ticks. That is, a tick is considered as an atomic group begin
tick if and only if the above mentioned field in it has the value of ATOMIC_GROUP_BEGIN_INDICATOR
property. At most one field can have ATOMIC_GROUP_BEGIN_INDICATOR
property. If none has, but a field with name TICK_STATUS is
present in the input tick descriptor, then value 32 of it will serve as
a marker for atomic group begin ticks.
  Which method, if both are applicable, is actually used is
controlled by PRIORITY_FOR_CONFLICTING_VALUES
parameter.

Atomic group end ticks are identified as follows:


  If the value of ATOMIC_GROUP_END_INDICATOR_FIELD_NAME
parameter is not empty, then the value of ATOMIC_GROUP_END_INDICATOR_FIELD_VALUE
parameter is used to identify atomic group end ticks.
  If a field in the input tick descriptor has a property with name ATOMIC_GROUP_END_INDICATOR,
then the value of that property will serve as a marker for atomic group
end ticks. That is, a tick is considered as an atomic group end tick if
and only if the above mentioned field in it has the value of ATOMIC_GROUP_END_INDICATOR
property. At most one field can have ATOMIC_GROUP_END_INDICATOR
property. If none has, but a field with name TICK_STATUS is
present in the input tick descriptor, then value 33 of it will serve as
a marker for atomic group end ticks.
  Which method, if both are applicable, is actually used is
controlled by PRIORITY_FOR_CONFLICTING_VALUES
parameter.

Order Book EPs process groups of ticks surrounded by hidden 'group
begin'/'group end' ticks atomically. To facilitate the correct handling
of the output of this EP in Order Book EPs, an option to create
begin/end ticks around the initial state is exposed via the ADD_STATE_BOUNDARY_TICKS
parameter.

Python
class name:
PrependInitialState

Input: A time series of ticks.

Output: A time series of ticks, starting from the ticks that
represent a state of the input time series at the start time and
followed by all ticks from the input time series.

Parameters:


  STATE_TICK_INDICATOR_FIELD_NAME
(string)
    A field that contains an indicator that a tick is a state tick
(that is, it belongs to a group of ticks that represent the current
state of a time series).
Default: TICK_STATUS

  
  STATE_TICK_INDICATOR_FIELD_VALUE
(string or number)
    The value of the field specified by the STATE_TICK_INDICATOR_FIELD_NAME
parameter that indicates that a tick is a state tick.
Default: 16

  
  EMPTY_STATE_INDICATOR_FIELD_NAME
(string)
    A field that contains an indicator that the state of a time
series is empty.
Default: TICK_STATUS

  
  EMPTY_STATE_INDICATOR_FIELD_VALUE
(string or number)
    The value of the field specified by parameter EMPTY_STATE_INDICATOR_FIELD_NAME
that indicates that the state of a time series is empty.
Default: 31

  
  MAX_GO_BACK_SECONDS (integer)
    The number of seconds to go back in search of the full state of
a time series, which could be an empty state.
Default: 86400

  
  STATE_KEY_FIELD_NAMES (string)
    A comma-separated list of state key fields (that is, fields for
which every unique combination of their values has associated state).
Default value: automatically discovered state key fields of input time
series, associated with its tick descriptor. If an input time series
includes the BUY_SELL_FLAG field,
this field is added to automatically discovered state key fields. This
is done for better integration with order book data, where the BUY_SELL_FLAG is an implicit state key
field. If this parameter has a non-empty value and the input tick
descriptor also has state keys defined, then which set is actually used
is controlled by the value of PRIORITY_FOR_CONFLICTING_VALUES
parameter.

  
  ADD_STATE_BOUNDARY_TICKS
(Boolean)
    If set to true, the initial
state of a time series is surrounded by an atomic group begin tick at
the start and an atomic group end tick at the end. The TICK_STATUS field must be present in the
input ticks, if no other method is used to designate atomic group begin/end ticks.
Default: false

  
  TREAT_MISSING_STATE_AS_EMPTY
(Boolean)
    If set to true, consider the
initial state of a time series empty if it was not found, even if some
ticks were seen when going MAX_GO_BACK_SECONDS
back from the start time of the query. If set to false, an exception will be thrown if an
initial state of a time series was not found while some ticks were seen
when going MAX_GO_BACK_SECONDS back
from the start time of the query
Default: false

  
  DELETE_STATE_KEY_INDICATOR_FIELD_NAME
(string)
    A field that contains an indicator that the state key of a given
tick is no longer a part of the state of a time series.

  
  DELETE_STATE_KEY_INDICATOR_FIELD_VALUE
(string or number)
    The value of the field specified by the DELETE_STATE_KEY_INDICATOR_FIELD_NAME
parameter that indicates that the state key of a given tick is no
longer a part of the state of a time series.

  
  STATE_END_TICK_INDICATOR_FIELD_NAME
(string)
    A field that contains an indicator that a tick is a state end
tick (that is, marks the end of a group of ticks that represent the
current state of a time series).

  
  STATE_END_TICK_INDICATOR_FIELD_VALUE
(string or number)
    The value of the field specified by the STATE_END_TICK_INDICATOR_FIELD_NAME
parameter that indicates that a tick is a state end tick.

  
  STATE_KEY_UPDATE_TIME_FIELD_NAME
(string)
If not empty, then specifies a field in the input tick descriptor,
which is expected to be either of msectime or of nsectime type. If the
value of that field in an input tick will be 0, then
PREPEND_INITIAL_STATE EP will replace its value by the timestamp of
that tick. If empty, but a field with name UPDATE_TIME or DELETED_TIME
is present (searched in this order in the input tick descriptor), then
the same applies to that field. This logic is used to keep timestamps
of ticks, encountered while going backwards to determine the state, as
these ticks are propagated using the start time of the query.
    Default: empty.
  ATOMIC_GROUP_BEGIN_INDICATOR_FIELD_NAME
(string)
    A field that contains an indicator that a tick is an atomic
group begin tick (that is, marks the start of an atomic group of ticks).

  
  ATOMIC_GROUP_BEGIN_INDICATOR_FIELD_VALUE
(string or number)
    The value of the field specified by the ATOMIC_GROUP_BEGIN_INDICATOR_FIELD_NAME
parameter that indicates that a tick is an atomic group begin tick.

  
  ATOMIC_GROUP_END_INDICATOR_FIELD_NAME
(string)
    A field that contains an indicator that a tick is an atomic
group end tick (that is, marks the end of an atomic group of ticks).

  
  ATOMIC_GROUP_END_INDICATOR_FIELD_VALUE
(string or number)
    The value of the field specified by the ATOMIC_GROUP_END_INDICATOR_FIELD_NAME
parameter that indicates that a tick is an atomic group end tick.

  
  PRIORITY_FOR_CONFLICTING_VALUES
(enumeration)
Used to give a priority to parameters of this EP (if set to EP) or to the input tick descriptor (if
set to TICK_DESCRIPTOR) when
choosing methods to identify state keys, empty state ticks, state
ticks, state end ticks and delete state key ticks.
Default: EP.
  

Examples: See the prepend_state
and snapshot_with_prepended_state
examples in prepend_initial_state.otq.


	"""
	class Parameters:
		state_tick_indicator_field_name = "STATE_TICK_INDICATOR_FIELD_NAME"
		state_tick_indicator_field_value = "STATE_TICK_INDICATOR_FIELD_VALUE"
		empty_state_indicator_field_name = "EMPTY_STATE_INDICATOR_FIELD_NAME"
		empty_state_indicator_field_value = "EMPTY_STATE_INDICATOR_FIELD_VALUE"
		max_go_back_seconds = "MAX_GO_BACK_SECONDS"
		state_key_field_names = "STATE_KEY_FIELD_NAMES"
		add_state_boundary_ticks = "ADD_STATE_BOUNDARY_TICKS"
		treat_missing_state_as_empty = "TREAT_MISSING_STATE_AS_EMPTY"
		delete_state_key_indicator_field_name = "DELETE_STATE_KEY_INDICATOR_FIELD_NAME"
		delete_state_key_indicator_field_value = "DELETE_STATE_KEY_INDICATOR_FIELD_VALUE"
		state_end_tick_indicator_field_name = "STATE_END_TICK_INDICATOR_FIELD_NAME"
		state_end_tick_indicator_field_value = "STATE_END_TICK_INDICATOR_FIELD_VALUE"
		state_key_update_time_field_name = "STATE_KEY_UPDATE_TIME_FIELD_NAME"
		priority_for_conflicting_values = "PRIORITY_FOR_CONFLICTING_VALUES"
		atomic_group_begin_indicator_field_name = "ATOMIC_GROUP_BEGIN_INDICATOR_FIELD_NAME"
		atomic_group_begin_indicator_field_value = "ATOMIC_GROUP_BEGIN_INDICATOR_FIELD_VALUE"
		atomic_group_end_indicator_field_name = "ATOMIC_GROUP_END_INDICATOR_FIELD_NAME"
		atomic_group_end_indicator_field_value = "ATOMIC_GROUP_END_INDICATOR_FIELD_VALUE"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["state_tick_indicator_field_name", "state_tick_indicator_field_value", "empty_state_indicator_field_name", "empty_state_indicator_field_value", "max_go_back_seconds", "state_key_field_names", "add_state_boundary_ticks", "treat_missing_state_as_empty", "delete_state_key_indicator_field_name", "delete_state_key_indicator_field_value", "state_end_tick_indicator_field_name", "state_end_tick_indicator_field_value", "state_key_update_time_field_name", "priority_for_conflicting_values", "atomic_group_begin_indicator_field_name", "atomic_group_begin_indicator_field_value", "atomic_group_end_indicator_field_name", "atomic_group_end_indicator_field_value"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["state_tick_indicator_field_name", "_default_state_tick_indicator_field_name", "state_tick_indicator_field_value", "_default_state_tick_indicator_field_value", "empty_state_indicator_field_name", "_default_empty_state_indicator_field_name", "empty_state_indicator_field_value", "_default_empty_state_indicator_field_value", "max_go_back_seconds", "_default_max_go_back_seconds", "state_key_field_names", "_default_state_key_field_names", "add_state_boundary_ticks", "_default_add_state_boundary_ticks", "treat_missing_state_as_empty", "_default_treat_missing_state_as_empty", "delete_state_key_indicator_field_name", "_default_delete_state_key_indicator_field_name", "delete_state_key_indicator_field_value", "_default_delete_state_key_indicator_field_value", "state_end_tick_indicator_field_name", "_default_state_end_tick_indicator_field_name", "state_end_tick_indicator_field_value", "_default_state_end_tick_indicator_field_value", "state_key_update_time_field_name", "_default_state_key_update_time_field_name", "priority_for_conflicting_values", "_default_priority_for_conflicting_values", "atomic_group_begin_indicator_field_name", "_default_atomic_group_begin_indicator_field_name", "atomic_group_begin_indicator_field_value", "_default_atomic_group_begin_indicator_field_value", "atomic_group_end_indicator_field_name", "_default_atomic_group_end_indicator_field_name", "atomic_group_end_indicator_field_value", "_default_atomic_group_end_indicator_field_value", "stack_info", "_used_strings"]

	class PriorityForConflictingValues:
		EP = "EP"
		TICK_DESCRIPTOR = "TICK_DESCRIPTOR"

	def __init__(self, state_tick_indicator_field_name="TICK_STATUS", state_tick_indicator_field_value=16, empty_state_indicator_field_name="TICK_STATUS", empty_state_indicator_field_value=31, max_go_back_seconds=86400, state_key_field_names="", add_state_boundary_ticks=False, treat_missing_state_as_empty=False, delete_state_key_indicator_field_name="", delete_state_key_indicator_field_value="", state_end_tick_indicator_field_name="", state_end_tick_indicator_field_value="", state_key_update_time_field_name="", priority_for_conflicting_values=PriorityForConflictingValues.EP, atomic_group_begin_indicator_field_name="", atomic_group_begin_indicator_field_value="", atomic_group_end_indicator_field_name="", atomic_group_end_indicator_field_value=""):
		_graph_components.EpBase.__init__(self, "PREPEND_INITIAL_STATE")
		self._default_state_tick_indicator_field_name = "TICK_STATUS"
		self.state_tick_indicator_field_name = state_tick_indicator_field_name
		self._default_state_tick_indicator_field_value = 16
		self.state_tick_indicator_field_value = state_tick_indicator_field_value
		self._default_empty_state_indicator_field_name = "TICK_STATUS"
		self.empty_state_indicator_field_name = empty_state_indicator_field_name
		self._default_empty_state_indicator_field_value = 31
		self.empty_state_indicator_field_value = empty_state_indicator_field_value
		self._default_max_go_back_seconds = 86400
		self.max_go_back_seconds = max_go_back_seconds
		self._default_state_key_field_names = ""
		self.state_key_field_names = state_key_field_names
		self._default_add_state_boundary_ticks = False
		self.add_state_boundary_ticks = add_state_boundary_ticks
		self._default_treat_missing_state_as_empty = False
		self.treat_missing_state_as_empty = treat_missing_state_as_empty
		self._default_delete_state_key_indicator_field_name = ""
		self.delete_state_key_indicator_field_name = delete_state_key_indicator_field_name
		self._default_delete_state_key_indicator_field_value = ""
		self.delete_state_key_indicator_field_value = delete_state_key_indicator_field_value
		self._default_state_end_tick_indicator_field_name = ""
		self.state_end_tick_indicator_field_name = state_end_tick_indicator_field_name
		self._default_state_end_tick_indicator_field_value = ""
		self.state_end_tick_indicator_field_value = state_end_tick_indicator_field_value
		self._default_state_key_update_time_field_name = ""
		self.state_key_update_time_field_name = state_key_update_time_field_name
		self._default_priority_for_conflicting_values = type(self).PriorityForConflictingValues.EP
		self.priority_for_conflicting_values = priority_for_conflicting_values
		self._default_atomic_group_begin_indicator_field_name = ""
		self.atomic_group_begin_indicator_field_name = atomic_group_begin_indicator_field_name
		self._default_atomic_group_begin_indicator_field_value = ""
		self.atomic_group_begin_indicator_field_value = atomic_group_begin_indicator_field_value
		self._default_atomic_group_end_indicator_field_name = ""
		self.atomic_group_end_indicator_field_name = atomic_group_end_indicator_field_name
		self._default_atomic_group_end_indicator_field_value = ""
		self.atomic_group_end_indicator_field_value = atomic_group_end_indicator_field_value
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_state_tick_indicator_field_name(self, value):
		self.state_tick_indicator_field_name = value
		return self

	def set_state_tick_indicator_field_value(self, value):
		self.state_tick_indicator_field_value = value
		return self

	def set_empty_state_indicator_field_name(self, value):
		self.empty_state_indicator_field_name = value
		return self

	def set_empty_state_indicator_field_value(self, value):
		self.empty_state_indicator_field_value = value
		return self

	def set_max_go_back_seconds(self, value):
		self.max_go_back_seconds = value
		return self

	def set_state_key_field_names(self, value):
		self.state_key_field_names = value
		return self

	def set_add_state_boundary_ticks(self, value):
		self.add_state_boundary_ticks = value
		return self

	def set_treat_missing_state_as_empty(self, value):
		self.treat_missing_state_as_empty = value
		return self

	def set_delete_state_key_indicator_field_name(self, value):
		self.delete_state_key_indicator_field_name = value
		return self

	def set_delete_state_key_indicator_field_value(self, value):
		self.delete_state_key_indicator_field_value = value
		return self

	def set_state_end_tick_indicator_field_name(self, value):
		self.state_end_tick_indicator_field_name = value
		return self

	def set_state_end_tick_indicator_field_value(self, value):
		self.state_end_tick_indicator_field_value = value
		return self

	def set_state_key_update_time_field_name(self, value):
		self.state_key_update_time_field_name = value
		return self

	def set_priority_for_conflicting_values(self, value):
		self.priority_for_conflicting_values = value
		return self

	def set_atomic_group_begin_indicator_field_name(self, value):
		self.atomic_group_begin_indicator_field_name = value
		return self

	def set_atomic_group_begin_indicator_field_value(self, value):
		self.atomic_group_begin_indicator_field_value = value
		return self

	def set_atomic_group_end_indicator_field_name(self, value):
		self.atomic_group_end_indicator_field_name = value
		return self

	def set_atomic_group_end_indicator_field_value(self, value):
		self.atomic_group_end_indicator_field_value = value
		return self

	@staticmethod
	def _get_name():
		return "PREPEND_INITIAL_STATE"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.state_tick_indicator_field_name != "TICK_STATUS": 
			desc += "STATE_TICK_INDICATOR_FIELD_NAME=" + py_to_str(self.state_tick_indicator_field_name) + ","
		if self.state_tick_indicator_field_value != 16: 
			desc += "STATE_TICK_INDICATOR_FIELD_VALUE=" + py_to_str(self.state_tick_indicator_field_value) + ","
		if self.empty_state_indicator_field_name != "TICK_STATUS": 
			desc += "EMPTY_STATE_INDICATOR_FIELD_NAME=" + py_to_str(self.empty_state_indicator_field_name) + ","
		if self.empty_state_indicator_field_value != 31: 
			desc += "EMPTY_STATE_INDICATOR_FIELD_VALUE=" + py_to_str(self.empty_state_indicator_field_value) + ","
		if self.max_go_back_seconds != 86400: 
			desc += "MAX_GO_BACK_SECONDS=" + py_to_str(self.max_go_back_seconds) + ","
		if self.state_key_field_names != "": 
			desc += "STATE_KEY_FIELD_NAMES=" + py_to_str(self.state_key_field_names) + ","
		if self.add_state_boundary_ticks != False: 
			desc += "ADD_STATE_BOUNDARY_TICKS=" + py_to_str(self.add_state_boundary_ticks) + ","
		if self.treat_missing_state_as_empty != False: 
			desc += "TREAT_MISSING_STATE_AS_EMPTY=" + py_to_str(self.treat_missing_state_as_empty) + ","
		if self.delete_state_key_indicator_field_name != "": 
			desc += "DELETE_STATE_KEY_INDICATOR_FIELD_NAME=" + py_to_str(self.delete_state_key_indicator_field_name) + ","
		if self.delete_state_key_indicator_field_value != "": 
			desc += "DELETE_STATE_KEY_INDICATOR_FIELD_VALUE=" + py_to_str(self.delete_state_key_indicator_field_value) + ","
		if self.state_end_tick_indicator_field_name != "": 
			desc += "STATE_END_TICK_INDICATOR_FIELD_NAME=" + py_to_str(self.state_end_tick_indicator_field_name) + ","
		if self.state_end_tick_indicator_field_value != "": 
			desc += "STATE_END_TICK_INDICATOR_FIELD_VALUE=" + py_to_str(self.state_end_tick_indicator_field_value) + ","
		if self.state_key_update_time_field_name != "": 
			desc += "STATE_KEY_UPDATE_TIME_FIELD_NAME=" + py_to_str(self.state_key_update_time_field_name) + ","
		if self.priority_for_conflicting_values != self.PriorityForConflictingValues.EP: 
			desc += "PRIORITY_FOR_CONFLICTING_VALUES=" + py_to_str(self.priority_for_conflicting_values) + ","
		if self.atomic_group_begin_indicator_field_name != "": 
			desc += "ATOMIC_GROUP_BEGIN_INDICATOR_FIELD_NAME=" + py_to_str(self.atomic_group_begin_indicator_field_name) + ","
		if self.atomic_group_begin_indicator_field_value != "": 
			desc += "ATOMIC_GROUP_BEGIN_INDICATOR_FIELD_VALUE=" + py_to_str(self.atomic_group_begin_indicator_field_value) + ","
		if self.atomic_group_end_indicator_field_name != "": 
			desc += "ATOMIC_GROUP_END_INDICATOR_FIELD_NAME=" + py_to_str(self.atomic_group_end_indicator_field_name) + ","
		if self.atomic_group_end_indicator_field_value != "": 
			desc += "ATOMIC_GROUP_END_INDICATOR_FIELD_VALUE=" + py_to_str(self.atomic_group_end_indicator_field_value) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class Table(_graph_components.EpBase):
	"""
		

TABLE

Type: Transformer

Description: Propagates specified tick fields without
changing their values. If the specified fields are not present in the
tick descriptor (due to a schema change or a JOIN), the values for
those fields are set to NaN for doubles/decimals, empty string for
strings, or to the default value (if specified).&nbsp;

Python
class name:&nbsp;Table

Input: A time series of ticks.

Output: A time series of ticks.

Parameters:


  FIELDS (field_name [type specification]
[(default value)] [,field_name [type specification] [(default
value)]] )
    A comma-separated list of fields to be extracted from input
ticks and propagated forward.
When a tick in the input stream does not contain one or more fields
specified in the list then the field will be created in the output
stream. A field is assumed to be double, unless its declaration
includes the type specification after the field name.
If no default value is specified for a missing field in the list, then double/decimal
fields will be initialized to NaN, integer
fields (of all sizes) and timestamp fields will be initialized
to 0, and string fields will
be initialized to an empty string.
Supported field types are listed here.
The order of fields in the output schema will match the order of fields
in this EP parameter if the type specification is provided for each
fiield. For a field without type specification, if that field is
present at the input of the TABLE EP, the type of that field at the
output of the TABLE EP&nbsp; will be the same as it is at the input.

  
  USE_REGEX (Boolean)
    If true, then field_names in
the FIELDS
parameter are treated as regular expressions. Notice that regular
expressions for field names are treated as if both their prefix and
their suffix are .*; that is, the
prefix and the suffix match any substring. As a result, the field name XX will match all of aXX, aXXB,
and XXb, when USE_REGEX=true. You can have a field name
begin from ^ to indicate that the .* prefix does not apply, and you can have
a field name end at $ to indicate
that the .* suffix does not apply.
Default: false

  
  KEEP_INPUT_FIELDS (Boolean)
    If set to true, all fields present in an input tick will be
present in the output tick.
Default: false

  
  KEEP_INITIAL_SCHEMA (Boolean)
    If set to true, the output schema will be computed once, upon
receipt of the first input tick descriptor, and will stay the same,
despite any subsequent changes in the input tick descriptor.&nbsp;
Default: false

  
  ACCUMULATE_INPUT_FIELDS
(Boolean)
    If set to true, any fields that were present on any tick in the
input time series will be present in the ticks of the output time
series. New fields will be added to the output tick at the point they
are first seen in the input time series. If any field already present
in the input is not present on a given input tick and is not listed in
the FIELDS parameter, then the type of this field will be determined
from the type of their latest values. This parameter must not be set to
    true if KEEP_INITIAL_SCHEMA is set to true.
Default: false

  

Examples:

FIELDS = PRICE, SIZE, COND stringFIELDS = PRICE, SIZE, COUNT int (0), COND string ("1")FIELDS = PRICE, SIZE, int COUNT (0), string[2] COND ("12")
See the TABLE_EXAMPLE example in TRANSFORMER_EXAMPLES.otq.


	"""
	class Parameters:
		fields = "FIELDS"
		use_regex = "USE_REGEX"
		keep_input_fields = "KEEP_INPUT_FIELDS"
		accumulate_input_fields = "ACCUMULATE_INPUT_FIELDS"
		keep_initial_schema = "KEEP_INITIAL_SCHEMA"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["fields", "use_regex", "keep_input_fields", "accumulate_input_fields", "keep_initial_schema"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["fields", "_default_fields", "use_regex", "_default_use_regex", "keep_input_fields", "_default_keep_input_fields", "accumulate_input_fields", "_default_accumulate_input_fields", "keep_initial_schema", "_default_keep_initial_schema", "stack_info", "_used_strings"]

	def __init__(self, fields="", use_regex=False, keep_input_fields=False, accumulate_input_fields=False, keep_initial_schema=False):
		_graph_components.EpBase.__init__(self, "TABLE")
		self._default_fields = ""
		self.fields = fields
		self._default_use_regex = False
		self.use_regex = use_regex
		self._default_keep_input_fields = False
		self.keep_input_fields = keep_input_fields
		self._default_accumulate_input_fields = False
		self.accumulate_input_fields = accumulate_input_fields
		self._default_keep_initial_schema = False
		self.keep_initial_schema = keep_initial_schema
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_fields(self, value):
		self.fields = value
		return self

	def set_use_regex(self, value):
		self.use_regex = value
		return self

	def set_keep_input_fields(self, value):
		self.keep_input_fields = value
		return self

	def set_accumulate_input_fields(self, value):
		self.accumulate_input_fields = value
		return self

	def set_keep_initial_schema(self, value):
		self.keep_initial_schema = value
		return self

	@staticmethod
	def _get_name():
		return "TABLE"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.fields != "": 
			desc += "FIELDS=" + py_to_str(self.fields) + ","
		if self.use_regex != False: 
			desc += "USE_REGEX=" + py_to_str(self.use_regex) + ","
		if self.keep_input_fields != False: 
			desc += "KEEP_INPUT_FIELDS=" + py_to_str(self.keep_input_fields) + ","
		if self.accumulate_input_fields != False: 
			desc += "ACCUMULATE_INPUT_FIELDS=" + py_to_str(self.accumulate_input_fields) + ","
		if self.keep_initial_schema != False: 
			desc += "KEEP_INITIAL_SCHEMA=" + py_to_str(self.keep_initial_schema) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class CreateTickBlocks(_graph_components.EpBase):
	"""
		

CREATE_TICK_BLOCKS

Type: Other

Description:&nbsp;Creates tick blocks (where each field is a
vector of values) from stream of ticks (see TickBlocks.htm
for the description of tick blocks and their processing). Each tick
block consists of up
to PERF.MAX_TICK_BLOCK_SIZE
rows. An exception is thrown if CREATE_TICK_BLOCKS EP is used in
the query that includes event processors incompatible with tick
blocks,&nbsp;DECLARE_STATE_VARIABLES
EP and order book aggregation EPs. An exception is also thrown if
CREATE_TICK_BLOCKS EP is used in a CEP query. This EP will simply
propagate its input if its input is tick blocks. Creation of tick
blocks from a stream of ticks is relatively slow, but if some EPs on
the putput of CREATE_TICK_BLOCKS have built-in support of tick blocks,
those EPs can produce enough time savings to justify use of
CREATE_TICK_BLOCKS EP. Nonetheless, this EP is mostly used for testing
purposes (because it makes it easy to test how other EPs handle tick
blocks). &nbsp;

Python
class name:&nbsp;CreateTickBlocks

Input: Time series of ticks

Output: A time series of ticks, propagated in the form of
per-field vectors of&nbsp;values.

See CREATE_TICK_BLOCKS.otq.


	"""
	class Parameters:
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = []
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["stack_info", "_used_strings"]

	def __init__(self):
		_graph_components.EpBase.__init__(self, "CREATE_TICK_BLOCKS")
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	@staticmethod
	def _get_name():
		return "CREATE_TICK_BLOCKS"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class RenameFieldsEp(_graph_components.EpBase):
	"""
		

RENAME_FIELDS

Type: Transformer

Description: Renames one or more fields for each tick in the
input stream. If a field whose name is to be changed does not exist, it
is ignored.

Python
class name:&nbsp;RenameFieldsEp

Input: A time series of ticks.

Output: A time series of ticks.

Parameters:


  RENAME_FIELDS (string)
    Specifies a comma-separated list of old-name=new-name pairs.

  
  USE_REGEX (Boolean)
    If true, then old-name=new-name pairs in the RENAME_FIELDS
parameter are treated as regular expressions. This allows bulk renaming
for field names. Notice that regular expressions for old names are
treated as if both their prefix and their suffix are .*, i.e. the
prefix and suffix match any substring. As a result, old-name XX will
match all of aXX, aXXB, and XXb, when USE_REGEX=true. You can have
old-name begin from ^ to indicate that .* prefix does not apply, and
you can have old name end at $ to indicate that .* suffix does not
apply.
Default: false

  
  FIELDS_TO_SKIP (string)
    A comma-separated list of regular expressions for specifying
fields that should be skipped (i.e., not be renamed). If a field is
matched by one of the specified regular expressions, it won't be
considered for renaming.
Default: empty

  

Examples:

RENAME_FIELD (RENAME_FIELDS="COND=CONDITION")

The following RENAME_FIELDS EP removes "left." and "right."
prefixes for all fields, beside the ones with the ".TIMESTAMP" suffix.

RENAME_FIELDS(RENAME_FIELDS="left\\.(.*)=\\1,right\\.(.*)=\\1",USE_REGEX=true,FIELDS_TO_SKIP=".*TIMESTAMP")

The following RENAME_FIELDS EP removes changes all field
names to upper case.

RENAME_FIELDS(RENAME_FIELDS="(.*)=\\u1",USE_REGEX=true)

See the RENAME_FIELDS example in TRANSFORMER_EXAMPLES.otq.


	"""
	class Parameters:
		rename_fields = "RENAME_FIELDS"
		use_regex = "USE_REGEX"
		fields_to_skip = "FIELDS_TO_SKIP"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["rename_fields", "use_regex", "fields_to_skip"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["rename_fields", "_default_rename_fields", "use_regex", "_default_use_regex", "fields_to_skip", "_default_fields_to_skip", "stack_info", "_used_strings"]

	def __init__(self, rename_fields="", use_regex=False, fields_to_skip=""):
		_graph_components.EpBase.__init__(self, "RENAME_FIELDS")
		self._default_rename_fields = ""
		self.rename_fields = rename_fields
		self._default_use_regex = False
		self.use_regex = use_regex
		self._default_fields_to_skip = ""
		self.fields_to_skip = fields_to_skip
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_rename_fields(self, value):
		self.rename_fields = value
		return self

	def set_use_regex(self, value):
		self.use_regex = value
		return self

	def set_fields_to_skip(self, value):
		self.fields_to_skip = value
		return self

	@staticmethod
	def _get_name():
		return "RENAME_FIELDS"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.rename_fields != "": 
			desc += "RENAME_FIELDS=" + py_to_str(self.rename_fields) + ","
		if self.use_regex != False: 
			desc += "USE_REGEX=" + py_to_str(self.use_regex) + ","
		if self.fields_to_skip != "": 
			desc += "FIELDS_TO_SKIP=" + py_to_str(self.fields_to_skip) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


RenameFields = RenameFieldsEp


class Replicate(_graph_components.EpBase):
	"""
		

REPLICATE

Type: Other

Description: Creates a replica of all input branches for each
symbol bound to this node. Inputs of this node are input streams for
each bound symbol. The output of this node is the same as its input.
REPLICATE should be used in graphs when the sources of an event
processor (EP) need to be replicated for different sets of symbols. In
such a case, REPLICATE should be made a source of that EP.

REPLICATE is used when the user wants to replicate many streams that
enter merge (or PRESORT) but some securities need to use a different
branch. REPLICATE simply replicates its input branches for all symbols
bound to it and propagates those branches to MERGE (PRESORT) as if
those were directly connected.

Python
class name:&nbsp;Replicate

Input: One or more time series for each bound symbol.

Output: Unchanged series of input ticks.


	"""
	class Parameters:
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = []
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["stack_info", "_used_strings"]

	def __init__(self):
		_graph_components.EpBase.__init__(self, "REPLICATE")
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	@staticmethod
	def _get_name():
		return "REPLICATE"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class ModifySymbolName(_graph_components.EpBase):
	"""
		

MODIFY_SYMBOL_NAME

Type: Other

Description: Modifies the name of the symbol that provides
input ticks for this node. The value of the new name must be a string.
This event processor (EP) also modifies the value of _SYMBOL_NAME by
setting it to compute the new symbol name. The modified value of
_SYMBOL_NAME applies to all EPs that are the sources of this EP. It
does not apply to its outputs (sinks).

Python
class name:&nbsp;ModifySymbolName

Input: A single series of ticks.

Output: An unchanged series of input ticks.

Parameters:


  SYMBOL_NAME (expression)
    A logical expression.&nbsp;

    
    Note: The resulting symbol cannot override the DB name
portion of the symbol. Overriding the DB name can only be done by
specifying
    &lt;DB name&gt;::&lt;Tick type&gt;
in the "Tick type" field of an EP.

  

Examples: A modified symbol name to end with an .N suffix:

MODIFY_SYMBOL_NAME(SYMBOL_NAME=_SYMBOL_NAME+'.N')


	"""
	class Parameters:
		symbol_name = "SYMBOL_NAME"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["symbol_name"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["symbol_name", "_default_symbol_name", "stack_info", "_used_strings"]

	def __init__(self, symbol_name=""):
		_graph_components.EpBase.__init__(self, "MODIFY_SYMBOL_NAME")
		self._default_symbol_name = ""
		self.symbol_name = symbol_name
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_symbol_name(self, value):
		self.symbol_name = value
		return self

	@staticmethod
	def _get_name():
		return "MODIFY_SYMBOL_NAME"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.symbol_name != "": 
			desc += "SYMBOL_NAME=" + py_to_str(self.symbol_name) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class MergeNonboundSymbols(_graph_components.EpBase):
	"""
		

MERGE_NONBOUND_SYMBOLS

Type: Union

Description: Merges ticks from multiple input streams into a
single output stream.

This EP is now considered obsolete, since dynamic symbol discovery
is now supported for bound symbol lists.&nbsp;MERGE_NONBOUND_SYMBOLS EP
came into existence mainly due to the lack of support, at that time,
for&nbsp;dynamic symbol discovery for bound symbols.

Unlike with MERGE EP, MERGE_NONBOUND_SYMBOLS EP merges non-bound
symbols. The primary objective of such behavior is to allow merged
symbols to be processed in different CEP threads before they reach this
EP when then serializes all its input messages into a single stream of
output ticks. One additional benefit of this behavior is that symbols
merged by this EP can be discovered dynamically, as CEP query is
proceeding.&nbsp;

Also, unlike MERGE EP, currently this event processor does not add
any fields to input ticks. This is to avoid any performance penalty
when merging multiple streams in the CEP query. If needed, ADD_FIELD(s)
EP or MERGE EP without any symbols bound to it could be used on inputs
of MERGE_NONBOUND_SYMBOLS EP to add fields such as SYMBOL_NAME,
TICK_TYPE, and DB_NAME.

A disadvantage of MERGE_NONBOUND_SYMBOLS EP is that it changes the
primary timestamp of the input ticks to the current time. This is to
preserve chronological order of ticks it produces without adding any
latency.&nbsp;

Notice that this EP should not be used at all in the historical
queries, since
this EP expects that ticks from different symbols arrive from any given
thread in a strictly chronological sequence, and across threads in
nearly chronological sequence (such that this EP would reorder ticks in
a not significant way). When issuing queries in historical mode, please
use MERGE EP in place of MERGE_NONBOUND_SYMBOLS EP, using non-bound
symbol list as one bound to MERGE.

Python
class name:&nbsp;MergeNonboundSymbols

Input: Multiple time series of ticks.

Output: A single, merged time series of ticks.

Parameters:

Currently, all input ticks must have the same schema.


	"""
	class Parameters:
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = []
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["stack_info", "_used_strings"]

	def __init__(self):
		_graph_components.EpBase.__init__(self, "MERGE_NONBOUND_SYMBOLS")
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	@staticmethod
	def _get_name():
		return "MERGE_NONBOUND_SYMBOLS"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class DbRenameField(_graph_components.EpBase):
	"""
		

DB/RENAME_FIELD

Type: Transformer

Description: This event processor facilitates field name
changes in OneTick archives. It must be the first (source) event
processor on the graph. It changes all archives that cover some of the
range between the start time and the end time of the query.

Input symbols for the queries that involve this event processor
should be of the form &lt;db_name&gt;::
or &lt;db_name&gt;::&lt;some symbol name&gt;.

Note: Currently, this event processor does not change field
names in accelerator and real-time databases.

Python
class name:
DbRenameField

Input: None

Output: One tick with the NUM_AFFECTED_ARCHIVES
field that represents the number of archives where the name of the
field was changed.

Parameters:


  OLD_FIELD_NAME (string)
    Specifies the name of the field in the archive that should be
changed.

  
  NEW_FIELD_NAME (string)
    Specifies the new name of the field.

  

Examples: Rename field PRICE2 to PRICE for tick type TRD:

DB/RENAME_FIELD (TRD,PRICE2,PRICE)


	"""
	class Parameters:
		old_field_name = "OLD_FIELD_NAME"
		new_field_name = "NEW_FIELD_NAME"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["old_field_name", "new_field_name"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["old_field_name", "_default_old_field_name", "new_field_name", "_default_new_field_name", "stack_info", "_used_strings"]

	def __init__(self, old_field_name="", new_field_name=""):
		_graph_components.EpBase.__init__(self, "DB/RENAME_FIELD")
		self._default_old_field_name = ""
		self.old_field_name = old_field_name
		self._default_new_field_name = ""
		self.new_field_name = new_field_name
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_old_field_name(self, value):
		self.old_field_name = value
		return self

	def set_new_field_name(self, value):
		self.new_field_name = value
		return self

	@staticmethod
	def _get_name():
		return "DB/RENAME_FIELD"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.old_field_name != "": 
			desc += "OLD_FIELD_NAME=" + py_to_str(self.old_field_name) + ","
		if self.new_field_name != "": 
			desc += "NEW_FIELD_NAME=" + py_to_str(self.new_field_name) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class SynchronizeTime(_graph_components.EpBase):
	"""
		

SYNCHRONIZE_TIME

Type: Sort

Description: Synchronizes tick propagation across different
branches in graph.

Python
class name:
SynchronizeTime

Input: Time series of ticks.

Output: Time series of ticks, sorted in time order across all
SYNCHRONIZE_TIME EPs with equal value of SYNC_POINT_ID parameter.

Parameters:


  SYNC_POINT_ID(integer)
    Specifies number for identifying synchronization group.
Destination nodes of SYNCHRONIZE_TIME EPs will receive ticks
that are sorted by timestamps across their synchronization group.

    SYNCHRONIZE_TIME EP must have exactly one source and destination
node (it can't be root or leaf node in the graph). SYNCHRONIZE_TIME EPs
with the same SYNC_POINT_ID can't be in sources and destinations of the
same EP (otherwise deadlock condition may occur).

  

See the SYNCHRONIZE_TIME__EXAMPLE
example in SORT_EXAMPLES.otq.


	"""
	class Parameters:
		sync_point_id = "SYNC_POINT_ID"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["sync_point_id"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["sync_point_id", "_default_sync_point_id", "stack_info", "_used_strings"]

	def __init__(self, sync_point_id=""):
		_graph_components.EpBase.__init__(self, "SYNCHRONIZE_TIME")
		self._default_sync_point_id = ""
		self.sync_point_id = sync_point_id
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_sync_point_id(self, value):
		self.sync_point_id = value
		return self

	@staticmethod
	def _get_name():
		return "SYNCHRONIZE_TIME"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.sync_point_id != "": 
			desc += "SYNC_POINT_ID=" + py_to_str(self.sync_point_id) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class Ml_dlibBinaryClassifyTrainRvmWithRadialBasisKernel(_graph_components.EpBase):
	"""
		

ML::DLIB_BINARY_CLASSIFY_TRAIN_RVM_WITH_RADIAL_BASIS_KERNEL

Type: Other

Description: This train
classification EP uses DLIB library implementation of RVM
(Relevance Vector Machine) for solving the binary classification
problem.

The RVM trainer is configured to use radial basis kernel.
Like other binary
classification EPs, it is required that the input data be labeled
as positive and negative with +1 and
-1 values, respectively.

In order to enable the event processor, please add LOAD_ML_DLIB_UDEPS=true to your
ONE_TICK_CONFIG file. Please check compatibility to see if
the EP is supported in your distribution.

Python
class name:
Ml_dlibBinaryClassifyTrainRvmWithRadialBasisKernel

Input: A time series of ticks containing a filed representing
the actual (expected) classification label (-1
or +1).

Output:The set of binary
performance metrics computed over the cross validation set selected
from the input data.

Parameters: The EP supports following set of parameters, the
description of the common parameters can be found here:


  FEATURES_FIELD_NAMES
(string)
  LABEL_FIELD_NAME
(string)
  INSTANCE_NAME
(string)
  ENABLE_PROBABILITY_ESTIMATES
(boolean)
  PROBABILITY_THRESHOLD_GRANULARITY
(double)
  OPTIMIZATION_OBJECTIVE
(string)
  TRAINING_SAMPLE_RATE
(int)
  RBF_GAMMA_VALUE (double) - The Gamma
value for Radial Basis kernel. Cannot be used when optimization
objective is specified. Intuitively, the gamma parameter defines how
far the influence of a single training example reaches, with low values
meaning 'far' and high values meaning 'close'.
  EPSILION (double) - one approach to reduce
the RVM training time it using a bigger stopping epsilon. However, this
might make the outputs less reliable. But sometimes it works out well.
Default: 0.001
  MAX_ITERATIONS (int) - this option can
be used to put an explicit limit on the number of iterations used by
the numeric solver. Default: 2000

Examples: See the rvm
example in ml_dlib_classify.otq.


	"""
	class Parameters:
		features_field_names = "FEATURES_FIELD_NAMES"
		label_field_name = "LABEL_FIELD_NAME"
		instance_name = "INSTANCE_NAME"
		enable_probability_estimates = "ENABLE_PROBABILITY_ESTIMATES"
		probability_threshold_granularity = "PROBABILITY_THRESHOLD_GRANULARITY"
		optimization_objective = "OPTIMIZATION_OBJECTIVE"
		rbf_gamma_value = "RBF_GAMMA_VALUE"
		epsilion = "EPSILION"
		max_iterations = "MAX_ITERATIONS"
		training_sample_rate = "TRAINING_SAMPLE_RATE"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["features_field_names", "label_field_name", "instance_name", "enable_probability_estimates", "probability_threshold_granularity", "optimization_objective", "rbf_gamma_value", "epsilion", "max_iterations", "training_sample_rate"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["features_field_names", "_default_features_field_names", "label_field_name", "_default_label_field_name", "instance_name", "_default_instance_name", "enable_probability_estimates", "_default_enable_probability_estimates", "probability_threshold_granularity", "_default_probability_threshold_granularity", "optimization_objective", "_default_optimization_objective", "rbf_gamma_value", "_default_rbf_gamma_value", "epsilion", "_default_epsilion", "max_iterations", "_default_max_iterations", "training_sample_rate", "_default_training_sample_rate", "stack_info", "_used_strings"]

	class OptimizationObjective:
		EMPTY = ""
		ACC = "ACC"
		AUC = "AUC"
		F_SCORE_1_ = "F_SCORE(1)"

	def __init__(self, features_field_names="", label_field_name="", instance_name="", enable_probability_estimates=False, probability_threshold_granularity="0.05", optimization_objective=OptimizationObjective.EMPTY, rbf_gamma_value="", epsilion="0.001", max_iterations=2000, training_sample_rate=4):
		_graph_components.EpBase.__init__(self, "ML::DLIB_BINARY_CLASSIFY_TRAIN_RVM_WITH_RADIAL_BASIS_KERNEL")
		self._default_features_field_names = ""
		self.features_field_names = features_field_names
		self._default_label_field_name = ""
		self.label_field_name = label_field_name
		self._default_instance_name = ""
		self.instance_name = instance_name
		self._default_enable_probability_estimates = False
		self.enable_probability_estimates = enable_probability_estimates
		self._default_probability_threshold_granularity = "0.05"
		self.probability_threshold_granularity = probability_threshold_granularity
		self._default_optimization_objective = type(self).OptimizationObjective.EMPTY
		self.optimization_objective = optimization_objective
		self._default_rbf_gamma_value = ""
		self.rbf_gamma_value = rbf_gamma_value
		self._default_epsilion = "0.001"
		self.epsilion = epsilion
		self._default_max_iterations = 2000
		self.max_iterations = max_iterations
		self._default_training_sample_rate = 4
		self.training_sample_rate = training_sample_rate
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_features_field_names(self, value):
		self.features_field_names = value
		return self

	def set_label_field_name(self, value):
		self.label_field_name = value
		return self

	def set_instance_name(self, value):
		self.instance_name = value
		return self

	def set_enable_probability_estimates(self, value):
		self.enable_probability_estimates = value
		return self

	def set_probability_threshold_granularity(self, value):
		self.probability_threshold_granularity = value
		return self

	def set_optimization_objective(self, value):
		self.optimization_objective = value
		return self

	def set_rbf_gamma_value(self, value):
		self.rbf_gamma_value = value
		return self

	def set_epsilion(self, value):
		self.epsilion = value
		return self

	def set_max_iterations(self, value):
		self.max_iterations = value
		return self

	def set_training_sample_rate(self, value):
		self.training_sample_rate = value
		return self

	@staticmethod
	def _get_name():
		return "ML::DLIB_BINARY_CLASSIFY_TRAIN_RVM_WITH_RADIAL_BASIS_KERNEL"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.features_field_names != "": 
			desc += "FEATURES_FIELD_NAMES=" + py_to_str(self.features_field_names) + ","
		if self.label_field_name != "": 
			desc += "LABEL_FIELD_NAME=" + py_to_str(self.label_field_name) + ","
		if self.instance_name != "": 
			desc += "INSTANCE_NAME=" + py_to_str(self.instance_name) + ","
		if self.enable_probability_estimates != False: 
			desc += "ENABLE_PROBABILITY_ESTIMATES=" + py_to_str(self.enable_probability_estimates) + ","
		if self.probability_threshold_granularity != "0.05": 
			desc += "PROBABILITY_THRESHOLD_GRANULARITY=" + py_to_str(self.probability_threshold_granularity) + ","
		if self.optimization_objective != self.OptimizationObjective.EMPTY: 
			desc += "OPTIMIZATION_OBJECTIVE=" + py_to_str(self.optimization_objective) + ","
		if self.rbf_gamma_value != "": 
			desc += "RBF_GAMMA_VALUE=" + py_to_str(self.rbf_gamma_value) + ","
		if self.epsilion != "0.001": 
			desc += "EPSILION=" + py_to_str(self.epsilion) + ","
		if self.max_iterations != 2000: 
			desc += "MAX_ITERATIONS=" + py_to_str(self.max_iterations) + ","
		if self.training_sample_rate != 4: 
			desc += "TRAINING_SAMPLE_RATE=" + py_to_str(self.training_sample_rate) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class Coalesce(_graph_components.EpBase):
	"""
		

COALESCE

Type: Union

Description: Used to fill the gaps in one time series with
the ticks from one or several other time series.

This event processor considers ticks that arrive from several
sources at the same time as being the same, allowing for possible delay
across the sources when determining whether the ticks are the same.
When the same tick arrives from several sources, it is only propagated
from the source that has the highest priority among those sources.

In order to distinguish time series the event processor adds the
SYMBOL_NAME field.

Python
class name:&nbsp;Coalesce

Input: Multiple time series of ticks.

Output: A coalesced time series of ticks.

Parameters:


  PRIORITY_ORDER (string)
    A comma-separated list of the names of input time series.

  
  MAX_SOURCE_DELAY (double)
    The maximum time in seconds by which a tick from one input time
series can arrive later than the same tick from another time series.

  

Note: Input ticks do not necessarily have the same structure
- they can have different fields. Also SOURCE field is added to each
tick which lacks it to identify the source from which the tick is
coming. Hence, one must avoid adding SOURCE field in event processors
positioned after COALSECE.

Example:

Given time series of the last trade price over every 60 seconds for
databases DB1 and DB2, fills the gaps in the time series for DB1 with
the values from the time series for DB2.

LAST(60),COALESCE("DB1::TRD,DB2::TRD",0);DB1::TRD+DB2::TRD

See the COALESCE_EXAMPLE example
in UNION_EXAMPLES.otq
and coalesce.otq.


	"""
	class Parameters:
		priority_order = "PRIORITY_ORDER"
		max_source_delay = "MAX_SOURCE_DELAY"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["priority_order", "max_source_delay"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["priority_order", "_default_priority_order", "max_source_delay", "_default_max_source_delay", "stack_info", "_used_strings"]

	def __init__(self, priority_order="", max_source_delay=0):
		_graph_components.EpBase.__init__(self, "COALESCE")
		self._default_priority_order = ""
		self.priority_order = priority_order
		self._default_max_source_delay = 0
		self.max_source_delay = max_source_delay
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_priority_order(self, value):
		self.priority_order = value
		return self

	def set_max_source_delay(self, value):
		self.max_source_delay = value
		return self

	@staticmethod
	def _get_name():
		return "COALESCE"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.priority_order != "": 
			desc += "PRIORITY_ORDER=" + py_to_str(self.priority_order) + ","
		if self.max_source_delay != 0: 
			desc += "MAX_SOURCE_DELAY=" + py_to_str(self.max_source_delay) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class WhereClause(_graph_components.EpBase):
	"""
		

WHERE_CLAUSE

Type: Filter

Description: Propagates only those ticks which satisfy the
clause specified by the parameter WHERE.

Python
class name:
WhereClause

Input: A time series of ticks.

Output: A time series of ticks.

Parameters:


  DISCARD_ON_MATCH
(Boolean)
  WHERE (Boolean expression)
    A logical expression.&nbsp;

  
  STOP_ON_FIRST_MISMATCH
    If set, no ticks will be propagated after the first tick that
does
not match the WHERE parameter. If no other branch of the query requires
additional ticks, the query will stop.

  

Example: Only keep ticks that are not from NYSE whose dollar
volume is over $500,000:

(PRICE*SIZE &gt;= 500000) AND (NOT (EXCHANGE = 'N'))

See the WHERE_CLAUSE examples in FILTER_EXAMPLES.otq.


	"""
	class Parameters:
		discard_on_match = "DISCARD_ON_MATCH"
		where = "WHERE"
		stop_on_first_mismatch = "STOP_ON_FIRST_MISMATCH"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["discard_on_match", "where", "stop_on_first_mismatch"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["discard_on_match", "_default_discard_on_match", "where", "_default_where", "stop_on_first_mismatch", "_default_stop_on_first_mismatch", "stack_info", "_used_strings"]

	def __init__(self, discard_on_match=False, where="", stop_on_first_mismatch=False):
		_graph_components.EpBase.__init__(self, "WHERE_CLAUSE")
		self._default_discard_on_match = False
		self.discard_on_match = discard_on_match
		self._default_where = ""
		self.where = where
		self._default_stop_on_first_mismatch = False
		self.stop_on_first_mismatch = stop_on_first_mismatch
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_discard_on_match(self, value):
		self.discard_on_match = value
		return self

	def set_where(self, value):
		self.where = value
		return self

	def set_stop_on_first_mismatch(self, value):
		self.stop_on_first_mismatch = value
		return self

	@staticmethod
	def _get_name():
		return "WHERE_CLAUSE"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.discard_on_match != False: 
			desc += "DISCARD_ON_MATCH=" + py_to_str(self.discard_on_match) + ","
		if self.where != "": 
			desc += "WHERE=" + py_to_str(self.where) + ","
		if self.stop_on_first_mismatch != False: 
			desc += "STOP_ON_FIRST_MISMATCH=" + py_to_str(self.stop_on_first_mismatch) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class Presort(_graph_components.EpBase):
	"""
		

PRESORT

Type: Sort

Description: Propagates the ticks unchanged but ensures that
the timestamp of each output tick is greater or equal to the timestamp
of the previous output tick.

Some event processors, such as aggregations, do not output ticks
immediately and ticks that are output by such event processors may need
to be presorted to make sure the time sequence is preserved when ticks
from several such processors enter another event processor.

Such presorting is done automatically, and the PRESORT event
processor is never necessary. However, automatic presorting attempts to
ensure the smallest memory usage, sometimes at the expense of
performance. Use of PRESORT can improve the performance in
certain situations, mostly when it is known that only a few ticks need
to be presorted.

An explicitly (rather than automatically) inserted PRESORT
event processor can be used to apply batching and/or concurrency
options across its inputs. This happens when BATCH_SIZE
parameter has a value, other than 0 and/or MAX_CONCURRENCY
parameter has a value other, than 1. In such a case a request group is
formed from the subgraphs, having input nodes of that PRESORT
node as their roots. That request group, to which the specified BATCH_SIZE
and MAX_CONCURRENCY are applied, is executed separately, the
result feeding that PRESORT. Securities, bound to such a PRESORT,
if any, are used as unbound securities for that request group so, that
its execution now becomes controllable by query batch size and number
of CPU cores to utilize.

Note, that in case if a PRESORT node has multiple bound
securities, the value of its MAX_CONCURRENCY parameter, if not
specified, is defaulted to the number of cores of the original query,
where this PRESORT is used.

Python
class name:&nbsp;Presort

Input: Multiple time series of ticks.

Output: Multiple time series of ticks, sorted in time order.

Parameters:


  BATCH_SIZE (numeric)
    Query batch size for the request group, formed from inputs of PRESORT
(see above). See Query
Performance Tuning section for details about how this parameter
affects processing of a query.
Default: 0

  
  MAX_CONCURRENCY (numeric)
    Number of CPU cores to utilize for the request group, formed
from inputs of PRESORT (see above). See Query Performance Tuning
section for details about how this parameter affects processing of a
query.
Default: Number of cores of the original query (where this PRESORT is
used)

  

Examples: Compute total 5 minute volume profiles across a set
of securities:

SUM (600, false, false, AS_SEPARATE_BUCKET, VOLUME,)PRESORT()TS_ADD (600,false,AS_SEPARATE_BUCKET,)
See the PRESORT__EXAMPLE example
in SORT_EXAMPLES.otq.


	"""
	class Parameters:
		batch_size = "BATCH_SIZE"
		max_concurrency = "MAX_CONCURRENCY"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["batch_size", "max_concurrency"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["batch_size", "_default_batch_size", "max_concurrency", "_default_max_concurrency", "stack_info", "_used_strings"]

	def __init__(self, batch_size=0, max_concurrency=""):
		_graph_components.EpBase.__init__(self, "PRESORT")
		self._default_batch_size = 0
		self.batch_size = batch_size
		self._default_max_concurrency = ""
		self.max_concurrency = max_concurrency
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_batch_size(self, value):
		self.batch_size = value
		return self

	def set_max_concurrency(self, value):
		self.max_concurrency = value
		return self

	@staticmethod
	def _get_name():
		return "PRESORT"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.batch_size != 0: 
			desc += "BATCH_SIZE=" + py_to_str(self.batch_size) + ","
		if self.max_concurrency != "": 
			desc += "MAX_CONCURRENCY=" + py_to_str(self.max_concurrency) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class SynchronizeTimeAcrossSymbols(_graph_components.EpBase):
	"""
		

SYNCHRONIZE_TIME_ACROSS_SYMBOLS

Type: Sort

Description: Propagates sorted time series of ticks.

This event processor was designed to be used in load transformers.
Generally, ticks flow to the output, sorted per symbol. However, there
is no guarantee that sorting will be done across symbols.
Thus, if for instance the event processor WRITE_TO_RAW
is used as the output node of the transformer, it overrides out of
order timestamps consequently yielding data loss. To overcome such kind
of problems, you can insert SYNCHRONIZE_TIME_ACROSS_SYMBOLS
just above these kind of event processors.

Nevertheless, SYNCHRONIZE_TIME_ACROSS_SYMBOLS can be also
used in ordinary queries. When used in a query, it sorts per symbol
batch.

Python
class name:&nbsp;SynchronizeTimeAcrossSymbols

Input: A time series of ticks

Output: A sorted time series of ticks

Parameters:


  EXPECTED_INPUT_PATTERN
    SYNCHRONIZE_TIME_ACROSS_SYMBOLS can perform better if it
has information about its input pattern.
Currently supported input patterns are GENERIC,
    AGGR_BUCKETS, and AGGR_BUCKETS_WITH_INFREQ_HRTBTS. While GENERIC mode is supposed to handle all
possible input patterns, AGGR_BUCKETS
and AGGR_BUCKETS_WITH_INFREQ_HRTBTS
are designed to handle aggregations having their BUCKET_TIME=BUCKET_END more optimally.

    AGGR_BUCKETS should be used
when heartbeat interval is less than or equal to bucket time interval.

    AGGR_BUCKETS_WITH_INFREQ_HRTBTS
can be always used instead of AGGR_BUCKETS,
though it may perform a little bit worse.

    Incorrectly setting an expected input pattern will
yield undefined behavior.
Default: GENERIC

  

Examples:

EXPECTED_INPUT_PATTERN()


	"""
	class Parameters:
		expected_input_pattern = "EXPECTED_INPUT_PATTERN"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["expected_input_pattern"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["expected_input_pattern", "_default_expected_input_pattern", "stack_info", "_used_strings"]

	class ExpectedInputPattern:
		AGGR_BUCKETS = "AGGR_BUCKETS"
		AGGR_BUCKETS_WITH_INFREQ_HRTBTS = "AGGR_BUCKETS_WITH_INFREQ_HRTBTS"
		GENERIC = "GENERIC"

	def __init__(self, expected_input_pattern=ExpectedInputPattern.GENERIC):
		_graph_components.EpBase.__init__(self, "SYNCHRONIZE_TIME_ACROSS_SYMBOLS")
		self._default_expected_input_pattern = type(self).ExpectedInputPattern.GENERIC
		self.expected_input_pattern = expected_input_pattern
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_expected_input_pattern(self, value):
		self.expected_input_pattern = value
		return self

	@staticmethod
	def _get_name():
		return "SYNCHRONIZE_TIME_ACROSS_SYMBOLS"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.expected_input_pattern != self.ExpectedInputPattern.GENERIC: 
			desc += "EXPECTED_INPUT_PATTERN=" + py_to_str(self.expected_input_pattern) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class Join(_graph_components.EpBase):
	"""
		

JOIN

Type: Join

Description: Joins the ticks from two time series based on
the JOIN_CRITERIA parameter. Output ticks have all the fields from each
input time series, including the timestamps of each individual tick, as
well as a timestamp for the resulting joined tick.

Both of the input time series must have associated names. In graph
queries, this is accomplished by assigning a node name to each input EP
of the JOIN event processor. In chain queries, the use of tick type in
the form &lt;db_name1&gt;::&lt;tick_type1&gt;+&lt;db_name2&gt;::&lt;tick_type2&gt;
(for example, TAQ::TRD+NYSE_OB::OB)
will result in an input time series of JOIN named &lt;tick_type1&gt;
and &lt;tick_type2&gt;. If an input time series of JOIN carries ticks
that were created earlier on the graph using another JOIN, node name _
(underscore character) can be assigned for this input. When an
underscore (_) character is used as a node name on an input of JOIN EP,
the fields from that input do not get node name pre-pended at the
output of JOIN EP.

Python
class name:&nbsp;Join

Input: Two time series of ticks.

Output: A time series of ticks.

Parameters:


  JOIN_CRITERIA (Boolean expression)
    A Boolean expression on the fields of the input series that
defines the join condition. References to the fields of the input
series should include the name of the series (e.g. A.PRICE = B.PRICE,
where A and B are the names of the input series).

    The special attribute &lt;input_ts_name&gt;.TIMESTAMP
can be used in JOIN_CRITERIA. It represents the value of the timestamp
for a corresponding input time series.

    You can also use pseudo-fields such as _SYMBOL_NAME,
_START_TIME, and _END_TIME (described in detail in the Pseudo-fields document).

    See Performance notes for more
information about JOIN performance.

  
  JOIN_TYPE (enumerated)
    If set to INNER, inner join is implemented (that is, only ticks
where JOIN_CRITERIA is satisfied will be propagated).
If set to LEFT_OUTER, ticks from the LEFT_SOURCE time series will be
propagated even if they did not have any match for JOIN_CRITERIA.

  
  LEFT_SOURCE (string)
    A name of the input time series that is considered a left time
series for the purposes of implementing the LEFT_OUTER join, as well as
for placing fields in the output ticks (fields of the left time series
will precede the fields of the right time series in the joined ticks).

  
  ADD_SOURCE_PREFIX (Boolean)
    If set to FALSE, names of
sources (that is, &lt;input_ts_name&gt;.
prefixes) are not prepended to field names of joined ticks. However,
field names for timestamps of joined ticks still have the form &lt;input_ts_name&gt;.TIMESTAMP
Default: TRUE

  

Performance notes
JOIN performance, in terms of memory usage and execution time,
depends on the type of the JOIN_CRITERIA
specified. In the general case, JOIN has to maintain its whole input in
memory and perform lookup for every pair of ticks coming from the input
sources, which can be very slow for large series.

However, many frequently encountered cases are optimized.

Time restrictions

It's clear that JOIN EP can work optimally when the scope of
comparison is limited by a timestamp-related condition. For example, if
we are interested in ticks from the two series that have equal
timestamps, there's no point maintaining the older ticks in memory. In
this case, the amount of data to store and compare becomes radically
reduced.

JOIN recognizes JOIN_CRITERIA that consist of several OR-ed clauses
(in the simplest case, there's no OR and only one clause), where each
clause contains a time restriction AND-ed with the rest of the
clause.

A time restriction is an invocation of SAMETIME or SAMETIME_AS_EXISTING
functions or a condition in the forms

[&lt;const integer&gt;+]&lt;input_ts_name1&gt;.TIMESTAMP&lt;relationship operator&gt;[&lt;const_integer&gt;+]&lt;input_ts_name2&gt;.TIMESTAMP
(Example: A.TIMESTAMP &lt;= 1 + B.TIMESTAMP)
or

[&lt;constinteger&gt;+]&lt;input_ts_name1&gt;.TIMESTAMP -([&lt;const_integer&gt;+]&lt;input_ts_name2&gt;.TIMESTAMP)&lt;relationship operator&gt; &lt;const_integer&gt;.
(Example: A.TIMESTAMP-B.TIMESTAMP = 2)

Above, the relationship operator is one of &lt;, &lt;=, =, &gt;, &gt;=.
Also, &lt;input_ts_name&gt;.TIMESTAMP-&lt;const_integer&gt;
can be used instead of [&lt;const
integer&gt;+]&lt;input_ts_name&gt;.TIMESTAMP.

The timestamp comparison condition above can also be applied to
nanosecond parts of involved timestamps, using function
NSECTIME_AS_LONG, in which case const_integer is interpreted as the
number of nanoseconds, rather than the number of milliseconds.
(Example:
NSECTIME_TO_LONG(trd.TIMESTAMP)-NSECTIME_TO_LONG(qte.TIMESTAMP)
&lt;=100)

Note that SAMETIME, SAMETIME_AS_EXISTING, as well as other time
conditions that are described in the previous paragraph, are
evaluated first. For example, the following JOIN_CRITERIA:

COND_1and/or COND_2and/or SAMETIME_AS_EXISTING_1and/or SAMETIME_AS_EXISTING_2and/or SAMETIME_AS_EXISTING_3and/or COND_3
will be evaluated in the following order:

SAMETIME_AS_EXISTING_1and/or SAMETIME_AS_EXISTING_2and/or SAMETIME_AS_EXISTING_3and/or COND_1and/or COND_2and/or COND_3
Indexing

When the OR-ed clauses contain a condition of the form &lt;input_ts_name1&gt;.FIELD1 =
&lt;input_ts_name1&gt;.FIELD2, JOIN indexes the input series
ticks by FIELD1 and FIELD2. This means that the series are still wholly
maintained in memory but the lookup is quite fast.

Note that currently only naked field comparisons are used for this
optimization. E.g. a condition of the form FUNC1(&lt;input_ts_name1&gt;.FIELD1)
= FUNC2(&lt;input_ts_name1&gt;.FIELD2) does not invoke
optimization. To make JOIN fast in this and other cases where two sides
of equality involve expressions only involving fields of one series,
perform ADD_FIELD in each series and use the newly added fields in a
field comparison.

Examples:

Join trade ticks with orders by the order id for LSE daily data:

JOIN( "TRD.SELL_ORDER=OB.ORDERCODE",LEFT_OUTER,TRD)

Join trades with the latest quote that arrived at least 15
milliseconds ago from the same exchange:

SAMETIME_AS_EXISTING(TRD.TIMESTAMP, QTE.TIMESTAMP,
15) AND TRD.EXCHANGE=QTE.EXCHANGE.

See the JOIN examples in JOIN_EXAMPLES.otq.


	"""
	class Parameters:
		join_criteria = "JOIN_CRITERIA"
		join_type = "JOIN_TYPE"
		left_source = "LEFT_SOURCE"
		add_source_prefix = "ADD_SOURCE_PREFIX"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["join_criteria", "join_type", "left_source", "add_source_prefix"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["join_criteria", "_default_join_criteria", "join_type", "_default_join_type", "left_source", "_default_left_source", "add_source_prefix", "_default_add_source_prefix", "stack_info", "_used_strings"]

	class JoinType:
		INNER = "INNER"
		LEFT_OUTER = "LEFT_OUTER"

	def __init__(self, join_criteria="", join_type=JoinType.LEFT_OUTER, left_source="", add_source_prefix=True):
		_graph_components.EpBase.__init__(self, "JOIN")
		self._default_join_criteria = ""
		self.join_criteria = join_criteria
		self._default_join_type = type(self).JoinType.LEFT_OUTER
		self.join_type = join_type
		self._default_left_source = ""
		self.left_source = left_source
		self._default_add_source_prefix = True
		self.add_source_prefix = add_source_prefix
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_join_criteria(self, value):
		self.join_criteria = value
		return self

	def set_join_type(self, value):
		self.join_type = value
		return self

	def set_left_source(self, value):
		self.left_source = value
		return self

	def set_add_source_prefix(self, value):
		self.add_source_prefix = value
		return self

	@staticmethod
	def _get_name():
		return "JOIN"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.join_criteria != "": 
			desc += "JOIN_CRITERIA=" + py_to_str(self.join_criteria) + ","
		if self.join_type != self.JoinType.LEFT_OUTER: 
			desc += "JOIN_TYPE=" + py_to_str(self.join_type) + ","
		if self.left_source != "": 
			desc += "LEFT_SOURCE=" + py_to_str(self.left_source) + ","
		if self.add_source_prefix != True: 
			desc += "ADD_SOURCE_PREFIX=" + py_to_str(self.add_source_prefix) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class JoinSameSizeTs(_graph_components.EpBase):
	"""
		

JOIN_SAME_SIZE_TS

Type: Join

Description: Joins multiple time series that have the same
size.
The order of the ticks in the output time series can be represented as
follows:

1st tick in series 1, 1st tick in series 22nd tick in series 1, 2nd tick in series 2...last tick in series 1, last tick in series 2
If the time series differ in size an exception will be thrown.

The fields in the output time series will be represented as &lt;source_time_series&gt;.&lt;field_name&gt;.

Python
class name:&nbsp;JoinSameSizeTs

Input: Multiple time series that have equal number of ticks

Output: A time series of ticks

Parameters:


  SOURCE_ORDER
    A comma-separated list of source names, which controls the order
of fields in the output ticks. Tick fields of listed sources will be
placed at the start of the output tick in the listed order. If two or
more source nodes share the same source name, that source name is not
allowed to be listed in the SOURCE_ORDER list.
Default: empty

  
  LEADING_SOURCE
    If specified, timestamp of the tick from the leading source will
be assigned to the output tick. Otherwise, the last tick that forms an
output tick will be used to determine the timestamp.

  
  CREATE_MATRIX
    If set to TRUE, tick descriptors from all sources are
expected to have the same number of fields and corresponding fields in
tick descriptors from different sources should have same types. In that
case, they are grouped into one matrix field. It is not necessary that
fields of both inputs have the same name, only types should be the
same. Let's say we have two input time series, PRICE1 is the first
field of the first time series and its type is DOUBLE, PRICE2 is the
first field of the second time series and its type is also DOUBLE, then
the output's first field is going to be a matrix of two doubles.
Default: FALSE

  
  ADD_SOURCE_PREFIX (Boolean)
    If set to FALSE, table names of sources (i.e. &lt;table_name&gt;.
prefixes) are not prepended to field names of joined ticks. However,
field names for timestamps of joined ticks still have the form &lt;table_name&gt;.TIMESTAMP.
Default: TRUE

  

Examples:

FIND_DB_SYMBOLS(PATTERN="%A") , QUERY_SYMBOLS() , LAST_TICK(BUCKET_INTERVAL=100000,BUCKET_INTERVAL_UNITS=TICKS) , JOIN_SAME_SIZE_TS()

See the JOIN_SAME_SIZE_TS example
in JOIN_EXAMPLES.otq.


	"""
	class Parameters:
		source_order = "SOURCE_ORDER"
		leading_source = "LEADING_SOURCE"
		create_matrix = "CREATE_MATRIX"
		add_source_prefix = "ADD_SOURCE_PREFIX"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["source_order", "leading_source", "create_matrix", "add_source_prefix"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["source_order", "_default_source_order", "leading_source", "_default_leading_source", "create_matrix", "_default_create_matrix", "add_source_prefix", "_default_add_source_prefix", "stack_info", "_used_strings"]

	def __init__(self, source_order="", leading_source="", create_matrix=False, add_source_prefix=True):
		_graph_components.EpBase.__init__(self, "JOIN_SAME_SIZE_TS")
		self._default_source_order = ""
		self.source_order = source_order
		self._default_leading_source = ""
		self.leading_source = leading_source
		self._default_create_matrix = False
		self.create_matrix = create_matrix
		self._default_add_source_prefix = True
		self.add_source_prefix = add_source_prefix
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_source_order(self, value):
		self.source_order = value
		return self

	def set_leading_source(self, value):
		self.leading_source = value
		return self

	def set_create_matrix(self, value):
		self.create_matrix = value
		return self

	def set_add_source_prefix(self, value):
		self.add_source_prefix = value
		return self

	@staticmethod
	def _get_name():
		return "JOIN_SAME_SIZE_TS"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.source_order != "": 
			desc += "SOURCE_ORDER=" + py_to_str(self.source_order) + ","
		if self.leading_source != "": 
			desc += "LEADING_SOURCE=" + py_to_str(self.leading_source) + ","
		if self.create_matrix != False: 
			desc += "CREATE_MATRIX=" + py_to_str(self.create_matrix) + ","
		if self.add_source_prefix != True: 
			desc += "ADD_SOURCE_PREFIX=" + py_to_str(self.add_source_prefix) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class TrdQuoteJoin(_graph_components.EpBase):
	"""
		

TRD_QUOTE_JOIN

Type: Join

Description: Joins trades with prevailing quotes. Appends the
trade tick with all fields for the prevailing quote tick and sends the
extended trade tick to the output stream.

Python
class name:
TrdQuoteJoin

Input: A time series of trade ticks, and a time series of
quote ticks.&nbsp;Both
input EPs that connect to TRD_QUOTE_JOIN EP must have node names, and
the input EP for the quote branch must have&nbsp;node name QTE. If only
one&nbsp;EP is present at the input of&nbsp;TRD_QUOTE_JOIN
EP, its tick type must be TRD+QTE.

Output: A time series of ticks.

Parameters:


  QUOTE_DELAY (seconds)
    The minimum number of seconds that needs to elapse between the
trade
and the quote before the quote can be considered for a join with the
trade.

  
  GO_BACK_TO_FIRST_QUOTE
(seconds)
    Specifies how far back to look for an existing quote when the
first trade precedes the first quote.

  

Examples: Extend trades with information from the most recent
quote at least 5 seconds old and look back up to 300 seconds for the
quote preceding the first trade:

TRD_QUOTE_JOIN (5, 300)

See the TRD_QUOTE_JOIN example in
JOIN_TICK_TYPES.otq.


	"""
	class Parameters:
		quote_delay = "QUOTE_DELAY"
		go_back_to_first_quote = "GO_BACK_TO_FIRST_QUOTE"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["quote_delay", "go_back_to_first_quote"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["quote_delay", "_default_quote_delay", "go_back_to_first_quote", "_default_go_back_to_first_quote", "stack_info", "_used_strings"]

	def __init__(self, quote_delay=0, go_back_to_first_quote=0):
		_graph_components.EpBase.__init__(self, "TRD_QUOTE_JOIN")
		self._default_quote_delay = 0
		self.quote_delay = quote_delay
		self._default_go_back_to_first_quote = 0
		self.go_back_to_first_quote = go_back_to_first_quote
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_quote_delay(self, value):
		self.quote_delay = value
		return self

	def set_go_back_to_first_quote(self, value):
		self.go_back_to_first_quote = value
		return self

	@staticmethod
	def _get_name():
		return "TRD_QUOTE_JOIN"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.quote_delay != 0: 
			desc += "QUOTE_DELAY=" + py_to_str(self.quote_delay) + ","
		if self.go_back_to_first_quote != 0: 
			desc += "GO_BACK_TO_FIRST_QUOTE=" + py_to_str(self.go_back_to_first_quote) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class TrdObJoin(_graph_components.EpBase):
	"""
		

TRD_OB_JOIN

Type: Join

Description: Joins trades with the prevailing order book
statistic. Appends the trade tick with size-weighted price and total
size computed over a specified number of order book levels prevailing
at the time and sends the extended trade tick to the output stream.

Python
class name:
TrdObJoin

Input: A time series of ticks of type TRD; a time series of
ticks of type OB.

Output: A time series of ticks.

Parameters:


  ORDER_BOOK_DELAY (seconds)
    The minimum number of seconds that needs to elapse between the
trade and quote before the quote can be considered for a join with the
trade.

  
  MAX_LEVELS (integer)
    The number of levels of the prevailing order book to join to the
trade tick.

  
  MAX_INITIALIZATION_DAYS
(integer)

Examples: Join to each trade tick size-weighted price and
total size computed for 3 levels as it was at least 5 seconds ago:

TRD_OB_JOIN (5, 3)

See the TRD_OB_JOIN example in JOIN_TICK_TYPES.otq.


	"""
	class Parameters:
		order_book_delay = "ORDER_BOOK_DELAY"
		max_levels = "MAX_LEVELS"
		max_initialization_days = "MAX_INITIALIZATION_DAYS"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["order_book_delay", "max_levels", "max_initialization_days"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["order_book_delay", "_default_order_book_delay", "max_levels", "_default_max_levels", "max_initialization_days", "_default_max_initialization_days", "stack_info", "_used_strings"]

	def __init__(self, order_book_delay=0, max_levels="", max_initialization_days=""):
		_graph_components.EpBase.__init__(self, "TRD_OB_JOIN")
		self._default_order_book_delay = 0
		self.order_book_delay = order_book_delay
		self._default_max_levels = ""
		self.max_levels = max_levels
		self._default_max_initialization_days = ""
		self.max_initialization_days = max_initialization_days
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_order_book_delay(self, value):
		self.order_book_delay = value
		return self

	def set_max_levels(self, value):
		self.max_levels = value
		return self

	def set_max_initialization_days(self, value):
		self.max_initialization_days = value
		return self

	@staticmethod
	def _get_name():
		return "TRD_OB_JOIN"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.order_book_delay != 0: 
			desc += "ORDER_BOOK_DELAY=" + py_to_str(self.order_book_delay) + ","
		if self.max_levels != "": 
			desc += "MAX_LEVELS=" + py_to_str(self.max_levels) + ","
		if self.max_initialization_days != "": 
			desc += "MAX_INITIALIZATION_DAYS=" + py_to_str(self.max_initialization_days) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class DbInsertRow(_graph_components.EpBase):
	"""
		

DB/INSERT_ROW

Type: Transformer

Description: Inserts one tick into the database. This event
processor (EP) must be a source on the graph. If the time series into
which you are inserting already has ticks (possibly even deleted), then
you can either use this EP (DB/INSERT_ROW) or the SQL INSERT command
(for example, via omdsql.exe). If the time series has no ticks so far,
either TICK_SCHEMA parameter must be
specified, or a default schema for that tick type must have been
configured via DB/MODIFY_DEFAULT_TICK_TYPES
event processor. Please note that the former option is not available
for SQL INSERT. If both are available, TICK_SCHEMA
takes precedence.

Also see the OneTick SQL Access documentation that describes the SQL
INSERT statement.

All corrections launched via this EP are assigned unique IDs. The ID
(a string value) is available to users in the EP output (OPERATION_ID
field).

The timestamp of the inserted tick will be equal to the query start
time, and the tick type of the inserted tick will be the tick type
specified inside the query.

This EP can detect if the memory loader is running. If it is not,
the corresponding error message will appear against the memdb row for
detailed output.

DB/INSERT_ROW, DB/DELETE, and DB/UPDATE EPs are not
suitable for only making mass updates of incorrect data. A data reload
is the recommended method in such a situation. See the OneTick Data
Modeling documentation for more information.

For generic recommendations on when to use DB/ EPs, please see
Knowledge Base article KB-37.

Python
class name:&nbsp;DbInsertRow

Input: A tick to insert.

Output: A tick with the NUM_AFFECTED_ROWS field.

Parameters:


  SET (expressions)
    Expressions are in the following form:

    field_name1=&lt;expression1&gt;,field_name2=&lt;expression2&gt;,&hellip;.,
field_nameN=&lt;expressionN&gt;

    An expression can be a constant, or a function of other fields.
The function can be constructed by using standard arithmetic operators:
+, -, *, /. &nbsp;In constructing the expression, one can use
comparison operators =, !=, &lt;, &lt;=, &gt;, &gt;=, as well as
Boolean operators AND, OR, NOT (true evaluates to 1 and false to 0).
Also, OneTick built-in functions can be used in the expression (see Catalog of Built-in Functions for details).

    You can also use pseudo-fields such as TIMESTAMP, _START_TIME,
and _END_TIME (described in detail in the Pseudo-fields document).

  
  TICK_SCHEMA (field_name [type
specification] [,field_name [type specification]] )
    Describes the tick schema. Supported type specifications are string[&lt;size&gt;],
    double, long, ulong, int, short,
    byte, msectime and nsectime. If there are already
one or more ticks present for that symbol within the same day, the tick
schema does not have to be specified.

  
  KEEP_HISTORY (Boolean)
    If set to true, TICK_STATUS and DELETED_TIME fields must be
present in the tick schema; the inserted tick will have an INSERTED
status and DELETED_TIME will be set to the current time. Otherwise, the
tick will be inserted as a usual tick.

    KEEP_HISTORY must be set to true for
modifications of in-memory databases.

  
  APPLY_ON_RELOAD
    If this is false, the correction will be applied to the
archive once, and will be recorded to the corrections file with
"expiration time", which is TIMEOUT seconds from that moment.
Once expired, neither archive loader (if archive is reloaded), nor
memory loader (if in-memory database exists for that day) will apply
this correction.
Default: true

  
  TIMEOUT
    If the correction is to be applied to the in-memory database,
the memory loader will log the operation result, and this event
processor will check the log, locating the operation by its ID. The
checking will be done once per second, and the maximum number of
attempts is controlled via the TIMEOUT parameter. If the entry is
found, NUM_AFFECTED_ROWS will be set correspondingly. Otherwise,
IN_PROGRESS will be set to 1.
This parameter also controls loader behavior if APPLY_ON_RELOAD is false (see below).
Default: 5

  
  ADD_OMDSEQ (Boolean)
    If set to true, the inserted tick will have OMDSEQ field, which will be set to 0. Otherwise it will be inserted without OMDSEQ field. 
Default: true

  

Examples:

DB::INSERT_ROW (SET='RECORD_TYPE='Q',PRICE=100.25')


	"""
	class Parameters:
		set = "SET"
		tick_schema = "TICK_SCHEMA"
		keep_history = "KEEP_HISTORY"
		apply_on_reload = "APPLY_ON_RELOAD"
		timeout = "TIMEOUT"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["set", "tick_schema", "keep_history", "apply_on_reload", "timeout"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["set", "_default_set", "tick_schema", "_default_tick_schema", "keep_history", "_default_keep_history", "apply_on_reload", "_default_apply_on_reload", "timeout", "_default_timeout", "stack_info", "_used_strings"]

	def __init__(self, set="", tick_schema="", keep_history=True, apply_on_reload=True, timeout=5):
		_graph_components.EpBase.__init__(self, "DB/INSERT_ROW")
		self._default_set = ""
		self.set = set
		self._default_tick_schema = ""
		self.tick_schema = tick_schema
		self._default_keep_history = True
		self.keep_history = keep_history
		self._default_apply_on_reload = True
		self.apply_on_reload = apply_on_reload
		self._default_timeout = 5
		self.timeout = timeout
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_set(self, value):
		self.set = value
		return self

	def set_tick_schema(self, value):
		self.tick_schema = value
		return self

	def set_keep_history(self, value):
		self.keep_history = value
		return self

	def set_apply_on_reload(self, value):
		self.apply_on_reload = value
		return self

	def set_timeout(self, value):
		self.timeout = value
		return self

	@staticmethod
	def _get_name():
		return "DB/INSERT_ROW"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.set != "": 
			desc += "SET=" + py_to_str(self.set) + ","
		if self.tick_schema != "": 
			desc += "TICK_SCHEMA=" + py_to_str(self.tick_schema) + ","
		if self.keep_history != True: 
			desc += "KEEP_HISTORY=" + py_to_str(self.keep_history) + ","
		if self.apply_on_reload != True: 
			desc += "APPLY_ON_RELOAD=" + py_to_str(self.apply_on_reload) + ","
		if self.timeout != 5: 
			desc += "TIMEOUT=" + py_to_str(self.timeout) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class JoinByTime(_graph_components.EpBase):
	"""
		

JOIN_BY_TIME

Type: Join

Description: Joins ticks from multiple input time series,
based on input tick timestamps.

Output ticks are formed by joining ticks from one or more input time
series at a time, based on a particular logic, controlled mainly by the
SAME_TIMESTAMP_JOIN_POLICY parameter. To remove possible
ambiguity, resulting from input time series having fields with the same
name, field names of output ticks are formed from those of input ticks
by prepending some table name (see below), associated with the
respective source, and a dot sign - &lt;table_name&gt;.&lt;field_name&gt;.
An output tick has the timestamp of the latest input tick forming it.
Unless output ticks are formed by joining input ticks of the same
timestamp only (MATCH_IF_IDENTICAL_TIMES is set to FALSE),
they also contain timestamps of joined ticks, with the naming
convention &lt;table_name&gt;.TIMESTAMP.

Table names for sources are determined using source names and symbol
names. If all sources have unique names, then source names are used as
table names. Otherwise, it's assumed that part of the sources (possibly
none) will have unique names and the other part (possibly all) will
share the same (possibly empty) name. In this case mixed table names
are used: table name will be equal to source name, if the latter is
unique, and equal to symbol name otherwise. For those sources where
symbol name is used as table name, symbol names must be non-empty and
unique.

Python
class name:&nbsp;JoinByTime

Input: Multiple time series of ticks.

Output: A time series of ticks.

Parameters:


  JOIN_TYPE (enumerated)
    Possible values are INNER and OUTER, with the
former one prohibiting an output tick from being formed, unless every
source has participated in forming it, while the latter one allows such
ticks to be propagated.
Default: OUTER

  
  KEEP_ZERO_DURATION_TICKS
(Boolean)
    Used for backward compatibility only, in order to determine the
correct default for the SAME_TIMESTAMP_JOIN_POLICY parameter. SAME_TIMESTAMP_JOIN_POLICY
is set to ARRIVAL_ORDER, if this parameter is set to TRUE,
and is set to LATEST_TICKS otherwise.
Default: TRUE

  
  SOURCE_ORDER (string)
    A list of comma-separated source names, which controls the order
of fields in output ticks. Tick fields of listed sources will be placed
at the start of the output tick in listed order. If two or more source
nodes share the same source name, that source name is not allowed to be
listed in the SOURCE_ORDER list.
Default: empty

  
  LEADING_SOURCES (string)
    A list of comma-separated source names, implying every source
with a source name, belonging to that list, is called a leading
source. If this parameter is empty, every source is considered to
be leading. A source name listed in LEADING_SOURCES is allowed
to be shared by two or more source nodes, implying that all those
source nodes are considered to be leading sources.
Usage depends on the value of the SAME_TIMESTAMP_JOIN_POLICY
parameter. If that parameter is set to ARRIVAL_ORDER, an output
tick is propagated only if it is formed after arrival of a tick from a
leading source. If it is set to LATEST_TICKS, an output tick is
propagated only if a tick from a leading source with the timestamp of
that output tick has arrived. If the parameter is set to either EACH_FOR_LEADER_WITH_FIRST
or EACH_FOR_LEADER_WITH_LATEST, only one source is allowed to
be leading, and an output tick is formed as long as a tick from that
source has arrived.
Default: empty

  
  MATCH_IF_IDENTICAL_TIMES
(Boolean)
    A TRUE value of this parameter causes an output tick to
be formed from input ticks with identical timestamps only. If JOIN_TYPE
is set to OUTER, default values of fields (NaN, 0, empty
string) are propagated for sources that did not tick at a given
timestamp. If this parameter is set to TRUE, the default value
of SAME_TIMESTAMP_JOIN_POLICY
parameter is set to LATEST_TICKS, regardless of the value of KEEP_ZERO_DURATION_TICKS
parameter.
Default: FALSE

  
  JOIN_KEYS (string)
    A list of comma-separated names of attributes. A non-empty list
causes every new tick to be joined with ticks from other sources that
have matching values for all specified attributes.
Default: empty

  
  SAME_TIMESTAMP_JOIN_POLICY
(enumerated)
    Drives the process of joining input ticks. Possible values are:

    
      ARRIVAL_ORDER - An output
tick is formed at the time of arrival of a particular input tick, by
joining the latest arrived input ticks from each source.
      LATEST_TICKS - An output tick
is formed at the time of expiration of a particular timestamp (i.e.,
after the last input tick of that timestamp arrives), by joining the
latest arrived input ticks from each source.
      EACH_FOR_LEADER_WITH_FIRST
- An output tick is formed at the time of arrival of a particular
tick from (the only) leading source, by joining it with the first ticks
to arrive of the same timestamp from every other source. If a
particular source does not have a tick of that timestamp, the tick
first to arrive of the previous timestamp is chosen (if any).
      EACH_FOR_LEADER_WITH_LATEST
- An output tick is formed at the time of arrival of a particular
tick from (the only) leading source, by joining it with the ticks last
to arrive of the same timestamp from every other source. If a
particular source does not have a tick of that timestamp, the tick last
to arrive of the previous timestamp is chosen (if any).
    
    Default: Depends on the value of KEEP_ZERO_DURATION_TICKS and MATCH_IF_IDENTICAL_TIMES
parameters.

  
  ADD_SOURCE_PREFIX (Boolean)
    If set to FALSE, table names of sources (i.e. &lt;table_name&gt;.
prefixes) are not prepended to field names of joined ticks. However,
field names for timestamps of joined ticks still have the form &lt;table_name&gt;.TIMESTAMP.
Default: TRUE

  
  USE_FULL_SYMBOL_NAME_AS_SOURCE_PREFIX
(Boolean)
    By default, whenever a symbol name is taken as a source name,
only the pure symbol name part, without the database name, is used.
When this parameter is set to TRUE, database name is also
prepended to the symbol name. Instead of usual colons (e.g. TAQ::MSFT),
however, underscores are used (TAQ__MSFT), since subsequent
double colons are not permitted in a field name.
Default: FALSE

  

Examples: Join trade ticks with quotes:

JOIN_BY_TIME(SOURCE_ORDER="TRD,QTE");TRD+QTE

See the JOIN_BY_TIME example in JOIN_EXAMPLES.otq.

See also this
knowledge base article (requires JIRA access).


	"""
	class Parameters:
		join_type = "JOIN_TYPE"
		keep_zero_duration_ticks = "KEEP_ZERO_DURATION_TICKS"
		source_order = "SOURCE_ORDER"
		leading_sources = "LEADING_SOURCES"
		match_if_identical_times = "MATCH_IF_IDENTICAL_TIMES"
		join_keys = "JOIN_KEYS"
		same_timestamp_join_policy = "SAME_TIMESTAMP_JOIN_POLICY"
		add_source_prefix = "ADD_SOURCE_PREFIX"
		use_full_symbol_name_as_source_prefix = "USE_FULL_SYMBOL_NAME_AS_SOURCE_PREFIX"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["join_type", "keep_zero_duration_ticks", "source_order", "leading_sources", "match_if_identical_times", "join_keys", "same_timestamp_join_policy", "add_source_prefix", "use_full_symbol_name_as_source_prefix"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["join_type", "_default_join_type", "keep_zero_duration_ticks", "_default_keep_zero_duration_ticks", "source_order", "_default_source_order", "leading_sources", "_default_leading_sources", "match_if_identical_times", "_default_match_if_identical_times", "join_keys", "_default_join_keys", "same_timestamp_join_policy", "_default_same_timestamp_join_policy", "add_source_prefix", "_default_add_source_prefix", "use_full_symbol_name_as_source_prefix", "_default_use_full_symbol_name_as_source_prefix", "stack_info", "_used_strings"]

	class JoinType:
		INNER = "INNER"
		OUTER = "OUTER"

	class SameTimestampJoinPolicy:
		EMPTY = ""
		ARRIVAL_ORDER = "ARRIVAL_ORDER"
		EACH_FOR_LEADER_WITH_FIRST = "EACH_FOR_LEADER_WITH_FIRST"
		EACH_FOR_LEADER_WITH_LATEST = "EACH_FOR_LEADER_WITH_LATEST"
		LATEST_TICKS = "LATEST_TICKS"

	def __init__(self, join_type=JoinType.OUTER, keep_zero_duration_ticks=True, source_order="", leading_sources="", match_if_identical_times=False, join_keys="", same_timestamp_join_policy=SameTimestampJoinPolicy.EMPTY, add_source_prefix=True, use_full_symbol_name_as_source_prefix=False):
		_graph_components.EpBase.__init__(self, "JOIN_BY_TIME")
		self._default_join_type = type(self).JoinType.OUTER
		self.join_type = join_type
		self._default_keep_zero_duration_ticks = True
		self.keep_zero_duration_ticks = keep_zero_duration_ticks
		self._default_source_order = ""
		self.source_order = source_order
		self._default_leading_sources = ""
		self.leading_sources = leading_sources
		self._default_match_if_identical_times = False
		self.match_if_identical_times = match_if_identical_times
		self._default_join_keys = ""
		self.join_keys = join_keys
		self._default_same_timestamp_join_policy = type(self).SameTimestampJoinPolicy.EMPTY
		self.same_timestamp_join_policy = same_timestamp_join_policy
		self._default_add_source_prefix = True
		self.add_source_prefix = add_source_prefix
		self._default_use_full_symbol_name_as_source_prefix = False
		self.use_full_symbol_name_as_source_prefix = use_full_symbol_name_as_source_prefix
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_join_type(self, value):
		self.join_type = value
		return self

	def set_keep_zero_duration_ticks(self, value):
		self.keep_zero_duration_ticks = value
		return self

	def set_source_order(self, value):
		self.source_order = value
		return self

	def set_leading_sources(self, value):
		self.leading_sources = value
		return self

	def set_match_if_identical_times(self, value):
		self.match_if_identical_times = value
		return self

	def set_join_keys(self, value):
		self.join_keys = value
		return self

	def set_same_timestamp_join_policy(self, value):
		self.same_timestamp_join_policy = value
		return self

	def set_add_source_prefix(self, value):
		self.add_source_prefix = value
		return self

	def set_use_full_symbol_name_as_source_prefix(self, value):
		self.use_full_symbol_name_as_source_prefix = value
		return self

	@staticmethod
	def _get_name():
		return "JOIN_BY_TIME"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.join_type != self.JoinType.OUTER: 
			desc += "JOIN_TYPE=" + py_to_str(self.join_type) + ","
		if self.keep_zero_duration_ticks != True: 
			desc += "KEEP_ZERO_DURATION_TICKS=" + py_to_str(self.keep_zero_duration_ticks) + ","
		if self.source_order != "": 
			desc += "SOURCE_ORDER=" + py_to_str(self.source_order) + ","
		if self.leading_sources != "": 
			desc += "LEADING_SOURCES=" + py_to_str(self.leading_sources) + ","
		if self.match_if_identical_times != False: 
			desc += "MATCH_IF_IDENTICAL_TIMES=" + py_to_str(self.match_if_identical_times) + ","
		if self.join_keys != "": 
			desc += "JOIN_KEYS=" + py_to_str(self.join_keys) + ","
		if self.same_timestamp_join_policy != self.SameTimestampJoinPolicy.EMPTY: 
			desc += "SAME_TIMESTAMP_JOIN_POLICY=" + py_to_str(self.same_timestamp_join_policy) + ","
		if self.add_source_prefix != True: 
			desc += "ADD_SOURCE_PREFIX=" + py_to_str(self.add_source_prefix) + ","
		if self.use_full_symbol_name_as_source_prefix != False: 
			desc += "USE_FULL_SYMBOL_NAME_AS_SOURCE_PREFIX=" + py_to_str(self.use_full_symbol_name_as_source_prefix) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class JoinWithQuery(_graph_components.EpBase):
	"""
		

JOIN_WITH_QUERY

Type: Join

Description: Joins every input tick with the resulting ticks
of a query, executed upon the arrival of that tick. The query name as
well as query parameters are subject to being reevaluated dynamically
each time an input tick arrives.

An output tick is formed by joining the respective input tick with
the successive resulting tick of the respective query. To avoid name
collisions, field names of resulting ticks are optionally prefixed with
the value of the PREFIX_FOR_OUTPUT_TICKS parameter (see below).
An output tick carries the timestamp of the respective input tick.
Timestamps of joined ticks are propagated in the special field TIMESTAMP,
prefixed accordingly.

Python
class name:
JoinWithQuery

Input: A time series of ticks.

Output: A time series of ticks.

Parameters:


  OTQ_QUERY (expression)
    Specifies the query to be executed as a path to an .otq file,
possibly followed by the query name
(&lt;otq_file_path&gt;::&lt;query_name&gt;)-the
latter is needed if the specified .otq file contains multiple queries.
The parameter OTQ_FILE_PATH in the OneTick main configuration file
specifies the set of directories, where JOIN_WITH_QUERY looks for the
file if the value of this parameter is a relative path. This parameter
is reevaluated upon the arrival of each tick, generally resulting in
different queries to be executed for different input ticks.

  
  SYMBOL_NAME (expression)
    The symbol name, which is used to override the external symbol
list of the executed query. This parameter is reevaluated upon the
arrival of each tick.
Default: _SYMBOL_NAME

  
  START_TIMESTAMP (expression)
    The start time to override that of the executed query (up to
nanosecond precision is supported). This parameter is reevaluated upon
the arrival of each tick.
Default: _START_TIME

  
  END_TIMESTAMP (expression)
    The end time to override that of the executed query (up to
nanosecond precision is supported). This parameter is reevaluated upon
the arrival of each tick.
Default: _END_TIME

  
  OTQ_QUERY_PARAMS (expression)
    Expression that evaluates to a string containing comma-separated
list of &lt;name&gt;=&lt;value&gt;
pairs, specifying OTQ parameters of the query to be executed. This
parameter is reevaluated upon the arrival of each tick.
Default: empty

  
  SYMBOL_DATE (expression)
    The symbol date to override that of the executed query. This
parameter is reevaluated upon the arrival of each tick. If not
specified, the symbol date from the inner query is used.
Default: empty

  
  JOIN_TYPE (enumerated)
    Possible values are INNER and OUTER, with the
former one prohibiting output ticks from being formed, unless query
execution has resulted in a non-empty set of ticks. An OUTER
join allows such unmatched ticks to still be propagated, joining them
with ticks formed from the fields and values specified in the DEFAULT_FIELDS_FOR_OUTER_JOIN
parameter.
Default: OUTER

  
  DEFAULT_FIELDS_FOR_OUTER_JOIN
(expression)
    A comma-separated list of fields in the following format: FIELD_1 [TYPE_1]=EXPR_1, FIELD_2 [TYPE_2]=EXPR_1,
&hellip; ,FIELD_N [TYPE_N]=EXPR_1, which specifies the names and
the values of the fields (also, optionally, the field type), used to
form ticks to be joined with unmatched input ticks for the OUTER
join. This parameter is reevaluated upon the arrival of each tick.
Default: empty

  
  PREFIX_FOR_OUTPUT_TICKS
(string)
    The prefix for the names of joined tick fields.
Default: empty

  
  PROCESS_QUERY_ASYNCHRONOUSLY
(Boolean)
    Switches between synchronous and asynchronous execution of
queries. While asynchronous execution is generally much more effective,
in certain cases synchronous execution may still be preferred (e.g.,
when there are a few input ticks, each initiating a memory-consuming
query).
In asynchronous mode typically while parallel thread is processing the
query, EP accumulates some input ticks. In CEP mode maximum latency for
accumulated ticks can be controlled via JOIN_WITH_QUERY.MAX_UNPROCESSED_TICK_LATENCY
config parameter (applies to the ticks that arrived after stitching).
Default: TRUE

  
  APPLY_TIMES_DAILY (Boolean)
    The "apply times daily" flag is used to override that of the
executed query. If this parameter is not specified, "apply times daily"
is taken from the first executed query, assuming every successive one
will have the same value - if not, an exception is thrown.
Default: empty

  
  TIMEZONE (expression)
    The "Timezone" value is used to override that of the executed
query. If this parameter is not specified, "Timezone" is taken from the
first executed query, assuming every successive one will have the same
value - if not, an exception is thrown. Expression in this
parameter can not contain tick field names.
Default: empty

  
  SYMBOL_PARAMS (expression)
    An expression that evaluates to a string containing a
comma-separated list of &lt;name&gt;=&lt;value&gt;
pairs, specifying symbol parameters of the security, which is used to
override an external symbol list of the executed query. This parameter
is reevaluated upon the arrival of each tick.
Default: If SYMBOL_NAME parameter is empty and DEFAULT_TO_VALUES_FROM_QUERY
is set to false, symbol parameters of a symbol (_SYMBOL_NAME) from the
parent query are used. Otherwise, default symbol parameters are empty.

  
  CACHING_SCOPE (string)
    Possible values are:

    
      NONE: Caching is disabled
      CROSS_SYMBOL: Cache is the same for all symbols (that
is, all parameters except symbol-name must be identical for the cache
entry to be used) 
      PER_SYMBOL: Cache is different for each symbol (passed
through the SYMBOL_NAME EP parameter). Cache entry is used if
all query parameters are identical.
    
    In both CROSS_SYMBOL and PER_SYMBOL cases, cache
is shared across unbound symbols of the main query.

    Default: NONE

  
  CACHE_EXPIRATION_INTERVAL
(integer)
    Specifies the cache expiration interval in seconds. This
parameter is supported only in CEP mode. It forces to re-execute the
query (even if there is a corresponding entry in cache), if cache data
is older than the specified interval.
Default: empty

  
  DEFAULT_TO_VALUES_FROM_QUERY
(Boolean or ONLY_FOR_SYMBOL_INFO)
    If true, then default values for SYMBOL_NAME, START_TIMESTAMP
and END_TIMESTAMP parameters are taken from executed query and
values from main query are used when "false" is specified. ONLY_FOR_SYMBOL_INFO
special value is also supported, in which case symbol is taken from
inner query, while main query start/end times are used.
Default: false

  
  SHARED_THREAD_COUNT (integer)
    Specifies number of threads for asynchronous processing per
unbound symbol list.
Default: 1

  
  WHERE (expression)
    A boolean expression (see below) which specifies criteria for
executing sub-query, if expression evaluates to false, OTQ_QUERY
is not executed and DEFAULT_FIELDS_FOR_OUTER_JOIN will be
joined with input tick.
Default: empty

  
   STRIP_QUOTES_WHEN_ASSIGNING_SYMBOL_AND_OTQ_PARAMS (expression)
    When set to true, symbol and otq parameters will have the quotes around parameter values removed when assigning those parameters values. COMPATIBILITY.JOIN_WITH_QUERY.STRIP_QUOTES_AROUND_SYMBOL_AND_OTQ_PARAMS config parameter also has the same effect (when both config parameter and EP paramter are specified,  the EP parameter will have higher priority).
Default: true

  

All of the expressions above are built from Boolean operators (AND,
OR, NOT), relationship operators (&lt;, &lt;=, &gt;, &gt;=, =, !=), and
arithmetic operators (+, -, *, /, +, also serve as string concatenation
operators) according to the usual rules of precedence, parentheses
being available to resolve ambiguities. Along with constant numeric and
string literals (both single and double-quoted), tick attributes and state variables, it is possible to use
OneTick functions like MIN, MAX, MOD, DIV, etc. (see the Catalog of built-in functions for the full
list).

The fields of preceding ticks (those that already arrived) can also
be involved in computations, except in the DEFAULT_FIELDS_FOR_OUTER_JOIN
parameter. In this case, field names are followed by a negative index
in square brackets, specifying how far to look (e.g., PRICE[-1], SIZE[-3], PRICE[2] etc.).

Pseudo-fields, such as TIMESTAMP,
_START_TIME, and _END_TIME, can also be used.

The _TIMEZONE pseudo-field value in the executed query
is not overridden by default. To specify TIMEZONE for the executed
query, you can use the TIMEZONE EP parameter or alternatively OTQ_QUERY_PARAMS
can be used.

Following configuration variables are also for JOIN_WITH_QUERY.


  JOIN_WITH_QUERY.MAX_UNPROCESSED_TICK_LATENCY: Specifies
max latency in CEP mode, when asynchronous processing is enabled. Not
applicable for historical queries.
  JOIN_WITH_QUERY.MAX_BATCHING_RANGE: Specifies time range
for ticks in CEP mode (in milliseconds) to process simultaneously, by
default only one tick at a time is processed by asynchronous thread.

Examples: Join trade ticks with quotes:

JOIN_WITH_QUERY(OTQ_QUERY="\\"./join_with_query_queries.otq::query_trd\\"",SYMBOL_NAME=_SYMBOL_NAME,START_TIMESTAMP=_START_TIME,END_TIMESTAMP=_END_TIME,OTQ_QUERY_PARAMS="'FIELD_VAL='+TOSTRING(SIZE)+',FIELD_NAME=SIZE'",PREFIX_FOR_OUTPUT_TICKS="QUERY.")
See the join_with_query examples
in JOIN_EXAMPLES.otq.


	"""
	class Parameters:
		otq_query = "OTQ_QUERY"
		symbol_name = "SYMBOL_NAME"
		start_timestamp = "START_TIMESTAMP"
		end_timestamp = "END_TIMESTAMP"
		otq_query_params = "OTQ_QUERY_PARAMS"
		join_type = "JOIN_TYPE"
		default_fields_for_outer_join = "DEFAULT_FIELDS_FOR_OUTER_JOIN"
		prefix_for_output_ticks = "PREFIX_FOR_OUTPUT_TICKS"
		process_query_asynchronously = "PROCESS_QUERY_ASYNCHRONOUSLY"
		apply_times_daily = "APPLY_TIMES_DAILY"
		timezone = "TIMEZONE"
		symbol_params = "SYMBOL_PARAMS"
		symbol_date = "SYMBOL_DATE"
		caching_scope = "CACHING_SCOPE"
		cache_expiration_interval = "CACHE_EXPIRATION_INTERVAL"
		default_to_values_from_query = "DEFAULT_TO_VALUES_FROM_QUERY"
		where = "WHERE"
		shared_thread_count = "SHARED_THREAD_COUNT"
		strip_quotes_when_assigning_symbol_and_otq_params = "STRIP_QUOTES_WHEN_ASSIGNING_SYMBOL_AND_OTQ_PARAMS"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["otq_query", "symbol_name", "start_timestamp", "end_timestamp", "otq_query_params", "join_type", "default_fields_for_outer_join", "prefix_for_output_ticks", "process_query_asynchronously", "apply_times_daily", "timezone", "symbol_params", "symbol_date", "caching_scope", "cache_expiration_interval", "default_to_values_from_query", "where", "shared_thread_count", "strip_quotes_when_assigning_symbol_and_otq_params"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["otq_query", "_default_otq_query", "symbol_name", "_default_symbol_name", "start_timestamp", "_default_start_timestamp", "end_timestamp", "_default_end_timestamp", "otq_query_params", "_default_otq_query_params", "join_type", "_default_join_type", "default_fields_for_outer_join", "_default_default_fields_for_outer_join", "prefix_for_output_ticks", "_default_prefix_for_output_ticks", "process_query_asynchronously", "_default_process_query_asynchronously", "apply_times_daily", "_default_apply_times_daily", "timezone", "_default_timezone", "symbol_params", "_default_symbol_params", "symbol_date", "_default_symbol_date", "caching_scope", "_default_caching_scope", "cache_expiration_interval", "_default_cache_expiration_interval", "default_to_values_from_query", "_default_default_to_values_from_query", "where", "_default_where", "shared_thread_count", "_default_shared_thread_count", "strip_quotes_when_assigning_symbol_and_otq_params", "_default_strip_quotes_when_assigning_symbol_and_otq_params", "stack_info", "_used_strings"]

	class JoinType:
		INNER = "INNER"
		OUTER = "OUTER"

	class CachingScope:
		CROSS_SYMBOL = "CROSS_SYMBOL"
		NONE = "NONE"
		PER_SYMBOL = "PER_SYMBOL"

	class DefaultToValuesFromQuery:
		ONLY_FOR_SYMBOL_INFO = "ONLY_FOR_SYMBOL_INFO"
		FALSE = "false"
		TRUE = "true"

	def __init__(self, otq_query="", symbol_name="", start_timestamp="", end_timestamp="", otq_query_params="", join_type=JoinType.OUTER, default_fields_for_outer_join="", prefix_for_output_ticks="", process_query_asynchronously=True, apply_times_daily=False, timezone="", symbol_params="", symbol_date="", caching_scope=CachingScope.NONE, cache_expiration_interval="", default_to_values_from_query=DefaultToValuesFromQuery.FALSE, where="", shared_thread_count="", strip_quotes_when_assigning_symbol_and_otq_params=False):
		_graph_components.EpBase.__init__(self, "JOIN_WITH_QUERY")
		self._default_otq_query = ""
		self.otq_query = otq_query
		self._default_symbol_name = ""
		self.symbol_name = symbol_name
		self._default_start_timestamp = ""
		self.start_timestamp = start_timestamp
		self._default_end_timestamp = ""
		self.end_timestamp = end_timestamp
		self._default_otq_query_params = ""
		self.otq_query_params = otq_query_params
		self._default_join_type = type(self).JoinType.OUTER
		self.join_type = join_type
		self._default_default_fields_for_outer_join = ""
		self.default_fields_for_outer_join = default_fields_for_outer_join
		self._default_prefix_for_output_ticks = ""
		self.prefix_for_output_ticks = prefix_for_output_ticks
		self._default_process_query_asynchronously = True
		self.process_query_asynchronously = process_query_asynchronously
		self._default_apply_times_daily = False
		self.apply_times_daily = apply_times_daily
		self._default_timezone = ""
		self.timezone = timezone
		self._default_symbol_params = ""
		self.symbol_params = symbol_params
		self._default_symbol_date = ""
		self.symbol_date = symbol_date
		self._default_caching_scope = type(self).CachingScope.NONE
		self.caching_scope = caching_scope
		self._default_cache_expiration_interval = ""
		self.cache_expiration_interval = cache_expiration_interval
		self._default_default_to_values_from_query = type(self).DefaultToValuesFromQuery.FALSE
		self.default_to_values_from_query = default_to_values_from_query
		self._default_where = ""
		self.where = where
		self._default_shared_thread_count = ""
		self.shared_thread_count = shared_thread_count
		self._default_strip_quotes_when_assigning_symbol_and_otq_params = False
		self.strip_quotes_when_assigning_symbol_and_otq_params = strip_quotes_when_assigning_symbol_and_otq_params
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_otq_query(self, value):
		self.otq_query = value
		return self

	def set_symbol_name(self, value):
		self.symbol_name = value
		return self

	def set_start_timestamp(self, value):
		self.start_timestamp = value
		return self

	def set_end_timestamp(self, value):
		self.end_timestamp = value
		return self

	def set_otq_query_params(self, value):
		self.otq_query_params = value
		return self

	def set_join_type(self, value):
		self.join_type = value
		return self

	def set_default_fields_for_outer_join(self, value):
		self.default_fields_for_outer_join = value
		return self

	def set_prefix_for_output_ticks(self, value):
		self.prefix_for_output_ticks = value
		return self

	def set_process_query_asynchronously(self, value):
		self.process_query_asynchronously = value
		return self

	def set_apply_times_daily(self, value):
		self.apply_times_daily = value
		return self

	def set_timezone(self, value):
		self.timezone = value
		return self

	def set_symbol_params(self, value):
		self.symbol_params = value
		return self

	def set_symbol_date(self, value):
		self.symbol_date = value
		return self

	def set_caching_scope(self, value):
		self.caching_scope = value
		return self

	def set_cache_expiration_interval(self, value):
		self.cache_expiration_interval = value
		return self

	def set_default_to_values_from_query(self, value):
		self.default_to_values_from_query = value
		return self

	def set_where(self, value):
		self.where = value
		return self

	def set_shared_thread_count(self, value):
		self.shared_thread_count = value
		return self

	def set_strip_quotes_when_assigning_symbol_and_otq_params(self, value):
		self.strip_quotes_when_assigning_symbol_and_otq_params = value
		return self

	@staticmethod
	def _get_name():
		return "JOIN_WITH_QUERY"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.otq_query != "": 
			desc += "OTQ_QUERY=" + py_to_str(self.otq_query) + ","
		if self.symbol_name != "": 
			desc += "SYMBOL_NAME=" + py_to_str(self.symbol_name) + ","
		if self.start_timestamp != "": 
			desc += "START_TIMESTAMP=" + py_to_str(self.start_timestamp) + ","
		if self.end_timestamp != "": 
			desc += "END_TIMESTAMP=" + py_to_str(self.end_timestamp) + ","
		if self.otq_query_params != "": 
			desc += "OTQ_QUERY_PARAMS=" + py_to_str(self.otq_query_params) + ","
		if self.join_type != self.JoinType.OUTER: 
			desc += "JOIN_TYPE=" + py_to_str(self.join_type) + ","
		if self.default_fields_for_outer_join != "": 
			desc += "DEFAULT_FIELDS_FOR_OUTER_JOIN=" + py_to_str(self.default_fields_for_outer_join) + ","
		if self.prefix_for_output_ticks != "": 
			desc += "PREFIX_FOR_OUTPUT_TICKS=" + py_to_str(self.prefix_for_output_ticks) + ","
		if self.process_query_asynchronously != True: 
			desc += "PROCESS_QUERY_ASYNCHRONOUSLY=" + py_to_str(self.process_query_asynchronously) + ","
		if self.apply_times_daily != False: 
			desc += "APPLY_TIMES_DAILY=" + py_to_str(self.apply_times_daily) + ","
		if self.timezone != "": 
			desc += "TIMEZONE=" + py_to_str(self.timezone) + ","
		if self.symbol_params != "": 
			desc += "SYMBOL_PARAMS=" + py_to_str(self.symbol_params) + ","
		if self.symbol_date != "": 
			desc += "SYMBOL_DATE=" + py_to_str(self.symbol_date) + ","
		if self.caching_scope != self.CachingScope.NONE: 
			desc += "CACHING_SCOPE=" + py_to_str(self.caching_scope) + ","
		if self.cache_expiration_interval != "": 
			desc += "CACHE_EXPIRATION_INTERVAL=" + py_to_str(self.cache_expiration_interval) + ","
		if self.default_to_values_from_query != self.DefaultToValuesFromQuery.FALSE: 
			desc += "DEFAULT_TO_VALUES_FROM_QUERY=" + py_to_str(self.default_to_values_from_query) + ","
		if self.where != "": 
			desc += "WHERE=" + py_to_str(self.where) + ","
		if self.shared_thread_count != "": 
			desc += "SHARED_THREAD_COUNT=" + py_to_str(self.shared_thread_count) + ","
		if self.strip_quotes_when_assigning_symbol_and_otq_params != False: 
			desc += "STRIP_QUOTES_WHEN_ASSIGNING_SYMBOL_AND_OTQ_PARAMS=" + py_to_str(self.strip_quotes_when_assigning_symbol_and_otq_params) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class JoinWithCollectionSummary(_graph_components.EpBase):
	"""
		

JOIN_WITH_COLLECTION_SUMMARY

Type: Join

Description: Upon the arrival of an input tick executes a
sub-query on a collection state variables
and joins resulting ticks with an input tick. The sub-query name as
well as query parameters are subject to being reevaluated dynamically
each time an input tick arrives.

An output tick is formed by joining the respective input tick with
the successive resulting tick of the respective query. To avoid name
collisions, field names of resulting ticks are optionally prefixed with
the value of the PREFIX_FOR_OUTPUT_TICKS parameter (see below).
An output tick carries the timestamp of the respective input tick.
Timestamps of joined ticks are propagated in the special field TIMESTAMP,
prefixed accordingly.

Python
class name:&nbsp;JoinWithCollectionSummary

Input: A time series of ticks.

Output: A time series of ticks.

Parameters:


  COLLECTION_NAME
    Specifies the name of collection state variable on which
sub-query will be executed (ticks of collection will be passed to
sub-query as input ticks).
    TICK_SET, TICK_SET_UNORDERED, TICK_LIST and TICK_DEQUE
tick collection state variable types are supported.

  
  OTQ_QUERY (expression)
    Specifies the sub-query to be executed as a path to an .otq
file, possibly followed by the query name
(&lt;otq_file_path&gt;::&lt;query_name&gt;)--the
latter is needed if the specified .otq file contains multiple queries.
The parameter OTQ_FILE_PATH in the OneTick main configuration file
specifies the set of directories, where JOIN_WITH_COLLECTION_SUMMARY
looks for the file if the value of this parameter is a relative path.
This parameter is reevaluated upon the arrival of each tick, generally
resulting in different queries to be executed for different input ticks.

  
  START_TIMESTAMP (expression)
    Start time in milliseconds, if specified only ticks in
collection that have higher or equal timestamp will be processed. This
parameter is reevaluated upon the arrival of each tick.
Default: empty

  
  END_TIMESTAMP (expression)
    End time in milliseconds, if specified only ticks in collection
that have lower timestamp will be processed. This parameter is
reevaluated upon the arrival of each tick.
Default: empty

  
  OTQ_QUERY_PARAMS (expression)
    Expression that evaluates to a string containing comma-separated
list of &lt;name&gt;=&lt;value&gt;
pairs, specifying OTQ parameters of the query to be executed. This
parameter is reevaluated upon the arrival of each tick.
Default: empty

  
  JOIN_TYPE (enumerated)
    Possible values are INNER and OUTER, with the
former one prohibiting output ticks from being formed, unless query
execution has resulted in a non-empty set of ticks. An OUTER
join allows such unmatched ticks to still be propagated, joining them
with ticks formed from the fields and values specified in the DEFAULT_FIELDS_FOR_OUTER_JOIN
parameter.
Default: OUTER

  
  DEFAULT_FIELDS_FOR_OUTER_JOIN
(expression)
    A comma-separated list of &lt;name&gt;=&lt;value&gt;
pairs, specifying names and values of the fields, used to form ticks to
be joined with unmatched input ticks for the OUTER join. This
parameter is reevaluated upon the arrival of each tick.
Default: empty

  
  PREFIX_FOR_OUTPUT_TICKS
(string)
    The prefix for the names of joined tick fields.
Default: empty

  
  CACHING_SCOPE (string)
    Possible values are:

    
      NONE: caching is disabled
      PER_SYMBOL: cache is different for each symbol
    
    Note that if tick collection changes during execution of main
query, output of JOIN_WITH_COLLECTION_SUMMARY ep might differ depending
on value of CACHING_SCOPE parameter.

    Default: NONE

  
  CACHE_EXPIRATION_INTERVAL
(integer)
    Specifies the cache expiration interval in seconds. This
parameter is supported only in CEP mode. It forces to re-execute the
query (even if there is a corresponding entry in cache), if cache data
is older than the specified interval.
Default: empty

  

All of the expressions above are built from Boolean operators (AND,
OR, NOT), relationship operators (&lt;, &lt;=, &gt;, &gt;=, =, !=), and
arithmetic operators (+, -, *, /, +, also serve as string concatenation
operators) according to the usual rules of precedence, parentheses
being available to resolve ambiguities. Along with constant numeric and
string literals (both single and double-quoted), tick attributes and state variables, it is possible to use
OneTick functions like MIN, MAX, MOD, DIV, etc. (see the Catalog of built-in functions for the full
list).

The fields of preceding ticks (those that already arrived) can also
be involved in computations. In this case, field names are followed by
a negative index in square brackets, specifying how far to look (e.g., PRICE[-1], SIZE[-3], PRICE[2] etc.).

Pseudo-fields, such as TIMESTAMP,
_START_TIME, and _END_TIME, can also be used.

Examples: Join input ticks with collection:

JOIN_WITH_COLLECTION_SUMMARY(COLLECTION_NAME=STATE::tick_collection,OTQ_QUERY="\\"./join_with_collection_summary_queries.otq::query_example\\"",START_TIMESTAMP=TIMESTAMP,OTQ_QUERY_PARAMS="'FIELD_VAL='+TOSTRING(SIZE)+',FIELD_NAME=SIZE'",PREFIX_FOR_OUTPUT_TICKS="QUERY.")
See the join_with_collection_summary
examples in JOIN_EXAMPLES.otq.


	"""
	class Parameters:
		collection_name = "COLLECTION_NAME"
		otq_query = "OTQ_QUERY"
		start_timestamp = "START_TIMESTAMP"
		end_timestamp = "END_TIMESTAMP"
		otq_query_params = "OTQ_QUERY_PARAMS"
		join_type = "JOIN_TYPE"
		default_fields_for_outer_join = "DEFAULT_FIELDS_FOR_OUTER_JOIN"
		prefix_for_output_ticks = "PREFIX_FOR_OUTPUT_TICKS"
		caching_scope = "CACHING_SCOPE"
		cache_expiration_interval = "CACHE_EXPIRATION_INTERVAL"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["collection_name", "otq_query", "start_timestamp", "end_timestamp", "otq_query_params", "join_type", "default_fields_for_outer_join", "prefix_for_output_ticks", "caching_scope", "cache_expiration_interval"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["collection_name", "_default_collection_name", "otq_query", "_default_otq_query", "start_timestamp", "_default_start_timestamp", "end_timestamp", "_default_end_timestamp", "otq_query_params", "_default_otq_query_params", "join_type", "_default_join_type", "default_fields_for_outer_join", "_default_default_fields_for_outer_join", "prefix_for_output_ticks", "_default_prefix_for_output_ticks", "caching_scope", "_default_caching_scope", "cache_expiration_interval", "_default_cache_expiration_interval", "stack_info", "_used_strings"]

	class JoinType:
		INNER = "INNER"
		OUTER = "OUTER"

	class CachingScope:
		NONE = "NONE"
		PER_SYMBOL = "PER_SYMBOL"

	def __init__(self, collection_name="", otq_query="", start_timestamp="", end_timestamp="", otq_query_params="", join_type=JoinType.OUTER, default_fields_for_outer_join="", prefix_for_output_ticks="", caching_scope=CachingScope.NONE, cache_expiration_interval=""):
		_graph_components.EpBase.__init__(self, "JOIN_WITH_COLLECTION_SUMMARY")
		self._default_collection_name = ""
		self.collection_name = collection_name
		self._default_otq_query = ""
		self.otq_query = otq_query
		self._default_start_timestamp = ""
		self.start_timestamp = start_timestamp
		self._default_end_timestamp = ""
		self.end_timestamp = end_timestamp
		self._default_otq_query_params = ""
		self.otq_query_params = otq_query_params
		self._default_join_type = type(self).JoinType.OUTER
		self.join_type = join_type
		self._default_default_fields_for_outer_join = ""
		self.default_fields_for_outer_join = default_fields_for_outer_join
		self._default_prefix_for_output_ticks = ""
		self.prefix_for_output_ticks = prefix_for_output_ticks
		self._default_caching_scope = type(self).CachingScope.NONE
		self.caching_scope = caching_scope
		self._default_cache_expiration_interval = ""
		self.cache_expiration_interval = cache_expiration_interval
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_collection_name(self, value):
		self.collection_name = value
		return self

	def set_otq_query(self, value):
		self.otq_query = value
		return self

	def set_start_timestamp(self, value):
		self.start_timestamp = value
		return self

	def set_end_timestamp(self, value):
		self.end_timestamp = value
		return self

	def set_otq_query_params(self, value):
		self.otq_query_params = value
		return self

	def set_join_type(self, value):
		self.join_type = value
		return self

	def set_default_fields_for_outer_join(self, value):
		self.default_fields_for_outer_join = value
		return self

	def set_prefix_for_output_ticks(self, value):
		self.prefix_for_output_ticks = value
		return self

	def set_caching_scope(self, value):
		self.caching_scope = value
		return self

	def set_cache_expiration_interval(self, value):
		self.cache_expiration_interval = value
		return self

	@staticmethod
	def _get_name():
		return "JOIN_WITH_COLLECTION_SUMMARY"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.collection_name != "": 
			desc += "COLLECTION_NAME=" + py_to_str(self.collection_name) + ","
		if self.otq_query != "": 
			desc += "OTQ_QUERY=" + py_to_str(self.otq_query) + ","
		if self.start_timestamp != "": 
			desc += "START_TIMESTAMP=" + py_to_str(self.start_timestamp) + ","
		if self.end_timestamp != "": 
			desc += "END_TIMESTAMP=" + py_to_str(self.end_timestamp) + ","
		if self.otq_query_params != "": 
			desc += "OTQ_QUERY_PARAMS=" + py_to_str(self.otq_query_params) + ","
		if self.join_type != self.JoinType.OUTER: 
			desc += "JOIN_TYPE=" + py_to_str(self.join_type) + ","
		if self.default_fields_for_outer_join != "": 
			desc += "DEFAULT_FIELDS_FOR_OUTER_JOIN=" + py_to_str(self.default_fields_for_outer_join) + ","
		if self.prefix_for_output_ticks != "": 
			desc += "PREFIX_FOR_OUTPUT_TICKS=" + py_to_str(self.prefix_for_output_ticks) + ","
		if self.caching_scope != self.CachingScope.NONE: 
			desc += "CACHING_SCOPE=" + py_to_str(self.caching_scope) + ","
		if self.cache_expiration_interval != "": 
			desc += "CACHE_EXPIRATION_INTERVAL=" + py_to_str(self.cache_expiration_interval) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class UpdateTickSets(_graph_components.EpBase):
	"""
		

UPDATE_TICK_SETS

Type: Transformer

Description: Inserts each input tick that is matching a
selection criterion to the specified TICK SET.

Python
class name:&nbsp;UpdateTickSets

Input: A time series of ticks.

Output: A time series of ticks.

Parameters:


  TICK_SETS (string)
    A mandatory parameter that specifies a comma-separated list of TICK
SETs to be updated. TICK SETs should be specified by full
name including the STATE:: prefix.

  
  WHERE (expression)
    Specifies a logical expression
for tick selection to insert into the given TICK SETs.&nbsp;.

  
  VALUE_FIELDS (string)
    Comma-separated list of value fields to be inserted into tick
sets
(if param is empty, all fields of input tick are inserted). Note that
this applies only to non-key fields (key-fields are always included).

  
  ERASE_CONDITION (expression)
    The ERASE_CONDITION specifies a criterion for ticks to
be erased from the given TICK SETs. The same rules of
constructing a WHERE expression apply also for ERASE_CONDITION
expression. In case for a particular tick both WHERE and ERASE_CONDITION
evaluate to true, ERASE_CONDITION takes precedence, i.e. the
tick is effectively erased from the given sets.

  

Examples:

UPDATE_TICK_SETS(TICK_SETS='STATE::S1,STATE::S2',WHERE='SIZE&lt;=100',ERASE_CONDITION='SIZE&gt;50')UPDATE_TICK_SETS(TICK_SETS='STATE::S1,STATE::S2',WHERE='SIZE&lt;=100')UPDATE_TICK_SETS(TICK_SETS='STATE::S1')
See the tick_set_1 example in TICK_SETS_AND_LISTS.otq.


	"""
	class Parameters:
		tick_sets = "TICK_SETS"
		where = "WHERE"
		value_fields = "VALUE_FIELDS"
		erase_condition = "ERASE_CONDITION"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["tick_sets", "where", "value_fields", "erase_condition"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["tick_sets", "_default_tick_sets", "where", "_default_where", "value_fields", "_default_value_fields", "erase_condition", "_default_erase_condition", "stack_info", "_used_strings"]

	def __init__(self, tick_sets="", where="", value_fields="", erase_condition=""):
		_graph_components.EpBase.__init__(self, "UPDATE_TICK_SETS")
		self._default_tick_sets = ""
		self.tick_sets = tick_sets
		self._default_where = ""
		self.where = where
		self._default_value_fields = ""
		self.value_fields = value_fields
		self._default_erase_condition = ""
		self.erase_condition = erase_condition
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_tick_sets(self, value):
		self.tick_sets = value
		return self

	def set_where(self, value):
		self.where = value
		return self

	def set_value_fields(self, value):
		self.value_fields = value
		return self

	def set_erase_condition(self, value):
		self.erase_condition = value
		return self

	@staticmethod
	def _get_name():
		return "UPDATE_TICK_SETS"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.tick_sets != "": 
			desc += "TICK_SETS=" + py_to_str(self.tick_sets) + ","
		if self.where != "": 
			desc += "WHERE=" + py_to_str(self.where) + ","
		if self.value_fields != "": 
			desc += "VALUE_FIELDS=" + py_to_str(self.value_fields) + ","
		if self.erase_condition != "": 
			desc += "ERASE_CONDITION=" + py_to_str(self.erase_condition) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class UpdateFromTickSet(_graph_components.EpBase):
	"""
		

UPDATE_FROM_TICK_SET

Type: Transformer

Description: Updates each input tick, that is matching a
selection criteria, with values received from tick set's tick, which
matches input tick with it's key fields.

Python
class name:&nbsp;UpdateFromTickSet

Input: A time series of ticks.

Output: A time series of ticks.

Parameters:


  TICK_SET (string)
    A mandatory parameter that specifies a TICK SET, whose
ticks will be used to modify the values of input ticks. TICK SET
should be specified by full name including the STATE:: prefix.

  
  WHERE (expression)
    Specifies a logical expression
for tick selection to get values from the given TICK SETs.&nbsp;

    
  

Examples:

UPDATE_FROM_TICK_SET(TICK_SET='STATE::S1',WHERE='SIZE&lt;=100')UPDATE_FROM_TICK_SET(TICK_SET='STATE::S1')
See the tick_set_3 example in TICK_SETS.otq.


	"""
	class Parameters:
		tick_set = "TICK_SET"
		where = "WHERE"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["tick_set", "where"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["tick_set", "_default_tick_set", "where", "_default_where", "stack_info", "_used_strings"]

	def __init__(self, tick_set="", where=""):
		_graph_components.EpBase.__init__(self, "UPDATE_FROM_TICK_SET")
		self._default_tick_set = ""
		self.tick_set = tick_set
		self._default_where = ""
		self.where = where
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_tick_set(self, value):
		self.tick_set = value
		return self

	def set_where(self, value):
		self.where = value
		return self

	@staticmethod
	def _get_name():
		return "UPDATE_FROM_TICK_SET"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.tick_set != "": 
			desc += "TICK_SET=" + py_to_str(self.tick_set) + ","
		if self.where != "": 
			desc += "WHERE=" + py_to_str(self.where) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class Ml_dlibRegressionPredict(_graph_components.EpBase):
	"""
		

ML::DLIB_REGRESSION_PREDICT

Type: Other

Description: This predict regression EP
can be used to predict the dependent variable from input features using
the trained function of any DLIB library
based regression EP.

In order to enable the event processor, please add LOAD_ML_DLIB_UDEPS=true to your
ONE_TICK_CONFIG file. Please check compatibility to see if
the EP is supported in your distribution.

Python
class name:&nbsp;Ml_dlibRegressionPredict

Input: A time series of ticks

Output: A time series of ticks with additional field which
will carry the predicted value.

Parameters: This EP supports the following set of parameters
(the description of the common parameters can be found here):


  OUTPUT_LABEL
(string)
  INSTANCE_NAME
(string)

Examples: See the predict
example in ml_dlib_classify.otq.


	"""
	class Parameters:
		output_label = "OUTPUT_LABEL"
		instance_name = "INSTANCE_NAME"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["output_label", "instance_name"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["output_label", "_default_output_label", "instance_name", "_default_instance_name", "stack_info", "_used_strings"]

	def __init__(self, output_label="", instance_name=""):
		_graph_components.EpBase.__init__(self, "ML::DLIB_REGRESSION_PREDICT")
		self._default_output_label = ""
		self.output_label = output_label
		self._default_instance_name = ""
		self.instance_name = instance_name
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_output_label(self, value):
		self.output_label = value
		return self

	def set_instance_name(self, value):
		self.instance_name = value
		return self

	@staticmethod
	def _get_name():
		return "ML::DLIB_REGRESSION_PREDICT"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.output_label != "": 
			desc += "OUTPUT_LABEL=" + py_to_str(self.output_label) + ","
		if self.instance_name != "": 
			desc += "INSTANCE_NAME=" + py_to_str(self.instance_name) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class Transpose(_graph_components.EpBase):
	"""
		

TRANSPOSE

Type: Transformer

Description: Transposes the input time series in
rows-to-columns and columns-to-rows directions by outputting certain
input ticks joined with several preceding ones or disjoining ticks to
several small ones. Different options suggest different ways to join
(disjoin) input ticks to form output ticks as well as to form output
field names.

Python
class name:&nbsp;Transpose

Input: A time series of ticks.

Output: A time series of ticks.

Parameters:


  DIRECTION (enumerated)
    This parameter specifies the transposing direction. Possible
options are:

    
      ROWS_TO_COLUMNS Joins certain
input ticks with several preceding ones. After joining, suffixes are
added to field names of the input to make output ones. Order and other
rules of joining are defined by the KEY_TYPE parameter. Output
ticks have the timestamp of the latest input tick forming them.
      COLUMNS_TO_ROWS Disjoins each
input tick to several small ones. Output ticks have the timestamp as
the input tick from which they have been disconnected. Also, if the
input has fields TIMESTAMP_&lt;suffix&gt;, then these values would be
used as timestamps of output ticks. Rules of disjoining are defined by
the KEY_TYPE parameter. The only common restriction to the
input schema for all key types is that input ticks should have only
NAME_&lt;suffix&gt; fields for each NAME and for each &lt;suffix&gt;.
Also, all fields with the same NAME should have the same type and size
(for string fields). Other restrictions to the input schema are
specific for each key type.
Examples of a forbidden input schema:
        [SIZE,PRICE] (no suffix); [SIZE_1,PRICE_2]
(no SIZE_2 and PRICE_1); [STR_A (string[10]), STR_B (string[20])] (STR
fields have different sizes)

        Examples of an allowed input schema (some of this may be
forbidden -- see the KEY_TYPE description below):

        [SIZE_1,PRICE_1];
[SIZE_2,TIMESTAMP_2,TIMESTAMP_1,SIZE_1]; [NAME1_A_B, NAME1_B,
NAME2_A_B, NAME2_B] (in this case suffixes are A_B and B)

      
    
    The event processor (EP) doesn't propagate default ticks.

  
  KEY_TYPE (enumerated)
    This parameter specifies how to form output ticks. Possible
options are:

    
      LAST_N With a rows-to-columns direction
for a specified positive integer N (1&le;N&le;10000), this
option joins every input tick, starting from the Nth, with
preceding N-1 ticks. In a sequence of joined input ticks, field
names of Kth(1&le;K&le;N), in reverse arrival
order, the input tick will be suffixed with "_K" to form output
field names. The later an input tick arrives, the earlier its fields
appear in the output tick. For example, if N is 3 and
input ticks have attributes PRICE, SIZE, then output ticks will have
attributes PRICE_1, SIZE_1, PRICE_2, SIZE_2, and PRICE_3, SIZE_3, where
PRICE_1 will be the PRICE of the third and PRICE_3 will be the PRICE of
the first, in arrival order, with the input tick forming an output one.
With a columns-to-rows direction, input ticks should have
fields with suffixes from 1 to N(1&le;N&le;10000). The event
processor makes N ticks from the first arrival tick (one for each field
set with the same suffix), then starting from the second tick it makes
one tick from each arrival using only fields with suffix 1. In other
words, if the input is transposed in the rows-to-columns direction
last-N key time series, the output of the event processor would be the
initial time series.
      NUMERIC_INDEX For a rows-to-columns
direction, a specified numeric field, called index, is
assumed to sequentially grow from 1 to at most MAX_N
for a specified integer MAX_N (1&le;MAX_N&le;10000), resetting
to 1 afterwards and periodically repeating the process through
the input time series (an example of such a time series would be the
output of the OB_SNAPSHOT EP,
order book level being the index). An output tick is produced once for
each period of the index field (each order book snapshot, in the
example above), at the time index is reset to 1. Field names of
an input tick with index N (1&le;N&le;MAX_N) are suffixed with "_N"
to form output field names. Fields of an input tick with a lower index
appear earlier in the respective output tick. Whenever index does not
reach MAX_N for a given period, output fields for missing
indexes (missing levels in order book snapshot) are filled with default
values. The index field is discarded from the output.
For a columns-to-rows direction for specified positive integer MAX_N
(1&le;MAX_N&le;10000) (can be also empty), input ticks should have
fields NAME1_INDEX1, NAME2_INDEX1,&hellip;,
NAMEM_INDEX1, NAME1_INDEX2,
NAME2_INDEX2,&hellip;, NAMEM_INDEX2,&hellip;,
NAME1_INDEXN, NAME2_INDEXN,&hellip;,
NAMEM_INDEXN in this exact order,
where N (number of different suffixes) is not greater than MAX_N.
In other words, in this case we should assume that the tick schema is
not a set of fields, but a sequence of fields. Use the PASSTHROUGH EP to make the order of
fields admissible for input. The EP makes N ticks from each
arrival tick (one for each field set with the same index). INDEX_FIELD_NAME
should be specified and a numeric field with such a name will be added
to the output and will carry the number of the index (i.e., M
for INDEXM). If SUFFIX_FIELD_NAME is specified,
the string field with such a name will be added to the output and will
carry the index.
      FIELD_VALUE
        With a rows-to-columns direction, the ticks in the
input time series are filtered to leave only those having one of the
specified index values V1, V2, &hellip;, VN in a
specified input field. When index values are not specified, no
tick will be filtered (i.e., the EP's output will be the same as if all
possible values of index were specified).

        The resulting time series is grouped by the input
timestamp and joined within it. First, a particular timestamp
produces M output ticks, where M is the maximum number
of input ticks of that timestamp having index VK for each K
        (1&le;K&le;N). The Jth (1&le;J&le;M)
output tick is composed of input ticks with Jth appearances of
specified values. If a particular value VK appears less than M
times during a particular timestamp, output fields for missing
appearances are filled with the default values. Field names of an input
tick having a value VK are suffixed by "_VK" to form
output field names. The order of input ticks in an output tick conforms
to the order index values are specified (see below). The index field is
discarded from the output. For example, if the input attributes are
PRICE, SIZE, EXCHANGE, the index field is EXCHANGE and index values are
T, N, then output ticks will have attributes PRICE_T, SIZE_T,
PRICE_N, SIZE_N.

        With the columns-to-rows direction, the EP works the
same way as for the numeric index. But the restriction of field order
is released. Also V1, V2, &hellip;, VN parameters can be
specified, which will be used to detect a suffix of a field (i.e., if
the field name contains more than one underscore). For example, for
NAME_A_B, if B is one of the specified values, it will be interpreted
as a suffix of the field name; otherwise, A_B will be interpreted.

      
    
    Default: LAST_N

  
  KEY_CONSTRAINT_VALUES (integer)
    N for LAST_N (can be empty if the direction is
columns-to-rows), MAX_N for NUMERIC_INDEX (can be empty
if the direction is columns-to-rows), and V1, V2, &hellip;, VN
or empty for FIELD_VALUE.
Default: empty

  
  INDEX_FIELD_NAME (string)
    The index field name, used for NUMERIC_INDEX and FIELD_VALUE.
Default: empty

  
  SUFFIX_FIELD_NAME (string)
    Used only for NUMERIC_INDEX, this parameter specifies
the name of an input field, the values of which are used as suffixes to
be appended to input field names to form output ones. It can be useful
in cases when more meaningful suffixes than bare "_K" (see
above) are desired. The suffix field is discarded from the output.
Default: empty

  
  KEEP_INPUT_TIMESTAMPS (Boolean)
    When a rows-to-columns direction specifies whether input
tick timestamps are to be added as columns to output ticks. Used for LAST_N
and NUMERIC_INDEX, since the FIELD_VALUE output tick
has the timestamp of all input ticks forming it.

    The columns-to-rows direction is ignored, and the EP, if
it can detect TIMESTAMP_&lt;suffix&gt; fields, propagates their
values as timestamps of new ticks. Otherwise, it propagates a timestamp
of the input tick as timestamp of each component tick.
Default: true

  
  OUTPUT_TICKS_WITH_ONLY_DFLT_VALS
(enumerated)
    This parameter indicates default tick propagation in case of DIRECTION=COLUMNS_TO_ROWS
and KEY_TYPE=FIELD_VALUE. Possible options are:

    
      empty
If input tick doesn't contain DFLT_VALS_GENERATED_FLAGS field
then default ticks are never propagated.
If input tick contains DFLT_VALS_GENERATED_FLAGS field then
only those default ticks are propagated which corresponding part in
input tick was not autogenerated with default values.
      YES
Default are always propagated.
      NO
Default ticks are never propagated.
    
    Default: empty

  
  ADD_DFLT_VALS_GENERATED_FLAGS
(Boolean)
    If true then in case of DIRECTION=ROWS_TO_COLUMNS and KEY_TYPE=FIELD_VALUE
extra column will be added to output ticks, indicating autogenerated
default ticks.
Default: empty

  

Examples: See OB_snapshot_transposing_examples.otq
and the TRANSPOSE example in transposing_examples.otq.


	"""
	class Parameters:
		direction = "DIRECTION"
		key_type = "KEY_TYPE"
		key_constraint_values = "KEY_CONSTRAINT_VALUES"
		index_field_name = "INDEX_FIELD_NAME"
		suffix_field_name = "SUFFIX_FIELD_NAME"
		keep_input_timestamps = "KEEP_INPUT_TIMESTAMPS"
		output_ticks_with_only_dflt_vals = "OUTPUT_TICKS_WITH_ONLY_DFLT_VALS"
		add_dflt_vals_generated_flags = "ADD_DFLT_VALS_GENERATED_FLAGS"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["direction", "key_type", "key_constraint_values", "index_field_name", "suffix_field_name", "keep_input_timestamps", "output_ticks_with_only_dflt_vals", "add_dflt_vals_generated_flags"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["direction", "_default_direction", "key_type", "_default_key_type", "key_constraint_values", "_default_key_constraint_values", "index_field_name", "_default_index_field_name", "suffix_field_name", "_default_suffix_field_name", "keep_input_timestamps", "_default_keep_input_timestamps", "output_ticks_with_only_dflt_vals", "_default_output_ticks_with_only_dflt_vals", "add_dflt_vals_generated_flags", "_default_add_dflt_vals_generated_flags", "stack_info", "_used_strings"]

	class Direction:
		COLUMNS_TO_ROWS = "COLUMNS_TO_ROWS"
		ROWS_TO_COLUMNS = "ROWS_TO_COLUMNS"

	class KeyType:
		FIELD_VALUE = "FIELD_VALUE"
		LAST_N = "LAST_N"
		NUMERIC_INDEX = "NUMERIC_INDEX"

	class OutputTicksWithOnlyDfltVals:
		EMPTY = ""
		NO = "NO"
		YES = "YES"

	def __init__(self, direction=Direction.ROWS_TO_COLUMNS, key_type=KeyType.LAST_N, key_constraint_values="", index_field_name="", suffix_field_name="", keep_input_timestamps=True, output_ticks_with_only_dflt_vals=OutputTicksWithOnlyDfltVals.EMPTY, add_dflt_vals_generated_flags=False):
		_graph_components.EpBase.__init__(self, "TRANSPOSE")
		self._default_direction = type(self).Direction.ROWS_TO_COLUMNS
		self.direction = direction
		self._default_key_type = type(self).KeyType.LAST_N
		self.key_type = key_type
		self._default_key_constraint_values = ""
		self.key_constraint_values = key_constraint_values
		self._default_index_field_name = ""
		self.index_field_name = index_field_name
		self._default_suffix_field_name = ""
		self.suffix_field_name = suffix_field_name
		self._default_keep_input_timestamps = True
		self.keep_input_timestamps = keep_input_timestamps
		self._default_output_ticks_with_only_dflt_vals = type(self).OutputTicksWithOnlyDfltVals.EMPTY
		self.output_ticks_with_only_dflt_vals = output_ticks_with_only_dflt_vals
		self._default_add_dflt_vals_generated_flags = False
		self.add_dflt_vals_generated_flags = add_dflt_vals_generated_flags
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_direction(self, value):
		self.direction = value
		return self

	def set_key_type(self, value):
		self.key_type = value
		return self

	def set_key_constraint_values(self, value):
		self.key_constraint_values = value
		return self

	def set_index_field_name(self, value):
		self.index_field_name = value
		return self

	def set_suffix_field_name(self, value):
		self.suffix_field_name = value
		return self

	def set_keep_input_timestamps(self, value):
		self.keep_input_timestamps = value
		return self

	def set_output_ticks_with_only_dflt_vals(self, value):
		self.output_ticks_with_only_dflt_vals = value
		return self

	def set_add_dflt_vals_generated_flags(self, value):
		self.add_dflt_vals_generated_flags = value
		return self

	@staticmethod
	def _get_name():
		return "TRANSPOSE"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.direction != self.Direction.ROWS_TO_COLUMNS: 
			desc += "DIRECTION=" + py_to_str(self.direction) + ","
		if self.key_type != self.KeyType.LAST_N: 
			desc += "KEY_TYPE=" + py_to_str(self.key_type) + ","
		if self.key_constraint_values != "": 
			desc += "KEY_CONSTRAINT_VALUES=" + py_to_str(self.key_constraint_values) + ","
		if self.index_field_name != "": 
			desc += "INDEX_FIELD_NAME=" + py_to_str(self.index_field_name) + ","
		if self.suffix_field_name != "": 
			desc += "SUFFIX_FIELD_NAME=" + py_to_str(self.suffix_field_name) + ","
		if self.keep_input_timestamps != True: 
			desc += "KEEP_INPUT_TIMESTAMPS=" + py_to_str(self.keep_input_timestamps) + ","
		if self.output_ticks_with_only_dflt_vals != self.OutputTicksWithOnlyDfltVals.EMPTY: 
			desc += "OUTPUT_TICKS_WITH_ONLY_DFLT_VALS=" + py_to_str(self.output_ticks_with_only_dflt_vals) + ","
		if self.add_dflt_vals_generated_flags != False: 
			desc += "ADD_DFLT_VALS_GENERATED_FLAGS=" + py_to_str(self.add_dflt_vals_generated_flags) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class VirtualOb(_graph_components.EpBase):
	"""
		

VIRTUAL_OB

Type: Transformer

Description: Creates a series of fake orders from a time
series of best bids and asks. The algorithm used is as follows: For a
tick with the best bid or ask, create an order tick adding the new best
bid or ask and an order withdrawing the old one. Virtual order books
can be created for multiple subgroups at once by using the QUOTE_SOURCE
parameter to specify a list of string fields to be used for grouping. A
separate book will be created for each combination.

Typically this would be followed by an order book processor such as OB_SNAPSHOT.

Python
class name:&nbsp;VirtualOb

Input: A time series of ticks with fields BID_PRICE,
BID_SIZE, ASK_PRICE, and ASK_SIZE.

Output: A time series of ticks.

Parameters:


  QUOTE_SOURCE_FIELDS (string [,
string]|)
    Specifies a list of string fields for grouping quotes. The
virtual
order book is then constructed for each subgroup separately and the SOURCE
field is constructed to contain the description of the group.
Default: empty

  
  QUOTE_TIMEOUT (seconds)
    Specifies
the maximum age of a quote that is not stale. A quote that is not
replaced after more than QUOTE_TIMEOUT seconds is considered stale, and
delete orders will be generated for it.

    A value of QUOTE_TIMEOUT can be fractional (for example, 3.51)
Default: empty (a quote is never considered stale)

  
  SHOW_FULL_DETAIL (Boolean)
    If set to TRUE, VIRTUAL_OB
will attempt to include all fields in the input tick when forming the
output tick. 
    ASK_X/BID_X fields will combine under paired field X
    
    ASK_X and BID_X must have the same type. 
If only ASK_X or BID_X exist, output will have X
and the missing field will be assumed to have its default value

    Paired and non-paired fields must not interfere with each other
and the fields originally added by this EP

Default: false
  OUTPUT_BOOK_FORMAT (string)
    Supported values are PRL and OB. When set to PRL, field SIZE of
output ticks represents current size for the tick's source, price, and
side, and the EP propagates PRICE and SOURCE as the state keys of its
output time series. When set to OB, field SIZE of output ticks
represents the delta of size&nbsp;for the tick's source, price, and
side, and the state key of the output ticks is empty.&nbsp;

Default: OB

Examples: Creates an imaginary stream of orders, which would
have resulted, into the current stream of best bids and asks for each
exchange.

VIRTUAL_OB (EXCHANGE)

See the VIRTUAL_OB_EXAMPLE
example in TRANSFORMER_EXAMPLES.otq?VIRTUAL_OB_EXAMPLE.


	"""
	class Parameters:
		quote_source_fields = "QUOTE_SOURCE_FIELDS"
		quote_timeout = "QUOTE_TIMEOUT"
		show_full_detail = "SHOW_FULL_DETAIL"
		output_book_format = "OUTPUT_BOOK_FORMAT"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["quote_source_fields", "quote_timeout", "show_full_detail", "output_book_format"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["quote_source_fields", "_default_quote_source_fields", "quote_timeout", "_default_quote_timeout", "show_full_detail", "_default_show_full_detail", "output_book_format", "_default_output_book_format", "stack_info", "_used_strings"]

	class OutputBookFormat:
		OB = "OB"
		PRL = "PRL"

	def __init__(self, quote_source_fields="", quote_timeout="", show_full_detail=False, output_book_format=OutputBookFormat.OB):
		_graph_components.EpBase.__init__(self, "VIRTUAL_OB")
		self._default_quote_source_fields = ""
		self.quote_source_fields = quote_source_fields
		self._default_quote_timeout = ""
		self.quote_timeout = quote_timeout
		self._default_show_full_detail = False
		self.show_full_detail = show_full_detail
		self._default_output_book_format = type(self).OutputBookFormat.OB
		self.output_book_format = output_book_format
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_quote_source_fields(self, value):
		self.quote_source_fields = value
		return self

	def set_quote_timeout(self, value):
		self.quote_timeout = value
		return self

	def set_show_full_detail(self, value):
		self.show_full_detail = value
		return self

	def set_output_book_format(self, value):
		self.output_book_format = value
		return self

	@staticmethod
	def _get_name():
		return "VIRTUAL_OB"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.quote_source_fields != "": 
			desc += "QUOTE_SOURCE_FIELDS=" + py_to_str(self.quote_source_fields) + ","
		if self.quote_timeout != "": 
			desc += "QUOTE_TIMEOUT=" + py_to_str(self.quote_timeout) + ","
		if self.show_full_detail != False: 
			desc += "SHOW_FULL_DETAIL=" + py_to_str(self.show_full_detail) + ","
		if self.output_book_format != self.OutputBookFormat.OB: 
			desc += "OUTPUT_BOOK_FORMAT=" + py_to_str(self.output_book_format) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class BookDiff(_graph_components.EpBase):
	"""
		

BOOK_DIFF

Type: Transformer

Description: Performs book diffing for every pair of
consecutive ticks, each representing a flat book of a fixed depth.

This EP can be thought to be an operation, opposite to OB_SNAPSHOT_FLAT EP. Every input tick,
different from the previous one, results in a series of output PRL
ticks to be propagated, each carrying information about a level
deletion, addition or update.

Python
class name:
BookDiff

Input: A time series of flat book ticks, just like the result
of OB_SNAPSHOT_FLAT EP.
More precisely, for a fixed depth N
of the flat book, input ticks should carry the fields BID_&lt;FIELD_NAME&gt;K and ASK_&lt;FIELD_NAME&gt;K, where 1 &lt;= K &lt;= N and &lt;FIELD_NAME&gt; ranges over a specific
set of fields F, among which PRICE and SIZE
are mandatory. 

Tick descriptor changes are not allowed in this event
processor.

Output A time series of PRL ticks, carrying information about
a level deletion, addition, or update.
Each tick carries the fields from the above-mentioned set F, plus BUY_SELL_FLAG,
RECORD_TYPE, TICK_STATUS, and DELETED_TIME. BUY_SELL_FLAG
carries the side (0 - bid, 1 - ask), the rest are for internal use.

Parameters:&nbsp;


  INCLUDE_INITIAL_BOOK (Boolean) If
set to true, the output of this EP begins with an initial order book,
which consists of a sequence of ticks that have&nbsp;RECORD_TYPE field
values, B, Z, R, R, R, ..., E.&nbsp;

Example:

See BOOK_DIFF_EXAMPLE.otq.


	"""
	class Parameters:
		include_initial_book = "INCLUDE_INITIAL_BOOK"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["include_initial_book"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["include_initial_book", "_default_include_initial_book", "stack_info", "_used_strings"]

	def __init__(self, include_initial_book=False):
		_graph_components.EpBase.__init__(self, "BOOK_DIFF")
		self._default_include_initial_book = False
		self.include_initial_book = include_initial_book
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_include_initial_book(self, value):
		self.include_initial_book = value
		return self

	@staticmethod
	def _get_name():
		return "BOOK_DIFF"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.include_initial_book != False: 
			desc += "INCLUDE_INITIAL_BOOK=" + py_to_str(self.include_initial_book) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class Distinct(_graph_components.EpBase):
	"""
		
    
      DISTINCT
    
    Type: Aggregation

    
      Description: Outputs all distinct values for a specified set of key
      fields.
    

    
      Python class name:&nbsp;Distinct
    

    Input: A time series of ticks.

    
      Output: A time series of ticks, one tick for each distinct value
      found.
    

    
      Parameters: See
      parameters common to generic aggregations.
    

    
      
        BUCKET_INTERVAL
        (seconds/ticks)
      
      
        BUCKET_INTERVAL_UNITS
        (enumerated type)
      
      
        OUTPUT_INTERVAL
        (seconds)
      
      
        OUTPUT_INTERVAL_UNITS
        (SECONDS/TICKS)
      
      
        IS_RUNNING_AGGR
        (Boolean)
      
      
        BUCKET_TIME
        (Boolean)
      
      
        BUCKET_END_CRITERIA
        (expression)
      
      
        BOUNDARY_TICK_BUCKET
        (NEW/PREVIOUS)
      
      
        PARTIAL_BUCKET_HANDLING
        (enumerated type)
      
      
        KEYS (string)
          
            Specifies a list of tick attributes for which unique values are
            found. The ticks in the input time series must contain those
            attributes.
          

        
      
      
        KEY_ATTRS_ONLY (Boolean)
        
          If set to true, output ticks will contain only key fields. Otherwise,
          output ticks will contain all fields of an input tick in which a given
          distinct combination of key values was first encountered.
          Default: true
        

      
      
        SELECTION (enumerated type)
        
          The two allowed values, FIRST and LAST, of this parameter determine
          whether distinct values will be selected from the beginning or from
          the end of the bucket.
          Default: FIRST
        

      
      
        SHOW_ONLY_CHANGES (Boolean)
        
          If set to true, only the first occurrence of specific values of keys
          will be reported when IS_RUNNING_AGGR=true. SELECTION must be set to
          FIRST when this flag is set to true.
          Default: false
        

      
      
        ADD_TICK_TIME (Boolean)
        
          If set to true, add TICK_TIME field to the output.
          Default: false
        

      
      
        PRODUCE_ONE_TICK_FOR_ALL_KEY_VALUES (Boolean)
        
          When set to true, it produces a single tick containing all unique
          values for the specified key field. These values will be
          pipe-separated (|). The output type for this field is varstring. This
          parameter requires that the KEYS parameter specifies exactly one
          field. Additionally, it only works when KEY_ATTRS_ONLY is also set.
          Default: false
        

      
    
    
      Notes: See the
      notes on generic aggregations.
    

    Examples:

    
      Find all exchanges that reported trades for a given symbol within every 5
      minute bucket:
    

    DISTINCT (BUCKET_INTERVAL="300",KEYS="EXCHANGE");TRD

    
      See the DISTINCT example in
      AGGREGATION_EXAMPLES.otq.
    

    
	"""
	class Parameters:
		bucket_interval = "BUCKET_INTERVAL"
		bucket_interval_units = "BUCKET_INTERVAL_UNITS"
		output_interval = "OUTPUT_INTERVAL"
		output_interval_units = "OUTPUT_INTERVAL_UNITS"
		is_running_aggr = "IS_RUNNING_AGGR"
		bucket_time = "BUCKET_TIME"
		bucket_end_criteria = "BUCKET_END_CRITERIA"
		boundary_tick_bucket = "BOUNDARY_TICK_BUCKET"
		partial_bucket_handling = "PARTIAL_BUCKET_HANDLING"
		keys = "KEYS"
		key_attrs_only = "KEY_ATTRS_ONLY"
		selection = "SELECTION"
		show_only_changes = "SHOW_ONLY_CHANGES"
		add_tick_time = "ADD_TICK_TIME"
		produce_one_tick_for_all_key_values = "PRODUCE_ONE_TICK_FOR_ALL_KEY_VALUES"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["bucket_interval", "bucket_interval_units", "output_interval", "output_interval_units", "is_running_aggr", "bucket_time", "bucket_end_criteria", "boundary_tick_bucket", "partial_bucket_handling", "keys", "key_attrs_only", "selection", "show_only_changes", "add_tick_time", "produce_one_tick_for_all_key_values"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["bucket_interval", "_default_bucket_interval", "bucket_interval_units", "_default_bucket_interval_units", "output_interval", "_default_output_interval", "output_interval_units", "_default_output_interval_units", "is_running_aggr", "_default_is_running_aggr", "bucket_time", "_default_bucket_time", "bucket_end_criteria", "_default_bucket_end_criteria", "boundary_tick_bucket", "_default_boundary_tick_bucket", "partial_bucket_handling", "_default_partial_bucket_handling", "keys", "_default_keys", "key_attrs_only", "_default_key_attrs_only", "selection", "_default_selection", "show_only_changes", "_default_show_only_changes", "add_tick_time", "_default_add_tick_time", "produce_one_tick_for_all_key_values", "_default_produce_one_tick_for_all_key_values", "stack_info", "_used_strings"]

	class BucketIntervalUnits:
		DAYS = "DAYS"
		FLEXIBLE = "FLEXIBLE"
		MONTHS = "MONTHS"
		SECONDS = "SECONDS"
		TICKS = "TICKS"

	class OutputIntervalUnits:
		SECONDS = "SECONDS"
		TICKS = "TICKS"

	class BucketTime:
		BUCKET_END = "BUCKET_END"
		BUCKET_START = "BUCKET_START"

	class BoundaryTickBucket:
		NEW = "NEW"
		PREVIOUS = "PREVIOUS"

	class PartialBucketHandling:
		AS_SEPARATE_BUCKET = "AS_SEPARATE_BUCKET"
		DISCARD = "DISCARD"
		MERGE_WITH_PREVIOUS = "MERGE_WITH_PREVIOUS"

	class Selection:
		FIRST = "FIRST"
		LAST = "LAST"

	def __init__(self, bucket_interval=0, bucket_interval_units=BucketIntervalUnits.SECONDS, output_interval="", output_interval_units=OutputIntervalUnits.SECONDS, is_running_aggr=False, bucket_time=BucketTime.BUCKET_END, bucket_end_criteria="", boundary_tick_bucket=BoundaryTickBucket.NEW, partial_bucket_handling=PartialBucketHandling.AS_SEPARATE_BUCKET, keys="", key_attrs_only=True, selection=Selection.FIRST, show_only_changes=False, add_tick_time=False, produce_one_tick_for_all_key_values=False):
		_graph_components.EpBase.__init__(self, "DISTINCT")
		self._default_bucket_interval = 0
		self.bucket_interval = bucket_interval
		self._default_bucket_interval_units = type(self).BucketIntervalUnits.SECONDS
		self.bucket_interval_units = bucket_interval_units
		self._default_output_interval = ""
		self.output_interval = output_interval
		self._default_output_interval_units = type(self).OutputIntervalUnits.SECONDS
		self.output_interval_units = output_interval_units
		self._default_is_running_aggr = False
		self.is_running_aggr = is_running_aggr
		self._default_bucket_time = type(self).BucketTime.BUCKET_END
		self.bucket_time = bucket_time
		self._default_bucket_end_criteria = ""
		self.bucket_end_criteria = bucket_end_criteria
		self._default_boundary_tick_bucket = type(self).BoundaryTickBucket.NEW
		self.boundary_tick_bucket = boundary_tick_bucket
		self._default_partial_bucket_handling = type(self).PartialBucketHandling.AS_SEPARATE_BUCKET
		self.partial_bucket_handling = partial_bucket_handling
		self._default_keys = ""
		self.keys = keys
		self._default_key_attrs_only = True
		self.key_attrs_only = key_attrs_only
		self._default_selection = type(self).Selection.FIRST
		self.selection = selection
		self._default_show_only_changes = False
		self.show_only_changes = show_only_changes
		self._default_add_tick_time = False
		self.add_tick_time = add_tick_time
		self._default_produce_one_tick_for_all_key_values = False
		self.produce_one_tick_for_all_key_values = produce_one_tick_for_all_key_values
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_bucket_interval(self, value):
		self.bucket_interval = value
		return self

	def set_bucket_interval_units(self, value):
		self.bucket_interval_units = value
		return self

	def set_output_interval(self, value):
		self.output_interval = value
		return self

	def set_output_interval_units(self, value):
		self.output_interval_units = value
		return self

	def set_is_running_aggr(self, value):
		self.is_running_aggr = value
		return self

	def set_bucket_time(self, value):
		self.bucket_time = value
		return self

	def set_bucket_end_criteria(self, value):
		self.bucket_end_criteria = value
		return self

	def set_boundary_tick_bucket(self, value):
		self.boundary_tick_bucket = value
		return self

	def set_partial_bucket_handling(self, value):
		self.partial_bucket_handling = value
		return self

	def set_keys(self, value):
		self.keys = value
		return self

	def set_key_attrs_only(self, value):
		self.key_attrs_only = value
		return self

	def set_selection(self, value):
		self.selection = value
		return self

	def set_show_only_changes(self, value):
		self.show_only_changes = value
		return self

	def set_add_tick_time(self, value):
		self.add_tick_time = value
		return self

	def set_produce_one_tick_for_all_key_values(self, value):
		self.produce_one_tick_for_all_key_values = value
		return self

	@staticmethod
	def _get_name():
		return "DISTINCT"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.bucket_interval != 0: 
			desc += "BUCKET_INTERVAL=" + py_to_str(self.bucket_interval) + ","
		if self.bucket_interval_units != self.BucketIntervalUnits.SECONDS: 
			desc += "BUCKET_INTERVAL_UNITS=" + py_to_str(self.bucket_interval_units) + ","
		if self.output_interval != "": 
			desc += "OUTPUT_INTERVAL=" + py_to_str(self.output_interval) + ","
		if self.output_interval_units != self.OutputIntervalUnits.SECONDS: 
			desc += "OUTPUT_INTERVAL_UNITS=" + py_to_str(self.output_interval_units) + ","
		if self.is_running_aggr != False: 
			desc += "IS_RUNNING_AGGR=" + py_to_str(self.is_running_aggr) + ","
		if self.bucket_time != self.BucketTime.BUCKET_END: 
			desc += "BUCKET_TIME=" + py_to_str(self.bucket_time) + ","
		if self.bucket_end_criteria != "": 
			desc += "BUCKET_END_CRITERIA=" + py_to_str(self.bucket_end_criteria) + ","
		if self.boundary_tick_bucket != self.BoundaryTickBucket.NEW: 
			desc += "BOUNDARY_TICK_BUCKET=" + py_to_str(self.boundary_tick_bucket) + ","
		if self.partial_bucket_handling != self.PartialBucketHandling.AS_SEPARATE_BUCKET: 
			desc += "PARTIAL_BUCKET_HANDLING=" + py_to_str(self.partial_bucket_handling) + ","
		if self.keys != "": 
			desc += "KEYS=" + py_to_str(self.keys) + ","
		if self.key_attrs_only != True: 
			desc += "KEY_ATTRS_ONLY=" + py_to_str(self.key_attrs_only) + ","
		if self.selection != self.Selection.FIRST: 
			desc += "SELECTION=" + py_to_str(self.selection) + ","
		if self.show_only_changes != False: 
			desc += "SHOW_ONLY_CHANGES=" + py_to_str(self.show_only_changes) + ","
		if self.add_tick_time != False: 
			desc += "ADD_TICK_TIME=" + py_to_str(self.add_tick_time) + ","
		if self.produce_one_tick_for_all_key_values != False: 
			desc += "PRODUCE_ONE_TICK_FOR_ALL_KEY_VALUES=" + py_to_str(self.produce_one_tick_for_all_key_values) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class InsertTick(_graph_components.EpBase):
	"""
		

INSERT_TICK

Type: Other

Description: Generates an output series based on the field
values of the input series.

Python
class name: InsertTick

Input: A time series of ticks.

Output: A time series of ticks.

Parameters:


  FIELDS (comma-separated list of output fields
defined as expressions)
	If not is not specified, EP will produce 0 initialized tick or ticks (depending on the NUM_TICKS_TO_INSERT parameter value) with input tick schema.

    If specified, it and must be set in the following
form: FIELD_1 [TYPE_1]=EXPR_1,FIELD_2 [TYPE_2]=EXPR_2,&hellip;, FIELD_N
[TYPE_N]=EXPR_N, where FIELD_1,FIELD_2,&hellip;,FIELD_N must be valid
field names and can be present in the input time series.
The special field name TIMESTAMP can be used to define timestamps of
output ticks. Supported types are listed here. If
the type of a field is omitted, it is deduced from the return type of
the respective expression. Otherwise, the return type of the respective
expression must be castable to the specified one.

    EXPR_1,EXPR_2,&hellip;,EXPR_N must be numeric or string
expressions built with boolean operators AND, OR, and NOT. An
expression accepts arithmetic operators: +, -, *, / and comparison
operators =, !=, &lt;, &lt;=, &gt;, &gt;=. Field names are not quoted,
are case sensitive, and have to be present in the input time series.
TIMESTAMP can be used to represent the input tick timestamp. String
literals must be surrounded by single or double quotes.

    In addition, expression accepts the following functions: power,
sqrt, log, log10, exp, and others (see Catalog
of Built-in functions for details).

    Fields of preceding (those that already arrived) ticks can also
be used in computations, in which the case field name is followed by a
negative integer index in square brackets specifying how far to look
back (e.g., TRD.PRICE[-1], TRD.SIZE[-3], etc.).

    State variables may also be involved in computations. State
variables are those starting with prefix "STATE::" and can have two
types: namely, double precision number and string. State variables have
a branch scope in queries. The built-in function UNDEFINED (see Catalog of Built-in functions) can be used to
test whether or not a state variable with a given name is defined and
initialized (for example, UNDEFINED("STATE::BEST_PRICE")).
See the UPDATE_FIELD event processor
for details about how the value of a state variable can be modified.

  
  WHERE (expression)
    Specifies a criterion for the selection of ticks whose arrival
results in an output tick generation. It can either be an expression of
the form above, or empty, in which case each input tick generates an
output one.

  
  PRESERVE_INPUT_TICKS (Boolean)
    A switch controlling whether input ticks have to be preserved in
output time series or not. While the former case results in fields of
input ticks to be present in the output time series together with those
defined by the FIELDS parameter, the latter case results in only
defined fields to be present. If a field of the input time series is
defined in the FIELDS parameter, the defined value takes precedence.

  
  NUM_TICKS_TO_INSERT (integer)
    Specifies the number of generated output ticks for each input
tick that should generate an output.
Default: 1

  
  INSERT_BEFORE (Boolean)
    If true, generated ticks for
each input tick will be inserted before that input tick in output time
series, otherwise generated ticks will be inserted after input tick.
Default: true

  

Example: Use this EP to add a tick with Over 1000 value in the FLAG field after each tick which SIZE is bigger than 1000:

INSERT_TICK(FIELDS="FLAG=\\"Over
1000\\"",WHERE="SIZE&gt;1000")

See the INSERT_TICK example in OTHER_EXAMPLES.otq.


	"""
	class Parameters:
		fields = "FIELDS"
		where = "WHERE"
		preserve_input_ticks = "PRESERVE_INPUT_TICKS"
		num_ticks_to_insert = "NUM_TICKS_TO_INSERT"
		insert_before = "INSERT_BEFORE"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["fields", "where", "preserve_input_ticks", "num_ticks_to_insert", "insert_before"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["fields", "_default_fields", "where", "_default_where", "preserve_input_ticks", "_default_preserve_input_ticks", "num_ticks_to_insert", "_default_num_ticks_to_insert", "insert_before", "_default_insert_before", "stack_info", "_used_strings"]

	def __init__(self, fields="", where="", preserve_input_ticks=True, num_ticks_to_insert=1, insert_before=True):
		_graph_components.EpBase.__init__(self, "INSERT_TICK")
		self._default_fields = ""
		self.fields = fields
		self._default_where = ""
		self.where = where
		self._default_preserve_input_ticks = True
		self.preserve_input_ticks = preserve_input_ticks
		self._default_num_ticks_to_insert = 1
		self.num_ticks_to_insert = num_ticks_to_insert
		self._default_insert_before = True
		self.insert_before = insert_before
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_fields(self, value):
		self.fields = value
		return self

	def set_where(self, value):
		self.where = value
		return self

	def set_preserve_input_ticks(self, value):
		self.preserve_input_ticks = value
		return self

	def set_num_ticks_to_insert(self, value):
		self.num_ticks_to_insert = value
		return self

	def set_insert_before(self, value):
		self.insert_before = value
		return self

	@staticmethod
	def _get_name():
		return "INSERT_TICK"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.fields != "": 
			desc += "FIELDS=" + py_to_str(self.fields) + ","
		if self.where != "": 
			desc += "WHERE=" + py_to_str(self.where) + ","
		if self.preserve_input_ticks != True: 
			desc += "PRESERVE_INPUT_TICKS=" + py_to_str(self.preserve_input_ticks) + ","
		if self.num_ticks_to_insert != 1: 
			desc += "NUM_TICKS_TO_INSERT=" + py_to_str(self.num_ticks_to_insert) + ","
		if self.insert_before != True: 
			desc += "INSERT_BEFORE=" + py_to_str(self.insert_before) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class InsertDataQualityEvent(_graph_components.EpBase):
	"""
		

INSERT_DATA_QUALITY_EVENT

Type: Other

Description: Generates an output series based on the field
values of the input series.

Python
class name:&nbsp;InsertDataQualityEvent

Input: A time series of ticks.

Output: A time series of ticks.

Parameters:


  DATA_QUALITY (string)
    Type of data quality event. Must be one of the supported data
quality event types

    
  
  WHERE (expression)
    Specifies a criterion for the selection of ticks whose arrival
results in generation of a data quality event. It is a logical
expression, the format of which is described in LogicalExpressions.htm. If this
expression returns true, a data quality event will be inserted into a
time series. The expression can also be empty, in which case each input
tick generates a data quality event.

  
  INSERT_BEFORE (Boolean)
    If true, generated data
quality event tick will be inserted before the input tick that
triggered its creation, otherwise generated data quality event will be
inserted after that input tick.
Default: true

  

Example: Use this EP to insert data quality event 'MISSING'
before each tick with SIZE bigger
than 1000:

INSERT_DATA_QUALITY_EVENT(DATA_QUALITY="MISSING",WHERE="SIZE&gt;1000")


	"""
	class Parameters:
		data_quality = "DATA_QUALITY"
		where = "WHERE"
		insert_before = "INSERT_BEFORE"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["data_quality", "where", "insert_before"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["data_quality", "_default_data_quality", "where", "_default_where", "insert_before", "_default_insert_before", "stack_info", "_used_strings"]

	class DataQuality:
		EMPTY = ""
		COLLECTOR_FAILURE = "COLLECTOR_FAILURE"
		MISSING = "MISSING"
		MOUNT_BAD = "MOUNT_BAD"
		OK = "OK"
		PATCHED = "PATCHED"
		QUALITY_DELAY_STITCHING_WITH_RT = "QUALITY_DELAY_STITCHING_WITH_RT"
		QUALITY_OK_STITCHING_WITH_RT = "QUALITY_OK_STITCHING_WITH_RT"
		STALE = "STALE"

	def __init__(self, data_quality=DataQuality.EMPTY, where="", insert_before=True):
		_graph_components.EpBase.__init__(self, "INSERT_DATA_QUALITY_EVENT")
		self._default_data_quality = type(self).DataQuality.EMPTY
		self.data_quality = data_quality
		self._default_where = ""
		self.where = where
		self._default_insert_before = True
		self.insert_before = insert_before
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_data_quality(self, value):
		self.data_quality = value
		return self

	def set_where(self, value):
		self.where = value
		return self

	def set_insert_before(self, value):
		self.insert_before = value
		return self

	@staticmethod
	def _get_name():
		return "INSERT_DATA_QUALITY_EVENT"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.data_quality != self.DataQuality.EMPTY: 
			desc += "DATA_QUALITY=" + py_to_str(self.data_quality) + ","
		if self.where != "": 
			desc += "WHERE=" + py_to_str(self.where) + ","
		if self.insert_before != True: 
			desc += "INSERT_BEFORE=" + py_to_str(self.insert_before) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class InsertHeartbeat(_graph_components.EpBase):
	"""
		

INSERT_HEARTBEAT

Type: Other

Description: Generates an output series that is an input time
series with some timer events (heartbeats) injected into it.

Python
class name:&nbsp;InsertHeartbeat

Input: A time series of ticks.

Output: A time series of ticks.

Parameters:


  WHERE (expression)
    Specifies a criterion for the selection of ticks whose arrival
results in generation of a timer event. It is a logical expression, the
format of which is described in LogicalExpressions.htm.
If this expression returns true, a timer event will be inserted into a
time series. The expression can also be empty, in which case each input
tick generates a heartbeat.

  
  INSERT_BEFORE (Boolean)
    If true, generated heartbeat
will be inserted before the input tick that triggered its creation,
otherwise generated heartbeat will be inserted after that input tick.
Default: true

  

Example: Use this EP to insert a heartbeat before each tick
with SIZE bigger than 1000:

INSERT_HEARTBEAT(WHERE="SIZE&gt;1000")


	"""
	class Parameters:
		where = "WHERE"
		insert_before = "INSERT_BEFORE"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["where", "insert_before"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["where", "_default_where", "insert_before", "_default_insert_before", "stack_info", "_used_strings"]

	def __init__(self, where="", insert_before=True):
		_graph_components.EpBase.__init__(self, "INSERT_HEARTBEAT")
		self._default_where = ""
		self.where = where
		self._default_insert_before = True
		self.insert_before = insert_before
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_where(self, value):
		self.where = value
		return self

	def set_insert_before(self, value):
		self.insert_before = value
		return self

	@staticmethod
	def _get_name():
		return "INSERT_HEARTBEAT"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.where != "": 
			desc += "WHERE=" + py_to_str(self.where) + ","
		if self.insert_before != True: 
			desc += "INSERT_BEFORE=" + py_to_str(self.insert_before) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class CreateTicksFromMatrix(_graph_components.EpBase):
	"""
		

CREATE_TICKS_FROM_MATRIX

Type: Transformer

Description: Modifies the input tick series by splitting the
specified matrix fields into lower dimensional pieces, each becoming
separate tick.

Python
class name:&nbsp;CreateTicksFromMatrix

Input: A time series of ticks, that contains matrix field(s).

Output: A time series of ticks.

Parameters:


  FIELDS - The name of the matrix fields that
should be split.
  DIMENSIONS_TO_REDUCE - The
number of dimensions of the input matrix that will be reduced. For
example, if 1 is specified and the input field contains 3-dimensional
matrices, the output will be 2-dimensional matrices.

Example:

See MATRICES_EXAMPLES.otq.


	"""
	class Parameters:
		fields = "FIELDS"
		dimensions_to_reduce = "DIMENSIONS_TO_REDUCE"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["fields", "dimensions_to_reduce"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["fields", "_default_fields", "dimensions_to_reduce", "_default_dimensions_to_reduce", "stack_info", "_used_strings"]

	def __init__(self, fields="", dimensions_to_reduce=1):
		_graph_components.EpBase.__init__(self, "CREATE_TICKS_FROM_MATRIX")
		self._default_fields = ""
		self.fields = fields
		self._default_dimensions_to_reduce = 1
		self.dimensions_to_reduce = dimensions_to_reduce
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_fields(self, value):
		self.fields = value
		return self

	def set_dimensions_to_reduce(self, value):
		self.dimensions_to_reduce = value
		return self

	@staticmethod
	def _get_name():
		return "CREATE_TICKS_FROM_MATRIX"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.fields != "": 
			desc += "FIELDS=" + py_to_str(self.fields) + ","
		if self.dimensions_to_reduce != 1: 
			desc += "DIMENSIONS_TO_REDUCE=" + py_to_str(self.dimensions_to_reduce) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class SaveSnapshot(_graph_components.EpBase):
	"""
		

SAVE_SNAPSHOT

Type: Other

Description: Saves last (at most) n ticks of each
group of ticks from the input time series in global storage or in a
memory mapped file under a specified snapshot name. Tick descriptor
should be the same for all ticks saved into the snapshot. These ticks
can then be read via the READ_SNAPSHOT
event processor by using the name of the snapshot and the same symbol
name (&lt;db_name&gt;::&lt;symbol&gt;)
that were used by the SAVE_SNAPSHOT.

Python
class name:&nbsp;SaveSnapshot

Input: A time series of ticks.

Output: None

Parameters:


  SNAPSHOT_NAME (string)
    The name of the snapshot, can be any string which doesn't
contain slashes or backslashes. Two snapshots can have the same name if
they are stored in memory mapped files for different databases. Also,
they can have the same names if they are stored in the memories of
different processes (different tick_servers). In all other cases the
names should be unique.
Default: VALUE

  
  SNAPSHOT_STORAGE (enumerated)
    This parameter specifies the place in which to store the
snapshot. Possible options are:

    
      MEMORY
        The snapshot will be stored in dynamic (heap) memory of the
process that executes SAVE_SNAPSHOT EP (tick_server in most cases).

      
      MEMORY_MAPPED_FILE
        The snapshot will be stored in a memory mapped file. For
each symbol to determine the location of the memory mapped file,
SAVE_SNAPSHOT looks at the SAVE_SNAPSHOT_DIR parameter in the locator
section for the database of the symbol. In a specified directory it
creates a new directory with the name of the snapshot and keeps the
memory mapped file and some other helper files there.

      
    
Default: MEMORY
  DEFAULT_DB (string)
    The ticks with empty symbol names or symbol names with no
database name as a prefix are saved as if they have symbol names equal
to DEFAULT_DB::SYMBOL_NAME (where SYMBOL_NAME can be
empty). These kinds of ticks, for example, can appear after merging
time series. To save/read these ticks to/from storage a dummy database
with the specified default name should be configured in the locator.
Default: CEP_SNAPSHOT

  
  DATABASE (string)
    Specifies the output database for saving the snapshot.
Default: &lt;empty&gt;

  
  SYMBOL_NAME_FIELD (string)
    If this parameter is specified, then each input time series is
assumed to be a union of several time series and the value of the
specified attribute of each tick determines to which time series the
tick actually belongs. These values should be pure symbol names (for
instance if the tick belongs to the time series DEMO_L1::A,
then the value of the corresponding attribute should be A) and
the database name will be taken from symbol of the merged time series.
Default: &lt;empty&gt;

  
  EXPECTED_SYMBOLS_PER_TIME_SERIES
(integer)
    This parameter makes sense only when SYMBOL_NAME_FIELD
is specified. It is the number of real symbols that are expected to
occur per input time series. Bigger numbers may result in larger memory
utilization by the query but will make the query faster.
Default: 1000

  
  NUM_TICKS (integer)
    The number of ticks to be stored for each group per each symbol.
Default: 1

  
  REREAD_PREVENTION_LEVEL
(integer)
    For better performance we do not use synchronization mechanisms
between the snapshot writer[s] and reader[s]. That is why when the
writer submits ticks for some symbol very quickly the reader may fail
to read those ticks and it will keep trying to reread them until it
succeeds. The REREAD_PREVENTION_LEVEL parameter addresses this problem.
The higher the reread prevention level the higher the chance for the
reader to read ticks successfully. But high prevention level also means
high memory utilization, that is why it is recommended to keep the
value of this parameter unchanged until you get an error about
inability of the reader to read the snapshot due to fast writer.
Default: 1

  
  GROUP_BY (string[,string [...]])
    When specified, the EP will keep the last n ticks of
each group for each symbol; otherwise it will just keep the last n
ticks of the input time series. The group is a list of input ticks with
the same values in the specified fields.
Default: &lt;empty&gt;

  
  EXPECTED_GROUPS_PER_SYMBOL
(integer)
    The number of expected groups of ticks for each time series. The
specified value is used only when GROUP_BY fields are
specified, otherwise it is ignored and we assume that the number of
expected groups is 1. The number hints the EP to allocate
memory for such number of tick groups each time a new group of ticks is
going to be created and no free memory is left.
Default: 10

  
  KEEP_SNAPSHOT_AFTER_QUERY
(Boolean)
    If the snapshot is saved in process memory and this parameter is
set, the saved snapshot continues to live after the query ends. If this
parameter is not set, the snapshot is removed as soon as the query
finishes and its name is released for saving new snapshots with the
same name. This parameter is ignored if the snapshot is saved in the
memory mapped file.
Default: false

  
  ALLOW_CONCURRENT_WRITERS
(Boolean)
    If this parameter is true multiple saver queries can
write to the same snapshot contemporaneously. But different writers
should write to different time series. Also, saver queries should run
inside the same process (i.e., different tick servers or loaders with
otq transformers cannot write to the same MEMORY_MAPPED_FILE
snapshot concurrently).
Default: false

  
  REMOVE_SNAPSHOT_UPON_START
(Boolean)
    If this parameter is true the snapshot will be removed
at the beginning of the query the next time SAVE_SNAPSHOT is called for
the same snapshot. If the parameter is false the snapshot with
the specified name will be appended to upon the next run of
SAVE_SNAPSHOT. There is also the NOT_SET option that operates
in the same way as true for MEMORY
snapshots or false for MEMORY_MAPPED_FILE
snapshots.
Default: NOT_SET

  

Access control: The event processor cannot be used by
default. To enable it, access control should be used.

Examples: Saves last 100 ticks under the name SNAPSHOT1 in
memory:

SAVE_SNAPSHOT (SNAPSHOT1, 100)

See the SAVE_SNAPSHOT example in OTHER_EXAMPLES.otq.


	"""
	class Parameters:
		snapshot_name = "SNAPSHOT_NAME"
		snapshot_storage = "SNAPSHOT_STORAGE"
		default_db = "DEFAULT_DB"
		database = "DATABASE"
		symbol_name_field = "SYMBOL_NAME_FIELD"
		expected_symbols_per_time_series = "EXPECTED_SYMBOLS_PER_TIME_SERIES"
		num_ticks = "NUM_TICKS"
		reread_prevention_level = "REREAD_PREVENTION_LEVEL"
		group_by = "GROUP_BY"
		expected_groups_per_symbol = "EXPECTED_GROUPS_PER_SYMBOL"
		keep_snapshot_after_query = "KEEP_SNAPSHOT_AFTER_QUERY"
		allow_concurrent_writers = "ALLOW_CONCURRENT_WRITERS"
		remove_snapshot_upon_start = "REMOVE_SNAPSHOT_UPON_START"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["snapshot_name", "snapshot_storage", "default_db", "database", "symbol_name_field", "expected_symbols_per_time_series", "num_ticks", "reread_prevention_level", "group_by", "expected_groups_per_symbol", "keep_snapshot_after_query", "allow_concurrent_writers", "remove_snapshot_upon_start"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["snapshot_name", "_default_snapshot_name", "snapshot_storage", "_default_snapshot_storage", "default_db", "_default_default_db", "database", "_default_database", "symbol_name_field", "_default_symbol_name_field", "expected_symbols_per_time_series", "_default_expected_symbols_per_time_series", "num_ticks", "_default_num_ticks", "reread_prevention_level", "_default_reread_prevention_level", "group_by", "_default_group_by", "expected_groups_per_symbol", "_default_expected_groups_per_symbol", "keep_snapshot_after_query", "_default_keep_snapshot_after_query", "allow_concurrent_writers", "_default_allow_concurrent_writers", "remove_snapshot_upon_start", "_default_remove_snapshot_upon_start", "stack_info", "_used_strings"]

	class SnapshotStorage:
		MEMORY = "MEMORY"
		MEMORY_MAPPED_FILE = "MEMORY_MAPPED_FILE"

	class RemoveSnapshotUponStart:
		NOT_SET = "NOT_SET"
		FALSE = "false"
		TRUE = "true"

	def __init__(self, snapshot_name="VALUE", snapshot_storage=SnapshotStorage.MEMORY, default_db="CEP_SNAPSHOT", database="", symbol_name_field="", expected_symbols_per_time_series="", num_ticks=1, reread_prevention_level=1, group_by="", expected_groups_per_symbol=10, keep_snapshot_after_query=True, allow_concurrent_writers=False, remove_snapshot_upon_start=RemoveSnapshotUponStart.NOT_SET):
		_graph_components.EpBase.__init__(self, "SAVE_SNAPSHOT")
		self._default_snapshot_name = "VALUE"
		self.snapshot_name = snapshot_name
		self._default_snapshot_storage = type(self).SnapshotStorage.MEMORY
		self.snapshot_storage = snapshot_storage
		self._default_default_db = "CEP_SNAPSHOT"
		self.default_db = default_db
		self._default_database = ""
		self.database = database
		self._default_symbol_name_field = ""
		self.symbol_name_field = symbol_name_field
		self._default_expected_symbols_per_time_series = ""
		self.expected_symbols_per_time_series = expected_symbols_per_time_series
		self._default_num_ticks = 1
		self.num_ticks = num_ticks
		self._default_reread_prevention_level = 1
		self.reread_prevention_level = reread_prevention_level
		self._default_group_by = ""
		self.group_by = group_by
		self._default_expected_groups_per_symbol = 10
		self.expected_groups_per_symbol = expected_groups_per_symbol
		self._default_keep_snapshot_after_query = True
		self.keep_snapshot_after_query = keep_snapshot_after_query
		self._default_allow_concurrent_writers = False
		self.allow_concurrent_writers = allow_concurrent_writers
		self._default_remove_snapshot_upon_start = type(self).RemoveSnapshotUponStart.NOT_SET
		self.remove_snapshot_upon_start = remove_snapshot_upon_start
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_snapshot_name(self, value):
		self.snapshot_name = value
		return self

	def set_snapshot_storage(self, value):
		self.snapshot_storage = value
		return self

	def set_default_db(self, value):
		self.default_db = value
		return self

	def set_database(self, value):
		self.database = value
		return self

	def set_symbol_name_field(self, value):
		self.symbol_name_field = value
		return self

	def set_expected_symbols_per_time_series(self, value):
		self.expected_symbols_per_time_series = value
		return self

	def set_num_ticks(self, value):
		self.num_ticks = value
		return self

	def set_reread_prevention_level(self, value):
		self.reread_prevention_level = value
		return self

	def set_group_by(self, value):
		self.group_by = value
		return self

	def set_expected_groups_per_symbol(self, value):
		self.expected_groups_per_symbol = value
		return self

	def set_keep_snapshot_after_query(self, value):
		self.keep_snapshot_after_query = value
		return self

	def set_allow_concurrent_writers(self, value):
		self.allow_concurrent_writers = value
		return self

	def set_remove_snapshot_upon_start(self, value):
		self.remove_snapshot_upon_start = value
		return self

	@staticmethod
	def _get_name():
		return "SAVE_SNAPSHOT"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.snapshot_name != "VALUE": 
			desc += "SNAPSHOT_NAME=" + py_to_str(self.snapshot_name) + ","
		if self.snapshot_storage != self.SnapshotStorage.MEMORY: 
			desc += "SNAPSHOT_STORAGE=" + py_to_str(self.snapshot_storage) + ","
		if self.default_db != "CEP_SNAPSHOT": 
			desc += "DEFAULT_DB=" + py_to_str(self.default_db) + ","
		if self.database != "": 
			desc += "DATABASE=" + py_to_str(self.database) + ","
		if self.symbol_name_field != "": 
			desc += "SYMBOL_NAME_FIELD=" + py_to_str(self.symbol_name_field) + ","
		if self.expected_symbols_per_time_series != "": 
			desc += "EXPECTED_SYMBOLS_PER_TIME_SERIES=" + py_to_str(self.expected_symbols_per_time_series) + ","
		if self.num_ticks != 1: 
			desc += "NUM_TICKS=" + py_to_str(self.num_ticks) + ","
		if self.reread_prevention_level != 1: 
			desc += "REREAD_PREVENTION_LEVEL=" + py_to_str(self.reread_prevention_level) + ","
		if self.group_by != "": 
			desc += "GROUP_BY=" + py_to_str(self.group_by) + ","
		if self.expected_groups_per_symbol != 10: 
			desc += "EXPECTED_GROUPS_PER_SYMBOL=" + py_to_str(self.expected_groups_per_symbol) + ","
		if self.keep_snapshot_after_query != True: 
			desc += "KEEP_SNAPSHOT_AFTER_QUERY=" + py_to_str(self.keep_snapshot_after_query) + ","
		if self.allow_concurrent_writers != False: 
			desc += "ALLOW_CONCURRENT_WRITERS=" + py_to_str(self.allow_concurrent_writers) + ","
		if self.remove_snapshot_upon_start != self.RemoveSnapshotUponStart.NOT_SET: 
			desc += "REMOVE_SNAPSHOT_UPON_START=" + py_to_str(self.remove_snapshot_upon_start) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class ReadSnapshot(_graph_components.EpBase):
	"""
		

READ_SNAPSHOT

Type: Other

Description: Reads ticks for a specified symbol name and
snapshot name from global memory storage or from a memory mapped file.
These ticks should be written there by the SAVE_SNAPSHOT event
processor. Ticks with an empty symbol name (for example, those after
merging the time series of different symbols) are saved under CEP_SNAPSHOT::
symbol name, so for reading such ticks a dummy database with the name CEP_SNAPSHOT
must be configured in the database locator file.

This event processor must be an input node of the graph, since it
generates ticks.

Python
class name:&nbsp;ReadSnapshot

Input: None

Output: A time series of ticks.

Parameters:


  SNAPSHOT_NAME (string)
    The name that was specified in SAVE_SNAPSHOT as a SNAPSHOT_NAME
during saving.
Default: VALUE

  
  SNAPSHOT_STORAGE (enumerated)
    This parameter specifies the place of storage of the snapshot.
Possible options are:

    
      MEMORY
        The snapshot is stored in the dynamic (heap) memory of the
process that ran (or is still running) the SAVE_SNAPSHOT
EP for the snapshot.

      
      MEMORY_MAPPED_FILE
        The snapshot is stored in a memory mapped file. For each
symbol to get the location of the snapshot in the file system,
READ_SNAPSHOT looks at the SAVE_SNAPSHOT_DIR
parameter value in the locator section for the database of the symbol.

      
    
    Default: MEMORY

  
  ALLOW_SNAPSHOT_ABSENCE
(Boolean)
    If specified, the EP does not display an error about missing
snapshot if the snapshot has not been saved or is still being saved.
Default: false

  

Examples: Reads and propagates the last ticks that are stored
under the name SNAPSHOT1:

READ_SNAPSHOT (SNAPSHOT1)

See the READ_SNAPSHOT example in OTHER_EXAMPLES.otq.


	"""
	class Parameters:
		snapshot_name = "SNAPSHOT_NAME"
		snapshot_storage = "SNAPSHOT_STORAGE"
		allow_snapshot_absence = "ALLOW_SNAPSHOT_ABSENCE"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["snapshot_name", "snapshot_storage", "allow_snapshot_absence"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["snapshot_name", "_default_snapshot_name", "snapshot_storage", "_default_snapshot_storage", "allow_snapshot_absence", "_default_allow_snapshot_absence", "stack_info", "_used_strings"]

	class SnapshotStorage:
		MEMORY = "MEMORY"
		MEMORY_MAPPED_FILE = "MEMORY_MAPPED_FILE"

	def __init__(self, snapshot_name="VALUE", snapshot_storage=SnapshotStorage.MEMORY, allow_snapshot_absence=False):
		_graph_components.EpBase.__init__(self, "READ_SNAPSHOT")
		self._default_snapshot_name = "VALUE"
		self.snapshot_name = snapshot_name
		self._default_snapshot_storage = type(self).SnapshotStorage.MEMORY
		self.snapshot_storage = snapshot_storage
		self._default_allow_snapshot_absence = False
		self.allow_snapshot_absence = allow_snapshot_absence
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_snapshot_name(self, value):
		self.snapshot_name = value
		return self

	def set_snapshot_storage(self, value):
		self.snapshot_storage = value
		return self

	def set_allow_snapshot_absence(self, value):
		self.allow_snapshot_absence = value
		return self

	@staticmethod
	def _get_name():
		return "READ_SNAPSHOT"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.snapshot_name != "VALUE": 
			desc += "SNAPSHOT_NAME=" + py_to_str(self.snapshot_name) + ","
		if self.snapshot_storage != self.SnapshotStorage.MEMORY: 
			desc += "SNAPSHOT_STORAGE=" + py_to_str(self.snapshot_storage) + ","
		if self.allow_snapshot_absence != False: 
			desc += "ALLOW_SNAPSHOT_ABSENCE=" + py_to_str(self.allow_snapshot_absence) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class ShowSnapshotList(_graph_components.EpBase):
	"""
		

SHOW_SNAPSHOT_LIST

Type: Other

Description: Outputs all snapshots names for global memory
storage and/or for memory mapped files. These snapshots should be
written there by the SAVE_SNAPSHOT event processor. Output ticks have SNAPSHOT_NAME, STORAGE_TYPE
and DB_NAME fields.

This event processor must be an input node of the graph, since it
generates ticks.

Python
class name:&nbsp;ShowSnapshotList

Input: None

Output: A time series of ticks.

Parameters:


  SNAPSHOT_STORAGE (enumerated)
    This parameter specifies the place of storage of the snapshot.
Possible options are:

    
      MEMORY
        The snapshot is stored in the dynamic (heap) memory of the
process that ran (or is still running) the SAVE_SNAPSHOT
EP for the snapshot.

      
      MEMORY_MAPPED_FILE
        The snapshot is stored in a memory mapped file. For each
symbol to get the location of the snapshot in the file system,
SHOW_SNAPSHOT_LIST looks at the SAVE_SNAPSHOT_DIR
parameter value in the locator section for the database of the symbol.

      
      ALL
        Shows both, MEMORY , MEMORY_MAPPED_FILE snapshots

      
    
    Default: ALL

  

See the SHOW_SNAPSHOT_LIST
example in SHOW_SNAPSHOT_LIST.otq.


	"""
	class Parameters:
		snapshot_storage = "SNAPSHOT_STORAGE"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["snapshot_storage"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["snapshot_storage", "_default_snapshot_storage", "stack_info", "_used_strings"]

	class SnapshotStorage:
		ALL = "ALL"
		MEMORY = "MEMORY"
		MEMORY_MAPPED_FILE = "MEMORY_MAPPED_FILE"

	def __init__(self, snapshot_storage=SnapshotStorage.ALL):
		_graph_components.EpBase.__init__(self, "SHOW_SNAPSHOT_LIST")
		self._default_snapshot_storage = type(self).SnapshotStorage.ALL
		self.snapshot_storage = snapshot_storage
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_snapshot_storage(self, value):
		self.snapshot_storage = value
		return self

	@staticmethod
	def _get_name():
		return "SHOW_SNAPSHOT_LIST"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.snapshot_storage != self.SnapshotStorage.ALL: 
			desc += "SNAPSHOT_STORAGE=" + py_to_str(self.snapshot_storage) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class JoinWithSnapshot(_graph_components.EpBase):
	"""
		

JOIN_WITH_SNAPSHOT

Type: Join

Description: Joins every input tick with the ticks from
snapshot.

An output tick is formed by joining the respective input tick with
the snapshot ticks. To avoid name collisions, field names of resulting
ticks are optionally prefixed with the value of the PREFIX_FOR_OUTPUT_TICKS
parameter (see below).

Python
class name:&nbsp;JoinWithSnapshot

Input: A time series of ticks.

Output: A time series of ticks.

Parameters:


  SNAPSHOT_NAME (expression)
    The snapshot name that was specified in SAVE_SNAPSHOT as a SNAPSHOT_NAME during
saving.

  
  SNAPSHOT_STORAGE (expression)
    This parameter specifies the place of storage of the snapshot.
Possible options are:

    
      MEMORY
        The snapshot is stored in the dynamic (heap) memory of the
process that ran (or is still running) the SAVE_SNAPSHOT
EP for the snapshot.

      
      MEMORY_MAPPED_FILE
        The snapshot is stored in a memory mapped file. For each
symbol to get the location of the snapshot in the file system,
READ_SNAPSHOT looks at the SAVE_SNAPSHOT_DIR
parameter value in the locator section for the database of the symbol.

      
    
Default: MEMORY
  
  ALLOW_SNAPSHOT_ABSENCE
(expression)
    If specified, the EP does not display an error about a missing
snapshot if the snapshot has not been saved or is still being saved.
Default: false

  
  JOIN_KEYS (expression)
    A list of comma-separated names of attributes. A non-empty list
causes input ticks to be joined only if all of them have matching
values for all specified attributes. Currently, these fields need to
match with GROUP_BY fields of the corresponding snapshot.
Default: empty

  
  SYMBOL_NAME_IN_SNAPSHOT
(expression)
    Expression that evaluates to a string containing symbol name.
Specified expression is reevaluated upon the arrival of each tick. If
this parameter is empty, the input symbol name is used.
Default: empty

  
  DATABASE (expression)
    The database to read the snapshot. If not specified database
from the symbol is used.
Default: empty

  
  DEFAULT_FIELDS_FOR_OUTER_JOIN
(expression)
    A comma-separated list of fields in the following format: FIELD_1 [TYPE_1]=EXPR_1, FIELD_2 [TYPE_2]=EXPR_2,
... , FIELD_N [TYPE_N]=EXPR_N, which specifies the names and the
values of the fields (also, optionally, the field type), used to form
ticks to be joined with unmatched input ticks. This parameter is
reevaluated upon the arrival of each tick.
Default: empty

  
  PREFIX_FOR_OUTPUT_TICKS
(string)
    The prefix for the names of joined tick fields.
Default: empty

  
  SNAPSHOT_FIELDS  (string)
    Specifies list of fields from the snaphot to join with input
ticks. When empty, all fields are included. 
Default: empty

  

Examples: 

JOIN_WITH_SNAPSHOT(SNAPSHOT_NAME=S1,SNAPSHOT_STORAGE=MEMORY_MAPPED_FILE,PREFIX_FOR_OUTPUT_TICKS="J.")
See the join_with_snapshot
example in OTHER_EXAMPLES.otq.


	"""
	class Parameters:
		snapshot_name = "SNAPSHOT_NAME"
		snapshot_storage = "SNAPSHOT_STORAGE"
		allow_snapshot_absence = "ALLOW_SNAPSHOT_ABSENCE"
		join_keys = "JOIN_KEYS"
		prefix_for_output_ticks = "PREFIX_FOR_OUTPUT_TICKS"
		symbol_name_in_snapshot = "SYMBOL_NAME_IN_SNAPSHOT"
		database = "DATABASE"
		default_fields_for_outer_join = "DEFAULT_FIELDS_FOR_OUTER_JOIN"
		snapshot_fields = "SNAPSHOT_FIELDS"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["snapshot_name", "snapshot_storage", "allow_snapshot_absence", "join_keys", "prefix_for_output_ticks", "symbol_name_in_snapshot", "database", "default_fields_for_outer_join", "snapshot_fields"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["snapshot_name", "_default_snapshot_name", "snapshot_storage", "_default_snapshot_storage", "allow_snapshot_absence", "_default_allow_snapshot_absence", "join_keys", "_default_join_keys", "prefix_for_output_ticks", "_default_prefix_for_output_ticks", "symbol_name_in_snapshot", "_default_symbol_name_in_snapshot", "database", "_default_database", "default_fields_for_outer_join", "_default_default_fields_for_outer_join", "snapshot_fields", "_default_snapshot_fields", "stack_info", "_used_strings"]

	class SnapshotStorage:
		MEMORY = "MEMORY"
		MEMORY_MAPPED_FILE = "MEMORY_MAPPED_FILE"

	def __init__(self, snapshot_name="VALUE", snapshot_storage=SnapshotStorage.MEMORY, allow_snapshot_absence=False, join_keys="", prefix_for_output_ticks="", symbol_name_in_snapshot="", database="", default_fields_for_outer_join="", snapshot_fields=""):
		_graph_components.EpBase.__init__(self, "JOIN_WITH_SNAPSHOT")
		self._default_snapshot_name = "VALUE"
		self.snapshot_name = snapshot_name
		self._default_snapshot_storage = type(self).SnapshotStorage.MEMORY
		self.snapshot_storage = snapshot_storage
		self._default_allow_snapshot_absence = False
		self.allow_snapshot_absence = allow_snapshot_absence
		self._default_join_keys = ""
		self.join_keys = join_keys
		self._default_prefix_for_output_ticks = ""
		self.prefix_for_output_ticks = prefix_for_output_ticks
		self._default_symbol_name_in_snapshot = ""
		self.symbol_name_in_snapshot = symbol_name_in_snapshot
		self._default_database = ""
		self.database = database
		self._default_default_fields_for_outer_join = ""
		self.default_fields_for_outer_join = default_fields_for_outer_join
		self._default_snapshot_fields = ""
		self.snapshot_fields = snapshot_fields
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_snapshot_name(self, value):
		self.snapshot_name = value
		return self

	def set_snapshot_storage(self, value):
		self.snapshot_storage = value
		return self

	def set_allow_snapshot_absence(self, value):
		self.allow_snapshot_absence = value
		return self

	def set_join_keys(self, value):
		self.join_keys = value
		return self

	def set_prefix_for_output_ticks(self, value):
		self.prefix_for_output_ticks = value
		return self

	def set_symbol_name_in_snapshot(self, value):
		self.symbol_name_in_snapshot = value
		return self

	def set_database(self, value):
		self.database = value
		return self

	def set_default_fields_for_outer_join(self, value):
		self.default_fields_for_outer_join = value
		return self

	def set_snapshot_fields(self, value):
		self.snapshot_fields = value
		return self

	@staticmethod
	def _get_name():
		return "JOIN_WITH_SNAPSHOT"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.snapshot_name != "VALUE": 
			desc += "SNAPSHOT_NAME=" + py_to_str(self.snapshot_name) + ","
		if self.snapshot_storage != self.SnapshotStorage.MEMORY: 
			desc += "SNAPSHOT_STORAGE=" + py_to_str(self.snapshot_storage) + ","
		if self.allow_snapshot_absence != False: 
			desc += "ALLOW_SNAPSHOT_ABSENCE=" + py_to_str(self.allow_snapshot_absence) + ","
		if self.join_keys != "": 
			desc += "JOIN_KEYS=" + py_to_str(self.join_keys) + ","
		if self.prefix_for_output_ticks != "": 
			desc += "PREFIX_FOR_OUTPUT_TICKS=" + py_to_str(self.prefix_for_output_ticks) + ","
		if self.symbol_name_in_snapshot != "": 
			desc += "SYMBOL_NAME_IN_SNAPSHOT=" + py_to_str(self.symbol_name_in_snapshot) + ","
		if self.database != "": 
			desc += "DATABASE=" + py_to_str(self.database) + ","
		if self.default_fields_for_outer_join != "": 
			desc += "DEFAULT_FIELDS_FOR_OUTER_JOIN=" + py_to_str(self.default_fields_for_outer_join) + ","
		if self.snapshot_fields != "": 
			desc += "SNAPSHOT_FIELDS=" + py_to_str(self.snapshot_fields) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class Diff(_graph_components.EpBase):
	"""
		

DIFF

Type: Other

Description: Compares two time series based on tick non-decreasing value fields specified in NON_DECREASING_VALUE_FIELDS,
outputting the differences between them. A tick from the
first time series is considered to match a tick from the second time
series if both ticks have identical non-decreasing
value fields values and&nbsp;matching values
of all fields that are present in both ticks and are not listed as the
fields to be ignored. A field is considered to match another field if
both fields have identical names, comparable types, and identical
values.
The first source is considered to be the base for starting the matching
process.
If a match is found, all ticks in the second source that occur before
the first matched tick are considered different from those in the first
source. The subsequent searches for the matches in the second source
will begin after this latest matched tick. The ticks from the second
source that are between the found matches&nbsp;are considered different
from the ticks in the first source


Both input time series must have associated names. In graph queries,
this is accomplished by assigning a source name to each input of the
DIFF event processor (EP). In chain queries, use of tick type in the
form:

&lt;db_name1&gt;::&lt;tick_type1&gt;+&lt;db_name2&gt;::&lt;tick_type2&gt;

(For example, TAQ::TRD+NYSE_OB::OB)
will result in an input time series of DIFF named &lt;tick_type1&gt;
and &lt;tick_type2&gt;.

When SHOW_MATCHING_TICKS=false (default case), the output of this
EP consists of unmatched ticks from both input
time series, joined pairwise by equal &nbsp;values of NON_DECREASING_VALUE_FIELDS. If, for a
given set of values of&nbsp;NON_DECREASING_VALUE_FIELDS,
the number
of unmatched ticks&nbsp;in the first time series
is not equal to the number of unmatched ticks&nbsp;in
the second time series, then the remaining after the pairwise
join ticks from one of the time series are output unpaired.

When SHOW_MATCHING_TICKS=true, the output of this EP consists
of&nbsp;matched ticks from both input
time series, joined pairwise by equal &nbsp;values of NON_DECREASING_VALUE_FIELDS &nbsp;and of
all other fields that are not explicitly&nbsp;listed as the fields to
ignore.&nbsp;

Field names in an output tick
are represented as
&lt;source_name&gt;.&lt;field_name&gt;. The output tick will also carry
the position(s) of input tick(s) in input time series in field
&lt;source_name&gt;.INDEX for each source. The lowest position number
is 1.&nbsp;

Python
class name:&nbsp;Diff

Input: Two time series of ticks.

Output: A time series of ticks.

Parameters:


  FIELDS (string [, string])
    A comma-separated list of fields to be used or ignored while
comparing input time series. If this value is not set, all fields are
used in comparison.
Default: empty

  
  NON_DECREASING_VALUE_FIELDS
(string [, string])
    A comma-separated list of non-decreasing value fields to be
considered for matching. If value of this parameter is TIMESTAMP, it compares two time series
based on tick timestamp.
If a value other than TIMESTAMP is
specified, a field named &lt;source_name&gt;.&lt;TIMESTAMP&gt;
will be added to the tick whose value equals the tick's primary
timestamp. 
Default: TIMESTAMP

  
  SHOW_MATCHING_TICKS  (bool)
     If specified, the output of this EP consists of matched ticks
from both input time series instead of unmatched ticks. 
The output tick timestamp is equal to the earliest timestamp of its
corresponding input ticks.
Default: false 

  
  SHOW_ALL_TICKS  (bool)
     If specified, the output of this EP consists of both matched and unmatched ticks
from both input time series. 
MATCH_STATUS field will be added to the output tick with the possible values of 

          0-different ticks
          1-matching ticks
          2-tick from one source only
        

Default: false 

  
  IGNORE (Boolean)
    If true, fields specified in
the FIELDS parameter are ignored while comparing
the input time series. Otherwise, only the fields that are specified in
the FIELDS parameter are used to calculate the
difference between them. If the FIELDS parameter
is empty, the value of this parameter is ignored.
Default: true

  
  OUTPUT_IGNORED_FIELDS (Boolean)
    If false, the fields which
are not used in comparison are excluded from the output. Otherwise, all
fields are propagated.
Default: true

  
  THRESHOLD (integer)
    Specifies the n number of first diff ticks to propagate.
Default: &lt;unlimited&gt;

  
  SHOW_ONLY_FIELDS_THAT_DIFFER
(Boolean)
    If true, the EP outputs only
the tick fields the values of which were different.
Default: false

  

See the DIFF example in OTHER_EXAMPLES.otq.


	"""
	class Parameters:
		fields = "FIELDS"
		non_decreasing_value_fields = "NON_DECREASING_VALUE_FIELDS"
		show_matching_ticks = "SHOW_MATCHING_TICKS"
		show_all_ticks = "SHOW_ALL_TICKS"
		ignore = "IGNORE"
		output_ignored_fields = "OUTPUT_IGNORED_FIELDS"
		threshold = "THRESHOLD"
		show_only_fields_that_differ = "SHOW_ONLY_FIELDS_THAT_DIFFER"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["fields", "non_decreasing_value_fields", "show_matching_ticks", "show_all_ticks", "ignore", "output_ignored_fields", "threshold", "show_only_fields_that_differ"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["fields", "_default_fields", "non_decreasing_value_fields", "_default_non_decreasing_value_fields", "show_matching_ticks", "_default_show_matching_ticks", "show_all_ticks", "_default_show_all_ticks", "ignore", "_default_ignore", "output_ignored_fields", "_default_output_ignored_fields", "threshold", "_default_threshold", "show_only_fields_that_differ", "_default_show_only_fields_that_differ", "stack_info", "_used_strings"]

	def __init__(self, fields="", non_decreasing_value_fields="TIMESTAMP", show_matching_ticks=False, show_all_ticks=False, ignore=True, output_ignored_fields=True, threshold="", show_only_fields_that_differ=False):
		_graph_components.EpBase.__init__(self, "DIFF")
		self._default_fields = ""
		self.fields = fields
		self._default_non_decreasing_value_fields = "TIMESTAMP"
		self.non_decreasing_value_fields = non_decreasing_value_fields
		self._default_show_matching_ticks = False
		self.show_matching_ticks = show_matching_ticks
		self._default_show_all_ticks = False
		self.show_all_ticks = show_all_ticks
		self._default_ignore = True
		self.ignore = ignore
		self._default_output_ignored_fields = True
		self.output_ignored_fields = output_ignored_fields
		self._default_threshold = ""
		self.threshold = threshold
		self._default_show_only_fields_that_differ = False
		self.show_only_fields_that_differ = show_only_fields_that_differ
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_fields(self, value):
		self.fields = value
		return self

	def set_non_decreasing_value_fields(self, value):
		self.non_decreasing_value_fields = value
		return self

	def set_show_matching_ticks(self, value):
		self.show_matching_ticks = value
		return self

	def set_show_all_ticks(self, value):
		self.show_all_ticks = value
		return self

	def set_ignore(self, value):
		self.ignore = value
		return self

	def set_output_ignored_fields(self, value):
		self.output_ignored_fields = value
		return self

	def set_threshold(self, value):
		self.threshold = value
		return self

	def set_show_only_fields_that_differ(self, value):
		self.show_only_fields_that_differ = value
		return self

	@staticmethod
	def _get_name():
		return "DIFF"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.fields != "": 
			desc += "FIELDS=" + py_to_str(self.fields) + ","
		if self.non_decreasing_value_fields != "TIMESTAMP": 
			desc += "NON_DECREASING_VALUE_FIELDS=" + py_to_str(self.non_decreasing_value_fields) + ","
		if self.show_matching_ticks != False: 
			desc += "SHOW_MATCHING_TICKS=" + py_to_str(self.show_matching_ticks) + ","
		if self.show_all_ticks != False: 
			desc += "SHOW_ALL_TICKS=" + py_to_str(self.show_all_ticks) + ","
		if self.ignore != True: 
			desc += "IGNORE=" + py_to_str(self.ignore) + ","
		if self.output_ignored_fields != True: 
			desc += "OUTPUT_IGNORED_FIELDS=" + py_to_str(self.output_ignored_fields) + ","
		if self.threshold != "": 
			desc += "THRESHOLD=" + py_to_str(self.threshold) + ","
		if self.show_only_fields_that_differ != False: 
			desc += "SHOW_ONLY_FIELDS_THAT_DIFFER=" + py_to_str(self.show_only_fields_that_differ) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class WriteToRaw(_graph_components.EpBase):
	"""
		

WRITE_TO_RAW

Type: OutputAdapter

Description: Writes the input tick series to a raw data file
in native format. The location of the raw data file is set via context,
database, location, and mount input parameters. This event processor
(EP) also propagates input ticks if the PROPAGATE_TICKS
parameter is set to TRUE.

The symbol name and tick type are retrieved from fields mentioned in
SYMBOL_NAME_FIELD and TICK_TYPE_FIELD parameters. If
they are not set, symbol name and tick type are used, and if they are
empty (which can, for example, happen after merging), for symbol name,
SYMBOL_NAME field content is retrieved and for tick type, TICK_TYPE
field content is retrieved.

WRITE_TO_RAW EP needs write permissions. See WRITE_TO_ONETICK_DB EP.

Writing to VDB is also supported. In that case symbols will be
partitioned and written to corresponding database mounts.

Python
class name:
WriteToRaw

Input: A time series of ticks.

Output: A time series of ticks or none.

Parameters:


  PROPAGATE_TICKS (Boolean)
    Switches propagation of the ticks. If set to true, ticks will be
propagated.
Default: true

  
  CONTEXT (string)
    The context used to look up the database.
Default: DEFAULT

  
  DATABASE (string)
    The name of the database into which data should be written.

  
  LOCATION (string)
    The raw data location within the database.
Default: PRIMARY

  
  MOUNT (string)
    The name of the mount. If set to auto_assign, mount name
will be assigned automatically. Mount name auto-assignment is only
supported when this EP is used inside otq transformer query and when
the database into which this EP is writing has
AUTO_DISCOVER_MOUNTS=true. In all other cases, auto_assign gets
converted to mount1.
Default: mount1

  
  SYMBOL_NAME_FIELD (string)
    The name of the field which contains the symbol name. If this
parameter is not set, then ticks symbol name is used. If it is empty,
an attempt is made to retrieve the symbol name from the field named
SYMBOL_NAME.

  
  TICK_TYPE_FIELD (string)
    The name of the field that contains the tick type. If this
parameter is not set, the tick type is used, and if it is empty, an
attempt is made to retrieve the tick type from the field named
TICK_TYPE.

  
  TICK_TYPE_NAME (string)
    The tick type that will be assigned to ticks.

  
  CORRECTION_TYPE_FIELD (string)
    The name of the field that contains the correction type. This
field will be removed. If this parameter is not set, no corrections
will be submitted.

    Value of this field will determine if the tick is a correction
and if so which type of correction it is:

    
      1 - cancellation
      2 - correction
      8 - insertion
      other value - regular tick
    
  
  ALLOW_CONCURRENT_WRITE
(Boolean)
    Allows different queries running on the same tick server to
write concurrently to the same CONTEXT/DATABASE/LOCATION/MOUNT/file. In
this mode tick timestamps will be set to system current time to satisfy
the non-decreasing timestamp order requirement of the raw data files.
Additionally,&nbsp;a heartbeat will be recorded into a raw data file
once every
second.
Default: false

  
  SUBMIT_CEP_HEARTBEATS (Boolean)
    This parameter determines whether heartbeats on the input of WRITE_TO_RAW EP are recorded into a raw
data file. Despite the name of this parameter, it applies to any
heartbeats on the input of WRITE_TO_RAW
EP, regardless of whether the query is a CEP query or is an otq
transformer, or this EP receives heartbeats generated within
a&nbsp;query. If the value of this parameter is set to true, those input heartbeats get submitted
into a raw data file. The timestamps of those input heartbeats get
preserved, except when ALLOW_CONCURRENT_WRITE
parameter is set to true. In the
latter case, the timestamps of input heartbeats become set to the
system current time. If the value of this parameter is set to false, the heartbeats on the input of this
EP are not submitted into a raw data file.&nbsp;
    
Notice that, regardless of the value of this parameter,&nbsp;generated
by this event processor heartbeats will be recorded into a raw data
file at least once a second.
    
Default: true

  
  KEEP_SYMBOL_NAME_AND_TICK_TYPE
(Boolean)
    If this parameter is set to false, then fields containing symbol
name and tick type are removed. These field names are read from SYMBOL_NAME_FIELD
and TICK_TYPE_FIELD input parameters. If they are not set, the
default field names SYMBOL_NAME and TICK_TYPE are used.
Default: true

  
  USE_CONTEXT_OF_QUERY (Boolean)
    If this parameter is set to true and the CONTEXT parameter is
not set, EP uses the context of the query instead of the default value
of the CONTEXT parameter.
Default: false

  
  AUTO_FLUSH_INTERVAL (integer)
    Specifies the auto flush interval in milliseconds for the
underlying raw writer.
Default: 1000

  

Examples: Writes raw data in the TEST database in the PRIMARY
location under mount1 and also propagates ticks:

WRITE_TO_RAW(true,DEFAULT,TEST,PRIMARY,mount1,,)

See the WRITE_TO_RAW example in Output_Adapters.otq.


	"""
	class Parameters:
		propagate_ticks = "PROPAGATE_TICKS"
		context = "CONTEXT"
		database = "DATABASE"
		location = "LOCATION"
		mount = "MOUNT"
		symbol_name_field = "SYMBOL_NAME_FIELD"
		tick_type_field = "TICK_TYPE_FIELD"
		tick_type_name = "TICK_TYPE_NAME"
		correction_type_field = "CORRECTION_TYPE_FIELD"
		allow_concurrent_write = "ALLOW_CONCURRENT_WRITE"
		submit_cep_heartbeats = "SUBMIT_CEP_HEARTBEATS"
		submit_data_quality_events = "SUBMIT_DATA_QUALITY_EVENTS"
		keep_symbol_name_and_tick_type = "KEEP_SYMBOL_NAME_AND_TICK_TYPE"
		use_context_of_query = "USE_CONTEXT_OF_QUERY"
		auto_flush_interval = "AUTO_FLUSH_INTERVAL"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["propagate_ticks", "context", "database", "location", "mount", "symbol_name_field", "tick_type_field", "tick_type_name", "correction_type_field", "allow_concurrent_write", "submit_cep_heartbeats", "submit_data_quality_events", "keep_symbol_name_and_tick_type", "use_context_of_query", "auto_flush_interval"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["propagate_ticks", "_default_propagate_ticks", "context", "_default_context", "database", "_default_database", "location", "_default_location", "mount", "_default_mount", "symbol_name_field", "_default_symbol_name_field", "tick_type_field", "_default_tick_type_field", "tick_type_name", "_default_tick_type_name", "correction_type_field", "_default_correction_type_field", "allow_concurrent_write", "_default_allow_concurrent_write", "submit_cep_heartbeats", "_default_submit_cep_heartbeats", "submit_data_quality_events", "_default_submit_data_quality_events", "keep_symbol_name_and_tick_type", "_default_keep_symbol_name_and_tick_type", "use_context_of_query", "_default_use_context_of_query", "auto_flush_interval", "_default_auto_flush_interval", "stack_info", "_used_strings"]

	class Mount:
		AUTO_ASSIGN = "auto_assign"
		MOUNT1 = "mount1"

	def __init__(self, propagate_ticks=True, context="DEFAULT", database="", location="PRIMARY", mount=Mount.MOUNT1, symbol_name_field="", tick_type_field="", tick_type_name="", correction_type_field="", allow_concurrent_write=False, submit_cep_heartbeats=True, submit_data_quality_events=False, keep_symbol_name_and_tick_type=True, use_context_of_query=False, auto_flush_interval=1000):
		_graph_components.EpBase.__init__(self, "WRITE_TO_RAW")
		self._default_propagate_ticks = True
		self.propagate_ticks = propagate_ticks
		self._default_context = "DEFAULT"
		self.context = context
		self._default_database = ""
		self.database = database
		self._default_location = "PRIMARY"
		self.location = location
		self._default_mount = type(self).Mount.MOUNT1
		self.mount = mount
		self._default_symbol_name_field = ""
		self.symbol_name_field = symbol_name_field
		self._default_tick_type_field = ""
		self.tick_type_field = tick_type_field
		self._default_tick_type_name = ""
		self.tick_type_name = tick_type_name
		self._default_correction_type_field = ""
		self.correction_type_field = correction_type_field
		self._default_allow_concurrent_write = False
		self.allow_concurrent_write = allow_concurrent_write
		self._default_submit_cep_heartbeats = True
		self.submit_cep_heartbeats = submit_cep_heartbeats
		self._default_submit_data_quality_events = False
		self.submit_data_quality_events = submit_data_quality_events
		self._default_keep_symbol_name_and_tick_type = True
		self.keep_symbol_name_and_tick_type = keep_symbol_name_and_tick_type
		self._default_use_context_of_query = False
		self.use_context_of_query = use_context_of_query
		self._default_auto_flush_interval = 1000
		self.auto_flush_interval = auto_flush_interval
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_propagate_ticks(self, value):
		self.propagate_ticks = value
		return self

	def set_context(self, value):
		self.context = value
		return self

	def set_database(self, value):
		self.database = value
		return self

	def set_location(self, value):
		self.location = value
		return self

	def set_mount(self, value):
		self.mount = value
		return self

	def set_symbol_name_field(self, value):
		self.symbol_name_field = value
		return self

	def set_tick_type_field(self, value):
		self.tick_type_field = value
		return self

	def set_tick_type_name(self, value):
		self.tick_type_name = value
		return self

	def set_correction_type_field(self, value):
		self.correction_type_field = value
		return self

	def set_allow_concurrent_write(self, value):
		self.allow_concurrent_write = value
		return self

	def set_submit_cep_heartbeats(self, value):
		self.submit_cep_heartbeats = value
		return self

	def set_submit_data_quality_events(self, value):
		self.submit_data_quality_events = value
		return self

	def set_keep_symbol_name_and_tick_type(self, value):
		self.keep_symbol_name_and_tick_type = value
		return self

	def set_use_context_of_query(self, value):
		self.use_context_of_query = value
		return self

	def set_auto_flush_interval(self, value):
		self.auto_flush_interval = value
		return self

	@staticmethod
	def _get_name():
		return "WRITE_TO_RAW"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.propagate_ticks != True: 
			desc += "PROPAGATE_TICKS=" + py_to_str(self.propagate_ticks) + ","
		if self.context != "DEFAULT": 
			desc += "CONTEXT=" + py_to_str(self.context) + ","
		if self.database != "": 
			desc += "DATABASE=" + py_to_str(self.database) + ","
		if self.location != "PRIMARY": 
			desc += "LOCATION=" + py_to_str(self.location) + ","
		if self.mount != self.Mount.MOUNT1: 
			desc += "MOUNT=" + py_to_str(self.mount) + ","
		if self.symbol_name_field != "": 
			desc += "SYMBOL_NAME_FIELD=" + py_to_str(self.symbol_name_field) + ","
		if self.tick_type_field != "": 
			desc += "TICK_TYPE_FIELD=" + py_to_str(self.tick_type_field) + ","
		if self.tick_type_name != "": 
			desc += "TICK_TYPE_NAME=" + py_to_str(self.tick_type_name) + ","
		if self.correction_type_field != "": 
			desc += "CORRECTION_TYPE_FIELD=" + py_to_str(self.correction_type_field) + ","
		if self.allow_concurrent_write != False: 
			desc += "ALLOW_CONCURRENT_WRITE=" + py_to_str(self.allow_concurrent_write) + ","
		if self.submit_cep_heartbeats != True: 
			desc += "SUBMIT_CEP_HEARTBEATS=" + py_to_str(self.submit_cep_heartbeats) + ","
		if self.submit_data_quality_events != False: 
			desc += "SUBMIT_DATA_QUALITY_EVENTS=" + py_to_str(self.submit_data_quality_events) + ","
		if self.keep_symbol_name_and_tick_type != True: 
			desc += "KEEP_SYMBOL_NAME_AND_TICK_TYPE=" + py_to_str(self.keep_symbol_name_and_tick_type) + ","
		if self.use_context_of_query != False: 
			desc += "USE_CONTEXT_OF_QUERY=" + py_to_str(self.use_context_of_query) + ","
		if self.auto_flush_interval != 1000: 
			desc += "AUTO_FLUSH_INTERVAL=" + py_to_str(self.auto_flush_interval) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class Pause(_graph_components.EpBase):
	"""
		

PAUSE

Type: Filter

Description: Pauses processing of each tick for number of
milliseconds specified via DELAY expression.

Python
class name:&nbsp;Pause

Input: A time series of ticks.

Output: A time series of ticks.

Parameters: See parameters common
to many filters.


  DELAY (numeric expression)
    Specifies the number of milliseconds for pausing.

    You can use pseudo-fields such as TIMESTAMP, _START_TIME, and
_END_TIME (described in detail in the Pseudo-fields
document).

    In addition, PAUSE accepts the functions power, sqrt, log,
log10, and exp (see Catalog of Built-in functions
for details). Fields of preceding ticks (those having already arrived)
can also be involved in computations, in which case the field name is
followed by a negative integer index in square brackets specifying how
far to look back (for example, PRICE[-1], SIZE[-3], and so forth.).

    State variables may as well be involved in computations. State
variables are those starting with the prefix "STATE::" and can have two
types, namely a double precision number and string. State variables
have a branch scope in queries. The built-in function UNDEFINED (see Catalog of Built-in functions) can be used to
test whether a state variable with a given name is defined and
initialized or not (for example, UNDEFINED("STATE::BEST_PRICE")). See
the UPDATE_FIELD event processor for
details about how the value of a state variable can be modified.

  
  BUSY_WAITING (Boolean)
    If "true" delay is done via busy loop (consuming CPU time).

    Default: false

  
  WHERE (Boolean)
    Takes a logical expression that conforms with the format
described in LogicalExpressions.

    If the logical expression resolves as TRUE
for the tick, PAUSE EP takes effect according to its configuration.
The default value is empty, which is equivalent to TRUE.

  

See the PAUSE example in FILTER_EXAMPLES.otq.


	"""
	class Parameters:
		delay = "DELAY"
		busy_waiting = "BUSY_WAITING"
		where = "WHERE"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["delay", "busy_waiting", "where"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["delay", "_default_delay", "busy_waiting", "_default_busy_waiting", "where", "_default_where", "stack_info", "_used_strings"]

	def __init__(self, delay="", busy_waiting=False, where=""):
		_graph_components.EpBase.__init__(self, "PAUSE")
		self._default_delay = ""
		self.delay = delay
		self._default_busy_waiting = False
		self.busy_waiting = busy_waiting
		self._default_where = ""
		self.where = where
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_delay(self, value):
		self.delay = value
		return self

	def set_busy_waiting(self, value):
		self.busy_waiting = value
		return self

	def set_where(self, value):
		self.where = value
		return self

	@staticmethod
	def _get_name():
		return "PAUSE"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.delay != "": 
			desc += "DELAY=" + py_to_str(self.delay) + ","
		if self.busy_waiting != False: 
			desc += "BUSY_WAITING=" + py_to_str(self.busy_waiting) + ","
		if self.where != "": 
			desc += "WHERE=" + py_to_str(self.where) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class Throw(_graph_components.EpBase):
	"""
		

THROW

Type: Filter

Description: Propagates error or throws an exception
(depending on SCOPE parameter) when condition provided by WHERE
parameter evaluates to true.

Python
class name:&nbsp;Throw

Input: A time series of ticks.

Output: A time series of ticks.

Parameters:


  WHERE (Boolean expression)

&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;
&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; A logical expression.

  SCOPE (enumerated type)
    Specifies the scope of error. Possible scopes are:

    
      QUERY - Throw an exception when the
condition evaluates to true.
      SYMBOL - Propagate error with
corresponding error code and error message parameters when the
condition evaluates to true.
    
  
  MESSAGE (string)
    Specifies error message that will be used.

  
  ERROR_CODE (numeric)
    Specifies the error code that will be used.
When SCOPE=SYMBOL, values from
interval [1 , 500] indicate warnings
and values from interval [1500,2000]
indicate errors. Note that tick propagation will not stop when a tick
matches the WHERE condition and a
warning code (error code from [1, 500])
is used, otherwise no ticks will be propagated after the first tick
that matches WHERE condition.
When SCOPE=QUERY, tick propagation
will always stop on the first tick that matches the WHERE condition.

  
  TREAT_MESSAGE_AS_PER_TICK_EXPR
(Boolean)
    If set to true, the error message is evaluated for each tick
that meets the WHERE clause condition. Thus, a
SQL-like message can be provided such as 'The
exchange is equal to EXCHANGE[-1]'. Here, the EXCHANGE field is taken from the previous
tick.
Default: false

  
  THROW_BEFORE_QUERY_EXECUTION
(Boolean)
    If set to true, the exception will be thrown before the
execution of
the query. This option is intended for supplying placeholder queries
that must always throw.
Default: false

  

Example: Discard the ticks that are not from NYSE and process
warning:

THROW("NOT(EXCHANGE='N')",SYMBOL,'Tick is not from
NYSE',1)

See the THROW example in FILTER_EXAMPLES.otq.


	"""
	class Parameters:
		where = "WHERE"
		scope = "SCOPE"
		message = "MESSAGE"
		error_code = "ERROR_CODE"
		treat_message_as_per_tick_expr = "TREAT_MESSAGE_AS_PER_TICK_EXPR"
		throw_before_query_execution = "THROW_BEFORE_QUERY_EXECUTION"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["where", "scope", "message", "error_code", "treat_message_as_per_tick_expr", "throw_before_query_execution"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["where", "_default_where", "scope", "_default_scope", "message", "_default_message", "error_code", "_default_error_code", "treat_message_as_per_tick_expr", "_default_treat_message_as_per_tick_expr", "throw_before_query_execution", "_default_throw_before_query_execution", "stack_info", "_used_strings"]

	class Scope:
		QUERY = "QUERY"
		SYMBOL = "SYMBOL"

	def __init__(self, where="", scope=Scope.QUERY, message="", error_code=1, treat_message_as_per_tick_expr=False, throw_before_query_execution=False):
		_graph_components.EpBase.__init__(self, "THROW")
		self._default_where = ""
		self.where = where
		self._default_scope = type(self).Scope.QUERY
		self.scope = scope
		self._default_message = ""
		self.message = message
		self._default_error_code = 1
		self.error_code = error_code
		self._default_treat_message_as_per_tick_expr = False
		self.treat_message_as_per_tick_expr = treat_message_as_per_tick_expr
		self._default_throw_before_query_execution = False
		self.throw_before_query_execution = throw_before_query_execution
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_where(self, value):
		self.where = value
		return self

	def set_scope(self, value):
		self.scope = value
		return self

	def set_message(self, value):
		self.message = value
		return self

	def set_error_code(self, value):
		self.error_code = value
		return self

	def set_treat_message_as_per_tick_expr(self, value):
		self.treat_message_as_per_tick_expr = value
		return self

	def set_throw_before_query_execution(self, value):
		self.throw_before_query_execution = value
		return self

	@staticmethod
	def _get_name():
		return "THROW"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.where != "": 
			desc += "WHERE=" + py_to_str(self.where) + ","
		if self.scope != self.Scope.QUERY: 
			desc += "SCOPE=" + py_to_str(self.scope) + ","
		if self.message != "": 
			desc += "MESSAGE=" + py_to_str(self.message) + ","
		if self.error_code != 1: 
			desc += "ERROR_CODE=" + py_to_str(self.error_code) + ","
		if self.treat_message_as_per_tick_expr != False: 
			desc += "TREAT_MESSAGE_AS_PER_TICK_EXPR=" + py_to_str(self.treat_message_as_per_tick_expr) + ","
		if self.throw_before_query_execution != False: 
			desc += "THROW_BEFORE_QUERY_EXECUTION=" + py_to_str(self.throw_before_query_execution) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class NamedQueueReader(_graph_components.EpBase):
	"""
		

NAMED_QUEUE_READER

Type: Other

Description: Picks ticks from corresponding named queue and
propagates.

Python
class name:&nbsp;NamedQueueReader

Input: None.

Output: A time series of ticks.

Parameters:


  QUEUE_NAME (string)
    Specifies the name of the queue.

  

You can use the following OneTick API classes to automate sending
ticks to NAMED_QUEUE_READER EP:


  omd::NamedQueue - suitable for
writing into named queues within the same process (can only be used for
CEP queries).
  omd::SymbolTickCollectionsForRemoteNamedQueues
- suitable for sending ticks from client to server side named queues.

Examples:

NAMED_QUEUE_READER(QUEUE_NAME="NAMED_QUEUE_1")


	"""
	class Parameters:
		queue_name = "QUEUE_NAME"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["queue_name"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["queue_name", "_default_queue_name", "stack_info", "_used_strings"]

	def __init__(self, queue_name=""):
		_graph_components.EpBase.__init__(self, "NAMED_QUEUE_READER")
		self._default_queue_name = ""
		self.queue_name = queue_name
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_queue_name(self, value):
		self.queue_name = value
		return self

	@staticmethod
	def _get_name():
		return "NAMED_QUEUE_READER"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.queue_name != "": 
			desc += "QUEUE_NAME=" + py_to_str(self.queue_name) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class ModifyTsProperties(_graph_components.EpBase):
	"""
		

MODIFY_TS_PROPERTIES

Type: Other

Description: Modifies a tick descriptor property (see&nbsp;td_properties.html)
of the tick descriptors propagated through the EP. Note that in most if
not all cases, modifying these properties has no relevance/effect on
analytics. The main application of this EP is to annotate a tick
descriptor with properties for further processing by loading logic.
This is useful if the query is used for transformation of ticks prior
to loading (see transforming_ticks_in_loader.html) or when the query
itself writes to a database or raw data using WRITE_TO_ONETICK_DB or WRITE_TO_RAW.

Python
class name:&nbsp;ModifyTsProperties

Input: A time series of ticks

Output: A time series of ticks

Parameters:


  PROPERTY_NAME (string)
    Name of the property to modify.

    This can be a user-defined name or one of the predefined
properties
(see docs/datamodeling.html#td_properties). Default: STATE_KEYS (see
docs/td_properties.html#STATE_KEYS)

  
  PROPERTY_VALUE (string)
    The value of the property.

  

Examples:

MODIFY_TS_PROPERTIES(PROPERTY_NAME="STATE_KEYS",PROPERTY_VALUE="BUY_SELL_FLAG,ORDER_ID")

	"""
	class Parameters:
		property_name = "PROPERTY_NAME"
		property_value = "PROPERTY_VALUE"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["property_name", "property_value"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["property_name", "_default_property_name", "property_value", "_default_property_value", "stack_info", "_used_strings"]

	class PropertyName:
		BOOK_INCLUDE_MARKET_ORDER_TICKS = "BOOK_INCLUDE_MARKET_ORDER_TICKS"
		BOOK_SIZE = "BOOK_SIZE"
		COMPRESSION_GROUPS = "COMPRESSION_GROUPS"
		CORRECT_METHOD = "CORRECT_METHOD"
		MATCH_METHOD = "MATCH_METHOD"
		PRICE_NOT_KEY = "PRICE_NOT_KEY"
		PRIMARY_KEY = "PRIMARY_KEY"
		STATE_KEYS = "STATE_KEYS"
		TICKS_ARE_AGGR_BUCKET_END = "TICKS_ARE_AGGR_BUCKET_END"

	def __init__(self, property_name=PropertyName.STATE_KEYS, property_value=""):
		_graph_components.EpBase.__init__(self, "MODIFY_TS_PROPERTIES")
		self._default_property_name = type(self).PropertyName.STATE_KEYS
		self.property_name = property_name
		self._default_property_value = ""
		self.property_value = property_value
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_property_name(self, value):
		self.property_name = value
		return self

	def set_property_value(self, value):
		self.property_value = value
		return self

	@staticmethod
	def _get_name():
		return "MODIFY_TS_PROPERTIES"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.property_name != self.PropertyName.STATE_KEYS: 
			desc += "PROPERTY_NAME=" + py_to_str(self.property_name) + ","
		if self.property_value != "": 
			desc += "PROPERTY_VALUE=" + py_to_str(self.property_value) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class InsertAtEnd(_graph_components.EpBase):
	"""
		

INSERT_AT_END

Type: Other

Description: INSERT_AT_END is a single-input single-output
EP. It has two modes: passthrough mode and no-pass mode.

In passthrough mode, it operates as a passthrough, except that it
adds a field AT_END, which is set to zero for all inbound ticks and set
to 1 for an additional tick that is generated when the data ends.

In no-pass mode, it creates only one tick at the end. If
FIELDS_OF_ADDED_TICK is specified, the created tick will have the
corresponding scheme, except if the scheme has a field whose name
matches DELIMITER_NAME (default AT_END) the value of this field will be
set to 1. If FIELDS_OF_ADDED_TICK is not specified, it creates only one
tick at the end, which has the AT_END field with value set to 1.

Additionally, if FIELDS_OF_ADDED_TICK is specified in passthrough
mode, then INSERT_IF_INPUT_IS_EMPTY must be set to true, in which case,
if there is no tick at all, the behavior will be the same as in no-pass
mode.

Python
class name:&nbsp;InsertAtEnd

Input: A time series of ticks.

Output: A time series of ticks.

Parameters:


  PROPAGATE_TICKS (Boolean)
    Indicates if the EP should work on passthrough mode or not.
Default: true (i.e., passthrough mode is turned on)

  
  DELIMITER_NAME (string)
    Identifies end field delimiter name.
Default: AT_END

  
  FIELDS_OF_ADDED_TICK (string)
    A parameter in the following form: FIELD_1
[TYPE_1]=CONST_EXPR_1, FIELD_2 [TYPE_2]=CONST_EXPR_1, &hellip; ,FIELD_N
[TYPE_N]=CONST_EXPR_1.
The name of the new field is "identifier." Supported types are listed here. If
the type specification is omitted, the type will be detected
automatically (long, double, decimal or string).
CONST_EXPR_ used above are logiical
expressions. 
Default: &lt;empty&gt;

  
  INSERT_IF_INPUT_IS_EMPTY
(Boolean)
    Regulates the behavior in case of passthrough mode if
FIELDS_OF_ADDED_TICK is specified.
if false, an exception will be thrown.
if true, if there is no tick at all, the behavior will be the same as
in the no-pass mode.
Default: false

  

See the INSERT_AT_END example in OTHER_EXAMPLES.otq.


	"""
	class Parameters:
		propagate_ticks = "PROPAGATE_TICKS"
		delimiter_name = "DELIMITER_NAME"
		fields_of_added_tick = "FIELDS_OF_ADDED_TICK"
		insert_if_input_is_empty = "INSERT_IF_INPUT_IS_EMPTY"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["propagate_ticks", "delimiter_name", "fields_of_added_tick", "insert_if_input_is_empty"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["propagate_ticks", "_default_propagate_ticks", "delimiter_name", "_default_delimiter_name", "fields_of_added_tick", "_default_fields_of_added_tick", "insert_if_input_is_empty", "_default_insert_if_input_is_empty", "stack_info", "_used_strings"]

	def __init__(self, propagate_ticks=True, delimiter_name="AT_END", fields_of_added_tick="", insert_if_input_is_empty=False):
		_graph_components.EpBase.__init__(self, "INSERT_AT_END")
		self._default_propagate_ticks = True
		self.propagate_ticks = propagate_ticks
		self._default_delimiter_name = "AT_END"
		self.delimiter_name = delimiter_name
		self._default_fields_of_added_tick = ""
		self.fields_of_added_tick = fields_of_added_tick
		self._default_insert_if_input_is_empty = False
		self.insert_if_input_is_empty = insert_if_input_is_empty
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_propagate_ticks(self, value):
		self.propagate_ticks = value
		return self

	def set_delimiter_name(self, value):
		self.delimiter_name = value
		return self

	def set_fields_of_added_tick(self, value):
		self.fields_of_added_tick = value
		return self

	def set_insert_if_input_is_empty(self, value):
		self.insert_if_input_is_empty = value
		return self

	@staticmethod
	def _get_name():
		return "INSERT_AT_END"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.propagate_ticks != True: 
			desc += "PROPAGATE_TICKS=" + py_to_str(self.propagate_ticks) + ","
		if self.delimiter_name != "AT_END": 
			desc += "DELIMITER_NAME=" + py_to_str(self.delimiter_name) + ","
		if self.fields_of_added_tick != "": 
			desc += "FIELDS_OF_ADDED_TICK=" + py_to_str(self.fields_of_added_tick) + ","
		if self.insert_if_input_is_empty != False: 
			desc += "INSERT_IF_INPUT_IS_EMPTY=" + py_to_str(self.insert_if_input_is_empty) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class ExecuteOtqRecursively(_graph_components.EpBase):
	"""
		

EXECUTE_OTQ_RECURSIVELY

Type: Other

Description: Executes the specified non CEP OTQ on all
discoverable OneTick hosts. The OTQ can be specified both by name (can
be also a file path) and by file content. In the former case the OTQ
should exist on all accessible hosts (error will be reported for the
ones which do not have it). All output ticks have the timestamp of the
query start time. The original timestamps of the output ticks are
written into TICK_TIME special field.

The EP allows query execution on any remote host by explicitly
specifying host name as a query symbol (represented as "REMOTE@server_ip:server_port").

Note that the start and end timestamps override the timestamps of
the specified OTQ query.

Python
class name:
ExecuteOtqRecursively

Input: No input (this EP is a data source and should be the
root of the graph).

Output: The combined output of the specified OTQ. Two extra
fields, namely, SERVER_ADDRESS and TICK_TIME are added
to all output ticks. The former contains the address of the server on
which the query was executed, the latter contains the original
timestamp of the output tick.

Parameters:


  FILE_NAME (string)
    Specifies the query to be executed (in local or remote location)
as a path to an .otq file, optionally followed by the query name.
Default value: &lt;empty&gt;

  
  FILE_CONTENTS (string)
    If not empty, the EP treats its value as a content of the OTQ
file from which it should construct the query. The FILE_NAME can be
omitted as it is not used.
Default value: &lt;empty&gt;

  
  OTQ_PARAMS (string)
    Comma-separated list of &lt;name&gt;=&lt;value&gt;
pairs, specifying OTQ parameters for the specified query.
Default value: &lt;empty&gt;

  
  DEEP_SCAN (Boolean)
    If true, the EP will process a deeper scan to execute the OTQ on
all accessible host.
Default value: false

  
  SKIP_LOCAL_HOST (Boolean)
    If true, the EP will not execute the query on the host
initiating the query (client host or the first tick server host).
Default value: false

  

Examples:

See the examples in execute_otq_recursively_example.otq.


	"""
	class Parameters:
		file_name = "FILE_NAME"
		file_contents = "FILE_CONTENTS"
		otq_params = "OTQ_PARAMS"
		deep_scan = "DEEP_SCAN"
		skip_local_host = "SKIP_LOCAL_HOST"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["file_name", "file_contents", "otq_params", "deep_scan", "skip_local_host"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["file_name", "_default_file_name", "file_contents", "_default_file_contents", "otq_params", "_default_otq_params", "deep_scan", "_default_deep_scan", "skip_local_host", "_default_skip_local_host", "stack_info", "_used_strings"]

	def __init__(self, file_name="", file_contents="", otq_params="", deep_scan=False, skip_local_host=False):
		_graph_components.EpBase.__init__(self, "EXECUTE_OTQ_RECURSIVELY")
		self._default_file_name = ""
		self.file_name = file_name
		self._default_file_contents = ""
		self.file_contents = file_contents
		self._default_otq_params = ""
		self.otq_params = otq_params
		self._default_deep_scan = False
		self.deep_scan = deep_scan
		self._default_skip_local_host = False
		self.skip_local_host = skip_local_host
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_file_name(self, value):
		self.file_name = value
		return self

	def set_file_contents(self, value):
		self.file_contents = value
		return self

	def set_otq_params(self, value):
		self.otq_params = value
		return self

	def set_deep_scan(self, value):
		self.deep_scan = value
		return self

	def set_skip_local_host(self, value):
		self.skip_local_host = value
		return self

	@staticmethod
	def _get_name():
		return "EXECUTE_OTQ_RECURSIVELY"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.file_name != "": 
			desc += "FILE_NAME=" + py_to_str(self.file_name) + ","
		if self.file_contents != "": 
			desc += "FILE_CONTENTS=" + py_to_str(self.file_contents) + ","
		if self.otq_params != "": 
			desc += "OTQ_PARAMS=" + py_to_str(self.otq_params) + ","
		if self.deep_scan != False: 
			desc += "DEEP_SCAN=" + py_to_str(self.deep_scan) + ","
		if self.skip_local_host != False: 
			desc += "SKIP_LOCAL_HOST=" + py_to_str(self.skip_local_host) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class CreateTickFromFidVals(_graph_components.EpBase):
	"""
		

CREATE_TICK_FROM_FID_VALS

Type: Transformer

Description: This event processor (EP) creates a tick from
READ_FROM_RAW RECORD_MILTI_TICK_BEGIN / RECORD_MILTI_TICK_END block of
ticks. For each fid newly created tick has a column with FID_&lt;fid&gt; name for positive fids and
a column with FID_m&lt;fid&gt; name
for negative fids. If in the input SYMBOL_NAME
field exists the EP keeps it in the output tick. All fields in the
output have string type with length that has STRING_VAL
field in READ_FROM_RAW EP.

Python
class name:&nbsp;CreateTickFromFidVals

Input: A time series of ticks from READ_FROM_RAW output.

Output: A tick for each group of ticks of READ_FROM_RAW
output.

Example: Transforms READ_FROM_RAW output then analyzes
it(note you may need to configure READ_FROM_RAW EP)

See FID_VAL_FILE_ANALYZER.otq.


	"""
	class Parameters:
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = []
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["stack_info", "_used_strings"]

	def __init__(self):
		_graph_components.EpBase.__init__(self, "CREATE_TICK_FROM_FID_VALS")
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	@staticmethod
	def _get_name():
		return "CREATE_TICK_FROM_FID_VALS"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class FidValFileAnalyzer(_graph_components.EpBase):
	"""
		

FID_VAL_FILE_ANALYZER

Type: Filter

Description: This event processor (EP) is for analyzing raw
rt files. Depending on LEVEL_OF_DETAIL parameter it returns per-fid,
per-symbol, or summary information of specified raw rt file.

When LEVEL_OF_DETAIL is PER_FID, for each fid the EP automatically
detects the data type, retrieves the data length for string types,
returns the number of occurrences of the fid, the number of distinct
values, a list of actual values, as well as the minimum and maximum
values for the numeric types.

When LEVEL_OF_DETAIL is PER_SYMBOL, for each symbol the EP returns
the number of messages for the symbol.

For the SUMMARY case, it only returns the number of ticks it has
processed.

Python
class name:&nbsp;FidValFileAnalyzer

Input: A time series of ticks from CREATE_TICK_FROM_FID_VALS
output.

Output: A tick for each fid or a tick for each symbol or
number of ticks: depending on the value of the LEVEL_OF_DETAIL
parameter.

Parameters:


  LEVEL_OF_DETAIL (string)
    Supported values are PER_FID, PER_SYMBOL and SUMMARY.

  
  MAX_DISTINCT_FID_VALS_TO_TRACK
(int)
    In PER_FID mode shows the maximum count of distinct values to
track for a fid. Distinct values beyong that limit will be silently
ignored.&nbsp;

    Default: 50

  
  DICTIONARY (string)
    File name that contains mapping from xxx to its yyy.
The file must have two columns: xxx and yyy.

  

Example: Does per-fid analysis of data given by READ_FROM_RAW
(note you may need to configure READ_FROM_RAW EP).
See FID_VAL_FILE_ANALYZER in FID_VAL_FILE_ANALYZER.otq?FID_VAL_FILE_ANALYZER.


	"""
	class Parameters:
		level_of_detail = "LEVEL_OF_DETAIL"
		max_distinct_fid_vals_to_track = "MAX_DISTINCT_FID_VALS_TO_TRACK"
		dictionary = "DICTIONARY"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["level_of_detail", "max_distinct_fid_vals_to_track", "dictionary"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["level_of_detail", "_default_level_of_detail", "max_distinct_fid_vals_to_track", "_default_max_distinct_fid_vals_to_track", "dictionary", "_default_dictionary", "stack_info", "_used_strings"]

	class LevelOfDetail:
		EMPTY = ""
		PER_FID = "PER_FID"
		PER_SYMBOL = "PER_SYMBOL"
		SUMMARY = "SUMMARY"

	def __init__(self, level_of_detail=LevelOfDetail.EMPTY, max_distinct_fid_vals_to_track=50, dictionary=""):
		_graph_components.EpBase.__init__(self, "FID_VAL_FILE_ANALYZER")
		self._default_level_of_detail = type(self).LevelOfDetail.EMPTY
		self.level_of_detail = level_of_detail
		self._default_max_distinct_fid_vals_to_track = 50
		self.max_distinct_fid_vals_to_track = max_distinct_fid_vals_to_track
		self._default_dictionary = ""
		self.dictionary = dictionary
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_level_of_detail(self, value):
		self.level_of_detail = value
		return self

	def set_max_distinct_fid_vals_to_track(self, value):
		self.max_distinct_fid_vals_to_track = value
		return self

	def set_dictionary(self, value):
		self.dictionary = value
		return self

	@staticmethod
	def _get_name():
		return "FID_VAL_FILE_ANALYZER"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.level_of_detail != self.LevelOfDetail.EMPTY: 
			desc += "LEVEL_OF_DETAIL=" + py_to_str(self.level_of_detail) + ","
		if self.max_distinct_fid_vals_to_track != 50: 
			desc += "MAX_DISTINCT_FID_VALS_TO_TRACK=" + py_to_str(self.max_distinct_fid_vals_to_track) + ","
		if self.dictionary != "": 
			desc += "DICTIONARY=" + py_to_str(self.dictionary) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class PnlRealized(_graph_components.EpBase):
	"""
		

PNL_REALIZED

Type: Other

Description: Computes the realized Profit and Loss (PNL) on
each tick and is applicable to both scenarios, whether selling after
buying or buying after selling.&nbsp;

Python
class name:
PnlRealized

Input: A time series of ticks that includes SIZE, PRICE
, and BUY_SELL_FLAG fields. Possible
values of BUY_SELL_FLAG field are
'B' or 'b' for buy and 'S' or 's' for sell.

Output: A time series of ticks which consists of all input
tick fields, unchanged, and an additional field for realized Profit and
Loss (PNL), with the name determined by the parameter OUTPUT_FIELD_NAME.

Parameters:


  COMPUTATION_METHOD (String)
    This parameter determines the approach used to calculate the
realized Profit and Loss (PnL). Possible options are:

    
      FIFO
        Stands for 'First-In-First-Out,' is used to calculate Profit
and
Loss (PnL) based on the principle that the first trading positions
bought are the first ones to be sold, or conversely, the first trading
positions sold are the first ones to be bought. A more detailed
description of FIFO method can be found at&nbsp;https://quant.stackexchange.com/questions/53415/pnl-with-fifo-and-lifo

        Default: FIFO

        Output example:

        Index	Symbol		 Time				SIZE		PRICE		 BUY_SELL_FLAG	PNL_REALIZED1	PnL_REALIZED::A	 2023/01/01 00:00:01.000	3.0000000	100.0000000	 B		0.00000002	PnL_REALIZED::A	 2023/01/01 00:00:02.000	10.0000000	105.0000000	 B		0.00000003	PnL_REALIZED::A	 2023/01/01 00:00:02.000	4.0000000	107.0000000	 S		23.0000000
      
    
  
  OUTPUT_FIELD_NAME (String)
    This parameter defines the name of the output field.

    Default: PNL_REALIZED

  

Examples:

See the examples in 
PNL_REALIZED_examples.otq.


	"""
	class Parameters:
		computation_method = "COMPUTATION_METHOD"
		output_field_name = "OUTPUT_FIELD_NAME"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["computation_method", "output_field_name"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["computation_method", "_default_computation_method", "output_field_name", "_default_output_field_name", "stack_info", "_used_strings"]

	def __init__(self, computation_method="FIFO", output_field_name="PNL_REALIZED", Out = ""):
		_graph_components.EpBase.__init__(self, "PNL_REALIZED")
		self._default_computation_method = "FIFO"
		self.computation_method = computation_method
		self._default_output_field_name = "PNL_REALIZED"
		self.output_field_name = output_field_name
		if Out != "":
			self.output_field_name=Out
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_computation_method(self, value):
		self.computation_method = value
		return self

	def set_output_field_name(self, value):
		self.output_field_name = value
		return self

	@staticmethod
	def _get_name():
		return "PNL_REALIZED"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.computation_method != "FIFO": 
			desc += "COMPUTATION_METHOD=" + py_to_str(self.computation_method) + ","
		if self.output_field_name != "PNL_REALIZED": 
			desc += "OUTPUT_FIELD_NAME=" + py_to_str(self.output_field_name) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class JoinWithAggregatedWindow(_graph_components.EpBase):
	"""
		

JOIN_WITH_AGGREGATED_WINDOW

Type: Aggregation

Description: Computes one or more aggregations on
AGGREGATION_INPUT time series and joins the result with each incoming
tick from PASS_INPUT time series.

Python
class name:&nbsp;JoinWithAggregatedWindow

Input: Two time series of ticks.

Output: A time series of ticks.

Parameters: See parameters
common to generic aggregations.


  AGGREGATION (A list of aggregations)
    The list of aggregations should be specified in the following
format:

    &lt;aggregation1&gt;(&lt;param1&gt;='&lt;value1&gt;',&lt;param2&gt;='&lt;value2&gt;',...)
label1,
&lt;aggregation2&gt;(&lt;param1&gt;='&lt;value1&gt;',&lt;param2&gt;='&lt;value2&gt;',...)
label2

    Labels are optional: if the label is omitted, the aggregation
name is used as the label. Output attributes of each aggregation in the
AGGREGATION parameter are named &lt;label&gt;.&lt;output
attribute name&gt; or just &lt;label&gt;,
if APPEND_OUTPUT_FIELD_NAME is
set to false.

    You can use an extra parameter REGEX
with each aggregation in AGGREGATION. For each field from the tick
descriptor that matches the provided regex, a new aggregation is added
to the list. Matched parts can be used to specify parameters and labels
(see examples). Note that aggregations are
created only when the first tick descriptor arrives, otherwise the
operation is ignored.

    For each aggregation in AGGREGATION, parameters are optional:
aggregation-specific defaults are used for omitted parameters.
Parameter names are the same as the ones used by the aggregation when
it is run outside of AGGREGATION.

    Note that AGGREGATION does not work with aggregation EPs that return multiple ticks
per bucket interval.

  
  BUCKET_INTERVAL
(seconds/ticks)
  When BUCKET_INTERVAL is set to 0, the computation of the
aggregation is performed for all ticks starting from the query's start
time and until PASS_INPUT tick's timestamp - PASS_SOURCE_DELAY_MSEC,
regardless of the value of BUCKET_INTERVAL_UNITS

  BUCKET_INTERVAL_UNITS
(enumerated type)
  BOUNDARY_AGGR_TICK_BEHAVIOR
(NEXT_WINDOW/PREV_WINDOW)
    Similar to BOUNDARY_TICK_BUCKET.
If set to NEXT_WINDOW, ticks from AGGREGATION_INPUT with the same
timestamp (+PASS_SOURCE_DELAY_MSEC) as the latest ticks from PASS_INPUT
will not be included in that tick's joined aggregation.
    
Example:
If a some amount of ticks coming from AGGREGATION_INPUT (A) and
PASS_INPUT (P) arrive in the configuration APAPAP_P',
where only the P' has an effective
timestamp different from the rest, when this parameter is set to : 

    
      PREV_WINDOW, all P ticks
will be joined and passed forward with the aggregation result that
includes all A ticks - P3P3P3_P'3
      NEXT_WINDOW, all P ticks
will be joined and passed forward before any A
ticks are processed - P0P0P0_P'3
    
  
  APPEND_OUTPUT_FIELD_NAME
(Boolean)
    If true (default), output
attributes of each aggregation in the COMPUTE parameter are named as &lt;aggregation label&gt;.&lt;output attribute
name&gt;.
If false, output attributes of each
aggregation in the COMPUTE parameter are named as &lt;aggregation label&gt;. When set to false, there can be only one output
attribute for each aggregation.

  
  AGGREGATION_SOURCE (string)
    Name of the input time series that will be fed into the
aggregation.

  
  PASS_SOURCE (string)
    Name of the input time series that will be joined with the
aggregation result.

  
  PASS_SOURCE_DELAY_MSEC 
(msecs)
    Specifies by how much any incoming tick from the PASS_INPUT is
delayed.
The effective timestamp of a tick from the PASS_INPUT with timestamp T
is T - PASS_SOURCE_DELAY_MSEC. This parameter may be negative, in which
case ticks from PASS_INPUT will be joined with the aggregation result
of a later timestamp

  

Examples:

The following example computes the average price of TRD ticks within
one minute of each QTE tick.

JOIN_WITH_AGGREGATED_WINDOW(AGGREGATION="AVERAGE(INPUT_FIELD_NAME
=
'PRICE')",BUCKET_INTERVAL=60,AGGREGATION_SOURCE='TRD',PASS_SOURCE='QTE')


	"""
	class Parameters:
		aggregation = "AGGREGATION"
		bucket_interval = "BUCKET_INTERVAL"
		bucket_interval_units = "BUCKET_INTERVAL_UNITS"
		boundary_aggr_tick_behavior = "BOUNDARY_AGGR_TICK_BEHAVIOR"
		pass_source_delay_msec = "PASS_SOURCE_DELAY_MSEC"
		append_output_field_name = "APPEND_OUTPUT_FIELD_NAME"
		group_by = "GROUP_BY"
		aggregation_source = "AGGREGATION_SOURCE"
		pass_source = "PASS_SOURCE"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["aggregation", "bucket_interval", "bucket_interval_units", "boundary_aggr_tick_behavior", "pass_source_delay_msec", "append_output_field_name", "group_by", "aggregation_source", "pass_source"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["aggregation", "_default_aggregation", "bucket_interval", "_default_bucket_interval", "bucket_interval_units", "_default_bucket_interval_units", "boundary_aggr_tick_behavior", "_default_boundary_aggr_tick_behavior", "pass_source_delay_msec", "_default_pass_source_delay_msec", "append_output_field_name", "_default_append_output_field_name", "group_by", "_default_group_by", "aggregation_source", "_default_aggregation_source", "pass_source", "_default_pass_source", "stack_info", "_used_strings"]

	class BucketIntervalUnits:
		DAYS = "DAYS"
		FLEXIBLE = "FLEXIBLE"
		MONTHS = "MONTHS"
		SECONDS = "SECONDS"
		TICKS = "TICKS"

	class BoundaryAggrTickBehavior:
		NEXT_WINDOW = "NEXT_WINDOW"
		PREV_WINDOW = "PREV_WINDOW"

	def __init__(self, aggregation="", bucket_interval=0, bucket_interval_units=BucketIntervalUnits.SECONDS, boundary_aggr_tick_behavior=BoundaryAggrTickBehavior.NEXT_WINDOW, pass_source_delay_msec=0, append_output_field_name=True, group_by="", aggregation_source="", pass_source=""):
		_graph_components.EpBase.__init__(self, "JOIN_WITH_AGGREGATED_WINDOW")
		self._default_aggregation = ""
		self.aggregation = aggregation
		self._default_bucket_interval = 0
		self.bucket_interval = bucket_interval
		self._default_bucket_interval_units = type(self).BucketIntervalUnits.SECONDS
		self.bucket_interval_units = bucket_interval_units
		self._default_boundary_aggr_tick_behavior = type(self).BoundaryAggrTickBehavior.NEXT_WINDOW
		self.boundary_aggr_tick_behavior = boundary_aggr_tick_behavior
		self._default_pass_source_delay_msec = 0
		self.pass_source_delay_msec = pass_source_delay_msec
		self._default_append_output_field_name = True
		self.append_output_field_name = append_output_field_name
		self._default_group_by = ""
		self.group_by = group_by
		self._default_aggregation_source = ""
		self.aggregation_source = aggregation_source
		self._default_pass_source = ""
		self.pass_source = pass_source
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_aggregation(self, value):
		self.aggregation = value
		return self

	def set_bucket_interval(self, value):
		self.bucket_interval = value
		return self

	def set_bucket_interval_units(self, value):
		self.bucket_interval_units = value
		return self

	def set_boundary_aggr_tick_behavior(self, value):
		self.boundary_aggr_tick_behavior = value
		return self

	def set_pass_source_delay_msec(self, value):
		self.pass_source_delay_msec = value
		return self

	def set_append_output_field_name(self, value):
		self.append_output_field_name = value
		return self

	def set_group_by(self, value):
		self.group_by = value
		return self

	def set_aggregation_source(self, value):
		self.aggregation_source = value
		return self

	def set_pass_source(self, value):
		self.pass_source = value
		return self

	@staticmethod
	def _get_name():
		return "JOIN_WITH_AGGREGATED_WINDOW"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.aggregation != "": 
			desc += "AGGREGATION=" + py_to_str(self.aggregation) + ","
		if self.bucket_interval != 0: 
			desc += "BUCKET_INTERVAL=" + py_to_str(self.bucket_interval) + ","
		if self.bucket_interval_units != self.BucketIntervalUnits.SECONDS: 
			desc += "BUCKET_INTERVAL_UNITS=" + py_to_str(self.bucket_interval_units) + ","
		if self.boundary_aggr_tick_behavior != self.BoundaryAggrTickBehavior.NEXT_WINDOW: 
			desc += "BOUNDARY_AGGR_TICK_BEHAVIOR=" + py_to_str(self.boundary_aggr_tick_behavior) + ","
		if self.pass_source_delay_msec != 0: 
			desc += "PASS_SOURCE_DELAY_MSEC=" + py_to_str(self.pass_source_delay_msec) + ","
		if self.append_output_field_name != True: 
			desc += "APPEND_OUTPUT_FIELD_NAME=" + py_to_str(self.append_output_field_name) + ","
		if self.group_by != "": 
			desc += "GROUP_BY=" + py_to_str(self.group_by) + ","
		if self.aggregation_source != "": 
			desc += "AGGREGATION_SOURCE=" + py_to_str(self.aggregation_source) + ","
		if self.pass_source != "": 
			desc += "PASS_SOURCE=" + py_to_str(self.pass_source) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class ResolveEnums(_graph_components.EpBase):
	"""
		

  RESOLVE_ENUMS

Type: Transformer


  Description:
  Replaces certain tick fields of integer types
  with string fields containing their corresponding string representation.
  See Enum Values for more information.


Input: A time series of ticks.

Output: A time series of ticks.

Examples:

RESOLVE_ENUMS

The EP has no parameters at the moment.


	"""
	class Parameters:
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = []
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["stack_info", "_used_strings"]

	def __init__(self):
		_graph_components.EpBase.__init__(self, "RESOLVE_ENUMS")
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	@staticmethod
	def _get_name():
		return "RESOLVE_ENUMS"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class EncodingConverter(_graph_components.EpBase):
	"""
		

ENCODING_CONVERTER

Type: Transformer

Description: Converts the values of selected target string or
varstring fields in each input tick to a specified encoding, with the
converted values output in varstring format.
NOTE: the most important supported encoding names are: ASCII, UTF-8, UTF-16, UTF-32, Latin-1,
Windows-1252, MacRoman, ISO 8859-1.
For the full list of existing string encoding names, please check the link

Python
class name:&nbsp;EncodingConverter

Input: A time series of ticks.

Output: A time series of ticks.

Parameters:


  FIELDS (string)
    Specifies a comma-separated list of fields to be converted. If
the parameter is empty, all string and varstring fields will be
converted..

  
  INPUT_ENCODING (string)
    The encoding format for the input data.

  
  INPUT_ENCODING (string)
    The encoding format for the output data.

  

Examples:

ENCODING_CONVERTER
(FIELDS="SPECIAL_NAME,VARSTRING_FILED_NAME",INPUT_ENCODING="ASCII",OUTPUT_ENCODING=
"UTF-8")

ENCODING_CONVERTER
(INPUT_ENCODING="ASCII",OUTPUT_ENCODING= "UTF-8")

See the ENCODING_CONVERTER_EXAMPLE
in ENCODING_CONVERTER_EXAMPLE.


	"""
	class Parameters:
		fields = "FIELDS"
		input_encoding = "INPUT_ENCODING"
		output_encoding = "OUTPUT_ENCODING"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["fields", "input_encoding", "output_encoding"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["fields", "_default_fields", "input_encoding", "_default_input_encoding", "output_encoding", "_default_output_encoding", "stack_info", "_used_strings"]

	def __init__(self, fields="", input_encoding="", output_encoding=""):
		_graph_components.EpBase.__init__(self, "ENCODING_CONVERTER")
		self._default_fields = ""
		self.fields = fields
		self._default_input_encoding = ""
		self.input_encoding = input_encoding
		self._default_output_encoding = ""
		self.output_encoding = output_encoding
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_fields(self, value):
		self.fields = value
		return self

	def set_input_encoding(self, value):
		self.input_encoding = value
		return self

	def set_output_encoding(self, value):
		self.output_encoding = value
		return self

	@staticmethod
	def _get_name():
		return "ENCODING_CONVERTER"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.fields != "": 
			desc += "FIELDS=" + py_to_str(self.fields) + ","
		if self.input_encoding != "": 
			desc += "INPUT_ENCODING=" + py_to_str(self.input_encoding) + ","
		if self.output_encoding != "": 
			desc += "OUTPUT_ENCODING=" + py_to_str(self.output_encoding) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class Om_matchingEngineInitialState(_graph_components.EpBase):
	"""
		

OM::MATCHING_ENGINE_INITIAL_STATE

Type: Transformer

Description: &nbsp;This EP is used, under the hood, by OM::MATCHING_ENGINE_SIMPLE EP in
order to recover its initial state when a query that includes OM::MATCHING_ENGINE_SIMPLE EP is
starting. &nbsp;

Based on a specified set of fields (referred below as state key
fields) for which every unique combination of their values has an
associated state, this EP outputs the latest tick before the start time
for each unique combination of values of those fields, followed by the
last atomic group of ticks prior to the query's start time. In other
words, this EP produces the time series state at the start time of the
query, followed by the last, possibly incomplete, atomic group of
ticks. The time series state, if not empty, is surrounded by a tick
with TICK_STATUS=32 (beginning of
the atomic group) and a tick with TICK_STATUS=33
(end of the atomic group). The atomic group that follows it consists of
a single tick or of a group of ticks that begins with a tick with TICK_STATUS=32 (beginning of the atomic
group). The latter group is incomplete if it does not end by a tick
with TICK_STATUS=33 (end of the
atomic group). If the last atomic group of ticks is incomplete, the
time series state does not account for this atomic tick group.

While searching for all unique combinations of values of state key
fields, this EP inspects ticks from the start time back into the past,
until one of the following occurs:


  A group of ticks that represent the current state of a time
series (state ticks) are found. The EP stops at the last tick from this
group going backward. The TICK_STATUS=16
is used to identify state ticks.
  A tick that indicates that the state is empty is found. The TICK_STATUS=31 is used to identify
such ticks.
  The EP goes back MAX_GO_BACK_SECONDS.
In this case, an exception is thrown because the initial state of the
time series was not found.

Python
class name:
Om_matchingEngineInitialState

Input: A time series of ticks.

Output: A time series of ticks, starting from the ticks that
represent a state of the input time series at the start time and
followed by the last atomic group of ticks prior to the query's start
time.

Parameters:


  MAX_GO_BACK_SECONDS (integer)
    The number of seconds to go back in search of the full state of
a time series, which could be an empty state.
Default: 86400 (one day)

  
  STATE_KEY_FIELD_NAMES (string)
    A comma-separated list of state key fields (that is, fields for
which every unique combination of their values has associated state).
Default value: automatically discovered state key fields of input time
series, associated with its tick descriptor. If an input time series
includes the BUY_SELL_FLAG field,
this field is added to automatically discovered state key fields.

  


	"""
	class Parameters:
		max_go_back_seconds = "MAX_GO_BACK_SECONDS"
		state_key_field_names = "STATE_KEY_FIELD_NAMES"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["max_go_back_seconds", "state_key_field_names"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["max_go_back_seconds", "_default_max_go_back_seconds", "state_key_field_names", "_default_state_key_field_names", "stack_info", "_used_strings"]

	def __init__(self, max_go_back_seconds=86400, state_key_field_names=""):
		_graph_components.EpBase.__init__(self, "OM::MATCHING_ENGINE_INITIAL_STATE")
		self._default_max_go_back_seconds = 86400
		self.max_go_back_seconds = max_go_back_seconds
		self._default_state_key_field_names = ""
		self.state_key_field_names = state_key_field_names
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_max_go_back_seconds(self, value):
		self.max_go_back_seconds = value
		return self

	def set_state_key_field_names(self, value):
		self.state_key_field_names = value
		return self

	@staticmethod
	def _get_name():
		return "OM::MATCHING_ENGINE_INITIAL_STATE"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.max_go_back_seconds != 86400: 
			desc += "MAX_GO_BACK_SECONDS=" + py_to_str(self.max_go_back_seconds) + ","
		if self.state_key_field_names != "": 
			desc += "STATE_KEY_FIELD_NAMES=" + py_to_str(self.state_key_field_names) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class CodeEp(_graph_components.EpBase):
	"""
		

CODE

Type: Other

Description: Allows for writing a UDF (refer to the OneTick
developer's guide for details) code directly in a query graph as a
parameter to this EP, automating further work of getting it compiled
(whenever applicable), loaded and the respective instance of CODE EP
behaving as if that UDF was used in place of it. This EP does not pay
any performance penalty with respect to real UDFs and built-in EPs,
except for compilation, which is part of the execution and occurs once
only for each new piece of code.

Access control

In order to use this event processor, access control must be setup
(refer to the OneTick Installation and Administration Guide). For each
role, one can specify a comma-separated list of languages that can be
used by that role (e.g., &lt;allow
role="developers" languages="C++,Perl"/&gt;). By omitting the
"languages" parameter, all languages are allowed. If a user has several
roles and at least one of them omits the "languages" parameter or
allows the required language, then access control is passed. One can
also specify comma-separated list of "languages", that can be used, in
the event processor level (not only role level). This means that
"languages" listed in the event processor level are allowed for all
users (whose roles aren't mentioned under the section for this event
processor, or user does not have any role in the access control file).
Role level permissions override event processor level permissions for
the users whose roles are mentioned in access control for this ep.

C++ and Java setup

Source code in these languages is compiled using makefiles. Users
may have their own or may use default makefiles provided in the OneTick
distribution under one_market_data/one_tick/src/code_ep_makefiles.
It has platform-specific makefiles for C++ language at ./cpp/Makefile_CODE.nt and ./cpp/Makefile_CODE.unix for Windows and
Unix-likes respectively, as well as a portable makefile for Java at ./java/Makefile_CODE_java.nt. DYNAMIC_CPP_CODE_MAKEFILE for C++ and DYNAMIC_JAVA_CODE_MAKEFILE for Java serve
the purpose of overriding paths to makefiles in the OneTick main
configuration file.

The maker executable name is subject to be controlled via the
configuration variable MAKE_COMMAND,
the default being nmake on Windows and make on Unix. The maker
executable is invoked with the following parameters set (using
name=value syntax, usual for make):


  ONE_TICK_INSTALL_DIR
    The path to the OneTick installation directory.

  
  STORE_DIR
    The path to the source file directory within the repository.

  
  CODE_NAME
    The real name of the source file, without an extension. In fact,
this is &lt;CODE_NAME&gt;_&lt;SIGNATURE&gt;.

  

Users may not only write their own makefiles, but may also alternate
the underlying build-system providing their own makers and input files
to them. In any case, invocation of a makefile has to either result in
the corresponding library or an error log file with compilation errors
present at the expected path (within DYNAMIC_CPP_CODE_DIR
and DYNAMIC_JAVA_CODE_DIR).

Perl setup

DYNAMIC_PERL_CODE_DIR, together
with the usual environment variables used for real Perl UDFs (see the
Perl API documentation for details), are sufficient.

Python setup

DYNAMIC_PYTHON_CODE_DIR, together
with the usual environment variables used for real Python UDFs (see the
Python API documentation for details), are sufficient. Also, the UDF_PY_DIR ONE_TICK_CONFIG
parameter should contain the path to the Python library repository (as DYNAMIC_PYTHON_CODE_DIR does).



Summary of ONE_TICK_CONFIG parameters:


  SHOW_DEV_ONLY_EP
    Switches CODE EP appearance among the list of available EPs.

  
  DYNAMIC_CPP_CODE_MAKEFILE
    The path to an alternative maker file for C++.

  
  DYNAMIC_CPP_CODE_DIR
    The path to the C++ library repository.

  
  DYNAMIC_JAVA_CODE_MAKEFILE
    The path to an alternative maker file for Java.

  
  DYNAMIC_JAVA_CODE_DIR
    The path to the Java library repository.

  
  DYNAMIC_PERL_CODE_DIR
    The path to the Perl library repository.

  
  DYNAMIC_PYTHON_CODE_DIR
    The path to the Python library repository.

  

  MAKE_COMMAND
    The path to an alternative maker.

  

Python
class name:&nbsp;CodeEp

Input: A time series of ticks.

Output: A time series of ticks.

Parameters:


  LANGUAGE (enum)
    UDF language. Possible choices are C++,
    JAVA, PERL
and Python.
Default: C++

  
  UDF_TYPE (enum)
    UDF type. Possible types are GENERIC,
    FILTER and AGGREGATION.
Default: GENERIC

  
  CODE_ENTRY_METHOD (enum)
    Two possible values, ONLY_EVENT_HANDLER
and FULL_UDF, indicate the user's
intention of either providing a body for the main event handler method
only (process_event for generic and aggregation type UDFs and
tick_matches for filter type UDFs) or the whole UDF code, respectively.
The latter case is applicable to more complex UDFs, when default
implementations are not appropriate for other methods as well.
Default: ONLY_EVENT_HANDLER

  
  CODE (string)
    UDF code. In GUI mode, this parameter can be set only after the
above three parameters are chosen, which allows for opening a
convenient code editor with language-driven syntax highlighting an
appropriate code template suggestion based on the values of UDF_TYPE and CODE_ENTRY_METHOD
parameters. Note: Beware of placeholders &lt;CODE_NAME&gt;
and &lt;SIGNATURE&gt; in templates
-- these should not be modified by users (see below).


  
  CODE_NAME (string)
    The UDF name, used similar to the real UDF name (see below for
details).

  
  CODE_PARAMETERS (string)
    A comma-separated list of name=value pairs that are passed as
parameters directly to the generated UDF.

  
  SUPPORTS_MULTIPLE_SOURCES
(Boolean)
    Should be set to true when the compiled UDF is supposed to
support multiple sources.
Default: false

  
  REMOVE_GEN_CODE_WHEN_DONE
(Boolean)
    Normally, when compilation of a piece of code fails, all
intermediate files (language- and platform-specific source and object
files, log files, etc.) are removed to avoid garbage accumulation. A
false value of this parameter prevents those files from being removed
if needed for some reason.
Default: true

  
  MACRO_PREFIX (char)
    If the value of this parameter is set, the CODE field is checked
for the presence of macros, which are expected to begin with the
character represented by this value.
Default: empty

  

Storage

All intermediate files (language- and platform-specific source and
object files, log files, etc.) and final libraries produced during
execution of graphs containing instances of CODE EP are centralized in
a single repository for a given programming language. Repositories have
different structures depending on UDF language and platform. Paths to
the repositories are specified in the main configuration file and are
per UDF language: DYNAMIC_CPP_CODE_DIR,
DYNAMIC_JAVA_CODE_DIR, DYNAMIC_PERL_CODE_DIR and DYNAMIC_PYTHON_CODE_DIR for C++, Java,
Perl and Python languages, respectively. Before manual clean up, please
make sure that no one tick process is accessing these repositories.

All files for a particular piece of code are formed by appending a
signature to the value of the CODE_NAME
parameter. A signature is generated internally for each piece of code
to efficiently differentiate between different UDFs with the same name
as well as different versions of the same UDF, to avoid clashes and
multiple compilations of the same piece of code. Placeholders &lt;CODE_NAME&gt; and &lt;SIGNATURE&gt;, used in code templates
opened in the GUI for user convenience, are treated internally and
should not in any way be modified by users.


	"""
	class Parameters:
		language = "LANGUAGE"
		udf_type = "UDF_TYPE"
		code_entry_method = "CODE_ENTRY_METHOD"
		code = "CODE"
		code_name = "CODE_NAME"
		code_parameters = "CODE_PARAMETERS"
		supports_multiple_sources = "SUPPORTS_MULTIPLE_SOURCES"
		remove_gen_code_when_done = "REMOVE_GEN_CODE_WHEN_DONE"
		macro_prefix = "MACRO_PREFIX"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["language", "udf_type", "code_entry_method", "code", "code_name", "code_parameters", "supports_multiple_sources", "remove_gen_code_when_done", "macro_prefix"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["language", "_default_language", "udf_type", "_default_udf_type", "code_entry_method", "_default_code_entry_method", "code", "_default_code", "code_name", "_default_code_name", "code_parameters", "_default_code_parameters", "supports_multiple_sources", "_default_supports_multiple_sources", "remove_gen_code_when_done", "_default_remove_gen_code_when_done", "macro_prefix", "_default_macro_prefix", "stack_info", "_used_strings"]

	class Language:
		C_PLUS__PLUS_ = "C++"
		JAVA = "JAVA"
		PERL = "PERL"
		PYTHON = "PYTHON"

	class UdfType:
		AGGREGATION = "AGGREGATION"
		FILTER = "FILTER"
		GENERIC = "GENERIC"

	class CodeEntryMethod:
		FULL_UDF = "FULL_UDF"
		ONLY_EVENT_HANDLER = "ONLY_EVENT_HANDLER"

	def __init__(self, language=Language.C_PLUS__PLUS_, udf_type=UdfType.GENERIC, code_entry_method=CodeEntryMethod.ONLY_EVENT_HANDLER, code="", code_name="", code_parameters="", supports_multiple_sources=False, remove_gen_code_when_done=True, macro_prefix=""):
		_graph_components.EpBase.__init__(self, "CODE")
		self._default_language = type(self).Language.C_PLUS__PLUS_
		self.language = language
		self._default_udf_type = type(self).UdfType.GENERIC
		self.udf_type = udf_type
		self._default_code_entry_method = type(self).CodeEntryMethod.ONLY_EVENT_HANDLER
		self.code_entry_method = code_entry_method
		self._default_code = ""
		self.code = code
		self._default_code_name = ""
		self.code_name = code_name
		self._default_code_parameters = ""
		self.code_parameters = code_parameters
		self._default_supports_multiple_sources = False
		self.supports_multiple_sources = supports_multiple_sources
		self._default_remove_gen_code_when_done = True
		self.remove_gen_code_when_done = remove_gen_code_when_done
		self._default_macro_prefix = ""
		self.macro_prefix = macro_prefix
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_language(self, value):
		self.language = value
		return self

	def set_udf_type(self, value):
		self.udf_type = value
		return self

	def set_code_entry_method(self, value):
		self.code_entry_method = value
		return self

	def set_code(self, value):
		self.code = value
		return self

	def set_code_name(self, value):
		self.code_name = value
		return self

	def set_code_parameters(self, value):
		self.code_parameters = value
		return self

	def set_supports_multiple_sources(self, value):
		self.supports_multiple_sources = value
		return self

	def set_remove_gen_code_when_done(self, value):
		self.remove_gen_code_when_done = value
		return self

	def set_macro_prefix(self, value):
		self.macro_prefix = value
		return self

	@staticmethod
	def _get_name():
		return "CODE"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.language != self.Language.C_PLUS__PLUS_: 
			desc += "LANGUAGE=" + py_to_str(self.language) + ","
		if self.udf_type != self.UdfType.GENERIC: 
			desc += "UDF_TYPE=" + py_to_str(self.udf_type) + ","
		if self.code_entry_method != self.CodeEntryMethod.ONLY_EVENT_HANDLER: 
			desc += "CODE_ENTRY_METHOD=" + py_to_str(self.code_entry_method) + ","
		if self.code != "": 
			desc += "CODE=" + py_to_str(self.code) + ","
		if self.code_name != "": 
			desc += "CODE_NAME=" + py_to_str(self.code_name) + ","
		if self.code_parameters != "": 
			desc += "CODE_PARAMETERS=" + py_to_str(self.code_parameters) + ","
		if self.supports_multiple_sources != False: 
			desc += "SUPPORTS_MULTIPLE_SOURCES=" + py_to_str(self.supports_multiple_sources) + ","
		if self.remove_gen_code_when_done != True: 
			desc += "REMOVE_GEN_CODE_WHEN_DONE=" + py_to_str(self.remove_gen_code_when_done) + ","
		if self.macro_prefix != "": 
			desc += "MACRO_PREFIX=" + py_to_str(self.macro_prefix) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


Code = CodeEp


class ShowCorrectedTicks(_graph_components.EpBase):
	"""
		

SHOW_CORRECTED_TICKS

Type: Filter

Description: This event processor shows ticks which were
corrected or canceled within the interval of the query. Only corrected
and correction ticks will be propagated.

A TICK_STATUS field is added to each tick.

For Corrections:


  Old tick TICK_STATUS = 5
  New tick TICK_STATUS = 6

For Cancellations:


  Old tick TICK_STATUS = 4
  New tick TICK_STATUS = 7

Python
class name:&nbsp;ShowCorrectedTicks

Input: A time series of ticks.

Output: A time series of ticks.

Parameters: There are no parameters for this event processor.

Examples: Show all ticks corrected within the time interval:

SHOW_CORRECTED_TICKS ()

See the SHOW_CORRECTED_TICKS
example in FILTER_EXAMPLES.otq.


	"""
	class Parameters:
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = []
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["stack_info", "_used_strings"]

	def __init__(self):
		_graph_components.EpBase.__init__(self, "SHOW_CORRECTED_TICKS")
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	@staticmethod
	def _get_name():
		return "SHOW_CORRECTED_TICKS"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class ShowDataQuality(_graph_components.EpBase):
	"""
		

SHOW_DATA_QUALITY

Type: Filter

Description: This event processor shows data quality events
within the interval of the query. A TICK_STATUS field is added
to each tick.

Python
class name:
ShowDataQuality

Input: A time series of ticks.

Output: A time series of data quality events.

Parameters: There are no parameters for this event processor.

Examples: Show all data quality events within time interval:

SHOW_DATA_QUALITY ()

See the SHOW_DATA_QUALITY example
in FILTER_EXAMPLES.otq.


	"""
	class Parameters:
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = []
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["stack_info", "_used_strings"]

	def __init__(self):
		_graph_components.EpBase.__init__(self, "SHOW_DATA_QUALITY")
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	@staticmethod
	def _get_name():
		return "SHOW_DATA_QUALITY"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class ShowSymbolErrors(_graph_components.EpBase):
	"""
		

SHOW_SYMBOL_ERRORS

Type: Filter

Description: This event processor propagates a tick
representing per-symbol error.

Python
class name:&nbsp;ShowSymbolErrors

Input: A time series of ticks.

Output: A time series of ticks.

Parameters: There are no parameters for this event processor.

Examples: Show all errors within time interval:

SHOW_SYMBOL_ERRORS ()

See the SHOW_SYMBOL_ERRORS
example in FILTER_EXAMPLES.otq.


	"""
	class Parameters:
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = []
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["stack_info", "_used_strings"]

	def __init__(self):
		_graph_components.EpBase.__init__(self, "SHOW_SYMBOL_ERRORS")
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	@staticmethod
	def _get_name():
		return "SHOW_SYMBOL_ERRORS"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class ShowHeartbeats(_graph_components.EpBase):
	"""
		

SHOW_HEARTBEATS

Type: Filter

Description: This event processor generates a tick per
heartbeat (timer event). The CURRENT_TIME field carries the time of the
heartbeat.

Python
class name:
ShowHeartbeats

Input: A time series of ticks.

Output: A tick for each arriving heartbeat.

Parameters:


  HEARTBEAT_LISTEN_MODE (enum)
    The possible values are ACTIVE and PASSIVE. If the listening
mode is ACTIVE the EP will show all possible heartbeats which can be
passed to the query, otherwise it will show the same frequency of
heartbeats which will be actually passed to the query after removing
the SHOW_HEARTBEATS EP.
For example if a query consists of a single aggregation EP with time
buckets, then it will get a heartbeat once per each bucket interval.
But after adding SHOW_HEARTBEATS EP with ACTIVE heartbeat listening
mode to the it the frequency of heartbeats will increase to the maximal
possible value and they will be shown by the EP. To see the heartbeats
of the same frequency as in initial query the PASSIVE listening mode
should be used.
Default: ACTIVE

  

Examples: Show all arriving heartbeats:

SHOW_HEARTBEATS (HEARTBEAT_LISTENING_MODE = PASSIVE)

See the SHOW_HEARTBEATS example
in FILTER_EXAMPLES.otq.


	"""
	class Parameters:
		heartbeat_listen_mode = "HEARTBEAT_LISTEN_MODE"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["heartbeat_listen_mode"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["heartbeat_listen_mode", "_default_heartbeat_listen_mode", "stack_info", "_used_strings"]

	class HeartbeatListenMode:
		ACTIVE = "ACTIVE"
		PASSIVE = "PASSIVE"

	def __init__(self, heartbeat_listen_mode=HeartbeatListenMode.ACTIVE):
		_graph_components.EpBase.__init__(self, "SHOW_HEARTBEATS")
		self._default_heartbeat_listen_mode = type(self).HeartbeatListenMode.ACTIVE
		self.heartbeat_listen_mode = heartbeat_listen_mode
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_heartbeat_listen_mode(self, value):
		self.heartbeat_listen_mode = value
		return self

	@staticmethod
	def _get_name():
		return "SHOW_HEARTBEATS"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.heartbeat_listen_mode != self.HeartbeatListenMode.ACTIVE: 
			desc += "HEARTBEAT_LISTEN_MODE=" + py_to_str(self.heartbeat_listen_mode) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class ShowSpecialMsgs(_graph_components.EpBase):
	"""
		

SHOW_SPECIAL_MSGS

Type: Filter

Description: This event processor shows ticks that represent
internal events generated by OneTick analytics to facilitate graph
processing.
Currently, only ticks that represent PREPARE_FOR_RUNNING_QUERY events
are produced.

Python
class name:&nbsp;ShowSpecialMsgs

Input: A time series of ticks

Output: A time series of ticks that represent internal events
generated by OneTick analytics to facilitate graph processing

Parameters: There are no parameters for this event processor.


	"""
	class Parameters:
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = []
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["stack_info", "_used_strings"]

	def __init__(self):
		_graph_components.EpBase.__init__(self, "SHOW_SPECIAL_MSGS")
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	@staticmethod
	def _get_name():
		return "SHOW_SPECIAL_MSGS"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class ShowTickDescriptor(_graph_components.EpBase):
	"""
		

SHOW_TICK_DESCRIPTOR

Type: Filter

Description: This event processor (EP) shows either
properties of each field of every arriving tick descriptor or
properties of each arriving descriptor, depending on the value of
LEVEL_OF_DETAIL parameter. In order to be able to see where one tick
descriptor ends and another one starts, output attribute INDEX carries
the order of arrival of a tick descriptor, starting from 1. The other
attributes of per-field output are NAME, TYPE, SIZE (in bytes),
STATE_KEY_FLAG (0/1) to indicate whether the field is a part of the
state key, and TYPE_FOR_EP to indicate which type specifier must be
used in EPs (ADD_FIELD , UPDATE_FIELD , etc.). The attributes of
per-tick descriptor output are INDEX, PROPERTY_NAME and PROPERTY_VALUE.

This EP shows the query-driven dynamic tick descriptor of the tick
stream, because the field structure of the tick stream can change
within the query by having PASSTHROUGH FIELDS=PRICE, SIZE, or MERGE on
QTE and TRD, for example. SHOW_TICK_DESCRIPTOR will display that
dynamic tick structure.

Per-tick descriptor output always includes a dynamically computed
property TICK_SIZE_BYTES, with property value set to the length of the
fixed-size part of a tick.

Alternatively, to see the static tick descriptor of the database,
use the SHOW_TICK_DESCRIPTOR_IN_DB
EP.

Python
class name:&nbsp;ShowTickDescriptor

Input: A time series of ticks.

Output: A tick for each field of each arriving tick
descriptor or a tick for each arriving tick descriptor, depending on
the value of LEVEL_OF_DETAIL parameter.

Parameters:


  LEVEL_OF_DETAIL (string)
    Supported values are PER_FIELD and PER_TICK_DESCRIPTOR.
Default: PER_FIELD

  

Examples: Show all fields of all arriving tick descriptors:

SHOW_TICK_DESCRIPTOR ()

See the SHOW_TICK_DESCRIPTOR
example in FILTER_EXAMPLES.otq.


	"""
	class Parameters:
		level_of_detail = "LEVEL_OF_DETAIL"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["level_of_detail"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["level_of_detail", "_default_level_of_detail", "stack_info", "_used_strings"]

	class LevelOfDetail:
		PER_FIELD = "PER_FIELD"
		PER_TICK_DESCRIPTOR = "PER_TICK_DESCRIPTOR"

	def __init__(self, level_of_detail=LevelOfDetail.PER_FIELD):
		_graph_components.EpBase.__init__(self, "SHOW_TICK_DESCRIPTOR")
		self._default_level_of_detail = type(self).LevelOfDetail.PER_FIELD
		self.level_of_detail = level_of_detail
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_level_of_detail(self, value):
		self.level_of_detail = value
		return self

	@staticmethod
	def _get_name():
		return "SHOW_TICK_DESCRIPTOR"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.level_of_detail != self.LevelOfDetail.PER_FIELD: 
			desc += "LEVEL_OF_DETAIL=" + py_to_str(self.level_of_detail) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class DbShowLastTickDescriptor(_graph_components.EpBase):
	"""
		

DB/SHOW_LAST_TICK_DESCRIPTOR

Type: Filter

Description: This event processor shows the last tick descriptor for the given tick type.
It must be the first (source) event processor on the graph.

Python
class name:&nbsp;DbShowLastTickDescriptor

Input: None

Output: Information about the last tick description fields: FIELD_NAME(string), FIELD_SIZE(integer), FIELD_TYPE_NAME(string), SCHEMA_INDEX(integer), PARTITION_NAME(string).

Parameters:


  USE_CACHE (Boolean)
    If set to true, the event processor will use the cached results
(see the META_DATA_CACHE_FILE description in the OneTick Installation
and Administration Guide) if available. If the cache is available only 
up to some point, the rest of the data will be calculated first. If set to false, all the data will be
re-calculated even if cache is available. Under no circumstances will
the cache be updated.
Default: true

  
  INCLUDE_MEMDB (Boolean)
  SHOW_INACCESSIBLE_DATA_START (Boolean)
    If set to true and the data is not accessible from any point within the requested range, 
      an additional tick will be displayed with the following special values and with timestamp set to the start of inaccessible data start time:
      FIELD_NAME="INACCESSIBLE_DATA_START".
      FIELD_SIZE=0.
      FIELD_TYPE_NAME="TYPE_INT32".
      SCHEMA_INDEX=-1.
      
Default: false

  


	"""
	class Parameters:
		include_memdb = "INCLUDE_MEMDB"
		use_cache = "USE_CACHE"
		show_inaccessible_data_start = "SHOW_INACCESSIBLE_DATA_START"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["include_memdb", "use_cache", "show_inaccessible_data_start"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["include_memdb", "_default_include_memdb", "use_cache", "_default_use_cache", "show_inaccessible_data_start", "_default_show_inaccessible_data_start", "stack_info", "_used_strings"]

	def __init__(self, include_memdb=True, use_cache=True, show_inaccessible_data_start=False):
		_graph_components.EpBase.__init__(self, "DB/SHOW_LAST_TICK_DESCRIPTOR")
		self._default_include_memdb = True
		self.include_memdb = include_memdb
		self._default_use_cache = True
		self.use_cache = use_cache
		self._default_show_inaccessible_data_start = False
		self.show_inaccessible_data_start = show_inaccessible_data_start
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_include_memdb(self, value):
		self.include_memdb = value
		return self

	def set_use_cache(self, value):
		self.use_cache = value
		return self

	def set_show_inaccessible_data_start(self, value):
		self.show_inaccessible_data_start = value
		return self

	@staticmethod
	def _get_name():
		return "DB/SHOW_LAST_TICK_DESCRIPTOR"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.include_memdb != True: 
			desc += "INCLUDE_MEMDB=" + py_to_str(self.include_memdb) + ","
		if self.use_cache != True: 
			desc += "USE_CACHE=" + py_to_str(self.use_cache) + ","
		if self.show_inaccessible_data_start != False: 
			desc += "SHOW_INACCESSIBLE_DATA_START=" + py_to_str(self.show_inaccessible_data_start) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class MktActivity(_graph_components.EpBase):
	"""
		

MKT_ACTIVITY

Type: Transformer

Description: Adds a string field named MKT_ACTIVITY to each
tick in the input tick stream. The value of this field is set to the
union of session flags that apply for the security at the time of the
tick, as specified in the calendar sections of the reference database
(see Reference Database Guide). The calendar can either be specified
explicitly by name (the CALENDAR sections of the reference database are
assumed to contain a calendar with such a name), or default to the
security- or exchange-level calendars for the queried symbol (the
SYMBOL_CALENDAR or EXCH_CALENDAR sections of the reference database).
The latter case requires a non-zero symbol date to be specified for
queried symbols.

Python
class name:&nbsp;MktActivity

Input: A time series of ticks.

Output: A time series of ticks, with session information.

Parameters:


  CALENDAR_NAME (string)
    The calendar name to choose for the respective calendar from the
CALENDAR sections of the reference database. Can not be used with
'CALENDAR_FIELD_NAME' parameter specified.
Default: empty

  
  CALENDAR_FIELD_NAME (string)
    The name of the field that contains per-tick calendar name to
choose for the respective calendar from the CALENDAR sections of the
reference database.
Default: empty

  

Examples: Add the MKT_ACTIVITY column to every tick:

PASSTHROUGH,MKT_ACTIVITY

Note: When applying MKT_ACTIVITY to aggregated data, please
take into account that session flags may change during the aggregation
bucket. The field MKT_ACTIVITY, in this case, will represent the
session flags at the time assigned to the bucket, which may be
different from the session flags at some times during the bucket.


	"""
	class Parameters:
		calendar_name = "CALENDAR_NAME"
		calendar_field_name = "CALENDAR_FIELD_NAME"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["calendar_name", "calendar_field_name"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["calendar_name", "_default_calendar_name", "calendar_field_name", "_default_calendar_field_name", "stack_info", "_used_strings"]

	def __init__(self, calendar_name="", calendar_field_name=""):
		_graph_components.EpBase.__init__(self, "MKT_ACTIVITY")
		self._default_calendar_name = ""
		self.calendar_name = calendar_name
		self._default_calendar_field_name = ""
		self.calendar_field_name = calendar_field_name
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_calendar_name(self, value):
		self.calendar_name = value
		return self

	def set_calendar_field_name(self, value):
		self.calendar_field_name = value
		return self

	@staticmethod
	def _get_name():
		return "MKT_ACTIVITY"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.calendar_name != "": 
			desc += "CALENDAR_NAME=" + py_to_str(self.calendar_name) + ","
		if self.calendar_field_name != "": 
			desc += "CALENDAR_FIELD_NAME=" + py_to_str(self.calendar_field_name) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class DbShowTickTypes(_graph_components.EpBase):
	"""
		

DB/SHOW_TICK_TYPES

Type: Filter

Description: This event processor shows tick types for a
specified database. It must be the first (source) event processor on
the graph.

Input symbols for the queries that involve this event processor
should be of the form &lt;db_name&gt;:: or &lt;db_name&gt;::&lt;some
symbol name&gt;.

Python
class name:&nbsp;DbShowTickTypes

Input: None

Output: A series of tick type names.

Parameters:


  USE_CACHE (Boolean)
    If set to true, the event processor will use the cached results
(see the META_DATA_CACHE_FILE description in the OneTick Installation
and Administration Guide) if available. If the cache is available only
up to some point, the rest of the data will be calculated and merged
with the cache contents. If set to false, all the data will be
re-calculated even if cache is available. Under no circumstances will
the cache be updated.
Default: true

  
  SHOW_SCHEMA (Boolean)
    If set to true, for each tick type its schema will be shown
(names, types and sizes of fields).
Default: false

  
  SCHEMAS_TO_SHOW (enumerated)
    Have an effect only if SHOW_SCHEMA is true. If set to UNION_OF_FIELDS, only one of the occurrence
of the field with its type is displayed for whatever field name ever
was for that tick type. When set to EACH_SCHEMA,
each field for a given tick type is displayed as many times as it
appears in schemas. Indexing of various schemes is performed. It is
possible that the same field will have different types for different
dates.
Default: UNION_OF_FIELDS

  
  INCLUDE_MEMDB (Boolean)

Examples:

Show all tick types for a database and their schemas (if requested):

SHOW_TICK_TYPES ()

See the DB_SHOW_TICK_TYPES
example in FILTER_EXAMPLES.otq.


	"""
	class Parameters:
		use_cache = "USE_CACHE"
		show_schema = "SHOW_SCHEMA"
		schemas_to_show = "SCHEMAS_TO_SHOW"
		include_memdb = "INCLUDE_MEMDB"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["use_cache", "show_schema", "schemas_to_show", "include_memdb"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["use_cache", "_default_use_cache", "show_schema", "_default_show_schema", "schemas_to_show", "_default_schemas_to_show", "include_memdb", "_default_include_memdb", "stack_info", "_used_strings"]

	class SchemasToShow:
		EACH_SCHEMA = "EACH_SCHEMA"
		UNION_OF_FIELDS = "UNION_OF_FIELDS"

	def __init__(self, use_cache=True, show_schema=False, schemas_to_show=SchemasToShow.UNION_OF_FIELDS, include_memdb=True):
		_graph_components.EpBase.__init__(self, "DB/SHOW_TICK_TYPES")
		self._default_use_cache = True
		self.use_cache = use_cache
		self._default_show_schema = False
		self.show_schema = show_schema
		self._default_schemas_to_show = type(self).SchemasToShow.UNION_OF_FIELDS
		self.schemas_to_show = schemas_to_show
		self._default_include_memdb = True
		self.include_memdb = include_memdb
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_use_cache(self, value):
		self.use_cache = value
		return self

	def set_show_schema(self, value):
		self.show_schema = value
		return self

	def set_schemas_to_show(self, value):
		self.schemas_to_show = value
		return self

	def set_include_memdb(self, value):
		self.include_memdb = value
		return self

	@staticmethod
	def _get_name():
		return "DB/SHOW_TICK_TYPES"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.use_cache != True: 
			desc += "USE_CACHE=" + py_to_str(self.use_cache) + ","
		if self.show_schema != False: 
			desc += "SHOW_SCHEMA=" + py_to_str(self.show_schema) + ","
		if self.schemas_to_show != self.SchemasToShow.UNION_OF_FIELDS: 
			desc += "SCHEMAS_TO_SHOW=" + py_to_str(self.schemas_to_show) + ","
		if self.include_memdb != True: 
			desc += "INCLUDE_MEMDB=" + py_to_str(self.include_memdb) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class ShowHiddenTicks(_graph_components.EpBase):
	"""
		

SHOW_HIDDEN_TICKS

Type: Filter

Description: Propagates all of a tick's fields without
changing their values. Propagates all ticks, even those with a status
not equal to 0, which are normally hidden.

Use this EP to display all original ticks and correction ticks in
the same time series in the same time sequence in which they were
originally sent with a filter to remove unneeded TICK_STATUS values.

Python
class name:&nbsp;ShowHiddenTicks

Input: A time series of ticks.

Output: A time series of ticks, one for each input tick.

Parameters: None

Examples:

SHOW_HIDDEN_TICKS()

See the SHOW_HIDDEN_TICKS example
in FILTER_EXAMPLES.otq.


	"""
	class Parameters:
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = []
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["stack_info", "_used_strings"]

	def __init__(self):
		_graph_components.EpBase.__init__(self, "SHOW_HIDDEN_TICKS")
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	@staticmethod
	def _get_name():
		return "SHOW_HIDDEN_TICKS"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class HideTicksWithStatus(_graph_components.EpBase):
	"""
		

HIDE_TICKS_WITH_STATUS

Type: Other

Description: This EP has effect that is opposite to the
effect of SHOW_HIDDEN_TICKS EP. It
makes visible ticks with TICK_STATUS not equal to 0 or 8 (which stands
for INSERT) become invisible and treated the same way all invisible
ticks are treated. It can be used to construct or fix order books, as
well as to inject or modify correction/cancellation ticks.

Python
class name:&nbsp;HideTicksWithStatus

Input: A time series of ticks.

Output: A time series of ticks, with ticks having TICK_STATUS
not equal to 0 or 8 made invisible.

Parameters: None

Examples:

HIDE_TICKS_WITH_STATUS()

See HIDE_TICKS_WITH_STATUS in OTHER_EXAMPLES.otq.


	"""
	class Parameters:
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = []
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["stack_info", "_used_strings"]

	def __init__(self):
		_graph_components.EpBase.__init__(self, "HIDE_TICKS_WITH_STATUS")
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	@staticmethod
	def _get_name():
		return "HIDE_TICKS_WITH_STATUS"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class ShowInitializationTicks(_graph_components.EpBase):
	"""
		

SHOW_INITIALIZATION_TICKS

Type: Filter

Description: Propagates only the ticks that arrived before
start time of the query (initialization ticks) and makes them visible.
Normally such ticks are not visible. The ticks are output in the order
opposite to the order of their original timestamps. The timestamp of
the ticks is set to the start time of the query. To see the original
timestamp, add another field to the tick and assign original timestamp
to it using the ADD_FIELD(s) EP.

To avoid affecting results of the EPs which require initialization
ticks, while observing the initialization ticks they are receiving,
this EP should be created on a separate branch, as shown in the example.

Python
class name:&nbsp;ShowInitializationTicks

Input: A time series of ticks.

Output: A time series of ticks, one for each tick.

Parameters: None

Examples:

See SHOW_INITIALIZATION_TICKS_EXAMPLES.otq.


	"""
	class Parameters:
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = []
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["stack_info", "_used_strings"]

	def __init__(self):
		_graph_components.EpBase.__init__(self, "SHOW_INITIALIZATION_TICKS")
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	@staticmethod
	def _get_name():
		return "SHOW_INITIALIZATION_TICKS"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class Mid(_graph_components.EpBase):
	"""
		

MID

Type: Transformer

Description: Adds to the stream of quote ticks a numeric
attribute named MID whose value for each tick is equal to the
mid-quote: MID = (BID_PRICE+ASK_PRICE)/2.

Python
class name:&nbsp;Mid

Input: A time series of ticks containing fields BID_PRICE and
ASK_PRICE.

Output: A time series of ticks.

Parameters:


  OUTPUT_FIELD_NAME
(string)

Examples: See the following:

See the MID example in AGGREGATION_EXAMPLES.otq.


	"""
	class Parameters:
		output_field_name = "OUTPUT_FIELD_NAME"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["output_field_name"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["output_field_name", "_default_output_field_name", "stack_info", "_used_strings"]

	def __init__(self, output_field_name="MID", Out = ""):
		_graph_components.EpBase.__init__(self, "MID")
		self._default_output_field_name = "MID"
		self.output_field_name = output_field_name
		if Out != "":
			self.output_field_name=Out
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_output_field_name(self, value):
		self.output_field_name = value
		return self

	@staticmethod
	def _get_name():
		return "MID"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.output_field_name != "MID": 
			desc += "OUTPUT_FIELD_NAME=" + py_to_str(self.output_field_name) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class AddField(_graph_components.EpBase):
	"""
		

ADD_FIELD

Type: Transformer

Description: Modifies the input tick series by adding a new
field to each tick. It uses logical
expressions to set the value of the field(s).

Python
class name:&nbsp;AddField

Input: A time series of ticks

Output: A modified time series of ticks with one output tick
for each input tick

Parameters:


  FIELD (field_name [type specification] )
    Specifies
the name of the field to be created. The name of the new field must be
different from the existing field names of the input tick series, and
cannot be an empty string. Supported data types are listed here.
If the type specification is omitted, then the type will be detected
automatically. When the type of the added field is automatically
detected to be string, the size of this field is assumed to be 64
bytes, unless the expression is wrapped into DECLARE_STRING_SIZE
built-in function, which allows to override the size of the
automatically detected string type of the result.

  
  VALUE (expression)
    Result of a logical expression.&nbsp;

    
  

Examples:

ADD_FIELD (DOLLAR_SIZE, SIZE*PRICE)

Adds a field called DOLLAR_SIZE, calculated from SIZE and PRICE.

ADD_FIELD (LARGE_TRADE, DOLLAR_SIZE &gt; 100000)

Adds a field called LARGE_TRADE -- the value will be 1 (true) if
DOLLAR_SIZE is greater than 10,000, else it will be 0 (false).

ADD_FIELD (PRIMARY_EXCHANGE, EXCHANGE = 'N' OR
EXCHANGE = 'A' OR EXCHANGE ='Q')

Adds a field called PRIMARY_EXCHANGE; the value will be 1 (true) if
EXCHANGE = N, A, or Q; otherwise 0 (false).

ADD_FIELD (STRING1, "MyString")

Adds a field called STRING1, with string value "MyString".

Examples:

See ADD_FIELD_EXAMPLES in TRANSFORMER_EXAMPLES.otq.


	"""
	class Parameters:
		field = "FIELD"
		value = "VALUE"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["field", "value"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["field", "_default_field", "value", "_default_value", "stack_info", "_used_strings"]

	def __init__(self, field="", value=""):
		_graph_components.EpBase.__init__(self, "ADD_FIELD")
		self._default_field = ""
		self.field = field
		self._default_value = ""
		self.value = value
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_field(self, value):
		self.field = value
		return self

	def set_value(self, value):
		self.value = value
		return self

	@staticmethod
	def _get_name():
		return "ADD_FIELD"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.field != "": 
			desc += "FIELD=" + py_to_str(self.field) + ","
		if self.value != "": 
			desc += "VALUE=" + py_to_str(self.value) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class AddFields(_graph_components.EpBase):
	"""
		

ADD_FIELDS

Type: Transformer

Description: Modifies the input time series adding a group of
new fields to each tick by specifying field names, field types, and
expressions used to compute their values.

Python
class name:&nbsp;AddFields

Input: A time series of ticks

Output: A modified time series of ticks with one output tick
for each input tick

Parameters:


  FIELDS (string)
    A mandatory parameter with the following form:

    FIELD_1 [TYPE_1]=EXPR_1, FIELD_2 [TYPE_2]=EXPR_2, &hellip; ,FIELD_N [TYPE_N]=EXPR_N
    The name of the new field must be different from the existing
field
names of the input tick series and it cannot be an empty string.
Supported types are listed here. If
the type specification is omitted, then the type will be detected
automatically (long, ulong, double,
    decimal, string, varstring
or matrix). When the type of the
added field is automatically detected to be string,
the size of this field is assumed to be 64 bytes, unless the expression
is wrapped into DECLARE_STRING_SIZE
built-in function, which allows to override the size of the
automatically detected string type of the result. When the type&nbsp;of
the
added field is nsectime, the expression assigned to this field should
be wrapped into NSECTIME
funciton.&nbsp;

The value of each fiield is the result of its&nbsp;logical expression.
  
  USE_REGEX (Boolean)
    If true, then expressions in FIELD parameter treated as
regular expressions. This allows creation of multiple fields. Every
field at the input of this EP is matched against the part of the regex
expression specified to the left of the equal sign, and for each
matching input field name, a corresponding output field name is
determined based on the part of regex expression after equal sign.
Notice that regular expressions for field names are treated as if both
their prefix and their suffix are .*, i.e. the prefix and suffix match
any substring. As a result, field name XX will match all of aXX, aXXB,
and XXb, when USE_REGEX=true. You can have a field name begin from ^ to
indicate that .* prefix does not apply, and you can have a field name
end at $ to indicate that .* suffix does not apply.
Default: false

  
  EXISTING_FIELDS_HANDLING (THROW/OVERRIDE)
    If set to THROW. an
exception
is thrown if the names of some or all of the added fields are
present in the input ticks. If set to KEEP_EXISTING existing values are kept and if set to OVERRIDE existing values are overridden by the values provided in FIELD parameter.
Default: THROW

  

Examples: Adds fields called STRING,
STRING10, BYTE,
SHORT, INT,
LONG, DOUBLE,
and MSECTIME, calculated from SIZE and PRICE.

ADD_FIELDS(FIELDS="string STRING=\\"HELLO
WORLD\\",STRING10 string[10]=\\"0123456789\\",byte BYTE=10,short
SHORT=20,INT int=30,long LONG=40,double DOUBLE=40.5*PRICE[-1], msectime
MSECTIME=1070289034001")

ADD_FIELDS
(FIELDS="NOTIONAL_(.*)=\\1*250",USE_REGEX="TRUE")

See ADD_FIELDS_EXAMPLE in TRANSFORMER_EXAMPLES.otq.


	"""
	class Parameters:
		fields = "FIELDS"
		use_regex = "USE_REGEX"
		existing_fields_handling = "EXISTING_FIELDS_HANDLING"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["fields", "use_regex", "existing_fields_handling"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["fields", "_default_fields", "use_regex", "_default_use_regex", "existing_fields_handling", "_default_existing_fields_handling", "stack_info", "_used_strings"]

	class ExistingFieldsHandling:
		KEEP_EXISTING = "KEEP_EXISTING"
		OVERRIDE = "OVERRIDE"
		THROW = "THROW"

	def __init__(self, fields="", use_regex=False, existing_fields_handling=ExistingFieldsHandling.THROW):
		_graph_components.EpBase.__init__(self, "ADD_FIELDS")
		self._default_fields = ""
		self.fields = fields
		self._default_use_regex = False
		self.use_regex = use_regex
		self._default_existing_fields_handling = type(self).ExistingFieldsHandling.THROW
		self.existing_fields_handling = existing_fields_handling
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_fields(self, value):
		self.fields = value
		return self

	def set_use_regex(self, value):
		self.use_regex = value
		return self

	def set_existing_fields_handling(self, value):
		self.existing_fields_handling = value
		return self

	@staticmethod
	def _get_name():
		return "ADD_FIELDS"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.fields != "": 
			desc += "FIELDS=" + py_to_str(self.fields) + ","
		if self.use_regex != False: 
			desc += "USE_REGEX=" + py_to_str(self.use_regex) + ","
		if self.existing_fields_handling != self.ExistingFieldsHandling.THROW: 
			desc += "EXISTING_FIELDS_HANDLING=" + py_to_str(self.existing_fields_handling) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class AddFieldsFromSymbolParams(_graph_components.EpBase):
	"""
		

ADD_FIELDS_FROM_SYMBOL_PARAMS

Type: Transformer

Description: Modifies the input time series by adding all
symbol parameters (preserving original value types) as new fields to
each tick.

Python
class name:&nbsp;AddFieldsFromSymbolParams

Input: A time series of ticks

Output: A modified time series of ticks with one output tick
for each input tick

Parameters:


  PREFIX (string)
    If the prefix is specified, it will be appended to each added
tick field name with the following form:

    PREFIX.SYMBOL_PARAM_NAME_1, PREFIX.SYMBOL_PARAM_NAME_2, &hellip; , PREFIX.SYMBOL_PARAM_NAME_N
  


	"""
	class Parameters:
		prefix = "PREFIX"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["prefix"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["prefix", "_default_prefix", "stack_info", "_used_strings"]

	def __init__(self, prefix=""):
		_graph_components.EpBase.__init__(self, "ADD_FIELDS_FROM_SYMBOL_PARAMS")
		self._default_prefix = ""
		self.prefix = prefix
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_prefix(self, value):
		self.prefix = value
		return self

	@staticmethod
	def _get_name():
		return "ADD_FIELDS_FROM_SYMBOL_PARAMS"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.prefix != "": 
			desc += "PREFIX=" + py_to_str(self.prefix) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class Percentile(_graph_components.EpBase):
	"""
		

PERCENTILE

Type: Transformer

Description: For each bucket, propagates its n-1 n-quantiles
where a comparison between ticks is done using a specified set of tick
fields. A new field (QUANTILE) with the quantile number is added. For
more details, see Quantile.
See Estimating
the quantiles of a population R-7, Excel row for the
precise formula used.

Python
class name:&nbsp;Percentile

Input: A time series of ticks.

Output: A time series of ticks.

Parameters: See parameters
common to generic aggregations.


  NUMBER_OF_QUANTILES (integer)
    Specifies the number n of quantiles. Setting it to 2
will propagate only one tick - the median.
Default: 2

  
  INPUT_FIELD_NAMES (field_name
[DESC|ASC], [field_name [DESC|ASC]],&hellip;)
    INPUT_FIELD_NAMES should contain numeric field names.
Unless otherwise specified, DESC will be used as the default comparison
order for all fields in INPUT_FIELD_NAMES.

  
  OUTPUT_FIELD_NAMES
(field_name,[field_name],&hellip;)
    OUTPUT_FIELD_NAMES and INPUT_FIELD_NAMES must have the same
number of fields.
If not set, OUTPUT_FIELD_NAMES will be equal to INPUT_FIELD_NAMES.

  
  BUCKET_INTERVAL
(seconds/ticks)
  BUCKET_INTERVAL_UNITS
(enumerated type)
  OUTPUT_INTERVAL
(seconds)
  OUTPUT_INTERVAL_UNITS
(SECONDS/TICKS)
  IS_RUNNING_AGGR
(Boolean)
  BUCKET_TIME
(Boolean)
  BUCKET_END_CRITERIA
(expression)
  BOUNDARY_TICK_BUCKET
(NEW/PREVIOUS)
  PARTIAL_BUCKET_HANDLING
(enumerated type)
  OUTPUT_FIELD_NAME
(string)
  GROUP_BY
(string)
  GROUPS_TO_DISPLAY
(enumerated)
  BUCKET_END_PER_GROUP
(Boolean)


	"""
	class Parameters:
		number_of_quantiles = "NUMBER_OF_QUANTILES"
		input_field_names = "INPUT_FIELD_NAMES"
		output_field_names = "OUTPUT_FIELD_NAMES"
		bucket_interval = "BUCKET_INTERVAL"
		bucket_interval_units = "BUCKET_INTERVAL_UNITS"
		output_interval = "OUTPUT_INTERVAL"
		output_interval_units = "OUTPUT_INTERVAL_UNITS"
		is_running_aggr = "IS_RUNNING_AGGR"
		bucket_time = "BUCKET_TIME"
		bucket_end_criteria = "BUCKET_END_CRITERIA"
		boundary_tick_bucket = "BOUNDARY_TICK_BUCKET"
		partial_bucket_handling = "PARTIAL_BUCKET_HANDLING"
		group_by = "GROUP_BY"
		groups_to_display = "GROUPS_TO_DISPLAY"
		bucket_end_per_group = "BUCKET_END_PER_GROUP"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["number_of_quantiles", "input_field_names", "output_field_names", "bucket_interval", "bucket_interval_units", "output_interval", "output_interval_units", "is_running_aggr", "bucket_time", "bucket_end_criteria", "boundary_tick_bucket", "partial_bucket_handling", "group_by", "groups_to_display", "bucket_end_per_group"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["number_of_quantiles", "_default_number_of_quantiles", "input_field_names", "_default_input_field_names", "output_field_names", "_default_output_field_names", "bucket_interval", "_default_bucket_interval", "bucket_interval_units", "_default_bucket_interval_units", "output_interval", "_default_output_interval", "output_interval_units", "_default_output_interval_units", "is_running_aggr", "_default_is_running_aggr", "bucket_time", "_default_bucket_time", "bucket_end_criteria", "_default_bucket_end_criteria", "boundary_tick_bucket", "_default_boundary_tick_bucket", "partial_bucket_handling", "_default_partial_bucket_handling", "group_by", "_default_group_by", "groups_to_display", "_default_groups_to_display", "bucket_end_per_group", "_default_bucket_end_per_group", "stack_info", "_used_strings"]

	class BucketIntervalUnits:
		DAYS = "DAYS"
		FLEXIBLE = "FLEXIBLE"
		MONTHS = "MONTHS"
		SECONDS = "SECONDS"
		TICKS = "TICKS"

	class OutputIntervalUnits:
		SECONDS = "SECONDS"
		TICKS = "TICKS"

	class BucketTime:
		BUCKET_END = "BUCKET_END"
		BUCKET_START = "BUCKET_START"

	class BoundaryTickBucket:
		NEW = "NEW"
		PREVIOUS = "PREVIOUS"

	class PartialBucketHandling:
		AS_SEPARATE_BUCKET = "AS_SEPARATE_BUCKET"
		DISCARD = "DISCARD"
		MERGE_WITH_PREVIOUS = "MERGE_WITH_PREVIOUS"

	class GroupsToDisplay:
		ALL = "ALL"
		EVENT_IN_LAST_BUCKET = "EVENT_IN_LAST_BUCKET"

	def __init__(self, number_of_quantiles="", input_field_names="", output_field_names="", bucket_interval=0, bucket_interval_units=BucketIntervalUnits.SECONDS, output_interval="", output_interval_units=OutputIntervalUnits.SECONDS, is_running_aggr=False, bucket_time=BucketTime.BUCKET_END, bucket_end_criteria="", boundary_tick_bucket=BoundaryTickBucket.NEW, partial_bucket_handling=PartialBucketHandling.AS_SEPARATE_BUCKET, group_by="", groups_to_display=GroupsToDisplay.ALL, bucket_end_per_group=False):
		_graph_components.EpBase.__init__(self, "PERCENTILE")
		self._default_number_of_quantiles = ""
		self.number_of_quantiles = number_of_quantiles
		self._default_input_field_names = ""
		self.input_field_names = input_field_names
		self._default_output_field_names = ""
		self.output_field_names = output_field_names
		self._default_bucket_interval = 0
		self.bucket_interval = bucket_interval
		self._default_bucket_interval_units = type(self).BucketIntervalUnits.SECONDS
		self.bucket_interval_units = bucket_interval_units
		self._default_output_interval = ""
		self.output_interval = output_interval
		self._default_output_interval_units = type(self).OutputIntervalUnits.SECONDS
		self.output_interval_units = output_interval_units
		self._default_is_running_aggr = False
		self.is_running_aggr = is_running_aggr
		self._default_bucket_time = type(self).BucketTime.BUCKET_END
		self.bucket_time = bucket_time
		self._default_bucket_end_criteria = ""
		self.bucket_end_criteria = bucket_end_criteria
		self._default_boundary_tick_bucket = type(self).BoundaryTickBucket.NEW
		self.boundary_tick_bucket = boundary_tick_bucket
		self._default_partial_bucket_handling = type(self).PartialBucketHandling.AS_SEPARATE_BUCKET
		self.partial_bucket_handling = partial_bucket_handling
		self._default_group_by = ""
		self.group_by = group_by
		self._default_groups_to_display = type(self).GroupsToDisplay.ALL
		self.groups_to_display = groups_to_display
		self._default_bucket_end_per_group = False
		self.bucket_end_per_group = bucket_end_per_group
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_number_of_quantiles(self, value):
		self.number_of_quantiles = value
		return self

	def set_input_field_names(self, value):
		self.input_field_names = value
		return self

	def set_output_field_names(self, value):
		self.output_field_names = value
		return self

	def set_bucket_interval(self, value):
		self.bucket_interval = value
		return self

	def set_bucket_interval_units(self, value):
		self.bucket_interval_units = value
		return self

	def set_output_interval(self, value):
		self.output_interval = value
		return self

	def set_output_interval_units(self, value):
		self.output_interval_units = value
		return self

	def set_is_running_aggr(self, value):
		self.is_running_aggr = value
		return self

	def set_bucket_time(self, value):
		self.bucket_time = value
		return self

	def set_bucket_end_criteria(self, value):
		self.bucket_end_criteria = value
		return self

	def set_boundary_tick_bucket(self, value):
		self.boundary_tick_bucket = value
		return self

	def set_partial_bucket_handling(self, value):
		self.partial_bucket_handling = value
		return self

	def set_group_by(self, value):
		self.group_by = value
		return self

	def set_groups_to_display(self, value):
		self.groups_to_display = value
		return self

	def set_bucket_end_per_group(self, value):
		self.bucket_end_per_group = value
		return self

	@staticmethod
	def _get_name():
		return "PERCENTILE"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.number_of_quantiles != "": 
			desc += "NUMBER_OF_QUANTILES=" + py_to_str(self.number_of_quantiles) + ","
		if self.input_field_names != "": 
			desc += "INPUT_FIELD_NAMES=" + py_to_str(self.input_field_names) + ","
		if self.output_field_names != "": 
			desc += "OUTPUT_FIELD_NAMES=" + py_to_str(self.output_field_names) + ","
		if self.bucket_interval != 0: 
			desc += "BUCKET_INTERVAL=" + py_to_str(self.bucket_interval) + ","
		if self.bucket_interval_units != self.BucketIntervalUnits.SECONDS: 
			desc += "BUCKET_INTERVAL_UNITS=" + py_to_str(self.bucket_interval_units) + ","
		if self.output_interval != "": 
			desc += "OUTPUT_INTERVAL=" + py_to_str(self.output_interval) + ","
		if self.output_interval_units != self.OutputIntervalUnits.SECONDS: 
			desc += "OUTPUT_INTERVAL_UNITS=" + py_to_str(self.output_interval_units) + ","
		if self.is_running_aggr != False: 
			desc += "IS_RUNNING_AGGR=" + py_to_str(self.is_running_aggr) + ","
		if self.bucket_time != self.BucketTime.BUCKET_END: 
			desc += "BUCKET_TIME=" + py_to_str(self.bucket_time) + ","
		if self.bucket_end_criteria != "": 
			desc += "BUCKET_END_CRITERIA=" + py_to_str(self.bucket_end_criteria) + ","
		if self.boundary_tick_bucket != self.BoundaryTickBucket.NEW: 
			desc += "BOUNDARY_TICK_BUCKET=" + py_to_str(self.boundary_tick_bucket) + ","
		if self.partial_bucket_handling != self.PartialBucketHandling.AS_SEPARATE_BUCKET: 
			desc += "PARTIAL_BUCKET_HANDLING=" + py_to_str(self.partial_bucket_handling) + ","
		if self.group_by != "": 
			desc += "GROUP_BY=" + py_to_str(self.group_by) + ","
		if self.groups_to_display != self.GroupsToDisplay.ALL: 
			desc += "GROUPS_TO_DISPLAY=" + py_to_str(self.groups_to_display) + ","
		if self.bucket_end_per_group != False: 
			desc += "BUCKET_END_PER_GROUP=" + py_to_str(self.bucket_end_per_group) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class Ml_binaryClassifyPerfEvaluator(_graph_components.EpBase):
	"""
		

ML::BINARY_CLASSIFY_PERF_EVALUATOR

Type: Aggregation

Description: For each bucket, computes and outputs the
specified prediction performance metrics.

Python
class name:&nbsp;Ml_binaryClassifyPerfEvaluator

Input: A time series of ticks containing a filed representing
predicted probability and a field representing the actual (expected)
value (-1 or +1).

Output: The set of binary
performance metrics computed over each bucket.

Parameters: See parameters
common to generic aggregations.


  BUCKET_INTERVAL
(seconds/ticks)
  BUCKET_INTERVAL_UNITS
(enumerated type)
  OUTPUT_INTERVAL
(seconds)
  OUTPUT_INTERVAL_UNITS
(enumerated type)
  IS_RUNNING_AGGR
(Boolean)
  BUCKET_TIME
(Boolean)
  BUCKET_END_CRITERIA
(expression)
  BOUNDARY_TICK_BUCKET
(NEW/PREVIOUS)
  ALL_FIELDS_FOR_SLIDING
(Boolean)
  PARTIAL_BUCKET_HANDLING
(enumerated type)
  FIELD_NAME_OF_EXPECTED_VALUE
(string) - the field name where the actual value (post-factum evaluated
label) is recorded.
  FIELD_NAME_OF_PREDICTED_VALUE(string)
- the field name where the predicted probability is recorded.
  PROBABILITY_THRESHOLD - the
double value from (0, 1) interval which will be used to classify
predicted probabilities into positive and negative classes. If not
specified then the best threshold computed from ROC Curve will be used.
  PROBABILITY_THRESHOLD_GRANULARITY
- is used to compute the Area
under ROC Curve metric. The ROC Curve is plotted according to
different threshold values from [0, 1] interval, and this option
defines the iteration step. The value should be in (0, 0.5] range.
Default: 0.05
  GROUP_BY
(string)

Notes: See the notes on
generic aggregations.

Examples: See the predict and stream examples
in ml_dlib_classify.otq.


	"""
	class Parameters:
		bucket_interval = "BUCKET_INTERVAL"
		bucket_interval_units = "BUCKET_INTERVAL_UNITS"
		output_interval = "OUTPUT_INTERVAL"
		output_interval_units = "OUTPUT_INTERVAL_UNITS"
		is_running_aggr = "IS_RUNNING_AGGR"
		bucket_time = "BUCKET_TIME"
		bucket_end_criteria = "BUCKET_END_CRITERIA"
		boundary_tick_bucket = "BOUNDARY_TICK_BUCKET"
		all_fields_for_sliding = "ALL_FIELDS_FOR_SLIDING"
		partial_bucket_handling = "PARTIAL_BUCKET_HANDLING"
		field_name_of_expected_value = "FIELD_NAME_OF_EXPECTED_VALUE"
		field_name_of_predicted_value = "FIELD_NAME_OF_PREDICTED_VALUE"
		probability_threshold = "PROBABILITY_THRESHOLD"
		probability_threshold_granularity = "PROBABILITY_THRESHOLD_GRANULARITY"
		group_by = "GROUP_BY"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["bucket_interval", "bucket_interval_units", "output_interval", "output_interval_units", "is_running_aggr", "bucket_time", "bucket_end_criteria", "boundary_tick_bucket", "all_fields_for_sliding", "partial_bucket_handling", "field_name_of_expected_value", "field_name_of_predicted_value", "probability_threshold", "probability_threshold_granularity", "group_by"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["bucket_interval", "_default_bucket_interval", "bucket_interval_units", "_default_bucket_interval_units", "output_interval", "_default_output_interval", "output_interval_units", "_default_output_interval_units", "is_running_aggr", "_default_is_running_aggr", "bucket_time", "_default_bucket_time", "bucket_end_criteria", "_default_bucket_end_criteria", "boundary_tick_bucket", "_default_boundary_tick_bucket", "all_fields_for_sliding", "_default_all_fields_for_sliding", "partial_bucket_handling", "_default_partial_bucket_handling", "field_name_of_expected_value", "_default_field_name_of_expected_value", "field_name_of_predicted_value", "_default_field_name_of_predicted_value", "probability_threshold", "_default_probability_threshold", "probability_threshold_granularity", "_default_probability_threshold_granularity", "group_by", "_default_group_by", "stack_info", "_used_strings"]

	class BucketIntervalUnits:
		DAYS = "DAYS"
		FLEXIBLE = "FLEXIBLE"
		MONTHS = "MONTHS"
		SECONDS = "SECONDS"
		TICKS = "TICKS"

	class OutputIntervalUnits:
		SECONDS = "SECONDS"
		TICKS = "TICKS"

	class BucketTime:
		BUCKET_END = "BUCKET_END"
		BUCKET_START = "BUCKET_START"

	class BoundaryTickBucket:
		NEW = "NEW"
		PREVIOUS = "PREVIOUS"

	class AllFieldsForSliding:
		WHEN_TICKS_EXIT_WINDOW = "WHEN_TICKS_EXIT_WINDOW"
		FALSE = "false"
		TRUE = "true"

	class PartialBucketHandling:
		AS_SEPARATE_BUCKET = "AS_SEPARATE_BUCKET"
		DISCARD = "DISCARD"
		MERGE_WITH_PREVIOUS = "MERGE_WITH_PREVIOUS"

	def __init__(self, bucket_interval=0, bucket_interval_units=BucketIntervalUnits.SECONDS, output_interval="", output_interval_units=OutputIntervalUnits.SECONDS, is_running_aggr=False, bucket_time=BucketTime.BUCKET_END, bucket_end_criteria="", boundary_tick_bucket=BoundaryTickBucket.NEW, all_fields_for_sliding=AllFieldsForSliding.FALSE, partial_bucket_handling=PartialBucketHandling.AS_SEPARATE_BUCKET, field_name_of_expected_value="", field_name_of_predicted_value="", probability_threshold="", probability_threshold_granularity="0.05", group_by=""):
		_graph_components.EpBase.__init__(self, "ML::BINARY_CLASSIFY_PERF_EVALUATOR")
		self._default_bucket_interval = 0
		self.bucket_interval = bucket_interval
		self._default_bucket_interval_units = type(self).BucketIntervalUnits.SECONDS
		self.bucket_interval_units = bucket_interval_units
		self._default_output_interval = ""
		self.output_interval = output_interval
		self._default_output_interval_units = type(self).OutputIntervalUnits.SECONDS
		self.output_interval_units = output_interval_units
		self._default_is_running_aggr = False
		self.is_running_aggr = is_running_aggr
		self._default_bucket_time = type(self).BucketTime.BUCKET_END
		self.bucket_time = bucket_time
		self._default_bucket_end_criteria = ""
		self.bucket_end_criteria = bucket_end_criteria
		self._default_boundary_tick_bucket = type(self).BoundaryTickBucket.NEW
		self.boundary_tick_bucket = boundary_tick_bucket
		self._default_all_fields_for_sliding = type(self).AllFieldsForSliding.FALSE
		self.all_fields_for_sliding = all_fields_for_sliding
		self._default_partial_bucket_handling = type(self).PartialBucketHandling.AS_SEPARATE_BUCKET
		self.partial_bucket_handling = partial_bucket_handling
		self._default_field_name_of_expected_value = ""
		self.field_name_of_expected_value = field_name_of_expected_value
		self._default_field_name_of_predicted_value = ""
		self.field_name_of_predicted_value = field_name_of_predicted_value
		self._default_probability_threshold = ""
		self.probability_threshold = probability_threshold
		self._default_probability_threshold_granularity = "0.05"
		self.probability_threshold_granularity = probability_threshold_granularity
		self._default_group_by = ""
		self.group_by = group_by
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_bucket_interval(self, value):
		self.bucket_interval = value
		return self

	def set_bucket_interval_units(self, value):
		self.bucket_interval_units = value
		return self

	def set_output_interval(self, value):
		self.output_interval = value
		return self

	def set_output_interval_units(self, value):
		self.output_interval_units = value
		return self

	def set_is_running_aggr(self, value):
		self.is_running_aggr = value
		return self

	def set_bucket_time(self, value):
		self.bucket_time = value
		return self

	def set_bucket_end_criteria(self, value):
		self.bucket_end_criteria = value
		return self

	def set_boundary_tick_bucket(self, value):
		self.boundary_tick_bucket = value
		return self

	def set_all_fields_for_sliding(self, value):
		self.all_fields_for_sliding = value
		return self

	def set_partial_bucket_handling(self, value):
		self.partial_bucket_handling = value
		return self

	def set_field_name_of_expected_value(self, value):
		self.field_name_of_expected_value = value
		return self

	def set_field_name_of_predicted_value(self, value):
		self.field_name_of_predicted_value = value
		return self

	def set_probability_threshold(self, value):
		self.probability_threshold = value
		return self

	def set_probability_threshold_granularity(self, value):
		self.probability_threshold_granularity = value
		return self

	def set_group_by(self, value):
		self.group_by = value
		return self

	@staticmethod
	def _get_name():
		return "ML::BINARY_CLASSIFY_PERF_EVALUATOR"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.bucket_interval != 0: 
			desc += "BUCKET_INTERVAL=" + py_to_str(self.bucket_interval) + ","
		if self.bucket_interval_units != self.BucketIntervalUnits.SECONDS: 
			desc += "BUCKET_INTERVAL_UNITS=" + py_to_str(self.bucket_interval_units) + ","
		if self.output_interval != "": 
			desc += "OUTPUT_INTERVAL=" + py_to_str(self.output_interval) + ","
		if self.output_interval_units != self.OutputIntervalUnits.SECONDS: 
			desc += "OUTPUT_INTERVAL_UNITS=" + py_to_str(self.output_interval_units) + ","
		if self.is_running_aggr != False: 
			desc += "IS_RUNNING_AGGR=" + py_to_str(self.is_running_aggr) + ","
		if self.bucket_time != self.BucketTime.BUCKET_END: 
			desc += "BUCKET_TIME=" + py_to_str(self.bucket_time) + ","
		if self.bucket_end_criteria != "": 
			desc += "BUCKET_END_CRITERIA=" + py_to_str(self.bucket_end_criteria) + ","
		if self.boundary_tick_bucket != self.BoundaryTickBucket.NEW: 
			desc += "BOUNDARY_TICK_BUCKET=" + py_to_str(self.boundary_tick_bucket) + ","
		if self.all_fields_for_sliding != self.AllFieldsForSliding.FALSE: 
			desc += "ALL_FIELDS_FOR_SLIDING=" + py_to_str(self.all_fields_for_sliding) + ","
		if self.partial_bucket_handling != self.PartialBucketHandling.AS_SEPARATE_BUCKET: 
			desc += "PARTIAL_BUCKET_HANDLING=" + py_to_str(self.partial_bucket_handling) + ","
		if self.field_name_of_expected_value != "": 
			desc += "FIELD_NAME_OF_EXPECTED_VALUE=" + py_to_str(self.field_name_of_expected_value) + ","
		if self.field_name_of_predicted_value != "": 
			desc += "FIELD_NAME_OF_PREDICTED_VALUE=" + py_to_str(self.field_name_of_predicted_value) + ","
		if self.probability_threshold != "": 
			desc += "PROBABILITY_THRESHOLD=" + py_to_str(self.probability_threshold) + ","
		if self.probability_threshold_granularity != "0.05": 
			desc += "PROBABILITY_THRESHOLD_GRANULARITY=" + py_to_str(self.probability_threshold_granularity) + ","
		if self.group_by != "": 
			desc += "GROUP_BY=" + py_to_str(self.group_by) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class TextDetectCategory(_graph_components.EpBase):
	"""
		

DETECT_CATEGORY

Type: Text

Description: Modifies the input tick series by adding a new
field to each tick. The new field contains the categories of the
specified text field. Those categories are determined from the
dictionary file. It contains mapping from words to its categories.

Python
class name:&nbsp;TextDetectCategory

Input: A time series of ticks.

Output: A modified time series of ticks, one output tick for
each input tick.

Parameters:


  KEY_FIELD (string)
    The name of the field for which categories should be added. The
field should be textual.

  
  DICTIONARY_FILE (string)
    File name that contains mapping from words to its categories.
That file must have two columns, single word and its categories,
separated by commas.

  
  MAPPED_VALUE_FIELD (string)
    The name of the added field.

    Default: CATEGORIES

  


	"""
	class Parameters:
		key_field = "KEY_FIELD"
		dictionary_file = "DICTIONARY_FILE"
		mapped_value_field = "MAPPED_VALUE_FIELD"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["key_field", "dictionary_file", "mapped_value_field"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["key_field", "_default_key_field", "dictionary_file", "_default_dictionary_file", "mapped_value_field", "_default_mapped_value_field", "stack_info", "_used_strings"]

	def __init__(self, key_field="", dictionary_file="", mapped_value_field="CATEGORIES"):
		_graph_components.EpBase.__init__(self, "TEXT/DETECT_CATEGORY")
		self._default_key_field = ""
		self.key_field = key_field
		self._default_dictionary_file = ""
		self.dictionary_file = dictionary_file
		self._default_mapped_value_field = "CATEGORIES"
		self.mapped_value_field = mapped_value_field
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_key_field(self, value):
		self.key_field = value
		return self

	def set_dictionary_file(self, value):
		self.dictionary_file = value
		return self

	def set_mapped_value_field(self, value):
		self.mapped_value_field = value
		return self

	@staticmethod
	def _get_name():
		return "TEXT/DETECT_CATEGORY"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.key_field != "": 
			desc += "KEY_FIELD=" + py_to_str(self.key_field) + ","
		if self.dictionary_file != "": 
			desc += "DICTIONARY_FILE=" + py_to_str(self.dictionary_file) + ","
		if self.mapped_value_field != "CATEGORIES": 
			desc += "MAPPED_VALUE_FIELD=" + py_to_str(self.mapped_value_field) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class UpdateField(_graph_components.EpBase):
	"""
		

UPDATE_FIELD

Type: Transformer

Description: Modifies a field of each input tick that matches
the selection criteria, setting its value to the result of an
expression.

Python
class name:&nbsp;UpdateField

Input: A time series of ticks.

Output: A time series of ticks.

A common source of unexpected behavior for
UPDATE_FIELD, especially if compared to UPDATE_FIELDS,
is the (by default) false value of PRESERVE_FIELD_TYPE
parameter. This is done for backward compatibility, since UPDATE_FIELD
used to deduce updated field type itself, rather than merely preserve
it. As a result, the deduced type may be different from the original
one.
The need to deduce updated field type particularly means, that
UPDATE_FIELD may analyze input tick descriptors upon their arrival,
while UPDATE_FIELDS always defers analysis of input tick descriptors
until the update condition is met. When update
condition uses the UNDEFINED
built-in function to filter unnecessary tick descriptors out, an error
about missing attribute may be thrown. To work this issue around,
PRESERVE_FIELD_TYPE must be set to true.

Currently, UPDATE_FIELD does not change the place of
the updated field in tick descriptors. To revert back to the old
behavior, when the updated field was being placed at the end of the
output tick descriptor, set the main configuration variable
COMPATIBILITY.UPDATE_FIELD.AS_OF_1_15 to a true value.

Parameters:


  FIELD (string)
    Specifies the name of the field to be modified. A special field,
TIMESTAMP, can be used to update the tick's timestamp.

    The UPDATE_FIELD event processor can also be used to initialize
or
modify the value of a state variable. State variables are those
starting with prefix "STATE::" and can have two types: namely, double
precision number and string. State variables have a branch scope in
queries. The built-in function UNDEFINED (see Catalog
of Built-in functions)
can be used to test whether a state variable with a given name is
defined and initialized or not (e.g., UNDEFINED("STATE::BEST_PRICE")).

    To update the value of a
field from a tick set, one of the following syntaxes can be used:

    STATE::SET_NAME.FIELD_NAME
STATE::SET_NAME.FIELD_NAME(KEY_1_VALUE,KEY_2_VALUE,...,KEY_N_VALUE)

    Where FIELD_NAME specifies a field to be updated.
KEY_1_VALUE,KEY_2_VALUE,...,KEY_N_VALUE are expressions evaluating to
key values. If key values are not specified (as in the first case),
they are taken from current tick fields with corresponding names. The
sequence of key values should be the same as in the TICK_SET's
declaration (see DECLARE_STATE_VARIABLES).
The following syntax allows specifying values for key fields in
arbitrary order.

    STATE::SET_NAME.FIELD_NAME(KEY_FIELD_1_NAME=KEY_1_VALUE,KEY_FIELD_2_NAME=KEY_2_VALUE,...,KEY_FIELD_N_NAME=KEY_N_VALUE)

  
  VALUE (expression)
    A logical expression.&nbsp;

    
  
  WHERE (expression)
    Specifies a logical expression&nbsp;criteria
for a selection of ticks to modify.&nbsp;

    
  
  PREV_VALUE_ON_MISMATCH
(Boolean)
    If set to true and the WHERE expression is not matched, the
previous
value of FIELD is propagated. If the WHERE expression is not matched
for the first tick in the input time series, and PREV_VALUE_ON_MISMATCH
is set to true, NaN is propagated if FIELD is numeric, an empty string
is propagated if FIELD is a string, and the current timestamp is
propagated if the tick timestamp is updated.

  
  PRESERVE_FIELD_TYPE (Boolean)
    If set to true, types of fields will be preserved and no type
conversion will be done. 

    See the important notes at the beginning of this
document to determine the value of this option (you will probably need
to set it to 'true').
Default: false

  
  ALLOW_UNORDERED_OUTPUT_TIMES
(Boolean)
    If set to true, tick timestamp can be updated by out of order
values.
Default: false

  

Examples:

UPDATE_FIELD (FIELD=SIZE,VALUE='SIZE*10',WHERE='SIZE =1')UPDATE_FIELD (FIELD=TIMESTAMP,VALUE='TIMESTAMP + 5000',WHERE='1',)
An alternative (to SUM event processor) way to compute total volume
in a state variable:

UPDATE_FIELD (FIELD=STATE::VOL,VALUE='0',WHERE='UNDEFINED("STATE::VOL")',)'UPDATE_FIELD (FIELD= STATE::VOL,VALUE=' STATE::VOL+SIZE',,)
Updating a field from a TICK_SET

UPDATE_FIELD
(FIELD=STATE::S1.SEQ_NUM(PRICE,SIZE),VALUE='SEQ_NUM-1',WHERE='SIZE!=125')

See the UPDATE_FIELD_TABLE_EXAMPLE
example in TRANSFORMER_EXAMPLES.otq.


	"""
	class Parameters:
		field = "FIELD"
		value = "VALUE"
		where = "WHERE"
		prev_value_on_mismatch = "PREV_VALUE_ON_MISMATCH"
		preserve_field_type = "PRESERVE_FIELD_TYPE"
		allow_unordered_output_times = "ALLOW_UNORDERED_OUTPUT_TIMES"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["field", "value", "where", "prev_value_on_mismatch", "preserve_field_type", "allow_unordered_output_times"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["field", "_default_field", "value", "_default_value", "where", "_default_where", "prev_value_on_mismatch", "_default_prev_value_on_mismatch", "preserve_field_type", "_default_preserve_field_type", "allow_unordered_output_times", "_default_allow_unordered_output_times", "stack_info", "_used_strings"]

	def __init__(self, field="", value="", where="", prev_value_on_mismatch=False, preserve_field_type=False, allow_unordered_output_times=False):
		_graph_components.EpBase.__init__(self, "UPDATE_FIELD")
		self._default_field = ""
		self.field = field
		self._default_value = ""
		self.value = value
		self._default_where = ""
		self.where = where
		self._default_prev_value_on_mismatch = False
		self.prev_value_on_mismatch = prev_value_on_mismatch
		self._default_preserve_field_type = False
		self.preserve_field_type = preserve_field_type
		self._default_allow_unordered_output_times = False
		self.allow_unordered_output_times = allow_unordered_output_times
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_field(self, value):
		self.field = value
		return self

	def set_value(self, value):
		self.value = value
		return self

	def set_where(self, value):
		self.where = value
		return self

	def set_prev_value_on_mismatch(self, value):
		self.prev_value_on_mismatch = value
		return self

	def set_preserve_field_type(self, value):
		self.preserve_field_type = value
		return self

	def set_allow_unordered_output_times(self, value):
		self.allow_unordered_output_times = value
		return self

	@staticmethod
	def _get_name():
		return "UPDATE_FIELD"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.field != "": 
			desc += "FIELD=" + py_to_str(self.field) + ","
		if self.value != "": 
			desc += "VALUE=" + py_to_str(self.value) + ","
		if self.where != "": 
			desc += "WHERE=" + py_to_str(self.where) + ","
		if self.prev_value_on_mismatch != False: 
			desc += "PREV_VALUE_ON_MISMATCH=" + py_to_str(self.prev_value_on_mismatch) + ","
		if self.preserve_field_type != False: 
			desc += "PRESERVE_FIELD_TYPE=" + py_to_str(self.preserve_field_type) + ","
		if self.allow_unordered_output_times != False: 
			desc += "ALLOW_UNORDERED_OUTPUT_TIMES=" + py_to_str(self.allow_unordered_output_times) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class UpdateFields(_graph_components.EpBase):
	"""
		

UPDATE_FIELDS

Type: Transformer

Description: Modifies a group of fields in each input tick, choosing
target fields and respective expressions to evaluate new values for
those fields either from SET or ELSE_SET parameters,
based on matching criteria specified via the WHERE parameter.

Python
class name:&nbsp;UpdateFields

Input: A time series of ticks.

Output: A time series of ticks.

Parameters:


  SET (string)
    A mandatory parameter of the following form: FIELD_1=EXPR_1,&hellip;,FIELD_N=EXPR_N,
where FIELD_1,&hellip;,FIELD_N are either input tick fields or state variables. They also can specify a
field from a tick set using the same syntax as in the UPDATE_FIELD EP. EXPR_1,&hellip;,EXPR_N
are logical expressions.

  
  ELSE_SET (string)
    An optional parameter of the same form as SET.

  
  WHERE (expression)
    A logical expression which
specifies matching criteria for input ticks: matching ticks are updated
according to the SET parameter's value, and those that do not
match are updated according to the value of the ELSE_SET
parameter, whenever it is not empty.

    
  
  USE_REGEX (Boolean)
    If true, then FIELD=EXPR pairs in the SET
and ELSE_SET parameters are treated as regular expressions.
This allows bulk updating of fields. Matched parts of FIELD can
be used in EXPR.
Notice that regular expressions for field names are treated as if both
their prefix and their suffix are .*, i.e. the prefix and suffix match
any substring. As a result, field name XX will match all of aXX, aXXB,
and XXb, when USE_REGEX=true. You can have a field name begin from ^ to
indicate that .* prefix does not apply, and you can have a field name
end at $ to indicate that .* suffix does not apply.
Default: false

  
  FIELDS_FILTER (string)
    An expression whose value is a string containing a
comma-separated
list of tick descriptor field names. If this parameter is specified, it
will further limit the list of fields to be updated. Specifically, the
list of fields to be updated specified in SET, whether
explicitly listed or specified via a regex, will further be intersected
with the list of fields returned by this expression. A useful function
to use in this expression is SELECT_MATCHING_FIELDS,
which allows for filtering fields by their names and types.

  

Examples:

UPDATE_FIELDS
(SET="SIZE=SIZE+1,STATE::D=STATE::I+1",ELSE_SET="SIZE=SIZE-1",WHERE=
'SIZE=1')

Updating fields with name NOTIONAL_(.*) with the product of
corresponding PRICEs and SIZEs.

UPDATE_FIELDS
(SET="NOTIONAL_(.*)=PRICE_\\1*SIZE_\\1",USE_REGEX="TRUE")

Doing the same as above but this time filtering fields also by their
types.

UPDATE_FIELDS
(SET="NOTIONAL_(.*)=PRICE_\\1*SIZE_\\1",USE_REGEX="TRUE",
FIELDS_FILTER="SELECT_MATCHING_FIELDS(\\".*\\",\\"double,int\\")")

Updating a field from a TICK_SET

UPDATE_FIELDS
(SET="STATE::S1.SEQ_NUM(PRICE,SIZE)=SEQ_NUM-1")

See the UPDATE_FIELDS_EXAMPLE in TRANSFORMER_EXAMPLES.otq.


	"""
	class Parameters:
		set = "SET"
		else_set = "ELSE_SET"
		where = "WHERE"
		use_regex = "USE_REGEX"
		fields_filter = "FIELDS_FILTER"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["set", "else_set", "where", "use_regex", "fields_filter"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["set", "_default_set", "else_set", "_default_else_set", "where", "_default_where", "use_regex", "_default_use_regex", "fields_filter", "_default_fields_filter", "stack_info", "_used_strings"]

	def __init__(self, set="", else_set="", where="", use_regex=False, fields_filter=""):
		_graph_components.EpBase.__init__(self, "UPDATE_FIELDS")
		self._default_set = ""
		self.set = set
		self._default_else_set = ""
		self.else_set = else_set
		self._default_where = ""
		self.where = where
		self._default_use_regex = False
		self.use_regex = use_regex
		self._default_fields_filter = ""
		self.fields_filter = fields_filter
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_set(self, value):
		self.set = value
		return self

	def set_else_set(self, value):
		self.else_set = value
		return self

	def set_where(self, value):
		self.where = value
		return self

	def set_use_regex(self, value):
		self.use_regex = value
		return self

	def set_fields_filter(self, value):
		self.fields_filter = value
		return self

	@staticmethod
	def _get_name():
		return "UPDATE_FIELDS"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.set != "": 
			desc += "SET=" + py_to_str(self.set) + ","
		if self.else_set != "": 
			desc += "ELSE_SET=" + py_to_str(self.else_set) + ","
		if self.where != "": 
			desc += "WHERE=" + py_to_str(self.where) + ","
		if self.use_regex != False: 
			desc += "USE_REGEX=" + py_to_str(self.use_regex) + ","
		if self.fields_filter != "": 
			desc += "FIELDS_FILTER=" + py_to_str(self.fields_filter) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class ExecuteExpressions(_graph_components.EpBase):
	"""
		

EXECUTE_EXPRESSIONS

Type: Transformer

Description: Executes a group of expressions.

Python
class name:&nbsp;ExecuteExpressions

Input: A time series of ticks.

Output: A time series of ticks.

Parameters:


  EXPRESSIONS (string)
    A mandatory parameter in the following form: EXPR_1;EXPR_1;&hellip;;EXPR_N, where EXPR_1,&hellip;,EXPR_N are expressions as
described below.

    Expressions used in this EP usually involve calling UD functions
having user defined types (see User defined types)
in their argument list.

    Expressions used in the EXPRESSIONS parameter are &nbsp;logical expression.

  

Examples:

EXECUTE_EXPRESSIONS
(EXPRESSIONS="STATE::MATRIX1.MULTIPLY(2.0)")


	"""
	class Parameters:
		expressions = "EXPRESSIONS"
		where = "WHERE"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["expressions", "where"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["expressions", "_default_expressions", "where", "_default_where", "stack_info", "_used_strings"]

	def __init__(self, expressions="", where=""):
		_graph_components.EpBase.__init__(self, "EXECUTE_EXPRESSIONS")
		self._default_expressions = ""
		self.expressions = expressions
		self._default_where = ""
		self.where = where
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_expressions(self, value):
		self.expressions = value
		return self

	def set_where(self, value):
		self.where = value
		return self

	@staticmethod
	def _get_name():
		return "EXECUTE_EXPRESSIONS"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.expressions != "": 
			desc += "EXPRESSIONS=" + py_to_str(self.expressions) + ","
		if self.where != "": 
			desc += "WHERE=" + py_to_str(self.where) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class UpdateTimestamp(_graph_components.EpBase):
	"""
		

UPDATE_TIMESTAMP

Type: Transformer

Description: Assigns alternative timestamps to input ticks
and propagates resulting time series sorted in ascending order of those
timestamps. Ticks with equal alternative timestamps are sorted in
ascending order of the respective original timestamps, and equal ones
among those are further sorted by ascending values of the OMDSEQ
field, if such a field is present. Alternative timestamps are taken
from specified input fields and are assumed to be either in
milliseconds or in nanoseconds (if the field has nsectime type
explicitly). Optionally, for lower than millisecond granularity
alternative timestamps, fractional (&lt; 1msec) parts of alternative
timestamps can be taken from an additional input field and will be
assumed to be in picoseconds. If the picoseconds' field is specified,
then even if alternative timestamps have nanosecond granularity, only
millisecond parts of them will be taken.

To put it more simply, UPDATE_TIMESTAMP
is used to replace the timestamp originally assigned to the tick with a
timestamp value from non-TIMESTAMP fields in the tick.

Currently, there are several issues related to UPDATE_TIMESTAMP that should be noted:


  Since the PICOS field is a logical extension of the timestamp, it
should be possible to update both the milliseconds and PICOS portions
of the new timestamp.
  The query must analyze ticks that may be in the DB before the
query start time and after the query end time whose replacement
timestamp lies between the start and end times.
  The replacement timestamps may not be greater than or equal to
the replacement timestamp of the previous tick for all ticks in the
input. In order to run aggregations, all ticks must have timestamps
greater than or equal to the timestamp of the previous tick.
  The fields holding the replacement timestamp may not be populated
for all input ticks (the default value for a missing timestamp field is
    0).

To avoid issues that may arise due to the time drift of involved
machines, network delays, or when subsequent ticks in the input hold
smaller timestamp values, UPDATE_TIMESTAMP conducts the following
procedures:


  It changes the start time of the query to original start time - MAX_DELAY_OF_ORIGINAL_TIMESTAMP
to make sure it processes all possible ticks that might have a new
timestamp in the range of [start, end).
  It changes the end time of the query to original end time + MAX_DELAY_OF_NEW_TIMESTAMP
to make sure it processes all possible ticks that might have a new
timestamp in the range of [start, end).
  It filters all ticks by their new timestamps to make sure they
are in the range [start, end).
  If MAX_OUT_OF_ORDER_INTERVAL_MSEC
is specified, it delays the ticks for the specified amount of time in
order to have an opportunity to sort them into proper sequence by time.


While designing the query, please take into consideration that
UPDATE_TIMESTAMP does not propagate ticks, alternative timestamps of
which do not belong to the original query interval. Therefore, in order
to avoid possible side effects resulting from processing of such ticks
by source (input) subgraphs of UPDATE_TIMESTAMP EPs, it is generally
advised to place UPDATE_TIMESTAMP EPs as close to source (input) nodes
of the query graph as possible.


Python
class name:&nbsp;UpdateTimestamp

Input: A time series of ticks.

Output: A time series of ticks.

Parameters:


  TIMESTAMP_FIELD_NAME (string)
    
Specifies the name of the input field, which is assumed to carry
alternative timestamps in milliseconds or nanoseconds (see above). In
other words, it is used to supply the milliseconds portion of the new
timestamp or simply the new timestamp in nanoseconds.

  
  TIMESTAMP_PSEC_FIELD_NAME
(optional, numeric)
    
Specifies the name of the input field, which is assumed to carry
fractional (&lt; 1msec) parts of alternative timestamps (in
picoseconds). In other words, it is used to supply the PICO portion of
the timestamp.
Default: empty

  
  MAX_OUT_OF_ORDER_INTERVAL_MSEC
(numeric, in milliseconds)
    
Specifies the maximum out-of-order interval for alternative
timestamps, in other words minimum delay, after which output ticks can
be propagated sorted by ascending values of alternative timestamps.

    Consider the following:
In order to handle out-of-order timestamps, the ticks need to be sorted
by the new timestamp before being sent on to the rest of the query.
Normally, in order to sort ticks, all of the ticks would need to be
accumulated, sorted when the end of the input is reached, and then all
of the ticks are released once the sort is complete. This process is
extremely inefficient (in terms of both space and CPU time) and it also
means that this couldn't be used with complex event processing (CEP).
Instead, this EP requires that the user specify the maximum amount of
delay that an out-of-order tick might incur in
MAX_OUT_OF_ORDER_INTERVAL_MSEC. This means that as soon as
MAX_OUT_OF_ORDER_INTERVAL_MSEC milliseconds have passed (based on the
timestamps of later ticks), a tick can be sent along for processing by
later steps. In addition, this means that we only need to sort at most
the number of ticks in each MAX_OUT_OF_ORDER_INTERVAL_MSEC interval.
Since the data might have a tick that is delayed by more than the
MAX_OUT_OF_ORDER_INTERVAL_MSEC interval, the OUT_OF_ORDER_TIMESTAMP_HANDLING
parameter tells the EP what to do with these ticks.
Default: 0

  
  MAX_DELAY_OF_ORIGINAL_TIMESTAMP
(numeric, in milliseconds)
    
Let us say the alternative timestamp is greater than the
original one for a particular tick, i.e., the original timestamp is delayed.
    MAX_DELAY_OF_ORIGINAL_TIMESTAMP defines at most how far back
it can be. Hence, it also defines how far to look back from the query
start time in order to capture the minimal possible value of an
alternative timestamp, which would still not be less than the query
start time, i.e., how much earlier than the query start time to look in
order to find ticks that may lie between the query start time and end
time (affecting _START_TIME and _END_TIME). This parameter is used to
adjust the query start time earlier. The only parameter that leads to
accumulation of ticks in the EP is MAX_OUT_OF_ORDER_INTERVAL_MSEC.
Default: 0

  
  MAX_DELAY_OF_NEW_TIMESTAMP
(numeric, in milliseconds)
    
Let us say the alternative timestamp is less than the original
timestamp for a particular tick, i.e., the alternative timestamp is delayed.
MAX_DELAY_OF_NEW_TIMESTAMP defines at most how far back it can be.
Hence, it also defines how far to look forward from the query end time
in order to capture the maximal possible value of the alternative
timestamp, which would still be less than the query end time, i.e., how
much later than the query end time to look in order to find ticks that
may lie between the query start time and end time (affecting _START_TIME and _END_TIME).
This parameter is used to adjust the query end time later. The only
parameter that leads to accumulation of ticks in the EP is MAX_OUT_OF_ORDER_INTERVAL_MSEC.
Default: 0

  
  OUT_OF_ORDER_TIMESTAMP_HANDLING
(enumerated type)
    
When only occasional, discrete values prevent alternative
timestamps in the input time series from having a much lower
out-of-order interval, it may be desirable not to complain on such
values, but rather handle them in some way to achieve lower delay. Two
possible values, USE_PREVIOUS_VALUE
and USE_ORIGINAL_TIMESTAMP, of this
parameter suggest two possible ways to handle such timestamps: either
by assigning a previous propagated alternative timestamp to them, or by
assigning the original timestamp respectively. Note that the latter
still does not guarantee proper order in general, since the
out-of-order interval may still be insufficient, in which case an
exception will be thrown.
Default: COMPLAIN

  
  MAX_DELAY_HANDLING (enumerated
type)
    
Specifies the handling of alternative timestamps for which one
of the maximal allowed delays is exceeded, in other words the
alternative timestamp is out of the closed surroundings of the original
timestamp defined by MAX_DELAY_OF_NEW_TIMESTAMP
from the left and MAX_DELAY_OF_ORIGINAL_TIMESTAMP
from the right. By default, an exception will be thrown in such cases.
Three possible values of this parameter suggest three possible ways to
handle such timestamps:
DISCARD causes the respective tick to be thrown out.
For USE_ORIGINAL_TIMESTAMP, the corresponding original timestamp is
assigned.
For USE_NEW_TIMESTAMP a new timestamp is tried. In this case, if a
previous propagated timestamp is greater than the new timestamp, then
the previous propagated timestamp is preferred. NOTE: A previously
propagated timestamp could be from a heartbeat. When there is no
previous propagated tick and the new timestamp falls behind the query
start time, the latter is preferred, while query end time is preferred
if the new timestamp exceeds it.

    NOTE: the action, assumed by MAX_DELAY_HANDLING, is performed
before any action is assumed by the OUT_OF_ORDER_TIMESTAMP_HANDLING
parameter.
Default: COMPLAIN

  
  ZERO_TIMESTAMP_HANDLING
(enumerated type)
    
Specifies the handling of alternative timestamps for which the
millisecond part is equal to 0. In other words, it tells the EP what to
do with ticks that don't have replacement timestamps. Usually such
timestamps are problematic and should be eliminated in some way.
Currently, the PRESERVE_SEQUENCE value of this parameter allows for
replacing such a timestamp by a maximal value among the least possible
exchange time for it (TIMESTAMP-MAX_DELAY_OF_NEW_TIMESTAMP)
and exchange time of the previous tick in the input sequence. These
heuristics will allow for preserving the order of such a tick in output
sequence if MAX_OUT_OF_ORDER_INTERVAL_MSEC
is 0. Note that this handling takes place prior to any action assumed
by the values of OUT_OF_ORDER_TIMESTAMP_HANDLING
and MAX_DELAY_HANDLING
parameters. In case if ZERO_TIMESTAMP_HANDLING parameter is set to
NONE, alternative timestamp is treated as the day start time of the
original timestamp. After than, actions, assumed by OUT_OF_ORDER_TIMESTAMP_HANDLING
and MAX_DELAY_HANDLING
parameters are performed, if applicable.
Default: NONE

  
  LOG_SEQUENCE_VIOLATIONS
(Boolean)
    If set to true, then warnings about actions, assumed by
non-default values of parameters OUT_OF_ORDER_TIMESTAMP_HANDLING,
    MAX_DELAY_HANDLING and ZERO_TIMESTAMP_HANDLING, are
logged into the log file.
Default: false

PROPAGATE_HEARTBEATS (Boolean)
    Specifies the way UPDATE_TIMESTAMP EP treats heartbeats. The value NEVER causes UPDATE_TIMESTAMP to ignore them. This is to eliminate the sometimes unlikely behavior of how heartbeats affect output tick timestamps (see above). The value ALWAYS causes UPDATE_TIMESTAMP to accept and propagate them. The empty value, the default behavior, causes UPDATE_TIMESTAMP to behave as if the value ALWAYS is specified, if the query is a CEP query, and as if the value NEVER is specified, if the query is not a CEP query.
Default: empty

  

Examples:

UPDATE_TIMESTAMP(TIMESTAMP_FIELD_NAME="EXCHANGE_TIME")UPDATE_TIMESTAMP(TIMESTAMP_FIELD_NAME="EXCHANGE_TIME", MAX_OUT_OF_ORDER_INTERVAL_MSEC=2000, MAX_DELAY_OF_ORIGINAL_TIMESTAMP=5000, MAX_DELAY_OF_NEW_TIMESTAMP=5000)

See the UPDATE_TIMESTAMP_EXAMPLE
example in TRANSFORMER_EXAMPLES.otq.


	"""
	class Parameters:
		timestamp_field_name = "TIMESTAMP_FIELD_NAME"
		timestamp_psec_field_name = "TIMESTAMP_PSEC_FIELD_NAME"
		max_out_of_order_interval_msec = "MAX_OUT_OF_ORDER_INTERVAL_MSEC"
		max_delay_of_original_timestamp = "MAX_DELAY_OF_ORIGINAL_TIMESTAMP"
		max_delay_of_new_timestamp = "MAX_DELAY_OF_NEW_TIMESTAMP"
		out_of_order_timestamp_handling = "OUT_OF_ORDER_TIMESTAMP_HANDLING"
		max_delay_handling = "MAX_DELAY_HANDLING"
		zero_timestamp_handling = "ZERO_TIMESTAMP_HANDLING"
		log_sequence_violations = "LOG_SEQUENCE_VIOLATIONS"
		propagate_heartbeats = "PROPAGATE_HEARTBEATS"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["timestamp_field_name", "timestamp_psec_field_name", "max_out_of_order_interval_msec", "max_delay_of_original_timestamp", "max_delay_of_new_timestamp", "out_of_order_timestamp_handling", "max_delay_handling", "zero_timestamp_handling", "log_sequence_violations", "propagate_heartbeats"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["timestamp_field_name", "_default_timestamp_field_name", "timestamp_psec_field_name", "_default_timestamp_psec_field_name", "max_out_of_order_interval_msec", "_default_max_out_of_order_interval_msec", "max_delay_of_original_timestamp", "_default_max_delay_of_original_timestamp", "max_delay_of_new_timestamp", "_default_max_delay_of_new_timestamp", "out_of_order_timestamp_handling", "_default_out_of_order_timestamp_handling", "max_delay_handling", "_default_max_delay_handling", "zero_timestamp_handling", "_default_zero_timestamp_handling", "log_sequence_violations", "_default_log_sequence_violations", "propagate_heartbeats", "_default_propagate_heartbeats", "stack_info", "_used_strings"]

	class OutOfOrderTimestampHandling:
		COMPLAIN = "COMPLAIN"
		USE_ORIGINAL_TIMESTAMP = "USE_ORIGINAL_TIMESTAMP"
		USE_PREVIOUS_VALUE = "USE_PREVIOUS_VALUE"

	class MaxDelayHandling:
		COMPLAIN = "COMPLAIN"
		DISCARD = "DISCARD"
		USE_NEW_TIMESTAMP = "USE_NEW_TIMESTAMP"
		USE_ORIGINAL_TIMESTAMP = "USE_ORIGINAL_TIMESTAMP"

	class ZeroTimestampHandling:
		NONE = "NONE"
		PRESERVE_SEQUENCE = "PRESERVE_SEQUENCE"

	class PropagateHeartbeats:
		EMPTY = ""
		ALWAYS = "ALWAYS"
		NEVER = "NEVER"

	def __init__(self, timestamp_field_name="", timestamp_psec_field_name="", max_out_of_order_interval_msec=0, max_delay_of_original_timestamp=0, max_delay_of_new_timestamp=0, out_of_order_timestamp_handling=OutOfOrderTimestampHandling.COMPLAIN, max_delay_handling=MaxDelayHandling.COMPLAIN, zero_timestamp_handling=ZeroTimestampHandling.NONE, log_sequence_violations=False, propagate_heartbeats=PropagateHeartbeats.EMPTY):
		_graph_components.EpBase.__init__(self, "UPDATE_TIMESTAMP")
		self._default_timestamp_field_name = ""
		self.timestamp_field_name = timestamp_field_name
		self._default_timestamp_psec_field_name = ""
		self.timestamp_psec_field_name = timestamp_psec_field_name
		self._default_max_out_of_order_interval_msec = 0
		self.max_out_of_order_interval_msec = max_out_of_order_interval_msec
		self._default_max_delay_of_original_timestamp = 0
		self.max_delay_of_original_timestamp = max_delay_of_original_timestamp
		self._default_max_delay_of_new_timestamp = 0
		self.max_delay_of_new_timestamp = max_delay_of_new_timestamp
		self._default_out_of_order_timestamp_handling = type(self).OutOfOrderTimestampHandling.COMPLAIN
		self.out_of_order_timestamp_handling = out_of_order_timestamp_handling
		self._default_max_delay_handling = type(self).MaxDelayHandling.COMPLAIN
		self.max_delay_handling = max_delay_handling
		self._default_zero_timestamp_handling = type(self).ZeroTimestampHandling.NONE
		self.zero_timestamp_handling = zero_timestamp_handling
		self._default_log_sequence_violations = False
		self.log_sequence_violations = log_sequence_violations
		self._default_propagate_heartbeats = type(self).PropagateHeartbeats.EMPTY
		self.propagate_heartbeats = propagate_heartbeats
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_timestamp_field_name(self, value):
		self.timestamp_field_name = value
		return self

	def set_timestamp_psec_field_name(self, value):
		self.timestamp_psec_field_name = value
		return self

	def set_max_out_of_order_interval_msec(self, value):
		self.max_out_of_order_interval_msec = value
		return self

	def set_max_delay_of_original_timestamp(self, value):
		self.max_delay_of_original_timestamp = value
		return self

	def set_max_delay_of_new_timestamp(self, value):
		self.max_delay_of_new_timestamp = value
		return self

	def set_out_of_order_timestamp_handling(self, value):
		self.out_of_order_timestamp_handling = value
		return self

	def set_max_delay_handling(self, value):
		self.max_delay_handling = value
		return self

	def set_zero_timestamp_handling(self, value):
		self.zero_timestamp_handling = value
		return self

	def set_log_sequence_violations(self, value):
		self.log_sequence_violations = value
		return self

	def set_propagate_heartbeats(self, value):
		self.propagate_heartbeats = value
		return self

	@staticmethod
	def _get_name():
		return "UPDATE_TIMESTAMP"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.timestamp_field_name != "": 
			desc += "TIMESTAMP_FIELD_NAME=" + py_to_str(self.timestamp_field_name) + ","
		if self.timestamp_psec_field_name != "": 
			desc += "TIMESTAMP_PSEC_FIELD_NAME=" + py_to_str(self.timestamp_psec_field_name) + ","
		if self.max_out_of_order_interval_msec != 0: 
			desc += "MAX_OUT_OF_ORDER_INTERVAL_MSEC=" + py_to_str(self.max_out_of_order_interval_msec) + ","
		if self.max_delay_of_original_timestamp != 0: 
			desc += "MAX_DELAY_OF_ORIGINAL_TIMESTAMP=" + py_to_str(self.max_delay_of_original_timestamp) + ","
		if self.max_delay_of_new_timestamp != 0: 
			desc += "MAX_DELAY_OF_NEW_TIMESTAMP=" + py_to_str(self.max_delay_of_new_timestamp) + ","
		if self.out_of_order_timestamp_handling != self.OutOfOrderTimestampHandling.COMPLAIN: 
			desc += "OUT_OF_ORDER_TIMESTAMP_HANDLING=" + py_to_str(self.out_of_order_timestamp_handling) + ","
		if self.max_delay_handling != self.MaxDelayHandling.COMPLAIN: 
			desc += "MAX_DELAY_HANDLING=" + py_to_str(self.max_delay_handling) + ","
		if self.zero_timestamp_handling != self.ZeroTimestampHandling.NONE: 
			desc += "ZERO_TIMESTAMP_HANDLING=" + py_to_str(self.zero_timestamp_handling) + ","
		if self.log_sequence_violations != False: 
			desc += "LOG_SEQUENCE_VIOLATIONS=" + py_to_str(self.log_sequence_violations) + ","
		if self.propagate_heartbeats != self.PropagateHeartbeats.EMPTY: 
			desc += "PROPAGATE_HEARTBEATS=" + py_to_str(self.propagate_heartbeats) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class ModifyQueryTimes(_graph_components.EpBase):
	"""
		

MODIFY_QUERY_TIMES

Type: Other

Description: Allows overriding a query's start and end times
for a particular branch of the query. The start and end times set in
this EP applies to all its source EPs, all the way to the inputs of the
graph or until another MODIFY_QUERY_TIMES EP.

Python
class name:&nbsp;ModifyQueryTimes

Input: A time series of ticks

Output: A time series of ticks

Parameters:


  START_TIME (expression)
    The start time to override for source EPs, in milliseconds.
Default: Start time for this EP

  
  END_TIME (expression)
    The end time to override for source EPs, in milliseconds.
Default: End time for this EP

  
  OUTPUT_TIMESTAMP (expression)
    An expression that produces an output timestamp for each tick.
If it is not specified, the output tick timestamps are calculated by
the following expression: _START_TIME+TIMESTAMP-START_TIME_EXPR,
where START_TIME_EXPR is an expression specified in the START_TIME
parameter.
Default: empty

  
  PROPAGATE_HEARTBEATS (Boolean)
    Controls heartbeat propagation, if FALSE, MODIFY_QUERY_TIMES
drops heartbeats.
Default: TRUE

  

All of the expressions above are built from Boolean operators (AND,
OR, NOT), relationship operators (&lt;, &lt;=, &gt;, &gt;=, =, !=), and
arithmetic operators (+, -, *, /, +, also serve as string concatenation
operators) according to the usual rules of precedence, with parentheses
being available to resolve ambiguities. Along with constant numeric and
string literals (both single and double-quoted), it is also possible to
use OneTick functions like MIN, MAX, MOD, DIV, and so forth (see the Catalog of built-in functions for the full
list).

For the OUTPUT_TIMESTAMP parameter, state variables and fields of current
and preceding ticks (those that already arrived) can also be involved
in computations. In the latter case, field names are followed by a
negative index in square brackets, specifying how far to look (e.g.,
PRICE[-1], SIZE[-3], PRICE[2] etc.).

Pseudo-fields, such as TIMESTAMP,
_START_TIME, and _END_TIME, can also be used for all of expressions.

Examples: Shift start and time for one hour.

MODIFY_QUERY_TIMES(START_TIME="_START_TIME+3600*1000",END_TIME="_END_TIME+3600*1000")MODIFY_QUERY_TIMES(START_TIME="_START_TIME+3600*1000",OUTPUT_TIMESTAMP=TIMESTAMP)
See the examples modify_query_times1 and modify_query_times2 in OTHER_EXAMPLES.otq.


	"""
	class Parameters:
		start_time = "START_TIME"
		end_time = "END_TIME"
		output_timestamp = "OUTPUT_TIMESTAMP"
		propagate_heartbeats = "PROPAGATE_HEARTBEATS"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["start_time", "end_time", "output_timestamp", "propagate_heartbeats"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["start_time", "_default_start_time", "end_time", "_default_end_time", "output_timestamp", "_default_output_timestamp", "propagate_heartbeats", "_default_propagate_heartbeats", "stack_info", "_used_strings"]

	def __init__(self, start_time="", end_time="", output_timestamp="", propagate_heartbeats=True):
		_graph_components.EpBase.__init__(self, "MODIFY_QUERY_TIMES")
		self._default_start_time = ""
		self.start_time = start_time
		self._default_end_time = ""
		self.end_time = end_time
		self._default_output_timestamp = ""
		self.output_timestamp = output_timestamp
		self._default_propagate_heartbeats = True
		self.propagate_heartbeats = propagate_heartbeats
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_start_time(self, value):
		self.start_time = value
		return self

	def set_end_time(self, value):
		self.end_time = value
		return self

	def set_output_timestamp(self, value):
		self.output_timestamp = value
		return self

	def set_propagate_heartbeats(self, value):
		self.propagate_heartbeats = value
		return self

	@staticmethod
	def _get_name():
		return "MODIFY_QUERY_TIMES"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.start_time != "": 
			desc += "START_TIME=" + py_to_str(self.start_time) + ","
		if self.end_time != "": 
			desc += "END_TIME=" + py_to_str(self.end_time) + ","
		if self.output_timestamp != "": 
			desc += "OUTPUT_TIMESTAMP=" + py_to_str(self.output_timestamp) + ","
		if self.propagate_heartbeats != True: 
			desc += "PROPAGATE_HEARTBEATS=" + py_to_str(self.propagate_heartbeats) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class HelperFixBookExchTime(_graph_components.EpBase):
	"""
		

HELPER/FIX_BOOK_EXCH_TIME

Type: Transformer

Description: Sets exchange time field for ticks with
RECORD_TYPE equal to 'B' or 'E' and intermediate state ticks. For ticks
where RECORD_TYPE is 'B', it sets exchange time of the next tick in the
time series. For ticks where RECORD_TYPE is 'E', it sets the exchange
time of the previous tick. For intermediate state ticks, it sets the
exchange time of the previous tick, if there is any; otherwise it sets
it to zero.

The exchange time field name is specified as an input parameter.

Python
class name:
HelperFixBookExchTime

Input: A time series of ticks.

Output: A modified time series of ticks, one output tick for
each input tick.

Parameters:


  EXCHANGE_TIME_FIELD (string)
    Specifies the name of the exchange time field.
Default: EXCH_TIME

  
  PREPARE_FOR_UPDATE_TIMESTAMP
(Boolean)
    If set to true exchange time will be stored in DELETED_TIME for
intermediate state ticks.
Default: false

  


	"""
	class Parameters:
		exchange_time_field = "EXCHANGE_TIME_FIELD"
		prepare_for_update_timestamp = "PREPARE_FOR_UPDATE_TIMESTAMP"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["exchange_time_field", "prepare_for_update_timestamp"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["exchange_time_field", "_default_exchange_time_field", "prepare_for_update_timestamp", "_default_prepare_for_update_timestamp", "stack_info", "_used_strings"]

	def __init__(self, exchange_time_field="EXCH_TIME", prepare_for_update_timestamp=False):
		_graph_components.EpBase.__init__(self, "HELPER/FIX_BOOK_EXCH_TIME")
		self._default_exchange_time_field = "EXCH_TIME"
		self.exchange_time_field = exchange_time_field
		self._default_prepare_for_update_timestamp = False
		self.prepare_for_update_timestamp = prepare_for_update_timestamp
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_exchange_time_field(self, value):
		self.exchange_time_field = value
		return self

	def set_prepare_for_update_timestamp(self, value):
		self.prepare_for_update_timestamp = value
		return self

	@staticmethod
	def _get_name():
		return "HELPER/FIX_BOOK_EXCH_TIME"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.exchange_time_field != "EXCH_TIME": 
			desc += "EXCHANGE_TIME_FIELD=" + py_to_str(self.exchange_time_field) + ","
		if self.prepare_for_update_timestamp != False: 
			desc += "PREPARE_FOR_UPDATE_TIMESTAMP=" + py_to_str(self.prepare_for_update_timestamp) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class InterceptHeartbeats(_graph_components.EpBase):
	"""
		

INTERCEPT_HEARTBEATS

Type: Filter

Description: This event processor intercepts heartbeat
events, leaving all other events of a time series unchanged

Python
class name:&nbsp;InterceptHeartbeats

Input: A time series of ticks.

Output: A time series of ticks.

Parameters: There are no parameters for this event processor.


	"""
	class Parameters:
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = []
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["stack_info", "_used_strings"]

	def __init__(self):
		_graph_components.EpBase.__init__(self, "INTERCEPT_HEARTBEATS")
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	@staticmethod
	def _get_name():
		return "INTERCEPT_HEARTBEATS"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class InterceptDataQuality(_graph_components.EpBase):
	"""
		

INTERCEPT_DATA_QUALITY

Type: Filter

Description: This event processor removes data quality
messages, thus preventing them from delivery to the client application.

Python
class name:&nbsp;InterceptDataQuality

Input: A time series of ticks.

Output: A time series of ticks.

Parameters: There are no parameters for this event processor.

Examples:

Hide all data quality events within time interval:

INTERCEPT_DATA_QUALITY ()


	"""
	class Parameters:
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = []
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["stack_info", "_used_strings"]

	def __init__(self):
		_graph_components.EpBase.__init__(self, "INTERCEPT_DATA_QUALITY")
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	@staticmethod
	def _get_name():
		return "INTERCEPT_DATA_QUALITY"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class InterceptSymbolErrors(_graph_components.EpBase):
	"""
		

INTERCEPT_SYMBOL_ERRORS

Type: Filter

Description: This event processor removes per-symbol errors,
thus preventing them from delivery to the client application.

Python
class name:&nbsp;InterceptSymbolErrors

Input: A time series of ticks.

Output: A time series of ticks.

Parameters: There are no parameters for this event processor.

Examples: Show all data quality events within time interval:

INTERCEPT_SYMBOL_ERRORS ()

See the INTERCEPT_SYMBOL_ERRORS
example in FILTER_EXAMPLES.otq.


	"""
	class Parameters:
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = []
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["stack_info", "_used_strings"]

	def __init__(self):
		_graph_components.EpBase.__init__(self, "INTERCEPT_SYMBOL_ERRORS")
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	@staticmethod
	def _get_name():
		return "INTERCEPT_SYMBOL_ERRORS"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class InterceptInitializationTicks(_graph_components.EpBase):
	"""
		

INTERCEPT_INITIALIZATION_TICKS

Type: Filter

Description: This event processor removes initialization
ticks, thus preventing them from delivery to the client application.

Python
class name:&nbsp;InterceptInitializationTicks

Input: A time series of ticks.

Output: A time series of ticks.

Parameters: There are no parameters for this event processor.

Examples:

Hide all data quality events within time interval:

INTERCEPT_INITIALIZATION_TICKS ()


	"""
	class Parameters:
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = []
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["stack_info", "_used_strings"]

	def __init__(self):
		_graph_components.EpBase.__init__(self, "INTERCEPT_INITIALIZATION_TICKS")
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	@staticmethod
	def _get_name():
		return "INTERCEPT_INITIALIZATION_TICKS"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class CorrectTickFilter(_graph_components.EpBase):
	"""
		

CORRECT_TICK_FILTER

Type: Filter

Description: This allows one to view data as it was as of any
point in time. Correction or cancellation ticks will be applied if they
arrived before the time specified by AS_OF_TIME (DISCARD_ON_MATCH =
False), and if they were applied to the ticks that arrived after that
as_of time.

Python
class name:&nbsp;CorrectTickFilter

Input: A time series of ticks.

Output: On IF output branch, a time series of ticks, with
correction and cancelllations that arrived after the as_of_time not
applied to the ticks that arrived before or at the the as_of_time. On
the
ELSE output branch, the correct ticks, incorrect copies of which
arrived before or at the
as_of_time,&nbsp;which were corrected after as_of_time will be
propagated. The ticks propagated on IF and ELSE branches get swapped
when DISCARD_ON_MATCH=true.&nbsp;

Parameters: See parameters common
to many filters.


  DISCARD_ON_MATCH
(Boolean) 
  AS_OF_TIME (YYYYMMDDhhmmss)
    The cut-off time, in the timezone of the query, for the
correction tick or cancellation to be applied (the correction or
cancellation must
happen before or at this cut-off time, or the original (corrected or
cancelled) tick must arrive&nbsp;after this cutoff time,&nbsp; for this
correction or cancellation to affect the output of this EP). The
special value NOW (stands for "as of
now") ensures that only the corrected ticks are being sent to the
output stream. Special value 0 (stands for "beginning of time") results
in the propagation of all uncorrected ticks and correction ticks as
part of the stream. Default value: NOW.

  

Examples:

Filter out corrections/cancellations that happened after
20050103120000. All corrections time stamped later than that will be
sent as part of the stream. The ones time stamped before it will be
applied and the original tick will not be in the output stream. The
returned view of data is as of 20050103120000.

CORRECT_TICK_FILTER (20050103120000, false)

See the CORRECT_TICK_FILTER
example in FILTER_EXAMPLES.otq.


	"""
	class Parameters:
		discard_on_match = "DISCARD_ON_MATCH"
		as_of_time = "AS_OF_TIME"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["discard_on_match", "as_of_time"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["discard_on_match", "_default_discard_on_match", "as_of_time", "_default_as_of_time", "stack_info", "_used_strings"]

	def __init__(self, discard_on_match=False, as_of_time="NOW"):
		_graph_components.EpBase.__init__(self, "CORRECT_TICK_FILTER")
		self._default_discard_on_match = False
		self.discard_on_match = discard_on_match
		self._default_as_of_time = "NOW"
		self.as_of_time = as_of_time
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_discard_on_match(self, value):
		self.discard_on_match = value
		return self

	def set_as_of_time(self, value):
		self.as_of_time = value
		return self

	@staticmethod
	def _get_name():
		return "CORRECT_TICK_FILTER"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.discard_on_match != False: 
			desc += "DISCARD_ON_MATCH=" + py_to_str(self.discard_on_match) + ","
		if self.as_of_time != "NOW": 
			desc += "AS_OF_TIME=" + py_to_str(self.as_of_time) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class Om_matchingEngineSimple(_graph_components.EpBase):
	"""
		

OM::MATCHING_ENGINE_SIMPLE

Type: Other

Description: Implements order matching logic for market,
limit, and cancel orders as well as maintains state and performs
recovery after a failure. An input tick is interpreted as an order
based on the values of some fields. Values of ORDER_TYPE and BUY_SELL_FLAG
specify rules for order processing, some orders can be processed only
instantly, others can rest in the book until some condition is met. For
those resting in the book, cancel orders can be submitted.

The currently supported order types and their behavior in normal
processing mode follow:


  Limit orders
    Limit order is an order to buy BUY_SELL_FLAG=0
or sell BUY_SELL_FLAG=1 the amount ORIG_SIZE at ORIG_PRICE
price or better.
If ORDER_TYPE=L, the part of the
order that can be matched immediately is matched and the remainder
rests in the book until a suiting order from other side arrives.
If ORDER_TYPE=LM, order type for
market makers, no part of the order can be matched immediately or else
it is rejected fully.
If ORDER_TYPE=LIOC, the part of the
order that can be matched immediately is matched and the remainder is
canceled.
If ORDER_TYPE=LFOK, the order is
matched only if it can be matched fully and immediately, otherwise it
is rejected.

  
  Market orders
    Market orders should have ORDER_TYPE
= M
A market buy order is an order to buy some amount by spending ORIG_SIZE amount of money at ORIG_PRICE price or better.
A market sell order is an order to sell ORIG_SIZE
amount at ORIG_PRICE price or better.
    ORIG_PRICE is optional in both
cases and if it is not specified any price will qualify. A market order
can be matched partially and the rest of the order will be canceled. An
order is rejected if it cannot be matched even partially.

  
  Stop orders
    Stop orders rest in the "hidden book", until a trade happens
with a price which crosses a threshold specified via STOP_PRICE field. A stop order with STOP_PRICE=PRICE1 will rest until a trade
happens, which has a price not less than PRICE1
if the stop order was buy, and not greater than PRICE1, otherwise.
Value ORDER_TYPE = LS corresponds to
stop limit orders and ORDER_TYPE = MS
to stop market orders.
When the condition is met, the order is turned into a common limit or
market order with all fields unchanged except ORDER_TYPE, that is, with ORDER_TYPE = L or ORDER_TYPE = M.

  
  Trailing stop orders
    Trailing stop orders rest in the "trailing hidden book" and
behave as if they are common stop orders with stop price recalculated
on every trade. For example, if the order is buy trailing stop order
and trade price is PRICE1, stop
price is calculated as PRICE1 + STOP_PRICE,
then if trade price goes down to PRICE2,
stop price is recalculated for PRICE2
and if trade price reaches stop price without going below price used
for stop price calculation, the trailing stop order turns into a common
one. Everything is symmetrical in case of a sell order.
Value ORDER_TYPE = LST corresponds
to trailing stop limit orders and ORDER_TYPE
= MST to trailing stop market orders.

  
  Take profit orders
    Take profit orders rest in the "hidden book", until a trade
happens with a price that crosses the threshold specified via the STOP_PRICE field. A take profit order with
    STOP_PRICE=PRICE1 will rest until
a trade happens, which has a price not greater than PRICE1 if the take profit order was buy,
and not less than PRICE1, otherwise.
The ORDER_TYPE = LP value
corresponds to take profit limit orders and ORDER_TYPE
= MP to take profit market orders.
When the condition is met, the order is turned into a regular limit or
market order without changing its fields except for ORDER_TYPE; that is, with ORDER_TYPE = L or ORDER_TYPE = M.

  
  One cancels the other
    An order with ORDER_TYPE=OCO
order combines a limit order with a stop market order. When either the
stop or limit price is reached and the order executed, the other order
automatically gets canceled. BUY_SELL_FLAG
in such orders refers to limit order and stop market order is
considered to be on opposite side. In case of sell limit order, the ORIG_SIZE for associated stop market order
is calculated as ORIG_SIZE*ORIG_PRICE.
In case of buy limit order ORIG_SIZE
is unchanged. If one of orders gets activated, all output ticks should
match the case when this order arrived as a separate order plus a
message for associated order cancellation.

  
  Replace orders
    Replace orders ORDER_TYPE = R
are used to replace price and size of a limit order which is currently
in the book. They should carry the same ORDER_ID
and BUY_SELL_FLAG as the original
order which they intend to replace. Replace order implies changing the
original size not live size, so if executed part of order is greater or
equal than new value of ORIG_SIZE,
the replace is rejected.

  
  Cancel orders
    Cancel orders are used to cancel an order which is currently in
the book and should carry ORDER_TYPE = C.
They should carry the same ORDER_ID
and BUY_SELL_FLAG as the original
order which they intend to cancel.

    Besides the normal processing mode, the EP has a frozen
processing mode, which is turned on if a tick arrives with ORDER_TYPE=F. In this mode, we accept
common limit orders and add them to the book without making an attempt
to match them. Cancels are handled as usual. When we return to normal
processing mode after receiving ORDER_TYPE=U,
all orders added to the book in frozen processing mode are matched
respectively to the order they arrived.

  

Python
class name:&nbsp;Om_matchingEngineSimple

Input:

Some input fields control EP logic, some are empty on input and set
on output and some are ignored and propagated unchanged for
corresponding output messages (a single input order can have numerous
associated output ticks).

Input tick samples:


  
    
      Field type
      Field name
      Example value
      Description
    
    
      uInt32
      OMDSEQ
      1500
      Propagated unchanged
    
    
      Int32
      ACCOUNT_ID
      4
      Propagated unchanged
    
    
      Int32
      ACCOUNT_GROUP_ID
      4
      Propagated unchanged
    
    
      Int32
      USER_ID
      145
      Propagated unchanged
    
    
      uInt64
      ORDER_ID
      14522
      Propagated unchanged
    
    
      String
      ORDER_TYPE
      "M"
      Order type (L, LM, LIOC, LFOK, M, LS, MS, LP, MP, LST, MST,
OCO, R, C)
    
    
      Int32
      TRD_PAIR_ID
      4
      Propagated unchanged
    
    
      uInt64
      FROM_WALLET_ID
      1300
      Propagated unchanged
    
    
      uInt64
      TO_WALLET_ID
      1302
      Propagated unchanged
    
    
      uInt64
      FEE_WALLET_ID
      1301
      Propagated unchanged
    
    
      Double
      SIZE
      0.0
      Set on output.
    
    
      Double
      PRICE
      0.0
      Set on output.
    
    
      Double
      STOP_VAL
      NaN
      NaN for Market and Limit orders. Represents an absolute or
relative stop price.
    
    
      Byte
      BUY_SELL_FLAG
      0
      Should be provided for cancellations too. 0 for BID/Buy, 1
for ASK/Sell
    
    
      Double
      ORIG_PRICE
      5000.123698
      Order price
    
    
      Double
      REMAINING_SIZE
      0.0
      Set on output
    
    
      Double
      ORIG_SIZE
      0.10021
      Size of order, interpreted depending on ORDER_TYPE
    
    
      nsec64
      UPDATE_TIME
      0
      Set on output
    
    
      Int32
      TICK_STATUS
      0
      Set on output
    
    
      String
      RECORD_TYPE
      
      Set on output
    
    
      String
      TICK_TYPE
      
      Set on output
    
    
      uInt64
      COUNTER_PARTY_ORDER_ID
      0
      Set on output
    
    
      Byte
      MARKET_TAKER_FLAG
      0
      Set on output
    
    
      msec64
      ORIG_TIME
      1540491900000
      Original order's timestamp
    
    
      Int32
      MESEQ
      0
      Set on output
    
    
      Double
      VWAP
      NaN
      Set on output
    
    
      Byte
      REASON
      0
      Set on output
    
  

Output:

For BOOK,TBOOK,FEEDBACK outputs
all fields not mentioned below inherit their value from input order
tick associated with them.

BOOK output has the same fields as
the input tick, except that the REMAINING_SIZE
field is replaced by the EXECUTED_SIZE
field and the ORDER_ID field is
added as a state key. This output has ticks maintaining the state of
the EP book, like PRL, HPRL. The
MATCHING_ENGINE_SIMPLE EP uses them to recover its state in case of
failure and those ticks can be queried by the user directly or via
special book-processing EPs. The output depends on ORDER_TYPE and BUY_SELL_FLAG:


  ORDER_TYPE=L
    
      If an order enters the PRL book without being matched, it has
the following values of size fields:
        SIZE=ORIG_SIZE,PRICE=ORIG_PRICE,EXECUTED_SIZE=0.0,RECORD_TYPE=N
      If an order rests in the book and is matched by another newly
arrived order, a PRL tick is generated with the following values of
size fields:
        SIZE=&lt;remaining size of the order
after this match&gt;,EXECUTED_SIZE=&lt;size of the current
match&gt;,RECORD_TYPE=U.
      If the order rests in the book and is canceled a PRL tick is
generated with the following values of size fields: SIZE=0.0,EXECUTED_SIZE=&lt;canceled
size&gt;,VWAP=&lt;vwap price&gt;,RECORD_TYPE=C.
      If a part of an order is matched a group of PRL ticks is
recorded starting with B tick and containing an update for each matched
level (as described in 2) ). If after match the order has a remaining
part, a PRL tick is generated with the following values of size fields:
        SIZE=&lt;remaining size of the
order&gt;, PRICE=ORIG_PRICE, EXECUTED_SIZE=&lt;total executed
size&gt;,RECORD_TYPE=N , and the group is ending with the
following PRL tick: SIZE=&lt;remaining size
of the newly arrived order&gt;,EXECUTED_SIZE=&lt;total executed size
for the newly arrived order&gt;,RECORD_TYPE=E.
    
  
  ORDER_TYPE=LM
    
      If the order can be matched even partially 2 PRL ticks are
generated, first with RECORD_TYPE=B,
second with the following fields: SIZE=ORIG_SIZE,PRICE=ORIG_PRICE,EXECUTED_SIZE=0.0,RECORD_TYPE=E.
      If the order is not matched and rests in the book point 2)
under ORDER_TYPE=L applies.
    
  
  ORDER_TYPE=LFOK
    
      If the order cannot be matched fully 2 PRL ticks are
generated, first with RECORD_TYPE=B,
second with the following fields: SIZE=ORIG_SIZE,EXECUTED_SIZE=0.0,RECORD_TYPE=E.
      If the order is matched point 4) under ORDER_TYPE=L applies (as the order is
matched only fully no tick with RECORD_TYPE=N
will be generated).
    
  
  ORDER_TYPE=LIOC or ORDER_TYPE=M,BUY_SELL_FLAG=1
    
      If the order cannot be matched even partially 2 PRL ticks are
generated, first with RECORD_TYPE=B,
second with the following fields: SIZE=ORIG_SIZE,EXECUTED_SIZE=0.0,RECORD_TYPE=E.
      If the order is matched (even partially) point 4) under ORDER_TYPE=L applies except no tick with RECORD_TYPE=N is generated.
    
  
  ORDER_TYPE=M,BUY_SELL_FLAG=0(the
case of market buy)
    
      Converted size is calculated and we try to fill only that
size, here are rules for calculation:
        
          Sum of PRICE*SIZE of
PRL orders starting from top of the opposite side of the book is
calculated until it exceeds value of ORIG_SIZE
of market buy order or until opposite side of the book fades.
          Simultaneously sum of SIZE
values is calculated for these levels.
          Converted size is &lt;sum
calculated at b.&gt; - (&lt;sum calculated at a.&gt; - ORIG_SIZE) /
(&lt;last level price&gt; in case if ORIG_SIZE
is exceeded and just &lt;sum calculated at
b.&gt; otherwise.
          Converted size value is possibly modified to pass
granularity rules specified by MATCHING_ENGINE.SIZE_STEP_PER_TRADING_PAIR_FILE
EP parameter.
        
      
      If available size is zero 2 PRL ticks are generated, first
with RECORD_TYPE=B, second with the
following fields: SIZE=ORIG_SIZE,EXECUTED_SIZE=0.0,RECORD_TYPE=E.
      If the order is matched point 4) under ORDER_TYPE=L applies except remaining size
is calculated based on converted size rather than ORIG_SIZE (no tick with RECORD_TYPE=N will be generated).
    
  
  ORDER_TYPE=LS/MS/LP/MP
    
      If the order does not pass order validation rules (limit
price is expected to be either bigger or smaller than last trade price
depending on order type, ORDER_ID
value should be unique), 2 PRL ticks are generated, first with RECORD_TYPE=B, second with the following
fields: SIZE=ORIG_SIZE,EXECUTED_SIZE=0.0,RECORD_TYPE=E.
      If the order is canceled while in the HPRL book, an HPRL tick
is generated with the following values of size fields: SIZE=0.0,EXECUTED_SIZE=&lt;canceled
size&gt;,RECORD_TYPE=C.
      If the order is activated and it exits in the HPRL book, the
following HPRL tick is generated: SIZE=0.0,EXECUTED_SIZE=0.0
and RECORD_TYPE = U. Later, the stop/profit order is handled as
a common limit or market order.
    
  

A pair of hidden ticks with RECORD_TYPE=B
and SIZE=ORIG_SIZE,EXECUTED_SIZE=0.0,RECORD_TYPE=E
is also produced when:


  Input tick has RECORD_TYPE=r.
  Matching is frozen and order isn't a common limit order (has ORDER_TYPE different from L).
  Order limit price violates the distance specified in file
referenced by MATCHING_ENGINE.CONFIGURATION_FILE
via MAX_PRICE_DISTANCE parameter.

Above mentioned logic checks for duplicate ORDER_ID value only when adding order to
book. Thus, if an order with duplicate ORDER_ID
arrives, part of it which can be matched immediately (for appropriate ORDER_TYPE values) is filled and the rest
is ignored with an appropriate warning logged.

After matching is unfrozen, for every limit order which
entered the book after freeze event an attempt of matching against the
previous book state is performed. Rules for handling ORDER_TYPE=L apply except a tick which had
RECORD_TYPE=N now has RECORD_TYPE=U and it is generated even if
the order is fully matched (in that case E tick has NaN values for size
fields and zero value for order id).

ORIG_TIME field for PRL tick type is set to timestamp when the
associated order entered the book. UPDATE_TIME
field for PRL tick type is set to
associated order modification timestamp.

MP,DMP tick types are
used for recovery reasons.

TBOOK output has the same schema
as input tick except REMAINING_SIZE
is replaced by EXECUTED_SIZE, BUY_SELL_FLAG is replaced by SELL_FLAG and ORDER_ID
field added as state key.


  If an input order has ORDER_TYPE=LST/MST
a TPRL tick with following fields is generated: SIZE=ORIG_SIZE,EXECUTED_SIZE=0.0,RECORD_TYPE=N.
  
  If the order rests in the book and is canceled a TPRL tick is
generated with the following values of size fields: SIZE=0.0,EXECUTED_SIZE=&lt;canceled
size&gt;,RECORD_TYPE=C.
  
  If the order is activated and exits TPRL book the following TPRL
tick is generated: SIZE=0.0,EXECUTED_SIZE=0.0
and RECORD_TYPE = U. Later the trailing order is handled as
common limit or market order.

FEEDBACK output has several types
of ticks.
Ticks with TICK_TYPE=FILL are
generated when a match occurs, a single tick for each of 2 sides having
SIZE set to the matched size, PRICE set to the matched price as well as REMAINING_SIZE, MARKET_TAKER_FLAG
and COUNTER_PARTY_ORDER_ID set to
appropriate values.
A tick with TICK_TYPE=CNC is
generated when a successful cancel occurs with SIZE set to canceled size.
A tick with TICK_TYPE=RJC is
generated when an order is rejected by matching engine with SIZE=ORIG_SIZE.
A tick with TICK_TYPE=RPL is
generated when a successful replace happens with ORIG_SIZE and ORIG_PRICE
set to corresponding values from original order and SIZE=&lt;ORIG_SIZE of replacing order&gt;
and PRICE=&lt;ORIG_PRICE of replacing
order&gt;.


FEEDBACK output schema is the same
as input tick:


  
    
      Field type
      Field name
      Example value
      Description
    
    
      uInt32
      OMDSEQ
      1500
      Same as in associated order.
    
    
      Int32
      ACCOUNT_ID
      4
      Same as in associated order.
    
    
      Int32
      ACCOUNT_GROUP_ID
      4
      Same as in associated order.
    
    
      Int32
      USER_ID
      145
      Same as in associated order.
    
    
      uInt64
      ORDER_ID
      14522
      Same as in associated order.
    
    
      String
      ORDER_TYPE
      "M"
      Set to order type associated order had when
filled/canceled/rejected. Might be different from original value in
associated input order (for example, LS is converted to L when stop
order is turned to common limit order).
    
    
      Int32
      TRD_PAIR_ID
      4
      Same as in associated order.
    
    
      uInt64
      FROM_WALLET_ID
      1300
      Same as in associated order.
    
    
      uInt64
      TO_WALLET_ID
      1302
      Same as in associated order.
    
    
      uInt64
      FEE_WALLET_ID
      1301
      Same as in associated order.
    
    
      Double
      SIZE
      0.0
      For FILLs set to size of current fill. For CNCs set to live
size prior cancel. For RJCs has same value as ORIG_SIZE.
    
    
      Double
      PRICE
      0.0
      For FILLs set to fill price.
    
    
      Double
      STOP_VAL
      NaN
      Same as in associated order.
    
    
      Byte
      BUY_SELL_FLAG
      0
      Same as in associated order.
    
    
      Double
      ORIG_PRICE
      5000.123698
      Same as in associated order.
    
    
      Double
      REMAINING_SIZE
      0.0
      For FILLs set to size remaining after this fill. For market
buys size remaining after this fill is calculated based on converted
size (see section for converted size calculation under ORDER_TYPE=M,BUY_SELL_FLAG=0)
    
    
      Double
      ORIG_SIZE
      0.10021
      Same as in associated order.
    
    
      nsec64
      UPDATE_TIME
      0
      Last modification time of the order, for FILLs set to
triggering order timestamp.
    
    
      Int32
      TICK_STATUS
      0
      
    
    
      String
      RECORD_TYPE
      
      For RJCs set to R.
    
    
      String
      TICK_TYPE
      
      "FILL","RJC" or "CNC"
    
    
      uInt64
      COUNTER_PARTY_ORDER_ID
      0
      For FILLs set to ORDER_ID of the order on opposite side of
the book which matched associated order
    
    
      Byte
      MARKET_TAKER_FLAG
      0
      1 if associated order is more recent than the order it
matched, 0 otherwise
    
    
      msec64
      ORIG_TIME
      1540491900000
      Timestamp of associated order entering the book.
    
    
      Int32
      MESEQ
      0
      Used for internal ordering
    
    
      Double
      VWAP
      NaN
      For FILLs set to volume weighted average price of the
particular fill.
    
    
      Byte
      REASON
      0
      Is set to following rejection reasons:
      
        102 - price too distance from last traded price
        103 - order type is not supported during trading pair freeze
        104 - an attempt to add an order with duplicate ORDER_ID
value
        105 - price violates rules for order type
        106 - opposite side size matching specified price is not
sufficient
        107 - market maker order rejected because price allows
immediate execution
        108 - opposite side size matching specified price is zero
        109 - an attempt to replace missing order
        110 - replace came for unsupported order type or for order
with insufficient size
      
And following cancellation reasons:
      
        150 - canceled size which cannot be matched immediately
        151 - canceled to prevent self trade
        152 - canceled as part of OCO order
      
      
    
  

TRD output is for TRD ticks having SIZE,PRICE,TAKER_SELL_FLAG fields. TAKER_SELL_FLAG field has the value of 1 if a sell order has triggered the trade;
and 0, otherwise.

Parameters:


  MAX_EXPECTED_BOOK_ENTRIES_PER_SIDE
(integer)
    Maximum expected number of active orders (not canceled or
filled) at each side of the book. Needed to optimize the EP order
processing time.
Default value: 30000

  
  TRAILING_BOOK_DUMP_INTERVAL
(integer)
    The amount of ticks written on trailing book stop price
recalculation might be huge and for that reason, the EP does not output
ticks for those changes. Instead, it outputs internal data needed to
restore that trailing book state. To minimize the time period this
"restoring" needs to consider, the state is dumped with some period in
seconds specified via this parameter.
Default value: 3600

  
  MAX_INITIALIZATION_DAYS
(integer)
    The number of days to go back in search of the full state of a
time series, which could be an empty state.
Default value: 2

  
  REQUIRE_INTEGER_SYMBOLS
(boolean)
    If set to true, symbol names should be numeric (representing
trading pair id). If this EP is used with OM::WALLET_MANAGER
EP, this option should be set to true.
Default value: false

  
  STATE_INITIALIZATION_OTQ
(string path)
    Path of the query used to retrieve the state of the engine. The
query is expected to have MAX_INITIALIZATION_DAYS
and TICK_TYPES otq parameters which
are set automatically before its invocation. If the query is not
specified the default behavior is to retrieve state ticks ordered by ORIG_TIME and MESEQ
field values.
Default value:

  
  STATE_INITIALIZATION_OTQ_PARAMS
(string)
    Comma-separated list of &lt;name&gt;=&lt;value&gt;
pairs representing otq parameters for state initialization query.
Default value:

  

Configuration parameters:


  MATCHING_ENGINE.CONFIGURATION_FILE
(string path)
    Path of the file specifying configuration parameters: size
granularity, max quote distances from the last trading price, flag for
self trade prevention. File contents is expected to be in a format
where each line consists of the following: &lt;PARAMETER_NAME&gt;,&lt;YYYYmmddhhmmss&gt;,&lt;trd
pair id&gt;,&lt;PARAMETER_VALUE&gt;.
    &lt;YYYYmmddhhmmss&gt; should
stand for time when a new value comes to effect. Multiple entries for
same trading pair id can be specified.

    
      For size step PARAMETER_NAME=SIZE_STEP,PARAMETER_VALUE=&lt;size
step&gt;, default value for size step is 1e-008.
      For max quote distance PARAMETER_NAME=MAX_PRICE_DISTANCE,PARAMETER_VALUE="&lt;ask
distance&gt;,&lt;bid distance&gt;", default value for distance
is NaN(any).
      For self trade prevention flag PARAMETER_NAME=SELF_TRADE_PREVENTION,PARAMETER_VALUE=&lt;boolean&gt;,
default value for flag is true.
    
Default value: empty
  MATCHING_ENGINE.REPORT_MESSAGE_RATE
(boolean)
    If set, it turns on logging of book sizes.
Default value: true

  
  MATCHING_ENGINE.NUM_TICKS_BETWEEN_REPORTS
(integer)
    Used in combination with the previous parameter. Interval in
ticks after in which logging is done.
Default value: 500000

  
  MATCHING_ENGINE.ALLOCATOR_BLOCK_SIZE
(integer)
    Block size for the underlying allocator object. Used for
performance optimizations.
Default value: 1024

  
  FX_EXCHANGE.FEE_CURRENCY_RATE_REFRESH_INTERVAL_SEC
(integer)
    This EP maintains a conversion rate map between currencies
refreshed after first tick arrival in the interval specified via this
parameter.
Default value: 900

  




	"""
	class Parameters:
		max_expected_book_entries_per_side = "MAX_EXPECTED_BOOK_ENTRIES_PER_SIDE"
		trailing_book_dump_interval = "TRAILING_BOOK_DUMP_INTERVAL"
		max_initialization_days = "MAX_INITIALIZATION_DAYS"
		state_initialization_otq = "STATE_INITIALIZATION_OTQ"
		state_initialization_otq_params = "STATE_INITIALIZATION_OTQ_PARAMS"
		require_integer_symbols = "REQUIRE_INTEGER_SYMBOLS"
		treat_mkt_buy_size_as_total_cost = "TREAT_MKT_BUY_SIZE_AS_TOTAL_COST"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["max_expected_book_entries_per_side", "trailing_book_dump_interval", "max_initialization_days", "state_initialization_otq", "state_initialization_otq_params", "require_integer_symbols", "treat_mkt_buy_size_as_total_cost"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["max_expected_book_entries_per_side", "_default_max_expected_book_entries_per_side", "trailing_book_dump_interval", "_default_trailing_book_dump_interval", "max_initialization_days", "_default_max_initialization_days", "state_initialization_otq", "_default_state_initialization_otq", "state_initialization_otq_params", "_default_state_initialization_otq_params", "require_integer_symbols", "_default_require_integer_symbols", "treat_mkt_buy_size_as_total_cost", "_default_treat_mkt_buy_size_as_total_cost", "stack_info", "_used_strings"]

	def __init__(self, max_expected_book_entries_per_side=30000, trailing_book_dump_interval=3600, max_initialization_days=2, state_initialization_otq="", state_initialization_otq_params="", require_integer_symbols="", treat_mkt_buy_size_as_total_cost=""):
		_graph_components.EpBase.__init__(self, "OM::MATCHING_ENGINE_SIMPLE")
		self._default_max_expected_book_entries_per_side = 30000
		self.max_expected_book_entries_per_side = max_expected_book_entries_per_side
		self._default_trailing_book_dump_interval = 3600
		self.trailing_book_dump_interval = trailing_book_dump_interval
		self._default_max_initialization_days = 2
		self.max_initialization_days = max_initialization_days
		self._default_state_initialization_otq = ""
		self.state_initialization_otq = state_initialization_otq
		self._default_state_initialization_otq_params = ""
		self.state_initialization_otq_params = state_initialization_otq_params
		self._default_require_integer_symbols = ""
		self.require_integer_symbols = require_integer_symbols
		self._default_treat_mkt_buy_size_as_total_cost = ""
		self.treat_mkt_buy_size_as_total_cost = treat_mkt_buy_size_as_total_cost
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_max_expected_book_entries_per_side(self, value):
		self.max_expected_book_entries_per_side = value
		return self

	def set_trailing_book_dump_interval(self, value):
		self.trailing_book_dump_interval = value
		return self

	def set_max_initialization_days(self, value):
		self.max_initialization_days = value
		return self

	def set_state_initialization_otq(self, value):
		self.state_initialization_otq = value
		return self

	def set_state_initialization_otq_params(self, value):
		self.state_initialization_otq_params = value
		return self

	def set_require_integer_symbols(self, value):
		self.require_integer_symbols = value
		return self

	def set_treat_mkt_buy_size_as_total_cost(self, value):
		self.treat_mkt_buy_size_as_total_cost = value
		return self

	@staticmethod
	def _get_name():
		return "OM::MATCHING_ENGINE_SIMPLE"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.max_expected_book_entries_per_side != 30000: 
			desc += "MAX_EXPECTED_BOOK_ENTRIES_PER_SIDE=" + py_to_str(self.max_expected_book_entries_per_side) + ","
		if self.trailing_book_dump_interval != 3600: 
			desc += "TRAILING_BOOK_DUMP_INTERVAL=" + py_to_str(self.trailing_book_dump_interval) + ","
		if self.max_initialization_days != 2: 
			desc += "MAX_INITIALIZATION_DAYS=" + py_to_str(self.max_initialization_days) + ","
		if self.state_initialization_otq != "": 
			desc += "STATE_INITIALIZATION_OTQ=" + py_to_str(self.state_initialization_otq) + ","
		if self.state_initialization_otq_params != "": 
			desc += "STATE_INITIALIZATION_OTQ_PARAMS=" + py_to_str(self.state_initialization_otq_params) + ","
		if self.require_integer_symbols != "": 
			desc += "REQUIRE_INTEGER_SYMBOLS=" + py_to_str(self.require_integer_symbols) + ","
		if self.treat_mkt_buy_size_as_total_cost != "": 
			desc += "TREAT_MKT_BUY_SIZE_AS_TOTAL_COST=" + py_to_str(self.treat_mkt_buy_size_as_total_cost) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class TrdVsMid(_graph_components.EpBase):
	"""
		

TRD_VS_MID

Type: Filter

Description: Propagates trade ticks based on the comparison
of their price the prevailing mid-quote at the time. MID_QUOTE =
(ASK_PRICE+BID_PRICE) / 2

Python
class name:
TrdVsMid

Input: A time series of ticks containing a field PRICE,
and a time series of ticks containing fields named ASK_PRICE
and BID_PRICE.
Both input EPs that connect to TRD_VS_MID EP must have node names, and
the input EP for the quote branch must have&nbsp;node name QTE. If
only one&nbsp;EP is present at the input of&nbsp;TRD_VS_MID
EP, its tick type must be TRD+QTE.

Output: A time series of filtered ticks.

Parameters: See parameters common
to many filters.


  DISCARD_ON_MATCH
(Boolean)
  RELATIONSHIP_TO_QUOTE
(LT|LE|EQ|NE|GE|GT)
Default value: GE

Examples: Only keep trade ticks that were made below the
mid-spread:

TRD_VS_MID (false, LT)

See the TRD_VS_MID example in FILTER_EXAMPLES.otq.


	"""
	class Parameters:
		discard_on_match = "DISCARD_ON_MATCH"
		relationship_to_mid = "RELATIONSHIP_TO_MID"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["discard_on_match", "relationship_to_mid"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["discard_on_match", "_default_discard_on_match", "relationship_to_mid", "_default_relationship_to_mid", "stack_info", "_used_strings"]

	class RelationshipToMid:
		EQ = "EQ"
		GE = "GE"
		GT = "GT"
		LE = "LE"
		LT = "LT"
		NE = "NE"

	def __init__(self, discard_on_match=False, relationship_to_mid=RelationshipToMid.GE):
		_graph_components.EpBase.__init__(self, "TRD_VS_MID")
		self._default_discard_on_match = False
		self.discard_on_match = discard_on_match
		self._default_relationship_to_mid = type(self).RelationshipToMid.GE
		self.relationship_to_mid = relationship_to_mid
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_discard_on_match(self, value):
		self.discard_on_match = value
		return self

	def set_relationship_to_mid(self, value):
		self.relationship_to_mid = value
		return self

	@staticmethod
	def _get_name():
		return "TRD_VS_MID"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.discard_on_match != False: 
			desc += "DISCARD_ON_MATCH=" + py_to_str(self.discard_on_match) + ","
		if self.relationship_to_mid != self.RelationshipToMid.GE: 
			desc += "RELATIONSHIP_TO_MID=" + py_to_str(self.relationship_to_mid) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class TrdVsQuote(_graph_components.EpBase):
	"""
		

TRD_VS_QUOTE

Type: Filter

Description: Propagates trade ticks based on the comparison
of their price to either the prevailing bid-price or the ask-price at
the time.

Python
class name:&nbsp;TrdVsQuote

Input: A time series of ticks containing a field PRICE;
a time series of ticks containing fields named ASK_PRICE and BID_PRICE.&nbsp;Both
input EPs that connect to TRD_VS_QUOTE EP must have node names, and
the input EP for the quote branch must have&nbsp;node name QTE. If
only one&nbsp;EP is present at the input of&nbsp;TRD_VS_QUOTE
EP, its tick type must be TRD+QTE.

Output: A time series of filtered ticks.

Parameters: See parameters common
to many filters.


  DISCARD_ON_MATCH
(Boolean)
  RELATIONSHIP_TO_QUOTE
(LT|LE|EQ|NE|GE|GT)
Default: GE
  SIDE (BID|ASK)
    Specifies whether the function is to be applied to buy orders (BID)
or sell orders (ASK).
Default: ASK

  

Examples: Only keep trade ticks that were done at the
prevailing ask price:

TRD_VS_QUOTE (false, EQ, ASK)

See the TRD_VS_QUOTE example in FILTER_EXAMPLES.otq.


	"""
	class Parameters:
		discard_on_match = "DISCARD_ON_MATCH"
		relationship_to_quote = "RELATIONSHIP_TO_QUOTE"
		side = "SIDE"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["discard_on_match", "relationship_to_quote", "side"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["discard_on_match", "_default_discard_on_match", "relationship_to_quote", "_default_relationship_to_quote", "side", "_default_side", "stack_info", "_used_strings"]

	class RelationshipToQuote:
		EQ = "EQ"
		GE = "GE"
		GT = "GT"
		LE = "LE"
		LT = "LT"
		NE = "NE"

	class Side:
		ASK = "ASK"
		BID = "BID"

	def __init__(self, discard_on_match=False, relationship_to_quote=RelationshipToQuote.GE, side=Side.ASK):
		_graph_components.EpBase.__init__(self, "TRD_VS_QUOTE")
		self._default_discard_on_match = False
		self.discard_on_match = discard_on_match
		self._default_relationship_to_quote = type(self).RelationshipToQuote.GE
		self.relationship_to_quote = relationship_to_quote
		self._default_side = type(self).Side.ASK
		self.side = side
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_discard_on_match(self, value):
		self.discard_on_match = value
		return self

	def set_relationship_to_quote(self, value):
		self.relationship_to_quote = value
		return self

	def set_side(self, value):
		self.side = value
		return self

	@staticmethod
	def _get_name():
		return "TRD_VS_QUOTE"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.discard_on_match != False: 
			desc += "DISCARD_ON_MATCH=" + py_to_str(self.discard_on_match) + ","
		if self.relationship_to_quote != self.RelationshipToQuote.GE: 
			desc += "RELATIONSHIP_TO_QUOTE=" + py_to_str(self.relationship_to_quote) + ","
		if self.side != self.Side.ASK: 
			desc += "SIDE=" + py_to_str(self.side) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class LeeAndReady(_graph_components.EpBase):
	"""
		

LEE_AND_READY

Type: Transformer

Description: Adds a numeric attribute to each tick in the
stream of trade ticks, the value of which classifies the trade as a
buy, a sell, or undefined.

This is an implementation of the Lee and Ready algorithm: Match up a
trade with the most recent good quote that is at least X seconds older
than the trade - if the trade's price is closer to the ask price,
label trade a buy (1); else, if it is closer to the bid price, label it
a sell (-1); else, if trade's price is at the mid-quote, then if it is
higher than the last trade's price, classify it as a buy (1); else, if
it is less, classify it as a sell (-1); else, if it is the same,
classify it the same way as the previous trade was classified. If all
of these fail, classify the trade as unknown (0).

Python
class name:
LeeAndReady

Input: LEE_AND_READY expects two sources with names "TRD" and
"QTE". While ticks propagated by "TRD" source should have the PRICE,SIZE fields, ticks propagated by QTE
source should have the ASK_PRICE,ASK_SIZE,BID_PRICE,BID_SIZE
fields.
Tick type TRD+QTE can be used if
sources are streams of trades and quotes.

Output: A time series of TRD ticks, with the Lee and Ready
indicator (BuySellFlag) added.

Parameters:


  QUOTE_DELAY (seconds, represented as
double)
    The minimum number of seconds that needs to elapse between the
trade and the quote before the quote can be considered for a join with
the trade. The value is a double number. Only the first three digits of
the fraction are currently used, thus the highest supported granularity
of quote delay is milliseconds. Sub-millisecond parts of the trade's
and the quote's timestamps are ignored when computing delay between
them.

  
  SHOW_QUOTE_FIELDS (Boolean)
    If set to true, the quote fields that classified trade will also
be shown for each trade. Note that if there were no quotes before
trade, then quote fields will be set to 0.
Default: false

  

Examples: Outputs a stream with an added buy/sell indicator
field computed by using the Lee_and_Ready algorithm with a 5-second
delay:

LEE_AND_READY (5)

See the LEE_AND_READY example in TRANSFORMER_EXAMPLES.otq.


	"""
	class Parameters:
		quote_delay = "QUOTE_DELAY"
		show_quote_fields = "SHOW_QUOTE_FIELDS"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["quote_delay", "show_quote_fields"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["quote_delay", "_default_quote_delay", "show_quote_fields", "_default_show_quote_fields", "stack_info", "_used_strings"]

	def __init__(self, quote_delay=0, show_quote_fields=False):
		_graph_components.EpBase.__init__(self, "LEE_AND_READY")
		self._default_quote_delay = 0
		self.quote_delay = quote_delay
		self._default_show_quote_fields = False
		self.show_quote_fields = show_quote_fields
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_quote_delay(self, value):
		self.quote_delay = value
		return self

	def set_show_quote_fields(self, value):
		self.show_quote_fields = value
		return self

	@staticmethod
	def _get_name():
		return "LEE_AND_READY"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.quote_delay != 0: 
			desc += "QUOTE_DELAY=" + py_to_str(self.quote_delay) + ","
		if self.show_quote_fields != False: 
			desc += "SHOW_QUOTE_FIELDS=" + py_to_str(self.show_quote_fields) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class FormatTick(_graph_components.EpBase):
	"""
		

FORMAT_TICK

Type: Transformers

Description:

In case if MSG_FORMAT is set to one of CSV, JSON,
XML, PROTOBUF,
AVRO, SBE,
or FIX,
input ticks are transformed into their representations in the specified
format so, that further they can be fed directly into the WRITE_TO_KAFKA
EP. Transformed ticks have a single field with name FORMATTED_TICK, whose value is the
representation of the input tick in the specified format.

In case if MSG_FORMAT is set to one of PROTOBUF_SCHEMA, AVRO_SCHEMA, or SBE_SCHEMA,&nbsp;
a single output tick is produced. In case of Protobuf and Avro, this
output tick carries, in text format, the list of Protobuf or Avro
schemas, each item in that list corresponding to one of the input tick
schemas (descriptors, in OneTick terminology). In case of SBE_SCHEMA, this single output
tick&nbsp;will contain the Intermediate Representation(IR) of
the SBE schema. For all of PROTOBUF_SCHEMA,
AVRO_SCHEMA, and SBE_SCHEMA, the output tick has a single
field, called&nbsp;SCHEMA.
The contents of this field are identical to those one would have in the
Protobuf schema file, Avro schema file, or SBE IR file. Note that the
input tick descriptors are grouped together across all input symbols,
so that the output tick is produced for only one of them, regardless of
whether concurrency and/or batching options are specified or not.
Moreover, the input symbol for which the output tick is produced is
chosen in a non-deterministic way, although in most cases it will be
the last symbol. Concurrency and batching on the client side, however,
may still result in multiple output ticks to be produced.

If MSG_FORMAT is set to one of PROTOBUF, AVRO,
SBE, PROTOBUF_SCHEMA,
AVRO_SCHEMA, or SBE_SCHEMA, names for schemas
corresponding to input tick descriptors are picked up either from the TICK_TYPE
field, if any, or, if no such field is present, then from the tick type
bound to the EP. In the latter case, if the bound tick type is empty, FORMAT_TICK
will check if there is only one schema in the FORMAT_FILE.
If so, the transformation will be done based on it. Otherwise, an error
will be generated.

Python
class name:&nbsp;FormatTick

Input: A time series of ticks

Output: A time series of ticks or none

Parameters:


  MSG_FORMAT (enum)
    Specifies the output format: CSV,
    JSON, XML,
    PROTOBUF, AVRO, SBE,
    PROTOBUF_SCHEMA, AVRO_SCHEMA, or SBE_SCHEMA
In case of CSV, each tick is
converted into a sequence of values (FIELD_VALUE1,FIELD_VALUE2, ...).
In case of JSON, ticks are converted
to json strings ({"FIELD_NAME1":FIELD_VALUE1,"FIELD_NAME2":FIELD_VALUE2
...}).
In case of XML,
ticks are converted to xml strings (&lt;tick&gt;
&lt;FIELD_NAME1&gt;FIELD_VALUE1&lt;/FIELD_NAME1&gt;
&lt;FIELD_NAME2&gt;FIELD_VALUE2&lt;/FIELD_NAME2&gt; ... &lt;/tick&gt;).
In case of PROTOBUF, ticks are
converted to strings in google Protobuf format.
In case of AVRO, ticks are converted
to strings in Avro format.
In case of SBE, ticks are converted
to strings in SBE format.
In case of FIX, ticks are converted
to strings in FIX format.
Note, that for fields, representing OneTick nanosecond granularity
timestamps, only millisecond parts are converted to Avro long values
and sub-millisecond parts are truncated. In order to retain
sub-millisecond parts, nanosecond granularity timestamps must first be
converted to OneTick long values.

  
  FORMAT_FILE
    The file, containing schemas for the corresponding types
specified in the MSG_FORMAT. Required, if MSG_FORMAT is
set to PROTOBUF, AVRO, SBE,
or FIX. In case of SBE, the file
pointed to by this parameter should be in the Intermediate
Representation(IR) format. For FIX, this parameter should point to
the file one would specify for the FIX_PUBLISHER_FIELD_FID_MAP
configuration parameter (for more information about the parameter,
please refer here).
The value of this parameter is required to be empty, if MSG_FORMAT
is set to PROTOBUF_SCHEMA, AVRO_SCHEMA, or SBE_SCHEMA.
The set of directories specified in the parameter CSV_FILE_PATH in the
OneTick configuration file are also being searched for the file if the
value of this parameter is a relative path.

    When run on a tick server with TICK_SERVER_CSV_CACHE_DIR OneTick
configuration variable pointing to some local directory, FORMAT_TICK EP
will make an attempt to fetch the format file from the client host, in
case if the file is not found locally.

    Special value CONFLUENT_SCHEMA_REPOSITORY
must be specified if using the confluent schema registry as the source
of PROTOBUF or AVRO schemas, in this case Schema Registry
address is taken from CONFLUENT_SCHEMA_REGISTRY_ADDRESS
configuration variable.

  
  SCHEMA_ID
    Schema id of PROTOBUF or AVRO schemas stored in Confluent Schema
Registry(non-negative integer value).

  

More on SBE
OneTick SBE defines special composites which will represent
different tick fields(in particular, timestamps and varstrings) in SBE format. IR
generation will be performed using these composites as well. The
composites provided below must be used to successfully perform encoding:


  varString - represents TYPE_VARSTRING
type fields

&lt;composite name="varString" description="Variable-length string."&gt; 	&lt;type name="length" primitiveType="uint32" maxValue="1073741824"/&gt; 	&lt;type name="varData" primitiveType="uint8" length="0"/&gt;&lt;/composite&gt; 

  UTCTimestamp - can represent one of the TYPE_TIME_NSEC64, TYPE_TIME_MSEC64, TYPE_TIME32, TYPE_MSEC_DAY_OFFSET
type fields. To be able to indicate the timestamp precision, a special unit encoding will be added after the
actual timestamp value, which may have one of the valus specified in
the special enum TimeUnit.
Optionally, it is allowed to store OneTick timestamp values in raw UINT64 SBE type value, however, it is not
recommended. If such conversion is detected, SBE will log a warning
about that. 

&lt;enum name="TimeUnit" encodingType="uint8"&gt;	&lt;validValue name="second"&gt;0&lt;/validValue&gt;	&lt;validValue name="millisecond"&gt;3&lt;/validValue&gt;	&lt;validValue name="microsecond"&gt;6&lt;/validValue&gt;	&lt;validValue name="nanosecond"&gt;9&lt;/validValue&gt;&lt;/enum&gt;&lt;composite name="UTCTimestamp" description="UTC timestamp with precision on the wire"&gt;	&lt;type name="time" primitiveType="uint64" /&gt;	&lt;type name="unit" primitiveType="uint8" /&gt;&lt;/composite&gt;&lt;composite name="UTCTimestampSecs" description="UTC timestamp with precision on the wire"&gt;	&lt;type name="time" primitiveType="uint64" /&gt;	&lt;type name="unit" primitiveType="uint8" presence="constant" valueRef="TimeUnit.second" /&gt;&lt;/composite&gt;&lt;composite name="UTCTimestampMillis" description="UTC timestamp with precision on the wire"&gt;	&lt;type name="time" primitiveType="uint64" /&gt;	&lt;type name="unit" primitiveType="uint8" presence="constant" valueRef="TimeUnit.millisecond" /&gt;&lt;/composite&gt;&lt;composite name="UTCTimestampMicros" description="UTC timestamp with precision on the wire"&gt;	&lt;type name="time" primitiveType="uint64" /&gt;	&lt;type name="unit" primitiveType="uint8" presence="constant" valueRef="TimeUnit.microsecond" /&gt;&lt;/composite&gt;&lt;composite name="UTCTimestampNanos" description="UTC timestamp with precision on the wire"&gt;	&lt;type name="time" primitiveType="uint64" /&gt;	&lt;type name="unit" primitiveType="uint8" presence="constant" valueRef="TimeUnit.nanosecond" /&gt;&lt;/composite&gt; 
More on FIX
To understand how the NaN and empty values are being
processed in the OneTick FIX, please refer to OneTick Fix
Publisher documentation.


	"""
	class Parameters:
		msg_format = "MSG_FORMAT"
		format_file = "FORMAT_FILE"
		schema_id = "SCHEMA_ID"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["msg_format", "format_file", "schema_id"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["msg_format", "_default_msg_format", "format_file", "_default_format_file", "schema_id", "_default_schema_id", "stack_info", "_used_strings"]

	class MsgFormat:
		AVRO = "AVRO"
		AVRO_SCHEMA = "AVRO_SCHEMA"
		CSV = "CSV"
		FIX = "FIX"
		JSON = "JSON"
		PROTOBUF = "PROTOBUF"
		PROTOBUF_SCHEMA = "PROTOBUF_SCHEMA"
		SBE = "SBE"
		SBE_SCHEMA = "SBE_SCHEMA"
		XML = "XML"

	def __init__(self, msg_format=MsgFormat.CSV, format_file="", schema_id=""):
		_graph_components.EpBase.__init__(self, "FORMAT_TICK")
		self._default_msg_format = type(self).MsgFormat.CSV
		self.msg_format = msg_format
		self._default_format_file = ""
		self.format_file = format_file
		self._default_schema_id = ""
		self.schema_id = schema_id
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_msg_format(self, value):
		self.msg_format = value
		return self

	def set_format_file(self, value):
		self.format_file = value
		return self

	def set_schema_id(self, value):
		self.schema_id = value
		return self

	@staticmethod
	def _get_name():
		return "FORMAT_TICK"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.msg_format != self.MsgFormat.CSV: 
			desc += "MSG_FORMAT=" + py_to_str(self.msg_format) + ","
		if self.format_file != "": 
			desc += "FORMAT_FILE=" + py_to_str(self.format_file) + ","
		if self.schema_id != "": 
			desc += "SCHEMA_ID=" + py_to_str(self.schema_id) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class Uptick(_graph_components.EpBase):
	"""
		

UPTICK

Type: Filter

Description: Propagates ticks based on its price being
strictly greater than that of the previous tick.

Python
class name:&nbsp;Uptick

Input: A time series of ticks containing a field named PRICE.

Output: A time series of ticks.

Parameters: See parameters common
to many filters.


  DISCARD_ON_MATCH
(Boolean)

Examples:

UPTICK (false)

See the UPTICK example in FILTER_EXAMPLES.otq.


	"""
	class Parameters:
		discard_on_match = "DISCARD_ON_MATCH"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["discard_on_match"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["discard_on_match", "_default_discard_on_match", "stack_info", "_used_strings"]

	def __init__(self, discard_on_match=False):
		_graph_components.EpBase.__init__(self, "UPTICK")
		self._default_discard_on_match = False
		self.discard_on_match = discard_on_match
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_discard_on_match(self, value):
		self.discard_on_match = value
		return self

	@staticmethod
	def _get_name():
		return "UPTICK"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.discard_on_match != False: 
			desc += "DISCARD_ON_MATCH=" + py_to_str(self.discard_on_match) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class ValueCompare(_graph_components.EpBase):
	"""
		

VALUE_COMPARE

Type: Filter

Description: Propagates ticks based on whether the value in
the field specified by FIELD is in the specified relationship
to the value specified by the parameter VALUE.

Python
class name:&nbsp;ValueCompare

Input: A time series of ticks.

Output: A time series of ticks.

Parameters: See parameters common
to many filters.


  DISCARD_ON_MATCH
(Boolean)
  FIELD
(string)
  VALUE (numeric or string)
  RELATIONSHIP_TO_CONST
(LT|LE|EQ|NE|GE|GT)
    Where:

    
      LT denotes "is less than"
      LE denotes "is less than or equal to"
      EQ denotes "is equal to"
      NE denotes "is not equal to"
      GE denotes "is greater than or equal to"
      GT denotes "is greater than"
    
Default: GE

Examples:

VALUE_COMPARE (EXCHANGE, N, EQ, false)

See the VALUE_COMPARE example in FILTER_EXAMPLES.otq.


	"""
	class Parameters:
		discard_on_match = "DISCARD_ON_MATCH"
		field = "FIELD"
		value = "VALUE"
		relationship_to_const = "RELATIONSHIP_TO_CONST"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["discard_on_match", "field", "value", "relationship_to_const"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["discard_on_match", "_default_discard_on_match", "field", "_default_field", "value", "_default_value", "relationship_to_const", "_default_relationship_to_const", "stack_info", "_used_strings"]

	class RelationshipToConst:
		EQ = "EQ"
		GE = "GE"
		GT = "GT"
		LE = "LE"
		LT = "LT"
		NE = "NE"

	def __init__(self, discard_on_match=False, field="", value="", relationship_to_const=RelationshipToConst.GE):
		_graph_components.EpBase.__init__(self, "VALUE_COMPARE")
		self._default_discard_on_match = False
		self.discard_on_match = discard_on_match
		self._default_field = ""
		self.field = field
		self._default_value = ""
		self.value = value
		self._default_relationship_to_const = type(self).RelationshipToConst.GE
		self.relationship_to_const = relationship_to_const
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_discard_on_match(self, value):
		self.discard_on_match = value
		return self

	def set_field(self, value):
		self.field = value
		return self

	def set_value(self, value):
		self.value = value
		return self

	def set_relationship_to_const(self, value):
		self.relationship_to_const = value
		return self

	@staticmethod
	def _get_name():
		return "VALUE_COMPARE"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.discard_on_match != False: 
			desc += "DISCARD_ON_MATCH=" + py_to_str(self.discard_on_match) + ","
		if self.field != "": 
			desc += "FIELD=" + py_to_str(self.field) + ","
		if self.value != "": 
			desc += "VALUE=" + py_to_str(self.value) + ","
		if self.relationship_to_const != self.RelationshipToConst.GE: 
			desc += "RELATIONSHIP_TO_CONST=" + py_to_str(self.relationship_to_const) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class RegexMatches(_graph_components.EpBase):
	"""
		

REGEX_MATCHES

Type: Filter

Description: Propagates ticks based on whether the value in
the field specified by FIELD matches the regular
expression specified by PATTERN.

Python
class name:&nbsp;RegexMatches

Input: A time series of ticks.

Output: A time series of ticks.

Parameters: See parameters common
to many filters.


  DISCARD_ON_MATCH
(Boolean)
  FIELD
(string)
  PATTERN (string)
    Pattern to match, expected to be a Perl 5 regular expression.

  

Examples:

REGEX_MATCHES(DISCARD_ON_MATCH=true,FIELD=SIZE,PATTERN="0.*0")

See the REGEX_MATCHES example in FILTER_EXAMPLES.otq.


	"""
	class Parameters:
		discard_on_match = "DISCARD_ON_MATCH"
		field = "FIELD"
		pattern = "PATTERN"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["discard_on_match", "field", "pattern"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["discard_on_match", "_default_discard_on_match", "field", "_default_field", "pattern", "_default_pattern", "stack_info", "_used_strings"]

	def __init__(self, discard_on_match=False, field="", pattern=""):
		_graph_components.EpBase.__init__(self, "REGEX_MATCHES")
		self._default_discard_on_match = False
		self.discard_on_match = discard_on_match
		self._default_field = ""
		self.field = field
		self._default_pattern = ""
		self.pattern = pattern
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_discard_on_match(self, value):
		self.discard_on_match = value
		return self

	def set_field(self, value):
		self.field = value
		return self

	def set_pattern(self, value):
		self.pattern = value
		return self

	@staticmethod
	def _get_name():
		return "REGEX_MATCHES"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.discard_on_match != False: 
			desc += "DISCARD_ON_MATCH=" + py_to_str(self.discard_on_match) + ","
		if self.field != "": 
			desc += "FIELD=" + py_to_str(self.field) + ","
		if self.pattern != "": 
			desc += "PATTERN=" + py_to_str(self.pattern) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class TimeFilter(_graph_components.EpBase):
	"""
		

TIME_FILTER

Type: Filter

Description: Propagates ticks if their timestamp falls within
a range between the START_TIME and END_TIME parameters
of the filter, for a specified TIMEZONE parameter.


Python
class name:&nbsp;TimeFilter

Input: A time series of ticks.

Output:A time series of ticks.

Parameters: See the parameters
common to many filters section.


  DISCARD_ON_MATCH
(Boolean)
  START_TIME (HHMMSSmmm or HH:MM:SS[.mmm])
Default: 0
  END_TIME (HHMMSSmmm or HH:MM:SS[.mmm])
    To filter ticks for an entire day, this parameter should be set
to 24:00:00.000.
Default: 0

  
  TIMEZONE
    Time zone of START_TIME and END_TIME.
Default: GMT

  
  DAY_PATTERNS (string)
    A comma-separated list of patterns that determines days for
which the ticks can be propagated.

    A tick can be propagated if its date matches one or more of the
patterns. Three supported pattern formats are:

    
      Format1: month.week.weekdays, 0 month means any month,
0
week means any week, 6 week means the last week of the month for a
given weekday(s), weekdays are digits for each day, 0 being Sunday.
      Format2: month/day, 0 month means any month.
      Format3: year/month/day, 0 year means any year, 0
month means any month.
    
    The query get_patterns
(CSV_FILE_LISTING) is used to read files containing DAY_PATTERNS
and substitutes the parameters (see Event Processor Parameters).

    The parameter DAY_PATTERNS can become long if particular dates
for which ticks should be filtered/propagated are listed.
In this case, it is recommended to save the dates in an ASCII file and
substitute the DAY_PATTERN parameter by eval,
which will call a separate query to read a file:

    DAY_PATTERNS = eval("THIS::get_pattern").PATTERN

    which implies that the current OTQ contains a query get_pattern that returns a single field, PATTERN, that contains all holiday
patterns.
You can also use ?THIS_DIR? to refer to queries in other OTQ files in the same directory. 


    Some practical examples include the following:

    
      Every May 1 each year:
        5/1

      
      Every first Monday of July:
        7.1.1

      
      Every last Thursday of November:
        11.6.4

      
      All workdays (ignoring holidays)
        0.0.1,0.0.2,0.0.3,0.0.4,0.0.5

      
    
  


  END_TIME_TICK_MATCHES (Boolean)
    If set to true, a tick with timestamp hour/minute/second of
which matches the value of END_TIME
is considered to belong to the time interval between START_TIME and
END_TIME.&nbsp;
Default: false

  



Note: If START_TIME is greater than END_TIME,
then END_TIME is considered to be the time of the next day.

Examples: Only keep ticks that arrived between 10 am and 2 pm
New York time:

TIME_FILTER (100000000,140000000,EST5EDT,false)

To eliminate weekends from a data range, set DAY_PATTERNS to 0.0.06
and DISCARD_ON_MATCH to "true."

TIME_FILTER (true,0,240000000,,"0.0.06")

See time_filter_holidays.otq
and the TIME_FILTER example in FILTER_EXAMPLES.otq.

 
Also see the corresponding MATCHES_TIME_FILTER built-in function

	"""
	class Parameters:
		discard_on_match = "DISCARD_ON_MATCH"
		start_time = "START_TIME"
		end_time = "END_TIME"
		timezone = "TIMEZONE"
		day_patterns = "DAY_PATTERNS"
		end_time_tick_matches = "END_TIME_TICK_MATCHES"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["discard_on_match", "start_time", "end_time", "timezone", "day_patterns", "end_time_tick_matches"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["discard_on_match", "_default_discard_on_match", "start_time", "_default_start_time", "end_time", "_default_end_time", "timezone", "_default_timezone", "day_patterns", "_default_day_patterns", "end_time_tick_matches", "_default_end_time_tick_matches", "stack_info", "_used_strings"]

	def __init__(self, discard_on_match=False, start_time="", end_time="", timezone="", day_patterns="", end_time_tick_matches=False):
		_graph_components.EpBase.__init__(self, "TIME_FILTER")
		self._default_discard_on_match = False
		self.discard_on_match = discard_on_match
		self._default_start_time = ""
		self.start_time = start_time
		self._default_end_time = ""
		self.end_time = end_time
		self._default_timezone = ""
		self.timezone = timezone
		self._default_day_patterns = ""
		self.day_patterns = day_patterns
		self._default_end_time_tick_matches = False
		self.end_time_tick_matches = end_time_tick_matches
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_discard_on_match(self, value):
		self.discard_on_match = value
		return self

	def set_start_time(self, value):
		self.start_time = value
		return self

	def set_end_time(self, value):
		self.end_time = value
		return self

	def set_timezone(self, value):
		self.timezone = value
		return self

	def set_day_patterns(self, value):
		self.day_patterns = value
		return self

	def set_end_time_tick_matches(self, value):
		self.end_time_tick_matches = value
		return self

	@staticmethod
	def _get_name():
		return "TIME_FILTER"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.discard_on_match != False: 
			desc += "DISCARD_ON_MATCH=" + py_to_str(self.discard_on_match) + ","
		if self.start_time != "": 
			desc += "START_TIME=" + py_to_str(self.start_time) + ","
		if self.end_time != "": 
			desc += "END_TIME=" + py_to_str(self.end_time) + ","
		if self.timezone != "": 
			desc += "TIMEZONE=" + py_to_str(self.timezone) + ","
		if self.day_patterns != "": 
			desc += "DAY_PATTERNS=" + py_to_str(self.day_patterns) + ","
		if self.end_time_tick_matches != False: 
			desc += "END_TIME_TICK_MATCHES=" + py_to_str(self.end_time_tick_matches) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class CharacterPresent(_graph_components.EpBase):
	"""
		

CHARACTER_PRESENT

Type: Filter

Description: Propagates ticks based on whether the value of
the field specified by FIELD contains a character in the set of
characters specified by CHARACTERS.

Python
class name:&nbsp;CharacterPresent

Input: A time series of ticks

Output: A filtered time series of ticks

Parameters: See parameters common
to many filters.


  FIELD
(mandatory, string)
    The name of the field to evaluate

  
  CHARACTERS (list of characters)
    A set of characters that are searched for in the value of the
FIELD

  
  CHARACTERS_FIELD (field name from
where need to take characters)
     If specified, will take a current value of that field and
append it to CHARACTERS, if any 

  
  DISCARD_ON_MATCH
(Boolean, optional, the default is false)
    Reverses the filtering results by excluding the ticks that have
any of the CHARACTERs in their FIELD.

  

Examples:

CHARACTER_PRESENT(FIELD=EXCHANGE,CHARACTERS=BT,
CHARACTERS_FIELD=OLD_EXCHANGE)

Only keeps ticks that have the B,T or current character set in
OLD_EXCHANGE field in their EXCHANGE field.

See CHARACTER_PRESENT in FILTER_EXAMPLES.otq.

 Appropriate for this EP, there is also a built-in function IS_CHARACTER_PRESENT.


	"""
	class Parameters:
		discard_on_match = "DISCARD_ON_MATCH"
		field = "FIELD"
		characters = "CHARACTERS"
		characters_field = "CHARACTERS_FIELD"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["discard_on_match", "field", "characters", "characters_field"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["discard_on_match", "_default_discard_on_match", "field", "_default_field", "characters", "_default_characters", "characters_field", "_default_characters_field", "stack_info", "_used_strings"]

	def __init__(self, discard_on_match=False, field="", characters="", characters_field=""):
		_graph_components.EpBase.__init__(self, "CHARACTER_PRESENT")
		self._default_discard_on_match = False
		self.discard_on_match = discard_on_match
		self._default_field = ""
		self.field = field
		self._default_characters = ""
		self.characters = characters
		self._default_characters_field = ""
		self.characters_field = characters_field
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_discard_on_match(self, value):
		self.discard_on_match = value
		return self

	def set_field(self, value):
		self.field = value
		return self

	def set_characters(self, value):
		self.characters = value
		return self

	def set_characters_field(self, value):
		self.characters_field = value
		return self

	@staticmethod
	def _get_name():
		return "CHARACTER_PRESENT"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.discard_on_match != False: 
			desc += "DISCARD_ON_MATCH=" + py_to_str(self.discard_on_match) + ","
		if self.field != "": 
			desc += "FIELD=" + py_to_str(self.field) + ","
		if self.characters != "": 
			desc += "CHARACTERS=" + py_to_str(self.characters) + ","
		if self.characters_field != "": 
			desc += "CHARACTERS_FIELD=" + py_to_str(self.characters_field) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class BytePresent(_graph_components.EpBase):
	"""
		

BYTE_PRESENT

Type: Filter

Description: Propagates ticks based on whether the value of
the field specified by FIELD contains a byte in
the set of bytes specified by BYTES.

Python
class name:&nbsp;BytePresent

Input: A time series of ticks

Output: A filtered time series of ticks

Parameters: See parameters common
to many filters.


  FIELD
(string)
  BYTES (comma-separated list of BYTES, both
hexadecimal and decimal formats supported)
    A set of bytes against which the field of the ticks will be
compared. Note that invalid byte values are converted to zero.
Default: empty set

  
  DISCARD_ON_MATCH
(Boolean)

Examples:

BYTE_PRESENT (MMID, "0x87,17", false)

Keeps Only the ticks that have bytes 0x87
or 17 in their MMID field.


	"""
	class Parameters:
		discard_on_match = "DISCARD_ON_MATCH"
		field = "FIELD"
		bytes = "BYTES"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["discard_on_match", "field", "bytes"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["discard_on_match", "_default_discard_on_match", "field", "_default_field", "bytes", "_default_bytes", "stack_info", "_used_strings"]

	def __init__(self, discard_on_match=False, field="", bytes=""):
		_graph_components.EpBase.__init__(self, "BYTE_PRESENT")
		self._default_discard_on_match = False
		self.discard_on_match = discard_on_match
		self._default_field = ""
		self.field = field
		self._default_bytes = ""
		self.bytes = bytes
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_discard_on_match(self, value):
		self.discard_on_match = value
		return self

	def set_field(self, value):
		self.field = value
		return self

	def set_bytes(self, value):
		self.bytes = value
		return self

	@staticmethod
	def _get_name():
		return "BYTE_PRESENT"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.discard_on_match != False: 
			desc += "DISCARD_ON_MATCH=" + py_to_str(self.discard_on_match) + ","
		if self.field != "": 
			desc += "FIELD=" + py_to_str(self.field) + ","
		if self.bytes != "": 
			desc += "BYTES=" + py_to_str(self.bytes) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class ValuePresent(_graph_components.EpBase):
	"""
		

VALUE_PRESENT

Type: Filter

Description: Propagates ticks based on whether the value in
the attribute specified by FIELD is in the comma-separated set
of values specified by the parameter VALUES.

Python
class name:&nbsp;ValuePresent

Input: A time series of ticks.

Output: A time series of ticks.

Parameters: See section parameters
common to many filters.


  DISCARD_ON_MATCH
(Boolean)
  FIELD
(string)
  VALUES (numeric or string)

Examples:

Propagate only ticks where EXCHANGE is equal to N or
B.

VALUE_PRESENT (EXCHANGE, "N,B", false)

See the VALUE_PRESENT example in FILTER_EXAMPLES.otq.


	"""
	class Parameters:
		discard_on_match = "DISCARD_ON_MATCH"
		field = "FIELD"
		values = "VALUES"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["discard_on_match", "field", "values"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["discard_on_match", "_default_discard_on_match", "field", "_default_field", "values", "_default_values", "stack_info", "_used_strings"]

	def __init__(self, discard_on_match=False, field="", values=""):
		_graph_components.EpBase.__init__(self, "VALUE_PRESENT")
		self._default_discard_on_match = False
		self.discard_on_match = discard_on_match
		self._default_field = ""
		self.field = field
		self._default_values = ""
		self.values = values
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_discard_on_match(self, value):
		self.discard_on_match = value
		return self

	def set_field(self, value):
		self.field = value
		return self

	def set_values(self, value):
		self.values = value
		return self

	@staticmethod
	def _get_name():
		return "VALUE_PRESENT"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.discard_on_match != False: 
			desc += "DISCARD_ON_MATCH=" + py_to_str(self.discard_on_match) + ","
		if self.field != "": 
			desc += "FIELD=" + py_to_str(self.field) + ","
		if self.values != "": 
			desc += "VALUES=" + py_to_str(self.values) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class OmBacktestingManager(_graph_components.EpBase):
	"""
		

OM/BACKTESTING_MANAGER

Type: Other

Description: Controls back-testing processes.

Python
class name:&nbsp;OmBacktestingManager

Input: None.

Output: Depends on use case.

Parameters:


  ACTION (string)
    The action to be performed. Supported values are:

    
      RUN_STRATEGY for running
strategies in backtesting mode (EP outputs run name)
      COMPUTE_STATISTICS for
calculating statistics on strategy runs (EP outputs statistics ticks)
      PARAMETER_OPTIMIZATION for
searching 'best' parameter combination using user-provided algorithm
(EP outputs best sets of parameters with statistics)
    
  
  STRATEGY_QUERY (string)
    The path to the strategy query

  
  STRATEGY_QUERY_PARAMS (string)
    The OTQ parameters for the strategy query

  
  SECONDARY_STRATEGY_QUERIES
(string)
    A comma-separated list of execution(child) queries, each
represented as &lt; execution strategy path
&gt;(&lt; execution strategy otq parameters &gt;)

  
  COMPUTE_STATISTICS_BY (string)
    The mode of statistical computation. Supported values are:

    
      BY_STRATEGY_SYMBOL,
statistics is calculated by strategy symbol (which has a form
&lt;strategy input symbol&amp;gt_&lt;strategy instance id&gt;). An
instance of the query is created for each strategy symbol of each
strategy instance.
      BY_ORDER_SYMBOL, statistics
is calculated by order symbol (symbol for which order was submitted).
An instance of the query is created for each order symbol of each
strategy instance. OM/RETRIEVE_ORDER_DATA
EP should be used to access order data (TRADE_UPDs/ORDER_UPDs) with
appropriate bound tick type.
      BY_STRATEGY_INSTANCE,
statistics is calculated by strategy instance. An instance of the query
is created for each strategy instance with input symbols set to
strategy symbols of the strategy instance.
    
    Default: BY_BACKTESTING_SYMBOL

  
  STATISTICS_QUERY (string)
    The path to the statistics computer query

  
  STATISTICS_QUERY_PARAMS
(string)
    The OTQ parameters for the statistics computer query

  
  PARAMETER_RANGES (string)
    Semicolon-separated parameters, each represented in the
following format:

    
      &lt;parameter name&gt;, &lt;parameter type&gt;, &lt;property
1&gt;&hellip;&lt;property N&gt;
      parameter type INT: &lt;parameter name&gt;,INT, &lt;initial
value&gt;, &lt;final value&gt;, &lt;step&gt;
      parameter type DOUBLE: &lt;parameter name&gt;,DOUBLE,
&lt;initial value&gt;, &lt;final value&gt;, &lt;step&gt;
      parameter type FIXED: &lt;parameter name&gt;,FIXED,
&lt;value&gt;
      parameter type LIST: &lt;parameter name&gt;,LIST, &lt;value
1&gt;,&hellip;&lt;value N&gt;
    
  
  BACKTESTING_PARENT_DB (string)
    The parent database for all databases that store the backtesting
data

  
  RUN_NAME (string)
    Name of the back-testing strategy run; given by the user by
specifying this parameter when starting a new run, or automatically
generated by the EP if not specified. Is used to identify back-testing
run when calculating statistics (ACTION=COMPUTE_STATISTICS).

  
  SIMULATOR_PATH (string)
    Exchange simulator shared library path. OneTick distribution
comes with sample simulator shared library, bin/backtesting_exchange_simulator.&lt;platform-specific
shared library extension&gt;, code for it present in examples/cplusplus/backtesting_exchange_simulator.cpp

  
  SIMULATOR_PARAMETERS (string)
    A comma-separated list of &lt;param_name&gt;=&lt;value&gt;
pairs passed to the exchange simulator.
Default: EMPTY

  
  STATISTICS_DB (string)
    Database "derived" from the database specified in the BACKTESTING_PARENT_DB parameter, used to
store backtesting statistics, which is created when run is performed
with ACTION=COMPUTE_STATISTICS.
Default: statistics

  
  OPTIMIZER (string)
    The path to the shared library, containing the parameter
combination search algorithm, which is used when ACTION=PARAMETER_OPTIMIZATION.&nbsp;OneTick
distribution comes with sample optimizer shared library, bin/optimizer_sample.&lt;platform-specific
shared library extension&gt;, code for it present in examples/cplusplus/basic_optimizer.cpp

  
  OPTIMIZER_PARAMS (string)
    Parameters passed to the parameter combination search algorithm
(specialized via the OPTIMIZER
parameter) in the following format:

    &lt;first parameter name&gt;=&lt;first parameter
value&gt;,&lt;second parameter name&gt;=&lt;second parameter
value&gt;...

  
  ENABLE_MARKET_IMPACT (boolean)
    Turns on the market impact, that is the secondary strategies can
alter the market data of the main strategy via reporting back market
data ticks starting from the last child(execution) query.

  

When ACTION=PARAMETER_OPTIMIZATION
is set and the strategy is running for a group of symbols, where
statistics for the symbols are being aggregated; the user can apply a
weight coefficient to each symbol by defining the WEIGHT symbol parameter.

Examples:


OM/BACKTESTING_MANAGER(PARAMETER_OPTIMIZATION,"THIS::strategy",,,"THIS::statistics",,"ID,INT,1,2,1;NAME,LIST,A,B","FIN",,"simulator_sample",,,"optimizer","FIELD_TO_OPTIMIZE=PROFIT",)
OM/BACKTESTING_MANAGER(PARAMETER_OPTIMIZATION,"?THIS_DIR?strategy.otq::strategy",,,"?THIS_DIR?statistics.otq::statistics",,"ID,INT,1,2,1;NAME,LIST,A,B","FIN",,"simulator_sample",,,"optimizer","FIELD_TO_OPTIMIZE=PROFIT",)

Run the parameter optimization algorithm for the strategy in the
current OTQ file named strategy with
the statistics calculator OTQ in this file named statistics using the optimization
algorithm optimizer with the
algorithm parameter FIELD_TO_OPTIMIZE=PROFIT
for the strategy parameter sets: ID=1,
NAME=A, ID=2, NAME=A, ID=1, NAME=B, ID=2,
NAME=B.




	"""
	class Parameters:
		action = "ACTION"
		strategy_query = "STRATEGY_QUERY"
		strategy_query_params = "STRATEGY_QUERY_PARAMS"
		secondary_strategy_queries = "SECONDARY_STRATEGY_QUERIES"
		compute_statistics_by = "COMPUTE_STATISTICS_BY"
		statistics_query = "STATISTICS_QUERY"
		statistics_query_params = "STATISTICS_QUERY_PARAMS"
		parameter_ranges = "PARAMETER_RANGES"
		backtesting_parent_db = "BACKTESTING_PARENT_DB"
		run_name = "RUN_NAME"
		simulator_path = "SIMULATOR_PATH"
		simulator_params = "SIMULATOR_PARAMS"
		statistics_db = "STATISTICS_DB"
		optimizer = "OPTIMIZER"
		optimizer_params = "OPTIMIZER_PARAMS"
		enable_market_impact = "ENABLE_MARKET_IMPACT"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["action", "strategy_query", "strategy_query_params", "secondary_strategy_queries", "compute_statistics_by", "statistics_query", "statistics_query_params", "parameter_ranges", "backtesting_parent_db", "run_name", "simulator_path", "simulator_params", "statistics_db", "optimizer", "optimizer_params", "enable_market_impact"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["action", "_default_action", "strategy_query", "_default_strategy_query", "strategy_query_params", "_default_strategy_query_params", "secondary_strategy_queries", "_default_secondary_strategy_queries", "compute_statistics_by", "_default_compute_statistics_by", "statistics_query", "_default_statistics_query", "statistics_query_params", "_default_statistics_query_params", "parameter_ranges", "_default_parameter_ranges", "backtesting_parent_db", "_default_backtesting_parent_db", "run_name", "_default_run_name", "simulator_path", "_default_simulator_path", "simulator_params", "_default_simulator_params", "statistics_db", "_default_statistics_db", "optimizer", "_default_optimizer", "optimizer_params", "_default_optimizer_params", "enable_market_impact", "_default_enable_market_impact", "stack_info", "_used_strings"]

	class Action:
		EMPTY = ""
		COMPUTE_STATISTICS = "COMPUTE_STATISTICS"
		PARAMETER_OPTIMIZATION = "PARAMETER_OPTIMIZATION"
		RUN_STRATEGY = "RUN_STRATEGY"

	class ComputeStatisticsBy:
		BY_ORDER_SYMBOL = "BY_ORDER_SYMBOL"
		BY_STRATEGY_INSTANCE = "BY_STRATEGY_INSTANCE"
		BY_STRATEGY_SYMBOL = "BY_STRATEGY_SYMBOL"

	def __init__(self, action=Action.EMPTY, strategy_query="", strategy_query_params="", secondary_strategy_queries="", compute_statistics_by=ComputeStatisticsBy.BY_STRATEGY_SYMBOL, statistics_query="", statistics_query_params="", parameter_ranges="", backtesting_parent_db="", run_name="", simulator_path="", simulator_params="", statistics_db="", optimizer="", optimizer_params="", enable_market_impact=False):
		_graph_components.EpBase.__init__(self, "OM/BACKTESTING_MANAGER")
		self._default_action = type(self).Action.EMPTY
		self.action = action
		self._default_strategy_query = ""
		self.strategy_query = strategy_query
		self._default_strategy_query_params = ""
		self.strategy_query_params = strategy_query_params
		self._default_secondary_strategy_queries = ""
		self.secondary_strategy_queries = secondary_strategy_queries
		self._default_compute_statistics_by = type(self).ComputeStatisticsBy.BY_STRATEGY_SYMBOL
		self.compute_statistics_by = compute_statistics_by
		self._default_statistics_query = ""
		self.statistics_query = statistics_query
		self._default_statistics_query_params = ""
		self.statistics_query_params = statistics_query_params
		self._default_parameter_ranges = ""
		self.parameter_ranges = parameter_ranges
		self._default_backtesting_parent_db = ""
		self.backtesting_parent_db = backtesting_parent_db
		self._default_run_name = ""
		self.run_name = run_name
		self._default_simulator_path = ""
		self.simulator_path = simulator_path
		self._default_simulator_params = ""
		self.simulator_params = simulator_params
		self._default_statistics_db = ""
		self.statistics_db = statistics_db
		self._default_optimizer = ""
		self.optimizer = optimizer
		self._default_optimizer_params = ""
		self.optimizer_params = optimizer_params
		self._default_enable_market_impact = False
		self.enable_market_impact = enable_market_impact
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_action(self, value):
		self.action = value
		return self

	def set_strategy_query(self, value):
		self.strategy_query = value
		return self

	def set_strategy_query_params(self, value):
		self.strategy_query_params = value
		return self

	def set_secondary_strategy_queries(self, value):
		self.secondary_strategy_queries = value
		return self

	def set_compute_statistics_by(self, value):
		self.compute_statistics_by = value
		return self

	def set_statistics_query(self, value):
		self.statistics_query = value
		return self

	def set_statistics_query_params(self, value):
		self.statistics_query_params = value
		return self

	def set_parameter_ranges(self, value):
		self.parameter_ranges = value
		return self

	def set_backtesting_parent_db(self, value):
		self.backtesting_parent_db = value
		return self

	def set_run_name(self, value):
		self.run_name = value
		return self

	def set_simulator_path(self, value):
		self.simulator_path = value
		return self

	def set_simulator_params(self, value):
		self.simulator_params = value
		return self

	def set_statistics_db(self, value):
		self.statistics_db = value
		return self

	def set_optimizer(self, value):
		self.optimizer = value
		return self

	def set_optimizer_params(self, value):
		self.optimizer_params = value
		return self

	def set_enable_market_impact(self, value):
		self.enable_market_impact = value
		return self

	@staticmethod
	def _get_name():
		return "OM/BACKTESTING_MANAGER"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.action != self.Action.EMPTY: 
			desc += "ACTION=" + py_to_str(self.action) + ","
		if self.strategy_query != "": 
			desc += "STRATEGY_QUERY=" + py_to_str(self.strategy_query) + ","
		if self.strategy_query_params != "": 
			desc += "STRATEGY_QUERY_PARAMS=" + py_to_str(self.strategy_query_params) + ","
		if self.secondary_strategy_queries != "": 
			desc += "SECONDARY_STRATEGY_QUERIES=" + py_to_str(self.secondary_strategy_queries) + ","
		if self.compute_statistics_by != self.ComputeStatisticsBy.BY_STRATEGY_SYMBOL: 
			desc += "COMPUTE_STATISTICS_BY=" + py_to_str(self.compute_statistics_by) + ","
		if self.statistics_query != "": 
			desc += "STATISTICS_QUERY=" + py_to_str(self.statistics_query) + ","
		if self.statistics_query_params != "": 
			desc += "STATISTICS_QUERY_PARAMS=" + py_to_str(self.statistics_query_params) + ","
		if self.parameter_ranges != "": 
			desc += "PARAMETER_RANGES=" + py_to_str(self.parameter_ranges) + ","
		if self.backtesting_parent_db != "": 
			desc += "BACKTESTING_PARENT_DB=" + py_to_str(self.backtesting_parent_db) + ","
		if self.run_name != "": 
			desc += "RUN_NAME=" + py_to_str(self.run_name) + ","
		if self.simulator_path != "": 
			desc += "SIMULATOR_PATH=" + py_to_str(self.simulator_path) + ","
		if self.simulator_params != "": 
			desc += "SIMULATOR_PARAMS=" + py_to_str(self.simulator_params) + ","
		if self.statistics_db != "": 
			desc += "STATISTICS_DB=" + py_to_str(self.statistics_db) + ","
		if self.optimizer != "": 
			desc += "OPTIMIZER=" + py_to_str(self.optimizer) + ","
		if self.optimizer_params != "": 
			desc += "OPTIMIZER_PARAMS=" + py_to_str(self.optimizer_params) + ","
		if self.enable_market_impact != False: 
			desc += "ENABLE_MARKET_IMPACT=" + py_to_str(self.enable_market_impact) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class VolumeLimit(_graph_components.EpBase):
	"""
		

VOLUME_LIMIT

Type: Filter

Description: Propagates ticks based on the comparison of
their total trading volume to the value specified by the parameter MAX_VOLUME.

Python
class name:&nbsp;VolumeLimit

Input: A time series of ticks containing a field named SIZE.

Output: A time series of ticks.

Parameters: See parameters common
to many filters.


  DISCARD_ON_MATCH
(Boolean)
  MAX_VOLUME (numeric)
  TICK_HANDLING_AT_CROSSED_BOUNDARY
(enum)
    The possible values are DISCARD and PROPAGATE. If the handling
mode is PROPAGATE, tick, for which total trading volume will become
larger than MAX_VOLUME, will be propagated,otherwise it will not.
Default: DISCARD

  

Examples: Filter out ticks after the cumulative volume
exceeds 1 million shares.

VOLUME_LIMIT (1000000, false)

See the VOLUME_LIMIT example in FILTER_EXAMPLES.otq.


	"""
	class Parameters:
		discard_on_match = "DISCARD_ON_MATCH"
		max_volume = "MAX_VOLUME"
		tick_handling_at_crossed_boundary = "TICK_HANDLING_AT_CROSSED_BOUNDARY"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["discard_on_match", "max_volume", "tick_handling_at_crossed_boundary"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["discard_on_match", "_default_discard_on_match", "max_volume", "_default_max_volume", "tick_handling_at_crossed_boundary", "_default_tick_handling_at_crossed_boundary", "stack_info", "_used_strings"]

	class TickHandlingAtCrossedBoundary:
		DISCARD = "DISCARD"
		PROPAGATE = "PROPAGATE"

	def __init__(self, discard_on_match=False, max_volume="", tick_handling_at_crossed_boundary=TickHandlingAtCrossedBoundary.DISCARD):
		_graph_components.EpBase.__init__(self, "VOLUME_LIMIT")
		self._default_discard_on_match = False
		self.discard_on_match = discard_on_match
		self._default_max_volume = ""
		self.max_volume = max_volume
		self._default_tick_handling_at_crossed_boundary = type(self).TickHandlingAtCrossedBoundary.DISCARD
		self.tick_handling_at_crossed_boundary = tick_handling_at_crossed_boundary
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_discard_on_match(self, value):
		self.discard_on_match = value
		return self

	def set_max_volume(self, value):
		self.max_volume = value
		return self

	def set_tick_handling_at_crossed_boundary(self, value):
		self.tick_handling_at_crossed_boundary = value
		return self

	@staticmethod
	def _get_name():
		return "VOLUME_LIMIT"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.discard_on_match != False: 
			desc += "DISCARD_ON_MATCH=" + py_to_str(self.discard_on_match) + ","
		if self.max_volume != "": 
			desc += "MAX_VOLUME=" + py_to_str(self.max_volume) + ","
		if self.tick_handling_at_crossed_boundary != self.TickHandlingAtCrossedBoundary.DISCARD: 
			desc += "TICK_HANDLING_AT_CROSSED_BOUNDARY=" + py_to_str(self.tick_handling_at_crossed_boundary) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class SkipBadTick(_graph_components.EpBase):
	"""
		

SKIP_BAD_TICK

Type: Filter

Description: Discards ticks based on whether the value of the
attribute specified by FIELD differs from the value of the same
attribute in the surrounding ticks more times than a given threshold.

Although this EP removes a bad tick from the time series, it still
uses the bad tick's value when determining whether subsequent ticks are
bad or not.

Also, when the query is running in CEP mode only the previous 5
ticks are compared against (because future ticks are not available yet).

Python
class name:&nbsp;SkipBadTick

Input: A time series of ticks.

Output: A time series of ticks.

Parameters:

See parameters common to many filters.


  DISCARD_ON_MATCH
(Boolean)
  JUMP_THRESHOLD (numeric)
    A threshold to determine if a tick is "good" or "bad."

    Good ticks are the ticks whose FIELD value differs less than
JUMP_THRESHOLD times from the FIELD's value of less than or half of the
surrounding n ticks.

    A tick would be "bad" even if a simple majority of comparisons
determines as such (by default that would be 6 comparisons [10/2 + 1]).
Default: 2

  
  FIELD
(string)
  NUM_NEIGHBOR_TICKS (integer)
    The number of ticks before this tick and after this tick to
compare a tick against. By default, 5 ticks before the tick (if
available) and 5 ticks after the tick (if available) are compared to
the tick.
Default: 5

  
  USE_ABSOLUTE_VALUES (Boolean)
    When set to true, use absolute values of numbers when checking
whether they are within the jump threshold.
Default: false

  

Examples: Only keep ticks whose price did not jump by more
than 5% relative to the surrounding ticks.

SKIP_BAD_TICK (1.05, PRICE, false)

See the SKIP_BAD_TICK example in FILTER_EXAMPLES.otq.


	"""
	class Parameters:
		discard_on_match = "DISCARD_ON_MATCH"
		jump_threshold = "JUMP_THRESHOLD"
		field = "FIELD"
		num_neighbor_ticks = "NUM_NEIGHBOR_TICKS"
		use_absolute_values = "USE_ABSOLUTE_VALUES"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["discard_on_match", "jump_threshold", "field", "num_neighbor_ticks", "use_absolute_values"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["discard_on_match", "_default_discard_on_match", "jump_threshold", "_default_jump_threshold", "field", "_default_field", "num_neighbor_ticks", "_default_num_neighbor_ticks", "use_absolute_values", "_default_use_absolute_values", "stack_info", "_used_strings"]

	def __init__(self, discard_on_match=False, jump_threshold=2, field="", num_neighbor_ticks=10, use_absolute_values=False):
		_graph_components.EpBase.__init__(self, "SKIP_BAD_TICK")
		self._default_discard_on_match = False
		self.discard_on_match = discard_on_match
		self._default_jump_threshold = 2
		self.jump_threshold = jump_threshold
		self._default_field = ""
		self.field = field
		self._default_num_neighbor_ticks = 10
		self.num_neighbor_ticks = num_neighbor_ticks
		self._default_use_absolute_values = False
		self.use_absolute_values = use_absolute_values
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_discard_on_match(self, value):
		self.discard_on_match = value
		return self

	def set_jump_threshold(self, value):
		self.jump_threshold = value
		return self

	def set_field(self, value):
		self.field = value
		return self

	def set_num_neighbor_ticks(self, value):
		self.num_neighbor_ticks = value
		return self

	def set_use_absolute_values(self, value):
		self.use_absolute_values = value
		return self

	@staticmethod
	def _get_name():
		return "SKIP_BAD_TICK"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.discard_on_match != False: 
			desc += "DISCARD_ON_MATCH=" + py_to_str(self.discard_on_match) + ","
		if self.jump_threshold != 2: 
			desc += "JUMP_THRESHOLD=" + py_to_str(self.jump_threshold) + ","
		if self.field != "": 
			desc += "FIELD=" + py_to_str(self.field) + ","
		if self.num_neighbor_ticks != 10: 
			desc += "NUM_NEIGHBOR_TICKS=" + py_to_str(self.num_neighbor_ticks) + ","
		if self.use_absolute_values != False: 
			desc += "USE_ABSOLUTE_VALUES=" + py_to_str(self.use_absolute_values) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class Limit(_graph_components.EpBase):
	"""
		

LIMIT

Type: Other

Description: Propagates ticks until the count limit is reached. Once the limit is reached, hidden ticks will still continue to propagate until the next regular tick appears.

Python
class name:&nbsp;Limit

Input: A time series of ticks.

Output:  A time series with up to N(determined by the TICK_LIMIT parameter) ticks

Parameters: 



  TICK_LIMIT
    Mandatory parameter. The number of regular ticks to propagate.
      Must be a non-negative integer or -1, which will mean no limit

  
	TICK_OFFSET
    The number of regular ticks to skip before starting to propagate.
      Must be a non-negative integer.

  
  APPLY_ACROSS_SYMBOLS
    If set to true, the tick limit and offset are counted across all symbols combined, rather than separately for each individual symbol.
		This parameter applies within symbols of a single database.
		Default value: false.

  

Examples:

LIMIT (10)


	"""
	class Parameters:
		tick_limit = "TICK_LIMIT"
		tick_offset = "TICK_OFFSET"
		apply_across_symbols = "APPLY_ACROSS_SYMBOLS"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["tick_limit", "tick_offset", "apply_across_symbols"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["tick_limit", "_default_tick_limit", "tick_offset", "_default_tick_offset", "apply_across_symbols", "_default_apply_across_symbols", "stack_info", "_used_strings"]

	def __init__(self, tick_limit="", tick_offset="", apply_across_symbols=False):
		_graph_components.EpBase.__init__(self, "LIMIT")
		self._default_tick_limit = ""
		self.tick_limit = tick_limit
		self._default_tick_offset = ""
		self.tick_offset = tick_offset
		self._default_apply_across_symbols = False
		self.apply_across_symbols = apply_across_symbols
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_tick_limit(self, value):
		self.tick_limit = value
		return self

	def set_tick_offset(self, value):
		self.tick_offset = value
		return self

	def set_apply_across_symbols(self, value):
		self.apply_across_symbols = value
		return self

	@staticmethod
	def _get_name():
		return "LIMIT"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.tick_limit != "": 
			desc += "TICK_LIMIT=" + py_to_str(self.tick_limit) + ","
		if self.tick_offset != "": 
			desc += "TICK_OFFSET=" + py_to_str(self.tick_offset) + ","
		if self.apply_across_symbols != False: 
			desc += "APPLY_ACROSS_SYMBOLS=" + py_to_str(self.apply_across_symbols) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class Interpolate(_graph_components.EpBase):
	"""
		

INTERPOLATE

Type: Transformer

Description: Compute a series of interpolated ticks at
regular intervals.

Python
class name:&nbsp;Interpolate

Input: A time series of ticks.

Output: A time series of ticks, one for each bucket.

Parameters:


  METHOD (enumerated type - LINEAR|STEP)
    When set to LINEAR, linear interpolation is performed. When set
to STEP, the last non-null value is taken as the interpolation result.

  
  INTERVAL (seconds)
  INPUT_FIELD_NAMES (string)
  OUTPUT_FIELD_NAMES (string)

Examples: Output a stream of interpolated prices every 5
seconds:

INTERPOLATE (LINEAR,5, PRICE)

See the INTERPOLATE example in TRANSFORMER_EXAMPLES.otq.


	"""
	class Parameters:
		method = "METHOD"
		interval = "INTERVAL"
		input_field_names = "INPUT_FIELD_NAMES"
		output_field_names = "OUTPUT_FIELD_NAMES"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["method", "interval", "input_field_names", "output_field_names"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["method", "_default_method", "interval", "_default_interval", "input_field_names", "_default_input_field_names", "output_field_names", "_default_output_field_names", "stack_info", "_used_strings"]

	class Method:
		LINEAR = "LINEAR"
		STEP = "STEP"

	def __init__(self, method=Method.LINEAR, interval="", input_field_names="", output_field_names=""):
		_graph_components.EpBase.__init__(self, "INTERPOLATE")
		self._default_method = type(self).Method.LINEAR
		self.method = method
		self._default_interval = ""
		self.interval = interval
		self._default_input_field_names = ""
		self.input_field_names = input_field_names
		self._default_output_field_names = ""
		self.output_field_names = output_field_names
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_method(self, value):
		self.method = value
		return self

	def set_interval(self, value):
		self.interval = value
		return self

	def set_input_field_names(self, value):
		self.input_field_names = value
		return self

	def set_output_field_names(self, value):
		self.output_field_names = value
		return self

	@staticmethod
	def _get_name():
		return "INTERPOLATE"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.method != self.Method.LINEAR: 
			desc += "METHOD=" + py_to_str(self.method) + ","
		if self.interval != "": 
			desc += "INTERVAL=" + py_to_str(self.interval) + ","
		if self.input_field_names != "": 
			desc += "INPUT_FIELD_NAMES=" + py_to_str(self.input_field_names) + ","
		if self.output_field_names != "": 
			desc += "OUTPUT_FIELD_NAMES=" + py_to_str(self.output_field_names) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class SwitchEp(_graph_components.EpBase):
	"""
		

SWITCH

Type: Other

Description: Dispatches each input tick to the corresponding
output as specified by the SWITCH expression value.

Python
class name:&nbsp;SwitchEp

Input: A time series of ticks.

Output: A single or multiple time series of ticks depending
on count of EP outputs (EP outputs are determined by CASES and
DEFAULT_OUTPUT parameters).

Parameters:


  SWITCH (expression)
    Specifies SWITCH expression, which is evaluated on each tick,
and output is chosen according to that value.

  
  CASES (string)
    Specifies mapping of values to output names in the following
format:

    &lt;LIST_OF_VALUES_1&gt;:&lt;OUTPUT_1&gt;;. .
.;&lt;LIST_OF_VALUES_n&gt;:&lt;OUTPUT_n&gt;

    &lt;LIST_OF_VALUES&gt; is a
comma-separated list of values or ranges. Ranges are specified in [RANGE_START,RANGE_END] format (see examples below). RANGE_START
and RANGE_END are included in the
range. Values (including RANGE_START
and RANGE_END) are provided as
constant expressions built from boolean operators (AND, OR, NOT),
relationship operators (&lt;, &lt;=, &gt;, &gt;=, =, !=), and
arithmetic operators (+, -, *, /, and + also serves as string
concatenation operator) according to the usual rules for precedence,
with parentheses available to resolve ambiguities. Alongside constant
numeric and (both single and double-quoted) string literals, it is
possible to use OneTick functions like MIN,
MAX, MOD, DIV (see the catalog of
built-in functions for the full list).

    You can also use pseudo-fields such as _SYMBOL_NAME, _START_TIME,
and _END_TIME (described in detail
in the Pseudo-fields document).

    &lt;OUTPUT&gt; is a name of
the output.

  
  DEFAULT_OUTPUT (string)
    Specifies the name of the default output. If the value of SWITCH
expression doesn't match to any values specified in CASES
parameter, tick is propagated to the default output.
Default: empty

  

Examples:

SWITCH(SWITCH=SIZE,CASES="100,[1100,1500]:OUT1;200:OUT1",DEFAULT_OUTPUT=DEF_OUT)

SWITCH(SWITCH=SIZE,CASES="[ATOL(_SYMBOL_PARAM.RANGE_START),800]:OUT1",DEFAULT_OUTPUT=OUT2)

See SWITCH examples in SWITCH_EP.otq.


	"""
	class Parameters:
		switch = "SWITCH"
		cases = "CASES"
		default_output = "DEFAULT_OUTPUT"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["switch", "cases", "default_output"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["switch", "_default_switch", "cases", "_default_cases", "default_output", "_default_default_output", "stack_info", "_used_strings"]

	def __init__(self, switch="", cases="", default_output=""):
		_graph_components.EpBase.__init__(self, "SWITCH")
		self._default_switch = ""
		self.switch = switch
		self._default_cases = ""
		self.cases = cases
		self._default_default_output = ""
		self.default_output = default_output
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_switch(self, value):
		self.switch = value
		return self

	def set_cases(self, value):
		self.cases = value
		return self

	def set_default_output(self, value):
		self.default_output = value
		return self

	@staticmethod
	def _get_name():
		return "SWITCH"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.switch != "": 
			desc += "SWITCH=" + py_to_str(self.switch) + ","
		if self.cases != "": 
			desc += "CASES=" + py_to_str(self.cases) + ","
		if self.default_output != "": 
			desc += "DEFAULT_OUTPUT=" + py_to_str(self.default_output) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


Switch = SwitchEp


class OrderByEp(_graph_components.EpBase):
	"""
		

ORDER_BY

Type: Sort

Description: Sorts the ticks using the specified set of tick
fields.

Python
class name:&nbsp;OrderByEp

Input: A time series of ticks.

Output: A time series of ticks.

Parameters:


  ORDER_BY (string [DESC|ASC], [string
[DESC|ASC]])
    A comma-separated list of field name/sorting direction pairs.
TIMESTAMP can be used as a field name. When some ticks have the same
values for all listed fields, they are propagated in the same order in
which they arrived into the ORDER_BY EP.
Default: DESC

  

Examples:

Sorts input trade ticks by exchange in descending order, and within
each exchange by trade size in ascending order.

ORDER_BY (EXCHANGE, SIZE ASC)

See the ORDER_BY__EXAMPLE example
in SORT_EXAMPLES.otq.


	"""
	class Parameters:
		order_by = "ORDER_BY"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["order_by"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["order_by", "_default_order_by", "stack_info", "_used_strings"]

	def __init__(self, order_by=""):
		_graph_components.EpBase.__init__(self, "ORDER_BY")
		self._default_order_by = ""
		self.order_by = order_by
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_order_by(self, value):
		self.order_by = value
		return self

	@staticmethod
	def _get_name():
		return "ORDER_BY"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.order_by != "": 
			desc += "ORDER_BY=" + py_to_str(self.order_by) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


OrderBy = OrderByEp


class Ranking(_graph_components.EpBase):
	"""
		

RANKING

Type: Transformer

Description: RANKING sorts a series of ticks over a bucket
interval using a specified set of tick fields and adds a new field
(RANKING) with the position of the tick in the sort order or the
percentage of ticks with values less than or equal to the value of the
tick.

RANKING does not affect the order of the outputted ticks.

Python
class name:&nbsp;Ranking

Input: A time series of ticks.

Output: A time series of ticks.

&nbsp;

Parameters: See parameters
common to generic aggregations.


  RANK_BY (field_name [DESC|ASC], [field_name
[DESC|ASC]],&hellip;)
  SHOW_RANK_AS (enumerated type)
    
      When set to ORDER, the RANKING field contains the position of
the tick in the sort order.
      When set to PERCENT_LE_VALUES, the RANKING field contains the
percentage of ticks that have higher or equal value of the position in
the sort order, relative to the tick.
      When set to PERCENT_LT_VALUES, the RANKING field contains the
percentage of ticks that have higher value of the position in the sort
order, relative to the tick.
      When set to PERCENTILE_STANDARD, the RANKING field contains
the percentile rank of the tick in the sort order, i.e., the percentage
plus the half percentage of the ticks that have respectively higher and
equal value of the position in the sort order, relative to the tick.
For more details, see Percentile Rank.
    
Default: ORDER
  INCLUDE_TICK_IN_PERCENTAGE
(Boolean)
    Specifies whether the current tick must be included in the set
of ticks, with respect to which percentage values are calculated. If
INCLUDE_TICK_IN_PERCENTAGE is not set and SHOW_RANK_AS is
PERCENTILE_STANDARD, the value TRUE will be used. If
INCLUDE_TICK_IN_PERCENTAGE is not set and SHOW_RANK_AS is not
PERCENTILE_STANDARD, the value FALSE will be used.

  
  BUCKET_INTERVAL
(seconds/ticks)
  BUCKET_INTERVAL_UNITS
(enumerated type)
  OUTPUT_INTERVAL
(seconds)
  OUTPUT_INTERVAL_UNITS
(enumerated type)
  BUCKET_TIME
(Boolean)
  BUCKET_END_CRITERIA
(expression)
  BOUNDARY_TICK_BUCKET
(NEW/PREVIOUS)
  OUTPUT_FIELD_NAME
(string)
  GROUP_BY
(string)
  GROUPS_TO_DISPLAY
(enumerated)
  BUCKET_END_PER_GROUP
(Boolean)
  PARTIAL_BUCKET_HANDLING
(enumerated type)

Example: We'll sort ticks by SIZE value in 5-minutes
interval buckets.

See the RANKING example in TRANSFORMER_EXAMPLES.otq.


	"""
	class Parameters:
		rank_by = "RANK_BY"
		show_rank_as = "SHOW_RANK_AS"
		include_tick_in_percentage = "INCLUDE_TICK_IN_PERCENTAGE"
		bucket_interval = "BUCKET_INTERVAL"
		bucket_interval_units = "BUCKET_INTERVAL_UNITS"
		output_interval = "OUTPUT_INTERVAL"
		output_interval_units = "OUTPUT_INTERVAL_UNITS"
		bucket_time = "BUCKET_TIME"
		bucket_end_criteria = "BUCKET_END_CRITERIA"
		boundary_tick_bucket = "BOUNDARY_TICK_BUCKET"
		group_by = "GROUP_BY"
		groups_to_display = "GROUPS_TO_DISPLAY"
		bucket_end_per_group = "BUCKET_END_PER_GROUP"
		partial_bucket_handling = "PARTIAL_BUCKET_HANDLING"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["rank_by", "show_rank_as", "include_tick_in_percentage", "bucket_interval", "bucket_interval_units", "output_interval", "output_interval_units", "bucket_time", "bucket_end_criteria", "boundary_tick_bucket", "group_by", "groups_to_display", "bucket_end_per_group", "partial_bucket_handling"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["rank_by", "_default_rank_by", "show_rank_as", "_default_show_rank_as", "include_tick_in_percentage", "_default_include_tick_in_percentage", "bucket_interval", "_default_bucket_interval", "bucket_interval_units", "_default_bucket_interval_units", "output_interval", "_default_output_interval", "output_interval_units", "_default_output_interval_units", "bucket_time", "_default_bucket_time", "bucket_end_criteria", "_default_bucket_end_criteria", "boundary_tick_bucket", "_default_boundary_tick_bucket", "group_by", "_default_group_by", "groups_to_display", "_default_groups_to_display", "bucket_end_per_group", "_default_bucket_end_per_group", "partial_bucket_handling", "_default_partial_bucket_handling", "stack_info", "_used_strings"]

	class ShowRankAs:
		DENSE_ORDER = "DENSE_ORDER"
		ORDER = "ORDER"
		PERCENTILE_STANDARD = "PERCENTILE_STANDARD"
		PERCENT_LE_VALUES = "PERCENT_LE_VALUES"
		PERCENT_LT_VALUES = "PERCENT_LT_VALUES"

	class BucketIntervalUnits:
		DAYS = "DAYS"
		FLEXIBLE = "FLEXIBLE"
		MONTHS = "MONTHS"
		SECONDS = "SECONDS"
		TICKS = "TICKS"

	class OutputIntervalUnits:
		SECONDS = "SECONDS"
		TICKS = "TICKS"

	class BucketTime:
		BUCKET_END = "BUCKET_END"
		BUCKET_START = "BUCKET_START"

	class BoundaryTickBucket:
		NEW = "NEW"
		PREVIOUS = "PREVIOUS"

	class GroupsToDisplay:
		ALL = "ALL"
		EVENT_IN_LAST_BUCKET = "EVENT_IN_LAST_BUCKET"

	class PartialBucketHandling:
		AS_SEPARATE_BUCKET = "AS_SEPARATE_BUCKET"
		DISCARD = "DISCARD"
		MERGE_WITH_PREVIOUS = "MERGE_WITH_PREVIOUS"

	def __init__(self, rank_by="", show_rank_as=ShowRankAs.ORDER, include_tick_in_percentage=False, bucket_interval=0, bucket_interval_units=BucketIntervalUnits.SECONDS, output_interval="", output_interval_units=OutputIntervalUnits.SECONDS, bucket_time=BucketTime.BUCKET_END, bucket_end_criteria="", boundary_tick_bucket=BoundaryTickBucket.NEW, group_by="", groups_to_display=GroupsToDisplay.ALL, bucket_end_per_group=False, partial_bucket_handling=PartialBucketHandling.AS_SEPARATE_BUCKET):
		_graph_components.EpBase.__init__(self, "RANKING")
		self._default_rank_by = ""
		self.rank_by = rank_by
		self._default_show_rank_as = type(self).ShowRankAs.ORDER
		self.show_rank_as = show_rank_as
		self._default_include_tick_in_percentage = False
		self.include_tick_in_percentage = include_tick_in_percentage
		self._default_bucket_interval = 0
		self.bucket_interval = bucket_interval
		self._default_bucket_interval_units = type(self).BucketIntervalUnits.SECONDS
		self.bucket_interval_units = bucket_interval_units
		self._default_output_interval = ""
		self.output_interval = output_interval
		self._default_output_interval_units = type(self).OutputIntervalUnits.SECONDS
		self.output_interval_units = output_interval_units
		self._default_bucket_time = type(self).BucketTime.BUCKET_END
		self.bucket_time = bucket_time
		self._default_bucket_end_criteria = ""
		self.bucket_end_criteria = bucket_end_criteria
		self._default_boundary_tick_bucket = type(self).BoundaryTickBucket.NEW
		self.boundary_tick_bucket = boundary_tick_bucket
		self._default_group_by = ""
		self.group_by = group_by
		self._default_groups_to_display = type(self).GroupsToDisplay.ALL
		self.groups_to_display = groups_to_display
		self._default_bucket_end_per_group = False
		self.bucket_end_per_group = bucket_end_per_group
		self._default_partial_bucket_handling = type(self).PartialBucketHandling.AS_SEPARATE_BUCKET
		self.partial_bucket_handling = partial_bucket_handling
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_rank_by(self, value):
		self.rank_by = value
		return self

	def set_show_rank_as(self, value):
		self.show_rank_as = value
		return self

	def set_include_tick_in_percentage(self, value):
		self.include_tick_in_percentage = value
		return self

	def set_bucket_interval(self, value):
		self.bucket_interval = value
		return self

	def set_bucket_interval_units(self, value):
		self.bucket_interval_units = value
		return self

	def set_output_interval(self, value):
		self.output_interval = value
		return self

	def set_output_interval_units(self, value):
		self.output_interval_units = value
		return self

	def set_bucket_time(self, value):
		self.bucket_time = value
		return self

	def set_bucket_end_criteria(self, value):
		self.bucket_end_criteria = value
		return self

	def set_boundary_tick_bucket(self, value):
		self.boundary_tick_bucket = value
		return self

	def set_group_by(self, value):
		self.group_by = value
		return self

	def set_groups_to_display(self, value):
		self.groups_to_display = value
		return self

	def set_bucket_end_per_group(self, value):
		self.bucket_end_per_group = value
		return self

	def set_partial_bucket_handling(self, value):
		self.partial_bucket_handling = value
		return self

	@staticmethod
	def _get_name():
		return "RANKING"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.rank_by != "": 
			desc += "RANK_BY=" + py_to_str(self.rank_by) + ","
		if self.show_rank_as != self.ShowRankAs.ORDER: 
			desc += "SHOW_RANK_AS=" + py_to_str(self.show_rank_as) + ","
		if self.include_tick_in_percentage != False: 
			desc += "INCLUDE_TICK_IN_PERCENTAGE=" + py_to_str(self.include_tick_in_percentage) + ","
		if self.bucket_interval != 0: 
			desc += "BUCKET_INTERVAL=" + py_to_str(self.bucket_interval) + ","
		if self.bucket_interval_units != self.BucketIntervalUnits.SECONDS: 
			desc += "BUCKET_INTERVAL_UNITS=" + py_to_str(self.bucket_interval_units) + ","
		if self.output_interval != "": 
			desc += "OUTPUT_INTERVAL=" + py_to_str(self.output_interval) + ","
		if self.output_interval_units != self.OutputIntervalUnits.SECONDS: 
			desc += "OUTPUT_INTERVAL_UNITS=" + py_to_str(self.output_interval_units) + ","
		if self.bucket_time != self.BucketTime.BUCKET_END: 
			desc += "BUCKET_TIME=" + py_to_str(self.bucket_time) + ","
		if self.bucket_end_criteria != "": 
			desc += "BUCKET_END_CRITERIA=" + py_to_str(self.bucket_end_criteria) + ","
		if self.boundary_tick_bucket != self.BoundaryTickBucket.NEW: 
			desc += "BOUNDARY_TICK_BUCKET=" + py_to_str(self.boundary_tick_bucket) + ","
		if self.group_by != "": 
			desc += "GROUP_BY=" + py_to_str(self.group_by) + ","
		if self.groups_to_display != self.GroupsToDisplay.ALL: 
			desc += "GROUPS_TO_DISPLAY=" + py_to_str(self.groups_to_display) + ","
		if self.bucket_end_per_group != False: 
			desc += "BUCKET_END_PER_GROUP=" + py_to_str(self.bucket_end_per_group) + ","
		if self.partial_bucket_handling != self.PartialBucketHandling.AS_SEPARATE_BUCKET: 
			desc += "PARTIAL_BUCKET_HANDLING=" + py_to_str(self.partial_bucket_handling) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class FindValueForPercentile(_graph_components.EpBase):
	"""
		

FIND_VALUE_FOR_PERCENTILE

Type: Aggregation

Description: For each aggregation bucket, finds the value
which has
percentile rank closest to PERCENTILE parameter. For more details, see Percentile rank.

Python
class name:&nbsp;FindValueForPercentile

Input: A time series of ticks.

Output: A time series of ticks.

Parameters: See parameters
common to generic aggregations.


  PERCENTILE (numeric)
    Specifies the percentile in 0-100 % range for which find the
value of input field that has percentile rank close to it.&nbsp;
Default: None

  
  COMPUTE_VALUE_AS&nbsp;(enumerated
type)
    When set to INTERPOLATED_VALUE,
the output field
contains the value interpolated from the value that has percentile
below specified and&nbsp;the value that has percentile above
specified.&nbsp;
When set to FIRST_VALUE_WITH_GE_PERCENTILE,
the output field contains the smallest value which has percentile that
is greater than or equal to the one specified.
Default:&nbsp;INTERPOLATED_VALUE

  
  ORDER (enumerated type)
    Specifies the order in which the values are sorted before
calculating the percentile.
Default:ASC

  
  BUCKET_INTERVAL
(seconds/ticks)
  BUCKET_INTERVAL_UNITS
(enumerated type)
  OUTPUT_INTERVAL
(seconds)
  OUTPUT_INTERVAL_UNITS
(SECONDS/TICKS)
  IS_RUNNING_AGGR
(Boolean)
  BUCKET_TIME
(Boolean)
  BUCKET_END_CRITERIA
(expression)
  BOUNDARY_TICK_BUCKET
(NEW/PREVIOUS)
  PARTIAL_BUCKET_HANDLING
(enumerated type)
  OUTPUT_FIELD_NAME
(string)
  GROUP_BY
(string)
  GROUPS_TO_DISPLAY
(enumerated)
  BUCKET_END_PER_GROUP
(Boolean)
  INPUT_FIELD_NAME
(string)


	"""
	class Parameters:
		bucket_interval = "BUCKET_INTERVAL"
		bucket_interval_units = "BUCKET_INTERVAL_UNITS"
		output_interval = "OUTPUT_INTERVAL"
		output_interval_units = "OUTPUT_INTERVAL_UNITS"
		is_running_aggr = "IS_RUNNING_AGGR"
		bucket_time = "BUCKET_TIME"
		bucket_end_criteria = "BUCKET_END_CRITERIA"
		boundary_tick_bucket = "BOUNDARY_TICK_BUCKET"
		all_fields_for_sliding = "ALL_FIELDS_FOR_SLIDING"
		partial_bucket_handling = "PARTIAL_BUCKET_HANDLING"
		output_field_name = "OUTPUT_FIELD_NAME"
		group_by = "GROUP_BY"
		groups_to_display = "GROUPS_TO_DISPLAY"
		bucket_end_per_group = "BUCKET_END_PER_GROUP"
		input_field_name = "INPUT_FIELD_NAME"
		percentile = "PERCENTILE"
		compute_value_as = "COMPUTE_VALUE_AS"
		order = "ORDER"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["bucket_interval", "bucket_interval_units", "output_interval", "output_interval_units", "is_running_aggr", "bucket_time", "bucket_end_criteria", "boundary_tick_bucket", "all_fields_for_sliding", "partial_bucket_handling", "output_field_name", "group_by", "groups_to_display", "bucket_end_per_group", "input_field_name", "percentile", "compute_value_as", "order"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["bucket_interval", "_default_bucket_interval", "bucket_interval_units", "_default_bucket_interval_units", "output_interval", "_default_output_interval", "output_interval_units", "_default_output_interval_units", "is_running_aggr", "_default_is_running_aggr", "bucket_time", "_default_bucket_time", "bucket_end_criteria", "_default_bucket_end_criteria", "boundary_tick_bucket", "_default_boundary_tick_bucket", "all_fields_for_sliding", "_default_all_fields_for_sliding", "partial_bucket_handling", "_default_partial_bucket_handling", "output_field_name", "_default_output_field_name", "group_by", "_default_group_by", "groups_to_display", "_default_groups_to_display", "bucket_end_per_group", "_default_bucket_end_per_group", "input_field_name", "_default_input_field_name", "percentile", "_default_percentile", "compute_value_as", "_default_compute_value_as", "order", "_default_order", "stack_info", "_used_strings"]

	class BucketIntervalUnits:
		DAYS = "DAYS"
		FLEXIBLE = "FLEXIBLE"
		MONTHS = "MONTHS"
		SECONDS = "SECONDS"
		TICKS = "TICKS"

	class OutputIntervalUnits:
		SECONDS = "SECONDS"
		TICKS = "TICKS"

	class BucketTime:
		BUCKET_END = "BUCKET_END"
		BUCKET_START = "BUCKET_START"

	class BoundaryTickBucket:
		NEW = "NEW"
		PREVIOUS = "PREVIOUS"

	class AllFieldsForSliding:
		WHEN_TICKS_EXIT_WINDOW = "WHEN_TICKS_EXIT_WINDOW"
		FALSE = "false"
		TRUE = "true"

	class PartialBucketHandling:
		AS_SEPARATE_BUCKET = "AS_SEPARATE_BUCKET"
		DISCARD = "DISCARD"
		MERGE_WITH_PREVIOUS = "MERGE_WITH_PREVIOUS"

	class GroupsToDisplay:
		ALL = "ALL"
		EVENT_IN_LAST_BUCKET = "EVENT_IN_LAST_BUCKET"

	class ComputeValueAs:
		FIRST_VALUE_WITH_GE_PERCENTILE = "FIRST_VALUE_WITH_GE_PERCENTILE"
		INTERPOLATED_VALUE = "INTERPOLATED_VALUE"

	class Order:
		ASC = "ASC"
		DESC = "DESC"

	def __init__(self, bucket_interval=0, bucket_interval_units=BucketIntervalUnits.SECONDS, output_interval="", output_interval_units=OutputIntervalUnits.SECONDS, is_running_aggr=False, bucket_time=BucketTime.BUCKET_END, bucket_end_criteria="", boundary_tick_bucket=BoundaryTickBucket.NEW, all_fields_for_sliding=AllFieldsForSliding.FALSE, partial_bucket_handling=PartialBucketHandling.AS_SEPARATE_BUCKET, output_field_name="VALUE", group_by="", groups_to_display=GroupsToDisplay.ALL, bucket_end_per_group=False, input_field_name="", percentile="", compute_value_as=ComputeValueAs.INTERPOLATED_VALUE, order=Order.ASC, In = "", Out = ""):
		_graph_components.EpBase.__init__(self, "FIND_VALUE_FOR_PERCENTILE")
		self._default_bucket_interval = 0
		self.bucket_interval = bucket_interval
		self._default_bucket_interval_units = type(self).BucketIntervalUnits.SECONDS
		self.bucket_interval_units = bucket_interval_units
		self._default_output_interval = ""
		self.output_interval = output_interval
		self._default_output_interval_units = type(self).OutputIntervalUnits.SECONDS
		self.output_interval_units = output_interval_units
		self._default_is_running_aggr = False
		self.is_running_aggr = is_running_aggr
		self._default_bucket_time = type(self).BucketTime.BUCKET_END
		self.bucket_time = bucket_time
		self._default_bucket_end_criteria = ""
		self.bucket_end_criteria = bucket_end_criteria
		self._default_boundary_tick_bucket = type(self).BoundaryTickBucket.NEW
		self.boundary_tick_bucket = boundary_tick_bucket
		self._default_all_fields_for_sliding = type(self).AllFieldsForSliding.FALSE
		self.all_fields_for_sliding = all_fields_for_sliding
		self._default_partial_bucket_handling = type(self).PartialBucketHandling.AS_SEPARATE_BUCKET
		self.partial_bucket_handling = partial_bucket_handling
		self._default_output_field_name = "VALUE"
		self.output_field_name = output_field_name
		self._default_group_by = ""
		self.group_by = group_by
		self._default_groups_to_display = type(self).GroupsToDisplay.ALL
		self.groups_to_display = groups_to_display
		self._default_bucket_end_per_group = False
		self.bucket_end_per_group = bucket_end_per_group
		self._default_input_field_name = ""
		self.input_field_name = input_field_name
		self._default_percentile = ""
		self.percentile = percentile
		self._default_compute_value_as = type(self).ComputeValueAs.INTERPOLATED_VALUE
		self.compute_value_as = compute_value_as
		self._default_order = type(self).Order.ASC
		self.order = order
		if In != "":
			self.input_field_name=In
		if Out != "":
			self.output_field_name=Out
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_bucket_interval(self, value):
		self.bucket_interval = value
		return self

	def set_bucket_interval_units(self, value):
		self.bucket_interval_units = value
		return self

	def set_output_interval(self, value):
		self.output_interval = value
		return self

	def set_output_interval_units(self, value):
		self.output_interval_units = value
		return self

	def set_is_running_aggr(self, value):
		self.is_running_aggr = value
		return self

	def set_bucket_time(self, value):
		self.bucket_time = value
		return self

	def set_bucket_end_criteria(self, value):
		self.bucket_end_criteria = value
		return self

	def set_boundary_tick_bucket(self, value):
		self.boundary_tick_bucket = value
		return self

	def set_all_fields_for_sliding(self, value):
		self.all_fields_for_sliding = value
		return self

	def set_partial_bucket_handling(self, value):
		self.partial_bucket_handling = value
		return self

	def set_output_field_name(self, value):
		self.output_field_name = value
		return self

	def set_group_by(self, value):
		self.group_by = value
		return self

	def set_groups_to_display(self, value):
		self.groups_to_display = value
		return self

	def set_bucket_end_per_group(self, value):
		self.bucket_end_per_group = value
		return self

	def set_input_field_name(self, value):
		self.input_field_name = value
		return self

	def set_percentile(self, value):
		self.percentile = value
		return self

	def set_compute_value_as(self, value):
		self.compute_value_as = value
		return self

	def set_order(self, value):
		self.order = value
		return self

	@staticmethod
	def _get_name():
		return "FIND_VALUE_FOR_PERCENTILE"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.bucket_interval != 0: 
			desc += "BUCKET_INTERVAL=" + py_to_str(self.bucket_interval) + ","
		if self.bucket_interval_units != self.BucketIntervalUnits.SECONDS: 
			desc += "BUCKET_INTERVAL_UNITS=" + py_to_str(self.bucket_interval_units) + ","
		if self.output_interval != "": 
			desc += "OUTPUT_INTERVAL=" + py_to_str(self.output_interval) + ","
		if self.output_interval_units != self.OutputIntervalUnits.SECONDS: 
			desc += "OUTPUT_INTERVAL_UNITS=" + py_to_str(self.output_interval_units) + ","
		if self.is_running_aggr != False: 
			desc += "IS_RUNNING_AGGR=" + py_to_str(self.is_running_aggr) + ","
		if self.bucket_time != self.BucketTime.BUCKET_END: 
			desc += "BUCKET_TIME=" + py_to_str(self.bucket_time) + ","
		if self.bucket_end_criteria != "": 
			desc += "BUCKET_END_CRITERIA=" + py_to_str(self.bucket_end_criteria) + ","
		if self.boundary_tick_bucket != self.BoundaryTickBucket.NEW: 
			desc += "BOUNDARY_TICK_BUCKET=" + py_to_str(self.boundary_tick_bucket) + ","
		if self.all_fields_for_sliding != self.AllFieldsForSliding.FALSE: 
			desc += "ALL_FIELDS_FOR_SLIDING=" + py_to_str(self.all_fields_for_sliding) + ","
		if self.partial_bucket_handling != self.PartialBucketHandling.AS_SEPARATE_BUCKET: 
			desc += "PARTIAL_BUCKET_HANDLING=" + py_to_str(self.partial_bucket_handling) + ","
		if self.output_field_name != "VALUE": 
			desc += "OUTPUT_FIELD_NAME=" + py_to_str(self.output_field_name) + ","
		if self.group_by != "": 
			desc += "GROUP_BY=" + py_to_str(self.group_by) + ","
		if self.groups_to_display != self.GroupsToDisplay.ALL: 
			desc += "GROUPS_TO_DISPLAY=" + py_to_str(self.groups_to_display) + ","
		if self.bucket_end_per_group != False: 
			desc += "BUCKET_END_PER_GROUP=" + py_to_str(self.bucket_end_per_group) + ","
		if self.input_field_name != "": 
			desc += "INPUT_FIELD_NAME=" + py_to_str(self.input_field_name) + ","
		if self.percentile != "": 
			desc += "PERCENTILE=" + py_to_str(self.percentile) + ","
		if self.compute_value_as != self.ComputeValueAs.INTERPOLATED_VALUE: 
			desc += "COMPUTE_VALUE_AS=" + py_to_str(self.compute_value_as) + ","
		if self.order != self.Order.ASC: 
			desc += "ORDER=" + py_to_str(self.order) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class FindValuesForPercentiles(_graph_components.EpBase):
	"""
		
    
      FIND_VALUES_FOR_PERCENTILES
    
    Type: Aggregation

    
      Description: For each aggregation bucket, finds the values which
      have percentile rank closest to PERCENTILES parameter. For more details,
      see
      Percentile rank. See also
      FIND_VALUE_FOR_PERCENTILE.
    

    
      Python class name:&nbsp;FindValuesForPercentiles
    

    Input: A time series of ticks.

    Output: A time series of ticks.

    
      Parameters: See
      parameters common to generic aggregations
      and
      
        parameters common with FIND_VALUE_FOR_PERCENTILE aggregation.
    

    
      
        PERCENTILES (numeric [, numeric])
        
          Specifies the percentile(s) in 0-100 % range for which find the values
          of input field that has percentile rank close to it.&nbsp;
          Default: None
        

      
      
        OUTPUT_FIELD_NAMES
        (field_name,[field_name])
        
          OUTPUT_FIELD_NAMES and PERCENTILES must have the same number of
          fields.
          Default: None
        

      
      
        
          COMPUTE_VALUE_AS
        
        (enumerated type)
      
      
        
          ORDER
        
        (enumerated type)
      
      
        BUCKET_INTERVAL_UNITS
        (enumerated type)
      
      
        OUTPUT_INTERVAL
        (seconds)
      
      
        OUTPUT_INTERVAL_UNITS
        (SECONDS/TICKS)
      
      
        IS_RUNNING_AGGR
        (Boolean)
      
      
        BUCKET_TIME
        (Boolean)
      
      
        BUCKET_END_CRITERIA
        (expression)
      
      
        BOUNDARY_TICK_BUCKET
        (NEW/PREVIOUS)
      
      
        PARTIAL_BUCKET_HANDLING
        (enumerated type)
      
      
      
        GROUP_BY
        (string)
      
      
        GROUPS_TO_DISPLAY
        (enumerated)
      
      
        BUCKET_END_PER_GROUP
        (Boolean)
      
      
        INPUT_FIELD_NAME
        (string)
      
    
    
	"""
	class Parameters:
		bucket_interval = "BUCKET_INTERVAL"
		bucket_interval_units = "BUCKET_INTERVAL_UNITS"
		output_interval = "OUTPUT_INTERVAL"
		output_interval_units = "OUTPUT_INTERVAL_UNITS"
		is_running_aggr = "IS_RUNNING_AGGR"
		bucket_time = "BUCKET_TIME"
		bucket_end_criteria = "BUCKET_END_CRITERIA"
		boundary_tick_bucket = "BOUNDARY_TICK_BUCKET"
		all_fields_for_sliding = "ALL_FIELDS_FOR_SLIDING"
		partial_bucket_handling = "PARTIAL_BUCKET_HANDLING"
		input_field_name = "INPUT_FIELD_NAME"
		output_field_names = "OUTPUT_FIELD_NAMES"
		group_by = "GROUP_BY"
		groups_to_display = "GROUPS_TO_DISPLAY"
		bucket_end_per_group = "BUCKET_END_PER_GROUP"
		percentiles = "PERCENTILES"
		compute_value_as = "COMPUTE_VALUE_AS"
		order = "ORDER"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["bucket_interval", "bucket_interval_units", "output_interval", "output_interval_units", "is_running_aggr", "bucket_time", "bucket_end_criteria", "boundary_tick_bucket", "all_fields_for_sliding", "partial_bucket_handling", "input_field_name", "output_field_names", "group_by", "groups_to_display", "bucket_end_per_group", "percentiles", "compute_value_as", "order"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["bucket_interval", "_default_bucket_interval", "bucket_interval_units", "_default_bucket_interval_units", "output_interval", "_default_output_interval", "output_interval_units", "_default_output_interval_units", "is_running_aggr", "_default_is_running_aggr", "bucket_time", "_default_bucket_time", "bucket_end_criteria", "_default_bucket_end_criteria", "boundary_tick_bucket", "_default_boundary_tick_bucket", "all_fields_for_sliding", "_default_all_fields_for_sliding", "partial_bucket_handling", "_default_partial_bucket_handling", "input_field_name", "_default_input_field_name", "output_field_names", "_default_output_field_names", "group_by", "_default_group_by", "groups_to_display", "_default_groups_to_display", "bucket_end_per_group", "_default_bucket_end_per_group", "percentiles", "_default_percentiles", "compute_value_as", "_default_compute_value_as", "order", "_default_order", "stack_info", "_used_strings"]

	class BucketIntervalUnits:
		DAYS = "DAYS"
		FLEXIBLE = "FLEXIBLE"
		MONTHS = "MONTHS"
		SECONDS = "SECONDS"
		TICKS = "TICKS"

	class OutputIntervalUnits:
		SECONDS = "SECONDS"
		TICKS = "TICKS"

	class BucketTime:
		BUCKET_END = "BUCKET_END"
		BUCKET_START = "BUCKET_START"

	class BoundaryTickBucket:
		NEW = "NEW"
		PREVIOUS = "PREVIOUS"

	class AllFieldsForSliding:
		WHEN_TICKS_EXIT_WINDOW = "WHEN_TICKS_EXIT_WINDOW"
		FALSE = "false"
		TRUE = "true"

	class PartialBucketHandling:
		AS_SEPARATE_BUCKET = "AS_SEPARATE_BUCKET"
		DISCARD = "DISCARD"
		MERGE_WITH_PREVIOUS = "MERGE_WITH_PREVIOUS"

	class GroupsToDisplay:
		ALL = "ALL"
		EVENT_IN_LAST_BUCKET = "EVENT_IN_LAST_BUCKET"

	class ComputeValueAs:
		FIRST_VALUE_WITH_GE_PERCENTILE = "FIRST_VALUE_WITH_GE_PERCENTILE"
		INTERPOLATED_VALUE = "INTERPOLATED_VALUE"

	class Order:
		ASC = "ASC"
		DESC = "DESC"

	def __init__(self, bucket_interval=0, bucket_interval_units=BucketIntervalUnits.SECONDS, output_interval="", output_interval_units=OutputIntervalUnits.SECONDS, is_running_aggr=False, bucket_time=BucketTime.BUCKET_END, bucket_end_criteria="", boundary_tick_bucket=BoundaryTickBucket.NEW, all_fields_for_sliding=AllFieldsForSliding.FALSE, partial_bucket_handling=PartialBucketHandling.AS_SEPARATE_BUCKET, input_field_name="", output_field_names="", group_by="", groups_to_display=GroupsToDisplay.ALL, bucket_end_per_group=False, percentiles="", compute_value_as=ComputeValueAs.INTERPOLATED_VALUE, order=Order.ASC, In = ""):
		_graph_components.EpBase.__init__(self, "FIND_VALUES_FOR_PERCENTILES")
		self._default_bucket_interval = 0
		self.bucket_interval = bucket_interval
		self._default_bucket_interval_units = type(self).BucketIntervalUnits.SECONDS
		self.bucket_interval_units = bucket_interval_units
		self._default_output_interval = ""
		self.output_interval = output_interval
		self._default_output_interval_units = type(self).OutputIntervalUnits.SECONDS
		self.output_interval_units = output_interval_units
		self._default_is_running_aggr = False
		self.is_running_aggr = is_running_aggr
		self._default_bucket_time = type(self).BucketTime.BUCKET_END
		self.bucket_time = bucket_time
		self._default_bucket_end_criteria = ""
		self.bucket_end_criteria = bucket_end_criteria
		self._default_boundary_tick_bucket = type(self).BoundaryTickBucket.NEW
		self.boundary_tick_bucket = boundary_tick_bucket
		self._default_all_fields_for_sliding = type(self).AllFieldsForSliding.FALSE
		self.all_fields_for_sliding = all_fields_for_sliding
		self._default_partial_bucket_handling = type(self).PartialBucketHandling.AS_SEPARATE_BUCKET
		self.partial_bucket_handling = partial_bucket_handling
		self._default_input_field_name = ""
		self.input_field_name = input_field_name
		self._default_output_field_names = ""
		self.output_field_names = output_field_names
		self._default_group_by = ""
		self.group_by = group_by
		self._default_groups_to_display = type(self).GroupsToDisplay.ALL
		self.groups_to_display = groups_to_display
		self._default_bucket_end_per_group = False
		self.bucket_end_per_group = bucket_end_per_group
		self._default_percentiles = ""
		self.percentiles = percentiles
		self._default_compute_value_as = type(self).ComputeValueAs.INTERPOLATED_VALUE
		self.compute_value_as = compute_value_as
		self._default_order = type(self).Order.ASC
		self.order = order
		if In != "":
			self.input_field_name=In
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_bucket_interval(self, value):
		self.bucket_interval = value
		return self

	def set_bucket_interval_units(self, value):
		self.bucket_interval_units = value
		return self

	def set_output_interval(self, value):
		self.output_interval = value
		return self

	def set_output_interval_units(self, value):
		self.output_interval_units = value
		return self

	def set_is_running_aggr(self, value):
		self.is_running_aggr = value
		return self

	def set_bucket_time(self, value):
		self.bucket_time = value
		return self

	def set_bucket_end_criteria(self, value):
		self.bucket_end_criteria = value
		return self

	def set_boundary_tick_bucket(self, value):
		self.boundary_tick_bucket = value
		return self

	def set_all_fields_for_sliding(self, value):
		self.all_fields_for_sliding = value
		return self

	def set_partial_bucket_handling(self, value):
		self.partial_bucket_handling = value
		return self

	def set_input_field_name(self, value):
		self.input_field_name = value
		return self

	def set_output_field_names(self, value):
		self.output_field_names = value
		return self

	def set_group_by(self, value):
		self.group_by = value
		return self

	def set_groups_to_display(self, value):
		self.groups_to_display = value
		return self

	def set_bucket_end_per_group(self, value):
		self.bucket_end_per_group = value
		return self

	def set_percentiles(self, value):
		self.percentiles = value
		return self

	def set_compute_value_as(self, value):
		self.compute_value_as = value
		return self

	def set_order(self, value):
		self.order = value
		return self

	@staticmethod
	def _get_name():
		return "FIND_VALUES_FOR_PERCENTILES"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.bucket_interval != 0: 
			desc += "BUCKET_INTERVAL=" + py_to_str(self.bucket_interval) + ","
		if self.bucket_interval_units != self.BucketIntervalUnits.SECONDS: 
			desc += "BUCKET_INTERVAL_UNITS=" + py_to_str(self.bucket_interval_units) + ","
		if self.output_interval != "": 
			desc += "OUTPUT_INTERVAL=" + py_to_str(self.output_interval) + ","
		if self.output_interval_units != self.OutputIntervalUnits.SECONDS: 
			desc += "OUTPUT_INTERVAL_UNITS=" + py_to_str(self.output_interval_units) + ","
		if self.is_running_aggr != False: 
			desc += "IS_RUNNING_AGGR=" + py_to_str(self.is_running_aggr) + ","
		if self.bucket_time != self.BucketTime.BUCKET_END: 
			desc += "BUCKET_TIME=" + py_to_str(self.bucket_time) + ","
		if self.bucket_end_criteria != "": 
			desc += "BUCKET_END_CRITERIA=" + py_to_str(self.bucket_end_criteria) + ","
		if self.boundary_tick_bucket != self.BoundaryTickBucket.NEW: 
			desc += "BOUNDARY_TICK_BUCKET=" + py_to_str(self.boundary_tick_bucket) + ","
		if self.all_fields_for_sliding != self.AllFieldsForSliding.FALSE: 
			desc += "ALL_FIELDS_FOR_SLIDING=" + py_to_str(self.all_fields_for_sliding) + ","
		if self.partial_bucket_handling != self.PartialBucketHandling.AS_SEPARATE_BUCKET: 
			desc += "PARTIAL_BUCKET_HANDLING=" + py_to_str(self.partial_bucket_handling) + ","
		if self.input_field_name != "": 
			desc += "INPUT_FIELD_NAME=" + py_to_str(self.input_field_name) + ","
		if self.output_field_names != "": 
			desc += "OUTPUT_FIELD_NAMES=" + py_to_str(self.output_field_names) + ","
		if self.group_by != "": 
			desc += "GROUP_BY=" + py_to_str(self.group_by) + ","
		if self.groups_to_display != self.GroupsToDisplay.ALL: 
			desc += "GROUPS_TO_DISPLAY=" + py_to_str(self.groups_to_display) + ","
		if self.bucket_end_per_group != False: 
			desc += "BUCKET_END_PER_GROUP=" + py_to_str(self.bucket_end_per_group) + ","
		if self.percentiles != "": 
			desc += "PERCENTILES=" + py_to_str(self.percentiles) + ","
		if self.compute_value_as != self.ComputeValueAs.INTERPOLATED_VALUE: 
			desc += "COMPUTE_VALUE_AS=" + py_to_str(self.compute_value_as) + ","
		if self.order != self.Order.ASC: 
			desc += "ORDER=" + py_to_str(self.order) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class ComputeEp(_graph_components.EpBase):
	"""
		

COMPUTE

Type: Aggregation

Description: For each bucket computes multiple aggregations,
all of which take the same parameters and input series.

Python
class name:
ComputeEp

Input: A time series of ticks

Output: Multiple time series of modified ticks: one time
series for each aggregation, one tick for each bucket period

Parameters: See parameters
common to generic aggregations.


  COMPUTE (A list of aggregations)
    The list of aggregations should be specified in the following
format:

    &lt;aggregation1&gt;(&lt;param1&gt;='&lt;value1&gt;',&lt;param2&gt;='&lt;value2&gt;',...)
label1,
&lt;aggregation2&gt;(&lt;param1&gt;='&lt;value1&gt;',&lt;param2&gt;='&lt;value2&gt;',...)
label12

    Labels are optional: if the label is omitted, the
aggregation name is used as the label. Output attributes of each
aggregation in the COMPUTE parameter are named &lt;label&gt;.&lt;output attribute name&gt;
or just &lt;label&gt;, if APPEND_OUTPUT_FIELD_NAME is set
to false.

    You can use an extra parameter REGEX
with each aggregation in COMPUTE. For each field from the tick
descriptor that matches the provided regex, a new aggregation is added
to the list. Matched parts can be used to specify parameters and labels
(see examples). Note that aggregations are
created only when the first tick descriptor arrives, otherwise the
operation is ignored.

    For
each aggregation in COMPUTE, parameters are optional:
aggregation-specific defaults are used for omitted parameters.
Parameter names are the same as the ones used by the aggregation when
it is run outside of COMPUTE.

    Note that COMPUTE does not work with aggregation
EPs that return multiple ticks per bucket interval.

  
  SHOW_ALL_FIELDS (Boolean)
    If both IS_RUNNING_AGGR and
SHOW_ALL_FIELDS are set to true,
output ticks include all fields from the input ticks. Also, in this
case, output ticks are created only when a tick enters&nbsp; the
sliding window.
If&nbsp;IS_RUNNING_AGGR is
set to true and SHOW_ALL_FIELDS are
set to WHEN_TICKS_EXIT_WINDOW,
output ticks include all fields from the input ticks. Also, in this
case, output ticks are created only when a tick&nbsp;leaves the sliding
window.
If IS_RUNNING_AGGR is set to true
but SHOW_ALL_FIELDS is set to false,
an output tick is created both when a tick enters and when a tick
exits&nbsp; the sliding window.
    
Default:&nbsp;false

  
  SHOW_INTERVALS_WITHOUT_TICKS&nbsp;(Boolean)
    If set to false,&nbsp; and BUCKET_INTERVAL &gt; 0,
output ticks are not produced for the bucket intervals during which no
input tick arrived.
If set to false,&nbsp; and OUTPUT_INTERVAL &gt;
0, output ticks are not produced for the output intervals during which
no input tick arrived.
    
Default: true

  
  BUCKET_INTERVAL
(seconds/ticks)
  BUCKET_INTERVAL_UNITS
(enumerated type)
  OUTPUT_INTERVAL
(seconds)
  OUTPUT_INTERVAL_UNITS
(SECONDS/TICKS)
  IS_RUNNING_AGGR
(Boolean)
  BUCKET_TIME
(Boolean)
  BUCKET_END_CRITERIA
(expression)
  BOUNDARY_TICK_BUCKET
(NEW/PREVIOUS)
  PARTIAL_BUCKET_HANDLING
(enumerated type)
  GROUP_BY
(string)
  GROUPS_TO_DISPLAY
(enumerated)
  BUCKET_END_PER_GROUP
(Boolean)
  APPEND_OUTPUT_FIELD_NAME
(Boolean)
    If true (default), output
attributes of each aggregation in the COMPUTE parameter are named as &lt;aggregation label&gt;.&lt;output attribute
name&gt;.
If false, output attributes of each
aggregation in the COMPUTE parameter are named as &lt;aggregation label&gt;. When set to false, there can be only one output
attribute for each aggregation.

  

Examples:

The following example computes the 1-minute price HLOC
(high-low-open-close).

COMPUTE(COMPUTE="HIGH,LOW,LAST,FIRST",BUCKET_INTERVAL=60)

The following example computes the 1-minute bid and ask volume
(high-low-open-close)


COMPUTE(COMPUTE=" SUM(INPUT_FIELD_NAME='ASK_SIZE')
ask_volume, SUM(INPUT_FIELD_NAME='BID_SIZE')
bid_volume",BUCKET_INTERVAL=60)

The following example computes the average for each field that has
the _PRICE suffix.

COMPUTE(COMPUTE="AVERAGE(REGEX='(.*)_PRICE',
INPUT_FIELD_NAME='\\0') avg_price_for_\\1", BUCKET_INTERVAL=3600)

See the STAT1 example in AGGREGATION_COMPUTE_EXAMPLES.otq.


	"""
	class Parameters:
		compute = "COMPUTE"
		show_all_fields = "SHOW_ALL_FIELDS"
		bucket_interval = "BUCKET_INTERVAL"
		bucket_interval_units = "BUCKET_INTERVAL_UNITS"
		output_interval = "OUTPUT_INTERVAL"
		output_interval_units = "OUTPUT_INTERVAL_UNITS"
		is_running_aggr = "IS_RUNNING_AGGR"
		bucket_time = "BUCKET_TIME"
		bucket_end_criteria = "BUCKET_END_CRITERIA"
		boundary_tick_bucket = "BOUNDARY_TICK_BUCKET"
		partial_bucket_handling = "PARTIAL_BUCKET_HANDLING"
		group_by = "GROUP_BY"
		groups_to_display = "GROUPS_TO_DISPLAY"
		bucket_end_per_group = "BUCKET_END_PER_GROUP"
		append_output_field_name = "APPEND_OUTPUT_FIELD_NAME"
		show_intervals_without_ticks = "SHOW_INTERVALS_WITHOUT_TICKS"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["compute", "show_all_fields", "bucket_interval", "bucket_interval_units", "output_interval", "output_interval_units", "is_running_aggr", "bucket_time", "bucket_end_criteria", "boundary_tick_bucket", "partial_bucket_handling", "group_by", "groups_to_display", "bucket_end_per_group", "append_output_field_name", "show_intervals_without_ticks"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["compute", "_default_compute", "show_all_fields", "_default_show_all_fields", "bucket_interval", "_default_bucket_interval", "bucket_interval_units", "_default_bucket_interval_units", "output_interval", "_default_output_interval", "output_interval_units", "_default_output_interval_units", "is_running_aggr", "_default_is_running_aggr", "bucket_time", "_default_bucket_time", "bucket_end_criteria", "_default_bucket_end_criteria", "boundary_tick_bucket", "_default_boundary_tick_bucket", "partial_bucket_handling", "_default_partial_bucket_handling", "group_by", "_default_group_by", "groups_to_display", "_default_groups_to_display", "bucket_end_per_group", "_default_bucket_end_per_group", "append_output_field_name", "_default_append_output_field_name", "show_intervals_without_ticks", "_default_show_intervals_without_ticks", "stack_info", "_used_strings"]

	class ShowAllFields:
		WHEN_TICKS_EXIT_WINDOW = "WHEN_TICKS_EXIT_WINDOW"
		FALSE = "false"
		TRUE = "true"

	class BucketIntervalUnits:
		DAYS = "DAYS"
		FLEXIBLE = "FLEXIBLE"
		MONTHS = "MONTHS"
		SECONDS = "SECONDS"
		TICKS = "TICKS"

	class OutputIntervalUnits:
		SECONDS = "SECONDS"
		TICKS = "TICKS"

	class BucketTime:
		BUCKET_END = "BUCKET_END"
		BUCKET_START = "BUCKET_START"

	class BoundaryTickBucket:
		NEW = "NEW"
		PREVIOUS = "PREVIOUS"

	class PartialBucketHandling:
		AS_SEPARATE_BUCKET = "AS_SEPARATE_BUCKET"
		DISCARD = "DISCARD"
		MERGE_WITH_PREVIOUS = "MERGE_WITH_PREVIOUS"

	class GroupsToDisplay:
		ALL = "ALL"
		EVENT_IN_LAST_BUCKET = "EVENT_IN_LAST_BUCKET"

	def __init__(self, compute="", show_all_fields=ShowAllFields.FALSE, bucket_interval=0, bucket_interval_units=BucketIntervalUnits.SECONDS, output_interval="", output_interval_units=OutputIntervalUnits.SECONDS, is_running_aggr=False, bucket_time=BucketTime.BUCKET_END, bucket_end_criteria="", boundary_tick_bucket=BoundaryTickBucket.NEW, partial_bucket_handling=PartialBucketHandling.AS_SEPARATE_BUCKET, group_by="", groups_to_display=GroupsToDisplay.ALL, bucket_end_per_group=False, append_output_field_name=True, show_intervals_without_ticks=True):
		_graph_components.EpBase.__init__(self, "COMPUTE")
		self._default_compute = ""
		self.compute = compute
		self._default_show_all_fields = type(self).ShowAllFields.FALSE
		self.show_all_fields = show_all_fields
		self._default_bucket_interval = 0
		self.bucket_interval = bucket_interval
		self._default_bucket_interval_units = type(self).BucketIntervalUnits.SECONDS
		self.bucket_interval_units = bucket_interval_units
		self._default_output_interval = ""
		self.output_interval = output_interval
		self._default_output_interval_units = type(self).OutputIntervalUnits.SECONDS
		self.output_interval_units = output_interval_units
		self._default_is_running_aggr = False
		self.is_running_aggr = is_running_aggr
		self._default_bucket_time = type(self).BucketTime.BUCKET_END
		self.bucket_time = bucket_time
		self._default_bucket_end_criteria = ""
		self.bucket_end_criteria = bucket_end_criteria
		self._default_boundary_tick_bucket = type(self).BoundaryTickBucket.NEW
		self.boundary_tick_bucket = boundary_tick_bucket
		self._default_partial_bucket_handling = type(self).PartialBucketHandling.AS_SEPARATE_BUCKET
		self.partial_bucket_handling = partial_bucket_handling
		self._default_group_by = ""
		self.group_by = group_by
		self._default_groups_to_display = type(self).GroupsToDisplay.ALL
		self.groups_to_display = groups_to_display
		self._default_bucket_end_per_group = False
		self.bucket_end_per_group = bucket_end_per_group
		self._default_append_output_field_name = True
		self.append_output_field_name = append_output_field_name
		self._default_show_intervals_without_ticks = True
		self.show_intervals_without_ticks = show_intervals_without_ticks
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_compute(self, value):
		self.compute = value
		return self

	def set_show_all_fields(self, value):
		self.show_all_fields = value
		return self

	def set_bucket_interval(self, value):
		self.bucket_interval = value
		return self

	def set_bucket_interval_units(self, value):
		self.bucket_interval_units = value
		return self

	def set_output_interval(self, value):
		self.output_interval = value
		return self

	def set_output_interval_units(self, value):
		self.output_interval_units = value
		return self

	def set_is_running_aggr(self, value):
		self.is_running_aggr = value
		return self

	def set_bucket_time(self, value):
		self.bucket_time = value
		return self

	def set_bucket_end_criteria(self, value):
		self.bucket_end_criteria = value
		return self

	def set_boundary_tick_bucket(self, value):
		self.boundary_tick_bucket = value
		return self

	def set_partial_bucket_handling(self, value):
		self.partial_bucket_handling = value
		return self

	def set_group_by(self, value):
		self.group_by = value
		return self

	def set_groups_to_display(self, value):
		self.groups_to_display = value
		return self

	def set_bucket_end_per_group(self, value):
		self.bucket_end_per_group = value
		return self

	def set_append_output_field_name(self, value):
		self.append_output_field_name = value
		return self

	def set_show_intervals_without_ticks(self, value):
		self.show_intervals_without_ticks = value
		return self

	@staticmethod
	def _get_name():
		return "COMPUTE"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.compute != "": 
			desc += "COMPUTE=" + py_to_str(self.compute) + ","
		if self.show_all_fields != self.ShowAllFields.FALSE: 
			desc += "SHOW_ALL_FIELDS=" + py_to_str(self.show_all_fields) + ","
		if self.bucket_interval != 0: 
			desc += "BUCKET_INTERVAL=" + py_to_str(self.bucket_interval) + ","
		if self.bucket_interval_units != self.BucketIntervalUnits.SECONDS: 
			desc += "BUCKET_INTERVAL_UNITS=" + py_to_str(self.bucket_interval_units) + ","
		if self.output_interval != "": 
			desc += "OUTPUT_INTERVAL=" + py_to_str(self.output_interval) + ","
		if self.output_interval_units != self.OutputIntervalUnits.SECONDS: 
			desc += "OUTPUT_INTERVAL_UNITS=" + py_to_str(self.output_interval_units) + ","
		if self.is_running_aggr != False: 
			desc += "IS_RUNNING_AGGR=" + py_to_str(self.is_running_aggr) + ","
		if self.bucket_time != self.BucketTime.BUCKET_END: 
			desc += "BUCKET_TIME=" + py_to_str(self.bucket_time) + ","
		if self.bucket_end_criteria != "": 
			desc += "BUCKET_END_CRITERIA=" + py_to_str(self.bucket_end_criteria) + ","
		if self.boundary_tick_bucket != self.BoundaryTickBucket.NEW: 
			desc += "BOUNDARY_TICK_BUCKET=" + py_to_str(self.boundary_tick_bucket) + ","
		if self.partial_bucket_handling != self.PartialBucketHandling.AS_SEPARATE_BUCKET: 
			desc += "PARTIAL_BUCKET_HANDLING=" + py_to_str(self.partial_bucket_handling) + ","
		if self.group_by != "": 
			desc += "GROUP_BY=" + py_to_str(self.group_by) + ","
		if self.groups_to_display != self.GroupsToDisplay.ALL: 
			desc += "GROUPS_TO_DISPLAY=" + py_to_str(self.groups_to_display) + ","
		if self.bucket_end_per_group != False: 
			desc += "BUCKET_END_PER_GROUP=" + py_to_str(self.bucket_end_per_group) + ","
		if self.append_output_field_name != True: 
			desc += "APPEND_OUTPUT_FIELD_NAME=" + py_to_str(self.append_output_field_name) + ","
		if self.show_intervals_without_ticks != True: 
			desc += "SHOW_INTERVALS_WITHOUT_TICKS=" + py_to_str(self.show_intervals_without_ticks) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


Compute = ComputeEp


class ShowSymbolNameInDb(_graph_components.EpBase):
	"""
		

SHOW_SYMBOL_NAME_IN_DB

Type: Other

Description: Adds the SYMBOL_NAME_IN_DB
field to input ticks, indicating the symbol name of the tick in the
database.

Python
class name:&nbsp;ShowSymbolNameInDb

Input: None

Output: A time series of ticks.

Parameters: None

Examples:

SHOW_SYMBOL_NAME_IN_DB()

See the SHOW_SYMBOL_NAME_IN_DB
example in OTHER_EXAMPLES.otq.


	"""
	class Parameters:
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = []
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["stack_info", "_used_strings"]

	def __init__(self):
		_graph_components.EpBase.__init__(self, "SHOW_SYMBOL_NAME_IN_DB")
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	@staticmethod
	def _get_name():
		return "SHOW_SYMBOL_NAME_IN_DB"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class StopQuery(_graph_components.EpBase):
	"""
		

STOP_QUERY

Type: Other

Description: Cancels a query by its global id. If a query
generates subqueries (they get the same global id) that run on other
hosts, STOP_QUERY will terminate them all.

QUERY_ID can be obtained from the Server Monitor which is embedded
in OneTickDisplay or from tick_server_monitor.exe.
Permission checks will be done on each contacted server.

Python
class name:&nbsp;StopQuery

Input: None

Output: A single tick representing the result of termination

Parameters:


  QUERY_ID - Global query id that will be used
to cancel the query on each server.

Examples:

STOP_QUERY(QUERY_ID="Rigel.176748.20140429094835.040.1")
This will try to stop the query with the given global id on each
accessible server, recursively.

See the STOP_QUERY example in OTHER_EXAMPLES.otq.


	"""
	class Parameters:
		query_id = "QUERY_ID"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["query_id"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["query_id", "_default_query_id", "stack_info", "_used_strings"]

	def __init__(self, query_id=-1):
		_graph_components.EpBase.__init__(self, "STOP_QUERY")
		self._default_query_id = -1
		self.query_id = query_id
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_query_id(self, value):
		self.query_id = value
		return self

	@staticmethod
	def _get_name():
		return "STOP_QUERY"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.query_id != -1: 
			desc += "QUERY_ID=" + py_to_str(self.query_id) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class ShowOtqList(_graph_components.EpBase):
	"""
		

SHOW_OTQ_LIST

Type: Other

Description: This EP lists local or remote OTQs depending on
OTQ_LOCALITY parameter. Search is performed in folders provided by
OTQ_FILE_PATH config variable.

Note that OTQ_LOCALITY is relative to host where the query gets
executed. Use of LOCAL:: for symbol name will trigger local execution.

It must be the first (source) EP on the graph.

Python
class name:&nbsp;ShowOtqList

Input: None.

Output: List of OTQs.

Parameters:


  OTQ_LOCALITY (enumerated type)
    Specifies locality of OTQs to list. Possible values are:

    
      LOCAL - List only local OTQs.
      REMOTE - List only remote OTQs.
    
    Default: LOCAL

  
  SHOW_INDIVIDUAL_QUERIES
(Boolean)
    Specifies if all individual queries in otq files must be shown.
Default: FALSE

  
  REMOTE_SERVERS_AND_DBS
(string)
    A comma-separated list of remote servers and DB names to
populate OTQs from, in addition to all servers configured in the
locator file. This parameter has its effect only when OTQ_LOCALITY is
set to REMOTE.
Default: EMPTY

  

Examples: List all local OTQs (LOCAL:: symbol name must be
provided):

SHOW_OTQ_LIST(LOCAL , FALSE)

See the SHOW_OTQ_LIST in OTHER_EXAMPLES.otq.


	"""
	class Parameters:
		otq_locality = "OTQ_LOCALITY"
		show_individual_queries = "SHOW_INDIVIDUAL_QUERIES"
		remote_servers_and_dbs = "REMOTE_SERVERS_AND_DBS"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["otq_locality", "show_individual_queries", "remote_servers_and_dbs"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["otq_locality", "_default_otq_locality", "show_individual_queries", "_default_show_individual_queries", "remote_servers_and_dbs", "_default_remote_servers_and_dbs", "stack_info", "_used_strings"]

	class OtqLocality:
		LOCAL = "LOCAL"
		REMOTE = "REMOTE"

	def __init__(self, otq_locality=OtqLocality.LOCAL, show_individual_queries=False, remote_servers_and_dbs=""):
		_graph_components.EpBase.__init__(self, "SHOW_OTQ_LIST")
		self._default_otq_locality = type(self).OtqLocality.LOCAL
		self.otq_locality = otq_locality
		self._default_show_individual_queries = False
		self.show_individual_queries = show_individual_queries
		self._default_remote_servers_and_dbs = ""
		self.remote_servers_and_dbs = remote_servers_and_dbs
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_otq_locality(self, value):
		self.otq_locality = value
		return self

	def set_show_individual_queries(self, value):
		self.show_individual_queries = value
		return self

	def set_remote_servers_and_dbs(self, value):
		self.remote_servers_and_dbs = value
		return self

	@staticmethod
	def _get_name():
		return "SHOW_OTQ_LIST"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.otq_locality != self.OtqLocality.LOCAL: 
			desc += "OTQ_LOCALITY=" + py_to_str(self.otq_locality) + ","
		if self.show_individual_queries != False: 
			desc += "SHOW_INDIVIDUAL_QUERIES=" + py_to_str(self.show_individual_queries) + ","
		if self.remote_servers_and_dbs != "": 
			desc += "REMOTE_SERVERS_AND_DBS=" + py_to_str(self.remote_servers_and_dbs) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class WriteToOnetickDb(_graph_components.EpBase):
	"""
		

WRITE_TO_ONETICK_DB

Type: OutputAdapter

Description: Loads data into the OneTick archive, accelerator
or memory databases. Database context and name are set via input
parameters. If the DATE parameter is set, an archive is created
for this day and tick timestamps are checked for. If DATE is
not set, ticks are written to accelerator database, if accelerator
database is not defined ticks are written to memory database.
Timestamps should fall into the time range determined by
DAY_BOUNDARY_TZ and DAY_BOUNDARY_OFFSET_HHMMSS database properties.
When writing to memory database, for CEP queries start and end times of
query should be in current or future days, for non-CEP queries start
time should be in current date. The OUT _OF_RANGE_TICK_ACTION
parameter indicates the action that should be taken to out-of-range
ticks. This event processor (EP) also propagates input ticks if the PROPAGATE_TICKS
parameter is set to TRUE.

The symbol name and tick type are retrieved from fields mentioned in
SYMBOL_NAME_FIELD and TICK_TYPE_FIELD parameters. If
they are not set, symbol name and tick type are used, and if they are
empty (which can, for example, happen after merging), for symbol name,
SYMBOL_NAME field content is retrieved and for tick type, TICK_TYPE
field content is retrieved.

In order to use the WRITE_TO_ONETICK_DB EP, every database would
have to be defined in the
&lt;installdir&gt;/client_data/config/locator.server of the tick server
process before a user can write to it. For example:

&lt;db id="MYDB" symbology="TICKER_CUSTOM" day_boundary_tz="EST5EDT" is_default="false"&gt;  &lt;LOCATIONS&gt; &lt;location access_method="file" location="C:/OMD/data/mydb" start_time="19910101000000" end_time="20201231000000" /&gt;  &lt;/LOCATIONS&gt;&lt;/db&gt;
Once defined, restart the tick server process to pick up the new
database from the locator or instruct the tick server to reread the
locator by using refresh_locator.exe
-server localhost:40001 (or whatever the tick server port is if
not the default, 40001). Note that for refresh_locator.exe to be able
to send the refresh command, the following line, ALLOW_REMOTE_CONTROL=yes, must be listed
in &lt;installdir&gt;/client_data/config/one_tick_config.txt. Once you
restart the tick server you will be able to use refresh_locator.exe.

Access control also needs to be defined for writing to the db. The
following line, ACCESS_CONTROL_FILE=c:\\omd\\client_data\\config\\access.txt,
would also need to be listed in one_tick_config.txt. The access file
has the user / role mappings plus database level permissions:

&lt;roles&gt;  &lt;role name="Users"&gt;    &lt;user name="David" /&gt;  &lt;/role&gt;  ... &lt;/roles&gt;&lt;databases&gt;  &lt;db ID="MYDB" read_access="true" write_access="false"&gt;    &lt;allow role="Users" read_access="true" write_access="true" /&gt;  &lt;/db&gt;&lt;/databases&gt;
In the above example, David is a part of the Users role. The MYDB
database by default is readable by everyone, but not writable. As the
only member of Users, David is the only person with write access to
MYDB. As with locators, there is a command to refresh the access
control/entitlements: refresh_access_control.exe
-localhost:40001. This has same dependency as refresh_locator in
that the one_tick_config.txt used by tick server must have ALLOW_REMOTE_CONTROL
enabled.

Python
class name:&nbsp;WriteToOnetickDb

Input: A time series of ticks.

Output: A time series of ticks or none.

Parameters:


  PROPAGATE_TICKS (Boolean)
    Switches propagation of the ticks. If set to true, ticks will be
propagated.
Default: true

  
  CONTEXT (string)
    The server context used to look up the database.
Default: DEFAULT

  
  DATABASE (string)
    The name of the database into which data should be loaded.

  
  DATE (YYYYMMDD)
    The archive database date.

  
  OUT_OF_RANGE_TICK_ACTION
(Enum)
    The action that should be taken for out of range ticks. Possible
values are "IGNORE", "EXCEPTION", and "LOAD".
Default: "IGNORE"

  
  SYMBOL_NAME_FIELD (string)
    The name of the field which contains the symbol name. If this
parameter is not set, then ticks symbol name is used. If it is empty,
an attempt is made to retrieve the symbol name from the field named
SYMBOL_NAME.

  
  TICK_TYPE_FIELD (string)
    The name of the field that contains the tick type. If this
parameter is not set, the tick type is used, and if it is empty, an
attempt is made to retrieve the tick type from the field named
TICK_TYPE.

  
  TIMESTAMP_FIELD (datetime)
    The name of the field that contains the timestamp. If this
parameter is not set, the ticks timestamp is used.

  
  CORRECTION_TYPE_FIELD (string)
    The name of the field that contains the correction type. This
field will be removed. If this parameter is not set, no corrections
will be submitted.

    Value of this field will determine if the tick is a correction
and if so which type of correction it is:

    
      1 - cancellation
      2 - correction
      8 - insertion
      other value - regular tick
    
  
  KEEP_SYMBOL_NAME_AND_TICK_TYPE
(Boolean)
    If this parameter is set to false, then fields containing symbol
name and tick type are removed. These field names are read from SYMBOL_NAME_FIELD
and TICK_TYPE_FIELD input parameters. If they are not set, the
default field names SYMBOL_NAME and TICK_TYPE are used.
Default: true

  
  KEEP_TIMESTAMP_FIELD (Boolean)
    If this parameter is set to false, then field containing
timestamp is removed. This field name is read from TIMESTAMP_FIELD
input parameter.
Default: true

  
  APPEND_MODE (Boolean)
    If set to true, new symbols are added to the existing archive
and existing ones can be modified (append new ticks, modify existing
ticks). This option is not valid for accelerator databases.
Default: false

  
  REPLACE_EXISTING_TIME_SERIES
(Boolean)
    In append mode, setting this option to true, instructs
the loader to replace existing time series, instead of appending to
them. Other time series will remain unchanged. If only data quality
changes are submitted for time series, symbol file criteria will be
used. That is, if PERF.ADD_SYMBOLS_WITHOUT_TICKS_TO_SYMBOL_LIST
configuration parameter (see config_variables.html) is set and causes
symbols without ticks to appear in symbol file, then it will be
replaced, and thus all existing ticks will be gone. Without that
option, the symbol will stay unchanged, and the newly submitted data
quality events will be discarded.
Default: false

  
  ALLOW_CONCURRENT_WRITE
(Boolean)
    Allows different queries running on the same tick server to load
concurrently into the same database. When ALLOW_CONCURRENT_WRITE is set
to true, tick server disables errors/warnings propagation to clients.
In such cases tick server log should be examined to find out whether
the load was completed successfully or not.

    When ALLOW_CONCURRENT_WRITE is set to false, client will be
notified about errors/warnings through registered query level callback.
For tickdb_query.exe query level callback can be specified using
-add_query_level_callback parameter.

    Currently, propagation of errors/warnings to clients is
supported only when writing to archive dbs.
Default: false

  
  USE_CONTEXT_OF_QUERY (Boolean)
    If this parameter is set to true and the CONTEXT parameter is
not set, EP uses the context of the query instead of the default value
of the CONTEXT parameter.
Default: false

  
  PRESERVE_FIELD_ORDER (Boolean)
    If set to true, the fields are written in the same order as in
input time-series.
Default: false

  

Example:

Loads data into the TEST database for date 20031201, ignores out of
range ticks, and propagates ticks:

WRITE_TO_ONETICK_DB(true,DEFAULT,TEST,20031201,IGNORE,,,true)


	"""
	class Parameters:
		propagate_ticks = "PROPAGATE_TICKS"
		context = "CONTEXT"
		database = "DATABASE"
		date = "DATE"
		out_of_range_tick_action = "OUT_OF_RANGE_TICK_ACTION"
		symbol_name_field = "SYMBOL_NAME_FIELD"
		tick_type_field = "TICK_TYPE_FIELD"
		correction_type_field = "CORRECTION_TYPE_FIELD"
		timestamp_field = "TIMESTAMP_FIELD"
		keep_symbol_name_and_tick_type = "KEEP_SYMBOL_NAME_AND_TICK_TYPE"
		keep_timestamp_field = "KEEP_TIMESTAMP_FIELD"
		append_mode = "APPEND_MODE"
		replace_existing_time_series = "REPLACE_EXISTING_TIME_SERIES"
		allow_concurrent_write = "ALLOW_CONCURRENT_WRITE"
		use_context_of_query = "USE_CONTEXT_OF_QUERY"
		preserve_field_order = "PRESERVE_FIELD_ORDER"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["propagate_ticks", "context", "database", "date", "out_of_range_tick_action", "symbol_name_field", "tick_type_field", "correction_type_field", "timestamp_field", "keep_symbol_name_and_tick_type", "keep_timestamp_field", "append_mode", "replace_existing_time_series", "allow_concurrent_write", "use_context_of_query", "preserve_field_order"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["propagate_ticks", "_default_propagate_ticks", "context", "_default_context", "database", "_default_database", "date", "_default_date", "out_of_range_tick_action", "_default_out_of_range_tick_action", "symbol_name_field", "_default_symbol_name_field", "tick_type_field", "_default_tick_type_field", "correction_type_field", "_default_correction_type_field", "timestamp_field", "_default_timestamp_field", "keep_symbol_name_and_tick_type", "_default_keep_symbol_name_and_tick_type", "keep_timestamp_field", "_default_keep_timestamp_field", "append_mode", "_default_append_mode", "replace_existing_time_series", "_default_replace_existing_time_series", "allow_concurrent_write", "_default_allow_concurrent_write", "use_context_of_query", "_default_use_context_of_query", "preserve_field_order", "_default_preserve_field_order", "stack_info", "_used_strings"]

	class OutOfRangeTickAction:
		EXCEPTION = "EXCEPTION"
		IGNORE = "IGNORE"
		LOAD = "LOAD"

	def __init__(self, propagate_ticks=True, context="DEFAULT", database="", date="", out_of_range_tick_action=OutOfRangeTickAction.IGNORE, symbol_name_field="", tick_type_field="", correction_type_field="", timestamp_field="", keep_symbol_name_and_tick_type=True, keep_timestamp_field=True, append_mode=False, replace_existing_time_series=False, allow_concurrent_write=False, use_context_of_query=False, preserve_field_order=False):
		_graph_components.EpBase.__init__(self, "WRITE_TO_ONETICK_DB")
		self._default_propagate_ticks = True
		self.propagate_ticks = propagate_ticks
		self._default_context = "DEFAULT"
		self.context = context
		self._default_database = ""
		self.database = database
		self._default_date = ""
		self.date = date
		self._default_out_of_range_tick_action = type(self).OutOfRangeTickAction.IGNORE
		self.out_of_range_tick_action = out_of_range_tick_action
		self._default_symbol_name_field = ""
		self.symbol_name_field = symbol_name_field
		self._default_tick_type_field = ""
		self.tick_type_field = tick_type_field
		self._default_correction_type_field = ""
		self.correction_type_field = correction_type_field
		self._default_timestamp_field = ""
		self.timestamp_field = timestamp_field
		self._default_keep_symbol_name_and_tick_type = True
		self.keep_symbol_name_and_tick_type = keep_symbol_name_and_tick_type
		self._default_keep_timestamp_field = True
		self.keep_timestamp_field = keep_timestamp_field
		self._default_append_mode = False
		self.append_mode = append_mode
		self._default_replace_existing_time_series = False
		self.replace_existing_time_series = replace_existing_time_series
		self._default_allow_concurrent_write = False
		self.allow_concurrent_write = allow_concurrent_write
		self._default_use_context_of_query = False
		self.use_context_of_query = use_context_of_query
		self._default_preserve_field_order = False
		self.preserve_field_order = preserve_field_order
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_propagate_ticks(self, value):
		self.propagate_ticks = value
		return self

	def set_context(self, value):
		self.context = value
		return self

	def set_database(self, value):
		self.database = value
		return self

	def set_date(self, value):
		self.date = value
		return self

	def set_out_of_range_tick_action(self, value):
		self.out_of_range_tick_action = value
		return self

	def set_symbol_name_field(self, value):
		self.symbol_name_field = value
		return self

	def set_tick_type_field(self, value):
		self.tick_type_field = value
		return self

	def set_correction_type_field(self, value):
		self.correction_type_field = value
		return self

	def set_timestamp_field(self, value):
		self.timestamp_field = value
		return self

	def set_keep_symbol_name_and_tick_type(self, value):
		self.keep_symbol_name_and_tick_type = value
		return self

	def set_keep_timestamp_field(self, value):
		self.keep_timestamp_field = value
		return self

	def set_append_mode(self, value):
		self.append_mode = value
		return self

	def set_replace_existing_time_series(self, value):
		self.replace_existing_time_series = value
		return self

	def set_allow_concurrent_write(self, value):
		self.allow_concurrent_write = value
		return self

	def set_use_context_of_query(self, value):
		self.use_context_of_query = value
		return self

	def set_preserve_field_order(self, value):
		self.preserve_field_order = value
		return self

	@staticmethod
	def _get_name():
		return "WRITE_TO_ONETICK_DB"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.propagate_ticks != True: 
			desc += "PROPAGATE_TICKS=" + py_to_str(self.propagate_ticks) + ","
		if self.context != "DEFAULT": 
			desc += "CONTEXT=" + py_to_str(self.context) + ","
		if self.database != "": 
			desc += "DATABASE=" + py_to_str(self.database) + ","
		if self.date != "": 
			desc += "DATE=" + py_to_str(self.date) + ","
		if self.out_of_range_tick_action != self.OutOfRangeTickAction.IGNORE: 
			desc += "OUT_OF_RANGE_TICK_ACTION=" + py_to_str(self.out_of_range_tick_action) + ","
		if self.symbol_name_field != "": 
			desc += "SYMBOL_NAME_FIELD=" + py_to_str(self.symbol_name_field) + ","
		if self.tick_type_field != "": 
			desc += "TICK_TYPE_FIELD=" + py_to_str(self.tick_type_field) + ","
		if self.correction_type_field != "": 
			desc += "CORRECTION_TYPE_FIELD=" + py_to_str(self.correction_type_field) + ","
		if self.timestamp_field != "": 
			desc += "TIMESTAMP_FIELD=" + py_to_str(self.timestamp_field) + ","
		if self.keep_symbol_name_and_tick_type != True: 
			desc += "KEEP_SYMBOL_NAME_AND_TICK_TYPE=" + py_to_str(self.keep_symbol_name_and_tick_type) + ","
		if self.keep_timestamp_field != True: 
			desc += "KEEP_TIMESTAMP_FIELD=" + py_to_str(self.keep_timestamp_field) + ","
		if self.append_mode != False: 
			desc += "APPEND_MODE=" + py_to_str(self.append_mode) + ","
		if self.replace_existing_time_series != False: 
			desc += "REPLACE_EXISTING_TIME_SERIES=" + py_to_str(self.replace_existing_time_series) + ","
		if self.allow_concurrent_write != False: 
			desc += "ALLOW_CONCURRENT_WRITE=" + py_to_str(self.allow_concurrent_write) + ","
		if self.use_context_of_query != False: 
			desc += "USE_CONTEXT_OF_QUERY=" + py_to_str(self.use_context_of_query) + ","
		if self.preserve_field_order != False: 
			desc += "PRESERVE_FIELD_ORDER=" + py_to_str(self.preserve_field_order) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class ReadFromRaw(_graph_components.EpBase):
	"""
		

READ_FROM_RAW

Type: Other

Description: Reads the raw data as ticks. It supports both
native raw data format and RT format. For the native format the ticks
correspond to the tick descriptor in the raw data file. For RT format,
the ticks have the following fields:


  FID
  LONG_VAL
  DOUBLE_VAL
  STRING_VAL
  REAL_STRING_VALUE_SIZE
  MESSAGE_TYPE
  DELIMITER

For RT format, the CONVERT_ALL_VALUES_TO_STRINGS
parameter is supported, in which case the output would become FID, STRING_VAL, REAL_STRING_VALUE_SIZE,
MESSAGE_TYPE, DELIMITER.
The DELIMITER field indicates the end of the message. The end of each
message is set to 'D'. For this tick,
other fields will be the defaults: NAN
for double, 0 for long, "" for string, and FID=0.

Also normalized output is allowed for RT format. For producing this,
normalization configuration files should be specified in the NORMALIZATION_CONFIG_DIR input parameter.

The database name is taken from symbols. For executing all symbols, &lt;db_name&gt;:: symbol should be
queried. If all symbols are queried, then either the MOUNT or FILENAME
parameter should be set with the proper value. In this case, SYMBOL_NAME and TICK_TYPE
fields will be added to the tick, which will contain the symbol name
and tick type, respectively. For native data, the CORRECTING_FLAG field will be added for
all cases, which will be non-empty for correction ticks and will
contain 'X' for cancel, 'C' for correct, and 'I' for insert messages.

Python
class name:&nbsp;ReadFromRaw

Input: None.

Output: A time series of ticks.

Parameters:


  RAW_DATA_TYPE (enum)
    The format of data. Possible values are native and RT.
Default: native

  
  MOUNT (string)
    The name of the mount. The value is optional -- all mounts will
be read if not specified. Note that either the MOUNT or FILENAME
input parameter can be specified.

  
  LOCATION (string)
    The raw data location within the database.
Default: PRIMARY

  
  FILENAME (string)
    The raw data file name, including the entire path. This value is
optional. Note that either the MOUNT
or FILENAME input parameter can be
specified.

  
  MAX_SYMBOL_NAME_SIZE (numeric)
    When the queried symbol is &lt;db_name&gt;::
then all symbols are queried and the SYMBOL_NAME
field is added to the tick. This parameter defines the size of this
field.
Default: 40

  
  TICK_TYPES (string)
    This can contain a comma-separated list of tick types or the
special value ALL_TICK_TYPES. It is
used only when the symbol name is in the form &lt;db_name&gt;::. When it is empty only
ticks with a bound tick type will be propagated. When a comma-separated
list is set, ticks with the selected tick types, and ALL_TICK_TYPES ticks for all tick types,
will be propagated.

  
  FIELDS (string)
    This can contain a comma-separated list of field names or the
special value ONLY_TIMESTAMP. It is
used only when the symbol name is in the form &lt;db_name&gt;::. When it is empty all
fields will be propagated. If there is a specified list, only selected
fields from that list, as well as SYMBOL_NAME,
    TICK_TYPE, and CORRECTING_FLAG fields, will be
propagated. If ONLY_TIMESTAMP is
selected only SYMBOL_NAME, TICK_TYPE, and CORRECTING_FLAG
fields will be propagated.

  
  CONVERT_ALL_VALUES_TO_STRINGS
(Boolean)
    Used only for RT format, when normalization is not chosen. If it
is set to "true" for RT, the output
tick has the following fields:

    
      FID, STRING_VAL
      REAL_STRING_VALUE_SIZE
      MESSAGE_TYPE
      DELIMITER
    
Default: false
  MAX_STRING_VALUE_SIZE (numeric)
    Used only for RT format, when normalization is not chosen.
Defines the max size for the STRING_VAL
field. The real size of the string value is shown in REAL_STRING_VALUE_SIZE field.
Default: 64

  
  DOUBLE_PRECISION (numeric)
    Used only for RT format, when normalization is not chosen and CONVERT_ALL_VALUES_TO_STRINGS is set to
true. Set the precision for DOUBLE_VAL
field.
Default: 6

  
  NORMALIZATION_CONFIG_DIR
(string)
    Used only for RT format. If it is set, normalization is done for
RT messages and this parameter specifies the location of normalization
configuration files.

  
  PREPEND_IMAGE (Boolean)
    Used only for RT format, when normalization is not chosen. If
set to "true" and the image does not fall into the queried interval, it
prepends the current image and status before the message.
Default: false

  
  SHOW_PER_MOUNT_EVENTS (Boolean)
    Used only for RT format when normalization is not chosen. If set
to true, the _PER_MOUNT_EVENTS symbol name should be
added in the symbols list and all messages without the symbol name will
be shown under this symbol name. If this parameter is chosen and _PER_MOUNT_EVENTS is not added, then an
error will appear. For all symbols being queried, when the symbol name
is &lt;db_name&gt;::, _PER_MOUNT_EVENTS should not be mentioned,
and in the output, _PER_MOUNT_EVENTS
will be added automatically for messages without symbols in the SYMBOL_NAME field. This parameter can be
used only when either the MOUNT or FILENAME parameter is set.
Default: false

  
  POLLING_INTERVAL (numeric)
    Used in cep queries. Specifies polling interval(in milliseconds)
for the underlying raw data reader.
Default: raw source specific(typically
100msec)

  
  MAX_INACTIVITY_INTERVAL
(numeric)
    Specifies maximal mount inactivity interval in seconds.
Default: 5

  
  ADDED_FIELD_NAME_SUFFIX
(string)
    The suffix to add to the names of additional fields (that is, SYMBOL_NAME, TICK_TYPE,
and CORRECTING_FLAG).
Default: empty

  
SINGLE_FID_MSG_FORMAT_FILE
    (string)
        Can only be specified together with MSG_TYPE. If both are specified, 
            we treat the raw data to have "full msg in single fid"
            format (e.g.kafka_native_collector_realtime with convert_msgs_to_ticks set to false, or a custom collector that creates raw data files with full message in a single fid).
            Currently supported formats are CSV, JSON, XML, PROTOBUF, AVRO, and SBE, with following format file descriptions correspondingly: ascii format
            description file, json format
            description file, xml format
            description file, protobuf format
            description file, avro format description
            file and sbe format description file.
    
MSG_TYPE
  (enum)
        Can only be specified together with SINGLE_FID_MSG_FORMAT_FILE. If both are specified, we treat the raw data to have "full msg in single fid" format.
          Possible values are CSV, JSON, XML, PROTOBUF and AVRO.

  
Access control:

In order to use this event processor, access control must be
configured (refer to the OneTick Installation and Administration Guide).

Examples: Reads raw data files from PRIMARY location for mount1:

READ_FROM_RAW(MOUNT=mount1)

See the READ_FROM_RAW example in OTHER_EXAMPLES.otq.


	"""
	class Parameters:
		raw_data_type = "RAW_DATA_TYPE"
		mount = "MOUNT"
		location = "LOCATION"
		filename = "FILENAME"
		max_symbol_name_size = "MAX_SYMBOL_NAME_SIZE"
		tick_types_field = "TICK_TYPES"
		fields = "FIELDS"
		convert_all_values_to_strings = "CONVERT_ALL_VALUES_TO_STRINGS"
		max_string_value_size = "MAX_STRING_VALUE_SIZE"
		double_precision = "DOUBLE_PRECISION"
		normalization_config_dir = "NORMALIZATION_CONFIG_DIR"
		prepend_image = "PREPEND_IMAGE"
		show_per_mount_events = "SHOW_PER_MOUNT_EVENTS"
		polling_interval = "POLLING_INTERVAL"
		max_inactivity_interval = "MAX_INACTIVITY_INTERVAL"
		added_field_name_suffix = "ADDED_FIELD_NAME_SUFFIX"
		ignore_missing_images = "IGNORE_MISSING_IMAGES"
		single_fid_msg_format_file = "SINGLE_FID_MSG_FORMAT_FILE"
		msg_type = "MSG_TYPE"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["raw_data_type", "mount", "location", "filename", "max_symbol_name_size", "tick_types_field", "fields", "convert_all_values_to_strings", "max_string_value_size", "double_precision", "normalization_config_dir", "prepend_image", "show_per_mount_events", "polling_interval", "max_inactivity_interval", "added_field_name_suffix", "ignore_missing_images", "single_fid_msg_format_file", "msg_type"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["raw_data_type", "_default_raw_data_type", "mount", "_default_mount", "location", "_default_location", "filename", "_default_filename", "max_symbol_name_size", "_default_max_symbol_name_size", "tick_types_field", "_default_tick_types_field", "fields", "_default_fields", "convert_all_values_to_strings", "_default_convert_all_values_to_strings", "max_string_value_size", "_default_max_string_value_size", "double_precision", "_default_double_precision", "normalization_config_dir", "_default_normalization_config_dir", "prepend_image", "_default_prepend_image", "show_per_mount_events", "_default_show_per_mount_events", "polling_interval", "_default_polling_interval", "max_inactivity_interval", "_default_max_inactivity_interval", "added_field_name_suffix", "_default_added_field_name_suffix", "ignore_missing_images", "_default_ignore_missing_images", "single_fid_msg_format_file", "_default_single_fid_msg_format_file", "msg_type", "_default_msg_type", "stack_info", "_used_strings"]

	class RawDataType:
		NATIVE = "native"
		RT = "rt"

	class MsgType:
		EMPTY = ""
		AVRO = "AVRO"
		CSV = "CSV"
		JSON = "JSON"
		PROTOBUF = "PROTOBUF"
		SBE = "SBE"
		XML = "XML"

	def __init__(self, raw_data_type=RawDataType.NATIVE, mount="", location="PRIMARY", filename="", max_symbol_name_size=40, tick_types_field="", fields="", convert_all_values_to_strings=False, max_string_value_size=64, double_precision=6, normalization_config_dir="", prepend_image=False, show_per_mount_events=False, polling_interval=-1, max_inactivity_interval=5, added_field_name_suffix="", ignore_missing_images=False, single_fid_msg_format_file="", msg_type=MsgType.EMPTY):
		_graph_components.EpBase.__init__(self, "READ_FROM_RAW")
		self._default_raw_data_type = type(self).RawDataType.NATIVE
		self.raw_data_type = raw_data_type
		self._default_mount = ""
		self.mount = mount
		self._default_location = "PRIMARY"
		self.location = location
		self._default_filename = ""
		self.filename = filename
		self._default_max_symbol_name_size = 40
		self.max_symbol_name_size = max_symbol_name_size
		self._default_tick_types_field = ""
		self.tick_types_field = tick_types_field
		self._default_fields = ""
		self.fields = fields
		self._default_convert_all_values_to_strings = False
		self.convert_all_values_to_strings = convert_all_values_to_strings
		self._default_max_string_value_size = 64
		self.max_string_value_size = max_string_value_size
		self._default_double_precision = 6
		self.double_precision = double_precision
		self._default_normalization_config_dir = ""
		self.normalization_config_dir = normalization_config_dir
		self._default_prepend_image = False
		self.prepend_image = prepend_image
		self._default_show_per_mount_events = False
		self.show_per_mount_events = show_per_mount_events
		self._default_polling_interval = -1
		self.polling_interval = polling_interval
		self._default_max_inactivity_interval = 5
		self.max_inactivity_interval = max_inactivity_interval
		self._default_added_field_name_suffix = ""
		self.added_field_name_suffix = added_field_name_suffix
		self._default_ignore_missing_images = False
		self.ignore_missing_images = ignore_missing_images
		self._default_single_fid_msg_format_file = ""
		self.single_fid_msg_format_file = single_fid_msg_format_file
		self._default_msg_type = type(self).MsgType.EMPTY
		self.msg_type = msg_type
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_raw_data_type(self, value):
		self.raw_data_type = value
		return self

	def set_mount(self, value):
		self.mount = value
		return self

	def set_location(self, value):
		self.location = value
		return self

	def set_filename(self, value):
		self.filename = value
		return self

	def set_max_symbol_name_size(self, value):
		self.max_symbol_name_size = value
		return self

	def set_tick_types_field(self, value):
		self.tick_types_field = value
		return self

	def set_fields(self, value):
		self.fields = value
		return self

	def set_convert_all_values_to_strings(self, value):
		self.convert_all_values_to_strings = value
		return self

	def set_max_string_value_size(self, value):
		self.max_string_value_size = value
		return self

	def set_double_precision(self, value):
		self.double_precision = value
		return self

	def set_normalization_config_dir(self, value):
		self.normalization_config_dir = value
		return self

	def set_prepend_image(self, value):
		self.prepend_image = value
		return self

	def set_show_per_mount_events(self, value):
		self.show_per_mount_events = value
		return self

	def set_polling_interval(self, value):
		self.polling_interval = value
		return self

	def set_max_inactivity_interval(self, value):
		self.max_inactivity_interval = value
		return self

	def set_added_field_name_suffix(self, value):
		self.added_field_name_suffix = value
		return self

	def set_ignore_missing_images(self, value):
		self.ignore_missing_images = value
		return self

	def set_single_fid_msg_format_file(self, value):
		self.single_fid_msg_format_file = value
		return self

	def set_msg_type(self, value):
		self.msg_type = value
		return self

	@staticmethod
	def _get_name():
		return "READ_FROM_RAW"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.raw_data_type != self.RawDataType.NATIVE: 
			desc += "RAW_DATA_TYPE=" + py_to_str(self.raw_data_type) + ","
		if self.mount != "": 
			desc += "MOUNT=" + py_to_str(self.mount) + ","
		if self.location != "PRIMARY": 
			desc += "LOCATION=" + py_to_str(self.location) + ","
		if self.filename != "": 
			desc += "FILENAME=" + py_to_str(self.filename) + ","
		if self.max_symbol_name_size != 40: 
			desc += "MAX_SYMBOL_NAME_SIZE=" + py_to_str(self.max_symbol_name_size) + ","
		if self.tick_types_field != "": 
			desc += "TICK_TYPES_FIELD=" + py_to_str(self.tick_types_field) + ","
		if self.fields != "": 
			desc += "FIELDS=" + py_to_str(self.fields) + ","
		if self.convert_all_values_to_strings != False: 
			desc += "CONVERT_ALL_VALUES_TO_STRINGS=" + py_to_str(self.convert_all_values_to_strings) + ","
		if self.max_string_value_size != 64: 
			desc += "MAX_STRING_VALUE_SIZE=" + py_to_str(self.max_string_value_size) + ","
		if self.double_precision != 6: 
			desc += "DOUBLE_PRECISION=" + py_to_str(self.double_precision) + ","
		if self.normalization_config_dir != "": 
			desc += "NORMALIZATION_CONFIG_DIR=" + py_to_str(self.normalization_config_dir) + ","
		if self.prepend_image != False: 
			desc += "PREPEND_IMAGE=" + py_to_str(self.prepend_image) + ","
		if self.show_per_mount_events != False: 
			desc += "SHOW_PER_MOUNT_EVENTS=" + py_to_str(self.show_per_mount_events) + ","
		if self.polling_interval != -1: 
			desc += "POLLING_INTERVAL=" + py_to_str(self.polling_interval) + ","
		if self.max_inactivity_interval != 5: 
			desc += "MAX_INACTIVITY_INTERVAL=" + py_to_str(self.max_inactivity_interval) + ","
		if self.added_field_name_suffix != "": 
			desc += "ADDED_FIELD_NAME_SUFFIX=" + py_to_str(self.added_field_name_suffix) + ","
		if self.ignore_missing_images != False: 
			desc += "IGNORE_MISSING_IMAGES=" + py_to_str(self.ignore_missing_images) + ","
		if self.single_fid_msg_format_file != "": 
			desc += "SINGLE_FID_MSG_FORMAT_FILE=" + py_to_str(self.single_fid_msg_format_file) + ","
		if self.msg_type != self.MsgType.EMPTY: 
			desc += "MSG_TYPE=" + py_to_str(self.msg_type) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class TickGenerator(_graph_components.EpBase):
	"""
		

TICK_GENERATOR

Type: Other

Description: A data source EP that generates a time series of
ticks constructing a tick for each bucket interval using specified
field names, field types, and constant expressions to compute their
values.

Python
class name:&nbsp;TickGenerator

Input: None

Output: A time series of ticks, one output tick for each
bucket interval.

Parameters:


  BUCKET_INTERVAL (double or NEVER)
    Determines the length (seconds) of each time interval. The EP
generates a tick (or multiple ticks, depending on value of parameter
NUM_TICKS_PER_TIMESTAMP) for each time interval beginning at the start
time of the query. Note that the length of the last interval can be
shorter if _END_TIME - _START_TIME is not divisible by the BUCKET_INTERVAL.
If 0 is specified, the whole query time is treated as one
interval. If NEVER is specified, no ticks will be generated. NEVER
can be used to comment out some part of the graph query.
Note that if start time of the query is equal to its end time it will
be treated as an interval.
Default: NEVER

  
  BUCKET_INTERVAL_UNITS (enum)
    Specifies the units for the size of the bucket interval.
The units can be either SECONDS, DAYS, or MONTHS.
Default: SECONDS

  
  BUCKET_TIME (enum)
    The possible values BUCKET_START and BUCKET_END cause generated
ticks to have their timestamp set to the start time or end time of the
interval for which they are generated, respectively.
Default: BUCKET_END

  
  NUM_TICKS_PER_TIMESTAMP (int)
    The number of ticks to generate for every value of timestamp.
Default: 1

  
  FIELDS (string)
    A mandatory parameter in the following form: FIELD_1
[TYPE_1]=CONST_EXPR_1, FIELD_2 [TYPE_2]=CONST_EXPR_1, &hellip; ,FIELD_N
[TYPE_N]=CONST_EXPR_1.
The name of the new field is "identifier." Supported types are listed here. If
the type specification is omitted, the type will be detected
automatically (long, double, decimal or string).

    CONST_EXPR_ used above are
logiical expressions.

    
  

Examples:

TICK_GENERATOR(BUCKET_INTERVAL = "1.5", BUCKET_TIME =
BUCKET_START, FIELDS="INT int = 10, SYMBOL_STRING string =
_SYMBOL_NAME, STRING5 string[5] = \\"TRD\\", DOUBLE double = POWER(1.24,
3), BYTE byte = DIV(450, 4), SHORT short = MOD(86,14), END_TIME
msectime = _END_TIME")

See the examples in TICK_GENERATOR.otq.


	"""
	class Parameters:
		bucket_interval = "BUCKET_INTERVAL"
		bucket_interval_units = "BUCKET_INTERVAL_UNITS"
		bucket_time = "BUCKET_TIME"
		num_ticks_per_timestamp = "NUM_TICKS_PER_TIMESTAMP"
		fields = "FIELDS"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["bucket_interval", "bucket_interval_units", "bucket_time", "num_ticks_per_timestamp", "fields"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["bucket_interval", "_default_bucket_interval", "bucket_interval_units", "_default_bucket_interval_units", "bucket_time", "_default_bucket_time", "num_ticks_per_timestamp", "_default_num_ticks_per_timestamp", "fields", "_default_fields", "stack_info", "_used_strings"]

	class BucketIntervalUnits:
		DAYS = "DAYS"
		MONTHS = "MONTHS"
		SECONDS = "SECONDS"

	class BucketTime:
		BUCKET_END = "BUCKET_END"
		BUCKET_START = "BUCKET_START"

	def __init__(self, bucket_interval="NEVER", bucket_interval_units=BucketIntervalUnits.SECONDS, bucket_time=BucketTime.BUCKET_END, num_ticks_per_timestamp=1, fields=""):
		_graph_components.EpBase.__init__(self, "TICK_GENERATOR")
		self._default_bucket_interval = "NEVER"
		self.bucket_interval = bucket_interval
		self._default_bucket_interval_units = type(self).BucketIntervalUnits.SECONDS
		self.bucket_interval_units = bucket_interval_units
		self._default_bucket_time = type(self).BucketTime.BUCKET_END
		self.bucket_time = bucket_time
		self._default_num_ticks_per_timestamp = 1
		self.num_ticks_per_timestamp = num_ticks_per_timestamp
		self._default_fields = ""
		self.fields = fields
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_bucket_interval(self, value):
		self.bucket_interval = value
		return self

	def set_bucket_interval_units(self, value):
		self.bucket_interval_units = value
		return self

	def set_bucket_time(self, value):
		self.bucket_time = value
		return self

	def set_num_ticks_per_timestamp(self, value):
		self.num_ticks_per_timestamp = value
		return self

	def set_fields(self, value):
		self.fields = value
		return self

	@staticmethod
	def _get_name():
		return "TICK_GENERATOR"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.bucket_interval != "NEVER": 
			desc += "BUCKET_INTERVAL=" + py_to_str(self.bucket_interval) + ","
		if self.bucket_interval_units != self.BucketIntervalUnits.SECONDS: 
			desc += "BUCKET_INTERVAL_UNITS=" + py_to_str(self.bucket_interval_units) + ","
		if self.bucket_time != self.BucketTime.BUCKET_END: 
			desc += "BUCKET_TIME=" + py_to_str(self.bucket_time) + ","
		if self.num_ticks_per_timestamp != 1: 
			desc += "NUM_TICKS_PER_TIMESTAMP=" + py_to_str(self.num_ticks_per_timestamp) + ","
		if self.fields != "": 
			desc += "FIELDS=" + py_to_str(self.fields) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class Ml_regressionPerfEvaluator(_graph_components.EpBase):
	"""
		

ML::REGRESSION_PERF_EVALUATOR

Type: Aggregation

Description: For each bucket, computes and outputs the
specified prediction performance metrics.

Python
class name:&nbsp;Ml_regressionPerfEvaluator

Input: A time series of ticks containing a filed representing
predicted value and a field representing the actual (expected) value.

Output:The set of regression
performance metrics computed over each bucket.

Parameters: See parameters
common to generic aggregations.


  BUCKET_INTERVAL
(seconds/ticks)
  BUCKET_INTERVAL_UNITS
(enumerated type)
  OUTPUT_INTERVAL
(seconds)
  OUTPUT_INTERVAL_UNITS
(enumerated type)
  IS_RUNNING_AGGR
(Boolean)
  BUCKET_TIME
(Boolean)
  BUCKET_END_CRITERIA
(expression)
  BOUNDARY_TICK_BUCKET
(NEW/PREVIOUS)
  ALL_FIELDS_FOR_SLIDING
(Boolean)
  PARTIAL_BUCKET_HANDLING
(enumerated type)
  FIELD_NAME_OF_EXPECTED_VALUE
(string) - the field name where the actual value (post-factum evaluated
label) is recorded.
  FIELD_NAME_OF_PREDICTED_VALUE(string)
- the field name where predicted value is recorded.
  GROUP_BY
(string)

Notes: See the notes on
generic aggregations.

Examples: See the evaluate
example in ml_dlib_regression.otq.


	"""
	class Parameters:
		bucket_interval = "BUCKET_INTERVAL"
		bucket_interval_units = "BUCKET_INTERVAL_UNITS"
		output_interval = "OUTPUT_INTERVAL"
		output_interval_units = "OUTPUT_INTERVAL_UNITS"
		is_running_aggr = "IS_RUNNING_AGGR"
		bucket_time = "BUCKET_TIME"
		bucket_end_criteria = "BUCKET_END_CRITERIA"
		boundary_tick_bucket = "BOUNDARY_TICK_BUCKET"
		all_fields_for_sliding = "ALL_FIELDS_FOR_SLIDING"
		partial_bucket_handling = "PARTIAL_BUCKET_HANDLING"
		field_name_of_expected_value = "FIELD_NAME_OF_EXPECTED_VALUE"
		field_name_of_predicted_value = "FIELD_NAME_OF_PREDICTED_VALUE"
		group_by = "GROUP_BY"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["bucket_interval", "bucket_interval_units", "output_interval", "output_interval_units", "is_running_aggr", "bucket_time", "bucket_end_criteria", "boundary_tick_bucket", "all_fields_for_sliding", "partial_bucket_handling", "field_name_of_expected_value", "field_name_of_predicted_value", "group_by"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["bucket_interval", "_default_bucket_interval", "bucket_interval_units", "_default_bucket_interval_units", "output_interval", "_default_output_interval", "output_interval_units", "_default_output_interval_units", "is_running_aggr", "_default_is_running_aggr", "bucket_time", "_default_bucket_time", "bucket_end_criteria", "_default_bucket_end_criteria", "boundary_tick_bucket", "_default_boundary_tick_bucket", "all_fields_for_sliding", "_default_all_fields_for_sliding", "partial_bucket_handling", "_default_partial_bucket_handling", "field_name_of_expected_value", "_default_field_name_of_expected_value", "field_name_of_predicted_value", "_default_field_name_of_predicted_value", "group_by", "_default_group_by", "stack_info", "_used_strings"]

	class BucketIntervalUnits:
		DAYS = "DAYS"
		FLEXIBLE = "FLEXIBLE"
		MONTHS = "MONTHS"
		SECONDS = "SECONDS"
		TICKS = "TICKS"

	class OutputIntervalUnits:
		SECONDS = "SECONDS"
		TICKS = "TICKS"

	class BucketTime:
		BUCKET_END = "BUCKET_END"
		BUCKET_START = "BUCKET_START"

	class BoundaryTickBucket:
		NEW = "NEW"
		PREVIOUS = "PREVIOUS"

	class AllFieldsForSliding:
		WHEN_TICKS_EXIT_WINDOW = "WHEN_TICKS_EXIT_WINDOW"
		FALSE = "false"
		TRUE = "true"

	class PartialBucketHandling:
		AS_SEPARATE_BUCKET = "AS_SEPARATE_BUCKET"
		DISCARD = "DISCARD"
		MERGE_WITH_PREVIOUS = "MERGE_WITH_PREVIOUS"

	def __init__(self, bucket_interval=0, bucket_interval_units=BucketIntervalUnits.SECONDS, output_interval="", output_interval_units=OutputIntervalUnits.SECONDS, is_running_aggr=False, bucket_time=BucketTime.BUCKET_END, bucket_end_criteria="", boundary_tick_bucket=BoundaryTickBucket.NEW, all_fields_for_sliding=AllFieldsForSliding.FALSE, partial_bucket_handling=PartialBucketHandling.AS_SEPARATE_BUCKET, field_name_of_expected_value="", field_name_of_predicted_value="", group_by=""):
		_graph_components.EpBase.__init__(self, "ML::REGRESSION_PERF_EVALUATOR")
		self._default_bucket_interval = 0
		self.bucket_interval = bucket_interval
		self._default_bucket_interval_units = type(self).BucketIntervalUnits.SECONDS
		self.bucket_interval_units = bucket_interval_units
		self._default_output_interval = ""
		self.output_interval = output_interval
		self._default_output_interval_units = type(self).OutputIntervalUnits.SECONDS
		self.output_interval_units = output_interval_units
		self._default_is_running_aggr = False
		self.is_running_aggr = is_running_aggr
		self._default_bucket_time = type(self).BucketTime.BUCKET_END
		self.bucket_time = bucket_time
		self._default_bucket_end_criteria = ""
		self.bucket_end_criteria = bucket_end_criteria
		self._default_boundary_tick_bucket = type(self).BoundaryTickBucket.NEW
		self.boundary_tick_bucket = boundary_tick_bucket
		self._default_all_fields_for_sliding = type(self).AllFieldsForSliding.FALSE
		self.all_fields_for_sliding = all_fields_for_sliding
		self._default_partial_bucket_handling = type(self).PartialBucketHandling.AS_SEPARATE_BUCKET
		self.partial_bucket_handling = partial_bucket_handling
		self._default_field_name_of_expected_value = ""
		self.field_name_of_expected_value = field_name_of_expected_value
		self._default_field_name_of_predicted_value = ""
		self.field_name_of_predicted_value = field_name_of_predicted_value
		self._default_group_by = ""
		self.group_by = group_by
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_bucket_interval(self, value):
		self.bucket_interval = value
		return self

	def set_bucket_interval_units(self, value):
		self.bucket_interval_units = value
		return self

	def set_output_interval(self, value):
		self.output_interval = value
		return self

	def set_output_interval_units(self, value):
		self.output_interval_units = value
		return self

	def set_is_running_aggr(self, value):
		self.is_running_aggr = value
		return self

	def set_bucket_time(self, value):
		self.bucket_time = value
		return self

	def set_bucket_end_criteria(self, value):
		self.bucket_end_criteria = value
		return self

	def set_boundary_tick_bucket(self, value):
		self.boundary_tick_bucket = value
		return self

	def set_all_fields_for_sliding(self, value):
		self.all_fields_for_sliding = value
		return self

	def set_partial_bucket_handling(self, value):
		self.partial_bucket_handling = value
		return self

	def set_field_name_of_expected_value(self, value):
		self.field_name_of_expected_value = value
		return self

	def set_field_name_of_predicted_value(self, value):
		self.field_name_of_predicted_value = value
		return self

	def set_group_by(self, value):
		self.group_by = value
		return self

	@staticmethod
	def _get_name():
		return "ML::REGRESSION_PERF_EVALUATOR"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.bucket_interval != 0: 
			desc += "BUCKET_INTERVAL=" + py_to_str(self.bucket_interval) + ","
		if self.bucket_interval_units != self.BucketIntervalUnits.SECONDS: 
			desc += "BUCKET_INTERVAL_UNITS=" + py_to_str(self.bucket_interval_units) + ","
		if self.output_interval != "": 
			desc += "OUTPUT_INTERVAL=" + py_to_str(self.output_interval) + ","
		if self.output_interval_units != self.OutputIntervalUnits.SECONDS: 
			desc += "OUTPUT_INTERVAL_UNITS=" + py_to_str(self.output_interval_units) + ","
		if self.is_running_aggr != False: 
			desc += "IS_RUNNING_AGGR=" + py_to_str(self.is_running_aggr) + ","
		if self.bucket_time != self.BucketTime.BUCKET_END: 
			desc += "BUCKET_TIME=" + py_to_str(self.bucket_time) + ","
		if self.bucket_end_criteria != "": 
			desc += "BUCKET_END_CRITERIA=" + py_to_str(self.bucket_end_criteria) + ","
		if self.boundary_tick_bucket != self.BoundaryTickBucket.NEW: 
			desc += "BOUNDARY_TICK_BUCKET=" + py_to_str(self.boundary_tick_bucket) + ","
		if self.all_fields_for_sliding != self.AllFieldsForSliding.FALSE: 
			desc += "ALL_FIELDS_FOR_SLIDING=" + py_to_str(self.all_fields_for_sliding) + ","
		if self.partial_bucket_handling != self.PartialBucketHandling.AS_SEPARATE_BUCKET: 
			desc += "PARTIAL_BUCKET_HANDLING=" + py_to_str(self.partial_bucket_handling) + ","
		if self.field_name_of_expected_value != "": 
			desc += "FIELD_NAME_OF_EXPECTED_VALUE=" + py_to_str(self.field_name_of_expected_value) + ","
		if self.field_name_of_predicted_value != "": 
			desc += "FIELD_NAME_OF_PREDICTED_VALUE=" + py_to_str(self.field_name_of_predicted_value) + ","
		if self.group_by != "": 
			desc += "GROUP_BY=" + py_to_str(self.group_by) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class ShowHostsForDbs(_graph_components.EpBase):
	"""
		

SHOW_HOSTS_FOR_DBS

Type: Other

Description: Shows all the hosts on which each database
physically resides. If the &lt;TICK_SERVERS&gt;
section exists in the locator, then databases on the specified hosts
are also considered.

Output ticks have the following fields:


  DB_NAME
  SERVER_ADDRESS
  INTERMEDIATE_SERVER_ADDRESSES

Output ticks are grouped by databases. The order depends on how
locations are specified in the locator, and on the depth of hosts from
the initial host. If the database is local, then SERVER_ADDRESS is populated with hostname:0. INTERMEDIATE_SERVER_ADDRESSES
shows the path from the first tick server to the last one. The server
on which the query begins to execute (as determined by the symbol used
for the query) is not included into the INTERMEDIATE_SERVER_ADDRESSES.

For virtual databases, this EP shows the hosts for all underlying
real databases.

Python
class name:&nbsp;ShowHostsForDbs

Output: A tick for each database for each host.

Example output:

1070289000000,F,Milkyway2:43052,Milkyway2:57958-&gt;Milkyway2:43052,LOCAL::,SHOW_HOSTS_FOR_DBS1070289000000,F,Milkyway2:58058,Milkyway2:57958-&gt;Milkyway2:58058,LOCAL::,SHOW_HOSTS_FOR_DBS1070289000000,LOCAL_DB,Milkyway2:57958,Milkyway2:57958,LOCAL::,SHOW_HOSTS_FOR_DBS1070289000000,LOCAL_VIRTUAL_DB,Milkyway2:43052,Milkyway2:57958-&gt;Milkyway2:43052,LOCAL::,SHOW_HOSTS_FOR_DBS1070289000000,LOCAL_VIRTUAL_DB,Milkyway2:58058,Milkyway2:57958-&gt;Milkyway2:58058,LOCAL::,SHOW_HOSTS_FOR_DBS1070289000000,LOCAL_VIRTUAL_DB,Milkyway2:53039,Milkyway2:57958-&gt;Milkyway2:53039,LOCAL::,SHOW_HOSTS_FOR_DBS1070289000000,REMOTE_VIRTUAL_DB,Milkyway2:50743,Milkyway2:57958-&gt;Milkyway2:50743,LOCAL::,SHOW_HOSTS_FOR_DBS1070289000000,REMOTE_VIRTUAL_DB,Milkyway2:58058,Milkyway2:57958-&gt;Milkyway2:50743-&gt;Milkyway2:58058,LOCAL::,SHOW_HOSTS_FOR_DBS1070289000000,VIRTUAL_DB1,Milkyway2:57958,Milkyway2:57958,LOCAL::,SHOW_HOSTS_FOR_DBS1070289000000,VIRTUAL_DB1,Milkyway2:43052,Milkyway2:57958-&gt;Milkyway2:43052,LOCAL::,SHOW_HOSTS_FOR_DBS1070289000000,VIRTUAL_DB1,Milkyway2:58058,Milkyway2:57958-&gt;Milkyway2:58058,LOCAL::,SHOW_HOSTS_FOR_DBS



	"""
	class Parameters:
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = []
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["stack_info", "_used_strings"]

	def __init__(self):
		_graph_components.EpBase.__init__(self, "SHOW_HOSTS_FOR_DBS")
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	@staticmethod
	def _get_name():
		return "SHOW_HOSTS_FOR_DBS"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class SetApplicationName(_graph_components.EpBase):
	"""
		

SET_APPLICATION_NAME

Type: Other

Description: Sets the application name for the query that
will be used in OT entitlement module for logging.

Python
class name:&nbsp;SetApplicationName

Input: A time series of ticks.

Output: A time series of ticks.

Parameters:


  APPLICATION_NAME (string)
    The application name.

  

Overriding application name isn't allowed, so either the query
should contain only one instance of SET_APPLICATION_NAME EP, of the
values of APPLICATION_NAME parameter should be the same.

Examples:

SET_APPLICATION_NAME(APPLICATION_NAME=app_name)




	"""
	class Parameters:
		application_name = "APPLICATION_NAME"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["application_name"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["application_name", "_default_application_name", "stack_info", "_used_strings"]

	def __init__(self, application_name=""):
		_graph_components.EpBase.__init__(self, "SET_APPLICATION_NAME")
		self._default_application_name = ""
		self.application_name = application_name
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_application_name(self, value):
		self.application_name = value
		return self

	@staticmethod
	def _get_name():
		return "SET_APPLICATION_NAME"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.application_name != "": 
			desc += "APPLICATION_NAME=" + py_to_str(self.application_name) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class CorpActions(_graph_components.EpBase):
	"""
		

CORP_ACTIONS

Type: Transformer

Description: Adjusts values using corporate actions
information loaded into OneTick from the reference data file. To use
it, location of reference database must be specified via OneTick
configuration.

Python
class name: CorpActions

Input: A time series of ticks.

Output: A time series of ticks with appropriate adjustments
made for corporate actions.

Parameters:


  ADJUSTMENT_DATE (YYYYMMDD or
YYYYMMDDhhmmss)
    The date as of which the values are adjusted. If it is not set,
the values are adjusted as of the end date in the query. All corporate
actions of the types specified in the parameters that lie between the
tick timestamp and the adjustment date will be applied to each tick.
Notice that the adjustment date is not affected neither by
_SYMBOL_PARAM._PARAM_END_TIME nor by the apply_times_daily
setting. When ADJUSTMENT_DATE parameter is in YYYYMMDD format, the time
is assumeed to be 17:00:00 GMT. Such time is chosen in order to try to
make sure that corporate actions were already reported for date
&lt;YYYYMMDD&gt; for all exchanges in the world, and the trading for
the following date did not yet begin on any exchange (and thus nothing
yet needs to be adjusted for the corporate actions that will be
reported for the following date).
    
Default value: the end date given in the query

  
  ADJUSTMENT_DATE_TZ (string)
    Timezone for adjustment date. When ADJUSTMENT_DATE parameter is
in YYYYMMDD format, ADJUSTMENT_DATE_TZ must be set to GMT.
Default value: GMT

  
  FIELDS (string [, string])
    A comma-separated list of fields to be adjusted. If this
parameter
is not set, some default adjustments will take place if appropriately
named fields exist in the tick:

    If the ADJUST_RULE parameter is set to PRICE, and the PRICE
field is
present, it will get adjusted. If the fields ASK_PRICE or BID_PRICE are
present, they will get adjusted. If fields ASK_VALUE or BID_VALUE are
present, they will get adjusted

    If the ADJUST_RULE parameter is set to SIZE, and the SIZE field
is
present, it will get adjusted. If the fields ASK_SIZE or BID_SIZE are
present, they will get adjusted. If fields ASK_VALUE or BID_VALUE are
present, they will get adjusted.

    Default value: Empty

  
  ADJUST_RULE (PRICE|SIZE)
    When set to PRICE, adjustments are applied under the assumption
that
fields to be adjusted contain prices (adjustment direction is
determined appropriately). When set to SIZE, adjustments are applied
under the assumption that fields contain sizes (adjustment direction is
opposite to that when the parameter's value is PRICE).
Default value: PRICE

  
  APPLY_SPLIT (Boolean)
    If true, adjustments for splits are applied.
Default value: true

  
  APPLY_SPINOFF (Boolean)
    If true, adjustments for spin-offs are applied. By default,
spinoff adjustments are not allowed when
ADJUST_RULE=SIZE. Main configuration parameter REF_DATA.ALLOW_SPINOFF_EVENTS_FOR_SIZE_ADJUST_RULE
can be used to override this restriction. 
Default value: false

  
  APPLY_CASH_DIVIDEND (Boolean)
    If true, adjustments for cash dividends are applied.
Default value: false

  
  APPLY_STOCK_DIVIDEND (Boolean)
    If true, adjustments for stock dividends are applied.
Default value: false

  
  APPLY_SECURITY_SPLICE (Boolean)
    If true, adjustments for security splices are applied.
Default value: false

  
  APPLY_OTHERS (string)
    A comma-separated list of names of custom adjustment types to
apply.
Default value: empty

  
  APPLY_ALL (Boolean)
    If true, applies all types of adjustments, both built-in and
custom.
Default value: false

  


See the Reference Database Guide for details about how to specify
custom adjustment types.

This processor can be customized and is naturally extensible to
other corporate actions such as mergers, warrants, rights, and other
types of events that need to be specially handled. Such events need to
be expressed as additive and multiplicative adjustments and supplied
through the reference data file.


Examples:

Outputs a stream of ticks adjusted for splits and cash dividends as
of 20031201:

CORP_ACTION (20031201, , PRICE, true, false, false, true);CORP_ACTION (20031201, , SIZE, true, false, false, false);


	"""
	class Parameters:
		adjustment_date = "ADJUSTMENT_DATE"
		adjustment_date_tz = "ADJUSTMENT_DATE_TZ"
		fields = "FIELDS"
		adjust_rule = "ADJUST_RULE"
		apply_split = "APPLY_SPLIT"
		apply_spinoff = "APPLY_SPINOFF"
		apply_rights = "APPLY_RIGHTS"
		apply_cash_dividend = "APPLY_CASH_DIVIDEND"
		apply_stock_dividend = "APPLY_STOCK_DIVIDEND"
		apply_security_splice = "APPLY_SECURITY_SPLICE"
		apply_others = "APPLY_OTHERS"
		apply_all = "APPLY_ALL"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["adjustment_date", "adjustment_date_tz", "fields", "adjust_rule", "apply_split", "apply_spinoff", "apply_rights", "apply_cash_dividend", "apply_stock_dividend", "apply_security_splice", "apply_others", "apply_all"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["adjustment_date", "_default_adjustment_date", "adjustment_date_tz", "_default_adjustment_date_tz", "fields", "_default_fields", "adjust_rule", "_default_adjust_rule", "apply_split", "_default_apply_split", "apply_spinoff", "_default_apply_spinoff", "apply_rights", "_default_apply_rights", "apply_cash_dividend", "_default_apply_cash_dividend", "apply_stock_dividend", "_default_apply_stock_dividend", "apply_security_splice", "_default_apply_security_splice", "apply_others", "_default_apply_others", "apply_all", "_default_apply_all", "stack_info", "_used_strings"]

	class AdjustRule:
		PRICE = "PRICE"
		SIZE = "SIZE"

	def __init__(self, adjustment_date="", adjustment_date_tz="GMT", fields="", adjust_rule=AdjustRule.PRICE, apply_split=True, apply_spinoff=False, apply_rights=False, apply_cash_dividend=False, apply_stock_dividend=False, apply_security_splice=False, apply_others="", apply_all=False):
		_graph_components.EpBase.__init__(self, "CORP_ACTIONS")
		self._default_adjustment_date = ""
		self.adjustment_date = adjustment_date
		self._default_adjustment_date_tz = "GMT"
		self.adjustment_date_tz = adjustment_date_tz
		self._default_fields = ""
		self.fields = fields
		self._default_adjust_rule = type(self).AdjustRule.PRICE
		self.adjust_rule = adjust_rule
		self._default_apply_split = True
		self.apply_split = apply_split
		self._default_apply_spinoff = False
		self.apply_spinoff = apply_spinoff
		self._default_apply_rights = False
		self.apply_rights = apply_rights
		self._default_apply_cash_dividend = False
		self.apply_cash_dividend = apply_cash_dividend
		self._default_apply_stock_dividend = False
		self.apply_stock_dividend = apply_stock_dividend
		self._default_apply_security_splice = False
		self.apply_security_splice = apply_security_splice
		self._default_apply_others = ""
		self.apply_others = apply_others
		self._default_apply_all = False
		self.apply_all = apply_all
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_adjustment_date(self, value):
		self.adjustment_date = value
		return self

	def set_adjustment_date_tz(self, value):
		self.adjustment_date_tz = value
		return self

	def set_fields(self, value):
		self.fields = value
		return self

	def set_adjust_rule(self, value):
		self.adjust_rule = value
		return self

	def set_apply_split(self, value):
		self.apply_split = value
		return self

	def set_apply_spinoff(self, value):
		self.apply_spinoff = value
		return self

	def set_apply_rights(self, value):
		self.apply_rights = value
		return self

	def set_apply_cash_dividend(self, value):
		self.apply_cash_dividend = value
		return self

	def set_apply_stock_dividend(self, value):
		self.apply_stock_dividend = value
		return self

	def set_apply_security_splice(self, value):
		self.apply_security_splice = value
		return self

	def set_apply_others(self, value):
		self.apply_others = value
		return self

	def set_apply_all(self, value):
		self.apply_all = value
		return self

	@staticmethod
	def _get_name():
		return "CORP_ACTIONS"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.adjustment_date != "": 
			desc += "ADJUSTMENT_DATE=" + py_to_str(self.adjustment_date) + ","
		if self.adjustment_date_tz != "GMT": 
			desc += "ADJUSTMENT_DATE_TZ=" + py_to_str(self.adjustment_date_tz) + ","
		if self.fields != "": 
			desc += "FIELDS=" + py_to_str(self.fields) + ","
		if self.adjust_rule != self.AdjustRule.PRICE: 
			desc += "ADJUST_RULE=" + py_to_str(self.adjust_rule) + ","
		if self.apply_split != True: 
			desc += "APPLY_SPLIT=" + py_to_str(self.apply_split) + ","
		if self.apply_spinoff != False: 
			desc += "APPLY_SPINOFF=" + py_to_str(self.apply_spinoff) + ","
		if self.apply_rights != False: 
			desc += "APPLY_RIGHTS=" + py_to_str(self.apply_rights) + ","
		if self.apply_cash_dividend != False: 
			desc += "APPLY_CASH_DIVIDEND=" + py_to_str(self.apply_cash_dividend) + ","
		if self.apply_stock_dividend != False: 
			desc += "APPLY_STOCK_DIVIDEND=" + py_to_str(self.apply_stock_dividend) + ","
		if self.apply_security_splice != False: 
			desc += "APPLY_SECURITY_SPLICE=" + py_to_str(self.apply_security_splice) + ","
		if self.apply_others != "": 
			desc += "APPLY_OTHERS=" + py_to_str(self.apply_others) + ","
		if self.apply_all != False: 
			desc += "APPLY_ALL=" + py_to_str(self.apply_all) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class DbDelete(_graph_components.EpBase):
	"""
		

DB/DELETE

Type: Transformer

Description: Deletes database ticks that match the selection
criteria. This event processor (EP) must be a source on the graph.

All corrections launched via this EP are assigned unique IDs. The ID
(a string value) is available to users in the EP output (OPERATION_ID
field).

This EP can detect if the memory loader is running. If it is not,
the corresponding error message will appear against the memdb row for
detailed output.

Python
class name:&nbsp;DbDelete

Input: A time series of ticks.

Output: A tick with the NUM_AFFECTED_ROWS field.

Parameters:


  WHERE (expression)
    Specifies a criteria for selecting ticks to delete. The
expression is built with boolean operators AND, OR, and NOT. It accepts
arithmetic operators +, -, *, / and comparison operators =, !=, &lt;,
&lt;=, &gt;, &gt;=. Field names are not quoted, are case sensitive, and
have to be present in the tick. String literals must be surrounded by
single or double quotes.

    The special field TIMESTAMP can be used in WHERE.
It represents the value of the tick timestamp.

    In addition, WHERE accepts OneTick built-in functions
(see Catalog of Built-in Functions for
details). Also, OneTick event processors that act as filters can be
used.

    Fields of preceding ticks (those having already arrived) (e.g.
PRICE[-1], SIZE[-3], etc.) cannot be used in the WHERE clause.

    You can also use pseudo-fields such as TIMESTAMP, _START_TIME,
and _END_TIME (described in detail in the Pseudo-fields document).

  
  KEEP_HISTORY (Boolean)
    Currently supported only for daily archives. If set to true,
TICK_STATUS and DELETED_TIME fields must be present in the tick schema;
the ticks will remain in the database with a DELETED status and
DELETED_TIME will be set to the current time. Otherwise, the ticks will
be physically deleted from the database.

  
  APPLY_ON_RELOAD
    If this is false, the correction will be applied to the
archive once, and will be recorded to the corrections file with
"expiration time", which is TIMEOUT seconds from that moment.
Once expired, neither archive loader (if archive is reloaded), nor
memory loader (if in-memory database exists for that day) will apply
this correction.
Default: true

  
  OUTPUT
    This parameter accepts two values: aggregated and detailed.
It controls how the correction result will be shown to the user.
Aggregated includes the OPERATION_ID field. For detailed output a
separate row will appear for each &lt;archive unit&gt; (day for
daily archives, month for monthly archives, the whole interval
for continuous archives, and memdb separately). The row will show the
number of affected rows, the operation ID (which is the same for all
rows), start/end times for that unit, whether the operation is in
progress, and the error message if the operation has failed.
Default: aggregated

  
  TIMEOUT
    If the correction is to be applied to the in-memory database,
the memory loader will log the operation result, and this event
processor will check the log, locating the operation by its ID. The
checking will be done once per second, and the maximum number of
attempts is controlled via the TIMEOUT parameter. If the entry is
found, NUM_AFFECTED_ROWS will be set correspondingly. Otherwise,
IN_PROGRESS will be set to 1.
This parameter also controls loader behavior if APPLY_ON_RELOAD is
false (see below).
Default: 5

  

Examples:

DB::DELETE (WHERE='SIZE &lt; 100')

See the Data Modeling in OneTick documentation for more information
about data modifications.


	"""
	class Parameters:
		where = "WHERE"
		keep_history = "KEEP_HISTORY"
		apply_on_reload = "APPLY_ON_RELOAD"
		output = "OUTPUT"
		timeout = "TIMEOUT"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["where", "keep_history", "apply_on_reload", "output", "timeout"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["where", "_default_where", "keep_history", "_default_keep_history", "apply_on_reload", "_default_apply_on_reload", "output", "_default_output", "timeout", "_default_timeout", "stack_info", "_used_strings"]

	class Output:
		AGGREGATED = "aggregated"
		DETAILED = "detailed"

	def __init__(self, where="", keep_history=True, apply_on_reload=True, output=Output.AGGREGATED, timeout=5):
		_graph_components.EpBase.__init__(self, "DB/DELETE")
		self._default_where = ""
		self.where = where
		self._default_keep_history = True
		self.keep_history = keep_history
		self._default_apply_on_reload = True
		self.apply_on_reload = apply_on_reload
		self._default_output = type(self).Output.AGGREGATED
		self.output = output
		self._default_timeout = 5
		self.timeout = timeout
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_where(self, value):
		self.where = value
		return self

	def set_keep_history(self, value):
		self.keep_history = value
		return self

	def set_apply_on_reload(self, value):
		self.apply_on_reload = value
		return self

	def set_output(self, value):
		self.output = value
		return self

	def set_timeout(self, value):
		self.timeout = value
		return self

	@staticmethod
	def _get_name():
		return "DB/DELETE"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.where != "": 
			desc += "WHERE=" + py_to_str(self.where) + ","
		if self.keep_history != True: 
			desc += "KEEP_HISTORY=" + py_to_str(self.keep_history) + ","
		if self.apply_on_reload != True: 
			desc += "APPLY_ON_RELOAD=" + py_to_str(self.apply_on_reload) + ","
		if self.output != self.Output.AGGREGATED: 
			desc += "OUTPUT=" + py_to_str(self.output) + ","
		if self.timeout != 5: 
			desc += "TIMEOUT=" + py_to_str(self.timeout) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class DbInsertDataQualityEvent(_graph_components.EpBase):
	"""
		

DB/INSERT_DATA_QUALITY_EVENT

Type: Transformer

Description: Inserts data quality event into the database.
The data quality event timestamp will be taken from the query start
time. The result is a single tick with the following three fields:


  NUM_AFFECTED_EVENTS
  OPERATION_ID
  IN_PROGRESS

Normally, NUM_AFFECTED_EVENTS will be 1 and IN_PROGRESS will be 0.
If the event is to be inserted into a memdb and the memory loader is
either not running or times out, NUM_AFFECTED_EVENTS will be 0 and
IN_PROGRESS will be 1. In that case, the result can be checked later
passing the returned operation id to DB/SHOW_CORRECTION_STATUS event
processor.

Python
class name:&nbsp;InsertDataQualityEvent

Input: A data quality event.

Output: A single tick with three fields: NUM_AFFECTED_EVENTS,
OPERATION_ID, and IN_PROGRESS.

Parameters:

See the Data Quality Events section in OneTick Modeling
documentation.


	"""
	class Parameters:
		data_quality = "DATA_QUALITY"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["data_quality"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["data_quality", "_default_data_quality", "stack_info", "_used_strings"]

	class DataQuality:
		EMPTY = ""
		COLLECTOR_FAILURE = "COLLECTOR_FAILURE"
		MISSING = "MISSING"
		MOUNT_BAD = "MOUNT_BAD"
		OK = "OK"
		PATCHED = "PATCHED"
		QUALITY_DELAY_STITCHING_WITH_RT = "QUALITY_DELAY_STITCHING_WITH_RT"
		QUALITY_OK_STITCHING_WITH_RT = "QUALITY_OK_STITCHING_WITH_RT"
		STALE = "STALE"

	def __init__(self, data_quality=DataQuality.EMPTY):
		_graph_components.EpBase.__init__(self, "DB/INSERT_DATA_QUALITY_EVENT")
		self._default_data_quality = type(self).DataQuality.EMPTY
		self.data_quality = data_quality
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_data_quality(self, value):
		self.data_quality = value
		return self

	@staticmethod
	def _get_name():
		return "DB/INSERT_DATA_QUALITY_EVENT"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.data_quality != self.DataQuality.EMPTY: 
			desc += "DATA_QUALITY=" + py_to_str(self.data_quality) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class DbRenameTickType(_graph_components.EpBase):
	"""
		

DB/RENAME_TICK_TYPE

Type: Transformer

Description: This event processor facilitates tick type
changes in OneTick archives. It must be the first (source) event
processor on the graph. It changes all archives that cover some of the
range between the start time and the end time of the query.

Input tick type is taken from the query. Input symbols for the
queries that involve this event processor should be of the form &lt;db_name&gt;:: or &lt;db_name&gt;::&lt;some symbol name&gt;.

Note: Currently, this event processor does not change tick
types in accelerator and real-time databases.

Python
class name:&nbsp;DbRenameTickType

Input: None

Output: For each day within the query range, one tick with
two fields: DATE representing the
affected date and NUM_AFFECTED_SYMBOLS
representing the number of symbols for the renamed tick type.

Parameters:


  NEW_TICK_TYPE (string)
    The new tick type.

  

Examples:

DB::RENAME_TICK_TYPE (NEW_TICK_TYPE='TRD')

See the Data Modeling in OneTick documentation for more information
about data modifications.


	"""
	class Parameters:
		new_tick_type = "NEW_TICK_TYPE"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["new_tick_type"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["new_tick_type", "_default_new_tick_type", "stack_info", "_used_strings"]

	def __init__(self, new_tick_type=""):
		_graph_components.EpBase.__init__(self, "DB/RENAME_TICK_TYPE")
		self._default_new_tick_type = ""
		self.new_tick_type = new_tick_type
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_new_tick_type(self, value):
		self.new_tick_type = value
		return self

	@staticmethod
	def _get_name():
		return "DB/RENAME_TICK_TYPE"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.new_tick_type != "": 
			desc += "NEW_TICK_TYPE=" + py_to_str(self.new_tick_type) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class DbRenameSymbol(_graph_components.EpBase):
	"""
		

DB/RENAME_SYMBOL

Type: Transformer

Description: This event processor facilitates symbol name
changes in OneTick archives. It must be the first (source) event
processor on the graph. It changes all archives that cover some of the
range between the start time and the end time of the query.

Symbol name and tick type are taken from the query. A special value
of 'ALL' is supported for the tick
type, in which case the symbol will be renamed for all existing tick
types.

Note: Currently, this event processor does not change symbol
names in accelerator and real-time databases.

Python
class name:&nbsp;DbRenameSymbol

Input: None

Output: For each day within the query range, one tick with
three fields: DATE representing the
affected date, NUM_AFFECTED_TICK_TYPES
representing the number of tick types for which the symbol name was
changed, and NEW_SYMBOL_NAME stating
the new symbol name.

Parameters:


  NEW_SYMBOL_NAME (string)
    The new symbol name (without the database name and the tick
type).

  

Examples:

DB::RENAME_SYMBOL (NEW_SYMBOL_NAME='A')

See the Data Modeling in OneTick documentation for more information
about data modifications.


	"""
	class Parameters:
		new_symbol_name = "NEW_SYMBOL_NAME"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["new_symbol_name"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["new_symbol_name", "_default_new_symbol_name", "stack_info", "_used_strings"]

	def __init__(self, new_symbol_name=""):
		_graph_components.EpBase.__init__(self, "DB/RENAME_SYMBOL")
		self._default_new_symbol_name = ""
		self.new_symbol_name = new_symbol_name
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_new_symbol_name(self, value):
		self.new_symbol_name = value
		return self

	@staticmethod
	def _get_name():
		return "DB/RENAME_SYMBOL"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.new_symbol_name != "": 
			desc += "NEW_SYMBOL_NAME=" + py_to_str(self.new_symbol_name) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class DbShowLoadedTimeRanges(_graph_components.EpBase):
	"""
		

DB/SHOW_LOADED_TIME_RANGES

Type: Other

Description: For a specified database within the queried
interval this event processor (EP) shows loaded tick time ranges:
start/end dates, start/end timestamps, number of loaded and total
partitions (the latter is important for virtual databases, for regular
databases number of total partitions is always 1), and IS_MEMORY_DB flag indicating whether interval is loaded from memory database or archive. It shows the start
date and end date and builds timestamps from these dates, with zero
hour, minute, and second and with database time zone. It also does a
boundary check, and if the loaded time range intersects with the
queried interval, while the query start date time is greater than the
start range, then the query start date and time will be shown.
Similarly, if the query end date and time is lesser than the end range,
the query end date and time will be shown. If the date is not loaded,
it will still be present in results, with NUM_LOADED_PARTITIONS=0.
If there is an intersection between memory and archive intervals, the intersection is always considered part of memory db (IS_MEMORY_DB=1).


It must be the first (source) EP on the graph.

Input symbols for the queries that involve this EP should be of the
form &lt;db_name&gt;:: or &lt;db_name&gt;::&lt;some symbolname&gt;.
In either case, the EP will only extract the DB name and will only
report the information about the DB as a whole, not about the specific
symbol.

Python
class name:&nbsp;DbShowLoadedTimeRanges

Input: A time series of ticks.

Output: Time ranges.

Parameters:


  USE_CACHE (Boolean)
    If set to true, the event processor will use the cached results
(see the META_DATA_CACHE_FILE in the
Locator Variables documentation), if available. If the cache is
available only up to some point, the rest of the data will be
calculated and merged with the cache contents. If set to false, all the
data will be recalculated even if the cache is available. Under no
circumstances will the cache be updated.
Default: true

  
  MAX_POSSIBLE_LOADED_DATE
    Possible values are TODAY and END_DATE. If set to TODAY
(default), the dates in the future are not returned by this event
processor. If set to END_DATE, returned ticks cover full time span of
the query.
Default: TODAY

  
  PREFER_SPEED_OVER_ACCURACY (Boolean)
    If set to true, the event processor will use consider as loaded
all days for which database's archive directory, configured in the
locator file, has a corresponding subdirectory with name in format
YYYYMMDD, without checking whether that subdirectory has a file named index. The &nbsp;check for the existence
of index file may be
expensive,&nbsp;in terms of time it takes, when the file resides on NFS
or on object storage, such as S3. 
Default: false

  

Examples: Shows time ranges:

DB/SHOW_LOADED_TIME_RANGES()

See the DB_SHOW_LOADED_TIME_RANGES
example in OTHER_EXAMPLES.otq.


	"""
	class Parameters:
		use_cache = "USE_CACHE"
		max_possible_loaded_date = "MAX_POSSIBLE_LOADED_DATE"
		prefer_speed_over_accuracy = "PREFER_SPEED_OVER_ACCURACY"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["use_cache", "max_possible_loaded_date", "prefer_speed_over_accuracy"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["use_cache", "_default_use_cache", "max_possible_loaded_date", "_default_max_possible_loaded_date", "prefer_speed_over_accuracy", "_default_prefer_speed_over_accuracy", "stack_info", "_used_strings"]

	class MaxPossibleLoadedDate:
		END_DATE = "END_DATE"
		TODAY = "TODAY"

	def __init__(self, use_cache=True, max_possible_loaded_date=MaxPossibleLoadedDate.TODAY, prefer_speed_over_accuracy=False):
		_graph_components.EpBase.__init__(self, "DB/SHOW_LOADED_TIME_RANGES")
		self._default_use_cache = True
		self.use_cache = use_cache
		self._default_max_possible_loaded_date = type(self).MaxPossibleLoadedDate.TODAY
		self.max_possible_loaded_date = max_possible_loaded_date
		self._default_prefer_speed_over_accuracy = False
		self.prefer_speed_over_accuracy = prefer_speed_over_accuracy
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_use_cache(self, value):
		self.use_cache = value
		return self

	def set_max_possible_loaded_date(self, value):
		self.max_possible_loaded_date = value
		return self

	def set_prefer_speed_over_accuracy(self, value):
		self.prefer_speed_over_accuracy = value
		return self

	@staticmethod
	def _get_name():
		return "DB/SHOW_LOADED_TIME_RANGES"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.use_cache != True: 
			desc += "USE_CACHE=" + py_to_str(self.use_cache) + ","
		if self.max_possible_loaded_date != self.MaxPossibleLoadedDate.TODAY: 
			desc += "MAX_POSSIBLE_LOADED_DATE=" + py_to_str(self.max_possible_loaded_date) + ","
		if self.prefer_speed_over_accuracy != False: 
			desc += "PREFER_SPEED_OVER_ACCURACY=" + py_to_str(self.prefer_speed_over_accuracy) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class DbUpdate(_graph_components.EpBase):
	"""
		

DB/UPDATE

Type: Transformer

Description: Modifies database ticks that match selection
criteria, setting their value to the result of an expression. This
event processor (EP) must be a source on the graph.

All corrections launched via this EP are assigned unique IDs. The ID
(a string value) is available to users in the EP output (OPERATION_ID
field).

This EP can detect if the memory loader is running. If it is not,
the corresponding error message will appear against the memdb row for
detailed output.

Python
class name:&nbsp;DbUpdate

Input: A time series of ticks.

Output: A tick with a NUM_AFFECTED_ROWS field.

Parameters:


  SET (expressions)
    Expressions are in the following form:
field_name1=&lt;expression1&gt;,field_name2=&lt;expression2&gt;,&hellip;.,
field_nameN=&lt;expressionN&gt;
An expression can be a constant, or a function of other fields. The
function can be constructed by using standard arithmetic operators: +,
-, *, /.
In constructing the expression, one can use comparison operators =, !=,
&lt;, &lt;=, &gt;, &gt;=, as well as Boolean operators AND, OR, NOT
(true evaluates to 1 and false to 0). Also, OneTick built-in functions
can be used in the expression (see Catalog of
Built-in Functions for details).
Fields of preceding ticks (those having already arrived) (e.g.,
PRICE[-1], SIZE[-3], etc.) currently cannot be used in the SET clause.
You can also use pseudo-fields such as TIMESTAMP, _START_TIME, and
_END_TIME (described in detail in the Pseudo-fields
document).

  
  WHERE (expression)
    Specifies the criteria for selecting ticks to modify. The
expression can be built with Boolean operators AND, OR, and NOT. It
accepts arithmetic operators +, -, *, / and comparison operators =, !=,
&lt;, &lt;=, &gt;, &gt;=. Field names must be present in the tick: they
are case-sensitive and are presented without quotes. String literals
must be surrounded by single or double quotes.
The special field TIMESTAMP can be used in WHERE. It
represents the value of the tick timestamp.
In addition, WHERE accepts OneTick built-in functions (see Catalog of Built-in Functions for details).
Also, OneTick event processors that act as a filter can be used.
Fields of preceding ticks (those having already arrived) (e.g.
PRICE[-1], SIZE[-3], etc.) cannot be used in the WHERE clause.
You can also use pseudo-fields such as TIMESTAMP, _START_TIME, and
_END_TIME (described in detail in the Pseudo-fields
document).

  
  KEEP_HISTORY (Boolean)
    Currently supported only for archive databases.
If set to false, corrected and canceled ticks are removed from the time
series.
If set to true, corrected and canceled ticks are stored in the time
series with their TICK_STATUS set to UPDATED and DELETED_TIME set to
the current time, respectively. Therefore, setting this parameter to
true requires that TICK_STATUS and DELETED_TIME fields are present in
the tick schema.
For accelerator and memory databases, this value cannot be changed from
the default.
Default: true

  
  APPLY_ON_RELOAD
    If this is false, the correction will be applied to the archive
once, and will be recorded to the corrections file with "expiration
time", which is TIMEOUT seconds from that moment. Once expired,
neither archive loader (if archive is reloaded), nor memory loader (if
in-memory database exists for that day) will apply this correction.
Default: true

  
  OUTPUT
    This parameter accepts two values: aggregated and detailed.
It controls how the correction result will be shown to the user.
Aggregated includes the OPERATION_ID field. For detailed output, a
separate row will appear for each &lt;archive
unit&gt; (day for daily archives, month for
monthly archives, the whole interval for continuous archives, and memdb
separately). The row will show the number of affected rows, the
operation ID (which is the same for all rows,) start/end times for that
unit, whether the operation is in progress and the error message if the
operation has failed.
Default: aggregated

  
  TIMEOUT
    If the correction is to be applied to the in-memory database,
the memory loader will log the operation result, and this event
processor will check the log, locating the operation by its ID. The
checking will be done once per second, and the maximum number of
attempts is controlled via the TIMEOUT parameter. If the entry is
found, NUM_AFFECTED_ROWS will be set correspondingly. Otherwise,
IN_PROGRESS will be set to 1.
This parameter also controls loader behavior if APPLY_ON_RELOAD is
false (see below).
Default: 5

  

Examples:

DB::UPDATE (SET='SIZE=SIZE*10',WHERE='SIZE &lt; 100')

See the Data Modeling in OneTick documentation for more information
about data modifications.


	"""
	class Parameters:
		set = "SET"
		where = "WHERE"
		keep_history = "KEEP_HISTORY"
		apply_on_reload = "APPLY_ON_RELOAD"
		output = "OUTPUT"
		timeout = "TIMEOUT"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["set", "where", "keep_history", "apply_on_reload", "output", "timeout"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["set", "_default_set", "where", "_default_where", "keep_history", "_default_keep_history", "apply_on_reload", "_default_apply_on_reload", "output", "_default_output", "timeout", "_default_timeout", "stack_info", "_used_strings"]

	class Output:
		AGGREGATED = "aggregated"
		DETAILED = "detailed"

	def __init__(self, set="", where="", keep_history=True, apply_on_reload=True, output=Output.AGGREGATED, timeout=5):
		_graph_components.EpBase.__init__(self, "DB/UPDATE")
		self._default_set = ""
		self.set = set
		self._default_where = ""
		self.where = where
		self._default_keep_history = True
		self.keep_history = keep_history
		self._default_apply_on_reload = True
		self.apply_on_reload = apply_on_reload
		self._default_output = type(self).Output.AGGREGATED
		self.output = output
		self._default_timeout = 5
		self.timeout = timeout
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_set(self, value):
		self.set = value
		return self

	def set_where(self, value):
		self.where = value
		return self

	def set_keep_history(self, value):
		self.keep_history = value
		return self

	def set_apply_on_reload(self, value):
		self.apply_on_reload = value
		return self

	def set_output(self, value):
		self.output = value
		return self

	def set_timeout(self, value):
		self.timeout = value
		return self

	@staticmethod
	def _get_name():
		return "DB/UPDATE"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.set != "": 
			desc += "SET=" + py_to_str(self.set) + ","
		if self.where != "": 
			desc += "WHERE=" + py_to_str(self.where) + ","
		if self.keep_history != True: 
			desc += "KEEP_HISTORY=" + py_to_str(self.keep_history) + ","
		if self.apply_on_reload != True: 
			desc += "APPLY_ON_RELOAD=" + py_to_str(self.apply_on_reload) + ","
		if self.output != self.Output.AGGREGATED: 
			desc += "OUTPUT=" + py_to_str(self.output) + ","
		if self.timeout != 5: 
			desc += "TIMEOUT=" + py_to_str(self.timeout) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class DiskUsage(_graph_components.EpBase):
	"""
		

DISK_USAGE

Type: Other

Description: This event processor (EP) shows the current
usage statistics on a disk (mount point, total size, used size,
available size, use percentage).

It must be the first (source) EP on the graph.

Input symbols for the queries that involve this EP are expected to
be any valid path on the required disk.

Python
class name:&nbsp;DiskUsage

Input: A time series of ticks.

Output: Disk usage statistics.

Parameters: There are no parameters for this event processor.

Examples: Shows the disk usage:

SHOW_DISK_USAGE()


	"""
	class Parameters:
		compute_real_usage = "COMPUTE_REAL_USAGE"
		values_type = "VALUES_TYPE"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["compute_real_usage", "values_type"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["compute_real_usage", "_default_compute_real_usage", "values_type", "_default_values_type", "stack_info", "_used_strings"]

	class ValuesType:
		B = "B"
		GB = "GB"
		KB = "KB"
		MB = "MB"

	def __init__(self, compute_real_usage=False, values_type=ValuesType.MB):
		_graph_components.EpBase.__init__(self, "DISK_USAGE")
		self._default_compute_real_usage = False
		self.compute_real_usage = compute_real_usage
		self._default_values_type = type(self).ValuesType.MB
		self.values_type = values_type
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_compute_real_usage(self, value):
		self.compute_real_usage = value
		return self

	def set_values_type(self, value):
		self.values_type = value
		return self

	@staticmethod
	def _get_name():
		return "DISK_USAGE"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.compute_real_usage != False: 
			desc += "COMPUTE_REAL_USAGE=" + py_to_str(self.compute_real_usage) + ","
		if self.values_type != self.ValuesType.MB: 
			desc += "VALUES_TYPE=" + py_to_str(self.values_type) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class FindDbSymbols(_graph_components.EpBase):
	"""
		

FIND_DB_SYMBOLS

Type: Other

Description: Takes all of the archives that fall into the
query range and produces a union of all their symbols. Since the
symbols' information is kept per database and databases can cover more
than one day, some symbols that are not loaded for the query range may
be retrieved. It also optionally translates the symbols into a
user-specified symbology and propagates those that match a specified
name pattern as a tick with a single field: SYMBOL_NAME.

If symbol name translation is performed, symbol names in native
database symbology can optionally be propagated as another tick field:
ORIGINAL_SYMBOL_NAME, in which case symbols with missing translations
are also propagated. Symbol name translation is performed as of the
query symbol date and requires the reference database to be configured.

The name of the database to query is extracted from the input
symbol. For example, if an input symbol is TAQ::, the symbols from the
TAQ database will be returned.

Parameter PATTERN can contain special characters % (matches
any number of characters) and _ (matches any character). For
example, pattern I_M% will return any symbol beginning with I and has M
as its third letter.

If parameter DISCARD_ON_MATCH is set to true, the names that do not
match the pattern will be propagated.

The timestamps of the ticks created by FIND_DB_SYMBOLS are set to
the start time of the query.

This event processor must be an input node of the graph, since it
generates ticks.

Python
class name:&nbsp;FindDbSymbols

Input: None

Output: A time series of ticks.

Parameters:


  PATTERN (string)
    The pattern for symbol selection. It can contain special
characters % (matches any number of characters) and _
(matches any character). To avoid this special interpretation of
characters % and _,the \\ character must be
added in front of them. Note that if symbol name translation is
required, translated rather than original symbol names are checked to
match the name pattern.&nbsp;
Default: %

  
  IGNORE_CASE_IN_PATTERN (Boolean)
    If set to true, the pattern for the symbol selection is applied
in the case-insensitive way.
Default:&nbsp;false

  
  SYMBOLOGY (string)
    The destination symbology for a symbol name translation, the
latter being performed, if destination symbology is not empty and is
different from that of the queried database.
Default: empty

  
  SHOW_ORIGINAL_SYMBOLS (Boolean)
    Switches original symbol name propagation as a tick field after
symbol name translation is performed. Note that if this parameter is
set to true, database symbols with
missing translations are also propagated.
Default: false

  
  TICK_TYPE (string)
    If specified, only symbols that have ticks for this tick type
will be returned. You can specify only a single tick type represented
as an allowed string, for example "SYM".
Default: empty

  
  SHOW_TICK_TYPE (Boolean)
    Adds a TICK_TYPE field to
output ticks with corresponding tick type values. Every symbol will be
output as many times as it has associated tick types, which means that
those ticks for the fixed symbol will differ only by tick type values.
Default: false

  
  DISCARD_ON_MATCH
(Boolean)
  CEP_METHOD (string)
    The method to be used for extracting database symbols in CEP
mode. Possible non-empty values are:

    
      USE_DB: symbols will
be extracted from the database with intervals specified by the POLL_FREQUENCY parameter, and new symbols
will be output.
      USE_CEP_ADAPTER: CEP
adapter will be used to retrieve and propagate the symbols with every
heartbeat. Note: symbols, that only have image ticks, will not be
discovered unless use_image_ticks_in_symbol_discovery=true in rawdb CEP
module's feed section.
    
Default: None, the EP will work the same way as for historical queries,
i.e. will query the database for symbols once.
  POLL_FREQUENCY (integer)
    Specifies the time interval in minutes to check the database for
new symbols. This parameter can be specified only if CEP_METHOD is set to USE_DB.
The minimum value is 1 minute.
Default: 1

  
  SYMBOLS_TO_RETURN (string)
    Indicates whether all symbols must be returned or only those
which are in the query time range. Possible values are:

    
      ALL_IN_DB: All
symbols are returned.
      WITH_TICKS_IN_QUERY_RANGE:
Only the symbols which have ticks in the query time range are returned.
This option is allowed only when CEP_METHOD
is set to USE_CEP_ADAPTER.
    
Default: ALL_IN_DB
PREPEND_DB_NAME (Boolean)
    Specifies whether to prepend the database name to the symbol name.
Default: true

  

Examples:

Find all NYSE symbols in a Refinitiv database:

FIND_DB_SYMBOLS("%.N")

This is often used in an .otq file for "eval(&lt;otq&gt;,
params)" statements when binding symbols.

See the STAGE_123_DB_CHAIN
example in OTHER_EXAMPLES.otq.


	"""
	class Parameters:
		pattern = "PATTERN"
		ignore_case_in_pattern = "IGNORE_CASE_IN_PATTERN"
		symbology = "SYMBOLOGY"
		show_original_symbols = "SHOW_ORIGINAL_SYMBOLS"
		tick_type_field = "TICK_TYPE"
		show_tick_type = "SHOW_TICK_TYPE"
		discard_on_match = "DISCARD_ON_MATCH"
		cep_method = "CEP_METHOD"
		poll_frequency = "POLL_FREQUENCY"
		symbols_to_return = "SYMBOLS_TO_RETURN"
		prepend_db_name = "PREPEND_DB_NAME"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["pattern", "ignore_case_in_pattern", "symbology", "show_original_symbols", "tick_type_field", "show_tick_type", "discard_on_match", "cep_method", "poll_frequency", "symbols_to_return", "prepend_db_name"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["pattern", "_default_pattern", "ignore_case_in_pattern", "_default_ignore_case_in_pattern", "symbology", "_default_symbology", "show_original_symbols", "_default_show_original_symbols", "tick_type_field", "_default_tick_type_field", "show_tick_type", "_default_show_tick_type", "discard_on_match", "_default_discard_on_match", "cep_method", "_default_cep_method", "poll_frequency", "_default_poll_frequency", "symbols_to_return", "_default_symbols_to_return", "prepend_db_name", "_default_prepend_db_name", "stack_info", "_used_strings"]

	class CepMethod:
		EMPTY = ""
		USE_CEP_ADAPTER = "USE_CEP_ADAPTER"
		USE_DB = "USE_DB"

	class SymbolsToReturn:
		ALL_IN_DB = "ALL_IN_DB"
		WITH_TICKS_IN_QUERY_RANGE = "WITH_TICKS_IN_QUERY_RANGE"

	def __init__(self, pattern="", ignore_case_in_pattern=False, symbology="", show_original_symbols=False, tick_type_field="", show_tick_type=False, discard_on_match=False, cep_method=CepMethod.EMPTY, poll_frequency="", symbols_to_return=SymbolsToReturn.ALL_IN_DB, prepend_db_name=True):
		_graph_components.EpBase.__init__(self, "FIND_DB_SYMBOLS")
		self._default_pattern = ""
		self.pattern = pattern
		self._default_ignore_case_in_pattern = False
		self.ignore_case_in_pattern = ignore_case_in_pattern
		self._default_symbology = ""
		self.symbology = symbology
		self._default_show_original_symbols = False
		self.show_original_symbols = show_original_symbols
		self._default_tick_type_field = ""
		self.tick_type_field = tick_type_field
		self._default_show_tick_type = False
		self.show_tick_type = show_tick_type
		self._default_discard_on_match = False
		self.discard_on_match = discard_on_match
		self._default_cep_method = type(self).CepMethod.EMPTY
		self.cep_method = cep_method
		self._default_poll_frequency = ""
		self.poll_frequency = poll_frequency
		self._default_symbols_to_return = type(self).SymbolsToReturn.ALL_IN_DB
		self.symbols_to_return = symbols_to_return
		self._default_prepend_db_name = True
		self.prepend_db_name = prepend_db_name
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_pattern(self, value):
		self.pattern = value
		return self

	def set_ignore_case_in_pattern(self, value):
		self.ignore_case_in_pattern = value
		return self

	def set_symbology(self, value):
		self.symbology = value
		return self

	def set_show_original_symbols(self, value):
		self.show_original_symbols = value
		return self

	def set_tick_type_field(self, value):
		self.tick_type_field = value
		return self

	def set_show_tick_type(self, value):
		self.show_tick_type = value
		return self

	def set_discard_on_match(self, value):
		self.discard_on_match = value
		return self

	def set_cep_method(self, value):
		self.cep_method = value
		return self

	def set_poll_frequency(self, value):
		self.poll_frequency = value
		return self

	def set_symbols_to_return(self, value):
		self.symbols_to_return = value
		return self

	def set_prepend_db_name(self, value):
		self.prepend_db_name = value
		return self

	@staticmethod
	def _get_name():
		return "FIND_DB_SYMBOLS"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.pattern != "": 
			desc += "PATTERN=" + py_to_str(self.pattern) + ","
		if self.ignore_case_in_pattern != False: 
			desc += "IGNORE_CASE_IN_PATTERN=" + py_to_str(self.ignore_case_in_pattern) + ","
		if self.symbology != "": 
			desc += "SYMBOLOGY=" + py_to_str(self.symbology) + ","
		if self.show_original_symbols != False: 
			desc += "SHOW_ORIGINAL_SYMBOLS=" + py_to_str(self.show_original_symbols) + ","
		if self.tick_type_field != "": 
			desc += "TICK_TYPE=" + py_to_str(self.tick_type_field) + ","
		if self.show_tick_type != False: 
			desc += "SHOW_TICK_TYPE=" + py_to_str(self.show_tick_type) + ","
		if self.discard_on_match != False: 
			desc += "DISCARD_ON_MATCH=" + py_to_str(self.discard_on_match) + ","
		if self.cep_method != self.CepMethod.EMPTY: 
			desc += "CEP_METHOD=" + py_to_str(self.cep_method) + ","
		if self.poll_frequency != "": 
			desc += "POLL_FREQUENCY=" + py_to_str(self.poll_frequency) + ","
		if self.symbols_to_return != self.SymbolsToReturn.ALL_IN_DB: 
			desc += "SYMBOLS_TO_RETURN=" + py_to_str(self.symbols_to_return) + ","
		if self.prepend_db_name != True: 
			desc += "PREPEND_DB_NAME=" + py_to_str(self.prepend_db_name) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class FindSnapshotSymbols(_graph_components.EpBase):
	"""
		

FIND_SNAPSHOT_SYMBOLS

Type: Other

Description: Takes snapshot's name and storage type and
produces a union of all symbols in the snapshot. It also optionally
translates the symbols into a user-specified symbology and propagates
those that match a specified name pattern as a tick with a single
field: SYMBOL_NAME.

If symbol name translation is performed, symbol names in native
snapshot can optionally be propagated as another tick field: ORIGINAL_SYMBOL_NAME, in which case
symbols with missing translations are also propagated. Symbol name
translation is performed as of the query symbol date and requires the
reference database to be configured.

The name of the database to query is extracted from the input
symbol. For example, if an input symbol is TAQ::,
the symbols from the TAQ database will be returned.

The PATTERN parameter can contain the
special characters % (matches any number of characters) and _
(matches any character). For example, the pattern I_M% returns any symbol beginning with I and has M
as its third letter.

If the DISCARD_ON_MATCH parameter
is set to true, the names that do
not match the pattern will be propagated.

The timestamps of the ticks created by FIND_SNAPSHOT_SYMBOLS are set
to the start time of the query.

This event processor must be an input node of the graph, since it
generates ticks.

Python
class name:&nbsp;FindSnapshotSymbols

Input: None

Output: A time series of ticks.

Parameters:


  SNAPSHOT_NAME (string)
    The name that was specified in SAVE_SNAPSHOT as a SNAPSHOT_NAME
during saving.
Default: VALUE

  
  SNAPSHOT_STORAGE (enumerated)
    This parameter specifies the place of storage of the snapshot.
Possible options are:

    
      MEMORY
        The snapshot is stored in the dynamic (heap) memory of the
process that ran (or is still running) the SAVE_SNAPSHOT
EP for the snapshot.

      
      MEMORY_MAPPED_FILE
        The snapshot is stored in a memory mapped file. For each
symbol to get the location of the snapshot in the file system,
FIND_SNAPSHOT_SYMBOLS looks at the SAVE_SNAPSHOT_DIR
parameter value in the locator section for the database of the symbol.
Default: MEMORY

      
    
  
  PATTERN (string)
    The pattern for symbol selection. It can contain special
characters % (matches any number of characters) and _
(matches any character). To avoid this special interpretation of
characters % and _,the \\ character must be
added in front of them. Note that if symbol name translation is
required, translated rather than original symbol names are checked to
match the name pattern.

  
  SYMBOLOGY (string)
    The destination symbology for a symbol name translation, the
latter being performed, if destination symbology is not empty and is
different from that of the queried database.
Default: empty

  
  SHOW_ORIGINAL_SYMBOLS (Boolean)
    Switches original symbol name propagation as a tick field after
symbol name translation is performed. Note that if this parameter is
set to true, database symbols with
missing translations are also propagated.
Default: false

  
  DISCARD_ON_MATCH
(Boolean)


	"""
	class Parameters:
		snapshot_name = "SNAPSHOT_NAME"
		snapshot_storage = "SNAPSHOT_STORAGE"
		pattern = "PATTERN"
		symbology = "SYMBOLOGY"
		show_original_symbols = "SHOW_ORIGINAL_SYMBOLS"
		discard_on_match = "DISCARD_ON_MATCH"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["snapshot_name", "snapshot_storage", "pattern", "symbology", "show_original_symbols", "discard_on_match"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["snapshot_name", "_default_snapshot_name", "snapshot_storage", "_default_snapshot_storage", "pattern", "_default_pattern", "symbology", "_default_symbology", "show_original_symbols", "_default_show_original_symbols", "discard_on_match", "_default_discard_on_match", "stack_info", "_used_strings"]

	class SnapshotStorage:
		MEMORY = "MEMORY"
		MEMORY_MAPPED_FILE = "MEMORY_MAPPED_FILE"

	def __init__(self, snapshot_name="VALUE", snapshot_storage=SnapshotStorage.MEMORY, pattern="", symbology="", show_original_symbols=False, discard_on_match=False):
		_graph_components.EpBase.__init__(self, "FIND_SNAPSHOT_SYMBOLS")
		self._default_snapshot_name = "VALUE"
		self.snapshot_name = snapshot_name
		self._default_snapshot_storage = type(self).SnapshotStorage.MEMORY
		self.snapshot_storage = snapshot_storage
		self._default_pattern = ""
		self.pattern = pattern
		self._default_symbology = ""
		self.symbology = symbology
		self._default_show_original_symbols = False
		self.show_original_symbols = show_original_symbols
		self._default_discard_on_match = False
		self.discard_on_match = discard_on_match
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_snapshot_name(self, value):
		self.snapshot_name = value
		return self

	def set_snapshot_storage(self, value):
		self.snapshot_storage = value
		return self

	def set_pattern(self, value):
		self.pattern = value
		return self

	def set_symbology(self, value):
		self.symbology = value
		return self

	def set_show_original_symbols(self, value):
		self.show_original_symbols = value
		return self

	def set_discard_on_match(self, value):
		self.discard_on_match = value
		return self

	@staticmethod
	def _get_name():
		return "FIND_SNAPSHOT_SYMBOLS"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.snapshot_name != "VALUE": 
			desc += "SNAPSHOT_NAME=" + py_to_str(self.snapshot_name) + ","
		if self.snapshot_storage != self.SnapshotStorage.MEMORY: 
			desc += "SNAPSHOT_STORAGE=" + py_to_str(self.snapshot_storage) + ","
		if self.pattern != "": 
			desc += "PATTERN=" + py_to_str(self.pattern) + ","
		if self.symbology != "": 
			desc += "SYMBOLOGY=" + py_to_str(self.symbology) + ","
		if self.show_original_symbols != False: 
			desc += "SHOW_ORIGINAL_SYMBOLS=" + py_to_str(self.show_original_symbols) + ","
		if self.discard_on_match != False: 
			desc += "DISCARD_ON_MATCH=" + py_to_str(self.discard_on_match) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class FxConverter(_graph_components.EpBase):
	"""
		

FX_CONVERTER

Type: Transformer

Description: Converts values into another currency using
per-symbol currency information loaded into OneTick from the reference
data file. To use it, the REF_DATA_DB_DIR configuration parameter must
be set to point to the location of the reference database (parameter
must be set in one_tick_config.txt - see the Main configuration file
section in OneTick Installation and Administration Guide). Use
of reference data can be avoided if the value of SOURCE_CURRENCY
parameter is set.&nbsp;

Python
class name:&nbsp;FxConverter

Input: A time series of ticks.

Output: A time series of ticks with values of one or more
fields adjusted for FX rate.

Parameters:


  TARGET_CURRENCY
    The currency into which values need to be converted.

  
  FX_DATABASE
    The OneTick database name that holds FX time series for
conversion from the symbol's currency into the target currency.

  
  FX_GRAPH
    The name of the .otq file that contains a graph that processes
ticks from the FX time series before they enter this currency
conversion node. If not specified, the PASSTHROUGH event processor with
parameter GO_BACK_SECONDS=86400 gets applied to FX time series ticks
before they enter this currency conversion node.

  
  FX_GRAPH_PARAMS
    An optional parameter that specifies parameters of the query
specified in FX_GRAPH.

  
  FX_SERIES_TICK_TYPE
    The tick type of FX ticks.
Default: TRD

  
  FIELDS_TO_CONVERT
    A comma-separated list of fields to be converted into the target
currency.
Default: If the ticks that need to be converted contain only one field,
this field will be converted.
Otherwise, it is PRICE, if present.
Otherwise, ASK_PRICE and BID_PRICE, if present.
Otherwise, ASK_VALUE and BID_VALUE, if present.

  
  FX_FIELD
    The field in the FX tick that carries the FX rate.
Default: If an FX tick contains only one field, the name of this field

  
  FX_RATE_TYPE
    Identifies the type of FX rate to use for conversion. For
example, intraday rate or daily rate can be used. The SYMBOLOGY_MAPPING
section of the OneTick reference data file which maps ISO_CURRENCY
symbology (&lt;ISO code for currency1&gt;/&lt;ISO code for
currency2&gt;) to the symbology of the FX_DATABASE, should have
FX_RATE_TYPE values in the exchange column for ISO_CURRENCY symbology.

  
  SOURCE_CURRENCY
    The currency of the input time series. If not specified, the
currency of the security that is represented by the input time series
is determined and used in the conversion process.

  
  SAME_TIMESTAMP_POLICY
    Policy used to select FX tick to apply to a given input tick.
Supported values are SAME_TIMESTAMP_POLICY_ARRIVAL_ORDER and
SAME_TIMESTAMP_POLICY_EACH_WITH_LATEST_FX. When the policy is set to
SAME_TIMESTAMP_POLICY_ARRIVAL_ORDER, the latest FX tick which arrived
at FX_CONVERTER EP before the input tick is used for FX conversion.
When the policy is set to SAME_TIMESTAMP_POLICY_EACH_WITH_LATEST_FX,
the latest FX tick that have the same or smaller timestamp than the
input tick is used for FX conversion.
Default: SAME_TIMESTAMP_POLICY_ARRIVAL_ORDER

  

Examples: Outputs a stream of trade ticks with PRICE field
converted to British pounds.

FX_CONVERTER (TARGET_CURRENCY='GBP',
FX_DATABASE='EBS',FX_FIELD='PRICE');


	"""
	class Parameters:
		target_currency = "TARGET_CURRENCY"
		fx_database = "FX_DATABASE"
		fx_graph = "FX_GRAPH"
		fx_graph_params = "FX_GRAPH_PARAMS"
		fx_series_tick_type = "FX_SERIES_TICK_TYPE"
		fields_to_convert = "FIELDS_TO_CONVERT"
		fx_field = "FX_FIELD"
		fx_rate_type = "FX_RATE_TYPE"
		source_currency = "SOURCE_CURRENCY"
		same_timestamp_policy = "SAME_TIMESTAMP_POLICY"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["target_currency", "fx_database", "fx_graph", "fx_graph_params", "fx_series_tick_type", "fields_to_convert", "fx_field", "fx_rate_type", "source_currency", "same_timestamp_policy"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["target_currency", "_default_target_currency", "fx_database", "_default_fx_database", "fx_graph", "_default_fx_graph", "fx_graph_params", "_default_fx_graph_params", "fx_series_tick_type", "_default_fx_series_tick_type", "fields_to_convert", "_default_fields_to_convert", "fx_field", "_default_fx_field", "fx_rate_type", "_default_fx_rate_type", "source_currency", "_default_source_currency", "same_timestamp_policy", "_default_same_timestamp_policy", "stack_info", "_used_strings"]

	class SameTimestampPolicy:
		ARRIVAL_ORDER = "ARRIVAL_ORDER"
		EACH_WITH_LATEST_FX = "EACH_WITH_LATEST_FX"

	def __init__(self, target_currency="", fx_database="", fx_graph="", fx_graph_params="", fx_series_tick_type="", fields_to_convert="", fx_field="", fx_rate_type="", source_currency="", same_timestamp_policy=SameTimestampPolicy.ARRIVAL_ORDER):
		_graph_components.EpBase.__init__(self, "FX_CONVERTER")
		self._default_target_currency = ""
		self.target_currency = target_currency
		self._default_fx_database = ""
		self.fx_database = fx_database
		self._default_fx_graph = ""
		self.fx_graph = fx_graph
		self._default_fx_graph_params = ""
		self.fx_graph_params = fx_graph_params
		self._default_fx_series_tick_type = ""
		self.fx_series_tick_type = fx_series_tick_type
		self._default_fields_to_convert = ""
		self.fields_to_convert = fields_to_convert
		self._default_fx_field = ""
		self.fx_field = fx_field
		self._default_fx_rate_type = ""
		self.fx_rate_type = fx_rate_type
		self._default_source_currency = ""
		self.source_currency = source_currency
		self._default_same_timestamp_policy = type(self).SameTimestampPolicy.ARRIVAL_ORDER
		self.same_timestamp_policy = same_timestamp_policy
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_target_currency(self, value):
		self.target_currency = value
		return self

	def set_fx_database(self, value):
		self.fx_database = value
		return self

	def set_fx_graph(self, value):
		self.fx_graph = value
		return self

	def set_fx_graph_params(self, value):
		self.fx_graph_params = value
		return self

	def set_fx_series_tick_type(self, value):
		self.fx_series_tick_type = value
		return self

	def set_fields_to_convert(self, value):
		self.fields_to_convert = value
		return self

	def set_fx_field(self, value):
		self.fx_field = value
		return self

	def set_fx_rate_type(self, value):
		self.fx_rate_type = value
		return self

	def set_source_currency(self, value):
		self.source_currency = value
		return self

	def set_same_timestamp_policy(self, value):
		self.same_timestamp_policy = value
		return self

	@staticmethod
	def _get_name():
		return "FX_CONVERTER"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.target_currency != "": 
			desc += "TARGET_CURRENCY=" + py_to_str(self.target_currency) + ","
		if self.fx_database != "": 
			desc += "FX_DATABASE=" + py_to_str(self.fx_database) + ","
		if self.fx_graph != "": 
			desc += "FX_GRAPH=" + py_to_str(self.fx_graph) + ","
		if self.fx_graph_params != "": 
			desc += "FX_GRAPH_PARAMS=" + py_to_str(self.fx_graph_params) + ","
		if self.fx_series_tick_type != "": 
			desc += "FX_SERIES_TICK_TYPE=" + py_to_str(self.fx_series_tick_type) + ","
		if self.fields_to_convert != "": 
			desc += "FIELDS_TO_CONVERT=" + py_to_str(self.fields_to_convert) + ","
		if self.fx_field != "": 
			desc += "FX_FIELD=" + py_to_str(self.fx_field) + ","
		if self.fx_rate_type != "": 
			desc += "FX_RATE_TYPE=" + py_to_str(self.fx_rate_type) + ","
		if self.source_currency != "": 
			desc += "SOURCE_CURRENCY=" + py_to_str(self.source_currency) + ","
		if self.same_timestamp_policy != self.SameTimestampPolicy.ARRIVAL_ORDER: 
			desc += "SAME_TIMESTAMP_POLICY=" + py_to_str(self.same_timestamp_policy) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class ModifyAccessControl(_graph_components.EpBase):
	"""
		

MODIFY_ACCESS_CONTROL

Type: Other

Description: Modifies the content of ACCESS_CONTROL_FILE by
adding or removing permissions for different users/roles. It can be
configured to modify access control recursively on all accessible hosts
or on an explicit host by specifying host name as a query symbol
(represented as "REMOTE@server_ip:server_port").

In order to update access control file ALLOW_REMOTE_CONTROL should
be enabled in OneTick config and user should have permissions for
MODIFY_ACCESS_CONTROL remote operation on the host (see The Remote
Operation section in OneTick Installation and Administration Guide).

Returns a tick per accessed server with either SUCCESS or
FAIL:&lt;error message&gt; in STATUS
field. All output ticks have the timestamp of the query start time.

Python
class name:&nbsp;ModifyAccessControl

Input: No input (this EP is a data source and should be the
root of the graph).

Output: A tick for each accessed server.

Parameters:


  ENTRY_TYPE (enumerated)
    This parameter the access category to modify. Possible options
are:

    
      ROLE
      DATABASE
      EVENT_PROCESSOR
      REMOTE_OPERATION
      RESOURCE
    
  
  ACTION (enumerated)
    The operation to perform for the specified component. Possible
options are:

    
      ADD
      MOD
      REMOVE
    
    Note that MOD will add the entry if the entry is missing in
access control file.

  
  ROLE_NAME
    The role for which requested action should be performed.

  
  USER
    The user for which requested action should be performed. Should
be specified only with ROLE entry
type.

  
  DATABASE
    The database ID for which requested action should be performed.
Should be specified only with DATABASE
entry type.

  
  EVENT_PROCESSOR
    The event processor for which requested action should be
performed. Should be specified only with EVENT_PROCESSOR
entry type.

    A not complete list of supported configurable event processors
are added as enumeration options (it is possible to specify any EP).

  
  REMOTE_OPERATION
    The remote operation for which requested action should be
performed. Should be specified only with REMOTE_OPERATION
entry type.

    A not complete list of supported remote operations are added as
enumeration options (it is possible to specify any remote operation).

  
  RESOURCE
    The resource ID for which requested action should be performed.
Should be specified only with RESOURCE
entry type.

    A not complete list of supported monitoring resources are added
as enumeration options (it is possible to specify any resource).

  
  PROPERTY_VALUE
    The new value of the property specified by PROPERTY_NAME.

  
  PROPERTY_NAME
    The property to update for the specified entry/role. The
property is dropped if used with REMOVE
action, .

  
  DEEP_SCAN (Boolean)
    If true, the EP will perform a deeper scan to modify the access
control on all accessible host.

    Default value: false

  
  SKIP_LOCAL_HOST (Boolean)
    If true, the EP will not modify the access control of the host
initiating the query (client host or the first tick server host).

    Default value: false

  

Examples:

See the examples in modify_access_control_example.otq.


	"""
	class Parameters:
		entry_type = "ENTRY_TYPE"
		action = "ACTION"
		role_name = "ROLE_NAME"
		user = "USER"
		database = "DATABASE"
		event_processor = "EVENT_PROCESSOR"
		graph_tick_function = "GRAPH_TICK_FUNCTION"
		remote_operation = "REMOTE_OPERATION"
		resource = "RESOURCE"
		property_value = "PROPERTY_VALUE"
		property_name = "PROPERTY_NAME"
		deep_scan = "DEEP_SCAN"
		skip_local_host = "SKIP_LOCAL_HOST"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["entry_type", "action", "role_name", "user", "database", "event_processor", "graph_tick_function", "remote_operation", "resource", "property_value", "property_name", "deep_scan", "skip_local_host"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["entry_type", "_default_entry_type", "action", "_default_action", "role_name", "_default_role_name", "user", "_default_user", "database", "_default_database", "event_processor", "_default_event_processor", "graph_tick_function", "_default_graph_tick_function", "remote_operation", "_default_remote_operation", "resource", "_default_resource", "property_value", "_default_property_value", "property_name", "_default_property_name", "deep_scan", "_default_deep_scan", "skip_local_host", "_default_skip_local_host", "stack_info", "_used_strings"]

	class EntryType:
		EMPTY = ""
		DATABASE = "DATABASE"
		EVENT_PROCESSOR = "EVENT_PROCESSOR"
		GRAPH_TICK_FUNCTION = "GRAPH_TICK_FUNCTION"
		REMOTE_OPERATION = "REMOTE_OPERATION"
		RESOURCE = "RESOURCE"
		ROLE = "ROLE"

	class Action:
		EMPTY = ""
		ADD = "ADD"
		MOD = "MOD"
		REMOVE = "REMOVE"

	class EventProcessor:
		EMPTY = ""
		CODE = "CODE"
		COMMAND_EXECUTE = "COMMAND_EXECUTE"
		CSV_FILE_LISTING = "CSV_FILE_LISTING"
		CSV_FILE_QUERY = "CSV_FILE_QUERY"
		DIRECTORY_LISTING = "DIRECTORY_LISTING"
		OTQ_QUERY = "OTQ_QUERY"
		R = "R"
		WRITE_TEXT = "WRITE_TEXT"

	class RemoteOperation:
		EMPTY = ""
		CONFIG_CHANGE = "CONFIG_CHANGE"
		CONFIG_READ = "CONFIG_READ"
		EXPOSED_EVENT_PROCESSORS = "EXPOSED_EVENT_PROCESSORS"
		MODIFY_ACCESS_CONTROL = "MODIFY_ACCESS_CONTROL"
		MONITORING = "MONITORING"
		QUERY_CACHING = "QUERY_CACHING"
		SHUTDOWN = "SHUTDOWN"

	class Resource:
		EMPTY = ""
		ARCHIVE_BYTES_READ = "ARCHIVE_BYTES_READ"
		CPU_TIME = "CPU_TIME"
		MAJOR_PAGE_FAULT = "MAJOR_PAGE_FAULT"
		MEMORY = "MEMORY"
		NUM_THREADS = "NUM_THREADS"
		SOCKET_READ = "SOCKET_READ"
		SOCKET_WRITE = "SOCKET_WRITE"

	class PropertyName:
		EMPTY = ""
		ALLOWED_DIRS = "ALLOWED_DIRS"
		ALLOW_REGULAR_EPS = "ALLOW_REGULAR_EPS"
		DESTROY_ACCESS = "DESTROY_ACCESS"
		DIRECTORIES = "DIRECTORIES"
		KILLING_QUERIES = "KILLING_QUERIES"
		LANGUAGES = "LANGUAGES"
		MAXIMUM_AGE = "MAXIMUM_AGE"
		MINIMUM_AGE = "MINIMUM_AGE"
		OTQ_DIR = "OTQ_DIR"
		PATH_SEPARATOR = "PATH_SEPARATOR"
		PER_QUERY_LIMIT = "PER_QUERY_LIMIT"
		READ_ACCESS = "READ_ACCESS"
		VIEWING_QUERY_CONTENTS = "VIEWING_QUERY_CONTENTS"
		WRITE_ACCESS = "WRITE_ACCESS"

	def __init__(self, entry_type=EntryType.EMPTY, action=Action.EMPTY, role_name="", user="", database="", event_processor=EventProcessor.EMPTY, graph_tick_function="", remote_operation=RemoteOperation.EMPTY, resource=Resource.EMPTY, property_value="", property_name=PropertyName.EMPTY, deep_scan=False, skip_local_host=False):
		_graph_components.EpBase.__init__(self, "MODIFY_ACCESS_CONTROL")
		self._default_entry_type = type(self).EntryType.EMPTY
		self.entry_type = entry_type
		self._default_action = type(self).Action.EMPTY
		self.action = action
		self._default_role_name = ""
		self.role_name = role_name
		self._default_user = ""
		self.user = user
		self._default_database = ""
		self.database = database
		self._default_event_processor = type(self).EventProcessor.EMPTY
		self.event_processor = event_processor
		self._default_graph_tick_function = ""
		self.graph_tick_function = graph_tick_function
		self._default_remote_operation = type(self).RemoteOperation.EMPTY
		self.remote_operation = remote_operation
		self._default_resource = type(self).Resource.EMPTY
		self.resource = resource
		self._default_property_value = ""
		self.property_value = property_value
		self._default_property_name = type(self).PropertyName.EMPTY
		self.property_name = property_name
		self._default_deep_scan = False
		self.deep_scan = deep_scan
		self._default_skip_local_host = False
		self.skip_local_host = skip_local_host
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_entry_type(self, value):
		self.entry_type = value
		return self

	def set_action(self, value):
		self.action = value
		return self

	def set_role_name(self, value):
		self.role_name = value
		return self

	def set_user(self, value):
		self.user = value
		return self

	def set_database(self, value):
		self.database = value
		return self

	def set_event_processor(self, value):
		self.event_processor = value
		return self

	def set_graph_tick_function(self, value):
		self.graph_tick_function = value
		return self

	def set_remote_operation(self, value):
		self.remote_operation = value
		return self

	def set_resource(self, value):
		self.resource = value
		return self

	def set_property_value(self, value):
		self.property_value = value
		return self

	def set_property_name(self, value):
		self.property_name = value
		return self

	def set_deep_scan(self, value):
		self.deep_scan = value
		return self

	def set_skip_local_host(self, value):
		self.skip_local_host = value
		return self

	@staticmethod
	def _get_name():
		return "MODIFY_ACCESS_CONTROL"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.entry_type != self.EntryType.EMPTY: 
			desc += "ENTRY_TYPE=" + py_to_str(self.entry_type) + ","
		if self.action != self.Action.EMPTY: 
			desc += "ACTION=" + py_to_str(self.action) + ","
		if self.role_name != "": 
			desc += "ROLE_NAME=" + py_to_str(self.role_name) + ","
		if self.user != "": 
			desc += "USER=" + py_to_str(self.user) + ","
		if self.database != "": 
			desc += "DATABASE=" + py_to_str(self.database) + ","
		if self.event_processor != self.EventProcessor.EMPTY: 
			desc += "EVENT_PROCESSOR=" + py_to_str(self.event_processor) + ","
		if self.graph_tick_function != "": 
			desc += "GRAPH_TICK_FUNCTION=" + py_to_str(self.graph_tick_function) + ","
		if self.remote_operation != self.RemoteOperation.EMPTY: 
			desc += "REMOTE_OPERATION=" + py_to_str(self.remote_operation) + ","
		if self.resource != self.Resource.EMPTY: 
			desc += "RESOURCE=" + py_to_str(self.resource) + ","
		if self.property_value != "": 
			desc += "PROPERTY_VALUE=" + py_to_str(self.property_value) + ","
		if self.property_name != self.PropertyName.EMPTY: 
			desc += "PROPERTY_NAME=" + py_to_str(self.property_name) + ","
		if self.deep_scan != False: 
			desc += "DEEP_SCAN=" + py_to_str(self.deep_scan) + ","
		if self.skip_local_host != False: 
			desc += "SKIP_LOCAL_HOST=" + py_to_str(self.skip_local_host) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class RefData(_graph_components.EpBase):
	"""
		

REF_DATA

Type: Other

Description: Shows reference data for the specified security
and reference data type. It can be used to view corporation actions,
symbol name changes, primary exchange info and symbology mapping for a
security(ies), as well as the list of symbologies, names of custom
adjustment types for corporate actions present in a reference database
as well as names of continuous contracts in database symbology.

This event processor must be an input node of the graph, since it
generates ticks.

Python
class name:&nbsp;RefData

Input: None

Output: A time series of ticks.

Parameters:


  REF_DATA_TYPE (enumerated type)
    Type of reference data to be queried. Possible values are:

    
      CORP_ACTIONS
      SYMBOL_NAME_HISTORY
      PRIMARY_EXCHANGE
      SYMBOL_CALENDAR
      SYMBOL_CURRENCY
      SYMBOLOGY_MAPPING
      SYMBOLOGY_LIST
      CUSTOM_ADJUSTMENT_TYPE_LIST
      ALL_CALENDARS
      ALL_CONTINUOUS_CONTRACT_NAMES
    
  

Examples: Show corporate actions for a given security:

REF_DATA (REF_DATA_TYPE='CORP_ACTIONS')

See the REF_DATA example in OTHER_EXAMPLES.otq.


	"""
	class Parameters:
		ref_data_type = "REF_DATA_TYPE"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["ref_data_type"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["ref_data_type", "_default_ref_data_type", "stack_info", "_used_strings"]

	class RefDataType:
		ALL_CALENDARS = "ALL_CALENDARS"
		ALL_CONTINUOUS_CONTRACT_NAMES = "ALL_CONTINUOUS_CONTRACT_NAMES"
		CORP_ACTIONS = "CORP_ACTIONS"
		CUSTOM_ADJUSTMENT_TYPE_LIST = "CUSTOM_ADJUSTMENT_TYPE_LIST"
		PRIMARY_EXCHANGE = "PRIMARY_EXCHANGE"
		SYMBOLOGY_LIST = "SYMBOLOGY_LIST"
		SYMBOLOGY_MAPPING = "SYMBOLOGY_MAPPING"
		SYMBOL_CALENDAR = "SYMBOL_CALENDAR"
		SYMBOL_CURRENCY = "SYMBOL_CURRENCY"
		SYMBOL_EXCHANGE = "SYMBOL_EXCHANGE"
		SYMBOL_NAME_HISTORY = "SYMBOL_NAME_HISTORY"

	def __init__(self, ref_data_type=RefDataType.SYMBOL_NAME_HISTORY):
		_graph_components.EpBase.__init__(self, "REF_DATA")
		self._default_ref_data_type = type(self).RefDataType.SYMBOL_NAME_HISTORY
		self.ref_data_type = ref_data_type
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_ref_data_type(self, value):
		self.ref_data_type = value
		return self

	@staticmethod
	def _get_name():
		return "REF_DATA"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.ref_data_type != self.RefDataType.SYMBOL_NAME_HISTORY: 
			desc += "REF_DATA_TYPE=" + py_to_str(self.ref_data_type) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class ShowTickDescriptorInDb(_graph_components.EpBase):
	"""
		

SHOW_TICK_DESCRIPTOR_IN_DB

Type: Filter

Description: This event processor (EP) shows the fields of
the archive database. The output fields are FIELD_NAME, TYPE, SIZE,
COMPRESSION_GROUP (fields with the same compression group compressed
together; 0 means that the field is not compressed), and COMPRESSED
(tells whether the field is, in fact, compressed. If set to 1, SIZE
carries a compressed size in bytes of the field, otherwise its size is
not compressed). COLUMN_INDEX(for databases with column storage mode
this is column number). If COLUMN_INDEX is -1, then database does not
use column storage. TYPE_FOR_EP indicates which type specifier must be
used in EPs (ADD_FIELD , UPDATE_FIELD , etc.)

This EP shows the static tick descriptor of the data in the
database. To see the dynamic tick descriptor potentially affected by
the EPs in your query, use SHOW_TICK_DESCRIPTOR
EP.

Python
class name:
ShowTickDescriptorInDb

Input: No input (this EP is a data source and should be the
root of the graph).

Output: A tick for each tick descriptor in each archive day.

Parameters: There are no parameters for this event processor.

Examples: Show all archive tick descriptors:

SHOW_TICK_DESCRIPTOR_IN_DB ()

See the SHOW_TICK_DESCRIPTOR_IN_DB
example in FILTER_EXAMPLES.otq.


	"""
	class Parameters:
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = []
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["stack_info", "_used_strings"]

	def __init__(self):
		_graph_components.EpBase.__init__(self, "SHOW_TICK_DESCRIPTOR_IN_DB")
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	@staticmethod
	def _get_name():
		return "SHOW_TICK_DESCRIPTOR_IN_DB"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class ShowOtEntitlements(_graph_components.EpBase):
	"""
		

SHOW_OT_ENTITLEMENTS

Type: Other

Description: Shows configuration of OT_entitlements module
that applies to the database of queried symbol(s). Only a specified
part of the entitlements configuration, specified via EP parameter
INFO_TYPE, is shown All output ticks have the timestamp of the query's
start time.

Python
class name:&nbsp;ShowOtEntitlements

Input: No input (this EP is a data source and should be the
root of the graph).

Output: A tick for each entry of each component.

Parameters:


  INFO_TYPE (enumerated)
    This parameter specifies the component to get information about.
Possible options are USERNAME_ROLE_MAP,
    SYMBOL_PE, ROLE_PE_MAP,
    ROLE_TICK_TYPES_MAP and ROLE_SYMBOL_LIMITS:

    
      USERNAME_ROLE_MAP
        If the SHOW_FOR_ALL_USERS option is not
specified, then EP shows the list of username and user role (user
group) mappings to which the user has access:

        
          USERNAME
          ROLE
        
        Output example:

        Index,Symbol,Time,USERNAME,ROLE1,DEMO_DATABASE::,2020/01/15,09:30:00.000,developer,role1
        If the SHOW_FOR_ALL_USERS option is
specified, then the EP shows all username/role mappings.

      
      SYMBOL_PE
        For this parameter SHOW_FOR_ALL_USERS
option will be skipped. The EP shows the list of symbol/PE mappings.

        
          SYMBOL
          PE
        
        Output example:

        Index,Symbol,Time,SYMBOL,PE1,DEMO_DATABASE::,2020/01/15 09:30:00.000,SYMBOL_0,0
      
      ROLE_TICK_TYPES_MAP
        If the SHOW_FOR_ALL_USERS option is not
set, the EP shows the list of role, role type (allow or deny),
comma-separated tick types information for the current user:

        
          ROLE
          ROLE_TYPE
          TICK_TYPES
        
        Output example:

        Index,Symbol,Time,ROLE,ROLE_TYPE,TICK_TYPES1,DEMO_DATABASE::,2020/01/15 09:30:00.000,role1,allow,PRL,QTE
        If the SHOW_FOR_ALL_USERS option is
set, the EP shows the list of role, role type (allow or deny),
comma-separated tick types information for all users

      
      ROLE_SYMBOL_LIMITS
        If the SHOW_FOR_ALL_USERS option is not
set, the EP shows the list of roles to their symbol limits for the
current user:

        
          ROLE
          PER_ROLE_LIMIT
          PER_USER_LIMIT
        
        Output example:

        Index,Symbol,Time,ROLE,PER_ROLE_LIMIT,PER_USER_LIMIT1,DEMO_DATABASE::,2020/01/15 09:30:00.000,role1,2,30
        If the SHOW_FOR_ALL_USERS option is
set, the EP shows the list of roles to their symbol limits for all
users.

      
      ROLE_PE_MAP
        If the SHOW_FOR_ALL_USERS option is not
set, the EP shows the list of user roles (user groups) to their
permissions for symbol groups for the current user:

        
          ROLE
          PE
          MIN_TICK_AGE
        
        Output example:

        Index,Symbol,Time,ROLE,PE,MIN_TICK_AGE1,DEMO_DATABASE::,2020/01/15 09:30:00.000,role1,1,00:20:00
        If SHOW_FOR_ALL_USERS
is set, the EP shows the list of user roles (user groups) to their
permissions for symbol groups for all users.

      
      ROLE_SYMBOL_MAP
        If the SHOW_FOR_ALL_USERS option is not
set, the EP shows the list of user roles (user groups) to their
permissions for symbols for the current user:

        
          ROLE
          SYMBOL
          MIN_TICK_AGE
        
        Output example:

        Index,Symbol,Time,ROLE,SYMBOL,MIN_TICK_AGE1,FULL_DEMO_L1::,2020/01/15 09:30:00.000,role1,^A.*,00:20:00
        If SHOW_FOR_ALL_USERS
is set, the EP shows the list of user roles (user groups) to their
permissions for symbols for all users.

      
    
  
  SHOW_FOR_ALL_USERS (Boolean)
    If true, the EP will show information for all roles instead of
the current user. Under OTHER_ROLE, the default permissions will be shown. For more details, see its meaning in different types.

    Default: false

  

Access control:

By default, all users are allowed to use SHOW_FOR_ALL_USERS=true
option. In order to prevent some roles from use of this option, while
still allowing other uses of SHOW_OT_ENTITLEMENTS event processor to
everyone, the following entry should be added to the access control
file (using appropriate role names ):

&lt;event_processors&gt;  &lt;ep ID="SHOW_OT_ENTITLEMENTS" EXECUTE_ACCESS="true"&gt;    &lt;allow role="role_X" SHOW_FOR_ALL_USERS_PERMISSION="false"/&gt;    &lt;allow role="role_Y" SHOW_FOR_ALL_USERS_PERMISSION="false"/&gt;  &lt;/ep&gt;&lt;/event_processors&gt;
Examples:

See the examples in show_ot_entitlemens_examples.otq.


	"""
	class Parameters:
		info_type = "INFO_TYPE"
		show_for_all_users = "SHOW_FOR_ALL_USERS"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["info_type", "show_for_all_users"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["info_type", "_default_info_type", "show_for_all_users", "_default_show_for_all_users", "stack_info", "_used_strings"]

	class InfoType:
		EMPTY = ""
		ROLE_PE_MAP = "ROLE_PE_MAP"
		ROLE_SYMBOL_LIMITS = "ROLE_SYMBOL_LIMITS"
		ROLE_SYMBOL_MAP = "ROLE_SYMBOL_MAP"
		ROLE_TICK_TYPES_MAP = "ROLE_TICK_TYPES_MAP"
		SYMBOL_PE = "SYMBOL_PE"
		USERNAME_ROLE_MAP = "USERNAME_ROLE_MAP"

	def __init__(self, info_type=InfoType.EMPTY, show_for_all_users=False):
		_graph_components.EpBase.__init__(self, "SHOW_OT_ENTITLEMENTS")
		self._default_info_type = type(self).InfoType.EMPTY
		self.info_type = info_type
		self._default_show_for_all_users = False
		self.show_for_all_users = show_for_all_users
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_info_type(self, value):
		self.info_type = value
		return self

	def set_show_for_all_users(self, value):
		self.show_for_all_users = value
		return self

	@staticmethod
	def _get_name():
		return "SHOW_OT_ENTITLEMENTS"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.info_type != self.InfoType.EMPTY: 
			desc += "INFO_TYPE=" + py_to_str(self.info_type) + ","
		if self.show_for_all_users != False: 
			desc += "SHOW_FOR_ALL_USERS=" + py_to_str(self.show_for_all_users) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class ShowQueryProperties(_graph_components.EpBase):
	"""
		

SHOW_QUERY_PROPERTIES

Type: Other

Description: This EP extracts the properties of an OTQ query.

SHOW_QUERY_PROPERTIES must be the first (that is to
say, the source) EP on the graph.

Python
class name:&nbsp;ShowQueryProperties

Input: OTQ query

Output: OTQ query properties

Parameters:


  QUERY_NAME (string)
    Specifies the OTQ query, the properties of which must be shown.
The following format should be used:

    &lt;OTQ_FILENAME&gt;::&lt;QUERY_NAME&gt;

  
  PROPERTY (enumerated type)
    Specifies the information that should be returned from the OTQ
query. Possible properties are:

    
      QUERY_PARAMETERS - Gets
parameters for the corresponding OTQ query
      OUTPUT_SCHEMA - Gets the output
schema for the corresponding OTQ (see also, REQUIRE_OUTPUT_PROTOTYPE
parameter)
      QUERY_PROPERTIES - Gets
properties for the corresponding OTQ query (start time, end time,
timezone, batch size, etc.)
      SYMBOL_LIST - Gets unbound
symbol list for the corresponding OTQ query
      QUERY_DESCRIPTION - Gets
description for the corresponding OTQ query
    
    Default: QUERY_PARAMETERS

  
  REQUIRE_OUTPUT_PROTOTYPE
(Boolean)
    If set to TRUE, schemas of only those queries are returned which
end with TABLE EP. When a query doesn't end with TABLE EP that query
needs to be executed to determine its actual schema. To allow execution
specify FALSE for this parameter. Note, that executing a query
potentially takes very long time.
Default: TRUE

  
  RESOLVE_OTQ_PARAMETERS
(Boolean)
    If set to TRUE, OTQ parameters of the queries are resolved and
their values are placed, otherwise parameters are not resolved. This
works for all properties except for OUTPUT_SCHEMA.
Default: TRUE

  
  SHOW_QUERY_PARAMETER_DESCRIPTIONS
(Boolean)
    If set to TRUE, descriptions of query parameters are fetched.
This works only for QUERY_PARAMETERS.
Default: FALSE

  

Examples: Shows parameters for TICK_GENERATOR.otq::Graph_1
query:

SHOW_QUERY_PROPERTIES(QUERY_NAME="OTHER_EXAMPLES.otq::SHOW_EP_LIST",PROPERTY=SYMBOL_LIST)

See the SHOW_QUERY_PROPERTIES
example in OTHER_EXAMPLES.otq.


	"""
	class Parameters:
		query_name = "QUERY_NAME"
		property = "PROPERTY"
		require_output_prototype = "REQUIRE_OUTPUT_PROTOTYPE"
		resolve_otq_parameters = "RESOLVE_OTQ_PARAMETERS"
		show_query_parameter_descriptions = "SHOW_QUERY_PARAMETER_DESCRIPTIONS"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["query_name", "property", "require_output_prototype", "resolve_otq_parameters", "show_query_parameter_descriptions"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["query_name", "_default_query_name", "property", "_default_property", "require_output_prototype", "_default_require_output_prototype", "resolve_otq_parameters", "_default_resolve_otq_parameters", "show_query_parameter_descriptions", "_default_show_query_parameter_descriptions", "stack_info", "_used_strings"]

	class Property:
		OUTPUT_SCHEMA = "OUTPUT_SCHEMA"
		QUERY_DESCRIPTION = "QUERY_DESCRIPTION"
		QUERY_PARAMETERS = "QUERY_PARAMETERS"
		QUERY_PROPERTIES = "QUERY_PROPERTIES"
		SYMBOL_LIST = "SYMBOL_LIST"

	def __init__(self, query_name="", property=Property.QUERY_PARAMETERS, require_output_prototype=True, resolve_otq_parameters=True, show_query_parameter_descriptions=False):
		_graph_components.EpBase.__init__(self, "SHOW_QUERY_PROPERTIES")
		self._default_query_name = ""
		self.query_name = query_name
		self._default_property = type(self).Property.QUERY_PARAMETERS
		self.property = property
		self._default_require_output_prototype = True
		self.require_output_prototype = require_output_prototype
		self._default_resolve_otq_parameters = True
		self.resolve_otq_parameters = resolve_otq_parameters
		self._default_show_query_parameter_descriptions = False
		self.show_query_parameter_descriptions = show_query_parameter_descriptions
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_query_name(self, value):
		self.query_name = value
		return self

	def set_property(self, value):
		self.property = value
		return self

	def set_require_output_prototype(self, value):
		self.require_output_prototype = value
		return self

	def set_resolve_otq_parameters(self, value):
		self.resolve_otq_parameters = value
		return self

	def set_show_query_parameter_descriptions(self, value):
		self.show_query_parameter_descriptions = value
		return self

	@staticmethod
	def _get_name():
		return "SHOW_QUERY_PROPERTIES"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.query_name != "": 
			desc += "QUERY_NAME=" + py_to_str(self.query_name) + ","
		if self.property != self.Property.QUERY_PARAMETERS: 
			desc += "PROPERTY=" + py_to_str(self.property) + ","
		if self.require_output_prototype != True: 
			desc += "REQUIRE_OUTPUT_PROTOTYPE=" + py_to_str(self.require_output_prototype) + ","
		if self.resolve_otq_parameters != True: 
			desc += "RESOLVE_OTQ_PARAMETERS=" + py_to_str(self.resolve_otq_parameters) + ","
		if self.show_query_parameter_descriptions != False: 
			desc += "SHOW_QUERY_PARAMETER_DESCRIPTIONS=" + py_to_str(self.show_query_parameter_descriptions) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class ShowArchiveStats(_graph_components.EpBase):
	"""
		

SHOW_ARCHIVE_STATS

Type: Other

Description: This event processor (EP) shows various stats
about the queried symbol, as well as an archive as a whole for each day
within the queried interval. Accelerator databases are not supported.
Memory databases will be ignored even within their life hours. It must
be the first (source) EP on the graph.


Python
class name:&nbsp;ShowArchiveStats

Input: A time series of ticks.

Output: Archive stats:


  COMPRESSION_TYPE - archive compression type. In older
archives native compression flag is not stored, so for example for gzip
compression this field may say "GZIP or NATIVE_PLUS_GZIP". The
meta_data_upgrader.exe tool can be used to determine and inject that
information in such cases in order to get a more precise result in this
field.
  TIME_RANGE_VALIDITY - whether lowest and highest loaded
timestamps (see below) are known. Like native compression flag, this
information is missing in older archives and can be added using
meta_data_upgrader tool.
  LOWEST_LOADED_DATETIME - the lowest loaded timestamp for
the queried interval (across all symbols)
  HIGHEST_LOADED_DATETIME - the highest loaded timestamp for
the queried interval (across all symbols)
  TOTAL_TICKS - the number of ticks for the queried interval
(across all symbols). Also missing in older archives and can be added
using meta_data_upgrader. If not available, -1 will be returned
  SYMBOL_DATA_SIZE - the size of the symbol in archive in
bytes. This information is also missing in older archives, however the
other options, it cannot later be added. In such cases -1 will be
returned.
  TOTAL_SYMBOLS - the number of symbols for the queried
interval
  TOTAL_SIZE - archive size in bytes for the queried
interval (including the garbage potentially accumulated during appends).
ARCHIVE_MODIFICATION_TIME - the latest modification time of the archive.

Parameters: There are no parameters for this event processor.

Examples: Shows archive stats:

SHOW_ARCHIVE_STATS()

See the SHOW_ARCHIVE_STATS
example in OTHER_EXAMPLES.otq.


	"""
	class Parameters:
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = []
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["stack_info", "_used_strings"]

	def __init__(self):
		_graph_components.EpBase.__init__(self, "SHOW_ARCHIVE_STATS")
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	@staticmethod
	def _get_name():
		return "SHOW_ARCHIVE_STATS"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class SymbologyMapping(_graph_components.EpBase):
	"""
		

SYMBOLOGY_MAPPING

Type: Other.

Description: Shows symbology mapping information for
specified security(ies) stored in the reference database. Input
(source) symbology is taken from the input symbol, if it has a
symbology part in it (e.g., RIC::REUTERS::MSFT), or defaults to that of
the input database, which is specified in the locator file.

This event processor must be an input node of the graph, since it
generates ticks.

Python
class name:&nbsp;SymbologyMapping

Input: None

Output: A time series of ticks.

Parameters:


  DEST_SYMBOLOGY (string)
    A mandatory parameter specifying the destination symbology for
symbol translation.

  

Example:

SYMBOLOGY_MAPPING(DEST_SYMBOLOGY='OID')

Access control: We can prevent symbol translation to certain symbologies by specifying a comma-separated list of symbologies for DISALLOWED_TARGET_SYMBOLOGIES restriction in MISCELLANEOUS_RESTRICTIONS. 

&lt;MISCELLANEOUS_RESTRICTIONS&gt;
  &lt;restr id="DISALLOWED_TARGET_SYMBOLOGIES" symbologies=""&gt;
	&lt;allow role="role1" symbologies="SYMBOLOGY1,SYMBOLOGY2"&gt;
  &lt;/restr&gt;
&lt;/MISCELLANEOUS_RESTRICTIONS&gt;

In this example users in role1 won't be able to perform translation into SYMBOLOGY1 and SYMBOLOGY2, no restrictions for users of other roles.


	"""
	class Parameters:
		dest_symbology = "DEST_SYMBOLOGY"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["dest_symbology"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["dest_symbology", "_default_dest_symbology", "stack_info", "_used_strings"]

	def __init__(self, dest_symbology=""):
		_graph_components.EpBase.__init__(self, "SYMBOLOGY_MAPPING")
		self._default_dest_symbology = ""
		self.dest_symbology = dest_symbology
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_dest_symbology(self, value):
		self.dest_symbology = value
		return self

	@staticmethod
	def _get_name():
		return "SYMBOLOGY_MAPPING"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.dest_symbology != "": 
			desc += "DEST_SYMBOLOGY=" + py_to_str(self.dest_symbology) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class WriteText(_graph_components.EpBase):
	"""
		

WRITE_TEXT

Type: OutputAdapter

Description: Writes the input tick series to a text file for
formatting the output. Also propagates input ticks if the PROPAGATE_TICKS
parameter is set to TRUE.

This event processor must not be used with num_cores greater than 1
and batch_size not equal to 0.

Python
class name:&nbsp;WriteText

Input: A time series of ticks.

Output: A time series of ticks or none.

Parameters:


  PROPAGATE_TICKS (Boolean)
    Switches propagation of the ticks. If set to true, ticks will be
propagated.
Default: true

  
  FLUSH (Boolean)
    When set to true, forces the
output to be flushed to disk after every tick. Notice that while this
setting makes results of the query recorded into a file without delay,
making them immediately available to applications that read this file,
it may slow down the query significantly.
Default: true

  
  OUTPUT_FILE_COMPRESSION_TYPE
    Specifies the compression type for the output file. Supported types include GZIP, ZSTD, and LZ4.
Default: NONE

  
  TREAT_INPUT_AS_BINARY (Boolean)
    Opens output file in binary mode to not modify content of ticks
when printing them to the file. Also in this mode EP prints no new line
to the file after every tick write.
Default: false

  
  OUTPUT_DIR (string)
    If specified, all output (output, warning, error, and data
quality) files will be redirected to it. If this directory does not
exist, it will get created.

  
  OUTPUT_HEADERS (Boolean)
    Switches the output of the headers. If this parameter is set, a
tick descriptor line appears in the output before the very first tick
for that query. If the structure of the output tick changes, another
tick descriptor line appears before the first changed tick. The header
line starts with #. The field names
are ordered as mandated by the ORDER parameter or, if it is
empty, in the order of appearance in the tick descriptor. Fields that
are not specified in the ORDER parameter will appear after
specified ones in the order of their appearance in the tick descriptor.

    The format of a tick descriptor header line is the following:

    #&lt;column1_name&gt;&lt;sep&gt;&lt;column2_name&gt;&lt;sep&gt;...&lt;columnN_name&gt;,

    where sep is the separator
defined in the SEPARATOR parameter.
Default: true

  
  OUTPUT_TYPES_IN_HEADERS
(Boolean)
    Switches the output of field types in the header lines. Type can
be long, double, decimal (128 bit base 10
floating point type), nsectime, msectime, varstring
or string[&lt;size&gt;]. For backward compatibility msectime
and nsectime can be printed as long if OneTick
configuration parameter
COMPATIBILITY.WRITE_TEXT.SHOW_TIME_TYPES_AS_LONG is set to true. OUTPUT_TYPES_IN_HEADERS
can be set to true only when OUTPUT_HEADERS is set to true.
Default: false

  
  ORDER (string)
    The field appearance order in the output. Field names should be
listed in the desired order separated with '|'
signs. If it is empty, fields will be written in the order of their
appearance in the tick descriptor.

    For instance:

    order=INTERNAL_CODE|ISIN|PRICE|SIZE

  
  SEPARATOR (string)
    The delimiter string. This doesn't have to be a single
character. Escape sequences are allowed for \\t
(tab), \\\\ (\\)
and \\xHH (hex codes). If not set,
"," will be used. If delimiter is a single character other than double
quotes, it will be properly escaped, that is, the fields that contain
it will be surrounded by double quotes.

  
  OUTPUT_FILE (string)
    The output file name for generated text data. If not set, the
standard output will be used. It allows adding symbol name, database
name, tick type, date of tick and query start time to the name. For
this parameter special words should be used, and then they will be
replaced with the appropriate values:

    
      %SYMBOL% - will be replaced
with symbol name,
      %DBNAME% - with database
name,
      %TICKTYPE% - with tick type,
      %DATE% - with date of tick,
      %STARTTIME% - with start
time of the query.
    
    For instance:

    output_%SYMBOL%_%DBNAME%.txt

    This format is also available for ERROR_FILE, WARNING_FILE
and DATA_QUALITY_FILE input parameters.

  
  ERROR_FILE (string)
    The file name where all error messages are directed. If not set
the standard error will be used.

  
  WARNING_FILE (string)
    The file name where all warning messages are directed. If not
set the standard error will be used.

  
  DATA_QUALITY_FILE (string)
    The file name where all data quality messages are directed. If
not set the standard error will be used.

  
  FORMATS_OF_FIELDS (string)
    Multiline input parameter. Each line contains field_name=field_output_format, where field_output_format is the format of the
field field_name. field_output_format
is simply the string with the placeholder for the field value like in
the standard C printf function - for more details see https://pubs.opengroup.org/onlinepubs/009695399/functions/printf.html.
For decimal fields "%f" and "%.[&lt;precision&gt;]f" formats are only
supported, first one being the default an outputting 6 decimal digits.
Also if the field_output_format starts
with "%|", it means that this is a
time field and should be in the format "%|tz|time_format_spec",
where the tzis the time zone (if not
specified GMT will be used), and time_format_spec
is a custom time format specification, which is the same as the one
used by the standard Cstrftime function (see http://pubs.opengroup.org/onlinepubs/009695399/functions/strftime.html).
In addition, you can also use "%q" , "%Q" , "%k"
and "%J" placeholders can also be
used, which will be replaced by 3 and 2 sign milliseconds, 6 sign
microseconds and 9 sign nanoseconds, respectively. "%#", "%-"
, "%U", "%N"
placeholders will be replaced by Unix timestamp, Unix timestamp in
milliseconds, microseconds and nanoseconds, respectively. "%+" and "%~"
placeholders will be replaced by milliseconds and nanoseconds passed
since midnight.
For instance, TIME=%|EST5EDT|%d-%m-%Y
%H:%M:%S.%q or EXCH_TIME=%||%H:%M:%S, etc.
    
     Note, that TIMESTAMP=%|tz|time_format_spec pair
carries a special meaning under certain circumstances. That is, if tick
descriptors do not have a field, named TIMESTAMP, then such a format
specification applies to tick timestamps and tick timestamps are output
even if PREPEND_TIMESTAMP
parameter is set to false. Moreover, if both TIMESTAMP field is present
and PREPEND_TIMESTAMP
parameter is set to true, then such a format specification applies both
to tick timestamps and values of the TIMESTAMP field.

  
  DOUBLE_FORMAT (string)
    This format will be used for fields that are holding double
values, and which are not specified in FORMAT_OF_FIELDS.
Default: %f

  
  PREPEND_SYMBOL_NAME (Boolean)
    If set to true, prepends symbol_name before other fields and
SYMBOL_NAME in the header line (if OUTPUT_HEADERS is set to
true).
Default: true

  
  PREPENDED_SYMBOL_NAME_SIZE
(integer)
    when PREPEND_SYMBOL_NAME is set to true, symbol will be
adjusted to this size. If set to 0 no adjustment will be done
Default: 0

  
  PREPEND_TIMESTAMP (Boolean)
    If set to true, tick timestamps, formatted as
YYYYMMDDhhmmss.qqqqqq in the GMT time zone, will be prepended to the
output lines. Header lines, if present, will have TIMESTAMP as the
first field name. If other, than the default output format for tick
timestamps is preferred, it can be specified
in the FORMAT_OF_FIELDS parameter.
Default: true

  
  APPEND (Boolean)
    If set to true, will try to append data to files (output, error,
warning, data_quality), instead of overwriting.
Default: false

  
  ALLOW_CONCURRENT_WRITE
(Boolean)
    Allows different queries running on the same server to write
concurrently to the same files (output, error, warning, data_quality).
Default: false

  

Access control:

By default, users are not allowed to use this event processor to
write into server directories. To be able to use this event processor
on the server, one should configure it in the access control file. If
WRITE_TEXT is present in the access control event processor list, then
only the specified roles can use this event processor for writing to
files (writing to stdout and stderr is allowed in any case).
Additionally, for each role allowed directories can be specified. If
some directory is allowed, all of its subdirectories are allowed.
Omitting this parameter means that all directories are allowed. If a
user has several roles, she has access to a directory if it is allowed
for at least one of her roles. One can also specify allowed directories
list in the event processor level (not only role level). This means
that directories listed in the event processor level are allowed for
all users (whose roles aren't mentioned under the section for this
event processor, or user does not have any role in the access control
file). Role level permissions override event processor level
permissions for the users whose roles are mentioned in access control
for this ep.

Directories are specified using any separator symbol (colon is the
default). Here is an example how to specify role level permissions:

&lt;allow role="developers"
directories="/home/developer;/work" path_separator=";"/&gt;.

Here is an example how to specify event processor level permissions:

&lt;ep id="WRITE_TEXT"
directories="/home/developer;/work" path_separator=";"&gt;

Notes:

Previously, this EP displayed the tick scheme for each symbol, 
even if it was the same for all symbols. To enable this behavior for compatibility 
purposes, you can set the COMPATIBILITY.SHOW_SCHEMA_PER_SYMBOL configuration variable to true.

Examples:

In this example, ticks are written into the "output.txt file" in the
"D:/WriteText/output" directory, error messages and warning messages
are written into "errors_and_warnings.txt" in the same directory, and
data quality messages are written into the standard error file (because
the parameter is empty). It prepends symbol_name and timestamp. Output
headers do not flush after every tick and propagate ticks. The fields
will be written in "EXCH_TIME|INTERNAL_CODE|ISIN|PRICE|SIZE"
order, the separator is a tab, and there are formats for three fields.
The internal code is an integer and will be printed with a width of 10.
The price is of double type and will be printed with 8 digits after the
dot. The timezone for exch_time is EST5EDT, and the output will be in
the form of:
"01&nbsp; hr&nbsp; 23&nbsp; min&nbsp;
45&nbsp; sec&nbsp; 678&nbsp; msec".

WRITE_TEXT(true,false,"D:/WriteText/output",true,"EXCH_TIME|INTERNAL_CODE|ISIN|PRICE|SIZE","\\t","output.txt","errors_and_warnings.txt","errors_and_warnings.txt",,"INTERNAL_CODE=%10d PRICE=%.8f EXCH_TIME=%|EST5EDT|%H  hr  %M  min  %S  sec  %q  msec",true,true)
See the WRITE_TEXT example in Output_Adapters.otq.


	"""
	class Parameters:
		propagate_ticks = "PROPAGATE_TICKS"
		flush = "FLUSH"
		output_file_compression_type = "OUTPUT_FILE_COMPRESSION_TYPE"
		treat_input_as_binary = "TREAT_INPUT_AS_BINARY"
		output_dir = "OUTPUT_DIR"
		output_headers = "OUTPUT_HEADERS"
		output_types_in_headers = "OUTPUT_TYPES_IN_HEADERS"
		order = "ORDER"
		separator = "SEPARATOR"
		output_file = "OUTPUT_FILE"
		error_file = "ERROR_FILE"
		warning_file = "WARNING_FILE"
		data_quality_file = "DATA_QUALITY_FILE"
		formats_of_fields = "FORMATS_OF_FIELDS"
		double_format = "DOUBLE_FORMAT"
		prepend_symbol_name = "PREPEND_SYMBOL_NAME"
		prepended_symbol_name_size = "PREPENDED_SYMBOL_NAME_SIZE"
		prepend_timestamp = "PREPEND_TIMESTAMP"
		append = "APPEND"
		allow_concurrent_write = "ALLOW_CONCURRENT_WRITE"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["propagate_ticks", "flush", "output_file_compression_type", "treat_input_as_binary", "output_dir", "output_headers", "output_types_in_headers", "order", "separator", "output_file", "error_file", "warning_file", "data_quality_file", "formats_of_fields", "double_format", "prepend_symbol_name", "prepended_symbol_name_size", "prepend_timestamp", "append", "allow_concurrent_write"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["propagate_ticks", "_default_propagate_ticks", "flush", "_default_flush", "output_file_compression_type", "_default_output_file_compression_type", "treat_input_as_binary", "_default_treat_input_as_binary", "output_dir", "_default_output_dir", "output_headers", "_default_output_headers", "output_types_in_headers", "_default_output_types_in_headers", "order", "_default_order", "separator", "_default_separator", "output_file", "_default_output_file", "error_file", "_default_error_file", "warning_file", "_default_warning_file", "data_quality_file", "_default_data_quality_file", "formats_of_fields", "_default_formats_of_fields", "double_format", "_default_double_format", "prepend_symbol_name", "_default_prepend_symbol_name", "prepended_symbol_name_size", "_default_prepended_symbol_name_size", "prepend_timestamp", "_default_prepend_timestamp", "append", "_default_append", "allow_concurrent_write", "_default_allow_concurrent_write", "stack_info", "_used_strings"]

	def __init__(self, propagate_ticks=True, flush=True, output_file_compression_type="NONE", treat_input_as_binary=False, output_dir="", output_headers=True, output_types_in_headers=False, order="", separator="", output_file="", error_file="", warning_file="", data_quality_file="", formats_of_fields="", double_format="%f", prepend_symbol_name=True, prepended_symbol_name_size="", prepend_timestamp=True, append=False, allow_concurrent_write=False):
		_graph_components.EpBase.__init__(self, "WRITE_TEXT")
		self._default_propagate_ticks = True
		self.propagate_ticks = propagate_ticks
		self._default_flush = True
		self.flush = flush
		self._default_output_file_compression_type = "NONE"
		self.output_file_compression_type = output_file_compression_type
		self._default_treat_input_as_binary = False
		self.treat_input_as_binary = treat_input_as_binary
		self._default_output_dir = ""
		self.output_dir = output_dir
		self._default_output_headers = True
		self.output_headers = output_headers
		self._default_output_types_in_headers = False
		self.output_types_in_headers = output_types_in_headers
		self._default_order = ""
		self.order = order
		self._default_separator = ""
		self.separator = separator
		self._default_output_file = ""
		self.output_file = output_file
		self._default_error_file = ""
		self.error_file = error_file
		self._default_warning_file = ""
		self.warning_file = warning_file
		self._default_data_quality_file = ""
		self.data_quality_file = data_quality_file
		self._default_formats_of_fields = ""
		self.formats_of_fields = formats_of_fields
		self._default_double_format = "%f"
		self.double_format = double_format
		self._default_prepend_symbol_name = True
		self.prepend_symbol_name = prepend_symbol_name
		self._default_prepended_symbol_name_size = ""
		self.prepended_symbol_name_size = prepended_symbol_name_size
		self._default_prepend_timestamp = True
		self.prepend_timestamp = prepend_timestamp
		self._default_append = False
		self.append = append
		self._default_allow_concurrent_write = False
		self.allow_concurrent_write = allow_concurrent_write
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_propagate_ticks(self, value):
		self.propagate_ticks = value
		return self

	def set_flush(self, value):
		self.flush = value
		return self

	def set_output_file_compression_type(self, value):
		self.output_file_compression_type = value
		return self

	def set_treat_input_as_binary(self, value):
		self.treat_input_as_binary = value
		return self

	def set_output_dir(self, value):
		self.output_dir = value
		return self

	def set_output_headers(self, value):
		self.output_headers = value
		return self

	def set_output_types_in_headers(self, value):
		self.output_types_in_headers = value
		return self

	def set_order(self, value):
		self.order = value
		return self

	def set_separator(self, value):
		self.separator = value
		return self

	def set_output_file(self, value):
		self.output_file = value
		return self

	def set_error_file(self, value):
		self.error_file = value
		return self

	def set_warning_file(self, value):
		self.warning_file = value
		return self

	def set_data_quality_file(self, value):
		self.data_quality_file = value
		return self

	def set_formats_of_fields(self, value):
		self.formats_of_fields = value
		return self

	def set_double_format(self, value):
		self.double_format = value
		return self

	def set_prepend_symbol_name(self, value):
		self.prepend_symbol_name = value
		return self

	def set_prepended_symbol_name_size(self, value):
		self.prepended_symbol_name_size = value
		return self

	def set_prepend_timestamp(self, value):
		self.prepend_timestamp = value
		return self

	def set_append(self, value):
		self.append = value
		return self

	def set_allow_concurrent_write(self, value):
		self.allow_concurrent_write = value
		return self

	@staticmethod
	def _get_name():
		return "WRITE_TEXT"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.propagate_ticks != True: 
			desc += "PROPAGATE_TICKS=" + py_to_str(self.propagate_ticks) + ","
		if self.flush != True: 
			desc += "FLUSH=" + py_to_str(self.flush) + ","
		if self.output_file_compression_type != "NONE": 
			desc += "OUTPUT_FILE_COMPRESSION_TYPE=" + py_to_str(self.output_file_compression_type) + ","
		if self.treat_input_as_binary != False: 
			desc += "TREAT_INPUT_AS_BINARY=" + py_to_str(self.treat_input_as_binary) + ","
		if self.output_dir != "": 
			desc += "OUTPUT_DIR=" + py_to_str(self.output_dir) + ","
		if self.output_headers != True: 
			desc += "OUTPUT_HEADERS=" + py_to_str(self.output_headers) + ","
		if self.output_types_in_headers != False: 
			desc += "OUTPUT_TYPES_IN_HEADERS=" + py_to_str(self.output_types_in_headers) + ","
		if self.order != "": 
			desc += "ORDER=" + py_to_str(self.order) + ","
		if self.separator != "": 
			desc += "SEPARATOR=" + py_to_str(self.separator) + ","
		if self.output_file != "": 
			desc += "OUTPUT_FILE=" + py_to_str(self.output_file) + ","
		if self.error_file != "": 
			desc += "ERROR_FILE=" + py_to_str(self.error_file) + ","
		if self.warning_file != "": 
			desc += "WARNING_FILE=" + py_to_str(self.warning_file) + ","
		if self.data_quality_file != "": 
			desc += "DATA_QUALITY_FILE=" + py_to_str(self.data_quality_file) + ","
		if self.formats_of_fields != "": 
			desc += "FORMATS_OF_FIELDS=" + py_to_str(self.formats_of_fields) + ","
		if self.double_format != "%f": 
			desc += "DOUBLE_FORMAT=" + py_to_str(self.double_format) + ","
		if self.prepend_symbol_name != True: 
			desc += "PREPEND_SYMBOL_NAME=" + py_to_str(self.prepend_symbol_name) + ","
		if self.prepended_symbol_name_size != "": 
			desc += "PREPENDED_SYMBOL_NAME_SIZE=" + py_to_str(self.prepended_symbol_name_size) + ","
		if self.prepend_timestamp != True: 
			desc += "PREPEND_TIMESTAMP=" + py_to_str(self.prepend_timestamp) + ","
		if self.append != False: 
			desc += "APPEND=" + py_to_str(self.append) + ","
		if self.allow_concurrent_write != False: 
			desc += "ALLOW_CONCURRENT_WRITE=" + py_to_str(self.allow_concurrent_write) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class Omd_writeToSolace(_graph_components.EpBase):
	"""
		

OMD::WRITE_TO_SOLACE

Type: Output Adapter

Description: Publishes incoming ticks to a Solace broker.

Python
class name:&nbsp;Omd_writeToSolace

Input: A time series of ticks

Output: A time series of ticks or none

Parameters:


  PROPAGATE_TICKS (Boolean)
    Switches on the propagation of ticks. If set to true, ticks are propagated.
Default: true

  
  HOSTNAME (string)
    The IP address (or host name) to connect to.

  
  VPN (string)
    The name of the Message VPN to attempt to join when connecting
to the broker.

  
  USERNAME (string)
    The username required for authentication.

  
  PASSWORD (string)
    The password required for authentication.

  
  ENDPOINT (string)
    Endpoint name, depending on used DELIVERY_MODE
this is either topic name(DIRECT mode
is used) or queue name(GUARANTEED
mode is used).

  
  DELIVERY_MODE (enum)
    Specifies the delivery mode: DIRECT
or GUARANTEED.

  
  MSG_FORMAT (enum)
    Specifies the output format: NAME_VALUE_PAIRS
, BINARY or STRING_VAL_OF_SINGLE_FIELD.
In case of NAME_VALUE_PAIRS, each
tick is converted into a sequence of name-value pairs.
In case of BINARY, ticks are sent in
binary format with occasional tick descriptor submissions. This format
is required if the subscriber is OneTick application. If another
application is going to subscribe for the published data, NAME_VALUE_PAIRS should be used to receive
self-descriptive ticks.
In case of STRING_VAL_OF_SINGLE_FIELD
the single field of the tick will be converted into string.EP throws an
exception when gets ticks that consist of more than one field.
Note that BINARY format provides
better performance. Default: NAME_VALUE_PAIRS

  
  TICK_DESCRIPTOR_RESEND_INTERVAL
(int64)
    Sets an interval to resend tick descriptor for each time series.

  


	"""
	class Parameters:
		propagate_ticks = "PROPAGATE_TICKS"
		hostname = "HOSTNAME"
		vpn = "VPN"
		username = "USERNAME"
		password = "PASSWORD"
		endpoint = "ENDPOINT"
		endpoint_type = "ENDPOINT_TYPE"
		delivery_mode = "DELIVERY_MODE"
		msg_format = "MSG_FORMAT"
		tick_descriptor_resend_interval = "TICK_DESCRIPTOR_RESEND_INTERVAL"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["propagate_ticks", "hostname", "vpn", "username", "password", "endpoint", "endpoint_type", "delivery_mode", "msg_format", "tick_descriptor_resend_interval"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["propagate_ticks", "_default_propagate_ticks", "hostname", "_default_hostname", "vpn", "_default_vpn", "username", "_default_username", "password", "_default_password", "endpoint", "_default_endpoint", "endpoint_type", "_default_endpoint_type", "delivery_mode", "_default_delivery_mode", "msg_format", "_default_msg_format", "tick_descriptor_resend_interval", "_default_tick_descriptor_resend_interval", "stack_info", "_used_strings"]

	class EndpointType:
		QUEUE = "QUEUE"
		TOPIC = "TOPIC"

	class DeliveryMode:
		DIRECT = "DIRECT"
		GUARANTEED = "GUARANTEED"

	class MsgFormat:
		BINARY = "BINARY"
		NAME_VALUE_PAIRS = "NAME_VALUE_PAIRS"
		STRING_VAL_OF_SINGLE_FIELD = "STRING_VAL_OF_SINGLE_FIELD"

	def __init__(self, propagate_ticks=True, hostname="", vpn="", username="", password="", endpoint="", endpoint_type=EndpointType.QUEUE, delivery_mode=DeliveryMode.GUARANTEED, msg_format=MsgFormat.NAME_VALUE_PAIRS, tick_descriptor_resend_interval=""):
		_graph_components.EpBase.__init__(self, "OMD::WRITE_TO_SOLACE")
		self._default_propagate_ticks = True
		self.propagate_ticks = propagate_ticks
		self._default_hostname = ""
		self.hostname = hostname
		self._default_vpn = ""
		self.vpn = vpn
		self._default_username = ""
		self.username = username
		self._default_password = ""
		self.password = password
		self._default_endpoint = ""
		self.endpoint = endpoint
		self._default_endpoint_type = type(self).EndpointType.QUEUE
		self.endpoint_type = endpoint_type
		self._default_delivery_mode = type(self).DeliveryMode.GUARANTEED
		self.delivery_mode = delivery_mode
		self._default_msg_format = type(self).MsgFormat.NAME_VALUE_PAIRS
		self.msg_format = msg_format
		self._default_tick_descriptor_resend_interval = ""
		self.tick_descriptor_resend_interval = tick_descriptor_resend_interval
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_propagate_ticks(self, value):
		self.propagate_ticks = value
		return self

	def set_hostname(self, value):
		self.hostname = value
		return self

	def set_vpn(self, value):
		self.vpn = value
		return self

	def set_username(self, value):
		self.username = value
		return self

	def set_password(self, value):
		self.password = value
		return self

	def set_endpoint(self, value):
		self.endpoint = value
		return self

	def set_endpoint_type(self, value):
		self.endpoint_type = value
		return self

	def set_delivery_mode(self, value):
		self.delivery_mode = value
		return self

	def set_msg_format(self, value):
		self.msg_format = value
		return self

	def set_tick_descriptor_resend_interval(self, value):
		self.tick_descriptor_resend_interval = value
		return self

	@staticmethod
	def _get_name():
		return "OMD::WRITE_TO_SOLACE"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.propagate_ticks != True: 
			desc += "PROPAGATE_TICKS=" + py_to_str(self.propagate_ticks) + ","
		if self.hostname != "": 
			desc += "HOSTNAME=" + py_to_str(self.hostname) + ","
		if self.vpn != "": 
			desc += "VPN=" + py_to_str(self.vpn) + ","
		if self.username != "": 
			desc += "USERNAME=" + py_to_str(self.username) + ","
		if self.password != "": 
			desc += "PASSWORD=" + py_to_str(self.password) + ","
		if self.endpoint != "": 
			desc += "ENDPOINT=" + py_to_str(self.endpoint) + ","
		if self.endpoint_type != self.EndpointType.QUEUE: 
			desc += "ENDPOINT_TYPE=" + py_to_str(self.endpoint_type) + ","
		if self.delivery_mode != self.DeliveryMode.GUARANTEED: 
			desc += "DELIVERY_MODE=" + py_to_str(self.delivery_mode) + ","
		if self.msg_format != self.MsgFormat.NAME_VALUE_PAIRS: 
			desc += "MSG_FORMAT=" + py_to_str(self.msg_format) + ","
		if self.tick_descriptor_resend_interval != "": 
			desc += "TICK_DESCRIPTOR_RESEND_INTERVAL=" + py_to_str(self.tick_descriptor_resend_interval) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class DbShowConfig(_graph_components.EpBase):
	"""
		

DB/SHOW_CONFIG

Type: Other

Description: Shows the specified configuration for a database.

Python
class name:&nbsp;DbShowConfig

Input: None

Output: One or multiple ticks

Parameters:


  DB_NAME
    Specifies the database name to get the configuration for. If not
specified, the database name is extracted from the bound symbol.

  
  CONFIG_TYPE (DB_TIME_INTERVALS,
LOCATOR_ENTRY - default)
    If LOCATOR_ENTRY is
specified, a string representing db's locator entry along with VDB_FLAG (this flag equals 1 when the database is virtual and 0 otherwise) will be returned.

    If DB_TIME_INTERVALS is
specified, then time intervals configured in the locator file will be
propagated including additional information, such as LOCATION, ARCHIVE_DURATION,
    DAY_BOUNDARY_TZ, DAY_BOUNDARY_OFFSET, ALTERNATIVE_LOCATIONS, etc.

  

Access control:

EP shows a configuration information if user has READ_CONFIG permission or there is no READ_CONFIG category in the
OneTickInstallation&amp;AdministrationGuide.html's "Access Control
File" section.

EP will not be accessible if READ_CONFIG
category exists but a given user is not listed there.

See OneTickInstallation&amp;AdministrationGuide.html's "Remote
Operations" section, for additional information.

Examples:

DB/SHOW_CONFIG(CONFIG_TYPE=LOCATOR_ENTRY)
See the DB_SHOW_CONFIG example in
OTHER_EXAMPLES.otq.


	"""
	class Parameters:
		db_name = "DB_NAME"
		config_type = "CONFIG_TYPE"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["db_name", "config_type"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["db_name", "_default_db_name", "config_type", "_default_config_type", "stack_info", "_used_strings"]

	class ConfigType:
		DB_TIME_INTERVALS = "DB_TIME_INTERVALS"
		LOCATOR_ENTRY = "LOCATOR_ENTRY"

	def __init__(self, db_name="", config_type=ConfigType.LOCATOR_ENTRY):
		_graph_components.EpBase.__init__(self, "DB/SHOW_CONFIG")
		self._default_db_name = ""
		self.db_name = db_name
		self._default_config_type = type(self).ConfigType.LOCATOR_ENTRY
		self.config_type = config_type
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_db_name(self, value):
		self.db_name = value
		return self

	def set_config_type(self, value):
		self.config_type = value
		return self

	@staticmethod
	def _get_name():
		return "DB/SHOW_CONFIG"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.db_name != "": 
			desc += "DB_NAME=" + py_to_str(self.db_name) + ","
		if self.config_type != self.ConfigType.LOCATOR_ENTRY: 
			desc += "CONFIG_TYPE=" + py_to_str(self.config_type) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class CreateCache(_graph_components.EpBase):
	"""
		

CREATE_CACHE


Query result caches

A family of event processors consisting of CREATE_CACHE, READ_CACHE, DELETE_CACHE
allow creation and manipulation of query result caches.

The cache
Query result cache is a named in-memory object maintained on the
server. As the name suggests, the object keeps query results, similar
to how a database would. Series can be retrieved from a cache using the
READ_CACHE event processor. The objective
is usually to avoid repeated costly computation by caching the result
of a query and reading the series from the cache when needed.

Basic flow
The first step is to create a cache . The creation step does NOT
populate the cache, it is only populated when an attempt is made to
read the data from it. A simple way to create a cache CREATE_CACHE
event processor takes a query name (file.otq::Query). This query will
be used to populate the cache with data, when needed. Again, the cache
is not populated by CREATE_CACHE.

There are alternative ways of creating the cache. For the cases
where you're not sure where to put the CREATE_CACHE event processor
(maybe there's ambiguity as to which query should create the cache),
READ_CACHE has an option to specify the query to perform the creation
on an as-needed basis.

There's also a configuration parameter QUERY_CACHES_FILE (see
config_vars.html) that allows creating the caches from a description in
the file (the format is described below.
Conceptually, all these methods do the same thing as invocation of
CREATE_CACHE.

The cache is populated when an attempt to read the data is made
using the READ_CACHE event processor. If you read from the same cache
twice in a row (using the exact same parameters, times, symbols in your
READ_CACHE EP), the first time around READ_CACHE will run the query in
question and return the data, but also store the data in the cache
(unless explicitly told not to cache, see parameters). The second
invocation of the same READ_CACHE request will return the data from the
cache. (This is the behavior for value AUTOMATIC of the READ_MODE parameter of READ_CACHE,
see its documentation for other options.)

So the idea is to always use READ_CACHE to fetch the data, knowing
that it will run the underlying query only when needed, otherwise
returning the cached results.

The cache can be disposed of using the DELETE_CACHE EP.

Caching granularity and different time intervals
In the previous section, we described the case when READ_CACHE is
invoked with the exact same parameters, symbols, times, etc. In this
case, the cache naturally assumes that the result should be the same
and returns that result. What happens when the query invocation is
slightly different?

First off, if the values of $-parameters of the query are different
across invocations (which is possible as READ_CACHE has an OTQ_PARAMS parameter), the results are assumed
to be unrelated and are cached separately.

When only the symbols specified to the query differ across
invocations, the query will need to be run for the previously missing
symbols. The results for the symbols that were already cached from
previous executions (assuming other parameters are the same) aren't
recomputed.

The most complicated question is: what happens when the start/end
times specified are different across calls to READ_CACHE?

By default (when INHERITABILITY
parameter is set to 'true'), READ_CACHE will only run the query for
intervals that are not already present in the cache. Note that this may
mean that an invocation of READ_CACHE can cause the query to be
executed for disjoint intervals.

For example, let's assume that we have times T1 &lt; T2 &lt; T3
&lt; T4. If we run the query for interval [T2, T3) first,
the result for that interval will be cached. If we then run the query
for the interval [T1, T4), the query will be run for intervals [T1,
T2) and [T3, T4), while the part in between will be fetched
("inherited") from the cache.

You can witness this behavior using the CACHE_EXAMPLE.otq. In this
example, the underlying query is set to add the time when each tick is
generated to the output. You can therefore see whether the tick comes
from the cache or is computed when you run READ_CACHE. If you play with
different intervals, you'll see that "older" ticks can be mixed with
"new" ones.

This behavior introduces important requirements on the queries used
in caching, discussed in the next section.

Note that this behavior of reusing existing intervals can be turned
off (using INHERITABILITY), in which
case you have to specify explicit time intervals that will be used in
READ_CACHE.

Underlying query requirements
While READ_CACHE will run any query, the way it coalesces intervals
to minimize querying when INHERITABILITY
is set to true, described in the previous section, implies requirements
on the query. Since READ_CACHE will fetch previously computed time
intervals from the cache, the results of the query must be consistent
with respect to this partial computation. Namely:

If the query is run for any intervals [A1, A2) and [B1, B2) and
the results of these runs overlap in time, the overlapping parts of the
result series should be the same.

In other words, the query results should be invariant to the
start/end times used. If the query doesn't satisfy this requirement,
the query results will be inconsistent depending on what intervals were
used and in what sequence, leading to strange, non-recurring results.

IMPORTANT: when caching with INHERITABILITY=true, only
use queries whose results are invariant to start/end times specified.

It is interesting to note that the cached query in CACHE_EXAMPLE.otq (called
slow_query) does not satisfy this requirement as it produces a field
"computed_time" whose value obviously changes for every two executions.
This field indicates which ticks were created via query execution and
which were fetched from the cache.

When INHERITABILITY is false, CREATE_CACHE mandates that a
predefined list of intervals that will be queried be specified via TIME_INTERVALS_TO_CACHE parameter.
The cache will be maintained separately for every interval from the
list. There's no issue of consistency in that case and the query
semantics becomes less important.

Query caches file
The caches can be pre-declared in a file specified by
QUERY_CACHES_FILE parameter (see config_vars.html). In this case,
there's no need to explicitly invoke CREATE_CACHE. The format of the
file is as follows:

&lt;VERSION_INFO VERSION="1"&gt; &lt;/VERSION_INFO&gt;&lt;CACHES&gt;  &lt;CACHE ID="cache_name" OTQ_PATH="otq_file_path" ...&gt;    &lt;TIME_INTERVALS&gt;      &lt;TIME_INTERVAL START_TIME="YYYYMMDDhhmmss[.msec]" END_TIME="YYYYMMDDhhmmss[.msec]" /&gt;      ...      &lt;TIME_INTERVAL START_TIME="YYYYMMDDhhmmss[.msec]" END_TIME="YYYYMMDDhhmmss[.msec]" /&gt;    &lt;/TIME_INTERVALS&gt;    &lt;OTQ_PARAMS&gt;      &lt;OTQ_PARAM NAME="string" VALUE="string" /&gt;      ...      &lt;OTQ_PARAM NAME="string" VALUE="string" /&gt;    &lt;/OTQ_PARAMS&gt;  &lt;/CACHE&gt;&lt;/CACHES&gt;
Supported properties for &lt;CACHE&gt; tag are INHERITABILITY,
TIME_GRANULARITY, TIME_GRANULARITY_UNITS, TIMEZONE,
ALLOW_SEARCH_TO_EVERYONE, ALLOW_DELETE_TO_EVERYONE and
ALLOW_UPDATE_TO_EVERYONE - the same as CREATE_CACHE
EP parameters. An example:

&lt;VERSION_INFO VERSION="1"&gt;&lt;/VERSION_INFO&gt;&lt;CACHES&gt;        &lt;CACHE ID="test" OTQ_PATH="./queries.otq::test_query" INHERITABILITY="true" CACHE_EXPIRATION_INTERVAL=60&gt;        &lt;/CACHE&gt;&lt;/CACHES&gt;

Type: Other

Description: Creates and configures a query result cache
object but does not populate it.

To populate the cache and access the query results, it should first
be created using CREATE_CACHE EP.
Created cache would then be used by queries that include READ_CACHE EP with READ_MODE parameter set to QUERY_ONLY or AUTOMATIC.

Python
class name:&nbsp;CreateCache

Input: None.

Output: Status tick.

Parameters:


  CACHE_NAME (string)
    Name of the cache to be created. The cache names are global on
the server that runs it, i.e. a cache created by one user can be used
by another user who knows its name. See parameters below that allow
controlling access to the cache. If the cache already exists, a
corresponding status tick will be returned. If you intend the cache to
be 'local' to the user, you can use an expression that computes a cache
name based on the user's name.

  
  OTQ_FILE_PATH (string)
    Path to the query that will be used to populate the cache, in filename.otq::QueryName
(of course, THIS::  and ?THIS_DIR? can also be used) format. Only queries residing on
the server that runs the CREATE_CACHE/READ_CACHE event processors are
currently supported.

    See the section on query
requirements above to ensure that your query satisfies them.

  
  INHERITABILITY (Boolean)
    Indicates whether results can be obtained by combining time
intervals that were cached with intervals freshly computed to obtain
results for larger intervals. See the section
above that describes this behavior. Note that when this option is off,
you are required to specify the allowed query intervals explicitly via TIME_INTERVALS_TO_CACHE parameter.

Default: true
  TIME_GRANULARITY (integer)
    Value N for seconds/days/months granularity means that start and
end time of the query have to be on N second/day/month boundaries
relative to start of the day/month/year. This doesn't affect the
frequency of data within the cache, just the start and end dates.

    For example,the combination of parameter values TIME_GRANULARITY=15, TIME_GRANULARITY_UNITS=DAYS
indicates that the query's start/end time should be the 15th or the
30th day of any month.

    Ignored, if TIME_GRANULARITY_UNITS=NONE.

  
  OTQ_PARAMS (string)
    Otq params of the query to be cached.

  
  TIME_GRANULARITY_UNITS (enum)
    Units used in TIME_GRANULARITY
parameter. Possible values: NONE, DAYS, MONTHS,
    SECONDS.
Default: NONE

  
  TIMEZONE (string)
    Timezone of the query to be cached.
Default: Local

  
  TIME_INTERVALS_TO_CACHE
(string)
    New line separated list of query start/end times in
&lt;start_time&gt;,&lt;end_time&gt; format, where &lt;start_time&gt;
and &lt;end_time&gt; should be in YYYYMMDDhhmmss[.msec] format. If
specified only these time intervals can be cached. Ignored if INHERITABILITY=true.

  
  ALLOW_DELETE_TO_EVERYONE
(Boolean)
    When set to true everyone is allowed to delete the cache.
Default: false

  
  ALLOW_UPDATE_TO_EVERYONE
(Boolean)
    When set to true everyone is allowed to update the cache.
Default: false

  
  ALLOW_SEARCH_TO_EVERYONE
(Boolean)
    When set to true everyone is allowed to read the cached data.
Default: true

  
  CACHE_EXPIRATION_INTERVAL
(integer)
    If set to a non-zero value determines the periodicity of cache
clearing, in seconds. The cache will be cleared every X seconds,
triggering new query executions when data is requested.
Default: 0

  

Access control: Ability to run this event processor is
regulated through the access control file (see QUERY_CACHING in
REMOTE_OPERATIONS in OneTickInstallation&amp;AdministrationGuide.html).
Only roles specified in the access control file with CREATE_PERMISSION
flag set to "true"/"YES" can use this event processor. If a user has
several roles, she has access if it is allowed for at least one of her
roles.
Here is an example:

&lt;remote_operations&gt;  ...  &lt;op id="QUERY_CACHING"&gt;    &lt;allow role="developer" CREATE_PERMISSION="YES"/&gt;  &lt;/op&gt;  ...&lt;/remote_operations&gt;
Examples:

CREATE_CACHE
(Test,"query_path.otq::Query",true,0,,NONE,,)

Create cache named "Test" with otq file path "query_path.otq", with
inheritability attribute, with no time granularity constraint.

See CACHE_EXAMPLE.otq


	"""
	class Parameters:
		cache_name = "CACHE_NAME"
		otq_file_path = "OTQ_FILE_PATH"
		inheritability = "INHERITABILITY"
		time_granularity = "TIME_GRANULARITY"
		otq_params = "OTQ_PARAMS"
		time_granularity_units = "TIME_GRANULARITY_UNITS"
		timezone = "TIMEZONE"
		time_intervals_to_cache = "TIME_INTERVALS_TO_CACHE"
		allow_delete_to_everyone = "ALLOW_DELETE_TO_EVERYONE"
		allow_update_to_everyone = "ALLOW_UPDATE_TO_EVERYONE"
		allow_search_to_everyone = "ALLOW_SEARCH_TO_EVERYONE"
		cache_expiration_interval = "CACHE_EXPIRATION_INTERVAL"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["cache_name", "otq_file_path", "inheritability", "time_granularity", "otq_params", "time_granularity_units", "timezone", "time_intervals_to_cache", "allow_delete_to_everyone", "allow_update_to_everyone", "allow_search_to_everyone", "cache_expiration_interval"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["cache_name", "_default_cache_name", "otq_file_path", "_default_otq_file_path", "inheritability", "_default_inheritability", "time_granularity", "_default_time_granularity", "otq_params", "_default_otq_params", "time_granularity_units", "_default_time_granularity_units", "timezone", "_default_timezone", "time_intervals_to_cache", "_default_time_intervals_to_cache", "allow_delete_to_everyone", "_default_allow_delete_to_everyone", "allow_update_to_everyone", "_default_allow_update_to_everyone", "allow_search_to_everyone", "_default_allow_search_to_everyone", "cache_expiration_interval", "_default_cache_expiration_interval", "stack_info", "_used_strings"]

	class TimeGranularityUnits:
		DAYS = "DAYS"
		MONTHS = "MONTHS"
		NONE = "NONE"
		SECONDS = "SECONDS"

	def __init__(self, cache_name="", otq_file_path="", inheritability=True, time_granularity=0, otq_params="", time_granularity_units=TimeGranularityUnits.NONE, timezone="", time_intervals_to_cache="", allow_delete_to_everyone=False, allow_update_to_everyone=False, allow_search_to_everyone=True, cache_expiration_interval=0):
		_graph_components.EpBase.__init__(self, "CREATE_CACHE")
		self._default_cache_name = ""
		self.cache_name = cache_name
		self._default_otq_file_path = ""
		self.otq_file_path = otq_file_path
		self._default_inheritability = True
		self.inheritability = inheritability
		self._default_time_granularity = 0
		self.time_granularity = time_granularity
		self._default_otq_params = ""
		self.otq_params = otq_params
		self._default_time_granularity_units = type(self).TimeGranularityUnits.NONE
		self.time_granularity_units = time_granularity_units
		self._default_timezone = ""
		self.timezone = timezone
		self._default_time_intervals_to_cache = ""
		self.time_intervals_to_cache = time_intervals_to_cache
		self._default_allow_delete_to_everyone = False
		self.allow_delete_to_everyone = allow_delete_to_everyone
		self._default_allow_update_to_everyone = False
		self.allow_update_to_everyone = allow_update_to_everyone
		self._default_allow_search_to_everyone = True
		self.allow_search_to_everyone = allow_search_to_everyone
		self._default_cache_expiration_interval = 0
		self.cache_expiration_interval = cache_expiration_interval
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_cache_name(self, value):
		self.cache_name = value
		return self

	def set_otq_file_path(self, value):
		self.otq_file_path = value
		return self

	def set_inheritability(self, value):
		self.inheritability = value
		return self

	def set_time_granularity(self, value):
		self.time_granularity = value
		return self

	def set_otq_params(self, value):
		self.otq_params = value
		return self

	def set_time_granularity_units(self, value):
		self.time_granularity_units = value
		return self

	def set_timezone(self, value):
		self.timezone = value
		return self

	def set_time_intervals_to_cache(self, value):
		self.time_intervals_to_cache = value
		return self

	def set_allow_delete_to_everyone(self, value):
		self.allow_delete_to_everyone = value
		return self

	def set_allow_update_to_everyone(self, value):
		self.allow_update_to_everyone = value
		return self

	def set_allow_search_to_everyone(self, value):
		self.allow_search_to_everyone = value
		return self

	def set_cache_expiration_interval(self, value):
		self.cache_expiration_interval = value
		return self

	@staticmethod
	def _get_name():
		return "CREATE_CACHE"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.cache_name != "": 
			desc += "CACHE_NAME=" + py_to_str(self.cache_name) + ","
		if self.otq_file_path != "": 
			desc += "OTQ_FILE_PATH=" + py_to_str(self.otq_file_path) + ","
		if self.inheritability != True: 
			desc += "INHERITABILITY=" + py_to_str(self.inheritability) + ","
		if self.time_granularity != 0: 
			desc += "TIME_GRANULARITY=" + py_to_str(self.time_granularity) + ","
		if self.otq_params != "": 
			desc += "OTQ_PARAMS=" + py_to_str(self.otq_params) + ","
		if self.time_granularity_units != self.TimeGranularityUnits.NONE: 
			desc += "TIME_GRANULARITY_UNITS=" + py_to_str(self.time_granularity_units) + ","
		if self.timezone != "": 
			desc += "TIMEZONE=" + py_to_str(self.timezone) + ","
		if self.time_intervals_to_cache != "": 
			desc += "TIME_INTERVALS_TO_CACHE=" + py_to_str(self.time_intervals_to_cache) + ","
		if self.allow_delete_to_everyone != False: 
			desc += "ALLOW_DELETE_TO_EVERYONE=" + py_to_str(self.allow_delete_to_everyone) + ","
		if self.allow_update_to_everyone != False: 
			desc += "ALLOW_UPDATE_TO_EVERYONE=" + py_to_str(self.allow_update_to_everyone) + ","
		if self.allow_search_to_everyone != True: 
			desc += "ALLOW_SEARCH_TO_EVERYONE=" + py_to_str(self.allow_search_to_everyone) + ","
		if self.cache_expiration_interval != 0: 
			desc += "CACHE_EXPIRATION_INTERVAL=" + py_to_str(self.cache_expiration_interval) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class ReadCache(_graph_components.EpBase):
	"""
		

READ_CACHE

Type: Other

Description: Returns cached results of a query from a query result cache or
performs a new query if appropriate parameters are set. The cache
should first be created, usually via CREATE_CACHE
event processor (EP). There are alternative options for cache creation,
including a QUERY_CACHES_FILE
or by setting the parameter CREATE_CACHE_QUERY
of this event processor.


If symbols have _PARAM_START_TIME/_PARAM_END_TIME symbol parameters defined,
these are used to define the caching/querying interval. These
parameters, as well as the _CLOSURE
symbol parameter, are not used to differentiate the series in the
result. In other words, if there are identical symbols A in the
query with different values of _PARAM_START_TIME and _PARAM_END_TIME,
these will map to the same cached series and the parameter values will
be dropped in the result, as they are used to determine the
query/caching interval.

When symbol date is specified, symbol is translated and stored in
cache in its database's symbology.

The cache is stored on the machine where the READ_CACHE EP is executed.

Python
class name:&nbsp;ReadCache

Input: None.

Database name must be provided in the non-bound symbols
list of the query to which this EP belongs. Database name specified in
the tick type (e.g. DB::TRD) does not overwrite the database specified
in the non-bound symbols list.

Output: A time series of ticks, one for each security.

Parameters:


  CACHE_NAME (string)
    Name of the cache to be created. See CACHE_NAME in CREATE_CACHE.

  
  READ_MODE (enum)
    If set to CACHE_ONLY, only
cached results are returned and queries are not performed. If set to
QUERY_ONLY the query is run irrespective of whether the result is
already available in the cache. AUTOMATIC mode performs the query if
the data is not found in the cache (see the section on caching granularity for a description
of how various parts of the result are cached).
Default: AUTOMATIC

  
  UPDATE_CACHE (Boolean)
     If set to true, updates the
cached data if READ_MODE=QUERY_ONLY
or if READ_MODE=AUTOMATIC and the
result data not found in the cache. Otherwise, the cache remains
unchanged.
Default: true

  
  PER_CACHE_OTQ_PARAMS (string)
    A comma-separated list of otq parameters values of which should
override the values specified for those otq parameters as part of
OTQ_PARAMS that was specified during creation of this cache. The query
result will be cached separately for each unique combination of values
of these otq parameters 

  
  CREATE_CACHE_QUERY (string)
    If a cache with the given name is not present, the query
provided in this param will be invoked, which should contain CREATE_CACHE EP to create the
corresponding cache. 

  

Access control: Cache creator has permission to read and
update the cache, limitations for other users should be applied through
access control file (QUERY_CACHING in Remote Operations section of
OneTickInstallation&amp;AdministrationGuide.html).
Roles specified in the access control file with UPDATE_PERMISSION flag set to "true" can
read or modify a cache through this event processor.
Roles specified in the access control file with SEARCH_PERMISSION flag set to "false" do
not have read permission.

If a user has several roles, she has access if it is allowed for at
least one of her roles.

Here is an example where the role 'developer' has permission to read
from the cache but not to modify it:




&lt;remote_operations&gt;  ...  &lt;op id="QUERY_CACHING"&gt;    &lt;allow role="developer" UPDATE_PERMISSION="NO" SEARCH_PERMISSION="YES"/&gt;  &lt;/op&gt;  ...&lt;/remote_operations&gt;
Examples:

Return cached values for the input symbols from a cache named
"Test", do not perform a query if the data is missing:

READ_CACHE (Test,CACHE_ONLY)

&nbsp;

See CACHE_EXAMPLE.otq


	"""
	class Parameters:
		cache_name = "CACHE_NAME"
		per_cache_otq_params = "PER_CACHE_OTQ_PARAMS"
		read_mode = "READ_MODE"
		update_cache = "UPDATE_CACHE"
		create_cache_query = "CREATE_CACHE_QUERY"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["cache_name", "per_cache_otq_params", "read_mode", "update_cache", "create_cache_query"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["cache_name", "_default_cache_name", "per_cache_otq_params", "_default_per_cache_otq_params", "read_mode", "_default_read_mode", "update_cache", "_default_update_cache", "create_cache_query", "_default_create_cache_query", "stack_info", "_used_strings"]

	class ReadMode:
		AUTOMATIC = "AUTOMATIC"
		CACHE_ONLY = "CACHE_ONLY"
		QUERY_ONLY = "QUERY_ONLY"

	def __init__(self, cache_name="", per_cache_otq_params="", read_mode=ReadMode.AUTOMATIC, update_cache=True, create_cache_query=""):
		_graph_components.EpBase.__init__(self, "READ_CACHE")
		self._default_cache_name = ""
		self.cache_name = cache_name
		self._default_per_cache_otq_params = ""
		self.per_cache_otq_params = per_cache_otq_params
		self._default_read_mode = type(self).ReadMode.AUTOMATIC
		self.read_mode = read_mode
		self._default_update_cache = True
		self.update_cache = update_cache
		self._default_create_cache_query = ""
		self.create_cache_query = create_cache_query
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_cache_name(self, value):
		self.cache_name = value
		return self

	def set_per_cache_otq_params(self, value):
		self.per_cache_otq_params = value
		return self

	def set_read_mode(self, value):
		self.read_mode = value
		return self

	def set_update_cache(self, value):
		self.update_cache = value
		return self

	def set_create_cache_query(self, value):
		self.create_cache_query = value
		return self

	@staticmethod
	def _get_name():
		return "READ_CACHE"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.cache_name != "": 
			desc += "CACHE_NAME=" + py_to_str(self.cache_name) + ","
		if self.per_cache_otq_params != "": 
			desc += "PER_CACHE_OTQ_PARAMS=" + py_to_str(self.per_cache_otq_params) + ","
		if self.read_mode != self.ReadMode.AUTOMATIC: 
			desc += "READ_MODE=" + py_to_str(self.read_mode) + ","
		if self.update_cache != True: 
			desc += "UPDATE_CACHE=" + py_to_str(self.update_cache) + ","
		if self.create_cache_query != "": 
			desc += "CREATE_CACHE_QUERY=" + py_to_str(self.create_cache_query) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class DeleteCache(_graph_components.EpBase):
	"""
		

DELETE_CACHE

Type: Other

Description: Deletes entire or some part of query cache
created by CREATE_CACHE event processor
(EP) or configured in special file referred by QUERY_CACHES_FILE
environment variable.

Python
class name:&nbsp;DeleteCache

Input: None.

Output: Status tick.

Parameters:


  CACHE_NAME (string)
    Name of the cache to be deleted.

  
  APPLY_TO_ENTIRE_CACHE (Boolean)
    If true, deletes the cache for all symbols and time intervals.
Default: true

  
  PER_CACHE_OTQ_PARAMS (string)
    Deletes cache that have been associated with this otq parameters
during its creation. Value of this parameter should be equal to the
value of PER_CACHE_OTQ_PARAMS of READ_CACHE EP 

  

Access control: Cache creator has permission to delete cache,
limitations for other users should be applied through access control
file (QUERY_CACHING operation in REMOTE_OPERATIONS). Only roles,
specified in access control file with DELETE_PERMISSION flag set to
"true", can use this event processor. If a user has several roles, she
has access if it is allowed for at least one of her roles.

Here is an example:

&lt;allow role="developer" CREATE_PERMISSION="YES"
DELETE_PERMISSION="YES"/&gt;

Note: Database name must be provided in the non-bound symbols
list of the query to which this EP belongs. Database name specified in
tick type does not overwrite the data of non-bound symbols list.

Examples:

DELETE_CACHE (Test,true)

Delete information about all symbols and time intervals from cache
named "Test".

See CACHE_EXAMPLE.otq


	"""
	class Parameters:
		cache_name = "CACHE_NAME"
		per_cache_otq_params = "PER_CACHE_OTQ_PARAMS"
		apply_to_entire_cache = "APPLY_TO_ENTIRE_CACHE"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["cache_name", "per_cache_otq_params", "apply_to_entire_cache"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["cache_name", "_default_cache_name", "per_cache_otq_params", "_default_per_cache_otq_params", "apply_to_entire_cache", "_default_apply_to_entire_cache", "stack_info", "_used_strings"]

	def __init__(self, cache_name="", per_cache_otq_params="", apply_to_entire_cache=True):
		_graph_components.EpBase.__init__(self, "DELETE_CACHE")
		self._default_cache_name = ""
		self.cache_name = cache_name
		self._default_per_cache_otq_params = ""
		self.per_cache_otq_params = per_cache_otq_params
		self._default_apply_to_entire_cache = True
		self.apply_to_entire_cache = apply_to_entire_cache
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_cache_name(self, value):
		self.cache_name = value
		return self

	def set_per_cache_otq_params(self, value):
		self.per_cache_otq_params = value
		return self

	def set_apply_to_entire_cache(self, value):
		self.apply_to_entire_cache = value
		return self

	@staticmethod
	def _get_name():
		return "DELETE_CACHE"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.cache_name != "": 
			desc += "CACHE_NAME=" + py_to_str(self.cache_name) + ","
		if self.per_cache_otq_params != "": 
			desc += "PER_CACHE_OTQ_PARAMS=" + py_to_str(self.per_cache_otq_params) + ","
		if self.apply_to_entire_cache != True: 
			desc += "APPLY_TO_ENTIRE_CACHE=" + py_to_str(self.apply_to_entire_cache) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class ModifyCacheConfig(_graph_components.EpBase):
	"""
		

MODIFY_CACHE_CONFIG

Type: Other

Description: Modifies configuration of a cache object created
using CREATE_CACHE, but does not populate it. Note that the contents of
the cache object for which configuration modification was applied are
not modified. 
If a cache object with specified name is not found, an exception is
thrown.&nbsp;

Configuration parameters supported by this EP are OTQ_FILE_PATH, INHERITABILITY,
TIME_GRANULARITY, OTQ_PARAMS, TIME_GRANULARITY_UNITS,
TIMEZONE, TIME_INTERVALS_TO_CACHE,
ALLOW_DELETE_TO_EVERYONE, ALLOW_SEARCH_TO_EVERYONE, ALLOW_UPDATE_TO_EVERYONE. You can find
more information on these configuration parameters in CREATE_CACHE. 
Python
class name:&nbsp;ModifyCacheConfig

Input: None.

Output: A single tick with a string field named STATUS, the
value of which should be&nbsp;SUCCESS.

Parameters:


  CACHE_NAME (string)
     The name of the cache whose configuration should be changed.

  
  CONFIG_PARAMETER_NAME (string)
     The name of the configuration parameter to be changed.

  
  CONFIG_PARAMETER_VALUE
(string)
     New value of configuration parameter.

  




	"""
	class Parameters:
		cache_name = "CACHE_NAME"
		config_parameter_name = "CONFIG_PARAMETER_NAME"
		config_parameter_value = "CONFIG_PARAMETER_VALUE"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["cache_name", "config_parameter_name", "config_parameter_value"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["cache_name", "_default_cache_name", "config_parameter_name", "_default_config_parameter_name", "config_parameter_value", "_default_config_parameter_value", "stack_info", "_used_strings"]

	def __init__(self, cache_name="", config_parameter_name="", config_parameter_value=""):
		_graph_components.EpBase.__init__(self, "MODIFY_CACHE_CONFIG")
		self._default_cache_name = ""
		self.cache_name = cache_name
		self._default_config_parameter_name = ""
		self.config_parameter_name = config_parameter_name
		self._default_config_parameter_value = ""
		self.config_parameter_value = config_parameter_value
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_cache_name(self, value):
		self.cache_name = value
		return self

	def set_config_parameter_name(self, value):
		self.config_parameter_name = value
		return self

	def set_config_parameter_value(self, value):
		self.config_parameter_value = value
		return self

	@staticmethod
	def _get_name():
		return "MODIFY_CACHE_CONFIG"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.cache_name != "": 
			desc += "CACHE_NAME=" + py_to_str(self.cache_name) + ","
		if self.config_parameter_name != "": 
			desc += "CONFIG_PARAMETER_NAME=" + py_to_str(self.config_parameter_name) + ","
		if self.config_parameter_value != "": 
			desc += "CONFIG_PARAMETER_VALUE=" + py_to_str(self.config_parameter_value) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class LicenseExportingCoresInfo(_graph_components.EpBase):
	"""
		

LICENSE/EXPORTING_CORES_INFO

Type: Other

Description: Shows information about exporting cores from
license server. This EP is currenbtly not supported (an exception is
thrown) by the license servers used in the Kubernetes setup (e.g.
short-term license grant setup)

This EP must be run REMOTE-ly on a license server.

Python
class name:&nbsp;LicenseExportingCoresInfo

Input: None.

Output: A tick with EXPORTED_TS_LICENSE_MAX_CORES and
EXPORTED_TS_LICENSE_CORES_IN_USE fields.&nbsp;


	"""
	class Parameters:
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = []
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["stack_info", "_used_strings"]

	def __init__(self):
		_graph_components.EpBase.__init__(self, "LICENSE/EXPORTING_CORES_INFO")
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	@staticmethod
	def _get_name():
		return "LICENSE/EXPORTING_CORES_INFO"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class ShowEpList(_graph_components.EpBase):
	"""
		

SHOW_EP_LIST

Type: Other

Description: Shows available event processors from all tick
servers accessible&nbsp;from
the process (it could be a client process, or could be a tick server,
depending on where a queried symbol resides) where the query containing
this event processor is executed.

Python
class name:&nbsp;ShowEpList

Input: None

Output: One tick for each parameter of every event processor.

Note: To define the properties of a UDEP (for example;
category, parameter dependencies, and so on), specify the
UDEP_GUI_DESCRIPTOR_FILES variable in the config file.

Parameters:


  LEVEL_OF_DETAIL string
    Specifies the level of detail to show. Supported values are PER_EP_PARAMETER and PER_EP.
Default value is PER_EP_PARAMETER.

  
  SHOW_DOCUMENTATION boolean
    Must be set to false if LEVEL_OF_DETAIL is PER_EP_PARAMETER.
When set to true, field DESCRIPTION will be added, with contents of the
document for this EP saved in it.
Default value is false.

  
  
    For PER_EP_PARAMETER, if an
event processor does not have parameters, only ticks are propagated
with HAS_PARAMETERS=false. In this
case, numeric fields describing parameters take the value 0 (except for PARAMETER_PROPERTY_FLAGS
that takes -1) and all string fields
take empty string. Each propagated tick contains the following fields:

    TIMESTAMP, EVENT_PROCESSOR_NAME, EP_CATEGORY,
EP_IS_SOURCE, HAS_PARAMETERS, PARAMETER_NAME, PARAMETER_DEFAULT_VALUE,
PARAMETER_DESCRIPTION, PARAMETER_PROPERTY_FLAGS, PARAMETER_CHOICES,
IS_ADVANCED, DEPENDS_EXPR, VALIDATOR_EXPR, TYPE_EXPR

    For PER_EP, a tick per event
processor is propagated. Each propagated tick contains the fields
below, with field DESCRIPTION added to the tick if SHOW_DOCUMENTATION
parameter is set to true

    TIMESTAMP, EVENT_PROCESSOR_NAME, CATEGORY,
IS_SOURCE

  

Example:

See the SHOW_EP_LIST example in OTHER_EXAMPLES.otq.


	"""
	class Parameters:
		level_of_detail = "LEVEL_OF_DETAIL"
		show_documentation = "SHOW_DOCUMENTATION"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["level_of_detail", "show_documentation"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["level_of_detail", "_default_level_of_detail", "show_documentation", "_default_show_documentation", "stack_info", "_used_strings"]

	class LevelOfDetail:
		PER_EP = "PER_EP"
		PER_EP_PARAMETER = "PER_EP_PARAMETER"

	def __init__(self, level_of_detail=LevelOfDetail.PER_EP_PARAMETER, show_documentation=False):
		_graph_components.EpBase.__init__(self, "SHOW_EP_LIST")
		self._default_level_of_detail = type(self).LevelOfDetail.PER_EP_PARAMETER
		self.level_of_detail = level_of_detail
		self._default_show_documentation = False
		self.show_documentation = show_documentation
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_level_of_detail(self, value):
		self.level_of_detail = value
		return self

	def set_show_documentation(self, value):
		self.show_documentation = value
		return self

	@staticmethod
	def _get_name():
		return "SHOW_EP_LIST"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.level_of_detail != self.LevelOfDetail.PER_EP_PARAMETER: 
			desc += "LEVEL_OF_DETAIL=" + py_to_str(self.level_of_detail) + ","
		if self.show_documentation != False: 
			desc += "SHOW_DOCUMENTATION=" + py_to_str(self.show_documentation) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class ShowEpInfo(_graph_components.EpBase):
	"""
		

SHOW_EP_INFO

Type: Other

Description: Shows description for requested event processor
searching all accessible tick servers.

Python
class name:&nbsp;ShowEpInfo

Input: None

Output: A tick describing event processor.

Note: To define the properties of a UDEP (for example;
category, parameter dependencies, and so on), specify the
UDEP_GUI_DESCRIPTOR_FILES variable in the config file.

Parameters:


  EP_NAME (string)
    Event processor name to retrieve description for.

  
  PREFER_LOCAL_INFO (boolean)
    If set to true and local
description is present, no attempt is performed to retrieve description
from accessible tick servers.

Default value is false.


	"""
	class Parameters:
		ep_name = "EP_NAME"
		prefer_local_info = "PREFER_LOCAL_INFO"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["ep_name", "prefer_local_info"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["ep_name", "_default_ep_name", "prefer_local_info", "_default_prefer_local_info", "stack_info", "_used_strings"]

	def __init__(self, ep_name="", prefer_local_info=False):
		_graph_components.EpBase.__init__(self, "SHOW_EP_INFO")
		self._default_ep_name = ""
		self.ep_name = ep_name
		self._default_prefer_local_info = False
		self.prefer_local_info = prefer_local_info
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_ep_name(self, value):
		self.ep_name = value
		return self

	def set_prefer_local_info(self, value):
		self.prefer_local_info = value
		return self

	@staticmethod
	def _get_name():
		return "SHOW_EP_INFO"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.ep_name != "": 
			desc += "EP_NAME=" + py_to_str(self.ep_name) + ","
		if self.prefer_local_info != False: 
			desc += "PREFER_LOCAL_INFO=" + py_to_str(self.prefer_local_info) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class ShowFunctionList(_graph_components.EpBase):
	"""
		

SHOW_FUNCTION_LIST

Type: Other

Description: Returns the list of functions and their
descriptions.

Python
class name:&nbsp;ShowFunctionList

Input: None

Output: A single tick for every parameter of each event
processor.

The PARAMETER_TYPE field takes
its values from the omd::DataType::data_type_t
enumeration.

If a function does not have parameters, only one tick is propagated
with HAS_PARAMETERS=false. In this
case, numeric fields that are describing parameters, take the value 0 (except for PARAMETER_TYPE,
which takes -1) and string fields
take empty string.

Parameters: None

See the SHOW_FUNCTION_LIST
example in OTHER_EXAMPLES.otq.


	"""
	class Parameters:
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = []
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["stack_info", "_used_strings"]

	def __init__(self):
		_graph_components.EpBase.__init__(self, "SHOW_FUNCTION_LIST")
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	@staticmethod
	def _get_name():
		return "SHOW_FUNCTION_LIST"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class ShowDbList(_graph_components.EpBase):
	"""
		

SHOW_DB_LIST

Type: Other

Description: Returns a list of all databases visible from the
process (it could be a client process, or could be a tick server,
depending on where a queried symbol resides) where
the query containing this event processor is executed. Those
databases are
specified in the locator file explicitly or are auto-discovered, by the
above-mentioned process.

Python
class name:&nbsp;ShowDbList

Input: None

Output: One tick for each database.

Parameters:


  CEP_ONLY (true,false)
    If set to true, only CEP databases will be returned.
Default: false.

  
  SHOW_DESCRIPTION (true,false)
    If set to true the output will also include the database's
description, in the field DESCRIPTION
Default: false.

  

Examples:

SHOW_DB_LIST(CEP_ONLY=false)
will return all available databases.

See the SHOW_DB_LIST example in OTHER_EXAMPLES.otq.


	"""
	class Parameters:
		cep_only = "CEP_ONLY"
		show_description = "SHOW_DESCRIPTION"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["cep_only", "show_description"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["cep_only", "_default_cep_only", "show_description", "_default_show_description", "stack_info", "_used_strings"]

	def __init__(self, cep_only=False, show_description=False):
		_graph_components.EpBase.__init__(self, "SHOW_DB_LIST")
		self._default_cep_only = False
		self.cep_only = cep_only
		self._default_show_description = False
		self.show_description = show_description
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_cep_only(self, value):
		self.cep_only = value
		return self

	def set_show_description(self, value):
		self.show_description = value
		return self

	@staticmethod
	def _get_name():
		return "SHOW_DB_LIST"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.cep_only != False: 
			desc += "CEP_ONLY=" + py_to_str(self.cep_only) + ","
		if self.show_description != False: 
			desc += "SHOW_DESCRIPTION=" + py_to_str(self.show_description) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class ShowSymbologyList(_graph_components.EpBase):
	"""
		

SHOW_SYMBOLOGY_LIST

Type: Other

Description: This EP lists local symbologies and symbologies
from directly or indirectly attached tick servers.

It must be the first (source) EP on the graph.

Python
class name:
ShowSymbologyList

Input: None.

Output: List of symbologies.

Parameters:

Examples:

See the SHOW_SYMBOLOGY_LIST in OTHER_EXAMPLES.otq.


	"""
	class Parameters:
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = []
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["stack_info", "_used_strings"]

	def __init__(self):
		_graph_components.EpBase.__init__(self, "SHOW_SYMBOLOGY_LIST")
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	@staticmethod
	def _get_name():
		return "SHOW_SYMBOLOGY_LIST"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class DbShowRealtimeStats(_graph_components.EpBase):
	"""
		

DB/SHOW_REALTIME_STATS

Type: Other

Description: Shows realtime database statistics. The database
name will be extracted from the bound security.

Python
class name:&nbsp;DbShowRealtimeStats

Input: None

Output: A single tick

Parameters: None


	"""
	class Parameters:
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = []
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["stack_info", "_used_strings"]

	def __init__(self):
		_graph_components.EpBase.__init__(self, "DB/SHOW_REALTIME_STATS")
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	@staticmethod
	def _get_name():
		return "DB/SHOW_REALTIME_STATS"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class ReloadConfig(_graph_components.EpBase):
	"""
		

RELOAD_CONFIG

Type: Other

Description: Forces the server to reload its configuration
specified by the CONFIG_TYPE
parameter.
If access control and remote monitoring are enabled on the server, then
the CONFIG_CHANGE permission should
be enabled for the user.

Previously, this EP was called REREAD_CONFIG. Queries that use the old
name will still function correctly.

Python
class name:&nbsp;ReloadConfig

Input: None

Output: A single tick with RESULT field.
If succeeded, a single tick with a field RESULT=0
is propagated, otherwise an exception is thrown.

Parameters:


  CONFIG_TYPE (LOCATOR, MAIN_CONFIG,
ACCESS_LIST)
    Specifies the configuration to reload.
Default value is LOCATOR.

  

Example:

RELOAD_CONFIG(CONFIG_TYPE=ACCESS_LIST)
will force the server to reload its access control file.

See the RELOAD_CONFIG example in OTHER_EXAMPLES.otq.


	"""
	class Parameters:
		config_type = "CONFIG_TYPE"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["config_type"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["config_type", "_default_config_type", "stack_info", "_used_strings"]

	class ConfigType:
		ACCESS_LIST = "ACCESS_LIST"
		LOCATOR = "LOCATOR"
		MAIN_CONFIG = "MAIN_CONFIG"

	def __init__(self, config_type=ConfigType.LOCATOR):
		_graph_components.EpBase.__init__(self, "RELOAD_CONFIG")
		self._default_config_type = type(self).ConfigType.LOCATOR
		self.config_type = config_type
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_config_type(self, value):
		self.config_type = value
		return self

	@staticmethod
	def _get_name():
		return "RELOAD_CONFIG"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.config_type != self.ConfigType.LOCATOR: 
			desc += "CONFIG_TYPE=" + py_to_str(self.config_type) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class ShowActiveQueryList(_graph_components.EpBase):
	"""
		

SHOW_ACTIVE_QUERY_LIST

Type: Other

Description: Shows the list of active queries along with
their properties on the server. If access control is enabled on the
server, then corresponding permissions will be required.
This event processor takes start time for the queries as a parameter.
Only queries that have been started after that time will be displayed.

Python
class name:&nbsp;ShowActiveQueryList

Input: None

Output: One tick for each query.

Parameters: None

Examples:

SHOW_ACTIVE_QUERY_LIST(START_DATE="04/29/2014",START_TIME="00:00:00",TIMEZONE=GMT)
This will show all queries that have started after specified time
and are currently active.

See the SHOW_ACTIVE_QUERY_LIST
example in OTHER_EXAMPLES.otq.


	"""
	class Parameters:
		start_date = "START_DATE"
		start_time = "START_TIME"
		timezone = "TIMEZONE"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["start_date", "start_time", "timezone"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["start_date", "_default_start_date", "start_time", "_default_start_time", "timezone", "_default_timezone", "stack_info", "_used_strings"]

	def __init__(self, start_date="", start_time="", timezone="GMT"):
		_graph_components.EpBase.__init__(self, "SHOW_ACTIVE_QUERY_LIST")
		self._default_start_date = ""
		self.start_date = start_date
		self._default_start_time = ""
		self.start_time = start_time
		self._default_timezone = "GMT"
		self.timezone = timezone
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_start_date(self, value):
		self.start_date = value
		return self

	def set_start_time(self, value):
		self.start_time = value
		return self

	def set_timezone(self, value):
		self.timezone = value
		return self

	@staticmethod
	def _get_name():
		return "SHOW_ACTIVE_QUERY_LIST"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.start_date != "": 
			desc += "START_DATE=" + py_to_str(self.start_date) + ","
		if self.start_time != "": 
			desc += "START_TIME=" + py_to_str(self.start_time) + ","
		if self.timezone != "GMT": 
			desc += "TIMEZONE=" + py_to_str(self.timezone) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class PerTickScript(_graph_components.EpBase):
	"""
		

PER_TICK_SCRIPT

Table of contents

EP
description and params

Declaring
and accessing variables

Script Language Features

Control flow

If else
statement

While statement

Switch statement

For statement

Comments

_ONCE attribute

Defining
functions

Importing
other scripts

Primitive
types

Non-primitive
types

Special
Local Variables

Tick Processing

Tick generation and filtering

Return statement

Special functions

Error handling

Access Control

Temporary
Directory

Config Variables

Examples

EP description and params
Type: Transformer

Description: The PER_TICK_SCRIPT event processor allows you to transform input time series by adding new fields, modifying existing field values, filtering ticks, or generating multiple output ticks. It provides a C-like scripting language with support for control flow, functions, and complex data manipulation. The script is
compiled and cached for performance (see the description of the PERF.PER_TICK_SCRIPT.CODE_CACHE_MAX_ENTRIES
configuration variable). The PER_TICK_SCRIPT EP is currently supported on Windows and Linux, it also has limited support for Mac, with only "gcc" compiler supported (see the description of the PER_TICK_SCRIPT.DEFAULT_COMPILER and PER_TICK_SCRIPT.TMP_SCRIPT_COMPILATION_DIR configuration variables).

Python
class name:&nbsp;PerTickScript

Input: A time series of ticks.

Output: A modified time series of ticks.

Parameters:


  SCRIPT (string)
    A mandatory parameter containing script code. The script has the
syntax described in the sections below.

  

Declaring and accessing variables
TYPE NAME = EXPRESSION;

This adds a new field to the tick. All types listed in Field Type
Declarations are supported. It is also possible to declare "script
local" variables by adding the LOCAL::
prefix to the variable name. This will not add fields to the tick
descriptor and the variables will be visible only from the script. For
local variables supported types are: long,
ulong, double,
decimal, string,
msectime and nsectime. Note that local string variables
can store values of arbitrary size, so unlike tick fields, size can not
be specified (when size is set for local string variables
PER_TICK_SCRIPT logs WARNING message, which can be disabled using COMPATIBILITY.PER_TICK_SCRIPT.ALLOW_LEN_FOR_LOCAL_STR
main config variable).

All variables must be declared before accessing them, and variable
declarations must be at the beginning of the script. The static storage type is also supported for
local variables (see Example 5), which are
initialized once, unlike non-static local variables that are
initialized on every input tick event.

The following syntax allows you to modify the value of a variable:

VARIABLE_NAME = EXPRESSION;

Where VARIABLE_NAME can be a tick
field name, a local variable name (LOCAL::A),
or a state variable name (STATE::A).
The EXPRESSION is a Logical Expression. Instead of the "=" operator, "+=",
"-=", "*=",
and "/=" operators can be also used.

Names used in PER_TICK_SCRIPT must comply with C++
variable naming conventions.
Note that, some valid OneTick names (for example, field, TICK_SET, and
TICK_LIST names), including those that start with a number, are not
valid C++ variable names.

Pseudo-fields such as TIMESTAMP, _START_TIME, and _END_TIME also can be used in the
expression (described in detail in the Pseudo-fields
document).

Both positive and negative history indexes are supported (see Example 11).

Filter EPs (like TIME_FILTER) also
can be used in expressions. Note that all parameters of Filter EPs should be specified as strings by
being enclosed in quotes, like TIME_FILTER("START_TIME=093000000","END_TIME=153500000");

Please note that unlike EPs that use logical expressions (e.g. ADD_FIELDS, UPDATE_FIELDS
... ), PER_TICK_SCRIPT applies integer arithmetic to division of
integers, whereas logical expression treat the division as division of
double numbers.


Script Language Features
Branching and loops
The following constructions can be used correspondingly for
branching operations and loops:

If else statement
if (CONDITIONAL_EXPRESSION) {        ...} else if (CONDITIONAL_EXPRESSION) {        ...} else {        ...}
While statement
while (CONDITIONAL_EXPRESSION) {        ...}
where CONDITIONAL_EXPRESSIONs are
expressions of Boolean type (expressions of integer type are implicitly
converted to bools) that can be composed of various expressions using
the AND (&amp;&amp;) or the OR(||) logical operations. While equality can
be checked with both "==" or "=" operators, it is recommended to use "==". The "LIKE"
and "ILIKE" operators also can be
used in expressions.

Switch statement
switch (EXPRESSION) {    case const-expression:    {        statement(s);        break; /* optional  */    }    case const-expression:    {        statement(s);        break; /* optional  */    }    ...    default: /* optional */    {        statement(s);    }}
Where EXPRESSION should be of
integral type.

For statement
The following two types of for-loops are also supported:

for (INITIALIZATION; CONDITIONAL_EXPRESSION; INCREASE) {        ...}



for (TICK_LIST_TICK LOCAL::T : STATE::TICK_LIST_STATE_VAR) {        ...}
INITIALIZATION and INCREASE are comma-separated lists of
assignment statements.

The second construction allows iterating over ticks from tick list or tick set state variable. TICK_LIST_TICK and TICK_SET_TICKs are special types of
user-defined objects representing a tick correspondingly from a tick
list or a tick set. TICK_LIST_TICK
has member functions (GET_LONG_VALUE,
SET_LONG_VALUE, GET_STRING_VALUE, SET_STRING_VALUE, GET_DOUBLE_VALUE, SET_DOUBLE_VALUE, GET_DECIMAL_VALUE, SET_DECIMAL_VALUE, GET_DATETIME_VALUE and SET_DATETIME_VALUE see Examples
3 and 4,
below) to access tick fields. It is also possible to copy tick field
values from iterated ticks to the current output tick, e.g.
LOCAL::OUTPUT_TICK, (for the fields that are
present in both ticks) using COPY_TICK function (see Example 7). The same methods are also present for
TICK_SET_TICK ( note that key fields
of the tick set are read-only). To access original tick field values,
the special variable LOCAL::INPUT_TICK
can be used, which (like TICK_LIST_TICK)
supports theGET_LONG_VALUE, GET_STRING_VALUE, GET_DOUBLE_VALUE and GET_DATETIME_VALUE member functions. All
these objects also have GET_TIMESTAMP
method to get tick timestamp.

continue and break statements can also be used in the
loop bodies.

Comments
C-style comments are allowed in the script, like: /* comment in PER_TICK_SCRIPT */. Note
that if comment contains an otq parameter, then rules for parameter
substitution applies to the contents of the comments as well.


_ONCE attribute
You can use the _ONCE pseudo
attribute before a statement or a code block (within curly brackets) to
make it run only once (the first time control reaches to the
statement). See Example 10.

Defining functions
It is allowed to define functions in the script and call them from
other functions (see Example 6).

The syntax for defining a function is as follows:

return_type function_name(arg1_type arg1, arg2_type arg2, .... , argN_type argN) {...}
The following restrictions apply to scripts that have functions:


  Scripts containing functions should also contain an entry point
function main, that returns long
and takes no arguments. The order of function definitions is not
important. The value returned by main indicates whether to propagate or
filter out the tick.
  Tick fields may be declared (that is, new fields can be added)
only in the main
function (static/non-static local variable declarations are allowed in
all functions and all functions can access/modify tick fields).

Importing other scripts
The script can import other per tick
script queries and call functions from there.

You can use the following syntax to import other script queries (see
the examples in PER_TICK_SCRIPT_EXAMPLES.otq):

import "otq_file_name::query_name" [as sript_alias];
Where script alias is optional and if specified, functions from the
imported query are referenced via script_alias, otherwise via query
name (see Example 8).

Note that, only queries with per tick
script query type can be imported.

Primitive types
The following primitive types are supported for both local variables
and tick fields:

long, ulong,
double, string,
msectime and nsectime

For tick fields, besides the ones mentioned above, all the types
that are listed in Supported field
types are also acceptable.

For integral types and floating point numbers +, -, *, /
operations are supported. Currently, no modulus
operator is supported, but there is MOD
built-in function for computing remainder.

+ is allowed between strings for
concatenation.

Between datetime (msectimetime and nsectimetime) types, only the - operator is allowed. It is also possible
to add an integral value to datetime
or subtract an integer from it. Any operation between floating point
and datetime types are not allowed.

Non-primitive types
The script can define non-primitive static local variables (see Example 5b), non-static objects are not currently
supported. All User-defined
and Built-in types
are supported. Currently, the available built-in types are:


  TICK_SET,TICK_SET_UNORDERED
/ TICK_SET_TICK:
Types for tick sets and corresponding ticks. The latter can be used to
access and modify fields of a tick from TICK_SET
or TICK_SET_UNORDERED
  TICK_LIST / TICK_LIST_TICK:
Types for tick list container and corresponding ticks
  TICK_DEQUE / TICK_DEQUE_TICK:
Types for tick deque container and corresponding ticks
  DYNAMIC_TICK:
Represents a tick with dynamic schema

For more info on non-primitive types and exposed functions see User-defined types.

Special Local Variables
The following special local variables are supported in the script
(they are used without being defined):


  LOCAL::INPUT_TICK: Can be used to access input tick
fields.
Values from input tick are read-only. This variable has the following
member functions: GET_LONG_VALUE, GET_STRING_VALUE, GET_DOUBLE_VALUE, GET_DECIMAL_VALUE and GET_DATETIME_VALUE.
  LOCAL::OUTPUT_TICK: Can be used to access and modify
output tick fields. It has the following functions: GET_LONG_VALUE, SET_LONG_VALUE,
    GET_STRING_VALUE, SET_STRING_VALUE, GET_DOUBLE_VALUE, SET_DOUBLE_VALUE, GET_DECIMAL_VALUE, SET_DECIMAL_VALUE, GET_DATETIME_VALUE and SET_DATETIME_VALUE.
  LOCAL::INPUT_TICK_DESCRIPTOR_FIELDS: Can be used to
iterate over all fields of an input tick. Tick fields are represented
via TICK_DESCRIPTOR_FIELD type, it has GET_FIELD_NAME (returning string that
contains field name, see Example 9), GET_TYPE and GET_SIZE
functions.

Tick Processing
Tick generation and filtering
The script can filter or generate ticks. It may return a Boolean
value indicating if the tick needs to be propagated (by default, all
ticks are propagated), see Example 2. ThePROPAGATE_TICK()
function can be used to generate ticks (that is, propagate the current
tick). The script can also filter specific fields from the output tick,
using the FILTER_FIELDS special
function or from the input tick using the FILTER_INPUT_FIELDS function. If present, these functions should be at the beginning of the
script/main-function (the first statement). Function parameters are as
follows:

FILTER_FIELDS(DROP_FIELDS_FLAG,"FIELD_1","FIELD_2",...)

Where the first argument DROP_FIELDS_FLAG
should have an integer value of 0 or
1, where 0
means that the specified fields should be propagated and 1
means that these fields should be dropped. Note that fields that are
declared in the script can not be used in the arguments of FILTER_FIELDS.

Return statement
All script-defined functions
except main must return value. The
type of return value expression must be the same as functions return
type. The value returned from the main
function or from a plain script (that is, with no functions) have
special meaning and indicate if the current ticks need to be propagated
(the values 0 / false mean that the tick needs to be
skipped and for 1 / true the tick is propagated forward).

Special functions
Beside built-in functions (which can be
called from different EPs) there are some functions that are
PER_TICK_SCRIPT specific. Functions in the following table can only be
called from the PER_TICK_SCRIPT EP.


  
    
      PER_TICK_SCRIPT
special functions
    
    
      COPY_TICK(tick
object)
      
      For the fields from the given tick object that are also
present in the current tick, copies the values into the current tick.
    
    
      FILTER_FIELDS(int
DROP_FIELDS_FLAG,string FIELD1,string FIELD2,...)FILTER_INPUT_FIELDS(int
DROP_FIELDS_FLAG,string FIELD1,string FIELD2,...)
      
      These functions can be used to filter fields from the output and input
ticks correspondingly. DROP_FIELDS_FLAG should have an integer value of 0 or 1. If
DROP_FIELDS_FLAG is 0, the specified fields are propagated and 1 means
that these fields are dropped.
    
    
      PROPAGATE_TICK()
      
      Propagates current tick to destination EPs. By default, after
script execution, tick is propagated to the next EPs (if script returns
false, the current tick is skipped and is not propagated further). If
tick needs to be propagated before script exits, then PROPAGATE_TICK
function can be called to propagate tick with current field values.
    
    
      THROW_EXCEPTION(string
message)
      
      Throws exception with the given message.
    
  

Error handling
The script can throw an exception by calling the THROW_EXCEPTION(message) function.
This function takes one argument (a string) that defines the error
message to be thrown.

Access Control
By default, all users are allowed to use this event processor. If
PER_TICK_SCRIPT is present in the access control event processor list,
only the specified roles can use this event processor.

Temporary directory
On Linux, PER_TICK_SCRIPT EP needs a temporary directory with write
and execute permissions (default is /tmp
directory). You can specify a custom temporary directory via the
TCC_TMP_PATH environment variable.

Config Variables
Here is the list of config variables that are related to
PER_TICK_SCRIPT EP (see config_variables.html).


  PER_TICK_SCRIPT.DEFAULT_COMPILER
- Specifies the compiler that is used to compile internally generated C
code. Supported compilers are "clang" and "tcc" (additionally, it is
possible to specify "gcc" and "vs" compilers, if they are present in
the PATH env var). Default compiler is tcc (see config_variables.html).
  PER_TICK_SCRIPT.TMP_SCRIPT_COMPILATION_DIR
- Specifies directory to store temporary files when compiling per tick
script code, not needed for default compiler (i.e. "tcc"). 
  PERF.PER_TICK_SCRIPT.CODE_CACHE_MAX_ENTRIES
- Maximum number of entries for PER_TICK_SCRIPT's global code cache.

Examples

  The following script adds a new field with the name
"EVEN_SUM_1_TO_10" and accumulates
the values in it using a for-loop counter:
    long LOCAL::X = 0;long EVEN_SUM_1_TO_10 = 0; /* Add a new field */for ( LOCAL::X=0; LOCAL::X &lt;= 10; LOCAL::X+=2){    /*Accumulate values*/    EVEN_SUM_1_TO_10 += LOCAL::X;}
  
  Tick insertion/filtering.
    
      The following one-line script skips ticks which size is not
100:
        /*Return true for ticks, which SIZE is 100*/return SIZE==100;
      
      The following one-line script filters out all input ticks:
        /* Drop all ticks */return false;
      
      Propagate only ticks with size 100:
        long main(){    if (SIZE == 100)    {        PROPAGATE_TICK();    }    return false;}
      
      Modify a field and propagate tick (totally 3 ticks will be
produced per input tick).
        SIZE = 1000;PROPAGATE_TICK();SIZE = 2000;PROPAGATE_TICK();/*Restore original values from input tick*/COPY_TICK(LOCAL::INPUT_TICK);/* Optional, has the same effect as no return statement at all*/return true;
      
    
  
  The following script adds a new "TOTAL_VOLUME" field and stores the sum of
the volumes for all the ticks from the TATE::list
    tick list in it. It is
assumed that the STATE::list state
variable is declared by the DECLARE_STATE_VARIABLES
EP and is visible to the PER_TICK_SCRIP EP:
    double TOTAL_VOLUME = 0.0;for (TICK_LIST_TICK LOCAL::t : STATE::list) {    /* add current VOLUME */    TOTAL_VOLUME += LOCAL::t.GET_LONG_VALUE("SIZE") * LOCAL::t.GET_DOUBLE_VALUE("PRICE");}
  
  The following script increases the price and size
field values twice for the ticks found in the STATE::list tick list:
    long LOCAL::CURRENT_SIZE = 0;double LOCAL::CURRENT_PRICE = 0.0;for (TICK_LIST_TICK LOCAL::t : STATE::list) {    LOCAL::CURRENT_SIZE = LOCAL::t.GET_LONG_VALUE("SIZE");    LOCAL::CURRENT_PRICE = LOCAL::t.GET_DOUBLE_VALUE("PRICE");    LOCAL::CURRENT_SIZE *= 2;    LOCAL::CURRENT_PRICE *= 2;    LOCAL::t.SET_LONG_VALUE("SIZE", LOCAL::CURRENT_SIZE);    LOCAL::t.SET_DOUBLE_VALUE("PRICE", LOCAL::CURRENT_PRICE);}
  
  Using static local variables:
    
      Primitive types
        static double LOCAL::VOLUME = 0.0;double CURRENT_VOL = 0.0;LOCAL::VOLUME += PRICE * SIZE;CURRENT_VOL = LOCAL::VOLUME;
      
      Local variable with non primitive type
        static TICK_LIST LOCAL::list;static TICK_SET LOCAL::S("LATEST_TICK", "SIZE,PRICE");static TICK_SET_UNORDERED LOCAL::SU("LATEST_TICK", 100, "SIZE");long LIST_SIZE = LOCAL::list.GET_SIZE();/* Add input tick to the list */LOCAL::list.PUSH_BACK(LOCAL::INPUT_TICK);/* Insert tick into the set */LOCAL::S.INSERT(LOCAL::OUTPUT_TICK);
      
    
  
  Defining a function in a script:
    string concat_strings(string s1, string s2){    return s1 + s2;}long main(){    string S = concat_strings("ABC","CBA");}
  
  Using the COPY_TICK function:
    for (TICK_LIST_TICK LOCAL::t1 : STATE::tl) {        COPY_TICK(LOCAL::t1);    PROPAGATE_TICK();}
  
  Importing script queries:
    
      
        /* Without alias use */import "helper_queries.otq::imported_functions";long main(){    long L = imported_functions.add_longs(1, 123);}
      
      
        /* With alias use */import "THIS::imported_functions" as f;long main(){    long L = f.add_longs(1, 123);}
      
    
  
  Using the LOCAL::INPUT_TICK_DESCRIPTOR_FIELDS
special local variable:
    string LOCAL::field_name = "";long LOCAL::field_value = 0;for (TICK_DESCRIPTOR_FIELD LOCAL::f : LOCAL::INPUT_TICK_DESCRIPTOR_FIELDS) {    LOCAL::field_name = LOCAL::f.GET_FIELD_NAME();    if (REGEX_MATCH(LOCAL::field_name,".*SIZE.*") != 0.0) {        LOCAL::field_value = LOCAL::OUTPUT_TICK.GET_LONG_VALUE(LOCAL::field_name);        LOCAL::OUTPUT_TICK.SET_LONG_VALUE(LOCAL::field_name, LOCAL::field_value + 1);       }}
  
  Adding fields to DYNAMIC_TICK
using _ONCE pseudo-attribute:
    long main (){	static DYNAMIC_TICK LOCAL::t;	_ONCE	{			LOCAL::t.ADD_FIELD("EXCH","string[4]","XY");			LOCAL::t.ADD_FIELD("SIZE","long",100);			LOCAL::t.ADD_FIELD("PRICE","double",1.123);	}	LOCAL::t.SET_LONG_VALUE("SIZE", LOCAL::t.GET_LONG_VALUE("SIZE") + 1);	STATE::list.PUSH_BACK(LOCAL::t);}
  
   Access fields of previous and succeeding ticks:
    long main (){	long PREV_SIZE = SIZE[-1];	long NEXT_SIZE = SIZE[1];}
  

See the PER_TICK_SCRIPT examples
in PER_TICK_SCRIPT_EXAMPLES.otq.


	"""
	class Parameters:
		script = "SCRIPT"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["script"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["script", "_default_script", "stack_info", "_used_strings"]

	def __init__(self, script=""):
		_graph_components.EpBase.__init__(self, "PER_TICK_SCRIPT")
		self._default_script = ""
		self.script = script
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_script(self, value):
		self.script = value
		return self

	@staticmethod
	def _get_name():
		return "PER_TICK_SCRIPT"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.script != "": 
			desc += "SCRIPT=" + py_to_str(self.script) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class ShowQueriesInServerQueue(_graph_components.EpBase):
	"""
		

SHOW_QUERIES_IN_SERVER_QUEUE

Type: Other

Description: Shows the list of active queries along with
their properties on the server. "viewing_query_list"
permission is required to execute this EP.


Python
class name:
ShowQueriesInServerQueue

Input: None

Output: One tick for each query.

Parameters: None

Examples:

SHOW_QUERIES_IN_SERVER_QUEUE()

	"""
	class Parameters:
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = []
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["stack_info", "_used_strings"]

	def __init__(self):
		_graph_components.EpBase.__init__(self, "SHOW_QUERIES_IN_SERVER_QUEUE")
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	@staticmethod
	def _get_name():
		return "SHOW_QUERIES_IN_SERVER_QUEUE"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class ExecuteOnRemoteHost(_graph_components.EpBase):
	"""
		

EXECUTE_ON_REMOTE_HOST

Type: Other

Description: Executes a query in a remote location. It
facilitates conducting operations (for which the local machine lacks
permissions) on a privileged remote machine. It sends a query (which
should have exactly 1 input and 1 output) to the remote host and feeds
it with its input ticks. After processing the ticks, the remote host
returns the results, which the EXECUTE_ON_REMOTE_HOST EP returns as its
output.

Python
class name:&nbsp;ExecuteOnRemoteHost

Input: A time series of ticks.

Output: A time series of ticks.

Parameters:


  REMOTE_DB_NAME (string)
    Database name, which is in a remote location. It can be either
an explicit remote location (represented as "REMOTE@server_ip:server_port")
or a remote database name.

  
  QUERY_NAME (string)
    Specifies the query to be executed in the remote location as a
path to an OTQ file, optionally followed by the query name. Query
should have only 1 input and 1 output.

  
  OTQ_PARAMETERS (expression)
    Comma-separated list of &lt;name&gt;=&lt;value&gt;
pairs, specifying OTQ parameters for the specified query.
Default: empty

  

Examples:

EXECUTE_ON_REMOTE_HOST(REMOTE_DB_NAME=REMOTE_TAQ,QUERY_NAME="THIS::write_to_db")

EXECUTE_ON_REMOTE_HOST(REMOTE_DB_NAME=REMOTE_TAQ,QUERY_NAME="?THIS_DIR?write_to_db.otq::write_to_db")

Example use case:

Consider a situation when a user wants to run queries on the main
tick server and write the results out into a DB that is set up on a
secondary tick server.
One way to achieve this is to run the queries with tickdb_query.exe,
output the results into CSV files, fix the headers, and then load the
CSV files into the target DB using the ASCII loader.
The recommended way, however, would be to use the
EXECUTE_ON_REMOTE_HOST EP, which has better performance and is more
convenient to apply.


	"""
	class Parameters:
		remote_db_name = "REMOTE_DB_NAME"
		query_name = "QUERY_NAME"
		otq_parameters = "OTQ_PARAMETERS"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["remote_db_name", "query_name", "otq_parameters"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["remote_db_name", "_default_remote_db_name", "query_name", "_default_query_name", "otq_parameters", "_default_otq_parameters", "stack_info", "_used_strings"]

	def __init__(self, remote_db_name="", query_name="", otq_parameters=""):
		_graph_components.EpBase.__init__(self, "EXECUTE_ON_REMOTE_HOST")
		self._default_remote_db_name = ""
		self.remote_db_name = remote_db_name
		self._default_query_name = ""
		self.query_name = query_name
		self._default_otq_parameters = ""
		self.otq_parameters = otq_parameters
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_remote_db_name(self, value):
		self.remote_db_name = value
		return self

	def set_query_name(self, value):
		self.query_name = value
		return self

	def set_otq_parameters(self, value):
		self.otq_parameters = value
		return self

	@staticmethod
	def _get_name():
		return "EXECUTE_ON_REMOTE_HOST"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.remote_db_name != "": 
			desc += "REMOTE_DB_NAME=" + py_to_str(self.remote_db_name) + ","
		if self.query_name != "": 
			desc += "QUERY_NAME=" + py_to_str(self.query_name) + ","
		if self.otq_parameters != "": 
			desc += "OTQ_PARAMETERS=" + py_to_str(self.otq_parameters) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class ShutdownCepAdapter(_graph_components.EpBase):
	"""
		

SHUTDOWN_CEP_ADAPTER

Type: Other

Description: Forces the server to shutdown CEP adapter for the database specified by the DB_NAME parameter.
Python class name: ShutdownCepAdapter

Input: None

Output: None

Parameters:


DB_NAME (string)
Specifies the database name.


Example:


SHUTDOWN_CEP_ADAPTER(DB_NAME="TEST_FEED")

will force the server to shutdown the CEP adapter for TEST_FEED database.

See the SHUTDOWN_CEP_ADAPTER example in OTHER_EXAMPLES.otq.


	"""
	class Parameters:
		db_name = "DB_NAME"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["db_name"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["db_name", "_default_db_name", "stack_info", "_used_strings"]

	def __init__(self, db_name=0):
		_graph_components.EpBase.__init__(self, "SHUTDOWN_CEP_ADAPTER")
		self._default_db_name = 0
		self.db_name = db_name
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_db_name(self, value):
		self.db_name = value
		return self

	@staticmethod
	def _get_name():
		return "SHUTDOWN_CEP_ADAPTER"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.db_name != 0: 
			desc += "DB_NAME=" + py_to_str(self.db_name) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class DbShowConfiguredTimeRanges(_graph_components.EpBase):
	"""
		

DB/SHOW_CONFIGURED_TIME_RANGES

Type: Other

Description: Returns the configured time ranges for the
database, without truncating them to the query start/end times (see DB/SHOW_LOADED_TIME_RANGES).

Python
class name:&nbsp;DbShowConfiguredTimeRanges

Input: None

Output: For each configured database time range, a single
tick is returned specifying the time range by start_date and end_date,
represented in milliseconds since epoch, location,
initial_location, access_method, archive_duration,
growable_archive, day_boundary_tz, day_boundary_offset, alternative_locations specified in
LOCATION section of locator files.

Parameters:


  DB_NAME
    Specifies the database name to get the configuration for. If not
specified, the database name is extracted from the bound symbol.

  

See the DB_SHOW_CONFIGURED_TIME_RANGES
example in OTHER_EXAMPLES.otq.


	"""
	class Parameters:
		db_name = "DB_NAME"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["db_name"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["db_name", "_default_db_name", "stack_info", "_used_strings"]

	def __init__(self, db_name=""):
		_graph_components.EpBase.__init__(self, "DB/SHOW_CONFIGURED_TIME_RANGES")
		self._default_db_name = ""
		self.db_name = db_name
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_db_name(self, value):
		self.db_name = value
		return self

	@staticmethod
	def _get_name():
		return "DB/SHOW_CONFIGURED_TIME_RANGES"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.db_name != "": 
			desc += "DB_NAME=" + py_to_str(self.db_name) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class DbDestroy(_graph_components.EpBase):
	"""
		

DB/DESTROY

Type: Other

Description: Destroys DB archives.

Python
class name:&nbsp;DbDestroy

Input: None

Output: Number of individual archives destroyed.

Parameters:


  SCOPE
    Possible values are ARCHIVES_IN_TIME_RANGE
and ENTIRE_DB. The default value is ARCHIVES_IN_TIME_RANGE.

    When ARCHIVES_IN_TIME_RANGE
is specified archives in the query range will be destroyed.

    When ENTIRE_DB is specified
all archives will be destroyed.

    Correction and lock files will be destroyed along
with their respective DB archives. If remote locations fall into the
query interval, then database parts located there will also be
destroyed.

  

The user must have explicit permission to destroy the database,
specified as follows:

&lt;databases&gt;  &lt;db id="FULL_DEMO_OB1"&gt;    &lt;allow role="god" write_access="yes" destroy_access="yes"/&gt;  &lt;/db&gt;&lt;/databases&gt;
Examples:

Suppose the locator entry for the database as following:

&lt;DB ID="A" SYMBOLOGY="TICKER_DEMO"  DAY_BOUNDARY_TZ="GMT" TIME_SERIES_IS_COMPOSITE="YES"&gt;  &lt;LOCATIONS&gt;    &lt;location access_method="file"  location="D:/OMD/one_market_data/one_tick/data/A" start_time="19930101000000" end_time="20201231000000" /&gt;  &lt;/LOCATIONS&gt;&lt;/DB&gt;
and under the location specified in the configuration, two archives
exist for 20010628 and 20040628. Then, executing the following
line for database A, we will result
in both archives removed along with their lock and correction files:

DB/DESTROY(SCOPE=ENITRE_DB)
Alternatively, executing the following line (for the interval 19930101000000 and 20100101000000), will also destroy both
archives:

DB/DESTROY(SCOPE=ARCHIVES_IN_TIME_RANGE)

	"""
	class Parameters:
		scope = "SCOPE"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["scope"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["scope", "_default_scope", "stack_info", "_used_strings"]

	class Scope:
		ARCHIVES_IN_TIME_RANGE = "ARCHIVES_IN_TIME_RANGE"
		ENTIRE_DB = "ENTIRE_DB"

	def __init__(self, scope=Scope.ARCHIVES_IN_TIME_RANGE):
		_graph_components.EpBase.__init__(self, "DB/DESTROY")
		self._default_scope = type(self).Scope.ARCHIVES_IN_TIME_RANGE
		self.scope = scope
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_scope(self, value):
		self.scope = value
		return self

	@staticmethod
	def _get_name():
		return "DB/DESTROY"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.scope != self.Scope.ARCHIVES_IN_TIME_RANGE: 
			desc += "SCOPE=" + py_to_str(self.scope) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class ShowDerivedDbList(_graph_components.EpBase):
	"""
		

SHOW_DERIVED_DB_LIST

Type: Other

Description: Shows the list of derived databases. See Derived Databases for details.

Python
class name:&nbsp;ShowDerivedDbList

Input: None

Output: A list of derived databases, represented in base_db_name//derived_db_name format.
Database names in the output are unique. Will skip processing of
directories which have &lt;.tmp&gt; extension.

Parameters:


  DB_DISCOVERY_SCOPE
(QUERY_HOST_AND_ALL_REACHABLE_HOSTS, QUERY_HOST_ONLY - default)
    When QUERY_HOST_AND_ALL_REACHABLE_HOSTS
is specified, an attempt will be performed to get derived databases
from all hosts reachable&nbsp;from
the process (it could be a client process, or could be a tick server,
depending on where a queried symbol resides) where the query containing
this event processor is executed.

    When QUERY_HOST_ONLY is
specified, only derived databases from the host on which the query is
performed will be returned.

  
  TIME_RANGE (CONFIGURED_TIME_INTERVAL -
default, QUERY_TIME_INTERVAL)
    When CONFIGURED_TIME_INTERVAL
is specified, an attempt will be performed to get all possible derived
databases.

    When QUERY_TIME_INTERVAL is
specified, only derived databases that have data/raw data within the
specified query time interval will be returned.

  
  SELECTION_CRITERIA (enum)
     Possible values are ALL, DERIVED_FROM_CURRENT_DB and DIRECT_CHILDREN_OF_CURRENT_DB. Default
value is ALL.

  

Example:

SHOW_DERIVED_DB_LIST(DB_DISCOVERY_SCOPE=QUERY_HOST_AND_ALL_REACHABLE_HOSTS)
See the SHOW_DERIVED_DB_LIST
example in OTHER_EXAMPLES.otq.


	"""
	class Parameters:
		db_discovery_scope = "DB_DISCOVERY_SCOPE"
		time_range = "TIME_RANGE"
		selection_criteria = "SELECTION_CRITERIA"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["db_discovery_scope", "time_range", "selection_criteria"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["db_discovery_scope", "_default_db_discovery_scope", "time_range", "_default_time_range", "selection_criteria", "_default_selection_criteria", "stack_info", "_used_strings"]

	class DbDiscoveryScope:
		QUERY_HOST_AND_ALL_REACHABLE_HOSTS = "QUERY_HOST_AND_ALL_REACHABLE_HOSTS"
		QUERY_HOST_ONLY = "QUERY_HOST_ONLY"

	class TimeRange:
		CONFIGURED_TIME_INTERVAL = "CONFIGURED_TIME_INTERVAL"
		QUERY_TIME_INTERVAL = "QUERY_TIME_INTERVAL"

	class SelectionCriteria:
		ALL = "ALL"
		DERIVED_FROM_CURRENT_DB = "DERIVED_FROM_CURRENT_DB"
		DIRECT_CHILDREN_OF_CURRENT_DB = "DIRECT_CHILDREN_OF_CURRENT_DB"

	def __init__(self, db_discovery_scope=DbDiscoveryScope.QUERY_HOST_ONLY, time_range=TimeRange.CONFIGURED_TIME_INTERVAL, selection_criteria=SelectionCriteria.ALL):
		_graph_components.EpBase.__init__(self, "SHOW_DERIVED_DB_LIST")
		self._default_db_discovery_scope = type(self).DbDiscoveryScope.QUERY_HOST_ONLY
		self.db_discovery_scope = db_discovery_scope
		self._default_time_range = type(self).TimeRange.CONFIGURED_TIME_INTERVAL
		self.time_range = time_range
		self._default_selection_criteria = type(self).SelectionCriteria.ALL
		self.selection_criteria = selection_criteria
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_db_discovery_scope(self, value):
		self.db_discovery_scope = value
		return self

	def set_time_range(self, value):
		self.time_range = value
		return self

	def set_selection_criteria(self, value):
		self.selection_criteria = value
		return self

	@staticmethod
	def _get_name():
		return "SHOW_DERIVED_DB_LIST"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.db_discovery_scope != self.DbDiscoveryScope.QUERY_HOST_ONLY: 
			desc += "DB_DISCOVERY_SCOPE=" + py_to_str(self.db_discovery_scope) + ","
		if self.time_range != self.TimeRange.CONFIGURED_TIME_INTERVAL: 
			desc += "TIME_RANGE=" + py_to_str(self.time_range) + ","
		if self.selection_criteria != self.SelectionCriteria.ALL: 
			desc += "SELECTION_CRITERIA=" + py_to_str(self.selection_criteria) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class ShowQueryFromLog(_graph_components.EpBase):
	"""
		

SHOW_QUERY_FROM_LOG

Type: Other

Description: Returns the query from a given log line. If
access control is enabled on the server, then corresponding permissions
will be required.

Python
class name:&nbsp;ShowQueryFromLog

Input: None

Output: Returns the log line about the query in OTQ format,
or throws an exception if the query was not found.

Parameters:


  LOG_FILE_NAME (string)
    Specifies the relative or absolute path to the log file.

  
  LINE_IN_THE_LOG (integer)
    Specifies the line number(both actual graph and RequestStart
lines can be specified)

  

Access control:

By default, all users are allowed to use this event processor for
reading from any directory. Limitations can be applied using access
control. If SHOW_QUERY_FROM_LOG is
present in an access control event processor list, then only the
specified roles can use this event processor. Additionally, for each
role allowed directories can be specified. If some directory is
allowed, all of its subdirectories are allowed. Omitting this parameter
means that all directories are allowed. If a user has several roles,
access is granted if at least one of her roles is allowed. One can also
specify allowed directories list in the event processor level (not only
role level). This means that directories listed in the event processor
level are allowed for all users (whose roles aren't mentioned under the
section for this event processor, or the user does not have any role in
the access control file). Role level permissions override event
processor level permissions for users whose roles are mentioned in
access control for this EP. If the LOG_FILE_NAME
parameter holds a relative path, then the SERVER_LOG_DIR
config parameter will be used to get the absolute path to the log file.
Directories are specified using any separator symbol (comma is the
default).


	"""
	class Parameters:
		log_file_name = "LOG_FILE_NAME"
		line_in_the_log = "LINE_IN_THE_LOG"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["log_file_name", "line_in_the_log"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["log_file_name", "_default_log_file_name", "line_in_the_log", "_default_line_in_the_log", "stack_info", "_used_strings"]

	def __init__(self, log_file_name="", line_in_the_log=""):
		_graph_components.EpBase.__init__(self, "SHOW_QUERY_FROM_LOG")
		self._default_log_file_name = ""
		self.log_file_name = log_file_name
		self._default_line_in_the_log = ""
		self.line_in_the_log = line_in_the_log
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_log_file_name(self, value):
		self.log_file_name = value
		return self

	def set_line_in_the_log(self, value):
		self.line_in_the_log = value
		return self

	@staticmethod
	def _get_name():
		return "SHOW_QUERY_FROM_LOG"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.log_file_name != "": 
			desc += "LOG_FILE_NAME=" + py_to_str(self.log_file_name) + ","
		if self.line_in_the_log != "": 
			desc += "LINE_IN_THE_LOG=" + py_to_str(self.line_in_the_log) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class DbGetSymbolOffsets(_graph_components.EpBase):
	"""
		

DB/GET_SYMBOL_OFFSETS

Type: Other

Description: This EP shows data date, data file (usually the
name of the file starts with data_unassigned), start and end offsets
within archive data file for each queried symbol for actually loaded
days.

It must be the first (source) EP on the graph.

Python
class name:&nbsp;DbGetSymbolOffsets

Input: None

Output: A time series of ticks with fields DATADATE,
DATA_FILE, START_OFFSET, and END_OFFSET

Parameters: None

Examples:

DB/GET_SYMBOL_OFFSETS ()





	"""
	class Parameters:
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = []
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["stack_info", "_used_strings"]

	def __init__(self):
		_graph_components.EpBase.__init__(self, "DB/GET_SYMBOL_OFFSETS")
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	@staticmethod
	def _get_name():
		return "DB/GET_SYMBOL_OFFSETS"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class CaptureReplayOtEvents(_graph_components.EpBase):
	"""
		

CAPTURE_REPLAY_OT_EVENTS

Type: Other

Description: Writes all the callbacks to a binary file. After
which, in the case of an error, it can find the problematic symbol
leading to an error or a crash. Also, it can replay the callbacks from
that file for any given symbol.

Python
class name:&nbsp;CaptureReplayOtEvents

Input: A time series of ticks

Output: A time series of ticks

Parameters:


  OPERATION_MODE (string)
    Specifies the operational mode of the EP:

    
      RECORD - Writes all
callbacks to a file.
      FIND_PROBLEMATIC_SYMBOL -
Outputs the symbol that should be set in the non-bound symbols list to
reproduce the issue.
      REPLAY_PROBLEMATIC_SYMBOL -
Propagates all callbacks in the file for that symbol. In this mode, the
non-bound symbols list must contain only one symbol or it must be empty.
    
    Default: RECORD

  
  RECORDED_DATA_FILE (string)
    The path to the file to write to (for the RECORD mode) or to
read from (for the REPLAY_PROBLEMATIC_SYMBOL and
FIND_PROBLEMATIC_SYMBOL modes).

  

Notes:

In RECORD mode, it can have only one input. In REPLAY mode, it is a
data source and cannot have any input. Its destination EP cannot have
another input in all modes.

Examples:

In this example, CAPTURE_REPLAY_OT_EVENTS writes callbacks to
D:\\CaptureReplayOtEvents\\output.txt:

CAPTURE_REPLAY_OT_EVENTS ("RECORD",
"D:\\CaptureReplayOtEvents\\\\output.txt")

In this example, CAPTURE_REPLAY_OT_EVENTS replays all callbacks for
the non-bound symbol from D:\\CaptureReplayOtEvents\\output.txt:

CAPTURE_REPLAY_OT_EVENTS
("REPLAY_PROBLEMATIC_SYMBOL", "D:\\CaptureReplayOtEvents\\\\output.txt")

In this example, CAPTURE_REPLAY_OT_EVENTS returns the symbol that
leads to an error for the recorded data in
D:\\CaptureReplayOtEvents\\output.txt:

CAPTURE_REPLAY_OT_EVENTS ("FIND_PROBLEMATIC_SYMBOL",
"D:\\CaptureReplayOtEvents\\\\output.txt")

See the examples in capture_replay_ot_events.otq.


	"""
	class Parameters:
		operation_mode = "OPERATION_MODE"
		recorded_data_file = "RECORDED_DATA_FILE"
		replay_for_empty_tick_type = "REPLAY_FOR_EMPTY_TICK_TYPE"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["operation_mode", "recorded_data_file", "replay_for_empty_tick_type"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["operation_mode", "_default_operation_mode", "recorded_data_file", "_default_recorded_data_file", "replay_for_empty_tick_type", "_default_replay_for_empty_tick_type", "stack_info", "_used_strings"]

	class OperationMode:
		FIND_PROBLEMATIC_SYMBOL = "FIND_PROBLEMATIC_SYMBOL"
		RECORD = "RECORD"
		REPLAY_PROBLEMATIC_SYMBOL = "REPLAY_PROBLEMATIC_SYMBOL"

	def __init__(self, operation_mode=OperationMode.RECORD, recorded_data_file="", replay_for_empty_tick_type=False):
		_graph_components.EpBase.__init__(self, "CAPTURE_REPLAY_OT_EVENTS")
		self._default_operation_mode = type(self).OperationMode.RECORD
		self.operation_mode = operation_mode
		self._default_recorded_data_file = ""
		self.recorded_data_file = recorded_data_file
		self._default_replay_for_empty_tick_type = False
		self.replay_for_empty_tick_type = replay_for_empty_tick_type
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_operation_mode(self, value):
		self.operation_mode = value
		return self

	def set_recorded_data_file(self, value):
		self.recorded_data_file = value
		return self

	def set_replay_for_empty_tick_type(self, value):
		self.replay_for_empty_tick_type = value
		return self

	@staticmethod
	def _get_name():
		return "CAPTURE_REPLAY_OT_EVENTS"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.operation_mode != self.OperationMode.RECORD: 
			desc += "OPERATION_MODE=" + py_to_str(self.operation_mode) + ","
		if self.recorded_data_file != "": 
			desc += "RECORDED_DATA_FILE=" + py_to_str(self.recorded_data_file) + ","
		if self.replay_for_empty_tick_type != False: 
			desc += "REPLAY_FOR_EMPTY_TICK_TYPE=" + py_to_str(self.replay_for_empty_tick_type) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class SchedulerGetExecutors(_graph_components.EpBase):
	"""
		

SCHEDULER/GET_EXECUTORS


Scheduler

Scheduler is a module which helps to start and terminate cloud
instances, based on current load and configuration.

The SCHEDULER/GET_EXECUTORS EP
allows you to request a list of tick servers from the Scheduler for
executing tasks. It is useful in scenarios where tasks can be
parallelized across multiple servers, such as raw data loading. An
example is OneTick
Cluster Archive Loader which utilizes this EP under the hood,
when&nbsp;configured to use a non-Kubernetes cluster, in order to
execute loading commands concurrently, on multiple hosts.&nbsp;


Usage
In order to be able to run this EP,&nbsp;Scheduler has to be
configured. Please refer to Scheduler
documention for more information.

The client locator should have Scheduler Cluster configured. For
this, the CLUSTER tag should have JOB_CLIENT set to NATIVE and SCHEDULER
attribute should be set to the proper Scheduler address

  &lt;CLUSTERS&gt;    &lt;CLUSTER ID="SCHEDULER_CLUSTER" JOB_CLIENT="NATIVE" SCHEDULER="address:port" /&gt;  &lt;/CLUSTERS&gt;
When using a client locator file with MASTER_AM_SERVERS section, there is no
need to manually configure the cluster, as it will be automatically
auto-discovered

Once such a configuration is provided, this EP can be run. As a
result, a list of tick servers will be obtained. These tick servers can
be used to execute commands using COMMAND_EXECUTE
EP

Response
If there are available tick servers, their addresses will be
returned in the ADDRESS:PORT format
in an output field TICK_SERVER_ADDRESS.
For every requested task, a new tick will be produced.

Behavior and Error Handling
In case there are not enough already running tick servers, the
Scheduler will try to allocate new cloud instances and return addresses
of tick servers from these host instances. If the request cannot be
immediately fulfilled due to the Scheduler reaching the maximum number
of host instances, a per-symbol warning All
running instances are busy and a new instance cannot be started for now
because the maximum number of instances is reached with error
code omd::SymbolError::LIMIT_IS_REACHED
will be produced. The Scheduler will then wait until resources become
available and propagate the addresses of tick servers as soon as new
host instances can be allocated or existing tick server hosts become
available.

If the Scheduler for the requested cluster is disabled during the SCHEDULER/GET_EXECUTORS EP run, the
request will fail with the message Scheduler
was disabled for the cluster and a per-symbol error code omd::SymbolError::EXECUTED_COMMAND_ERROR.

All allocated instances for the current cluster are
terminated when the Scheduler becomes disabled for this cluster.


Type: Other

Description: Request a list of tick servers from the
Scheduler for execution of tasks.

Python
class name:&nbsp;SchedulerGetExecutors

Input: None.

Output: A time series of ticks.

Parameters:


  CLUSTER_ID (string)
    The unique ID of a cluster from which tick servers are requested.
The EP will fail, if the cluster ID of a non-existing cluster is
specified or the cluster is not configured to use scheduler.

  
  NUM_TASKS (integer)
    A number of tasks which need to be started.
The scheduler will produce a separate tick for every requested task,
with the tick containing the address of the tick server.

  
  NUM_TASKS_PER_HOST (integer)
    Maximal number of tasks which can be executed on the same tick
server host.

  
  USE_PUBLIC_ADDRESSES (Boolean)
    Tells Scheduler whether a public or a private address of the
tick server host should be returned.

  

Examples:

See the GET_EXECUTORS_FOR_2_TASKS
example in SCHEDULER_GET_EXECUTORS_EXAMPLE.otq.


	"""
	class Parameters:
		cluster_id = "CLUSTER_ID"
		num_tasks = "NUM_TASKS"
		num_tasks_per_host = "NUM_TASKS_PER_HOST"
		use_public_addresses = "USE_PUBLIC_ADDRESSES"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["cluster_id", "num_tasks", "num_tasks_per_host", "use_public_addresses"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["cluster_id", "_default_cluster_id", "num_tasks", "_default_num_tasks", "num_tasks_per_host", "_default_num_tasks_per_host", "use_public_addresses", "_default_use_public_addresses", "stack_info", "_used_strings"]

	def __init__(self, cluster_id="", num_tasks="", num_tasks_per_host="", use_public_addresses=False):
		_graph_components.EpBase.__init__(self, "SCHEDULER/GET_EXECUTORS")
		self._default_cluster_id = ""
		self.cluster_id = cluster_id
		self._default_num_tasks = ""
		self.num_tasks = num_tasks
		self._default_num_tasks_per_host = ""
		self.num_tasks_per_host = num_tasks_per_host
		self._default_use_public_addresses = False
		self.use_public_addresses = use_public_addresses
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_cluster_id(self, value):
		self.cluster_id = value
		return self

	def set_num_tasks(self, value):
		self.num_tasks = value
		return self

	def set_num_tasks_per_host(self, value):
		self.num_tasks_per_host = value
		return self

	def set_use_public_addresses(self, value):
		self.use_public_addresses = value
		return self

	@staticmethod
	def _get_name():
		return "SCHEDULER/GET_EXECUTORS"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.cluster_id != "": 
			desc += "CLUSTER_ID=" + py_to_str(self.cluster_id) + ","
		if self.num_tasks != "": 
			desc += "NUM_TASKS=" + py_to_str(self.num_tasks) + ","
		if self.num_tasks_per_host != "": 
			desc += "NUM_TASKS_PER_HOST=" + py_to_str(self.num_tasks_per_host) + ","
		if self.use_public_addresses != False: 
			desc += "USE_PUBLIC_ADDRESSES=" + py_to_str(self.use_public_addresses) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class StartQueryOnServer(_graph_components.EpBase):
	"""
		

START_QUERY_ON_SERVER

Type: Other

Description: Starts the specified query on server after which
client disconnects. EP returns a single tick, specifying query status
and query id which allows client to monitor the query.

Python
class name:&nbsp;StartQueryOnServer

Input: None.

Output: A single tick with the following schema:


  
    
      Field type
      Field name
      Example value
      Description
    
    
      String
      QUERY_STATUS
      RUNNING
      Possible values are FAILED,
      RUNNING, TIMEOUT, FINISHED
    
    
      String
      QUERY_ID
      hostname.26939.20200119093242.427.13
      This field is present only if query ran long enough to
extract query id
    
    
      String
      ERROR_MSG
      Exception occurred during processing query...
      This field presents only if QUERY_STATUS=FAILED
    
  

Symbols from outer query are used. If outer query has only &amp;ltDBNAME&amp;gt:: as a symbol, then
symbols from the specified query are used.

Parameters:


  OTQ_FILE_PATH (string)
    The path to an .otq file, possibly followed by the query name (&lt;otq_file_path&gt;::&lt;query_name&gt;),
the latter being needed if the specified .otq file contains multiple
queries.

  
  OTQ_PARAMS (string)
    A comma-separated list of &lt;param_name&gt;=&lt;value&gt;
pairs, used to override default values of .otq parameters in a
specified .otq file.
Default: EMPTY

  
  WAIT_FOR_CEP_STITCHING
(Boolean)
    If specified, query returns only after all symbols stitched with
realtime ticks.
Default: false

  
  TIMEOUT (time in seconds)
    Specifies the time to wait for query. If query doesn't start
after that time, EP will send a tick with QUERY_STATUS=TIMEOUT.
Default: 10 seconds

  




	"""
	class Parameters:
		otq_file_path = "OTQ_FILE_PATH"
		otq_params = "OTQ_PARAMS"
		wait_for_cep_stitching = "WAIT_FOR_CEP_STITCHING"
		timeout = "TIMEOUT"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["otq_file_path", "otq_params", "wait_for_cep_stitching", "timeout"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["otq_file_path", "_default_otq_file_path", "otq_params", "_default_otq_params", "wait_for_cep_stitching", "_default_wait_for_cep_stitching", "timeout", "_default_timeout", "stack_info", "_used_strings"]

	def __init__(self, otq_file_path="", otq_params="", wait_for_cep_stitching="", timeout=""):
		_graph_components.EpBase.__init__(self, "START_QUERY_ON_SERVER")
		self._default_otq_file_path = ""
		self.otq_file_path = otq_file_path
		self._default_otq_params = ""
		self.otq_params = otq_params
		self._default_wait_for_cep_stitching = ""
		self.wait_for_cep_stitching = wait_for_cep_stitching
		self._default_timeout = ""
		self.timeout = timeout
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_otq_file_path(self, value):
		self.otq_file_path = value
		return self

	def set_otq_params(self, value):
		self.otq_params = value
		return self

	def set_wait_for_cep_stitching(self, value):
		self.wait_for_cep_stitching = value
		return self

	def set_timeout(self, value):
		self.timeout = value
		return self

	@staticmethod
	def _get_name():
		return "START_QUERY_ON_SERVER"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.otq_file_path != "": 
			desc += "OTQ_FILE_PATH=" + py_to_str(self.otq_file_path) + ","
		if self.otq_params != "": 
			desc += "OTQ_PARAMS=" + py_to_str(self.otq_params) + ","
		if self.wait_for_cep_stitching != "": 
			desc += "WAIT_FOR_CEP_STITCHING=" + py_to_str(self.wait_for_cep_stitching) + ","
		if self.timeout != "": 
			desc += "TIMEOUT=" + py_to_str(self.timeout) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class TryCatch(_graph_components.EpBase):
	"""
		

TRY_CATCH

Type: Other

Description: Executes a specified query graph converting
exceptions caught during its execution into per symbol errors. The
ticks incoming at the input of this EP will be propagated to the input
node of the specified query, while all the output ticks of the latter
will be propagated to the output of this EP. A regular expression can
be specified via the EXCEPTION_TEXT_REGEX parameter such that
only those exceptions whose text message contains matching text will be
converted to per symbol errors, while others will terminate the whole
query as usual.

Python
class name:
TryCatch

Input: The same as for the input node of the specified query
graph. (for example, if it is a source of ticks, then this EP should
have no input as well). 

Output: A time series of ticks.

Parameters:


  QUERY_NAME (string)
    Specifies the query to be executed as a path to an .otq file,
possibly followed by the query name (&lt;otq_file_path&gt;::&lt;query_name&gt;).
The specified query must have a single input node and a single output
node.

  
  OTQ_PARAMETERS (string)
    Comma separated list of &lt;name&gt;=&lt;value&gt; pairs
specifying OTQ parameters of the query to be executed.
Default: empty

  
  EXCEPTION_TEXT_REGEX (string)
    Only those exceptions whose text contains a substring that
matches the specified regular epxression will be converted to per
symbol errors. Others will be propagated and will terminate the query.
If left empty, all exceptions will be converted to per symbol errors
Default: empty

  
  LOG_EXCEPTION (boolean)
    Specifies whether the exception which was caught and converted
to a per symbol error should also appear in logs.
Default: false

  


	"""
	class Parameters:
		query_name = "QUERY_NAME"
		otq_parameters = "OTQ_PARAMETERS"
		exception_text_regex = "EXCEPTION_TEXT_REGEX"
		log_exception = "LOG_EXCEPTION"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["query_name", "otq_parameters", "exception_text_regex", "log_exception"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["query_name", "_default_query_name", "otq_parameters", "_default_otq_parameters", "exception_text_regex", "_default_exception_text_regex", "log_exception", "_default_log_exception", "stack_info", "_used_strings"]

	def __init__(self, query_name="", otq_parameters="", exception_text_regex="", log_exception=False):
		_graph_components.EpBase.__init__(self, "TRY_CATCH")
		self._default_query_name = ""
		self.query_name = query_name
		self._default_otq_parameters = ""
		self.otq_parameters = otq_parameters
		self._default_exception_text_regex = ""
		self.exception_text_regex = exception_text_regex
		self._default_log_exception = False
		self.log_exception = log_exception
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_query_name(self, value):
		self.query_name = value
		return self

	def set_otq_parameters(self, value):
		self.otq_parameters = value
		return self

	def set_exception_text_regex(self, value):
		self.exception_text_regex = value
		return self

	def set_log_exception(self, value):
		self.log_exception = value
		return self

	@staticmethod
	def _get_name():
		return "TRY_CATCH"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.query_name != "": 
			desc += "QUERY_NAME=" + py_to_str(self.query_name) + ","
		if self.otq_parameters != "": 
			desc += "OTQ_PARAMETERS=" + py_to_str(self.otq_parameters) + ","
		if self.exception_text_regex != "": 
			desc += "EXCEPTION_TEXT_REGEX=" + py_to_str(self.exception_text_regex) + ","
		if self.log_exception != False: 
			desc += "LOG_EXCEPTION=" + py_to_str(self.log_exception) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class DbShowDefaultTickTypes(_graph_components.EpBase):
	"""
		

DB/SHOW_DEFAULT_TICK_TYPES

Type: Other

Description: This event processor shows default schemas for
the specified (or all)&nbsp; tick types for the queried database. These
schemas are created and modified using DB/MODIFY_DEFAULT_TICK_TYPES
event processor.
They are recorded in the directory pointed to by the DEFAULT_SCHEMA_DIR
locator parameter for this database. These
schemas are used in the queries that have Apply
default db schema query
property enabled. In those queries, the default schema configured for a
tick type of a database overrides tick descriptors of ticks extracted
from that tick type of that database. These schemas are also used in DB/INSERT_ROW event processor if no ticks
are present for the symbol and no schema is specified.

Python
class name:
DbShowDefaultTickTypes

Input: A time series of ticks.

Output: A tick for each field of the configured default
schema, for the specified tick type or, if no tick type was specified
in this EP, schemas for all tick types will be shown.

Parameters:


  TICK_TYPE (string)
    If specified, output ticks will be produced only for the schema
of this tick type.
Default: empty

  

Examples:

Show all default tick types for a database and their schemas:

DB/SHOW_DEFAULT_TICK_TYPES ()


	"""
	class Parameters:
		tick_type_field = "TICK_TYPE"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["tick_type_field"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["tick_type_field", "_default_tick_type_field", "stack_info", "_used_strings"]

	def __init__(self, tick_type_field=""):
		_graph_components.EpBase.__init__(self, "DB/SHOW_DEFAULT_TICK_TYPES")
		self._default_tick_type_field = ""
		self.tick_type_field = tick_type_field
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_tick_type_field(self, value):
		self.tick_type_field = value
		return self

	@staticmethod
	def _get_name():
		return "DB/SHOW_DEFAULT_TICK_TYPES"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.tick_type_field != "": 
			desc += "TICK_TYPE_FIELD=" + py_to_str(self.tick_type_field) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class DbModifyDefaultTickTypes(_graph_components.EpBase):
	"""
		

DB/MODIFY_DEFAULT_TICK_TYPES

Type: Other

Description: This event processor creates or modifies the
default tick schemas for the specified tick type (DEFAULT_SCHEMA_DIR locator variable must
be configured for the database). These
schemas are used in the queries that have Apply
default db schema query
property enabled. In those queries, the default schema configured for a
tick type of a database overrides tick descriptors of ticks extracted
from that tick type of that database. These schemas are also used in DB/INSERT_ROW event processor if no ticks
are present for the symbol and no schema is specified.

In order to view currently configured schemas, please use DB/SHOW_DEFAULT_TICK_TYPES
event processor.

Python
class name:&nbsp;DbModifyDefaultTickTypes

Input: A time series of ticks.

Output: A time series of ticks.

Parameters:


  TICK_TYPE (string)
    The tick type, whose schema should be modified.

  
  SCHEMA (field_name field_type , field_type
field_name )
    A comma-separated list of fields to use as a default structure
for TICK_TYPE.
The list of supported types are listed here.

  
  ACTION (string)
    Supported values are ADD, MODIFY and DELETE.
If set to ADD, new tick type will be added to the list. If
set to MODIFY, the schema of existing TICK_TYPE will be changed with
the specified SCHEMA. If set to DELETE, the schema of specified
TICK_TYPE will be removed from the list.
Default: ADD

  
  TS_PROPERTIES (property_name =
property_value, )
    A comma-separated list of property_name = property_value
strings, which represent the tick descriptor properties.
Default: empty

  

Examples:

Add a tick type A with 3 fields:

DB/MODIFY_DEFAULT_TICK_TYPES(TICK_TYPE=A,SCHEMA="double
PRICE, SIZE int, NEW_FLD string[20]")


	"""
	class Parameters:
		tick_type_field = "TICK_TYPE"
		schema = "SCHEMA"
		action = "ACTION"
		ts_properties = "TS_PROPERTIES"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["tick_type_field", "schema", "action", "ts_properties"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["tick_type_field", "_default_tick_type_field", "schema", "_default_schema", "action", "_default_action", "ts_properties", "_default_ts_properties", "stack_info", "_used_strings"]

	class Action:
		ADD = "ADD"
		DELETE = "DELETE"
		MODIFY = "MODIFY"

	def __init__(self, tick_type_field="", schema="", action=Action.ADD, ts_properties=""):
		_graph_components.EpBase.__init__(self, "DB/MODIFY_DEFAULT_TICK_TYPES")
		self._default_tick_type_field = ""
		self.tick_type_field = tick_type_field
		self._default_schema = ""
		self.schema = schema
		self._default_action = type(self).Action.ADD
		self.action = action
		self._default_ts_properties = ""
		self.ts_properties = ts_properties
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_tick_type_field(self, value):
		self.tick_type_field = value
		return self

	def set_schema(self, value):
		self.schema = value
		return self

	def set_action(self, value):
		self.action = value
		return self

	def set_ts_properties(self, value):
		self.ts_properties = value
		return self

	@staticmethod
	def _get_name():
		return "DB/MODIFY_DEFAULT_TICK_TYPES"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.tick_type_field != "": 
			desc += "TICK_TYPE_FIELD=" + py_to_str(self.tick_type_field) + ","
		if self.schema != "": 
			desc += "SCHEMA=" + py_to_str(self.schema) + ","
		if self.action != self.Action.ADD: 
			desc += "ACTION=" + py_to_str(self.action) + ","
		if self.ts_properties != "": 
			desc += "TS_PROPERTIES=" + py_to_str(self.ts_properties) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class Om_walletManager(_graph_components.EpBase):
	"""
		

OM/WALLET_MANAGER

Type: Other

Description: Implements wallet management logic for various
types of orders, including limit and market orders, and for
cancellations. Every order is to trade specified amount of currency, in
exchange for money in another currency. Thus, every order involves two
currencies. These pairs of currencies are referred below as trading
pairs.

LOAD_OM config parameter should be
set in order to be able to use this EP.

Reservation process
Every trading pair consists of a market currency (there may be more
than one market currencies), and non-market currency. FROM_WALLET_ID always represents market
currency, and TO_WALLET_ID
represents non-market currency.

This is how PRICE and SIZE are interpreted for limit and market
orders submitted into this EP:

limit sell - sell SIZE of non-market currency coins at PRICE
limit buy - buy SIZE of non-market currency coins at PRICE
market sell - sell SIZE of non-market currency coins
market buy - buy non-market currency coins for total amount of SIZE
market coins

For limit sell and market sell orders, this EP reserves SIZE currency from TO_WALLET_ID, and adds SIZE * PRICE (of the fill) to FROM_WALLET_ID when gets fills from
matching engine. For limit buy orders, this EP reserves SIZE * PRICE from FROM_WALLET_ID, and adds SIZE currency to TO_WALLET_ID when gets fills from matching
engine. For market buy orders, this EP reserves SIZE from FROM_WALLET_ID,
and adds the value of SIZE of the fill,
which represents amount of bought non-market currency coins),
multiplied by PRICE (of the fill) to TO_WALLET_ID
when it gets fills. Orders may execute partially, in partially executed
fill SIZE fields is the executed
size, and REMAINING_SIZE field is
the remaining size.

In the above mentioned reservation process EP doesn't
check PRICE and SIZE fields for NaN.

Rejections
EP will reject the order in the following cases. A RJC tick will be
generated for each case.


  When source wallet doesn't exist
  When trading is frozen for the trading pair
  When source wallet exists and trading is frozen for it
  When source wallet exists and withdrawals from it are frozen
  When destination wallet exists and trading is frozen for it
  When trading pair for that order isn't found in TRD_PAIR_FEE_MAP_OTQ query (all trading
pairs should be listed in that mapping)
  When for a limit buy order reservation size is smaller than 10e-8
  When source wallet doesn't have enough balance

EP will reject the cancellation in the following cases. No RJC tick
will be generated.


  When source wallet doesn't exist (such scenario can happen if at
the moment EP processes the cancellation the order was already fully
executed, wallet became empty and got removed)
  When trading is frozen for the trading pair
  When source wallet exists and trading is frozen for it
  When destination wallet exists and trading is frozen for it

Applying fees
For each user, this EP should get a fee message, which specifies
what percentage (floating point value in the range of 0 to 100) of
filled order should be charged as a fee for taker and maker orders.
This message is not mandatory. No exception will be thrown if it was
not provided for a given user. The fee will be assumed to be 0 if fee
message was not submitted for the user.

An additional wallet can be supplied via FEE_WALLET_ID
field. For maker orders EP doesn't check PAY_FROM_FEE_WALLET
flag, it always tries to take fee from fee wallet. If there isn't
enough balance, the remaining fee will be taken from the destination
wallet. For taker orders, if PAY_FROM_FEE_WALLET=1,
fee will be taken from fee wallet. If balance isn't sufficient, the
remaining fee will be taken from destination wallet. If PAY_FROM_FEE_WALLET=0, fee will be taken
from the destination wallet. Fees can also be negative, which means
that EP will add money to the fee or destination wallet. If fee wallet
is missing, and EP is going to add money to it, it'll implicitly create
the wallet. Fee wallets for all users should have the same currency.

For taker orders, when PAY_FROM_FEE_WALLET=1
is set, but fee wallet doesn't have enough balance, fee taken from the
destination wallet will be multiplied with DEST_WALLET_FEE_MULTIPLIER
value (see it documented here).

When a fee is taken from fee wallet, EP performs currency
conversion. If destination wallet is the one with market currency, EP
first tries to get the rate between market currency and fee currency
from the matching engine. If it's missing there, EP will try to get
default rate. Default rates between market currencies and fee currency
should be specified via DFLT_FEE_RATE_TRD_PAIR_MAP_OTQ
EP parameter. Returned ticks from the query should have TRD_PAIR_ID and DEFAULT_RATE
fields. Additionally, it's possible to pass otq parameters to that
query via DFLT_FEE_RATE_TRD_PAIR_MAP_OTQ_PARAM
EP parameter.

Example output of default fee rate trading pair map query:

#TRD_PAIR_ID,DEFAULT_RATE1, 500.12251, 1000.366
If rate is missing from both matching engine and from default rates
map, EP will throw an exception. If destination wallet is the one with
non-market currency, two conversions should be done. The rate between
market currency and non-market currency EP gets from the arrived fill.
The rate between market currency and fee currency will be calculated as
mentioned above. In order to get the rate between currencies from
matching engine, EP needs to know the trading pair for them. For that
purpose, users are required to set FEE_TRD_PAIR_MAP_OTQ
EP parameter, to point to the query which has those mappings. Output
ticks of that query should have TRD_PAIR_ID
and FEE_TRD_PAIR_ID fields.
Additionally, it's possible to pass otq parameters to that query via FEE_TRD_PAIR_MAP_OTQ_PARAMS EP parameter.

Example output of fee trading pair map query:

#TRD_PAIR_ID,FEE_TRD_PAIR_ID1, 12, 13, 14, 15, 110, 211, 212, 213, 214, 2
Configuration file
WALLET_MANAGER.CONFIGURATION_FILE
is a mandatory configuration parameter, where all time based parameters
should be configured. Currently supported parameters are:


  DEST_WALLET_FEE_MULTIPLIER - the multiplier which should be used
for taker orders when PAY_FROM_FEE_WALLET=1
and fee wallet doesn't have enough balance. The format is DEST_WALLET_FEE_MULTIPLIER,&lt;YYYYMMDDhhmmss&gt;,&lt;multiplier&gt;

Dummy symbols
CEP adapter starts replaying heartbeats after getting the first tick
for the symbol. If after the restart none of the symbols within
partition tick, no heartbeat for that partition will be submitted, and
the whole system will hang. To avoid this problem, FEE ticks should be
submitted for dummy symbols periodically. Any non-numeric symbol is
considered as dummy by the EP. EP propagates heartbeats for them, and
ignores all the other messages (which means the values of the fields in
those FEE ticks don't matter). Dummy symbols should be configured as
other symbols, except that they don't need to be configured in the
account group thread id map query.

Fill processing
In order to ensure both chronological and fully repeatable
processing of all input data, EP should take care of random real-time
delays with which feedback messages (filled orders, successful
cancellations and rejections) arrive at wallet manager. For this
reason, value specified for FILL_PROCESSING_DELAY
parameter is added to the primary timestamp of all messages that arrive
from the feedback connection. If a tick from orders input arrives at EP
with timestamp greater than the timestamp of last arrived feedback
event (primary timestamp + FILL_PROCESSING_DELAY),
EP delays processing of all its input events until it gets feedback
event, for which primary timestamp + FILL_PROCESSING_DELAY
is higher than received order's timestamp.

Input: EP has 5 inputs:

Orders input
Orders input includes various types of orders, including limit and
market orders, and cancellations. In case of cancellations, this EP
simply propagates ticks to OM::MATCHING_ENGINE_SIMPLE.
Matched orders and successfully applied cancellations reach OM::WALLET_MANAGER EP indirectly, via OM::WALLET_MANAGER_FEEDBACK EP which helps
close a feedback loop of the trading application. For the incoming
orders, OM::WALLET_MANAGER first
checks if wallet has enough currency to process the order. If not,
marks order as rejected ( RECORD_TYPE='r')
and propagates the order to the matching engine. If the order isn't
rejected, EP reserves money from wallet, which will be confirmed after OM::WALLET_MANAGER EP gets a corresponding
fill message from the feedback (after order will be executed in OM::MATCHING_ENGINE_SIMPLE).

For performance reasons, it's advisable to have fields in order
input ordered by size, and all string fields at the end. That'll
eliminate the need of reordering fields during recording the data.

Mandatory fields for this input are the following:


  
    
      Field type
      Field name
      Example value
      Description
    
    
      uInt64
      ORDER_ID
      14522
      Unique identifier of the order
    
    
      uInt64
      FROM_WALLET_ID
      1300
      
    
    
      uInt64
      TO_WALLET_ID
      1302
      
    
    
      uInt64
      FEE_WALLET_ID
      1301
      
    
    
      uInt64
      CORRELATION_ID
      14522
      
    
    
      Double
      SIZE
      1.099998
      Cancellations should have SIZE=0
    
    
      Double
      PRICE
      5000.123698
      NaN for Market, Buy-Stop, Sell-Stop, and Trailing Stop
orders. Represents a limit price.
    
    
      Double
      STOP_VAL
      NaN
      NaN for Market and Limit orders. Represents a stop price.
    
    
      uInt32
      OMDSEQ
      1500
      
    
    
      Int32
      ACCOUNT_GROUP_ID
      4
      
    
    
      Int32
      ACCOUNT_ID
      145
      
    
    
      Int32
      USER_ID
      145
      
    
    
      Int32
      TRD_PAIR_ID
      4
      
    
    
      Byte
      PAY_FROM_FEE_WALLET
      1
      Used only for taker orders, should be either 0 or 1.
    
    
      Byte
      BUY_SELL_FLAG
      0
      0 for BID/Buy, 1 for ASK/Sell
    
    
      Byte
      REASON
      215
      Used only for cancellations. Values from 200-255 are reserved
for users.
    
    
      String
      ORDER_TYPE
      "M"
      L - common limit order
LIOC - immediate-or-kill limit order
LFOK - fill-or-kill limit order
LS - stop limit order
LST - limit trailing stop
LP - take profit limit order
LM - market maker limit order
MST - market trailing stop
M - common market order
MS - stop market order
MP - take profit market order
C - cancel order
      
    
  

Wallet input
From this input EP gets wallet deposit, withdrawal, transfer and
freeze messages. EP implicitly creates wallets after getting:


  wallet deposit messages
  wallet transfer messages (for destination wallet only)
  wallet freeze messages
  orders (for destination wallet only)

In other cases EP will complain about using non-existing wallet. EP
removes wallets when both available and reserved balances are smaller
than 10e-8, and neither of
trading_frozen and withdrawal_frozen flags are set.

Wallet deposit tick schema

  
    
      Field type
      Field name
      Example value
      Description
    
    
      uInt32
      OMDSEQ
      2000
      
    
    
      uInt64
      CORRELATION_ID
      14522
      
    
    
      Int32
      ACCOUNT_ID
      200
      
    
    
      String
      WALLET_MSG_TYPE
      "DPST"
      
    
    
      uInt64
      WALLET_ID
      5000
      
    
    
      uInt64
      FROM_WALLET_ID
      0
      Not used, value doesn't matter
    
    
      Double
      SIZE
      100000
      
    
    
      Byte
      WITHDRAWAL_FROZEN
      0
      Not used, value doesn't matter
    
    
      Byte
      TRADING_FROZEN
      0
      Not used, value doesn't matter
    
  

Wallet withdrawals tick schema

  
    
      Field type
      Field name
      Example value
      Description
    
    
      uInt32
      OMDSEQ
      2000
      
    
    
      uInt64
      CORRELATION_ID
      14522
      
    
    
      Int32
      ACCOUNT_ID
      200
      
    
    
      String
      WALLET_MSG_TYPE
      "WTHD"
      
    
    
      uInt64
      WALLET_ID
      5000
      
    
    
      uInt64
      FROM_WALLET_ID
      0
      Not used, value doesn't matter
    
    
      Double
      SIZE
      100000
      
    
    
      Byte
      WITHDRAWAL_FROZEN
      0
      Not used, value doesn't matter
    
    
      Byte
      TRADING_FROZEN
      0
      Not used, value doesn't matter
    
  

Wallet transfer tick schema

  
    
      Field type
      Field name
      Example value
      Description
    
    
      uInt32
      OMDSEQ
      2000
      
    
    
      uInt64
      CORRELATION_ID
      14522
      
    
    
      Int32
      ACCOUNT_ID
      200
      
    
    
      String
      WALLET_MSG_TYPE
      "TRNS"
      
    
    
      uInt64
      WALLET_ID
      5000
      
    
    
      uInt64
      FROM_WALLET_ID
      5002
      
    
    
      Double
      SIZE
      100000
      
    
    
      Byte
      WITHDRAWAL_FROZEN
      0
      Not used, value doesn't matter
    
    
      Byte
      TRADING_FROZEN
      0
      Not used, value doesn't matter
    
  

Wallet freeze tick schema

  
    
      Field type
      Field name
      Example value
      Description
    
    
      uInt32
      OMDSEQ
      2000
      
    
    
      uInt64
      CORRELATION_ID
      14522
      
    
    
      Int32
      ACCOUNT_ID
      200
      
    
    
      String
      WALLET_MSG_TYPE
      "FRZ"
      
    
    
      uInt64
      WALLET_ID
      5000
      
    
    
      uInt64
      FROM_WALLET_ID
      0
      Not used, value doesn't matter
    
    
      Double
      SIZE
      0
      Not used, value doesn't matter
    
    
      Byte
      WITHDRAWAL_FROZEN
      1
      When this flag is set, withdrawals for a wallet aren't allowed
    
    
      Byte
      TRADING_FROZEN
      0
      When this flag is set, orders and cancellations will be
rejected
    
  

Fee input
From this input EP gets fees, in percentages. Tick schema is the
following:


  
    
      Field type
      Field name
      Example value
      Description
    
    
      uInt32
      OMDSEQ
      2000
      
    
    
      Int32
      ACCOUNT_ID
      125
      
    
    
      Double
      TAKER_FEE
      0.2
      For taker orders
    
    
      Double
      MAKER_FEE
      -0.1
      For maker orders
    
  

Freeze input
From this input EP gets freeze and unfreeze messages for trading
pair. EP expects to get trading pair freeze messages under one of
symbol names that are processed by a thread which is executing a given
instance of EP. Unfreeze of the same trading pair should come under the
same symbol name. If EP is processed by multiple threads, a freeze and
corresponding unfreeze message should be reported to one symbol
processed by a given thread, for all threads.

When trading is frozen (TRADING_FROZEN=1),
EP will reject all orders/cancellations for that trading pair, until it
gets unfreeze message for that treading pair (TRADING_FROZEN=0).
When matching is frozen (MATCHING_ENGINE_FROZEN=1),
OM::MATCHING_ENGINE_SIMPLE will add
orders to the book, but will not do matching, until it receives
unfreeze message for that trading pair (MATCHING_ENGINE_FROZEN=0).
EP processes TRADING_FROZEN flag,
and propagates MATCHING_ENGINE_FROZEN
flag to OM::MATCHING_ENGINE_SIMPLE
as an order (ORDER_TYPE=F to freeze
and ORDER_TYPE=U to unfreeze).

Tick schema is the following:


  
    
      Field type
      Field name
      Example value
      Description
    
    
      uInt32
      OMDSEQ
      2000
      
    
    
      Int32
      TRD_PAIR_ID
      2
      
    
    
      Byte
      TRADING_FROZEN
      1
      
    
    
      Byte
      MATCHING_ENGINE_FROZEN
      0
      
    
  

Python
class name:&nbsp;Om_walletManager

Output: EP has 6 outputs.

Orders output
Orders output has the same fields as input order, but for rejected
orders/cancellations has additional RECORD_TYPE='r'
field.

Wallet output
For each wallet change EP propagates a wallet update tick. Those
updates should be stored under WLTU tick type, for recovery purpose.
State key for wallet updates is WALLET_ID.
State keys are described in datamodeling.html (section: Non-order book
time series with state keys).

Wallet updates have the following fields:


  
    
      Field type
      Field name
      Example value
      Description
    
    
      Int32
      ACCOUNT_GROUP_ID
      3
      
    
    
      Int32
      ACCOUNT_ID
      240
      
    
    
      uInt64
      CORRELATION_ID
      20040
      
    
    
      Uint64
      WALLET_ID
      1550
      
    
    
      Double
      AVAILABLE_SIZE
      12000.000000
      
    
    
      Double
      RESERVED_SIZE
      4.123988
      
    
    
      Byte
      WITHDRAWAL_FROZEN
      0
      
    
    
      Byte
      TRADING_FROZEN
      0
      
    
    
      Byte
      UPDATE_CAUSE
      1
      
    
  

UPDATE_CAUSE field can have the
following values:


  1 - deposit
  2 - withdrawal
  3 - transfer from
  4 - transfer to
  5 - freeze
  6 - reservation
  7 - rejection
  8 - cancellation
  9 - fill
  10 - fee

Confirmations output
If wallet transfer or wallet withdrawal messages fail (either
becuase the wallet doesn't exist, or withdrawals for that wallet are
frozen, or there isn't sufficient money to withdraw), EP generates a
confirmation tick. Confirmation ticks should be stored under CNFR tick
type, for recovery purpose. Tick schema is the following:


  
    
      Field type
      Field name
      Example value
      Description
    
    
      Int32
      ACCOUNT_GROUP_ID
      2
      
    
    
      Int32
      ACCOUNT_ID
      1220
      
    
    
      uInt32
      OMDSEQ
      15600
      
    
    
      uInt64
      CORRELATION_ID
      1222500
      
    
  

Fills output
After processing a fill from Matching Engine, EP generates a fill
tick with the following fields. Fill ticks should be stored under FILL
tick type, for recovery purpose.


  
    
      Field type
      Field name
      Example value
      Description
    
    
      Int32
      ACCOUNT_GROUP_ID
      3
      
    
    
      Int32
      ACCOUNT_ID
      2200
      
    
    
      Int32
      USER_ID
      2600
      
    
    
      uInt64
      FROM_WALLET_ID
      51000
      
    
    
      uInt64
      TO_WALLET_ID
      51002
      
    
    
      uInt64
      FEE_WALLET_ID
      51001
      
    
    
      Int32
      TRD_PAIR_ID
      6
      
    
    
      Byte
      BUY_SELL_FLAG
      0
      
    
    
      Double
      SIZE
      0.999896
      Executed size
    
    
      Double
      ORIG_SIZE
      1.000896
      Original size of the order
    
    
      Double
      REMAINING_SIZE
      0.001000
      Remaining size of the order
    
    
      Double
      PRICE
      5000.899965
      Executed price
    
    
      Double
      STOP_VAL
      NaN
      Represents an absolute or relative stop price
    
    
      Double
      VWAP
      NaN
      Volume weighted average price of the particular fill
    
    
      Double
      ORIG_PRICE
      5000.899965
      Original order's price
    
    
      String
      ORDER_TYPE
      "M"
      
    
    
      uInt64
      ORDER_ID
      563320
      
    
    
      uInt32
      OMDSEQ
      4500
      
    
    
      uInt64
      CORRELATION_ID
      1222500
      
    
    
      
    
    
      Byte
      PAY_FROM_FEE_WALLET
      1
      
    
    
      Double
      FROM_WALLET_AVAILABLE_SIZE
      4000.666000
      Available size of market currency wallet after applying the
fill.
    
    
      Double
      FROM_WALLET_RESERVED_SIZE
      0.563000
      Reserved size of market currency wallet after applying the
fill.
    
    
      Double
      TO_WALLET_AVAILABLE_SIZE
      10000.456329
      Available size of non-market currency wallet after applying
the fill.
    
    
      Double
      TO_WALLET_RESERVED_SIZE
      500.123998
      Reserved size of non-market currency wallet after applying
the fill.
    
    
      Double
      FEE_WALLET_PAYMENT
      0.004599
      Fee taken from fee wallet (negative when EP adds fee to the
fee wallet), 0 if fee was taken to destination wallet
    
    
      Double
      DEST_WALLET_FEE
      0
      Fee taken from destination wallet, 0 if fee was taken from
fee wallet
    
    
      uInt64
      COUNTER_PARTY_ORDER_ID
      1256
      
    
    
      Byte
      MARKET_TAKER_FLAG
      0
      
    
    
      msec64
      CURRENT_TIME
      1540492000000
      
    
    
      nsec64
      ORIG_TIME
      1540491900000
      Original order's timestamp
    
  

Rejection output
For rejected orders (by both OM::WALLET_MANAGER
and OM::MATCHING_ENGINE_SIMPLE EPs)
EP generates a tick with input ORD tick schema + ORIG_TIME fields. Ticks from this output
should be stored under RJC tick type, for recovery purpose. For orders
rejected by OM::WALLET_MANAGER REASON field will have one of the
following values.


  1 - source wallet wasn't found
  2 - for buy limit orders SIZE
* PRICE &lt; 10e-8
  3 - wallet doesn't have enough balance
  4 - trading for source wallet is frozen
  5 - trading for destination wallet is frozen
  6 - trading for trading pair id is frozen
  7 - withdrawals for source wallet are frozen
  8 - trading pair id is missing from TRD_PAIR_FEE_MAP_OTQ
query

Cancellation output
For successfully canceled orders EP generates a tick with input ORD
tick schema + ORIG_TIME + CANCELLED_SIZE + VWAP fields. If WALLET_MANAGER.SUBMIT_FULL_CNC_TICKS
configuration parameter is set to false,
CNC ticks will have only ORDER_ID, ORIG_TIME and VWAP
fields. Ticks from this output should be stored under CNC tick type,
for recovery purpose.

Fee and freeze outputs
Fee and freeze outputs has the same information as input fee and
freeze messages, those outputs are only for recovery purposes. Fee
ticks should be stored under FEE tick type, freeze ticks should be
stored under FRZ tick type, for recovery purpose.

Parameters:


  USER_STATE_DB_NAME - used for recovery,
should point to the database where EP output was written before the
restart
  MAX_ACCOUNT_GROUP_ID - maximum account
group id
  MAX_ACCOUNT_ID - maximum account id
  MAX_TRADING_PAIR_ID - maximum value of
trading pair id
  FILL_PROCESSING_DELAY - specifies the
delay for feedback events in milliseconds
  MAX_INACTIVITY_INTERVAL - maximum
allowed internal for input partition inactivity in seconds. Default: 30
  MAX_INITIALIZATION_DAYS - the number
of days to go back in search of the full state of a time series.
Default: 2
  TRD_PAIR_FEE_MAP_OTQ - otq query, which
contains mappings between trading pair ids and fee trading pair ids.
This is a mandatory option. Default: ""
  TRD_PAIR_FEE_MAP_OTQ_PARAMS -
otq parameters for fee trading pair map otq query. Default: ""
  DFLT_FEE_RATE_TRD_PAIR_MAP_OTQ
- otq query, which contains default currency rates between market
currencies and fee currency. This is a mandatory option. Default: ""
  DFLT_FEE_RATE_TRD_PAIR_MAP_OTQ_PARAMS
- otq parameters for default fee rate trading pair map otq query.
Default: ""
  ORDER_TICK_SCHEMA_FILE - specifies
the file which contains order/cancellation tick schema in the format type field_name1 = default_value,.... This
is a mandatory parameter. If file isn't found in the current directory,
it's searched in the CSV_FILE_PATH.

Configuration parameters:


  WALLET_MANAGER.CONFIGURATION_FILE - path of the wallet manager
configuration file
  WALLET_MANAGER.REPORT_MESSAGE_RATE - enables message rate
reporting. Default: true
  WALLET_MANAGER.NUM_TICKS_BETWEEN_REPORTS - number of ticks
between two reports. Default: 500,000
  WALLET_MANAGER.MAX_INTERNAL_ERRORS_TO_SURVIVE - number of
internal errors to survive. Default: 0
  WALLET_MANAGER.MAX_ACCUMULATED_MESSAGE_COUNT - maximum number of
input ticks to accumulate. When accumulated messages' count exceeds
configured value, wallet manager will sleep until queue size gets
decreased. Default: 1,000,000
  WALLET_MANAGER.SLEEP_INTERVAL_WHEN_QUEUE_IS_FULL - sleep interval
in milliseconds to sleep when queue is full. Default: 100 msec
  WALLET_MANAGER.SUBMIT_FULL_CNC_TICKS - if set, EP will submit CNC
ticks with full tick schema. Default: true




	"""
	class Parameters:
		user_state_db_name = "USER_STATE_DB_NAME"
		max_account_group_id = "MAX_ACCOUNT_GROUP_ID"
		max_account_id = "MAX_ACCOUNT_ID"
		max_trading_pair_id = "MAX_TRADING_PAIR_ID"
		fill_processing_delay = "FILL_PROCESSING_DELAY"
		max_inactivity_interval = "MAX_INACTIVITY_INTERVAL"
		max_initialization_days = "MAX_INITIALIZATION_DAYS"
		trd_pair_fee_map_otq = "TRD_PAIR_FEE_MAP_OTQ"
		trd_pair_fee_map_otq_params = "TRD_PAIR_FEE_MAP_OTQ_PARAMS"
		dflt_fee_rate_trd_pair_map_otq = "DFLT_FEE_RATE_TRD_PAIR_MAP_OTQ"
		dflt_fee_rate_trd_pair_map_otq_params = "DFLT_FEE_RATE_TRD_PAIR_MAP_OTQ_PARAMS"
		order_tick_schema_file = "ORDER_TICK_SCHEMA_FILE"
		state_initialization_otq = "STATE_INITIALIZATION_OTQ"
		state_initialization_otq_params = "STATE_INITIALIZATION_OTQ_PARAMS"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["user_state_db_name", "max_account_group_id", "max_account_id", "max_trading_pair_id", "fill_processing_delay", "max_inactivity_interval", "max_initialization_days", "trd_pair_fee_map_otq", "trd_pair_fee_map_otq_params", "dflt_fee_rate_trd_pair_map_otq", "dflt_fee_rate_trd_pair_map_otq_params", "order_tick_schema_file", "state_initialization_otq", "state_initialization_otq_params"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["user_state_db_name", "_default_user_state_db_name", "max_account_group_id", "_default_max_account_group_id", "max_account_id", "_default_max_account_id", "max_trading_pair_id", "_default_max_trading_pair_id", "fill_processing_delay", "_default_fill_processing_delay", "max_inactivity_interval", "_default_max_inactivity_interval", "max_initialization_days", "_default_max_initialization_days", "trd_pair_fee_map_otq", "_default_trd_pair_fee_map_otq", "trd_pair_fee_map_otq_params", "_default_trd_pair_fee_map_otq_params", "dflt_fee_rate_trd_pair_map_otq", "_default_dflt_fee_rate_trd_pair_map_otq", "dflt_fee_rate_trd_pair_map_otq_params", "_default_dflt_fee_rate_trd_pair_map_otq_params", "order_tick_schema_file", "_default_order_tick_schema_file", "state_initialization_otq", "_default_state_initialization_otq", "state_initialization_otq_params", "_default_state_initialization_otq_params", "stack_info", "_used_strings"]

	def __init__(self, user_state_db_name="", max_account_group_id="", max_account_id="", max_trading_pair_id="", fill_processing_delay="", max_inactivity_interval="", max_initialization_days="", trd_pair_fee_map_otq="", trd_pair_fee_map_otq_params="", dflt_fee_rate_trd_pair_map_otq="", dflt_fee_rate_trd_pair_map_otq_params="", order_tick_schema_file="", state_initialization_otq="", state_initialization_otq_params=""):
		_graph_components.EpBase.__init__(self, "OM::WALLET_MANAGER")
		self._default_user_state_db_name = ""
		self.user_state_db_name = user_state_db_name
		self._default_max_account_group_id = ""
		self.max_account_group_id = max_account_group_id
		self._default_max_account_id = ""
		self.max_account_id = max_account_id
		self._default_max_trading_pair_id = ""
		self.max_trading_pair_id = max_trading_pair_id
		self._default_fill_processing_delay = ""
		self.fill_processing_delay = fill_processing_delay
		self._default_max_inactivity_interval = ""
		self.max_inactivity_interval = max_inactivity_interval
		self._default_max_initialization_days = ""
		self.max_initialization_days = max_initialization_days
		self._default_trd_pair_fee_map_otq = ""
		self.trd_pair_fee_map_otq = trd_pair_fee_map_otq
		self._default_trd_pair_fee_map_otq_params = ""
		self.trd_pair_fee_map_otq_params = trd_pair_fee_map_otq_params
		self._default_dflt_fee_rate_trd_pair_map_otq = ""
		self.dflt_fee_rate_trd_pair_map_otq = dflt_fee_rate_trd_pair_map_otq
		self._default_dflt_fee_rate_trd_pair_map_otq_params = ""
		self.dflt_fee_rate_trd_pair_map_otq_params = dflt_fee_rate_trd_pair_map_otq_params
		self._default_order_tick_schema_file = ""
		self.order_tick_schema_file = order_tick_schema_file
		self._default_state_initialization_otq = ""
		self.state_initialization_otq = state_initialization_otq
		self._default_state_initialization_otq_params = ""
		self.state_initialization_otq_params = state_initialization_otq_params
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_user_state_db_name(self, value):
		self.user_state_db_name = value
		return self

	def set_max_account_group_id(self, value):
		self.max_account_group_id = value
		return self

	def set_max_account_id(self, value):
		self.max_account_id = value
		return self

	def set_max_trading_pair_id(self, value):
		self.max_trading_pair_id = value
		return self

	def set_fill_processing_delay(self, value):
		self.fill_processing_delay = value
		return self

	def set_max_inactivity_interval(self, value):
		self.max_inactivity_interval = value
		return self

	def set_max_initialization_days(self, value):
		self.max_initialization_days = value
		return self

	def set_trd_pair_fee_map_otq(self, value):
		self.trd_pair_fee_map_otq = value
		return self

	def set_trd_pair_fee_map_otq_params(self, value):
		self.trd_pair_fee_map_otq_params = value
		return self

	def set_dflt_fee_rate_trd_pair_map_otq(self, value):
		self.dflt_fee_rate_trd_pair_map_otq = value
		return self

	def set_dflt_fee_rate_trd_pair_map_otq_params(self, value):
		self.dflt_fee_rate_trd_pair_map_otq_params = value
		return self

	def set_order_tick_schema_file(self, value):
		self.order_tick_schema_file = value
		return self

	def set_state_initialization_otq(self, value):
		self.state_initialization_otq = value
		return self

	def set_state_initialization_otq_params(self, value):
		self.state_initialization_otq_params = value
		return self

	@staticmethod
	def _get_name():
		return "OM::WALLET_MANAGER"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.user_state_db_name != "": 
			desc += "USER_STATE_DB_NAME=" + py_to_str(self.user_state_db_name) + ","
		if self.max_account_group_id != "": 
			desc += "MAX_ACCOUNT_GROUP_ID=" + py_to_str(self.max_account_group_id) + ","
		if self.max_account_id != "": 
			desc += "MAX_ACCOUNT_ID=" + py_to_str(self.max_account_id) + ","
		if self.max_trading_pair_id != "": 
			desc += "MAX_TRADING_PAIR_ID=" + py_to_str(self.max_trading_pair_id) + ","
		if self.fill_processing_delay != "": 
			desc += "FILL_PROCESSING_DELAY=" + py_to_str(self.fill_processing_delay) + ","
		if self.max_inactivity_interval != "": 
			desc += "MAX_INACTIVITY_INTERVAL=" + py_to_str(self.max_inactivity_interval) + ","
		if self.max_initialization_days != "": 
			desc += "MAX_INITIALIZATION_DAYS=" + py_to_str(self.max_initialization_days) + ","
		if self.trd_pair_fee_map_otq != "": 
			desc += "TRD_PAIR_FEE_MAP_OTQ=" + py_to_str(self.trd_pair_fee_map_otq) + ","
		if self.trd_pair_fee_map_otq_params != "": 
			desc += "TRD_PAIR_FEE_MAP_OTQ_PARAMS=" + py_to_str(self.trd_pair_fee_map_otq_params) + ","
		if self.dflt_fee_rate_trd_pair_map_otq != "": 
			desc += "DFLT_FEE_RATE_TRD_PAIR_MAP_OTQ=" + py_to_str(self.dflt_fee_rate_trd_pair_map_otq) + ","
		if self.dflt_fee_rate_trd_pair_map_otq_params != "": 
			desc += "DFLT_FEE_RATE_TRD_PAIR_MAP_OTQ_PARAMS=" + py_to_str(self.dflt_fee_rate_trd_pair_map_otq_params) + ","
		if self.order_tick_schema_file != "": 
			desc += "ORDER_TICK_SCHEMA_FILE=" + py_to_str(self.order_tick_schema_file) + ","
		if self.state_initialization_otq != "": 
			desc += "STATE_INITIALIZATION_OTQ=" + py_to_str(self.state_initialization_otq) + ","
		if self.state_initialization_otq_params != "": 
			desc += "STATE_INITIALIZATION_OTQ_PARAMS=" + py_to_str(self.state_initialization_otq_params) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class Om_walletManagerFeedback(_graph_components.EpBase):
	"""
		

OM/WALLET_MANAGER_FEEDBACK

Type: Other

Description: Passes ticks back to OM/WALLET_MANAGER EP.

Python
class name:
Om_walletManagerFeedback

Input: Matched orders and cancellations.

EP works directly with OM/WALLET_MANAGER
EP, propagating its input ticks to OM/WALLET_MANAGER using an internal
queue. The explicit connection between the two EPs does not exist to
prevent cycles in the query graph.




	"""
	class Parameters:
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = []
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["stack_info", "_used_strings"]

	def __init__(self):
		_graph_components.EpBase.__init__(self, "OM::WALLET_MANAGER_FEEDBACK")
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	@staticmethod
	def _get_name():
		return "OM::WALLET_MANAGER_FEEDBACK"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class OmStrategyInstanceManager(_graph_components.EpBase):
	"""
		

OM/STRATEGY_INSTANCE_MANAGER

Type: Other

Description: This event processor is a part of the order management framework. It
controls trading strategy instance execution processes.

Python
class name:&nbsp;OmStrategyInstanceManager

Input: None

Output: The underlying strategy output that has the OM_OUTPUT
tag, if applicable.

Symbols list: The query that contains the OM/STRATEGY_INSTANCE_MANAGER
EP must run against the following template database:

&lt;db ID="BASE_STRATEGY_DB" SYMBOLOGY="OM" &gt;  &lt;FEED TYPE="db_view"&gt;    &lt;options view_support="1" pass_symbol_params_in_subscription="true" force_book_inserter_checking="true" /&gt;  &lt;/FEED&gt;  &lt;LOCATIONS&gt;    &lt;location access_method="file" location="dummy" start_time="20050101000000" end_time="20200101000000" /&gt;  &lt;/LOCATIONS&gt;&lt;/db&gt;
You can only change the database ID. A typical query symbol looks
like this: BASE_STRATEGY_DB::GOOGL.

The tick type is not critical: any tick type you specify should work.

Symbol parameters: All symbol parameters that we specify here
are passed to the underlying strategy instance query as symbol
parameters.

Parameters:


  STRATEGY_INSTANCE_NAME
(string)
    The strategy instance name, that is stored in database, created
via the OM/STRATEGY_DB_MANAGER
EP.

  
  ACTION (string)
    The action to be performed with the selected strategy instance.
Supported values are:

    
      SUBSCRIBE_SYMBOLS:
Starts the strategy instance for a given set of pure symbols if the
strategy is not running; otherwise, if the strategy is already up, it
subscribes the set of pure symbols to the running strategy. The start
and end times of the query are not critical for this action, since we
will execute it for 2 seconds, anyway.
No output is expected for this action.
      VIEW_SYMBOLS: With
this action you can view the output ticks of your strategy instance
query.
Note, that before running a query with VIEW_SYMBOLS,
you have to run a query for the same set of symbols with the SUBSCRIBE_SYMBOLS action. You will see the
results of your strategy instance in a period specified via the query
start/end; hence, as the strategy instance query generates output ticks
in the realtime mode, your query also must be a CEP.
It is also important to note that when the top-level (that is, the OM/STRATEGY_INSTANCE_MANAGER)
query ends or the user manually stops it, the underlying strategy
instance remains up and running. That is because this action does not
make any changes to the running strategy: it merely shows the result
ticks of the strategy instance query.
      UNSUBSCRIBE_SYMBOLS:
Unsubscribes the specified symbols from the strategy instance query.
Like in the case of the SUBSCRIBE_SYMBOLS
action, the start and end times of the query are not critical for this
action, since we will execute it for 2 seconds anyway.
No output is expected for this action.
      STOP_STRATEGY: Stops
the strategy instance query; that is, it unsubscribes from all
subscribed symbols. For this action, the start and end times of the
query are not critical for this action, as well.
No output is expected for this action.
    
  




	"""
	class Parameters:
		strategy_instance_name = "STRATEGY_INSTANCE_NAME"
		action = "ACTION"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["strategy_instance_name", "action"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["strategy_instance_name", "_default_strategy_instance_name", "action", "_default_action", "stack_info", "_used_strings"]

	class Action:
		EMPTY = ""
		STOP_STRATEGY = "STOP_STRATEGY"
		SUBSCRIBE_SYMBOLS = "SUBSCRIBE_SYMBOLS"
		UNSUBSCRIBE_SYMBOLS = "UNSUBSCRIBE_SYMBOLS"
		VIEW_SYMBOLS = "VIEW_SYMBOLS"

	def __init__(self, strategy_instance_name="", action=Action.EMPTY):
		_graph_components.EpBase.__init__(self, "OM/STRATEGY_INSTANCE_MANAGER")
		self._default_strategy_instance_name = ""
		self.strategy_instance_name = strategy_instance_name
		self._default_action = type(self).Action.EMPTY
		self.action = action
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_strategy_instance_name(self, value):
		self.strategy_instance_name = value
		return self

	def set_action(self, value):
		self.action = value
		return self

	@staticmethod
	def _get_name():
		return "OM/STRATEGY_INSTANCE_MANAGER"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.strategy_instance_name != "": 
			desc += "STRATEGY_INSTANCE_NAME=" + py_to_str(self.strategy_instance_name) + ","
		if self.action != self.Action.EMPTY: 
			desc += "ACTION=" + py_to_str(self.action) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class OmStrategyDbManager(_graph_components.EpBase):
	"""
		

OM/STRATEGY_DB_MANAGER

Type: Other

Description: This event processor is a part of the order management framework. It
manages the database that is designed to store the
Order-Management-System-related strategies, the users, and the
relationships among them. The following UML diagram describes the
underlying database:



Pay attention to the strategies and strategy_instances
databases:


  In the strategies table, we specify strategy OTQ files
  In the strategy_instances table, we enrich the strategy
query with specific OTQ parameters

The relationships between user accounts and strategies are defined
in the roles_strategy_groups table.
Users are grouped in the roles table, and strategies are
grouped in the strategy_groups table; permissions are granted
based on what kind of permission does a role have on a strategy group.
There are 3 possible permissions:


  View permission
Users who have this permission can only run queries that view the
results of a running strategy.
  Execute permission
Besides the View permission privileges, users who have this permission
can also start and stop strategies.
  Edit permission
Being the highest permission, besides the aforementioned privileges,
users who have it can also edit strategies, add or delete new users,
and fully manage this database. This permission must be granted only to
a small group of administrators.

There is a special way to add the first admin user with the Edit
permission. For that, you have to initialize the OM.ROOT_USER ONE_TICK_CONFIG variable.
However, once you initialize it, you should specify the
already-existing username in OM.ROOT_USER.

The username in the database must match a username of
the computer account, as we automatically pick it to identify a user.

Python
class name:
OmStrategyDbManager

Input: None

Output: None

Symbols list: Specifying the database against which the query
that contains OM/STRATEGY_DB_MANAGER EP should run is not
critical, as we will create the underlying database under the OM.ROOT folder, anyway. Typically, we are
using LOCAL::. Specifying the tick
type is not critical as well, and you can specify anything for it.

Parameters:


  ACTION (string)
    The action to be performed. Those are the supported actions ADD_ROLE, ADD_STRATEGY, ADD_STRATEGY_GROUP, ADD_STRATEGY_INSTANCE, ADD_USER, ATTACH_STRATEGY_GROUP_TO_ROLE, DETACH_STRATEGY_GROUP_FROM_ROLE, REMOVE_ROLE, REMOVE_STRATEGY, REMOVE_STRATEGY_GROUP, REMOVE_STRATEGY_INSTANCE, and REMOVE_USER
You have to fill the needed fields according to the UML
diagram to perform the action. For example if you want to ADD_USER
you should specify USERNAME and ROLE, or if you want to
    ATTACH_STRATEGY_GROUP_TO_ROLE you should specify also STRATEGY_GROUP,
    ROLE and PERMISSION fields

  
  ROLE (string)
    Role name.

  
  STRATEGY_GROUP (string)
    Strategy group name.

  
  USERNAME (string)
    Accounts table username.

  
  STRATEGY (string)
    Strategy name.

  
  STRATEGY_INSTANCE (string)
    Strategy instance name.

  
  PERMISSION (integer)
    

    
0 - for no permission
1 - for view permission
2 - for execute permission
3 - for edit permission
    
The edit permission should be given to admin group members
usually.&nbsp;
    Default:0
    

  


  STRATEGY_PATH (string)
    Strategy otq query path that is visible from our SQL DBs.

  
  STRATEGY_INSTANCE_PARAMS
(string)
    Comma separated list of strategy otq parameters.

  




	"""
	class Parameters:
		action = "ACTION"
		role = "ROLE"
		strategy_group = "STRATEGY_GROUP"
		username = "USERNAME"
		strategy = "STRATEGY"
		strategy_instance = "STRATEGY_INSTANCE"
		permission = "PERMISSION"
		strategy_path = "STRATEGY_PATH"
		strategy_instance_params = "STRATEGY_INSTANCE_PARAMS"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["action", "role", "strategy_group", "username", "strategy", "strategy_instance", "permission", "strategy_path", "strategy_instance_params"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["action", "_default_action", "role", "_default_role", "strategy_group", "_default_strategy_group", "username", "_default_username", "strategy", "_default_strategy", "strategy_instance", "_default_strategy_instance", "permission", "_default_permission", "strategy_path", "_default_strategy_path", "strategy_instance_params", "_default_strategy_instance_params", "stack_info", "_used_strings"]

	class Action:
		EMPTY = ""
		ADD_ROLE = "ADD_ROLE"
		ADD_STRATEGY = "ADD_STRATEGY"
		ADD_STRATEGY_GROUP = "ADD_STRATEGY_GROUP"
		ADD_STRATEGY_INSTANCE = "ADD_STRATEGY_INSTANCE"
		ADD_USER = "ADD_USER"
		ATTACH_STRATEGY_GROUP_TO_ROLE = "ATTACH_STRATEGY_GROUP_TO_ROLE"
		DETACH_STRATEGY_GROUP_FROM_ROLE = "DETACH_STRATEGY_GROUP_FROM_ROLE"
		REMOVE_ROLE = "REMOVE_ROLE"
		REMOVE_STRATEGY = "REMOVE_STRATEGY"
		REMOVE_STRATEGY_GROUP = "REMOVE_STRATEGY_GROUP"
		REMOVE_STRATEGY_INSTANCE = "REMOVE_STRATEGY_INSTANCE"
		REMOVE_USER = "REMOVE_USER"

	class Permission:
		EMPTY = ""
		EDIT = "EDIT"
		EXECUTE = "EXECUTE"

	def __init__(self, action=Action.EMPTY, role="", strategy_group="", username="", strategy="", strategy_instance="", permission=Permission.EMPTY, strategy_path="", strategy_instance_params=""):
		_graph_components.EpBase.__init__(self, "OM/STRATEGY_DB_MANAGER")
		self._default_action = type(self).Action.EMPTY
		self.action = action
		self._default_role = ""
		self.role = role
		self._default_strategy_group = ""
		self.strategy_group = strategy_group
		self._default_username = ""
		self.username = username
		self._default_strategy = ""
		self.strategy = strategy
		self._default_strategy_instance = ""
		self.strategy_instance = strategy_instance
		self._default_permission = type(self).Permission.EMPTY
		self.permission = permission
		self._default_strategy_path = ""
		self.strategy_path = strategy_path
		self._default_strategy_instance_params = ""
		self.strategy_instance_params = strategy_instance_params
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_action(self, value):
		self.action = value
		return self

	def set_role(self, value):
		self.role = value
		return self

	def set_strategy_group(self, value):
		self.strategy_group = value
		return self

	def set_username(self, value):
		self.username = value
		return self

	def set_strategy(self, value):
		self.strategy = value
		return self

	def set_strategy_instance(self, value):
		self.strategy_instance = value
		return self

	def set_permission(self, value):
		self.permission = value
		return self

	def set_strategy_path(self, value):
		self.strategy_path = value
		return self

	def set_strategy_instance_params(self, value):
		self.strategy_instance_params = value
		return self

	@staticmethod
	def _get_name():
		return "OM/STRATEGY_DB_MANAGER"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.action != self.Action.EMPTY: 
			desc += "ACTION=" + py_to_str(self.action) + ","
		if self.role != "": 
			desc += "ROLE=" + py_to_str(self.role) + ","
		if self.strategy_group != "": 
			desc += "STRATEGY_GROUP=" + py_to_str(self.strategy_group) + ","
		if self.username != "": 
			desc += "USERNAME=" + py_to_str(self.username) + ","
		if self.strategy != "": 
			desc += "STRATEGY=" + py_to_str(self.strategy) + ","
		if self.strategy_instance != "": 
			desc += "STRATEGY_INSTANCE=" + py_to_str(self.strategy_instance) + ","
		if self.permission != self.Permission.EMPTY: 
			desc += "PERMISSION=" + py_to_str(self.permission) + ","
		if self.strategy_path != "": 
			desc += "STRATEGY_PATH=" + py_to_str(self.strategy_path) + ","
		if self.strategy_instance_params != "": 
			desc += "STRATEGY_INSTANCE_PARAMS=" + py_to_str(self.strategy_instance_params) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class OmOrderProcessor(_graph_components.EpBase):
	"""
		

OM/ORDER_PROCESSOR

Type: Other

Description: Used to place orders in trading strategies in
both real-time (to be released) and back-testing modes. In real-time
mode it sends orders to the market. In back-testing mode it sends
orders to the exchange simulator or to a child (execution) strategy.

Python
class name:&nbsp;OmOrderProcessor

Input: Order action ticks

Output: The original tick plus CLIENT_REF1 field if it was
not present in the original tick. This field should be unique.




	"""
	class Parameters:
		instance_name = "INSTANCE_NAME"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["instance_name"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["instance_name", "_default_instance_name", "stack_info", "_used_strings"]

	def __init__(self, instance_name=""):
		_graph_components.EpBase.__init__(self, "OM/ORDER_PROCESSOR")
		self._default_instance_name = ""
		self.instance_name = instance_name
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_instance_name(self, value):
		self.instance_name = value
		return self

	@staticmethod
	def _get_name():
		return "OM/ORDER_PROCESSOR"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.instance_name != "": 
			desc += "INSTANCE_NAME=" + py_to_str(self.instance_name) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class OmStatistics(_graph_components.EpBase):
	"""
		

OM/STATISTICS

Type: Other

Description: Allows computing statistics values based on its
input ticks, using the corresponding compute_statistics.otq file.
Computed statistics values are stored in memory and can be accessed by
the  GET_STATISTICS 
function. See back-testing document
for more details.

Python
class name:&nbsp;OmStatistics

Input: ORDER_UPD, TRADE_UPD, and other ticks that can be used
by the provided statistics computer OTQ file to compute the statistics.

Output: None.

Parameters:


  DEFAULT_STATISTICS_COMPUTER_OTQ_PARAMS
(string)
    Parameters for default statistics computation OTQ file. The
default statistics computer OTQ file should reside in $OM_HOME/compute_statistics.otq.
Default: empty

  
  CUSTOM_STATISTICS_COMPUTER_OTQ
(string)
    Custom statistics computation OTQ file to compute additional
statistics not computed by default.
Default: empty

  
  CUSTOM_STATISTICS_COMPUTER_OTQ_PARAMS
(string)
    Parameters for custom statistics computation OTQ file.
Default: empty

  
  COMPUTE_DEFAULT_STATISTICS
(Boolean)
    Compute default statistics. The parameter is relevant only if CUSTOM_STATISTICS_COMPUTER_OTQ
is given.
Default: false

  
  STATISTICS_COMPUTATION_INTERVAL
(Seconds)
    Time interval for computing statistics values.
Default: 1

  

Examples:

OM/STATISTICS()
Compute default statistics.




	"""
	class Parameters:
		propagate = "PROPAGATE"
		default_statistics_computer_otq_params = "DEFAULT_STATISTICS_COMPUTER_OTQ_PARAMS"
		custom_statistics_computer_otq = "CUSTOM_STATISTICS_COMPUTER_OTQ"
		custom_statistics_computer_otq_params = "CUSTOM_STATISTICS_COMPUTER_OTQ_PARAMS"
		statistics_computation_interval = "STATISTICS_COMPUTATION_INTERVAL"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["propagate", "default_statistics_computer_otq_params", "custom_statistics_computer_otq", "custom_statistics_computer_otq_params", "statistics_computation_interval"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["propagate", "_default_propagate", "default_statistics_computer_otq_params", "_default_default_statistics_computer_otq_params", "custom_statistics_computer_otq", "_default_custom_statistics_computer_otq", "custom_statistics_computer_otq_params", "_default_custom_statistics_computer_otq_params", "statistics_computation_interval", "_default_statistics_computation_interval", "stack_info", "_used_strings"]

	def __init__(self, propagate="", default_statistics_computer_otq_params="", custom_statistics_computer_otq="", custom_statistics_computer_otq_params="", statistics_computation_interval=""):
		_graph_components.EpBase.__init__(self, "OM/STATISTICS")
		self._default_propagate = ""
		self.propagate = propagate
		self._default_default_statistics_computer_otq_params = ""
		self.default_statistics_computer_otq_params = default_statistics_computer_otq_params
		self._default_custom_statistics_computer_otq = ""
		self.custom_statistics_computer_otq = custom_statistics_computer_otq
		self._default_custom_statistics_computer_otq_params = ""
		self.custom_statistics_computer_otq_params = custom_statistics_computer_otq_params
		self._default_statistics_computation_interval = ""
		self.statistics_computation_interval = statistics_computation_interval
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_propagate(self, value):
		self.propagate = value
		return self

	def set_default_statistics_computer_otq_params(self, value):
		self.default_statistics_computer_otq_params = value
		return self

	def set_custom_statistics_computer_otq(self, value):
		self.custom_statistics_computer_otq = value
		return self

	def set_custom_statistics_computer_otq_params(self, value):
		self.custom_statistics_computer_otq_params = value
		return self

	def set_statistics_computation_interval(self, value):
		self.statistics_computation_interval = value
		return self

	@staticmethod
	def _get_name():
		return "OM/STATISTICS"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.propagate != "": 
			desc += "PROPAGATE=" + py_to_str(self.propagate) + ","
		if self.default_statistics_computer_otq_params != "": 
			desc += "DEFAULT_STATISTICS_COMPUTER_OTQ_PARAMS=" + py_to_str(self.default_statistics_computer_otq_params) + ","
		if self.custom_statistics_computer_otq != "": 
			desc += "CUSTOM_STATISTICS_COMPUTER_OTQ=" + py_to_str(self.custom_statistics_computer_otq) + ","
		if self.custom_statistics_computer_otq_params != "": 
			desc += "CUSTOM_STATISTICS_COMPUTER_OTQ_PARAMS=" + py_to_str(self.custom_statistics_computer_otq_params) + ","
		if self.statistics_computation_interval != "": 
			desc += "STATISTICS_COMPUTATION_INTERVAL=" + py_to_str(self.statistics_computation_interval) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class OmRetrieveOrderData(_graph_components.EpBase):
	"""
		

OM/RETRIEVE_ORDER_DATA

Type: Other

Description: Allows to access order data (TRADE_UPD,
ORDER_UPD and other tick types) while back-testing statistics
computation by order symbol (OM/BACKTESTING_MANAGER(COMPUTE_STATISTICS_BY=BY_ORDER_SYMBOL)).

Python
class name:&nbsp;OmRetrieveOrderData

Input: None

Output: A time series of ticks.




	"""
	class Parameters:
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = []
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["stack_info", "_used_strings"]

	def __init__(self):
		_graph_components.EpBase.__init__(self, "OM/RETRIEVE_ORDER_DATA")
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	@staticmethod
	def _get_name():
		return "OM/RETRIEVE_ORDER_DATA"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class MrRunMapTask(_graph_components.EpBase):
	"""
		
	"""
	class Parameters:
		working_dir = "WORKING_DIR"
		split_id = "SPLIT_ID"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["working_dir", "split_id"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["working_dir", "_default_working_dir", "split_id", "_default_split_id", "stack_info", "_used_strings"]

	def __init__(self, working_dir="", split_id=""):
		_graph_components.EpBase.__init__(self, "MR/RUN_MAP_TASK")
		self._default_working_dir = ""
		self.working_dir = working_dir
		self._default_split_id = ""
		self.split_id = split_id
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_working_dir(self, value):
		self.working_dir = value
		return self

	def set_split_id(self, value):
		self.split_id = value
		return self

	@staticmethod
	def _get_name():
		return "MR/RUN_MAP_TASK"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.working_dir != "": 
			desc += "WORKING_DIR=" + py_to_str(self.working_dir) + ","
		if self.split_id != "": 
			desc += "SPLIT_ID=" + py_to_str(self.split_id) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class Omd_writeToAmqp(_graph_components.EpBase):
	"""
		

OMD::WRITE_TO_AMQP

Type: Output Adapter

Description: Publishes incoming ticks to AMQP broker. To use
this event processor you should set the LOAD_AMQP configuration
variable by adding it to the main configuration file as follows:

LOAD_AMQP=true

Python
class name:&nbsp;Omd_writeToAmqp

Input: A time series of ticks

Output: A time series of ticks or none

Parameters:


  PROPAGATE_TICKS (Boolean)
    Switches on the propagation of ticks. If set to true, ticks will
be propagated.
Default: true

  
  BROKER_ADDRESS (string)
    The address of the broker, represented as host:port.

  
  TARGET_ADDRESS (string)
    The target in AMQP broker to collect data from.

  
  AMQP_VERSION (string)
    The AMQP version. Valid values are: amqp0-10
and amqp1.0
Default: amqp1.0

  
  ASYNCH_PUBLISHING (Boolean)
    Specifies whether asynchronous publishing is enabled or not.
Default: true

  
  CONNECTION_OPTIONS (opt1=val1,opt2=val2,..,optK=valK})
    Represents a list of options for the connection to the broker.
To see a complete list of the available connection options, see the
AMQP Collector documentation.

  
  MSG_FORMAT (enum)
    Specifies the output format: NAME_VALUE_PAIRS
or STRING_VAL_OF_SINGLE_FIELD.
In case of NAME_VALUE_PAIRS, each
tick is converted into a sequence of name-value pairs.
In case of STRING_VAL_OF_SINGLE_FIELD
the single field of the tick will be converted into string.EP throws an
exception when gets ticks that consist of more than one field.
Default: NAME_VALUE_PAIRS

  
  AUTHENTICATION_METHOD (enum)
    Specifies the authentication method to be used to establish the
connection to the broker. Valid values are: none.
Default: none

  


	"""
	class Parameters:
		propagate_ticks = "PROPAGATE_TICKS"
		broker_address = "BROKER_ADDRESS"
		target_address = "TARGET_ADDRESS"
		amqp_version = "AMQP_VERSION"
		asynch_publishing = "ASYNCH_PUBLISHING"
		connection_options = "CONNECTION_OPTIONS"
		msg_format = "MSG_FORMAT"
		authentication_method = "AUTHENTICATION_METHOD"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["propagate_ticks", "broker_address", "target_address", "amqp_version", "asynch_publishing", "connection_options", "msg_format", "authentication_method"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["propagate_ticks", "_default_propagate_ticks", "broker_address", "_default_broker_address", "target_address", "_default_target_address", "amqp_version", "_default_amqp_version", "asynch_publishing", "_default_asynch_publishing", "connection_options", "_default_connection_options", "msg_format", "_default_msg_format", "authentication_method", "_default_authentication_method", "stack_info", "_used_strings"]

	class AmqpVersion:
		AMQP0_10 = "amqp0-10"
		AMQP1_0 = "amqp1.0"

	class MsgFormat:
		NAME_VALUE_PAIRS = "NAME_VALUE_PAIRS"
		STRING_VAL_OF_SINGLE_FIELD = "STRING_VAL_OF_SINGLE_FIELD"

	def __init__(self, propagate_ticks=True, broker_address="", target_address="", amqp_version=AmqpVersion.AMQP1_0, asynch_publishing=True, connection_options="", msg_format=MsgFormat.NAME_VALUE_PAIRS, authentication_method="NONE"):
		_graph_components.EpBase.__init__(self, "OMD::WRITE_TO_AMQP")
		self._default_propagate_ticks = True
		self.propagate_ticks = propagate_ticks
		self._default_broker_address = ""
		self.broker_address = broker_address
		self._default_target_address = ""
		self.target_address = target_address
		self._default_amqp_version = type(self).AmqpVersion.AMQP1_0
		self.amqp_version = amqp_version
		self._default_asynch_publishing = True
		self.asynch_publishing = asynch_publishing
		self._default_connection_options = ""
		self.connection_options = connection_options
		self._default_msg_format = type(self).MsgFormat.NAME_VALUE_PAIRS
		self.msg_format = msg_format
		self._default_authentication_method = "NONE"
		self.authentication_method = authentication_method
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_propagate_ticks(self, value):
		self.propagate_ticks = value
		return self

	def set_broker_address(self, value):
		self.broker_address = value
		return self

	def set_target_address(self, value):
		self.target_address = value
		return self

	def set_amqp_version(self, value):
		self.amqp_version = value
		return self

	def set_asynch_publishing(self, value):
		self.asynch_publishing = value
		return self

	def set_connection_options(self, value):
		self.connection_options = value
		return self

	def set_msg_format(self, value):
		self.msg_format = value
		return self

	def set_authentication_method(self, value):
		self.authentication_method = value
		return self

	@staticmethod
	def _get_name():
		return "OMD::WRITE_TO_AMQP"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.propagate_ticks != True: 
			desc += "PROPAGATE_TICKS=" + py_to_str(self.propagate_ticks) + ","
		if self.broker_address != "": 
			desc += "BROKER_ADDRESS=" + py_to_str(self.broker_address) + ","
		if self.target_address != "": 
			desc += "TARGET_ADDRESS=" + py_to_str(self.target_address) + ","
		if self.amqp_version != self.AmqpVersion.AMQP1_0: 
			desc += "AMQP_VERSION=" + py_to_str(self.amqp_version) + ","
		if self.asynch_publishing != True: 
			desc += "ASYNCH_PUBLISHING=" + py_to_str(self.asynch_publishing) + ","
		if self.connection_options != "": 
			desc += "CONNECTION_OPTIONS=" + py_to_str(self.connection_options) + ","
		if self.msg_format != self.MsgFormat.NAME_VALUE_PAIRS: 
			desc += "MSG_FORMAT=" + py_to_str(self.msg_format) + ","
		if self.authentication_method != "NONE": 
			desc += "AUTHENTICATION_METHOD=" + py_to_str(self.authentication_method) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class Omd_odbcQuery(_graph_components.EpBase):
	"""
		

OMD::ODBC_QUERY

Type: Other

Description: Queries an ODBC data source and propagates the
resulting set of tuples as a time series of ticks. A time series is
generated for every symbol of the graph/chain query and the symbol can
be passed down to SQL (see SQL parameter description below ). If an
attribute with the name TIMESTAMP is present in the input schema, it is
assumed to be of either SQL DATE or SQL TIMESTAMP type, in which case
output tick timestamps will carry values of that field and the field
itself will not be propagated. If such a field is absent, output tick
timestamps will be equal to query end time. Separate ODBC connection
will be created for each symbol in case of merging/joining results of
OMD::ODBC_QUERY with other streams of data or with another instance of
OMD::ODBC_QUERY EP. To avoid this and have a single connection per
thread, PRESORT EP can be placed before the EP where the results of
OMD::ODBC_QUERY (and possibly of other EPs placed after it, on the same
query branch), get merged/joined with some other query branch or get
merged/joined with other symbols.

To be able to run it, you need to have a ODBC driver manager
available on your box (comes with Windows OS; unixodbc package may need
to be installed on UNIX OS).

Also, the following entry needs to be added to the main
configuration file:

LOAD_ODBC_UDF=true

Python
class name:&nbsp;Omd_odbcQuery

Input: None

Output: A time series of ticks.

Parameters:


  DSN (string)
    The name of an ODBC DSN to query.

  
  CONNECTION_STRING (string)
    Connection string which should be used, to connect to ODBC
driver.
If this parameter is used; DSN, USER, PASSWORD parameters should be
empty and the AUTHENTICATION_TYPE
parameter should be empty, or set to its default value (i.e.,
Username/Password).

  
  AUTHENTICATION_TYPE (string)
    Possible values:

    
      System. In this case a trusted connection to a data
source will be used. If authentication is enabled in the server
configuration file (see the OneTick Installation and Administration
Guide for details), the client will be authenticated and the client's
credentials will be used for connection; otherwise, server credentials
will be used.
      Username/Password. The specified username and password
will be used to connect to the data source.
    
    Default: Username/Password

  
  USER (string)
    The connection user name (optional). Ignored if
AUTHENTICATION_TYPE is System.
Default: EMPTY

  
  PASSWORD (string)
    The connection password (optional). Ignored if
AUTHENTICATION_TYPE is System.
Default: EMPTY

  
  SQL (string)
    A query in SQL language, which may optionally contain parameter
placeholders. There are 3 types of placeholders: &lt;_SYMBOL_NAME&gt;,
&lt;_START_TIME&gt;, and &lt;_END_TIME&gt;. Whenever specified,
&lt;_SYMBOL_NAME&gt; will be replaced by the pure symbol name part of
the query (e.g., when querying 2 symbols TAQ::MSFT and TAQ::CSCO,
&lt;_SYMBOL_NAME&gt; will be replaced by MSFT and CSCO one after
another). &lt;_START_TIME&gt; and &lt;_END_TIME&gt; will be replaced
either by values of expression-parameters START_TIME_EXPR and EXD_TIME_EXPR
(see below), or by SQL timestamp literals (in the form of 'YYYY-MM-DD HH:MM:SS.sss TIMEZONE')
specifying query start and end times, respectively, with non-empty
values of START_TIME_EXPR and EXD_TIME_EXPR taking
precedence.

  
  SYMBOLOGY (string)
    SYMBOLOGY of symbol names (values of SYMBOL_NAME field) of the
data source. If specified, &lt;_SYMBOL_NAME&gt; placeholder in sql
query will be replaced with mapping of the symbol name of the query for
provided symbology. Specified symbology will be propagated to outputs
of this event processor.

  
  ALLOW_UNORDERED_TICKS (Boolean)
    If set to true, other EPs will be notified that OMD::ODBC_QUERY
will send ticks unordered by timestamp, so they can throw if they
require time-ordered ticks.

    If set to false, OMD::ODBC_QUERY will complain when ticks
unordered by timestamp arrive.
Default: false

  
  PRESERVE_UNICODE_FIELDS
(string)
    By default ODBCQuery queries all data as ANSI, ODBC Driver
Manager then tries to convert Unicode characters to ANSI. This
parameter is a comma-separated list of fields, which have Unicode types
in the data source and will be propagated without conversion.
UNICODE_CHAR_TYPE field property will be set to UCS-2 for these fields.
Default: EMPTY

  
  NUMERIC_SCALE (int)
    Scale(number of digits after the decimal point) for SQL_NUMERIC
and SQL_DECIMAL columns. Precision(maximum number of digits) is always
set to 34.
Default: 8

  
  TZ (string)
    The time zone used to interpret SQL DATE and TIMESTAMP
attributes.
Default: Local time zone

  
  START_TIME_EXPR (expression)
    A constant expression used to replace the
&lt;_START_TIME&gt; placeholder in an SQL query. It can be constructed
using standard arithmetic operators: +, -, *, /. In
constructing the expression, one can use comparison operators =, !=,
&lt;, &lt;=, &gt;, and &gt;= as well as Boolean operators AND, OR, NOT (true
evaluates to 1 and false to 0). Also, OneTick built-in
functions can be used in the expression (see Catalog
of Built-in functions for details).

The double and decimal values will be treated as YYYYMMDDHHmmss[.nanoseconds] if possible(the time zone will be considered the local time zone), otherwise as UNIX milliseconds (dropping the fractional part).
Datetime will be considered with nanosecond precision.
Integer values will be treated as UNIX milliseconds.
String values will be processed as is (you can even pass expressions in string form). Supported time values are described in the Specifying query start and end times and the symbol date section of OneTick SQL Access document.


    You can also use pseudo-fields such as TIMESTAMP, _START_TIME,
and _END_TIME (described in detail in the Pseudo-fields document).
Default: EMPTY

  
  END_TIME_EXPR (expression)
    A constant expression used to replace &lt;_END_TIME&gt;
placeholder in an SQL query. It can be constructed using standard
arithmetic operators: +, -, *, /. In constructing the expression, one
can use comparison operators =, !=, &lt;, &lt;=, &gt;, and &gt;= as
well as Boolean operators AND, OR, NOT (true evaluates to 1 and false
to 0). Also, OneTick built-in functions can be used in the expression
(see Catalog of Built-in functions for
details).

The processing of values is the same as for START_TIME_EXPR.

    You can also use pseudo-fields such as TIMESTAMP, _START_TIME,
and _END_TIME (described in detail in the Pseudo-fields document).
Default: EMPTY

  
  APPLY_SYMBOL_NAME_HISTORY (Boolean)
    If set to false, OMD::ODBC_QUERY will not take into account the
symbol name history when substituting the SQL query with actual symbol
names.

    If set to true, OMD::ODBC_QUERY will resolve symbol name changes
according to the reference data by substituting the SQL query
accordingly, provided the symbol date is specified and the SQL query
contains all three placeholders _SYMBOL_NAME, _START_TIME, _END_TIME.

    Note that ODBC data source needs to support "UNION ALL" syntax
for this functionality to work.

    Default: FALSE

  

Examples:

Retrieves closing prices of securities as of 20031201:

OMD::ODBC_QUERY(DSN='DAILY_DB',USER=",PASSWORD=",SQL='SELECT
CLOSING_PRICE FROM DAILY_INFO WHERE SYMBOL_NAME='&lt;_SYMBOL_NAME&gt;'
AND EFFECTIVE_DATE=20031201,TZ='GMT',START_TIME_EXPR=",END_TIME_EXPR=")

Retrieves FX rates from GBP to USD in between 1 hour before query
start time and query end time. Query start and end times are formatted
as SQL timestamp literals when being used as placeholder replacements.

OMD::ODBC_QUERY(DSN='FX',USER=",PASSWORD=",SQL='SELECT
RATE FROM FX_RATES WHERE ID='GBP/USD' AND TIMESTAMP &gt;=
'&lt;_START_TIME&gt;' AND TIMESTAMP &lt;
'&lt;_END_TIME&gt;',TZ='GMT',START_TIME_EXPR='TIME_FORMAT("%Y-%m-%d
%H:%M:%S.%q",_START_TIME-3600000) + "GMT"',END_TIME_EXPR='
TIME_FORMAT("%Y-%m-%d %H:%M:%S.%q",_END_TIME) + "GMT"')


	"""
	class Parameters:
		dsn = "DSN"
		connection_string = "CONNECTION_STRING"
		authentication_type = "AUTHENTICATION_TYPE"
		user = "USER"
		password = "PASSWORD"
		sql = "SQL"
		symbology = "SYMBOLOGY"
		allow_unordered_ticks = "ALLOW_UNORDERED_TICKS"
		tz = "TZ"
		start_time_expr = "START_TIME_EXPR"
		end_time_expr = "END_TIME_EXPR"
		apply_symbol_name_history = "APPLY_SYMBOL_NAME_HISTORY"
		numeric_scale = "NUMERIC_SCALE"
		preserve_unicode_fields = "PRESERVE_UNICODE_FIELDS"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["dsn", "connection_string", "authentication_type", "user", "password", "sql", "symbology", "allow_unordered_ticks", "tz", "start_time_expr", "end_time_expr", "apply_symbol_name_history", "numeric_scale", "preserve_unicode_fields"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["dsn", "_default_dsn", "connection_string", "_default_connection_string", "authentication_type", "_default_authentication_type", "user", "_default_user", "password", "_default_password", "sql", "_default_sql", "symbology", "_default_symbology", "allow_unordered_ticks", "_default_allow_unordered_ticks", "tz", "_default_tz", "start_time_expr", "_default_start_time_expr", "end_time_expr", "_default_end_time_expr", "apply_symbol_name_history", "_default_apply_symbol_name_history", "numeric_scale", "_default_numeric_scale", "preserve_unicode_fields", "_default_preserve_unicode_fields", "stack_info", "_used_strings"]

	class AuthenticationType:
		SYSTEM = "System"
		USERNAME_PASSWORD = "Username/Password"

	def __init__(self, dsn="", connection_string="", authentication_type=AuthenticationType.USERNAME_PASSWORD, user="", password="", sql="", symbology="", allow_unordered_ticks=False, tz="", start_time_expr="", end_time_expr="", apply_symbol_name_history=False, numeric_scale=8, preserve_unicode_fields=""):
		_graph_components.EpBase.__init__(self, "OMD::ODBC_QUERY")
		self._default_dsn = ""
		self.dsn = dsn
		self._default_connection_string = ""
		self.connection_string = connection_string
		self._default_authentication_type = type(self).AuthenticationType.USERNAME_PASSWORD
		self.authentication_type = authentication_type
		self._default_user = ""
		self.user = user
		self._default_password = ""
		self.password = password
		self._default_sql = ""
		self.sql = sql
		self._default_symbology = ""
		self.symbology = symbology
		self._default_allow_unordered_ticks = False
		self.allow_unordered_ticks = allow_unordered_ticks
		self._default_tz = ""
		self.tz = tz
		self._default_start_time_expr = ""
		self.start_time_expr = start_time_expr
		self._default_end_time_expr = ""
		self.end_time_expr = end_time_expr
		self._default_apply_symbol_name_history = False
		self.apply_symbol_name_history = apply_symbol_name_history
		self._default_numeric_scale = 8
		self.numeric_scale = numeric_scale
		self._default_preserve_unicode_fields = ""
		self.preserve_unicode_fields = preserve_unicode_fields
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_dsn(self, value):
		self.dsn = value
		return self

	def set_connection_string(self, value):
		self.connection_string = value
		return self

	def set_authentication_type(self, value):
		self.authentication_type = value
		return self

	def set_user(self, value):
		self.user = value
		return self

	def set_password(self, value):
		self.password = value
		return self

	def set_sql(self, value):
		self.sql = value
		return self

	def set_symbology(self, value):
		self.symbology = value
		return self

	def set_allow_unordered_ticks(self, value):
		self.allow_unordered_ticks = value
		return self

	def set_tz(self, value):
		self.tz = value
		return self

	def set_start_time_expr(self, value):
		self.start_time_expr = value
		return self

	def set_end_time_expr(self, value):
		self.end_time_expr = value
		return self

	def set_apply_symbol_name_history(self, value):
		self.apply_symbol_name_history = value
		return self

	def set_numeric_scale(self, value):
		self.numeric_scale = value
		return self

	def set_preserve_unicode_fields(self, value):
		self.preserve_unicode_fields = value
		return self

	@staticmethod
	def _get_name():
		return "OMD::ODBC_QUERY"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.dsn != "": 
			desc += "DSN=" + py_to_str(self.dsn) + ","
		if self.connection_string != "": 
			desc += "CONNECTION_STRING=" + py_to_str(self.connection_string) + ","
		if self.authentication_type != self.AuthenticationType.USERNAME_PASSWORD: 
			desc += "AUTHENTICATION_TYPE=" + py_to_str(self.authentication_type) + ","
		if self.user != "": 
			desc += "USER=" + py_to_str(self.user) + ","
		if self.password != "": 
			desc += "PASSWORD=" + py_to_str(self.password) + ","
		if self.sql != "": 
			desc += "SQL=" + py_to_str(self.sql) + ","
		if self.symbology != "": 
			desc += "SYMBOLOGY=" + py_to_str(self.symbology) + ","
		if self.allow_unordered_ticks != False: 
			desc += "ALLOW_UNORDERED_TICKS=" + py_to_str(self.allow_unordered_ticks) + ","
		if self.tz != "": 
			desc += "TZ=" + py_to_str(self.tz) + ","
		if self.start_time_expr != "": 
			desc += "START_TIME_EXPR=" + py_to_str(self.start_time_expr) + ","
		if self.end_time_expr != "": 
			desc += "END_TIME_EXPR=" + py_to_str(self.end_time_expr) + ","
		if self.apply_symbol_name_history != False: 
			desc += "APPLY_SYMBOL_NAME_HISTORY=" + py_to_str(self.apply_symbol_name_history) + ","
		if self.numeric_scale != 8: 
			desc += "NUMERIC_SCALE=" + py_to_str(self.numeric_scale) + ","
		if self.preserve_unicode_fields != "": 
			desc += "PRESERVE_UNICODE_FIELDS=" + py_to_str(self.preserve_unicode_fields) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class Omd_writeToOdbc(_graph_components.EpBase):
	"""
		

OMD::WRITE_TO_ODBC

Type: Other

Description: Writes the input tick series to an
ODBC-supported database.

To be able to run it, you need to have an ODBC driver manager
available on your box (comes with Windows OS; unixodbc package may need
to be installed on UNIX OS).

In addition, the following entry needs to be added to the main
configuration file:

LOAD_ODBC_UDF=true

Supports transactions. If transactions are enabled all the changes
in the transaction interval will we rolled back in case of any error in
the query.

Python
class name:&nbsp;Omd_writeToOdbc

Input: A time series of ticks.

Output: A time series of ticks or none.

Parameters:


  DSN (string)
    The name of an ODBC DSN to query.

  
  CONNECTION_STRING (string)
    Connection string that should be used to connect to the ODBC
driver.
If this parameter is used, the DSN, USER, PASSWORD parameters should be
empty and the AUTHENTICATION_TYPE
parameter should be empty, or set to its default value (i.e.,
Username/Password).

  
  AUTHENTICATION_TYPE (string)
    Possible values:

    
      System: In this case, a trusted connection to the ODBC
driver will be used. If authentication is enabled in the server
configuration file (see the OneTick Installation and Administration
Guide for details), the client will be authenticated and the client's
credentials will be used for connection; otherwise, server credentials
will be used.
      Username/Password: The specified username and password
will be used to connect to the ODBC driver.
    
    Default: Username/Password

  
  USER (string)
    The connection user name (optional). Ignored if
AUTHENTICATION_TYPE is System.
Default: EMPTY

  
  PASSWORD (string)
    The connection password (optional). Ignored if
AUTHENTICATION_TYPE is System.
Default: EMPTY

  
  TABLE (string)
    Table to insert into. The value of this parameter should be a
fully qualified table name, i.e., should include database name, owner,
etc.

  
  FIELDS (string)
    Fields to insert. Field names must match the attributes of an
input tick and match the fields in the table they are inserted
into(also by case if the ODBC data source is case sensitive). Table
fields must have sufficient size but their types can be different from
the types of the fields in the tick. If type conversion is possible it
will be performed. If this parameter is empty, all attributes of an
input tick will be inserted. If "TIMESTAMP" is present in the field
list, the value will be taken from a tick field if such field is
present, primary tick timestamp will be used otherwise.
Default: EMPTY

  
  PROPAGATE_TICKS (Boolean)
    Switches propagation of the ticks. If set to true, ticks will be
propagated.
Default: true

  
  TZ (string)
    Destination time zone used to convert time and date fields of
the tick.
Default: GMT

  
  MAX_VARLEN_DATA_LENGTH (int)
    Some ODBC data sources require specifying maximum length of
variable length fields. A check will be performed, if the driver does
need this value all variable length values larger than the specified
value will be truncated. If the driver doesn't need the maximum length
this parameter will be ignored.
Default: 0

  
  TRANSACTION_INTERVAL_UNITS
(enumerated type)
    
      
        Specifies the units for the size of the transaction
interval. The units can be either SECONDS, TICKS, FLEXIBLE, NONE.

      
      When the units are set to SECONDS, input tick series must be
sorted by timestamp. Only one input stream is allowed, use MERGE in
case of multiple symbols.
      Units can't be set to TICKS for historical queries.
      When the units are set to FLEXIBLE,
TRANSACTION_BOUNDARY_CRITERIA can be specified. Only one input stream
is allowed, use MERGE in case of multiple symbols.
      When set to NONE, transactions are disabled,
TRANSACTION_BOUNDARY_CRITERIA must be empty and TRANSACTION_INTERVAL
set to 0 in that case.
    
    Default: SECONDS

  
  TRANSACTION_INTERVAL
(seconds/ticks)
    Determines the length of each transaction (seconds or ticks,
depending on value of TRANSACTION_INTERVAL_UNITS). If
TRANSACTION_INTERVAL is set to 0, all the ticks will be submitted in a
single transaction.

    
      If TRANSACTION_INTERVAL_UNITS is set to SECONDS, the
transactions are defined by a [start_time,
end_time] pairs of timestamps, where end_time is greater than
start time by the specified amount of seconds.
Whenever the timestamp of the received tick is out of this time range,
the transaction is committed and a new one is started.
      If TRANSACTION_INTERVAL_UNITS is set to TICKS, the
transaction size is defined by the n number of ticks. Each
transaction, except the last one, will contain n ticks. The
last transaction can contain less than n ticks.
    
  
  TRANSACTION_BOUNDARY_CRITERIA
(expression)
    This specifies criteria for when to commit a transaction.
Expression is built with Boolean operators AND, OR, and NOT. Accepts
arithmetic operators: +, -, *, / and comparison operators =, !=, &lt;,
&lt;=, &gt;, &gt;=. Field names are not quoted, are case sensitive, and
have to be present in the tick. String literals must be surrounded by
single or double quotes.

    The special field TIMESTAMP can be used in WHERE. It represents
the value of the tick timestamp.

    In addition, WHERE accepts OneTick built-in functions (see Catalog of Built-in functions for details).
Also, OneTick event processors that act as a filter can be used.

    Fields of preceding ticks (those that already arrived) can also
be involved in computations. In such cases, field names are followed by
a negative integer index in square brackets, specifying how far to look
back (e.g., PRICE[-1], SIZE[-3], etc.). The expression can involve
state variables.

    The following special attributes can be used in
TRANSACTION_BOUNDARY_CRITERIA:

    
      TIMESTAMP
        The value of the tick timestamp.

      
      _START_TIME
        The query start time. The start time for a query for a given
day when apply times daily flag is set.

      
      _END_TIME
        The query end time. The end time for a query for a given day
when apply times daily flag is set.

      
    
  
  BOUNDARY_TICK_TRANSACTION
(NEW/PREVIOUS)
    Default: NEW

    If BOUNDARY_TICK_TRANSACTION is set to PREVIOUS:

    
      When TRANSACTION_INTERVAL_UNITS is set to FLEXIBLE, the tick
that matches the criteria and thus causes the commit of current
transaction and start of a new one belongs to the transaction being
committed.
      When TRANSACTION_INTERVAL_UNITS is set to SECONDS, the tick
timestamp of which is equal to the end time of transaction interval
belongs to the transaction being committed.
    
    If BOUNDARY_TICK_TRANSACTION is set to NEW:

    
      When TRANSACTION_INTERVAL_UNITS is set to FLEXIBLE, the tick
that matches the criteria will be the last tick in current transaction.
      When TRANSACTION_INTERVAL_UNITS is set to SECONDS, the tick
timestamp of which is equal to the end time of transaction interval
will be the last tick in current transaction.
    
  
  NUMERIC_SCALE (int)
    Scale(number of digits after the decimal point) for the
OT_DECIMAL128 fields. Precision(maximum number of digits) is always set
to 34.
Default: 8

  

Examples:

In this example, PRICE and SIZE fields of the ticks are written into
the [output_table] table of [master] database, variable length values
are limited to 120 bytes, transactions are committed each 5 minutes and
the tick with boundary timestamp is included in the previous
transaction:

OMD::WRITE_TO_ODBC(DSN="DEST_DSN",AUTHENTICATION_TYPE="System",TABLE="[master].[dbo].[output_table]",FIELDS="PRICE,SIZE",MAX_VARLEN_DATA_LENGTH="120",TRANSACTION_INTERVAL_UNITS="SECONDS",TRANSACTION_INTERVAL="300",
BOUNDARY_TICK_TRANSACTION="PREVIOUS")

Same example with disabled transactions:

OMD::WRITE_TO_ODBC(DSN="DEST_DSN",AUTHENTICATION_TYPE="System",TABLE="[master].[dbo].[output_table]",FIELDS="PRICE,SIZE",MAX_VARLEN_DATA_LENGTH="120",TRANSACTION_INTERVAL_UNITS="NONE")


	"""
	class Parameters:
		dsn = "DSN"
		connection_string = "CONNECTION_STRING"
		authentication_type = "AUTHENTICATION_TYPE"
		user = "USER"
		password = "PASSWORD"
		table = "TABLE"
		fields = "FIELDS"
		propagate_ticks = "PROPAGATE_TICKS"
		tz = "TZ"
		max_varlen_data_length = "MAX_VARLEN_DATA_LENGTH"
		transaction_interval_units = "TRANSACTION_INTERVAL_UNITS"
		transaction_interval = "TRANSACTION_INTERVAL"
		transaction_boundary_criteria = "TRANSACTION_BOUNDARY_CRITERIA"
		boundary_tick_transaction = "BOUNDARY_TICK_TRANSACTION"
		numeric_scale = "NUMERIC_SCALE"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["dsn", "connection_string", "authentication_type", "user", "password", "table", "fields", "propagate_ticks", "tz", "max_varlen_data_length", "transaction_interval_units", "transaction_interval", "transaction_boundary_criteria", "boundary_tick_transaction", "numeric_scale"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["dsn", "_default_dsn", "connection_string", "_default_connection_string", "authentication_type", "_default_authentication_type", "user", "_default_user", "password", "_default_password", "table", "_default_table", "fields", "_default_fields", "propagate_ticks", "_default_propagate_ticks", "tz", "_default_tz", "max_varlen_data_length", "_default_max_varlen_data_length", "transaction_interval_units", "_default_transaction_interval_units", "transaction_interval", "_default_transaction_interval", "transaction_boundary_criteria", "_default_transaction_boundary_criteria", "boundary_tick_transaction", "_default_boundary_tick_transaction", "numeric_scale", "_default_numeric_scale", "stack_info", "_used_strings"]

	class AuthenticationType:
		SYSTEM = "System"
		USERNAME_PASSWORD = "Username/Password"

	class TransactionIntervalUnits:
		FLEXIBLE = "FLEXIBLE"
		NONE = "NONE"
		SECONDS = "SECONDS"
		TICKS = "TICKS"

	class BoundaryTickTransaction:
		NEW = "NEW"
		PREVIOUS = "PREVIOUS"

	def __init__(self, dsn="", connection_string="", authentication_type=AuthenticationType.USERNAME_PASSWORD, user="", password="", table="", fields="", propagate_ticks=True, tz="GMT", max_varlen_data_length=0, transaction_interval_units=TransactionIntervalUnits.SECONDS, transaction_interval=0, transaction_boundary_criteria="", boundary_tick_transaction=BoundaryTickTransaction.NEW, numeric_scale=8):
		_graph_components.EpBase.__init__(self, "OMD::WRITE_TO_ODBC")
		self._default_dsn = ""
		self.dsn = dsn
		self._default_connection_string = ""
		self.connection_string = connection_string
		self._default_authentication_type = type(self).AuthenticationType.USERNAME_PASSWORD
		self.authentication_type = authentication_type
		self._default_user = ""
		self.user = user
		self._default_password = ""
		self.password = password
		self._default_table = ""
		self.table = table
		self._default_fields = ""
		self.fields = fields
		self._default_propagate_ticks = True
		self.propagate_ticks = propagate_ticks
		self._default_tz = "GMT"
		self.tz = tz
		self._default_max_varlen_data_length = 0
		self.max_varlen_data_length = max_varlen_data_length
		self._default_transaction_interval_units = type(self).TransactionIntervalUnits.SECONDS
		self.transaction_interval_units = transaction_interval_units
		self._default_transaction_interval = 0
		self.transaction_interval = transaction_interval
		self._default_transaction_boundary_criteria = ""
		self.transaction_boundary_criteria = transaction_boundary_criteria
		self._default_boundary_tick_transaction = type(self).BoundaryTickTransaction.NEW
		self.boundary_tick_transaction = boundary_tick_transaction
		self._default_numeric_scale = 8
		self.numeric_scale = numeric_scale
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_dsn(self, value):
		self.dsn = value
		return self

	def set_connection_string(self, value):
		self.connection_string = value
		return self

	def set_authentication_type(self, value):
		self.authentication_type = value
		return self

	def set_user(self, value):
		self.user = value
		return self

	def set_password(self, value):
		self.password = value
		return self

	def set_table(self, value):
		self.table = value
		return self

	def set_fields(self, value):
		self.fields = value
		return self

	def set_propagate_ticks(self, value):
		self.propagate_ticks = value
		return self

	def set_tz(self, value):
		self.tz = value
		return self

	def set_max_varlen_data_length(self, value):
		self.max_varlen_data_length = value
		return self

	def set_transaction_interval_units(self, value):
		self.transaction_interval_units = value
		return self

	def set_transaction_interval(self, value):
		self.transaction_interval = value
		return self

	def set_transaction_boundary_criteria(self, value):
		self.transaction_boundary_criteria = value
		return self

	def set_boundary_tick_transaction(self, value):
		self.boundary_tick_transaction = value
		return self

	def set_numeric_scale(self, value):
		self.numeric_scale = value
		return self

	@staticmethod
	def _get_name():
		return "OMD::WRITE_TO_ODBC"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.dsn != "": 
			desc += "DSN=" + py_to_str(self.dsn) + ","
		if self.connection_string != "": 
			desc += "CONNECTION_STRING=" + py_to_str(self.connection_string) + ","
		if self.authentication_type != self.AuthenticationType.USERNAME_PASSWORD: 
			desc += "AUTHENTICATION_TYPE=" + py_to_str(self.authentication_type) + ","
		if self.user != "": 
			desc += "USER=" + py_to_str(self.user) + ","
		if self.password != "": 
			desc += "PASSWORD=" + py_to_str(self.password) + ","
		if self.table != "": 
			desc += "TABLE=" + py_to_str(self.table) + ","
		if self.fields != "": 
			desc += "FIELDS=" + py_to_str(self.fields) + ","
		if self.propagate_ticks != True: 
			desc += "PROPAGATE_TICKS=" + py_to_str(self.propagate_ticks) + ","
		if self.tz != "GMT": 
			desc += "TZ=" + py_to_str(self.tz) + ","
		if self.max_varlen_data_length != 0: 
			desc += "MAX_VARLEN_DATA_LENGTH=" + py_to_str(self.max_varlen_data_length) + ","
		if self.transaction_interval_units != self.TransactionIntervalUnits.SECONDS: 
			desc += "TRANSACTION_INTERVAL_UNITS=" + py_to_str(self.transaction_interval_units) + ","
		if self.transaction_interval != 0: 
			desc += "TRANSACTION_INTERVAL=" + py_to_str(self.transaction_interval) + ","
		if self.transaction_boundary_criteria != "": 
			desc += "TRANSACTION_BOUNDARY_CRITERIA=" + py_to_str(self.transaction_boundary_criteria) + ","
		if self.boundary_tick_transaction != self.BoundaryTickTransaction.NEW: 
			desc += "BOUNDARY_TICK_TRANSACTION=" + py_to_str(self.boundary_tick_transaction) + ","
		if self.numeric_scale != 8: 
			desc += "NUMERIC_SCALE=" + py_to_str(self.numeric_scale) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class Omd_writeToSmtp(_graph_components.EpBase):
	"""
		

OMD::WRITE_TO_SMTP

Type: Output Adapter

Description: Writes the input tick series into SMTP.

It requires access control, by default.

In addition, you need to add the following entries to the main
configuration file:

LOAD_SMTP_UDEP = trueSMTP.SERVERSMTP.ENCRYPTION_TYPE OPENSSL/NONE (default OPENSSL)SMTP.PORT default 465 if ENCRYPTION_TYPE is OPENSSL else 25SMTP.ACCOUNTSMTP.PASSWORD
Python
class name:&nbsp;Omd_writeToSmtp

Parameters:


  MSG_BODY_FIELD_NAME (string)
    The field name which value should go as a message body.
It is not necessary to fill this if the input tick fields count is one.
The input field type should be either string or varstring.

  
  Sender (string)
    Changes the value of the MAIL FROM header and the sender's
address in the body of the email.
If it matches the name of the field and the field is either string or
varstring, then the field's value is used.

  
  To (string)
    Receivers, represented as a comma-separated list.
If any element in the list matches the name of the field and the field
is either string or varstring, then the field's value is used.

  
  Cc (string)
    CC Receivers, represented as a comma-separated list.
If any element in the list matches the name of the field and the field
is either string or varstring, then the field's value is used.

  
  Bcc (string)
    BCC Receivers, represented as a comma-separated list.
If any element in the list matches the name of field and the field is
either string or varstring, then the field's value is used.

  
  Subject (string)
    Subject of the message.
If it matches the name of the field and the field is either string or
varstring, then the field's value is used.

  
  PROPAGATE_TICKS (Boolean)
    Switches on the propagation of ticks. If set to true, ticks are propagated.
Default: true

  
  TREAT_CONNECTION_ERRORS_AS_WARNINGS
(Boolean)
    Treat connection errors as warnings. If set to true, connection errors are treated as
warnings.
Default: false

  
  LOGGING_LEVEL (int)
    Specifies the logging level. Possible values are 0(lowest), 1,
2, 3(highest).
Default: 2

  

Examples:

The usage example of the EP shown below:

OMD::WRITE_TO_SMTP(MSG_BODY_FIELD_NAME=some_fild,To="to_fild,to@somemail.com,to2@somemail.com",Cc="cc@somemail.com",Bcc="bcc@somemail.com",Subject="some_subject")
The example of configuration file is shown below:

LOAD_SMTP_UDEP=TRUESMTP.SERVER="smtp.gmail.com"SMTP.PORT="465"SMTP.ENCRYPTION_TYPE="OPENSSL"SMTP.ACCOUNT="SomeMail@gmail.com"SMTP.PASSWORD="SomePassword"
The example of access control file is shown below:

&lt;event_processors&gt;  &lt;ep id="OMD::WRITE_TO_SMTP" EXECUTE_ACCESS="true" /&gt;&lt;/event_processors&gt;

	"""
	class Parameters:
		msg_body_field_name = "MSG_BODY_FIELD_NAME"
		sender = "Sender"
		to = "To"
		cc = "Cc"
		bcc = "Bcc"
		subject = "Subject"
		propagate_ticks = "PROPAGATE_TICKS"
		treat_connection_errors_as_warnings = "TREAT_CONNECTION_ERRORS_AS_WARNINGS"
		logging_level = "LOGGING_LEVEL"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["msg_body_field_name", "sender", "to", "cc", "bcc", "subject", "propagate_ticks", "treat_connection_errors_as_warnings", "logging_level"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["msg_body_field_name", "_default_msg_body_field_name", "sender", "_default_sender", "to", "_default_to", "cc", "_default_cc", "bcc", "_default_bcc", "subject", "_default_subject", "propagate_ticks", "_default_propagate_ticks", "treat_connection_errors_as_warnings", "_default_treat_connection_errors_as_warnings", "logging_level", "_default_logging_level", "stack_info", "_used_strings"]

	def __init__(self, msg_body_field_name="", sender="", to="", cc="", bcc="", subject="", propagate_ticks=True, treat_connection_errors_as_warnings=False, logging_level=2):
		_graph_components.EpBase.__init__(self, "OMD::WRITE_TO_SMTP")
		self._default_msg_body_field_name = ""
		self.msg_body_field_name = msg_body_field_name
		self._default_sender = ""
		self.sender = sender
		self._default_to = ""
		self.to = to
		self._default_cc = ""
		self.cc = cc
		self._default_bcc = ""
		self.bcc = bcc
		self._default_subject = ""
		self.subject = subject
		self._default_propagate_ticks = True
		self.propagate_ticks = propagate_ticks
		self._default_treat_connection_errors_as_warnings = False
		self.treat_connection_errors_as_warnings = treat_connection_errors_as_warnings
		self._default_logging_level = 2
		self.logging_level = logging_level
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_msg_body_field_name(self, value):
		self.msg_body_field_name = value
		return self

	def set_sender(self, value):
		self.sender = value
		return self

	def set_to(self, value):
		self.to = value
		return self

	def set_cc(self, value):
		self.cc = value
		return self

	def set_bcc(self, value):
		self.bcc = value
		return self

	def set_subject(self, value):
		self.subject = value
		return self

	def set_propagate_ticks(self, value):
		self.propagate_ticks = value
		return self

	def set_treat_connection_errors_as_warnings(self, value):
		self.treat_connection_errors_as_warnings = value
		return self

	def set_logging_level(self, value):
		self.logging_level = value
		return self

	@staticmethod
	def _get_name():
		return "OMD::WRITE_TO_SMTP"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.msg_body_field_name != "": 
			desc += "MSG_BODY_FIELD_NAME=" + py_to_str(self.msg_body_field_name) + ","
		if self.sender != "": 
			desc += "SENDER=" + py_to_str(self.sender) + ","
		if self.to != "": 
			desc += "TO=" + py_to_str(self.to) + ","
		if self.cc != "": 
			desc += "CC=" + py_to_str(self.cc) + ","
		if self.bcc != "": 
			desc += "BCC=" + py_to_str(self.bcc) + ","
		if self.subject != "": 
			desc += "SUBJECT=" + py_to_str(self.subject) + ","
		if self.propagate_ticks != True: 
			desc += "PROPAGATE_TICKS=" + py_to_str(self.propagate_ticks) + ","
		if self.treat_connection_errors_as_warnings != False: 
			desc += "TREAT_CONNECTION_ERRORS_AS_WARNINGS=" + py_to_str(self.treat_connection_errors_as_warnings) + ","
		if self.logging_level != 2: 
			desc += "LOGGING_LEVEL=" + py_to_str(self.logging_level) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class Ml_dlibBinaryClassifyTrainSvmCWithRadialBasisKernel(_graph_components.EpBase):
	"""
		

ML::DLIB_BINARY_CLASSIFY_TRAIN_SVM_C_WITH_RADIAL_BASIS_KERNEL

Type: Other

Description: This train
classification EP uses DLIB library implementation of C SVM
(Support Vector Machine) for solving the binary classification problem.
The SVM_C trainer is configured to use radial basis kernel.
Like other binary
classification EPs it is required that the input data be labeled as
positive and negative with +1 and -1 values respectively.

In order to enable the event processor, please add LOAD_ML_DLIB_UDEPS=true to your
ONE_TICK_CONFIG file. Please check compatibility to see if
the EP is supported in your distribution.

Python
class name:&nbsp;Ml_dlibBinaryClassifyTrainSvmCWithRadialBasisKernel

Input: A time series of ticks containing a filed representing
the actual (expected) classification label (-1
or +1).

Output:The set of binary
performance metrics computed over the cross validation set selected
from the input data.

Parameters: The EP supports following set of parameters, the
description of the common parameters can be found here:


  FEATURES_FIELD_NAMES
(string)
  LABEL_FIELD_NAME
(string)
  INSTANCE_NAME
(string)
  ENABLE_PROBABILITY_ESTIMATES
(boolean)
  PROBABILITY_THRESHOLD_GRANULARITY
(double)
  OPTIMIZATION_OBJECTIVE
(string)
  TRAINING_SAMPLE_RATE
(int)
  RBF_GAMMA_VALUE (double) - The Gamma
value for Radial Basis kernel. Cannot be used when optimization
objective is specified. Intuitively, the gamma parameter defines how
far the influence of a single training example reaches, with low values
meaning 'far' and high values meaning 'close'.
  EPSILION (double) - one approach to reduce
the SVM training time it using a bigger stopping epsilon. However, this
might make the outputs less reliable. But sometimes it works out well.
Default: 0.001
  C_1_CLASS (double) - The SVM regularization
parameter for the +1 class. It is the parameter that determines the
trade off between trying to fit the + 1 training data exactly or
allowing more errors but hopefully improving the generalization ability
of the resulting classifier.Larger values encourage exact fitting while
smaller values of C1 may encourage better generalization. Default: 1
  C_2_CLASS (double) - The SVM regularization
parameter for the -1 class. It is the parameter that determines the
trade off between trying to fit the - 1 training data exactly or
allowing more errors but hopefully improving the generalization ability
of the resulting classifier.Larger values encourage exact fitting while
smaller values of C2 may encourage better generalization. Default: 1

Examples: See the svm_c
example in ml_dlib_classify.otq.


	"""
	class Parameters:
		features_field_names = "FEATURES_FIELD_NAMES"
		label_field_name = "LABEL_FIELD_NAME"
		instance_name = "INSTANCE_NAME"
		enable_probability_estimates = "ENABLE_PROBABILITY_ESTIMATES"
		probability_threshold_granularity = "PROBABILITY_THRESHOLD_GRANULARITY"
		optimization_objective = "OPTIMIZATION_OBJECTIVE"
		rbf_gamma_value = "RBF_GAMMA_VALUE"
		epsilion = "EPSILION"
		c_1_class = "C_1_CLASS"
		c_2_class = "C_2_CLASS"
		training_sample_rate = "TRAINING_SAMPLE_RATE"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["features_field_names", "label_field_name", "instance_name", "enable_probability_estimates", "probability_threshold_granularity", "optimization_objective", "rbf_gamma_value", "epsilion", "c_1_class", "c_2_class", "training_sample_rate"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["features_field_names", "_default_features_field_names", "label_field_name", "_default_label_field_name", "instance_name", "_default_instance_name", "enable_probability_estimates", "_default_enable_probability_estimates", "probability_threshold_granularity", "_default_probability_threshold_granularity", "optimization_objective", "_default_optimization_objective", "rbf_gamma_value", "_default_rbf_gamma_value", "epsilion", "_default_epsilion", "c_1_class", "_default_c_1_class", "c_2_class", "_default_c_2_class", "training_sample_rate", "_default_training_sample_rate", "stack_info", "_used_strings"]

	class OptimizationObjective:
		EMPTY = ""
		ACC = "ACC"
		AUC = "AUC"
		F_SCORE_1_ = "F_SCORE(1)"

	def __init__(self, features_field_names="", label_field_name="", instance_name="", enable_probability_estimates=False, probability_threshold_granularity="0.05", optimization_objective=OptimizationObjective.EMPTY, rbf_gamma_value="", epsilion="0.001", c_1_class=1, c_2_class=1, training_sample_rate=4):
		_graph_components.EpBase.__init__(self, "ML::DLIB_BINARY_CLASSIFY_TRAIN_SVM_C_WITH_RADIAL_BASIS_KERNEL")
		self._default_features_field_names = ""
		self.features_field_names = features_field_names
		self._default_label_field_name = ""
		self.label_field_name = label_field_name
		self._default_instance_name = ""
		self.instance_name = instance_name
		self._default_enable_probability_estimates = False
		self.enable_probability_estimates = enable_probability_estimates
		self._default_probability_threshold_granularity = "0.05"
		self.probability_threshold_granularity = probability_threshold_granularity
		self._default_optimization_objective = type(self).OptimizationObjective.EMPTY
		self.optimization_objective = optimization_objective
		self._default_rbf_gamma_value = ""
		self.rbf_gamma_value = rbf_gamma_value
		self._default_epsilion = "0.001"
		self.epsilion = epsilion
		self._default_c_1_class = 1
		self.c_1_class = c_1_class
		self._default_c_2_class = 1
		self.c_2_class = c_2_class
		self._default_training_sample_rate = 4
		self.training_sample_rate = training_sample_rate
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_features_field_names(self, value):
		self.features_field_names = value
		return self

	def set_label_field_name(self, value):
		self.label_field_name = value
		return self

	def set_instance_name(self, value):
		self.instance_name = value
		return self

	def set_enable_probability_estimates(self, value):
		self.enable_probability_estimates = value
		return self

	def set_probability_threshold_granularity(self, value):
		self.probability_threshold_granularity = value
		return self

	def set_optimization_objective(self, value):
		self.optimization_objective = value
		return self

	def set_rbf_gamma_value(self, value):
		self.rbf_gamma_value = value
		return self

	def set_epsilion(self, value):
		self.epsilion = value
		return self

	def set_c_1_class(self, value):
		self.c_1_class = value
		return self

	def set_c_2_class(self, value):
		self.c_2_class = value
		return self

	def set_training_sample_rate(self, value):
		self.training_sample_rate = value
		return self

	@staticmethod
	def _get_name():
		return "ML::DLIB_BINARY_CLASSIFY_TRAIN_SVM_C_WITH_RADIAL_BASIS_KERNEL"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.features_field_names != "": 
			desc += "FEATURES_FIELD_NAMES=" + py_to_str(self.features_field_names) + ","
		if self.label_field_name != "": 
			desc += "LABEL_FIELD_NAME=" + py_to_str(self.label_field_name) + ","
		if self.instance_name != "": 
			desc += "INSTANCE_NAME=" + py_to_str(self.instance_name) + ","
		if self.enable_probability_estimates != False: 
			desc += "ENABLE_PROBABILITY_ESTIMATES=" + py_to_str(self.enable_probability_estimates) + ","
		if self.probability_threshold_granularity != "0.05": 
			desc += "PROBABILITY_THRESHOLD_GRANULARITY=" + py_to_str(self.probability_threshold_granularity) + ","
		if self.optimization_objective != self.OptimizationObjective.EMPTY: 
			desc += "OPTIMIZATION_OBJECTIVE=" + py_to_str(self.optimization_objective) + ","
		if self.rbf_gamma_value != "": 
			desc += "RBF_GAMMA_VALUE=" + py_to_str(self.rbf_gamma_value) + ","
		if self.epsilion != "0.001": 
			desc += "EPSILION=" + py_to_str(self.epsilion) + ","
		if self.c_1_class != 1: 
			desc += "C_1_CLASS=" + py_to_str(self.c_1_class) + ","
		if self.c_2_class != 1: 
			desc += "C_2_CLASS=" + py_to_str(self.c_2_class) + ","
		if self.training_sample_rate != 4: 
			desc += "TRAINING_SAMPLE_RATE=" + py_to_str(self.training_sample_rate) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class Ml_dlibBinaryClassifyTrainKrrWithRadialBasisKernel(_graph_components.EpBase):
	"""
		

ML::DLIB_BINARY_CLASSIFY_TRAIN_KRR_WITH_RADIAL_BASIS_KERNEL

Type: Other

Description: This train
classification EP uses DLIB library implementation of KRR
(Kernel Ridge Regression) algorithm for solving the binary
classification problem. The KRR trainer is configured to use radial basis kernel.
Like other binary
classification EPs it is required that the input data be labeled as
positive and negative with +1 and -1 values respectively.

In order to enable the event processor, please add LOAD_ML_DLIB_UDEPS=true to your
ONE_TICK_CONFIG file. Please check compatibility to see if
the EP is supported in your distribution.

Python
class name:&nbsp;Ml_dlibBinaryClassifyTrainKrrWithRadialBasisKernel

Input: A time series of ticks containing a filed representing
the actual (expected) classification label (-1
or +1).

Output:The set of binary
performance metrics computed over the cross validation set selected
from the input data.

Parameters: The EP supports following set of parameters, the
description of the common parameters can be found here:


  FEATURES_FIELD_NAMES
(string)
  LABEL_FIELD_NAME
(string)
  INSTANCE_NAME
(string)
  ENABLE_PROBABILITY_ESTIMATES
(boolean)
  PROBABILITY_THRESHOLD_GRANULARITY
(double)
  OPTIMIZATION_OBJECTIVE
(string)
  TRAINING_SAMPLE_RATE
(int)
  RBF_GAMMA_VALUE (double) - The Gamma
value for Radial Basis kernel. Cannot be used when optimization
objective is specified. Intuitively, the gamma parameter defines how
far the influence of a single training example reaches, with low values
meaning 'far' and high values meaning 'close'.
  SEARCH_LAMBDA_VALUES (string) -
the semicolon ';' separated list of double values indicating the range
of lambda which will be searched to find the better classifier.
  MAX_BASIS_SIZE (long) - The maximum
number of basis vectors to use..

Examples: See the krr
example in ml_dlib_classify.otq.


	"""
	class Parameters:
		features_field_names = "FEATURES_FIELD_NAMES"
		label_field_name = "LABEL_FIELD_NAME"
		instance_name = "INSTANCE_NAME"
		enable_probability_estimates = "ENABLE_PROBABILITY_ESTIMATES"
		probability_threshold_granularity = "PROBABILITY_THRESHOLD_GRANULARITY"
		optimization_objective = "OPTIMIZATION_OBJECTIVE"
		rbf_gamma_value = "RBF_GAMMA_VALUE"
		search_lambda_values = "SEARCH_LAMBDA_VALUES"
		max_basis_size = "MAX_BASIS_SIZE"
		training_sample_rate = "TRAINING_SAMPLE_RATE"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["features_field_names", "label_field_name", "instance_name", "enable_probability_estimates", "probability_threshold_granularity", "optimization_objective", "rbf_gamma_value", "search_lambda_values", "max_basis_size", "training_sample_rate"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["features_field_names", "_default_features_field_names", "label_field_name", "_default_label_field_name", "instance_name", "_default_instance_name", "enable_probability_estimates", "_default_enable_probability_estimates", "probability_threshold_granularity", "_default_probability_threshold_granularity", "optimization_objective", "_default_optimization_objective", "rbf_gamma_value", "_default_rbf_gamma_value", "search_lambda_values", "_default_search_lambda_values", "max_basis_size", "_default_max_basis_size", "training_sample_rate", "_default_training_sample_rate", "stack_info", "_used_strings"]

	class OptimizationObjective:
		EMPTY = ""
		ACC = "ACC"
		AUC = "AUC"
		F_SCORE_1_ = "F_SCORE(1)"

	def __init__(self, features_field_names="", label_field_name="", instance_name="", enable_probability_estimates=False, probability_threshold_granularity="0.05", optimization_objective=OptimizationObjective.EMPTY, rbf_gamma_value="", search_lambda_values="", max_basis_size="", training_sample_rate=4):
		_graph_components.EpBase.__init__(self, "ML::DLIB_BINARY_CLASSIFY_TRAIN_KRR_WITH_RADIAL_BASIS_KERNEL")
		self._default_features_field_names = ""
		self.features_field_names = features_field_names
		self._default_label_field_name = ""
		self.label_field_name = label_field_name
		self._default_instance_name = ""
		self.instance_name = instance_name
		self._default_enable_probability_estimates = False
		self.enable_probability_estimates = enable_probability_estimates
		self._default_probability_threshold_granularity = "0.05"
		self.probability_threshold_granularity = probability_threshold_granularity
		self._default_optimization_objective = type(self).OptimizationObjective.EMPTY
		self.optimization_objective = optimization_objective
		self._default_rbf_gamma_value = ""
		self.rbf_gamma_value = rbf_gamma_value
		self._default_search_lambda_values = ""
		self.search_lambda_values = search_lambda_values
		self._default_max_basis_size = ""
		self.max_basis_size = max_basis_size
		self._default_training_sample_rate = 4
		self.training_sample_rate = training_sample_rate
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_features_field_names(self, value):
		self.features_field_names = value
		return self

	def set_label_field_name(self, value):
		self.label_field_name = value
		return self

	def set_instance_name(self, value):
		self.instance_name = value
		return self

	def set_enable_probability_estimates(self, value):
		self.enable_probability_estimates = value
		return self

	def set_probability_threshold_granularity(self, value):
		self.probability_threshold_granularity = value
		return self

	def set_optimization_objective(self, value):
		self.optimization_objective = value
		return self

	def set_rbf_gamma_value(self, value):
		self.rbf_gamma_value = value
		return self

	def set_search_lambda_values(self, value):
		self.search_lambda_values = value
		return self

	def set_max_basis_size(self, value):
		self.max_basis_size = value
		return self

	def set_training_sample_rate(self, value):
		self.training_sample_rate = value
		return self

	@staticmethod
	def _get_name():
		return "ML::DLIB_BINARY_CLASSIFY_TRAIN_KRR_WITH_RADIAL_BASIS_KERNEL"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.features_field_names != "": 
			desc += "FEATURES_FIELD_NAMES=" + py_to_str(self.features_field_names) + ","
		if self.label_field_name != "": 
			desc += "LABEL_FIELD_NAME=" + py_to_str(self.label_field_name) + ","
		if self.instance_name != "": 
			desc += "INSTANCE_NAME=" + py_to_str(self.instance_name) + ","
		if self.enable_probability_estimates != False: 
			desc += "ENABLE_PROBABILITY_ESTIMATES=" + py_to_str(self.enable_probability_estimates) + ","
		if self.probability_threshold_granularity != "0.05": 
			desc += "PROBABILITY_THRESHOLD_GRANULARITY=" + py_to_str(self.probability_threshold_granularity) + ","
		if self.optimization_objective != self.OptimizationObjective.EMPTY: 
			desc += "OPTIMIZATION_OBJECTIVE=" + py_to_str(self.optimization_objective) + ","
		if self.rbf_gamma_value != "": 
			desc += "RBF_GAMMA_VALUE=" + py_to_str(self.rbf_gamma_value) + ","
		if self.search_lambda_values != "": 
			desc += "SEARCH_LAMBDA_VALUES=" + py_to_str(self.search_lambda_values) + ","
		if self.max_basis_size != "": 
			desc += "MAX_BASIS_SIZE=" + py_to_str(self.max_basis_size) + ","
		if self.training_sample_rate != 4: 
			desc += "TRAINING_SAMPLE_RATE=" + py_to_str(self.training_sample_rate) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class Ml_dlibBinaryClassifyPredict(_graph_components.EpBase):
	"""
		

ML::DLIB_BINARY_CLASSIFY_PREDICT

Type: Other

Description: This predict
binary classification EP can be used to classify input ticks using
the trained classifier of any DLIB
library based binary classification EP.

In order to enable the event processor, please add LOAD_ML_DLIB_UDEPS=true to your
ONE_TICK_CONFIG file. Please check compatibility to see if
the EP is supported in your distribution.

Python
class name:&nbsp;Ml_dlibBinaryClassifyPredict

Input: A time series of ticks

Output: A time series of ticks with additional fields which
will carry the predicted value, the measure of confidence and
probability of being classified into positive class if ENABLE_PROBABILITY_ESTIMATES
is set.

Parameters: This EP supports the following set of parameters
(the description of the common parameters can be found here):


  OUTPUT_LABEL
(string)
  INSTANCE_NAME
(string)
  PROBABILITY_THRESHOLD
(double)

Examples: See the predict
example in ml_dlib_classify.otq.


	"""
	class Parameters:
		output_label = "OUTPUT_LABEL"
		instance_name = "INSTANCE_NAME"
		probability_threshold = "PROBABILITY_THRESHOLD"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["output_label", "instance_name", "probability_threshold"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["output_label", "_default_output_label", "instance_name", "_default_instance_name", "probability_threshold", "_default_probability_threshold", "stack_info", "_used_strings"]

	def __init__(self, output_label="", instance_name="", probability_threshold=""):
		_graph_components.EpBase.__init__(self, "ML::DLIB_BINARY_CLASSIFY_PREDICT")
		self._default_output_label = ""
		self.output_label = output_label
		self._default_instance_name = ""
		self.instance_name = instance_name
		self._default_probability_threshold = ""
		self.probability_threshold = probability_threshold
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_output_label(self, value):
		self.output_label = value
		return self

	def set_instance_name(self, value):
		self.instance_name = value
		return self

	def set_probability_threshold(self, value):
		self.probability_threshold = value
		return self

	@staticmethod
	def _get_name():
		return "ML::DLIB_BINARY_CLASSIFY_PREDICT"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.output_label != "": 
			desc += "OUTPUT_LABEL=" + py_to_str(self.output_label) + ","
		if self.instance_name != "": 
			desc += "INSTANCE_NAME=" + py_to_str(self.instance_name) + ","
		if self.probability_threshold != "": 
			desc += "PROBABILITY_THRESHOLD=" + py_to_str(self.probability_threshold) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class Ml_dlibRegressionTrainKrrWithRadialBasisKernel(_graph_components.EpBase):
	"""
		

ML::DLIB_REGRESSION_TRAIN_KRR_WITH_RADIAL_BASIS_KERNEL

Type: Other

Description: This train regression EP
uses DLIB library implementation
of KRR (Kernel Ridge Regression) algorithm for solving the
regression problem. The KRR trainer is configured to use radial basis kernel.

In order to enable the event processor, please add LOAD_ML_DLIB_UDEPS=true to your
ONE_TICK_CONFIG file. Please check compatibility to see if
the EP is supported in your distribution.

Python
class name:&nbsp;Ml_dlibRegressionTrainKrrWithRadialBasisKernel

Input: A time series of ticks containing configured feature
and output label fields.

Output:The set of regression
metrics computed over the cross validation set selected from the
input data.

Parameters: The EP supports following set of parameters, the
description of the common parameters can be found here:


  FEATURES_FIELD_NAMES
(string)
  LABEL_FIELD_NAME
(string)
  INSTANCE_NAME
(string)
  TRAINING_SAMPLE_RATE
(int)
  RBF_GAMMA_VALUE (double) - The Gamma
value for Radial Basis kernel. Intuitively, the gamma parameter defines
how far the influence of a single training example reaches, with low
values meaning 'far' and high values meaning 'close'. Default: 0.05
  SEARCH_LAMBDA_VALUES (string) -
the semicolon ';' separated list of double values indicating the range
of lambda which will be searched to find the better classifier.
  MAX_BASIS_SIZE (long) - The maximum
number of basis vectors to use..

Examples: See the krr
example in ml_dlib_regression.otq.


	"""
	class Parameters:
		features_field_names = "FEATURES_FIELD_NAMES"
		label_field_name = "LABEL_FIELD_NAME"
		instance_name = "INSTANCE_NAME"
		rbf_gamma_value = "RBF_GAMMA_VALUE"
		search_lambda_values = "SEARCH_LAMBDA_VALUES"
		max_basis_size = "MAX_BASIS_SIZE"
		training_sample_rate = "TRAINING_SAMPLE_RATE"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["features_field_names", "label_field_name", "instance_name", "rbf_gamma_value", "search_lambda_values", "max_basis_size", "training_sample_rate"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["features_field_names", "_default_features_field_names", "label_field_name", "_default_label_field_name", "instance_name", "_default_instance_name", "rbf_gamma_value", "_default_rbf_gamma_value", "search_lambda_values", "_default_search_lambda_values", "max_basis_size", "_default_max_basis_size", "training_sample_rate", "_default_training_sample_rate", "stack_info", "_used_strings"]

	def __init__(self, features_field_names="", label_field_name="", instance_name="", rbf_gamma_value="", search_lambda_values="", max_basis_size="", training_sample_rate=4):
		_graph_components.EpBase.__init__(self, "ML::DLIB_REGRESSION_TRAIN_KRR_WITH_RADIAL_BASIS_KERNEL")
		self._default_features_field_names = ""
		self.features_field_names = features_field_names
		self._default_label_field_name = ""
		self.label_field_name = label_field_name
		self._default_instance_name = ""
		self.instance_name = instance_name
		self._default_rbf_gamma_value = ""
		self.rbf_gamma_value = rbf_gamma_value
		self._default_search_lambda_values = ""
		self.search_lambda_values = search_lambda_values
		self._default_max_basis_size = ""
		self.max_basis_size = max_basis_size
		self._default_training_sample_rate = 4
		self.training_sample_rate = training_sample_rate
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_features_field_names(self, value):
		self.features_field_names = value
		return self

	def set_label_field_name(self, value):
		self.label_field_name = value
		return self

	def set_instance_name(self, value):
		self.instance_name = value
		return self

	def set_rbf_gamma_value(self, value):
		self.rbf_gamma_value = value
		return self

	def set_search_lambda_values(self, value):
		self.search_lambda_values = value
		return self

	def set_max_basis_size(self, value):
		self.max_basis_size = value
		return self

	def set_training_sample_rate(self, value):
		self.training_sample_rate = value
		return self

	@staticmethod
	def _get_name():
		return "ML::DLIB_REGRESSION_TRAIN_KRR_WITH_RADIAL_BASIS_KERNEL"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.features_field_names != "": 
			desc += "FEATURES_FIELD_NAMES=" + py_to_str(self.features_field_names) + ","
		if self.label_field_name != "": 
			desc += "LABEL_FIELD_NAME=" + py_to_str(self.label_field_name) + ","
		if self.instance_name != "": 
			desc += "INSTANCE_NAME=" + py_to_str(self.instance_name) + ","
		if self.rbf_gamma_value != "": 
			desc += "RBF_GAMMA_VALUE=" + py_to_str(self.rbf_gamma_value) + ","
		if self.search_lambda_values != "": 
			desc += "SEARCH_LAMBDA_VALUES=" + py_to_str(self.search_lambda_values) + ","
		if self.max_basis_size != "": 
			desc += "MAX_BASIS_SIZE=" + py_to_str(self.max_basis_size) + ","
		if self.training_sample_rate != 4: 
			desc += "TRAINING_SAMPLE_RATE=" + py_to_str(self.training_sample_rate) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class Ml_dlibRegressionTrainSvrWithRadialBasisKernel(_graph_components.EpBase):
	"""
		

ML::DLIB_REGRESSION_TRAIN_SVR_WITH_RADIAL_BASIS_KERNEL

Type: Other

Description: This train regression EP
uses DLIB library implementation
of SVR epsilon-insensitive support vector regression algorithm. The
SVR trainer is configured to use radial basis kernel.

In order to enable the event processor, please add LOAD_ML_DLIB_UDEPS=true to your
ONE_TICK_CONFIG file. Please check compatibility to see if
the EP is supported in your distribution.

Python
class name:
Ml_dlibRegressionTrainSvrWithRadialBasisKernel

Input: A time series of ticks containing configured feature
and output label fields.

Output:The set of regression
metrics computed over the cross validation set selected from the
input data.

Parameters: The EP supports following set of parameters, the
description of the common parameters can be found here:


  FEATURES_FIELD_NAMES
(string)
  LABEL_FIELD_NAME
(string)
  INSTANCE_NAME
(string)
  TRAINING_SAMPLE_RATE
(int)
  RBF_GAMMA_VALUE (double) - The Gamma
value for Radial Basis kernel. Intuitively, the gamma parameter defines
how far the influence of a single training example reaches, with low
values meaning 'far' and high values meaning 'close'. Default: 0.05
  EPSILION (double) - The error epsilon that
determines when training should stop. Generally a good value for this
is 0.001. Smaller values may result in a more accurate solution but
take longer to execute. Default: 0.001
  EPSILION_INSENSITIVITY
(double) - Epsilon-insensitive regression means we do regression but
stop trying to fit a data point once it is 'close enough'. And this
value defines the concept of 'close enough'. Default: 0.001
  C_CLASS (double) - The SVR regularization
parameter. It is the parameter that determines the trade-off between
trying to reduce the training error or allowing more errors but
hopefully improving the generalization of the resulting
decision_function. Larger values encourage exact fitting while smaller
values of C may encourage better generalization.

Examples: See the svr
example in ml_dlib_regression.otq.


	"""
	class Parameters:
		features_field_names = "FEATURES_FIELD_NAMES"
		label_field_name = "LABEL_FIELD_NAME"
		instance_name = "INSTANCE_NAME"
		rbf_gamma_value = "RBF_GAMMA_VALUE"
		epsilion = "EPSILION"
		epsilion_insensitivity = "EPSILION_INSENSITIVITY"
		c_class = "C_CLASS"
		training_sample_rate = "TRAINING_SAMPLE_RATE"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["features_field_names", "label_field_name", "instance_name", "rbf_gamma_value", "epsilion", "epsilion_insensitivity", "c_class", "training_sample_rate"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["features_field_names", "_default_features_field_names", "label_field_name", "_default_label_field_name", "instance_name", "_default_instance_name", "rbf_gamma_value", "_default_rbf_gamma_value", "epsilion", "_default_epsilion", "epsilion_insensitivity", "_default_epsilion_insensitivity", "c_class", "_default_c_class", "training_sample_rate", "_default_training_sample_rate", "stack_info", "_used_strings"]

	def __init__(self, features_field_names="", label_field_name="", instance_name="", rbf_gamma_value="", epsilion="0.001", epsilion_insensitivity="0.001", c_class="", training_sample_rate=4):
		_graph_components.EpBase.__init__(self, "ML::DLIB_REGRESSION_TRAIN_SVR_WITH_RADIAL_BASIS_KERNEL")
		self._default_features_field_names = ""
		self.features_field_names = features_field_names
		self._default_label_field_name = ""
		self.label_field_name = label_field_name
		self._default_instance_name = ""
		self.instance_name = instance_name
		self._default_rbf_gamma_value = ""
		self.rbf_gamma_value = rbf_gamma_value
		self._default_epsilion = "0.001"
		self.epsilion = epsilion
		self._default_epsilion_insensitivity = "0.001"
		self.epsilion_insensitivity = epsilion_insensitivity
		self._default_c_class = ""
		self.c_class = c_class
		self._default_training_sample_rate = 4
		self.training_sample_rate = training_sample_rate
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_features_field_names(self, value):
		self.features_field_names = value
		return self

	def set_label_field_name(self, value):
		self.label_field_name = value
		return self

	def set_instance_name(self, value):
		self.instance_name = value
		return self

	def set_rbf_gamma_value(self, value):
		self.rbf_gamma_value = value
		return self

	def set_epsilion(self, value):
		self.epsilion = value
		return self

	def set_epsilion_insensitivity(self, value):
		self.epsilion_insensitivity = value
		return self

	def set_c_class(self, value):
		self.c_class = value
		return self

	def set_training_sample_rate(self, value):
		self.training_sample_rate = value
		return self

	@staticmethod
	def _get_name():
		return "ML::DLIB_REGRESSION_TRAIN_SVR_WITH_RADIAL_BASIS_KERNEL"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.features_field_names != "": 
			desc += "FEATURES_FIELD_NAMES=" + py_to_str(self.features_field_names) + ","
		if self.label_field_name != "": 
			desc += "LABEL_FIELD_NAME=" + py_to_str(self.label_field_name) + ","
		if self.instance_name != "": 
			desc += "INSTANCE_NAME=" + py_to_str(self.instance_name) + ","
		if self.rbf_gamma_value != "": 
			desc += "RBF_GAMMA_VALUE=" + py_to_str(self.rbf_gamma_value) + ","
		if self.epsilion != "0.001": 
			desc += "EPSILION=" + py_to_str(self.epsilion) + ","
		if self.epsilion_insensitivity != "0.001": 
			desc += "EPSILION_INSENSITIVITY=" + py_to_str(self.epsilion_insensitivity) + ","
		if self.c_class != "": 
			desc += "C_CLASS=" + py_to_str(self.c_class) + ","
		if self.training_sample_rate != 4: 
			desc += "TRAINING_SAMPLE_RATE=" + py_to_str(self.training_sample_rate) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class Ml_dlibBinaryClassifyStreamSvmPegasosWithRadialBasisKernel(_graph_components.EpBase):
	"""
		

ML::DLIB_BINARY_CLASSIFY_STREAM_SVM_PEGASOS_WITH_RADIAL_BASIS_KERNEL

Type: Other

Description: This streaming
classification EP uses DLIB library implementation of online
SVM Pegasos (Support Vector Machine with Pegasos algorithm)
algorithm for training a support vector machine for solving binary
classification problems. This SVM training algorithm has two
interesting properties. First, the pegasos algorithm itself converges
to the solution in an amount of time unrelated to the size of the
training set (in addition to being quite fast to begin with). This
makes it an appropriate algorithm for learning from very large
datasets. Second, this object uses the kcentroid object to maintain a
sparse approximation of the learned decision function. This means that
the number of support vectors in the resulting decision function is
also unrelated to the size of the dataset (in normal SVM training
algorithms, the number of support vectors grows approximately linearly
with the size of the training set).

The SVM Pegasos trainer is configured to use radial basis kernel.
Like other binary
classification EPs it is required that the input data be labeled as
positive and negative with +1 and -1 values respectively.

In order to enable the event processor, please add LOAD_ML_DLIB_UDEPS=true to your
ONE_TICK_CONFIG file. Please check compatibility to see if
the EP is supported in your distribution.

Python
class name:&nbsp;Ml_dlibBinaryClassifyStreamSvmPegasosWithRadialBasisKernel

Input: A time series of ticks containing a filed representing
the actual (expected) classification label (-1
or +1).

Output: A time series of ticks with additional fields which
will carry the predicted value and the measure of confidence.

Parameters: The EP supports following set of parameters, the
description of the common parameters can be found here:


  FEATURES_FIELD_NAMES
(string)
  LABEL_FIELD_NAME
(string)
  OUTPUT_LABEL
(string)
  RBF_GAMMA_VALUE (double) - The Gamma
value for Radial Basis kernel. Cannot be used when optimization
objective is specified. Intuitively, the gamma parameter defines how
far the influence of a single training example reaches, with low values
meaning 'far' and high values meaning 'close'.
  LAMBDA_CLASS_1 (double) - The SVM
regularization term for the +1 class. It is the parameter that
determines the trade off between trying to fit the + 1 training data
exactly or allowing more errors but hopefully improving the
generalization ability of the resulting classifier. Smaller values
encourage exact fitting while larger values may encourage better
generalization.It is also worth noting that the number of iterations it
takes for this algorithm to converge is proportional to 1 / lambda. So
smaller values of this term cause the running time of this algorithm to
increase. Default:0.0001
  LAMBDA_CLASS_2 (double) - The SVM
regularization term for the -1 class. It has the same properties as the
    LAMBDA_CLASS1 parameter except
that it applies to the - 1 class. Default:0.0001
  MAX_NUM_SV (long) - The maximum number of
support vectors this object is allowed to use.
  TOLERANCE (double) - The tolerance used by
the internal kcentroid object to represent the learned decision
function.Smaller values of this tolerance will result in a more
accurate representation of the decision function but will use more
support vectors(up to a max of MAX_NUM_SV)

Examples: See the svm_pegasos
example in ml_dlib_classify.otq.


	"""
	class Parameters:
		features_field_names = "FEATURES_FIELD_NAMES"
		label_field_name = "LABEL_FIELD_NAME"
		output_label = "OUTPUT_LABEL"
		prediction_horizon_msec = "PREDICTION_HORIZON_MSEC"
		instance_name = "INSTANCE_NAME"
		rbf_gamma_value = "RBF_GAMMA_VALUE"
		lambda_class_1 = "LAMBDA_CLASS_1"
		lambda_class_2 = "LAMBDA_CLASS_2"
		max_num_sv = "MAX_NUM_SV"
		tolerance = "TOLERANCE"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["features_field_names", "label_field_name", "output_label", "prediction_horizon_msec", "instance_name", "rbf_gamma_value", "lambda_class_1", "lambda_class_2", "max_num_sv", "tolerance"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["features_field_names", "_default_features_field_names", "label_field_name", "_default_label_field_name", "output_label", "_default_output_label", "prediction_horizon_msec", "_default_prediction_horizon_msec", "instance_name", "_default_instance_name", "rbf_gamma_value", "_default_rbf_gamma_value", "lambda_class_1", "_default_lambda_class_1", "lambda_class_2", "_default_lambda_class_2", "max_num_sv", "_default_max_num_sv", "tolerance", "_default_tolerance", "stack_info", "_used_strings"]

	def __init__(self, features_field_names="", label_field_name="", output_label="", prediction_horizon_msec="", instance_name="", rbf_gamma_value="0.01", lambda_class_1="", lambda_class_2="", max_num_sv="", tolerance=""):
		_graph_components.EpBase.__init__(self, "ML::DLIB_BINARY_CLASSIFY_STREAM_SVM_PEGASOS_WITH_RADIAL_BASIS_KERNEL")
		self._default_features_field_names = ""
		self.features_field_names = features_field_names
		self._default_label_field_name = ""
		self.label_field_name = label_field_name
		self._default_output_label = ""
		self.output_label = output_label
		self._default_prediction_horizon_msec = ""
		self.prediction_horizon_msec = prediction_horizon_msec
		self._default_instance_name = ""
		self.instance_name = instance_name
		self._default_rbf_gamma_value = "0.01"
		self.rbf_gamma_value = rbf_gamma_value
		self._default_lambda_class_1 = ""
		self.lambda_class_1 = lambda_class_1
		self._default_lambda_class_2 = ""
		self.lambda_class_2 = lambda_class_2
		self._default_max_num_sv = ""
		self.max_num_sv = max_num_sv
		self._default_tolerance = ""
		self.tolerance = tolerance
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_features_field_names(self, value):
		self.features_field_names = value
		return self

	def set_label_field_name(self, value):
		self.label_field_name = value
		return self

	def set_output_label(self, value):
		self.output_label = value
		return self

	def set_prediction_horizon_msec(self, value):
		self.prediction_horizon_msec = value
		return self

	def set_instance_name(self, value):
		self.instance_name = value
		return self

	def set_rbf_gamma_value(self, value):
		self.rbf_gamma_value = value
		return self

	def set_lambda_class_1(self, value):
		self.lambda_class_1 = value
		return self

	def set_lambda_class_2(self, value):
		self.lambda_class_2 = value
		return self

	def set_max_num_sv(self, value):
		self.max_num_sv = value
		return self

	def set_tolerance(self, value):
		self.tolerance = value
		return self

	@staticmethod
	def _get_name():
		return "ML::DLIB_BINARY_CLASSIFY_STREAM_SVM_PEGASOS_WITH_RADIAL_BASIS_KERNEL"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.features_field_names != "": 
			desc += "FEATURES_FIELD_NAMES=" + py_to_str(self.features_field_names) + ","
		if self.label_field_name != "": 
			desc += "LABEL_FIELD_NAME=" + py_to_str(self.label_field_name) + ","
		if self.output_label != "": 
			desc += "OUTPUT_LABEL=" + py_to_str(self.output_label) + ","
		if self.prediction_horizon_msec != "": 
			desc += "PREDICTION_HORIZON_MSEC=" + py_to_str(self.prediction_horizon_msec) + ","
		if self.instance_name != "": 
			desc += "INSTANCE_NAME=" + py_to_str(self.instance_name) + ","
		if self.rbf_gamma_value != "0.01": 
			desc += "RBF_GAMMA_VALUE=" + py_to_str(self.rbf_gamma_value) + ","
		if self.lambda_class_1 != "": 
			desc += "LAMBDA_CLASS_1=" + py_to_str(self.lambda_class_1) + ","
		if self.lambda_class_2 != "": 
			desc += "LAMBDA_CLASS_2=" + py_to_str(self.lambda_class_2) + ","
		if self.max_num_sv != "": 
			desc += "MAX_NUM_SV=" + py_to_str(self.max_num_sv) + ","
		if self.tolerance != "": 
			desc += "TOLERANCE=" + py_to_str(self.tolerance) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class Ml_dlibRegressionStreamKrlsWithRadialBasisKernel(_graph_components.EpBase):
	"""
		

ML::DLIB_REGRESSION_STREAM_KRLS_WITH_RADIAL_BASIS_KERNEL

Type: Other

Description: This streaming regression EP
uses DLIB library implementation
of KRLS kernel recursive least squares algorithm for solving the
on-line regression problem. The KRLS trainer is configured to use radial basis kernel.

In order to enable the event processor, please add LOAD_ML_DLIB_UDEPS=true to your
ONE_TICK_CONFIG file. Please check compatibility to see if
the EP is supported in your distribution.

Python
class name:&nbsp;Ml_dlibRegressionStreamKrlsWithRadialBasisKernel

Input: A time series of ticks containing configured feature
and output label fields.

Output: A time series of ticks with additional field which
will carry the predicted value.

Parameters: The EP supports following set of parameters, the
description of the common parameters can be found here:


  FEATURES_FIELD_NAMES
(string)
  LABEL_FIELD_NAME
(string)
  OUTPUT_LABEL
(string)
  PREDICTION_HORIZON_MSEC
(long)
  RBF_GAMMA_VALUE (double) - The Gamma
value for Radial Basis kernel. Intuitively, the gamma parameter defines
how far the influence of a single training example reaches, with low
values meaning 'far' and high values meaning 'close'. Default: 0.01
  TOLERANCE (double) - The tolerance to use
for the approximately linearly dependent test in the KRLS algorithm.
This is a number which governs how accurately this object will
approximate the decision function it is learning. Smaller values
generally result in a more accurate estimate while also resulting in a
bigger set of dictionary vectors in the learned decision function.
Default: 0.001.
  MAX_DICTIONARY_SIZE (long) - The
maximum number of dictionary vectors this object will use at a time.
Default: 1000000

Examples: See the krls
example in ml_dlib_regression.otq.


	"""
	class Parameters:
		features_field_names = "FEATURES_FIELD_NAMES"
		label_field_name = "LABEL_FIELD_NAME"
		output_label = "OUTPUT_LABEL"
		prediction_horizon_msec = "PREDICTION_HORIZON_MSEC"
		instance_name = "INSTANCE_NAME"
		rbf_gamma_value = "RBF_GAMMA_VALUE"
		tolerance = "TOLERANCE"
		max_dictionary_size = "MAX_DICTIONARY_SIZE"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["features_field_names", "label_field_name", "output_label", "prediction_horizon_msec", "instance_name", "rbf_gamma_value", "tolerance", "max_dictionary_size"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["features_field_names", "_default_features_field_names", "label_field_name", "_default_label_field_name", "output_label", "_default_output_label", "prediction_horizon_msec", "_default_prediction_horizon_msec", "instance_name", "_default_instance_name", "rbf_gamma_value", "_default_rbf_gamma_value", "tolerance", "_default_tolerance", "max_dictionary_size", "_default_max_dictionary_size", "stack_info", "_used_strings"]

	def __init__(self, features_field_names="", label_field_name="", output_label="", prediction_horizon_msec="", instance_name="", rbf_gamma_value="0.01", tolerance="0.001", max_dictionary_size=1000000):
		_graph_components.EpBase.__init__(self, "ML::DLIB_REGRESSION_STREAM_KRLS_WITH_RADIAL_BASIS_KERNEL")
		self._default_features_field_names = ""
		self.features_field_names = features_field_names
		self._default_label_field_name = ""
		self.label_field_name = label_field_name
		self._default_output_label = ""
		self.output_label = output_label
		self._default_prediction_horizon_msec = ""
		self.prediction_horizon_msec = prediction_horizon_msec
		self._default_instance_name = ""
		self.instance_name = instance_name
		self._default_rbf_gamma_value = "0.01"
		self.rbf_gamma_value = rbf_gamma_value
		self._default_tolerance = "0.001"
		self.tolerance = tolerance
		self._default_max_dictionary_size = 1000000
		self.max_dictionary_size = max_dictionary_size
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_features_field_names(self, value):
		self.features_field_names = value
		return self

	def set_label_field_name(self, value):
		self.label_field_name = value
		return self

	def set_output_label(self, value):
		self.output_label = value
		return self

	def set_prediction_horizon_msec(self, value):
		self.prediction_horizon_msec = value
		return self

	def set_instance_name(self, value):
		self.instance_name = value
		return self

	def set_rbf_gamma_value(self, value):
		self.rbf_gamma_value = value
		return self

	def set_tolerance(self, value):
		self.tolerance = value
		return self

	def set_max_dictionary_size(self, value):
		self.max_dictionary_size = value
		return self

	@staticmethod
	def _get_name():
		return "ML::DLIB_REGRESSION_STREAM_KRLS_WITH_RADIAL_BASIS_KERNEL"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.features_field_names != "": 
			desc += "FEATURES_FIELD_NAMES=" + py_to_str(self.features_field_names) + ","
		if self.label_field_name != "": 
			desc += "LABEL_FIELD_NAME=" + py_to_str(self.label_field_name) + ","
		if self.output_label != "": 
			desc += "OUTPUT_LABEL=" + py_to_str(self.output_label) + ","
		if self.prediction_horizon_msec != "": 
			desc += "PREDICTION_HORIZON_MSEC=" + py_to_str(self.prediction_horizon_msec) + ","
		if self.instance_name != "": 
			desc += "INSTANCE_NAME=" + py_to_str(self.instance_name) + ","
		if self.rbf_gamma_value != "0.01": 
			desc += "RBF_GAMMA_VALUE=" + py_to_str(self.rbf_gamma_value) + ","
		if self.tolerance != "0.001": 
			desc += "TOLERANCE=" + py_to_str(self.tolerance) + ","
		if self.max_dictionary_size != 1000000: 
			desc += "MAX_DICTIONARY_SIZE=" + py_to_str(self.max_dictionary_size) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class Ml_dlibRegressionStreamRls(_graph_components.EpBase):
	"""
		

ML::DLIB_REGRESSION_STREAM_RLS

Type: Other

Description: This streaming regression EP
uses DLIB library implementation
of RLS recursive least squares algorithm for solving the on-line
regression problem.

In order to enable the event processor, please add LOAD_ML_DLIB_UDEPS=true to your
ONE_TICK_CONFIG file. Please check compatibility to see if
the EP is supported in your distribution.

Python
class name:&nbsp;Ml_dlibRegressionStreamRls

Input: A time series of ticks containing configured feature
and output label fields.

Output: A time series of ticks with additional field which
will carry the predicted value.

Parameters: The EP supports following set of parameters, the
description of the common parameters can be found here:


  FEATURES_FIELD_NAMES
(string)
  LABEL_FIELD_NAME
(string)
  OUTPUT_LABEL
(string)
  PREDICTION_HORIZON_MSEC
(long)
  FORGET_FACTOR (double) - The
exponential forgetting factor. A value of 1 disables forgetting and
results in normal least squares regression. On the other hand, a
smaller value causes the regression to forget about old training
examples and prefer instead to fit more recent examples. The closer the
forget factor is to zero the faster old examples are forgotten.
Default: 1.0
  C_CLASS (double) - The regularization
parameter. It is the parameter that determines the trade-off between
trying to fit the training data or allowing more errors but hopefully
improving the generalization of the resulting regression. Larger values
encourage exact fitting while smaller values of C may encourage better
generalization. Default: 1000.0.

Examples: See the rls
example in ml_dlib_regression.otq.


	"""
	class Parameters:
		features_field_names = "FEATURES_FIELD_NAMES"
		label_field_name = "LABEL_FIELD_NAME"
		output_label = "OUTPUT_LABEL"
		prediction_horizon_msec = "PREDICTION_HORIZON_MSEC"
		instance_name = "INSTANCE_NAME"
		forget_factor = "FORGET_FACTOR"
		c_class = "C_CLASS"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["features_field_names", "label_field_name", "output_label", "prediction_horizon_msec", "instance_name", "forget_factor", "c_class"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["features_field_names", "_default_features_field_names", "label_field_name", "_default_label_field_name", "output_label", "_default_output_label", "prediction_horizon_msec", "_default_prediction_horizon_msec", "instance_name", "_default_instance_name", "forget_factor", "_default_forget_factor", "c_class", "_default_c_class", "stack_info", "_used_strings"]

	def __init__(self, features_field_names="", label_field_name="", output_label="", prediction_horizon_msec="", instance_name="", forget_factor="1.0", c_class="1000.0"):
		_graph_components.EpBase.__init__(self, "ML::DLIB_REGRESSION_STREAM_RLS")
		self._default_features_field_names = ""
		self.features_field_names = features_field_names
		self._default_label_field_name = ""
		self.label_field_name = label_field_name
		self._default_output_label = ""
		self.output_label = output_label
		self._default_prediction_horizon_msec = ""
		self.prediction_horizon_msec = prediction_horizon_msec
		self._default_instance_name = ""
		self.instance_name = instance_name
		self._default_forget_factor = "1.0"
		self.forget_factor = forget_factor
		self._default_c_class = "1000.0"
		self.c_class = c_class
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_features_field_names(self, value):
		self.features_field_names = value
		return self

	def set_label_field_name(self, value):
		self.label_field_name = value
		return self

	def set_output_label(self, value):
		self.output_label = value
		return self

	def set_prediction_horizon_msec(self, value):
		self.prediction_horizon_msec = value
		return self

	def set_instance_name(self, value):
		self.instance_name = value
		return self

	def set_forget_factor(self, value):
		self.forget_factor = value
		return self

	def set_c_class(self, value):
		self.c_class = value
		return self

	@staticmethod
	def _get_name():
		return "ML::DLIB_REGRESSION_STREAM_RLS"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.features_field_names != "": 
			desc += "FEATURES_FIELD_NAMES=" + py_to_str(self.features_field_names) + ","
		if self.label_field_name != "": 
			desc += "LABEL_FIELD_NAME=" + py_to_str(self.label_field_name) + ","
		if self.output_label != "": 
			desc += "OUTPUT_LABEL=" + py_to_str(self.output_label) + ","
		if self.prediction_horizon_msec != "": 
			desc += "PREDICTION_HORIZON_MSEC=" + py_to_str(self.prediction_horizon_msec) + ","
		if self.instance_name != "": 
			desc += "INSTANCE_NAME=" + py_to_str(self.instance_name) + ","
		if self.forget_factor != "1.0": 
			desc += "FORGET_FACTOR=" + py_to_str(self.forget_factor) + ","
		if self.c_class != "1000.0": 
			desc += "C_CLASS=" + py_to_str(self.c_class) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class Ml_dlibClusteringKMeansWithRadialBasisKernel(_graph_components.EpBase):
	"""
		

ML::DLIB_CLUSTERING_K_MEANS_WITH_RADIAL_BASIS_KERNEL

Type: Other

Description: This clustering EP uses DLIB library
implementation of a kernelized
k-means clustering algorithm. The k-means clustering algorithm is
configured to use radial
basis kernel.

In order to enable the event processor, please add LOAD_ML_DLIB_UDEPS=true to your
ONE_TICK_CONFIG file. Please check compatibility to see if
the EP is supported in your distribution.

Python
class name:&nbsp;Ml_dlibClusteringKMeansWithRadialBasisKernel

Input: A time series of ticks containing configured feature
fields.

Output: A time series of ticks with additional field which
will carry cluster identifier.

Parameters: The EP supports following set of parameters, the
description of the common parameters can be found here:


  FEATURES_FIELD_NAMES
(string)
  RBF_GAMMA_VALUE (double) - The Gamma
value for Radial Basis kernel. Intuitively, the gamma parameter defines
how far the influence of a single training example reaches, with low
values meaning 'far' and high values meaning 'close'. Default: 0.01
  MAX_DICTIONARY_SIZE (long) - The
maximum number of dictionary vectors this object will use at a time.
Default: 1000000
  NUM_CLUSTERS (int) - The desired number
of clusters. Default: 2

Examples: See the k_means
example in ml_dlib_clusterization.otq.


	"""
	class Parameters:
		features_field_names = "FEATURES_FIELD_NAMES"
		output_label = "OUTPUT_LABEL"
		rbf_gamma_value = "RBF_GAMMA_VALUE"
		max_dictionary_size = "MAX_DICTIONARY_SIZE"
		num_clusters = "NUM_CLUSTERS"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["features_field_names", "output_label", "rbf_gamma_value", "max_dictionary_size", "num_clusters"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["features_field_names", "_default_features_field_names", "output_label", "_default_output_label", "rbf_gamma_value", "_default_rbf_gamma_value", "max_dictionary_size", "_default_max_dictionary_size", "num_clusters", "_default_num_clusters", "stack_info", "_used_strings"]

	def __init__(self, features_field_names="", output_label="", rbf_gamma_value="0.01", max_dictionary_size=1000000, num_clusters=2):
		_graph_components.EpBase.__init__(self, "ML::DLIB_CLUSTERING_K_MEANS_WITH_RADIAL_BASIS_KERNEL")
		self._default_features_field_names = ""
		self.features_field_names = features_field_names
		self._default_output_label = ""
		self.output_label = output_label
		self._default_rbf_gamma_value = "0.01"
		self.rbf_gamma_value = rbf_gamma_value
		self._default_max_dictionary_size = 1000000
		self.max_dictionary_size = max_dictionary_size
		self._default_num_clusters = 2
		self.num_clusters = num_clusters
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_features_field_names(self, value):
		self.features_field_names = value
		return self

	def set_output_label(self, value):
		self.output_label = value
		return self

	def set_rbf_gamma_value(self, value):
		self.rbf_gamma_value = value
		return self

	def set_max_dictionary_size(self, value):
		self.max_dictionary_size = value
		return self

	def set_num_clusters(self, value):
		self.num_clusters = value
		return self

	@staticmethod
	def _get_name():
		return "ML::DLIB_CLUSTERING_K_MEANS_WITH_RADIAL_BASIS_KERNEL"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.features_field_names != "": 
			desc += "FEATURES_FIELD_NAMES=" + py_to_str(self.features_field_names) + ","
		if self.output_label != "": 
			desc += "OUTPUT_LABEL=" + py_to_str(self.output_label) + ","
		if self.rbf_gamma_value != "0.01": 
			desc += "RBF_GAMMA_VALUE=" + py_to_str(self.rbf_gamma_value) + ","
		if self.max_dictionary_size != 1000000: 
			desc += "MAX_DICTIONARY_SIZE=" + py_to_str(self.max_dictionary_size) + ","
		if self.num_clusters != 2: 
			desc += "NUM_CLUSTERS=" + py_to_str(self.num_clusters) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


class Omd_readFromUrl(_graph_components.EpBase):
	"""
		

OMD::READ_FROM_URL

Type: Other

Description: Reads data from arbitrary web sites and
propagates the resulting set of tuples as a time series of ticks. A
time series is generated for every symbol of the graph/chain query. The
whole content of a website is propagated as a single tick if no
separator is specified and the maximum output chunk size is 0. The timestamps of all ticks are equal
to the query start time.

To enable this feature, please add the following line to the
one_tick_config file:

LOAD_READ_FROM_URL_UDF=true

Python
class name:&nbsp;Omd_readFromUrl

Input: None

Output: A time series of ticks.

Parameters:


  URL (string)
    URL of a website, from where we are going to read data.

  
  MAX_OUTPUT_CHUNK_SIZE (integer)
    Amount of data (in bytes) that will be carried in a single
output tick.

    Default: 0. In this case, all
data between separators are delivered in a single tick.

  
  SEPARATOR (string)
    If a separator character is specified, a new chunk is created
after each continuous sequence of a separator character.

  
  REQUEST_HEADER_FIELDS (string)
    Name and value pairs of HTTP request header fields with the
following form:

    FIELD_1=VALUE_1, FIELD_2=VALUE_2, &hellip; ,FIELD_N=VALUE_N
  
  HTTP_METHOD (string)
    Default: GET. HTTP method
type for request

    Currently the following HTTP request types are supported:

    
      POST
        Do a POST HTTP request.

      
      PUT
        Do a PUT HTTP request.

      
      GET
        Do a GET HTTP request.

      
      HEAD
        Do a HEAD HTTP request.

      
    
  
  REQUEST_BODY (string)
    Body text for request. Allowed only for POST and PUT.

  
  IP_V6_CONNECTION_WAIT_TIME_MSEC
(integer)
    Head start for ipv6 for happy eyeballs.

    Default: 0.

  
  Usage example:
OMD::READ_FROM_URL(URL="http://examplehost.com/post",REQUEST_HEADER_FIELDS="Content-type=application/json,Content-length=100,HTTP_METHOD=POST,REQUEST_BODY=TEST_DATAAAA)

  Authentication:
REQUEST_HEADER_FIELDS can be used for authentication, as shown in the
example below that uses OAuth authentication:

  REQUEST_HEADER_FIELDS: Authorization=Bearer ${accessToken}



	"""
	class Parameters:
		url = "URL"
		request_header_fields = "REQUEST_HEADER_FIELDS"
		max_output_chunk_size = "MAX_OUTPUT_CHUNK_SIZE"
		separator = "SEPARATOR"
		http_method = "HTTP_METHOD"
		request_body = "REQUEST_BODY"
		ip_v6_connection_wait_time_msec = "IP_V6_CONNECTION_WAIT_TIME_MSEC"
		stack_info = "STACK_INFO"

		@staticmethod
		def list_parameters():
			list_val = ["url", "request_header_fields", "max_output_chunk_size", "separator", "http_method", "request_body", "ip_v6_connection_wait_time_msec"]
			if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
				list_val.append("stack_info")
			return list_val

	__slots__ = ["url", "_default_url", "request_header_fields", "_default_request_header_fields", "max_output_chunk_size", "_default_max_output_chunk_size", "separator", "_default_separator", "http_method", "_default_http_method", "request_body", "_default_request_body", "ip_v6_connection_wait_time_msec", "_default_ip_v6_connection_wait_time_msec", "stack_info", "_used_strings"]

	class HttpMethod:
		GET = "GET"
		HEAD = "HEAD"
		POST = "POST"
		PUT = "PUT"

	def __init__(self, url="", request_header_fields="", max_output_chunk_size=0, separator="", http_method=HttpMethod.GET, request_body="", ip_v6_connection_wait_time_msec=""):
		_graph_components.EpBase.__init__(self, "OMD::READ_FROM_URL")
		self._default_url = ""
		self.url = url
		self._default_request_header_fields = ""
		self.request_header_fields = request_header_fields
		self._default_max_output_chunk_size = 0
		self.max_output_chunk_size = max_output_chunk_size
		self._default_separator = ""
		self.separator = separator
		self._default_http_method = type(self).HttpMethod.GET
		self.http_method = http_method
		self._default_request_body = ""
		self.request_body = request_body
		self._default_ip_v6_connection_wait_time_msec = ""
		self.ip_v6_connection_wait_time_msec = ip_v6_connection_wait_time_msec
		self._used_strings = {}
		for param_name in type(self).__dict__:
			param_val = getattr(self, param_name, '')
			if isinstance(param_val, str) and _internal_utils.get_reference_counted_prefix() in param_val and not (param_val in self._used_strings):
				_internal_utils.inc_ref_count(param_val)
				self._used_strings[param_val] = 1
		import sys
		frame = sys._getframe(1)
		self.stack_info = frame.f_code.co_filename + ":" + str(frame.f_lineno)

	def set_url(self, value):
		self.url = value
		return self

	def set_request_header_fields(self, value):
		self.request_header_fields = value
		return self

	def set_max_output_chunk_size(self, value):
		self.max_output_chunk_size = value
		return self

	def set_separator(self, value):
		self.separator = value
		return self

	def set_http_method(self, value):
		self.http_method = value
		return self

	def set_request_body(self, value):
		self.request_body = value
		return self

	def set_ip_v6_connection_wait_time_msec(self, value):
		self.ip_v6_connection_wait_time_msec = value
		return self

	@staticmethod
	def _get_name():
		return "OMD::READ_FROM_URL"

	def _to_string(self, for_repr=False):
		name = self._get_name()
		desc = name + "("
		py_to_str = _utils.onetick_repr if for_repr else str
		if self.url != "": 
			desc += "URL=" + py_to_str(self.url) + ","
		if self.request_header_fields != "": 
			desc += "REQUEST_HEADER_FIELDS=" + py_to_str(self.request_header_fields) + ","
		if self.max_output_chunk_size != 0: 
			desc += "MAX_OUTPUT_CHUNK_SIZE=" + py_to_str(self.max_output_chunk_size) + ","
		if self.separator != "": 
			desc += "SEPARATOR=" + py_to_str(self.separator) + ","
		if self.http_method != self.HttpMethod.GET: 
			desc += "HTTP_METHOD=" + py_to_str(self.http_method) + ","
		if self.request_body != "": 
			desc += "REQUEST_BODY=" + py_to_str(self.request_body) + ","
		if self.ip_v6_connection_wait_time_msec != "": 
			desc += "IP_V6_CONNECTION_WAIT_TIME_MSEC=" + py_to_str(self.ip_v6_connection_wait_time_msec) + ","
		if _config.API_CONFIG['SHOW_STACK_INFO'] == 1:
			desc += "STACK_INFO=" + py_to_str(self.stack_info) + ","
		desc = desc[:-1]
		if desc != name:
			desc += ")"
		if for_repr:
			return desc + '()' if desc == name else desc
		desc += "\n"
		if len(self._get_symbol_strings()) > 0:
			desc += "SYMBOLS=[" + ", ".join(self._get_symbol_strings()) + "]\n"
		if len(self.tick_types_) > 0:
			desc += "TICK_TYPES=[" + ', '.join(self.tick_types_) + "]\n"
		if self.process_node_locally_:
			desc += "PROCESS_NODE_LOCALLY=True\n"
		if self.node_name_ != "":
			desc += "NODE_NAME=" + self.node_name_ + "\n"
		return desc
	
	def __repr__(self):
		return self._to_string(for_repr=True)
	
	def __str__(self):
		return self._to_string()

	def __del__(self):
		for param_name in getattr(self, '_used_strings', []):
			_internal_utils.dec_ref_count(param_name)
			if _internal_utils.get_ref_count(param_name) == 0:
				_internal_utils.remove_from_memory(param_name)


