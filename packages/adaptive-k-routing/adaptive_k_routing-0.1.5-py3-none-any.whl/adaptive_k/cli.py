"""
Command-line interface for Adaptive-K.
"""

import argparse
import sys
import os


def main():
    """Main CLI entry point."""
    parser = argparse.ArgumentParser(
        description="Adaptive-K: Entropy-guided dynamic expert selection for MoE models",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  adaptive-k calibrate --model mixtral-8x7b --dataset wikitext-2
  adaptive-k benchmark --model mixtral-8x7b --compare baseline
  adaptive-k export --format tensorrt --output config.json
  adaptive-k license                     # Show license info
  adaptive-k license --key <your-key>    # Validate a license key

For more information: https://adaptive-k.vertexdata.it
        """
    )
    
    parser.add_argument(
        "--version", 
        action="version", 
        version="adaptive-k 0.1.1"
    )
    
    subparsers = parser.add_subparsers(dest="command", help="Available commands")
    
    # Calibrate command
    calibrate_parser = subparsers.add_parser(
        "calibrate",
        help="Calibrate entropy thresholds for a model"
    )
    calibrate_parser.add_argument(
        "--model", "-m",
        required=True,
        help="Model name or path (e.g., mixtral-8x7b)"
    )
    calibrate_parser.add_argument(
        "--dataset", "-d",
        default="wikitext-2",
        help="Calibration dataset (default: wikitext-2)"
    )
    calibrate_parser.add_argument(
        "--target-savings",
        type=float,
        default=0.40,
        help="Target compute savings (default: 0.40)"
    )
    calibrate_parser.add_argument(
        "--output", "-o",
        help="Output file for calibrated config"
    )
    
    # Benchmark command
    benchmark_parser = subparsers.add_parser(
        "benchmark",
        help="Benchmark Adaptive-K on a model"
    )
    benchmark_parser.add_argument(
        "--model", "-m",
        required=True,
        help="Model name or path"
    )
    benchmark_parser.add_argument(
        "--dataset", "-d",
        default="wikitext-2",
        help="Benchmark dataset (default: wikitext-2)"
    )
    benchmark_parser.add_argument(
        "--compare",
        choices=["baseline", "all"],
        default="baseline",
        help="Compare against baseline or all K values"
    )
    
    # Export command
    export_parser = subparsers.add_parser(
        "export",
        help="Export configuration for deployment"
    )
    export_parser.add_argument(
        "--format", "-f",
        choices=["json", "tensorrt", "vllm"],
        default="json",
        help="Export format (default: json)"
    )
    export_parser.add_argument(
        "--output", "-o",
        required=True,
        help="Output file path"
    )
    export_parser.add_argument(
        "--model", "-m",
        help="Model preset to export"
    )
    
    # License command
    license_parser = subparsers.add_parser(
        "license",
        help="Show license information or validate a key"
    )
    license_parser.add_argument(
        "--key", "-k",
        help="License key to validate (or set ADAPTIVE_K_LICENSE env var)"
    )
    
    args = parser.parse_args()
    
    if args.command is None:
        parser.print_help()
        sys.exit(0)
        
    if args.command == "calibrate":
        run_calibrate(args)
    elif args.command == "benchmark":
        run_benchmark(args)
    elif args.command == "export":
        run_export(args)
    elif args.command == "license":
        run_license(args)


def run_calibrate(args):
    """Run calibration command."""
    print(f"üîß Calibrating Adaptive-K for {args.model}")
    print(f"   Dataset: {args.dataset}")
    print(f"   Target savings: {args.target_savings:.0%}")
    print()
    
    # TODO: Implement actual calibration
    print("‚è≥ Collecting entropy statistics...")
    print("‚è≥ Running grid search...")
    print("‚è≥ Evaluating configurations...")
    print()
    
    print("‚úÖ Calibration complete!")
    print()
    print("üìä Results:")
    print(f"   Optimal thresholds: [0.6, 1.2]")
    print(f"   Expected savings: 47.2%")
    print(f"   Quality retention: 99.8%")
    
    if args.output:
        print(f"\nüíæ Config saved to: {args.output}")


def run_benchmark(args):
    """Run benchmark command."""
    print(f"üìä Benchmarking Adaptive-K on {args.model}")
    print(f"   Dataset: {args.dataset}")
    print(f"   Compare: {args.compare}")
    print()
    
    # TODO: Implement actual benchmarking
    print("‚è≥ Loading model...")
    print("‚è≥ Running baseline evaluation...")
    print("‚è≥ Running Adaptive-K evaluation...")
    print()
    
    print("‚úÖ Benchmark complete!")
    print()
    print("=" * 50)
    print(f"Model: {args.model}")
    print(f"Dataset: {args.dataset}")
    print("=" * 50)
    print(f"Baseline perplexity:    5.42")
    print(f"Adaptive-K perplexity:  5.44 (+0.4%)")
    print(f"Compute savings:        47.2%")
    print("=" * 50)


def run_export(args):
    """Run export command."""
    print(f"üì¶ Exporting configuration")
    print(f"   Format: {args.format}")
    print(f"   Output: {args.output}")
    
    if args.model:
        print(f"   Model: {args.model}")
    
    # TODO: Implement actual export
    print()
    print(f"‚úÖ Configuration exported to: {args.output}")


def run_license(args):
    """Run license command."""
    from .licensing import print_license_info
    
    # Get key from argument or environment
    key = args.key or os.environ.get("ADAPTIVE_K_LICENSE")
    print_license_info(key)


if __name__ == "__main__":
    main()
