# Routing profile library - Routes models to their backends
# =========================================================================================
# This file controls which backend serves which model.
# Simply change the 'active' field to switch profiles,
# or you can add your own custom profiles.
#
# Documentation: https://docs.pipelex.com
# Support: https://go.pipelex.com/discord
# =========================================================================================

# Which profile to use (change this to switch routing)
active = "pipelex_gateway_first"

# We recommend using the "pipelex_gateway_first" profile to get a head start with all models.
# To use the Pipelex Gateway backend:
# 1. Join our Discord community to get your free API key (no credit card required):
#    Visit https://go.pipelex.com/discord and request your key in the appropriate channel
# 2. Set the environment variable (or add it to your .env file):
#    - Linux/macOS: export PIPELEX_GATEWAY_API_KEY="your-api-key"
#    - Windows CMD: set PIPELEX_GATEWAY_API_KEY=your-api-key
#    - Windows PowerShell: $env:PIPELEX_GATEWAY_API_KEY="your-api-key"
# 3. The .pipelex/inference/backends.toml is already configured with api_key = "${PIPELEX_GATEWAY_API_KEY}"
#    which will get the key from the environment variable.

# =========================================================================================
# Routing Profiles
# =========================================================================================

[profiles.pipelex_gateway_first]
description = "Use Pipelex Gateway backend for all its supported models"
default = "pipelex_gateway"
fallback_order = [
    "pipelex_gateway",
    "azure_openai",
    "bedrock",
    "google",
    "blackboxai",
    "mistral",
    "fal",
]

[profiles.pipelex_gateway_first.routes]
# Pattern matching: "model-pattern" = "backend-name"

[profiles.pipelex_gateway_first.optional_routes] # Each optional route is considered only if its backend is available
"gpt-*" = "pipelex_gateway"
"gpt-image-1" = "openai"
"claude-*" = "pipelex_gateway"
"grok-*" = "pipelex_gateway"
"gemini-*" = "pipelex_gateway"
"*-sdxl" = "fal"
"flux-*" = "fal"
"mistral-ocr" = "mistral"

[profiles.all_pipelex_gateway]
description = "Use Pipelex Gateway for all its supported models"
default = "pipelex_gateway"

[profiles.all_anthropic]
description = "Use Anthropic backend for all its supported models"
default = "anthropic"

[profiles.all_azure_openai]
description = "Use Azure OpenAI backend for all its supported models"
default = "azure_openai"

[profiles.all_bedrock]
description = "Use Bedrock backend for all its supported models"
default = "bedrock"

[profiles.all_blackboxai]
description = "Use BlackBoxAI backend for all its supported models"
default = "blackboxai"

[profiles.all_fal]
description = "Use FAL backend for all its supported models"
default = "fal"

[profiles.all_google]
description = "Use Google GenAI backend for all its supported models"
default = "google"

[profiles.all_groq]
description = "Use groq backend for all its supported models"
default = "groq"

[profiles.all_huggingface]
description = "Use HuggingFace backend for all its supported models"
default = "huggingface"

[profiles.all_mistral]
description = "Use Mistral backend for all its supported models"
default = "mistral"

[profiles.all_ollama]
description = "Use Ollama backend for all its supported models"
default = "ollama"

[profiles.all_openai]
description = "Use OpenAI backend for all its supported models"
default = "openai"

[profiles.all_portkey]
description = "Use Portkey backend for all its supported models"
default = "portkey"

[profiles.all_scaleway]
description = "Use Scaleway backend for all its supported models"
default = "scaleway"

[profiles.all_xai]
description = "Use xAI backend for all its supported models"
default = "xai"

[profiles.all_internal]
description = "Use internal backend for all its supported models"
default = "internal"

# =========================================================================================
# Custom Profiles
# =========================================================================================
# Add your own profiles below following the same pattern:
#
# [profiles.your_profile_name]
# description = "What this profile does"
# default = "backend-name"  # Where to route models by default
# [profiles.your_profile_name.routes]
# "model-pattern" = "backend-name"  # Specific routing rules
#
# Pattern matching supports:
# - Exact names: "gpt-4o-mini"
# - Wildcards: "claude-*" (matches all models starting with claude-)
# - Partial wildcards: "*-sonnet" (matches all sonnet variants)

# =========================================================================================
# Example of a custom routing profile with mostly pattern matching and one specific model
# =========================================================================================
[profiles.example_routing_using_patterns]
description = "Example routing profile using patterns"
default = "pipelex_gateway"

[profiles.example_routing_using_patterns.routes]
# Pattern matching: "model-pattern" = "backend-name"
"gpt-*" = "azure_openai"
"claude-*" = "bedrock"
"gemini-*" = "google"
"grok-*" = "xai"
"*-sdxl" = "fal"
"flux-*" = "fal"
"gpt-image-1" = "openai"

# =========================================================================================
# Example of a custom routing profile with specific model matching
# =========================================================================================

[profiles.example_routing_using_specific_models]
description = "Example routing profile using specific models"

[profiles.example_routing_using_specific_models.routes]
"gpt-5-nano" = "pipelex_gateway"
"gpt-4o-mini" = "blackboxai"
"gpt-5-mini" = "openai"
"gpt-5-chat" = "azure_openai"

"claude-4-sonnet" = "pipelex_gateway"
"claude-3.7-sonnet" = "blackboxai"

"gemini-2.5-flash-lite" = "pipelex_gateway"
"gemini-2.5-flash" = "blackboxai"
"gemini-2.5-pro" = "vertexai"

"grok-3" = "pipelex_gateway"
"grok-3-mini" = "xai"
