from matplotlib.patches import Patch
from collections import Counter
from graphviz import Digraph
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
#######################################################################################################################
# Реализация дерева решений
#######################################################################################################################
class DecisionTree:
    """
    Реализация дерева решений для задач классификации и регрессии.
    """
    def __init__(self,model_type = 'Classifier', max_depth=None, min_samples_leaf=1, max_leaves=None, criterion="gini", check_k_threshold=0, min_gain=0.01, class_weights=None, chek_k_features=0):
        """
        Инициализация дерева решений.

        Args:
            model_type (str, optional): Тип модели: 'Classifier' или 'Regressor'. Defaults to 'Classifier'.
            max_depth (int, optional): Максимальная глубина дерева. Defaults to None.
            min_samples_leaf (int, optional): Минимальное количество объектов в листе. Defaults to 1.
            max_leaves (int, optional): Максимальное количество листьев. Defaults to None.
            criterion (str, optional): Критерий для оценки качества разбиения.
                Для классификации: 'gini', 'entropy', 'misclassification'.
                Для регрессии: 'mse', 'mae'.
                Defaults to "gini".
            check_k_threshold (int, optional): Количество порогов для проверки при поиске лучшего разбиения.
                Если 0, проверяются все уникальные значения. Defaults to 0.
            min_gain (float, optional): Минимальное улучшение качества разбиения для совершения разделения.
                Defaults to 0.01.
            class_weights (dict, optional): Веса для классов в формате {class: weight}.
                Используется только для классификации. Defaults to None.
            chek_k_features (int, optional): Количество случайных признаков для проверки при каждом разделении.
                Если 0, используются все признаки. Defaults to 0.
        """
        self.model_type = model_type
        self.max_depth = max_depth
        self.min_samples_leaf = min_samples_leaf
        self.max_leaves = max_leaves
        
        self.criterion = criterion.lower()  # Приводим к нижнему регистру для унификации
        assert (model_type=='Regressor' and criterion in ['mse','mae']) or (model_type=='Classifier' and criterion in ['gini','entropy','misclassification','mae','mse']), 'Неправильно выбрана функция информативности'
        
        self.check_k_threshold = check_k_threshold
        assert self.check_k_threshold >= 0, 'Параметр check_k_threshold должен быть неотрицательным'
        if self.check_k_threshold == 0:
            self.check_k_threshold = np.inf
        
        self.min_gain = min_gain
        if self.min_gain == None:
            self.min_gain = 0
        assert self.min_gain >= 0, 'Параметр min_gain должен быть неотрицательным'
        
        self.tree = None
        self.leaf_count = 0  # Счетчик листьев
        
        self.class_weights = class_weights  # Поддержка весов классов
        assert isinstance(self.class_weights, dict) or self.class_weights is None, 'class_weights должен быть словарем вида {class: weight}'
        
        self.chek_k_features = chek_k_features
        

        # Проверяем корректность критерия
        valid_criteria = {"gini", "entropy", "misclassification", "mae", "mse"}
        if self.criterion not in valid_criteria:
            raise ValueError(f"Некорректный критерий '{criterion}'. Выберите из {valid_criteria}.")
        
    def fit(self, X, y):
        """
        Обучение модели дерева решений на заданных данных.

        Args:
            X (np.ndarray): Матрица признаков формы (n_samples, n_features).
            y (np.ndarray): Вектор целевых значений формы (n_samples,).

        Построение дерева начинается с вызова рекурсивной функции `build_tree`.
        """
        # Проверяем корректность весов
        if self.class_weights==None:
            self.class_weights = {cls: 1 for cls in np.unique(y)}
        else:
            unique_classes = np.unique(y)
            for cls in unique_classes:
                if cls not in list(self.class_weights.keys()):
                    raise ValueError(f"Класс {cls} отсутствует в class_weights.")
        
        if self.chek_k_features>X.shape[1]:
            self.chek_k_features=X.shape[1]
            print(f'chek_k_features({self.chek_k_features}) не может быть больше, чем столбцов в X. chek_k_features взято за количество столбцов в X ')
        
        # Построение дерева начинается с вызова рекурсивной функции `build_tree`
        self.leaf_count = 0  # Сбрасываем счетчик листьев перед обучением
        self.tree = self.build_tree(X, y)

    def build_tree(self, X, y, depth=0):
        """
        Рекурсивно строит дерево решений.
        
        Args:
            X (np.ndarray): Матрица признаков для текущего узла.
            y (np.ndarray): Вектор целевых значений для текущего узла.
            depth (int, optional): Текущая глубина дерева. Defaults to 0.
        
        Returns:
            dict or object: Словарь, представляющий узел дерева (если не лист),
                            или значение листа (класс/среднее).
        """
        # Условие остановки рекурсии
        if (
            (self.max_depth is not None and depth >= self.max_depth)  # Превышена глубина
            or len(set(y)) == 1  # Все элементы относятся к одному классу
            or len(y) < self.min_samples_leaf  # Мало объектов для деления
            or (self.max_leaves is not None and self.leaf_count >= self.max_leaves)  # Превышено количество листьев
        ):
            self.leaf_count += 1  # Создаем новый лист
            return self.majority_class(y)

        # Найти лучшее разбиение
        feature, threshold = self.best_split(X, y)
        if feature is None:
            self.leaf_count += 1  # Если деление невозможно, создаем лист
            return self.majority_class(y)
        
        # Разделить данные на левое и правое поддеревья
        left_indices = X[:, feature] < threshold
        right_indices = X[:, feature] >= threshold

        left_child = self.build_tree(X[left_indices], y[left_indices], depth + 1)
        right_child = self.build_tree(X[right_indices], y[right_indices], depth + 1)

        return {"feature": feature, "threshold": threshold, "left": left_child, "right": right_child}

    def best_split(self, X, y):
        """
        Находит лучшее разбиение данных по одному из признаков.

        Args:
            X (np.ndarray): Матрица признаков.
            y (np.ndarray): Вектор целевых значений.

        Returns:
            tuple: (индекс лучшего признака, значение порога).
        """

        best_feature, best_threshold = None, None
        
        # Устанавливаем начальное значение метрики в зависимости от типа критерия
        if self.criterion in {"gini", "entropy", "mae", "mse"}:
            best_metric = float("inf")  # Минимизируем
        elif self.criterion == "misclassification":
            best_metric = -float("inf")  # Максимизируем
        
        n_samples, n_features = X.shape

        if self.chek_k_features==0:
            features = list(range(n_features))
        else:
            features = np.random.choice(list(range(n_features)),self.chek_k_features,False)
            
        for feature in features:
            unique_values = np.unique(X[:, feature])
            
            if np.isinf(self.check_k_threshold):
                thresholds = unique_values  # Если параметр бесконечен, используем все уникальные значения.
            else:
                thresholds = np.linspace(unique_values.min(), unique_values.max(), int(self.check_k_threshold))

    
            
            
            for threshold in thresholds:
                left_indices = X[:, feature] < threshold
                right_indices = X[:, feature] >= threshold
                
                # Проверяем минимальное количество объектов в листе
                if (
                    len(left_indices[left_indices]) < self.min_samples_leaf
                    or len(right_indices[right_indices]) < self.min_samples_leaf
                ):
                    continue

                metric = self.split_metric(y[left_indices], y[right_indices])

                # Обновляем лучший критерий в зависимости от его направления (минимизация/максимизация)
                if (
                    (self.criterion in {"gini", "entropy", "mae", "mse"} and metric < best_metric)
                    or (self.criterion == "misclassification" and metric > best_metric)
                ) and abs(metric-best_metric) >= self.min_gain:
                    best_metric, best_feature, best_threshold = metric, feature, threshold

        return best_feature, best_threshold


    def split_metric(self, left_y, right_y):
        """
        Вычисляет критерий для разделения.
        
        Args:
            left_y (np.ndarray): Вектор целевых значений для левого поддерева.
            right_y (np.ndarray): Вектор целевых значений для правого поддерева.
        
        Returns:
            float: Значение метрики для данного разделения.
        """
        if self.criterion == "gini":
            return self.gini_index(left_y, right_y)
        elif self.criterion == "entropy":
            return self.entropy_index(left_y, right_y)
        elif self.criterion == "misclassification":
            return self.misclassification_error(left_y, right_y)
        elif self.criterion == "mae":
            return self.mean_absolute_error(left_y, right_y)
        elif self.criterion == "mse":
            return self.mean_squared_error(left_y, right_y)

    
    def gini_index(self, left_y, right_y):
        """
        Вычисляет взвешенный индекс Джини для разделения.

        Args:
            left_y (np.ndarray): Целевые значения левого поддерева.
            right_y (np.ndarray): Целевые значения правого поддерева.

        Returns:
            float: Индекс Джини.
        """
        def gini(y):
            counts = Counter(y)
            total = sum(self.class_weights.get(cls, 1) * count for cls, count in counts.items())
            return 1 - sum((self.class_weights.get(cls, 1) * count / total) ** 2 for cls, count in counts.items())

        n = len(left_y) + len(right_y)
        left_weighted = (len(left_y) / n) * gini(left_y)
        right_weighted = (len(right_y) / n) * gini(right_y)
        return left_weighted + right_weighted
    

    def entropy_index(self, left_y, right_y):
        """
        Вычисляет взвешенную энтропию для разделения.

        Args:
            left_y (np.ndarray): Целевые значения левого поддерева.
            right_y (np.ndarray): Целевые значения правого поддерева.

        Returns:
            float: Энтропия.
        """
        def entropy(y):
            counts = Counter(y)
            total = sum(self.class_weights.get(cls, 1) * count for cls, count in counts.items())
            return -sum((self.class_weights.get(cls, 1) * count / total) * 
                        np.log2((self.class_weights.get(cls, 1) * count / total))
                        for cls, count in counts.items() if count > 0)

        n = len(left_y) + len(right_y)
        left_weighted = (len(left_y) / n) * entropy(left_y)
        right_weighted = (len(right_y) / n) * entropy(right_y)
        return left_weighted + right_weighted


    def misclassification_error(self, left_y, right_y):
        """
        Вычисляет взвешенную ошибку классификации для разделения.

        Args:
            left_y (np.ndarray): Целевые значения левого поддерева.
            right_y (np.ndarray): Целевые значения правого поддерева.

        Returns:
            float: Ошибка классификации.
        """
        def misclass_error(y):
            # Считаем количество объектов каждого класса
            counts = Counter(y)
            # Учитываем веса классов
            total_weight = sum(self.class_weights.get(cls, 1) * count for cls, count in counts.items())
            # Определяем частоту самого популярного класса с учетом весов
            max_class_weighted_freq = max(self.class_weights.get(cls, 1) * count / total_weight for cls, count in counts.items())
            # Ошибка классификации = 1 - частота самого популярного класса
            return 1 - max_class_weighted_freq

        n = len(left_y) + len(right_y)  # Общее число объектов
        # Взвешенная ошибка классификации для левого и правого подмножеств
        left_weighted = (len(left_y) / n) * misclass_error(left_y)
        right_weighted = (len(right_y) / n) * misclass_error(right_y)
        return left_weighted + right_weighted


    def mean_absolute_error(self, left_y, right_y):
        """
        Вычисляет взвешенную среднюю абсолютную ошибку (MAE) для разделения.

        Args:
            left_y (np.ndarray): Целевые значения левого поддерева.
            right_y (np.ndarray): Целевые значения правого поддерева.

        Returns:
            float: Средняя абсолютная ошибка.
        """
        def mae(y):
            mean = np.mean(y)
            return np.mean(np.abs(y - mean))

        n = len(left_y) + len(right_y)
        return (len(left_y) / n) * mae(left_y) + (len(right_y) / n) * mae(right_y)


    def mean_squared_error(self, left_y, right_y):
        """
        Вычисляет взвешенную среднюю квадратичную ошибку (MSE) для разделения.

        Args:
            left_y (np.ndarray): Целевые значения левого поддерева.
            right_y (np.ndarray): Целевые значения правого поддерева.

        Returns:
            float: Средняя квадратичная ошибка.
        """
        def mse(y):
            mean = np.mean(y)
            return np.mean((y - mean) ** 2)

        n = len(left_y) + len(right_y)
        return (len(left_y) / n) * mse(left_y) + (len(right_y) / n) * mse(right_y)

     
    def majority_class(self, y):
        """
        Определяет преобладающий класс или среднее значение в листе.

        Args:
            y (np.ndarray): Вектор целевых значений в листе.

        Returns:
            object: Преобладающий класс (для классификации) или среднее значение (для регрессии).
        """
        counts = Counter(y)
        weighted_counts = {cls: count * self.class_weights.get(cls, 1) for cls, count in counts.items()}
        return max(weighted_counts, key=weighted_counts.get) if self.model_type == "Classifier" else np.mean(y)


    def predict(self, X):
        """
        Предсказывает целевые значения для входных данных.

        Args:
            X (np.ndarray): Матрица признаков для предсказания.

        Returns:
            np.ndarray: Вектор предсказанных значений.
        """
        return np.array([self._predict(sample, self.tree) for sample in X])

    def _predict(self, sample, tree):
        """
        Рекурсивное предсказание для каждого узла дерева

        Args:
            sample (np.ndarray): Один образец данных.
            tree (dict or object): Текущий узел дерева.

        Returns:
            object: Предсказанное значение.
        """
        if not isinstance(tree, dict):
            return tree

        feature = tree["feature"]
        threshold = tree["threshold"]
        if sample[feature] < threshold:
            return self._predict(sample, tree["left"])
        else:
            return self._predict(sample, tree["right"])

    def visualize_tree(self):
        """
        Визуализирует и сохраняет дерево решений в виде графа.

        Returns:
            graphviz.Digraph: Граф, изображающий дерево решений.
        """
        dot = Digraph()
        self._add_nodes_edges(dot, self.tree)
        dot.render("decision_tree", format="png", cleanup=True)  # Сохранить дерево как PNG
        return dot

    def _add_nodes_edges(self, dot, tree, parent=None, edge_label=""):
        """
        Рекурсивно добавляет узлы и ребра к графу, представляющему дерево решений.

        Args:
            dot (graphviz.Digraph): Граф, в который добавляются узлы и ребра.
            tree (dict or object): Текущий узел дерева.
            parent (str, optional): Идентификатор родительского узла. Defaults to None.
            edge_label (str, optional): Метка для ребра. Defaults to "".
        """
        # Если это листовой узел, добавляем его как конечный класс
        if not isinstance(tree, dict):
            node_id = str(id(tree))
            dot.node(node_id, label=f"Class: {tree}", shape="ellipse", color="lightblue2", style="filled")
            if parent:
                dot.edge(parent, node_id, label=edge_label)
            return

        # Создаем узел с условием разбиения
        node_id = str(id(tree))
        feature = tree["feature"]
        threshold = tree["threshold"]
        label = f"X[{feature}] < {threshold:.2f}"
        dot.node(node_id, label=label, shape="box", color="lightgrey", style="filled")

        # Если это не корень, добавляем ребро к узлу
        if parent:
            dot.edge(parent, node_id, label=edge_label)

        # Рекурсивно добавляем левое и правое поддеревья
        self._add_nodes_edges(dot, tree["left"], parent=node_id, edge_label="True")
        self._add_nodes_edges(dot, tree["right"], parent=node_id, edge_label="False")

    def plot_decision_boundaries(self, X, y, feature_indices=(0, 1), resolution=100, figsize=(8, 6)):
        """
        Визуализирует границы принятия решений для 2D-данных.

        Args:
            X (np.ndarray): Матрица признаков (n_samples, n_features).
            y (np.ndarray): Вектор меток классов (n_samples,).
            feature_indices (tuple, optional): Индексы двух признаков для визуализации.
                Defaults to (0, 1).
            resolution (int, optional): Плотность сетки для построения областей.
                Defaults to 100.
            figsize (tuple, optional): Размер изображения. Defaults to (8, 6).
        """
        if len(feature_indices) != 2:
            raise ValueError("Для визуализации выберите ровно 2 признака.")

        feature1, feature2 = feature_indices
        x_min, x_max = X[:, feature1].min() - 1, X[:, feature1].max() + 1
        y_min, y_max = X[:, feature2].min() - 1, X[:, feature2].max() + 1

        # Создаем сетку точек
        xx, yy = np.meshgrid(
            np.linspace(x_min, x_max, resolution),
            np.linspace(y_min, y_max, resolution)
        )
        grid_points = np.c_[xx.ravel(), yy.ravel()]

        # Предсказываем классы для каждой точки сетки
        full_dim_grid = np.zeros((grid_points.shape[0], X.shape[1]))
        full_dim_grid[:, feature1] = grid_points[:, 0]
        full_dim_grid[:, feature2] = grid_points[:, 1]
        grid_predictions = self.predict(full_dim_grid).reshape(xx.shape)

        # Визуализируем границы
        plt.figure(figsize=figsize)
        n_classes = len(set(y))
        cmap_background = plt.get_cmap("tab20", n_classes)
        cmap_points = plt.get_cmap("viridis", n_classes)

        plt.contourf(xx, yy, grid_predictions, alpha=0.3, cmap=cmap_background)
        scatter = plt.scatter(X[:, feature1], X[:, feature2], c=y, cmap=cmap_points, edgecolor="k", s=30)

        plt.xlabel(f"Feature {feature1}")
        plt.ylabel(f"Feature {feature2}")
        plt.title("Decision Boundaries")

        if self.model_type!='Regressor':
            # Добавляем легенду
            legend1 = plt.legend(*scatter.legend_elements(), title="Classes (Points)", loc="upper right")
            plt.gca().add_artist(legend1)

            # Добавляем таблицу цветов фона
            class_patches = [
                Patch(color=cmap_background(i), label=f"Class {i}") for i in range(n_classes)
            ]
            legend2 = plt.legend(handles=class_patches, title="Classes (Background)", loc="lower right")
            plt.gca().add_artist(legend2)
        
        plt.show()
    
    def plot_regr(self, y_true, y_pred):
        """
        Строит график рассеяния истинных и предсказанных значений для регрессии.

        Args:
            y_true (np.ndarray): Истинные целевые значения.
            y_pred (np.ndarray): Предсказанные целевые значения.

        Notes:
            - Эта функция доступна только для моделей регрессии (`self.model_type == 'Regressor'`).
        """
        assert self.model_type == 'Regressor', 'Эта функция только для регрессии'
        plt.scatter(y_true, y_pred)
        plt.scatter(y_true, y_true)
        plt.show()
#######################################################################################################################
TREES = [DecisionTree]