from __future__ import annotations # for DfTransformer
import numpy as np
import pandas as pd
import sys # built-in
import os # built-in
# import re # built-in
try:
    import regex as re
    HAS_REGEX=True
except ImportError:
    import re
    HAS_REGEX=False

try:
    import regex
    from regex import Pattern as RegexPattern
except ImportError:
    regex = None
    RegexPattern = type(re.compile('')) if 're' in locals() else None

import ast
from tqdm import tqdm
import threading
from multiprocessing import cpu_count
from functools import lru_cache
from IPython.display import display
import shutil
import logging
from pathlib import Path
from datetime import datetime, time, date, timedelta 
import stat
import platform
import subprocess

from typing import Dict, List, Optional, Union, Any, Tuple, Literal,Callable,Pattern,Set,Iterable,get_origin,get_args
from types import GeneratorType
from regex import X
import traceback
from .utils import decorators
import matplotlib.pyplot as plt

# try:
#     get_ipython().run_line_magic("load_ext", "autoreload")
#     get_ipython().run_line_magic("autoreload", "2")
# except NameError:
#     pass

import glob  # built-in
import importlib
import inspect
import pkgutil
import pytest

import warnings
import unicodedata

warnings.simplefilter("ignore", category=pd.errors.SettingWithCopyWarning)
warnings.filterwarnings("ignore", category=pd.errors.PerformanceWarning)
warnings.filterwarnings("ignore")

# repmat function
from collections.abc import Iterable
from copy import deepcopy
from itertools import repeat

from functools import wraps
import linecache 

try:
    import pkg_resources
except ImportError:
    pkg_resources = None

try:
    import importlib.metadata as metadata  # Python 3.8+
except ImportError:
    import importlib_metadata as metadata  # For older versions via backport
_usages_pd=None

# ======== Common used built-in functions ======== 
def cp_func(module_path: str, func_name: str):
    """
    usage:
    figsets = cp_func(".plot", "figsets")
    """

    import importlib
    import inspect
    
    # Check if this is an alias request
    if module_path is None or module_path == "" or module_path == ".":
        # Alias mode: find function in caller's namespace
        
        # Get the immediate caller's globals
        caller_globals = inspect.currentframe().f_back.f_globals
        
        # Verify the function exists
        if func_name not in caller_globals:
            # Provide helpful error
            available_funcs = [
                name for name, obj in caller_globals.items()
                if callable(obj) and not name.startswith('_')
            ]
            raise NameError(
                f"Function '{func_name}' not found. Available: {available_funcs[:10]}"
            )
        
        original_func = caller_globals[func_name]
        
        # Create alias wrapper
        def _wrapper(*args, **kwargs):
            return original_func(*args, **kwargs)
        
        _wrapper.__name__ = func_name
        
        # Optional: Copy more metadata if you want
        if hasattr(original_func, '__doc__'):
            _wrapper.__doc__ = original_func.__doc__
        
        return _wrapper
    #  import from external
    def _wrapper(*args, **kwargs):
        module = importlib.import_module(module_path, package=__package__)
        return getattr(module, func_name)(*args, **kwargs)
    _wrapper.__name__ = func_name
    return _wrapper

# batch creation "import from .plot"
figsets=cp_func(".plot", "figsets")
subplot=cp_func(".plot", "subplot")
get_color=cp_func(".plot", "get_color")
get_cmap=cp_func(".plot", "get_cmap")

# *********** specifif for stext func ***********
COMMON_PATTERNS = None
def load_common_patterns(verbose:bool=False):
    global COMMON_PATTERNS
    if COMMON_PATTERNS is None:
        pattern_file = os.path.join(os.path.dirname(os.path.abspath(__file__)), "data/re_common_pattern.json")
        if os.path.exists(pattern_file):
            with open(pattern_file, "r") as f:
                COMMON_PATTERNS = json.load(f)
        else:
            COMMON_PATTERNS = {}
    print('COMMON_PATTERNS json is loaded') if verbose else None
    return COMMON_PATTERNS
try:
    COMMON_PATTERNS = load_common_patterns()
except:
    pass
 
# *********** specifif for stext func ***********
class PkgManager:
    """
    PkgManager.uninstall("py2ls")
    PkgManager.uninstall("py2ls", mode="startswith")
    PkgManager.uninstall("py2ls", mode="endswith")
    PkgManager.uninstall("py2ls", mode="contains")
    PkgManager.uninstall("py2ls", mode="regex")

    PkgManager.timemachine()
    """

    @staticmethod
    def uninstall(
        kw: Union[str, List[str]],
        mode: str = "exact",
        dry_run: bool = False,
        make_backup: bool = True,
        make_log: bool = True,
        station: Optional[str] = None,
    ) -> None:
        if station is None:
            station = os.path.dirname(os.path.dirname(sys.executable)) 
        os.makedirs(station, exist_ok=True)
        timestamp = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")

        if isinstance(kw, str):
            kw = [kw]
        kw = [k.lower() for k in kw] if mode != "regex" else kw
        mode = mode.lower()
        valid_modes = {"exact", "startswith", "endswith", "contains", "regex"}
        if mode not in valid_modes:
            raise ValueError(f"Mode must be one of {valid_modes}")

        installed_packages = {pkg.key: pkg.version for pkg in pkg_resources.working_set}
        matched: Set[str] = set()

        for name in installed_packages:
            for key in kw:
                if (
                    (mode == "exact" and name == key)
                    or (mode == "startswith" and name.startswith(key))
                    or (mode == "endswith" and name.endswith(key))
                    or (mode == "contains" and key in name)
                    or (mode == "regex" and re.search(key, name))
                ):
                    matched.add(name)
                    break

        if not matched:
            print("No packages matched the criteria.")
            return

        if make_backup and not dry_run:
            backup_path = os.path.join(station, f"requirements_backup_{timestamp}.txt")
            with open(backup_path, "w") as f:
                subprocess.run(["pip", "freeze"], stdout=f, check=True)
            print(f"Backup created at: '{backup_path}'")

        if dry_run:
            print("[DRY RUN] The following packages would be uninstalled:")
            for pkg in sorted(matched):
                print(f"  - {pkg}=={installed_packages[pkg]}")
            return

        print(f"[UNINSTALLING] {len(matched)} packages:")
        for pkg in sorted(matched):
            print(f"  - {pkg}=={installed_packages[pkg]}")
            subprocess.run(["pip", "uninstall", "-y", pkg], check=True)

        if make_log:
            log_path = os.path.join(station, f"log_uninstall.txt")
            with open(log_path, "w") as f:
                f.write(f"# Uninstallation log created at {timestamp}\n")
                f.write(f"# Mode: {mode}, Keywords: {kw}\n\n")
                for pkg in sorted(matched):
                    f.write(f"{pkg}=={installed_packages[pkg]}\n")
            print(f"Log written to '{log_path}'")

    @staticmethod
    def list_backups(station: Optional[str] = None) -> List[str]:
        if station is None:
            station = os.path.dirname(sys.executable)
            if os.name == "nt":
                station = os.path.dirname(station)
        return sorted(glob.glob(os.path.join(station, "requirements_backup_*.txt")))

    @staticmethod
    def list_logs(station: Optional[str] = None) -> List[str]:
        if station is None:
            station = os.path.dirname(sys.executable)
            if os.name == "nt":
                station = os.path.dirname(station)
        return sorted(glob.glob(os.path.join(station, "uninstall_*.txt")))

    @staticmethod
    def restore(
        timestamp: Optional[str] = None,
        station: Optional[str] = None,
        dry_run: bool = False,
    ) -> None:
        if station is None:
            station = os.path.dirname(sys.executable)
            if os.name == "nt":
                station = os.path.dirname(station)

        backups = PkgManager.list_backups(station)
        logs = PkgManager.list_logs(station)

        if not timestamp:
            print("Available restore points:\n\nBackups:")
            for i, backup in enumerate(backups, 1):
                ts = os.path.basename(backup)[18:-4]
                print(f"  {i}. {ts} (backup)")
            print("\nUninstall logs:")
            for i, log in enumerate(logs, len(backups) + 1):
                ts = os.path.basename(log)[10:-4]
                print(f"  {i}. {ts} (log)")
            print("\nSpecify timestamp or selection number to restore.")
            return

        try:
            selection = int(timestamp)
            all_files = backups + logs
            if 1 <= selection <= len(all_files):
                file_path = all_files[selection - 1]
                is_log = selection > len(backups)
            else:
                raise ValueError("Invalid selection number")
        except ValueError:
            backup_pattern = os.path.join(
                station, f"requirements_backup_{timestamp}.txt"
            )
            log_pattern = os.path.join(station, f"uninstall_{timestamp}.txt")
            matching_backups = glob.glob(backup_pattern)
            matching_logs = glob.glob(log_pattern)

            if matching_backups:
                file_path = matching_backups[0]
                is_log = False
            elif matching_logs:
                file_path = matching_logs[0]
                is_log = True
            else:
                print(f"No backup or log found for timestamp: {timestamp}")
                return

        with open(file_path, "r") as f:
            packages = [
                line.strip() for line in f if line.strip() and not line.startswith("#")
            ]

        if dry_run:
            print(
                f"[DRY RUN] Would restore {len(packages)} packages from:\n  {file_path}"
            )
            for pkg in packages:
                print(f"  - {pkg}")
            return

        print(f"[RESTORING] {len(packages)} packages from:\n  {file_path}")
        for pkg in packages:
            print(f"  - Installing {pkg}")
            subprocess.run(["pip", "install", pkg], check=True)

    @staticmethod
    def timemachine(station: Optional[str] = None) -> None:
        if station is None:
            station = os.path.dirname(sys.executable)
            if os.name == "nt":
                station = os.path.dirname(station)

        backups = PkgManager.list_backups(station)
        logs = PkgManager.list_logs(station)

        if not backups and not logs:
            print("No backup or log files found.")
            return

        print("\nTime Machine - Available Restore Points:")
        print("--------------------------------------")
        print("\nBackups (complete environment snapshots):")
        for i, backup in enumerate(backups, 1):
            ts = os.path.basename(backup)[18:-4]
            print(f"  {i}. {ts}")
        print("\nUninstall Logs (specific package lists):")
        for i, log in enumerate(logs, len(backups) + 1):
            ts = os.path.basename(log)[10:-4]
            print(f"  {i}. {ts}")
        print("\n0. Exit Time Machine")

        while True:
            try:
                choice = input("\nSelect a restore point (number) or '0' to exit: ")
                if choice == "0":
                    return
                selection = int(choice)
                all_files = backups + logs
                if 1 <= selection <= len(all_files):
                    file_path = all_files[selection - 1]
                    timestamp = os.path.basename(file_path).split("_")[-1][:-4]
                    PkgManager.restore(timestamp, station)
                    return
                else:
                    print("Invalid selection. Please try again.")
            except ValueError:
                print("Please enter a valid number.")  
    @staticmethod
    def listfunc(
        where: Union[str, Any]= None,
        query: Optional[str] = None,
        return_output:bool=False,
        show_all: bool = False,
        include_dunder: bool = False,
        only_defined: bool = True,
        verbose: bool = False
    ) -> Dict[str, Any]:
        """
        Recursively list functions defined in a package/module and its submodules.
        If `where=None`, returns the installed pip packages instead.

        Args:
            where (str or module or None): Module/package to inspect, or None for full pip list.
            query (str): Optional search string for fuzzy match.
            show_all (bool): Show all callables including those starting with '_'.
            include_dunder (bool): Include dunder (__) methods like __init__.
            only_defined (bool): Only show functions actually defined in the module.
            verbose (bool): Show detailed skip/load error messages.

        Returns:
            dict: Nested dictionary with module names and their function lists or pip list.
        """
        if where is None:
            # Return pip list instead
            print("üì¶ Installed pip packages:")
            pip_packages = {dist.metadata['Name']: dist.version for dist in metadata.distributions()}
            if query:
                func_OI = strcmp(query, list(pip_packages.keys()))[0]
                print(f"  - {func_OI}=={pip_packages[func_OI]}")
                return {func_OI: pip_packages[func_OI]}
            for name, version in sorted(pip_packages.items()):
                print(f"  - {name}=={version}")
            return pip_packages
        if isinstance(where, str):
            try:
                mod = importlib.import_module(where)
            except ModuleNotFoundError:
                print(f"Module '{where}' not found.")
                return {}
        else:
            mod = where

        root_name = mod.__name__
        results = {}

        def list_functions_in_module(module, module_name) -> List[str]:
            funcs = []
            for name in dir(module):
                attr = getattr(module, name)
                if callable(attr):
                    if not show_all:
                        if name.startswith("__") and not include_dunder:
                            continue
                        if name.startswith("_") and not name.startswith("__"):
                            continue
                    if only_defined and getattr(attr, "__module__", "") != module_name:
                        continue
                    funcs.append(name)
            if query:
                from difflib import get_close_matches
                funcs = get_close_matches(query, funcs, n=10, cutoff=0.3)
            return sorted(set(funcs))

        def walk_package(mod) -> Dict[str, Any]:
            subresults = {}
            modname = mod.__name__
            funcs = list_functions_in_module(mod, modname)
            if funcs:
                print(f"\nüéí: {modname}")
                for f in funcs:
                    print(f"  - {f}")
                subresults[modname] = funcs

            if hasattr(mod, '__path__'):  # If it's a package
                for finder, name, ispkg in pkgutil.walk_packages(mod.__path__, prefix=mod.__name__ + "."):
                    try:
                        submod = importlib.import_module(name)
                        submod_result = walk_package(submod)
                        subresults.update(submod_result)
                    except pytest.skip.Exception as e:
                        if verbose:
                            print(f"Ê≠£Â∏∏Ë∑≥Ëøá: Skipped test module {name}: {e}")
                    except Exception as e:
                        if verbose:
                            print(f"Âõ†ÈîôË∑≥Ëøá {name}: {e}")
            return subresults

        results[root_name] = walk_package(mod)
        return results if return_output else None
    @staticmethod
    def get_version(pkg):
        import importlib.metadata

        def get_v(pkg_name):
            try:
                version = importlib.metadata.version(pkg_name)
                print(f"version {pkg_name} == {version}")
            except importlib.metadata.PackageNotFoundError:
                print(f"Package '{pkg_name}' not found")

        if isinstance(pkg, str):
            get_v(pkg)
        elif isinstance(pkg, list):
            [get_v(pkg_) for pkg_ in pkg]

    @staticmethod
    def install(
        packages: Union[str, List[str]] = None,
        upgrade: bool = False,
        requirements_file: Optional[str] = None,
        editable: bool = False,
        extra_args: Optional[List[str]] = None,
        verbose: bool = False,
    ) -> None:
        """
        Install packages or from requirements file with optional flags.
        Supports mapping import names to correct pip package names.

        Args:
            packages (str or List[str], optional): Package name(s) to install or import names.
            upgrade (bool): Use --upgrade flag.
            requirements_file (str, optional): Path to requirements.txt.
            editable (bool): Install packages as editable (-e).
            extra_args (List[str], optional): Additional pip install arguments.
            verbose (bool): Show full pip output.
        """
        if not packages and not requirements_file:
            raise ValueError("Specify packages or requirements_file")

        if isinstance(packages, str):
            packages = [packages]

        # Map common import names to pip package names
        import_to_pip_map = {
            "bluetooth": "pybluez",
            "PIL": "Pillow",
            "bs4": "beautifulsoup4",
            "Crypto": "pycryptodome",
            # add more mappings as needed
        }

        def is_installed(pkg_name: str) -> bool:
            try:
                importlib.metadata.version(pkg_name)
                return True
            except importlib.metadata.PackageNotFoundError:
                return False

        def run_pip_cmd(args: List[str]):
            kwargs = {"text": True}
            if verbose:
                kwargs["stdout"] = None
                kwargs["stderr"] = None
            else:
                kwargs["stdout"] = subprocess.PIPE
                kwargs["stderr"] = subprocess.PIPE
            subprocess.run([sys.executable, "-m", "pip"] + args, check=True, **kwargs)

        base_args = ["install"]
        if upgrade:
            base_args.append("--upgrade")
        if extra_args:
            base_args.extend(extra_args)

        if requirements_file:
            print(f"Installing from requirements file: {requirements_file}")
            run_pip_cmd(base_args + ["-r", requirements_file])
            print("Installation successful.")
            return

        for pkg in packages:
            # Map import name to pip package name if available
            pip_pkg = import_to_pip_map.get(pkg, pkg)
            target = f"-e {pip_pkg}" if editable else pip_pkg

            if is_installed(pip_pkg):
                print(f"Package '{pip_pkg}' is already installed. Skipping.")
                continue

            print(f"Installing {target} ...")
            try:
                run_pip_cmd(base_args + [target])
                print(f"Installed {target} successfully.")
            except subprocess.CalledProcessError as e:
                print(f"Failed to install {target}: {e}")

    @staticmethod
    def is_stdlib(module_name: str) -> bool:
        """
        Check if a module is part of the Python standard library.
        """
        if module_name in sys.builtin_module_names:
            return True
        spec = importlib.util.find_spec(module_name)
        if spec is None or spec.origin is None:
            return False
        return "site-packages" not in (spec.origin or "")

    @staticmethod
    def extract_imports_from_file(filepath: str) -> Set[str]:
        """
        Parse a Python file and extract top-level imported modules.
        """
        with open(filepath, "r", encoding=get_encoding(filepath), errors="ignore") as file:
            node = ast.parse(file.read(), filename=filepath)
        imports = set()
        for n in ast.walk(node):
            if isinstance(n, ast.Import):
                for alias in n.names:
                    imports.add(alias.name.split(".")[0])
            elif isinstance(n, ast.ImportFrom):
                if n.module:
                    imports.add(n.module.split(".")[0])
        return imports

    @staticmethod
    def get_requirements(root_dir: str, exclude_stdlib: bool = True) -> List[str]:
        """
        Recursively find all imported packages in a given directory.

        Args:
            root_dir (str): Path to the folder containing .py files.
            exclude_stdlib (bool): Whether to exclude standard library modules.

        Returns:
            List[str]: Sorted list of required third-party packages.
        """
        all_imports = set()
        for dirpath, _, filenames in os.walk(root_dir):
            for filename in filenames:
                if filename.endswith(".py"):
                    filepath = os.path.join(dirpath, filename)
                    try:
                        imports = PkgManager.extract_imports_from_file(filepath)
                        all_imports.update(imports)
                    except Exception as e:
                        print(f"Error parsing {filepath}: {e}")

        if exclude_stdlib:
            non_std_imports = {
                pkg for pkg in all_imports if not PkgManager.is_stdlib(pkg)
            }
        else:
            non_std_imports = all_imports

        return sorted(non_std_imports)

    @staticmethod
    def py2to3(
        path: Union[str, List[str]],
        write: bool = False,
        backup: bool = True,
        fixer_list: Optional[List[str]] = None,
        ignore_fixers: Optional[List[str]] = None,
        verbose: bool = False,
        dry_run: bool = False,
    ) -> None:
        """
        Run 2to3 tool on given file(s) or directory.

        Args:
            path (str or list of str): File or directory path(s) to convert.
            write (bool): Whether to write changes to files (default False).
            backup (bool): Whether to create backup files (default True).
            fixer_list (list of str, optional): List of fixers to apply (e.g. ['fix_print', 'fix_raise']).
            ignore_fixers (list of str, optional): List of fixers to ignore.
            verbose (bool): Show detailed 2to3 output.
            dry_run (bool): Only show what would be changed, no write.
        """
        if isinstance(path, str):
            paths = [path]
        else:
            paths = path

        # Prepare the list of python files to process
        files_to_process = []
        for p in paths:
            if not os.path.exists(p):
                print(f"Path does not exist: {p}")
                continue
            if os.path.isfile(p) and p.endswith(".py"):
                files_to_process.append(p)
            elif os.path.isdir(p):
                for root, _, files in os.walk(p):
                    for f in files:
                        if f.endswith(".py"):
                            files_to_process.append(os.path.join(root, f))

        if not files_to_process:
            print("No Python files found to process.")
            return

        # Construct common args for lib2to3 
        args = []

        if write:
            args.append("-w")
            if not backup:
                args.append("-n")  # no backup, but must have -w
        else:
            # ‰∏çÂÜôÂõûÊñá‰ª∂Êó∂Ôºå‰∏çËÉΩÁî® -nÔºåÊîπÁî® -p ÊâìÂç∞ diff
            args.append("-p")

        if fixer_list:
            for fixer in fixer_list:
                args.extend(["-f", fixer])
        if ignore_fixers:
            for fixer in ignore_fixers:
                args.extend(["-x", fixer])
        if verbose:
            args.append("-v")

        for p in paths:
            if not os.path.exists(p):
                print(f"Path does not exist: {p}")
                continue

            cmd = [sys.executable, "-m", "lib2to3"] + args + [p]

            print(f"Running 2to3 on {p} with args: {' '.join(args)}")
            try:
                completed = subprocess.run(cmd, check=True, text=True, capture_output=not verbose)
                if not verbose and completed.stdout:
                    print(completed.stdout)
                if not verbose and completed.stderr:
                    print(completed.stderr)
            except subprocess.CalledProcessError as e:
                print(f"2to3 failed on {p}: {e}")

def debugger(func):
    """
    Ë∞ÉËØïÂô®Ë£ÖÈ•∞Âô®ÔºåÊèê‰æõËØ¶ÁªÜÁöÑÈîôËØØ‰ø°ÊÅØÂíå‰∏ä‰∏ãÊñá
    """
    @wraps(func)
    def wrapper(*args, **kwargs):
        try:
            return func(*args, **kwargs)
        except Exception as e:
            # Ëé∑ÂèñËØ¶ÁªÜÁöÑÈîôËØØ‰ø°ÊÅØ
            error_type = type(e).__name__
            error_message = str(e)
            
            # Ëé∑ÂèñÂÆåÊï¥ÁöÑË∞ÉÁî®Ê†à
            stack = traceback.extract_tb(sys.exc_info()[2])
            
            # Ëé∑ÂèñÂèëÁîüÈîôËØØÁöÑÂ∏ß
            frame = inspect.currentframe()
            try:
                # Ëé∑ÂèñË∞ÉÁî®Ê†à‰∏≠ÁöÑÊâÄÊúâÂ±ÄÈÉ®ÂèòÈáè
                local_vars = []
                while frame:
                    local_vars.append((frame, frame.f_locals.copy()))
                    frame = frame.f_back
            finally:
                del frame
            
            print("\n" + "="*80)
            print("DEBUGGER - ERROR REPORT")
            print("="*80)
            
            # ÊâìÂç∞Âü∫Êú¨ÈîôËØØ‰ø°ÊÅØ
            print(f"Error Type: {error_type}")
            print(f"Error Message: {error_message}")
            print(f"Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S.%f')}")
            print(f"Function: {func.__name__}")
            print(f"Module: {func.__module__}")
            
            # ÊâìÂç∞ËØ¶ÁªÜÁöÑË∞ÉÁî®Ê†à
            print("\n" + "-"*80)
            print("CALL STACK TRACE:")
            print("-"*80)
            
            for i, frame_info in enumerate(stack):
                filename = frame_info.filename
                lineno = frame_info.lineno
                function = frame_info.name
                code_line = frame_info.line
                
                print(f"\n[{i}] File: {filename}")
                print(f"    Line: {lineno} | Function: {function}")
                if code_line:
                    print(f"    Code: {code_line.strip()}")
                
                # ÊòæÁ§∫ÈîôËØØË°åÂë®Âõ¥ÁöÑ‰∏ä‰∏ãÊñá
                if i == len(stack) - 1:  # ÊúÄÂêé‰∏Ä‰∏™ÊòØÂÆûÈôÖÈîôËØØÂèëÁîüÁöÑÂú∞Êñπ
                    print(f"\n    CONTEXT AROUND LINE {lineno}:")
                    print("    " + "-" * 60)
                    
                    # ÊòæÁ§∫ÂâçÂêé5Ë°åÁöÑ‰ª£Á†Å
                    start_line = max(1, lineno - 5)
                    end_line = lineno + 5
                    
                    for context_line in range(start_line, end_line + 1):
                        line_content = linecache.getline(filename, context_line).rstrip()
                        marker = ">>>" if context_line == lineno else "   "
                        print(f"    {marker} {context_line:4d}: {line_content}")
            
            # ÊâìÂç∞Â±ÄÈÉ®ÂèòÈáè
            print("\n" + "-"*80)
            print("LOCAL VARIABLES AT ERROR POINT:")
            print("-"*80)
            
            if local_vars:
                current_frame, current_locals = local_vars[0]  # ÊúÄÂÜÖÂ±ÇÁöÑÂ∏ß
                for var_name, var_value in current_locals.items():
                    try:
                        var_repr = repr(var_value)
                        if len(var_repr) > 100:
                            var_repr = var_repr[:100] + "..."
                        print(f"    {var_name}: {var_repr}")
                    except:
                        print(f"    {var_name}: <Êó†Ê≥ïÊòæÁ§∫ÂÄº>")
            
            # ÊâìÂç∞ÂáΩÊï∞ÂèÇÊï∞
            print("\n" + "-"*80)
            print("FUNCTION ARGUMENTS:")
            print("-"*80)
            
            arg_names = func.__code__.co_varnames[:func.__code__.co_argcount]
            for i, arg in enumerate(args):
                if i < len(arg_names):
                    arg_name = arg_names[i]
                    try:
                        arg_repr = repr(arg)
                        if len(arg_repr) > 100:
                            arg_repr = arg_repr[:100] + "..."
                        print(f"    {arg_name}: {arg_repr}")
                    except:
                        print(f"    {arg_name}: <Êó†Ê≥ïÊòæÁ§∫ÂÄº>")
                else:
                    try:
                        arg_repr = repr(arg)
                        if len(arg_repr) > 100:
                            arg_repr = arg_repr[:100] + "..."
                        print(f"    *arg[{i}]: {arg_repr}")
                    except:
                        print(f"    *arg[{i}]: <Êó†Ê≥ïÊòæÁ§∫ÂÄº>")
            
            for kwarg_name, kwarg_value in kwargs.items():
                try:
                    kwarg_repr = repr(kwarg_value)
                    if len(kwarg_repr) > 100:
                        kwarg_repr = kwarg_repr[:100] + "..."
                    print(f"    {kwarg_name}: {kwarg_repr}")
                except:
                    print(f"    {kwarg_name}: <Êó†Ê≥ïÊòæÁ§∫ÂÄº>")
            
            # ÊâìÂç∞ÂÆåÊï¥ÁöÑtraceback
            print("\n" + "-"*80)
            print("COMPLETE TRACEBACK:")
            print("-"*80)
            traceback.print_exc()
            
            print("\n" + "="*80)
            print("DEBUGGING SUGGESTIONS:")
            print("="*80)
            
            # Ê†πÊçÆÈîôËØØÁ±ªÂûãÊèê‰æõË∞ÉËØïÂª∫ËÆÆ
            if "NoneType" in error_type and "object" in error_message:
                print("ÂèØËÉΩÂ∞ùËØïËÆøÈóÆ‰∫Ü None ÂØπË±°ÁöÑÂ±ûÊÄßÊàñÊñπÊ≥ï")
                print("Ê£ÄÊü•ÂèòÈáèÊòØÂê¶‰∏∫ None ÂêéÂÜçËøõË°åÊìç‰Ωú")
            
            elif "KeyError" in error_type:
                print("Â≠óÂÖ∏‰∏≠‰∏çÂ≠òÂú®ÊåáÂÆöÁöÑÈîÆ")
                print("‰ΩøÁî® .get() ÊñπÊ≥ïÊàñÊ£ÄÊü•ÈîÆÊòØÂê¶Â≠òÂú®")
            
            elif "IndexError" in error_type:
                print("ÂàóË°®Á¥¢ÂºïË∂ÖÂá∫ËåÉÂõ¥")
                print("Ê£ÄÊü•ÂàóË°®ÈïøÂ∫¶ÂíåÁ¥¢ÂºïÂÄº")
            
            elif "AttributeError" in error_type:
                print("ÂØπË±°Ê≤°ÊúâËØ•Â±ûÊÄßÊàñÊñπÊ≥ï")
                print("Ê£ÄÊü•ÂØπË±°Á±ªÂûãÂíåÂèØÁî®ÁöÑÂ±ûÊÄß")
            
            elif "FileNotFoundError" in error_type:
                print("Êñá‰ª∂ÊàñÁõÆÂΩï‰∏çÂ≠òÂú®")
                print("Ê£ÄÊü•Êñá‰ª∂Ë∑ØÂæÑÂíåÊùÉÈôê")
            
            print("="*80 + "\n")
            
            # ÈáçÊñ∞ÊäõÂá∫ÂºÇÂ∏∏
            raise
    
    return wrapper

def debug(func=None, *, enable=True):
    """
    ÂèØÈÄâÁöÑË∞ÉËØïË£ÖÈ•∞Âô®ÔºåÂèØ‰ª•ÈÄöËøáÂèÇÊï∞ÊéßÂà∂ÊòØÂê¶ÂêØÁî®
    """
    def decorator(f):
        if enable:
            return debugger(f)
        return f
    
    if func is None:
        return decorator
    return decorator(func)

# ÂÖ®Â±ÄÈîôËØØÂ§ÑÁêÜÂô®
def global_debugger():
    """
    ËÆæÁΩÆÂÖ®Â±ÄÂºÇÂ∏∏Â§ÑÁêÜÂô®
    """
    def global_exception_handler(exctype, value, tb):
        if exctype != KeyboardInterrupt:  # ÂøΩÁï•Ctrl+C
            print("\nGLOBAL EXCEPTION CAUGHT!")
            # ÂàõÂª∫‰∏Ä‰∏™ÁÆÄÂçïÁöÑÂáΩÊï∞Êù•Ëß¶ÂèëË∞ÉËØïÂô®ÊòæÁ§∫
            def dummy_func():
                raise value from None
            
            try:
                debugger(dummy_func)()
            except:
                pass
            
            # ÂèØ‰ª•ÈÄâÊã©ÈÄÄÂá∫ÊàñÁªßÁª≠
            if input("\nContinue execution? (y/n): ").lower() != 'y':
                sys.exit(1)
    
    sys.excepthook = global_exception_handler

# ‰ΩøÁî®Á§∫‰æã
"""
    # ËÆæÁΩÆÂÖ®Â±ÄÂºÇÂ∏∏Â§ÑÁêÜÂô®
    global_debugger()
    
    # Á§∫‰æã‰ΩøÁî®
    @debugger
    def problematic_function(x, y, name="test"):
        #‰∏Ä‰∏™ÊúâÈóÆÈ¢òÁöÑÂáΩÊï∞Áî®‰∫éÊµãËØï
        z = x + y
        result = z / 0  # ËøôÈáå‰ºöÈô§Èõ∂ÈîôËØØ
        data = {"key": "value"}
        return data["nonexistent_key"]  # ËøôÈáå‰ºöKeyError
    
    # ÊµãËØïÂáΩÊï∞
    try:
        problematic_function(10, 0, name="debug_test")
    except:
        pass  # ÈîôËØØÂ∑≤ÁªèË¢´Ë∞ÉËØïÂô®Â§ÑÁêÜ‰∫Ü
"""
from enum import Enum
from dataclasses import is_dataclass

####################################
# helper calss for man
####################################
class ParameterKind(Enum):
    POSITIONAL_ONLY = "positional_only"
    POSITIONAL_OR_KEYWORD = "positional_or_keyword"
    VAR_POSITIONAL = "var_positional"  # *args
    KEYWORD_ONLY = "keyword_only"
    VAR_KEYWORD = "var_keyword"  # **kwargs


def man(
    func,
    include_private: bool = False,
    resolve_types: bool = True,
    format_output: str = "pretty",
    return_obj: bool = False,
    verbose: bool = True,
) -> Union[list, str, None]:
    """
    Enhanced function to extract comprehensive parameter information from any Python function.

    Args:
        func: The function to analyze (can be any callable, module, or object)
        include_private: Whether to include private parameters (starting with '_')
        resolve_types: Whether to resolve complex type hints to readable strings
        format_output: Output format - 'dict', 'json', 'pretty', 'markdown', 'error'
        return_obj: Whether to return the result object
        verbose: Whether to provide detailed error information

    Returns:
        Parameter information in the requested format, or None if return_obj=False
    """
    usage_str = """
    Usage:
        def func_xample(a: int, b: str = "hello", *args, **kwargs):
             pass
        man(func_xample)
        man(func_xample, format_output='markdown')
    """

    ####################################
    # helper funcs for man
    ####################################
    def _get_enhanced_type_info(param, func, resolve_types: bool) -> dict:
        """Extract comprehensive type information for a parameter."""
        annotation = param.annotation

        if annotation is inspect.Parameter.empty:
            return {"type_str": "Any", "details": None, "is_optional": True}

        if not resolve_types:
            return {"type_str": str(annotation), "details": None, "is_optional": False}

        # Resolve complex type hints
        type_str, details, is_optional = _resolve_type_hint(annotation)

        return {"type_str": type_str, "details": details, "is_optional": is_optional}

    def _resolve_type_hint(annotation) -> tuple:
        """Resolve complex type hints to readable strings."""
        origin = get_origin(annotation)
        args = get_args(annotation)

        # Handle Union and Optional types
        if origin is Union:
            if len(args) == 2 and type(None) in args:
                # This is an Optional[Type]
                non_none_args = [arg for arg in args if arg is not type(None)]
                if non_none_args:
                    base_type, details, _ = _resolve_type_hint(non_none_args[0])
                    return f"Optional[{base_type}]", details, True
            return (
                f"Union[{', '.join(_resolve_type_hint(arg)[0] for arg in args)}]",
                None,
                False,
            )

        # Handle generic types (List, Dict, etc.)
        if origin and hasattr(origin, "__name__"):
            type_name = origin.__name__
            if args:
                arg_strs = [_resolve_type_hint(arg)[0] for arg in args]
                return (
                    f"{type_name}[{', '.join(arg_strs)}]",
                    {"generic_args": arg_strs},
                    False,
                )
            return type_name, None, False

        # Handle simple types and classes
        if hasattr(annotation, "__name__"):
            return annotation.__name__, None, False

        return str(annotation), None, False

    def _get_default_info(param) -> dict:
        """Extract comprehensive default value information."""
        if param.default is inspect.Parameter.empty:
            return {"value": None, "repr": "No default", "has_default": False}

        # Special case for common default values
        default = param.default

        # Get a safe representation
        try:
            default_repr = repr(default)
            # Limit long representations
            if len(default_repr) > 50:
                default_repr = f"{default_repr[:47]}..."
        except Exception:
            default_repr = "<unrepresentable>"

        return {"value": default, "repr": default_repr, "has_default": True}

    def _get_kind_info(param) -> dict:
        """Get enhanced parameter kind information."""
        kind_map = {
            inspect.Parameter.POSITIONAL_ONLY: {
                "kind": ParameterKind.POSITIONAL_ONLY.value,
                "description": "Positional-only parameter",
            },
            inspect.Parameter.POSITIONAL_OR_KEYWORD: {
                "kind": ParameterKind.POSITIONAL_OR_KEYWORD.value,
                "description": "Positional or keyword parameter",
            },
            inspect.Parameter.VAR_POSITIONAL: {
                "kind": ParameterKind.VAR_POSITIONAL.value,
                "description": "Variable positional arguments (*args)",
            },
            inspect.Parameter.KEYWORD_ONLY: {
                "kind": ParameterKind.KEYWORD_ONLY.value,
                "description": "Keyword-only parameter",
            },
            inspect.Parameter.VAR_KEYWORD: {
                "kind": ParameterKind.VAR_KEYWORD.value,
                "description": "Variable keyword arguments (**kwargs)",
            },
        }

        return kind_map.get(
            param.kind, {"kind": "unknown", "description": "Unknown parameter kind"}
        )

    def _is_parameter_required(param, has_default: bool) -> bool:
        """Determine if a parameter is required."""
        if has_default:
            return False
        if param.kind in (
            inspect.Parameter.VAR_POSITIONAL,
            inspect.Parameter.VAR_KEYWORD,
        ):
            return False
        return True

    def _get_parameter_metadata(param, func) -> dict:
        """Extract additional parameter metadata."""
        metadata = {}

        # Check if parameter has a docstring in function doc
        doc = inspect.getdoc(func)
        if doc:
            # Simple heuristic: look for parameter mentions in docstring
            lines = doc.split("\n")
            for line in lines:
                if f"{param.name}:" in line.lower():
                    metadata["doc_mentioned"] = True
                    break

        return metadata

    def _get_function_metadata(func, params) -> dict:
        """Extract function-level metadata."""
        return {
            "function_name": func.__name__,
            "module": getattr(func, "__module__", "unknown"),
            "total_parameters": len(params),
            "required_parameters": sum(1 for p in params if p["required"]),
            "has_varargs": any(
                p["kind"] == ParameterKind.VAR_POSITIONAL.value for p in params
            ),
            "has_varkwargs": any(
                p["kind"] == ParameterKind.VAR_KEYWORD.value for p in params
            ),
            "docstring": inspect.getdoc(func) or "No docstring",
        }

    def _format_output(params, function_info, format_type: str) -> Union[list, str]:
        """Format the output according to the requested format."""
        if format_type == "dict":
            return {"function_info": function_info, "parameters": params}

        elif format_type == "json":
            # Custom JSON encoder for complex objects
            class CustomEncoder(json.JSONEncoder):
                def default(self, obj):
                    if isinstance(obj, type):
                        return obj.__name__
                    try:
                        return super().default(obj)
                    except:
                        return str(obj)

            return json.dumps(
                {"function_info": function_info, "parameters": params},
                cls=CustomEncoder,
                indent=2,
            )

        elif format_type == "pretty":
            return _format_pretty(params, function_info)

        elif format_type == "markdown":
            return _format_markdown(params, function_info)

        elif format_type == "error":
            return _format_error_pretty(params, function_info)

        else:
            raise ValueError(f"Unknown format: {format_type}")

    def _format_pretty(params, function_info) -> str:
        """Format output as a pretty string."""
        lines = []
        lines.append(
            f"'{function_info['function_name']}':from {function_info['module']} import {function_info['function_name']} #{function_info['required_parameters']}/{function_info['total_parameters']} params are required"
        )
        for param in params:
            req = "x" if param["required"] else " "
            lines.append(
                f"  [{req}] {param['name']}: {param['type']} = {param['default_repr']}"
            )

            if param["type_details"]:
                lines.append(f"       Details: {param['type_details']}")
        lines.append("")

        return "\n".join(lines)

    def _format_markdown(params, function_info) -> str:
        """Format output as Markdown."""
        lines = []
        lines.append(f"# Function: `{function_info['function_name']}`")
        lines.append("")
        lines.append(f"**Module:** `{function_info['module']}`  ")
        lines.append(f"**Total Parameters:** {function_info['total_parameters']}  ")
        lines.append(
            f"**Required Parameters:** {function_info['required_parameters']}  "
        )
        lines.append("")

        if function_info["docstring"] and function_info["docstring"] != "No docstring":
            lines.append("## Docstring")
            lines.append("```")
            lines.append(function_info["docstring"])
            lines.append("```")
            lines.append("")

        lines.append("## Parameters")
        lines.append("")
        lines.append("| Name | Type | Default | Kind | Required |")
        lines.append("|------|------|---------|------|----------|")

        for param in params:
            req = "True" if param["required"] else "False"
            default = (
                f"`{param['default_repr']}`" if param["has_default"] else "**None**"
            )
            lines.append(
                f"| `{param['name']}` | `{param['type']}` | {default} | "
                f"{param['kind_description']} | {req} |"
            )

        return "\n".join(lines)

    def _format_error_pretty(error_info, function_info=None) -> str:
        """Format error output as a pretty string."""
        lines = []
        lines.append(f"{error_info['message']}")
        lines.append(f"   Type: {error_info['type']}")
        
        if 'object_repr' in error_info:
            lines.append(f"   Object: {error_info['object_repr']}")
        
        if 'suggestions' in error_info and verbose:
            lines.append("")
            lines.append("Suggestions:")
            for suggestion in error_info['suggestions']:
                lines.append(f"   ‚Ä¢ {suggestion}")
        
        # Additional module-specific suggestions
        if error_info['type'] == 'module' and verbose:
            lines.append("")
            lines.append("   Try inspecting specific functions from this module:")
            lines.append("   Example: man(module.function_name)")
            lines.append("   To see available functions: dir(module)")
        
        return "\n".join(lines)

    def _handle_non_callable(obj, format_output: str) -> Union[dict, str]:
        """Handle non-callable objects gracefully."""
        error_info = {
            "error": True,
            "type": type(obj).__name__,
            "message": f"Object of type {type(obj).__name__} is not callable",
            "object_repr": str(obj)[:100] + "..." if len(str(obj)) > 100 else str(obj)
        }
        
        # Provide helpful suggestions based on object type
        if hasattr(obj, '__file__'):
            error_info["suggestions"] = [
                "This appears to be a module. Try inspecting specific functions from this module.",
                "Example: man(module.function_name)",
                "To see available functions: dir(module) or [f for f in dir(module) if callable(getattr(module, f))]"
            ]
        elif hasattr(obj, '__class__'):
            error_info["suggestions"] = [
                "This appears to be a class instance. Try:",
                "- man(Class.method_name) for specific methods",
                "- dir(obj) to see available methods"
            ]
        
        if format_output == "error":
            return error_info
        elif format_output == "pretty":
            return _format_error_pretty(error_info)
        elif format_output == "json":
            return json.dumps(error_info, indent=2)
        else:  # dict format
            return error_info

    ####################################
    # main func for man
    ####################################
    
    # First, check if it's actually callable
    if not callable(func):
        res_man = _handle_non_callable(func, format_output)
        print(res_man)
        return res_man if return_obj else None
    
    try:
        sig = inspect.signature(func)
    except (ValueError, TypeError) as e:
        error_info = {
            "error": True,
            "type": type(func).__name__,
            "message": f"Cannot inspect function: {e}",
            "object_repr": str(func)[:100] + "..." if len(str(func)) > 100 else str(func)
        }
        
        if format_output == "pretty":
            res_man = _format_error_pretty(error_info)
        elif format_output == "json":
            res_man = json.dumps(error_info, indent=2)
        else:
            res_man = error_info

        return res_man if return_obj else None

    results = []
    params=[]

    for name, param in sig.parameters.items():
        # Skip private parameters if requested
        if not include_private and name.startswith("_"):
            continue

        # Get enhanced type information
        type_info = _get_enhanced_type_info(param, func, resolve_types)

        # Enhanced default value handling
        default_info = _get_default_info(param)

        # Enhanced parameter kind
        kind_info = _get_kind_info(param)

        # Additional metadata
        metadata = _get_parameter_metadata(param, func)

        param_data = {
            "name": name,
            "type": type_info["type_str"],
            "required": _is_parameter_required(param, default_info["has_default"]),
            "type_details": type_info["details"],
            "default": default_info["value"],
            "default_repr": default_info["repr"],
            "has_default": default_info["has_default"],
            "is_optional": type_info["is_optional"],
            "kind": kind_info["kind"],
            "kind_description": kind_info["description"],
            "metadata": metadata,
        }

        results.append(param_data)
    params=[i for i in sig.parameters]

    # Add function metadata
    function_info = _get_function_metadata(func, results)
    res_man = _format_output(results, function_info, format_output)
    print(res_man) if verbose else None
    return params if return_obj else None

def now(
    s: str = "datetime",
    fmt: Optional[str] = None,
    return_obj: Optional[bool] = None,
    delta_day: int = 0,
    delta_hour: int = 0,
    delta_minute: int = 0,
    delta_second: int = 0,
    verbose: bool = True
):
    """
    Returns various time-related info based on the argument `s`.

    Parameters:
    - s (str): The timepoint you want, accepts common aliases.
    - return_obj (bool): If True, returns the corresponding object instead of string.
    - delta_day, delta_hour, delta_minute, delta_second (int): time delta to add/subtract.
    - fmt (str): Custom format string for output (e.g., "%d.%m.%y").

    Returns:
    - str, int, or datetime/date/time object depending on `return_obj` and `s`.

    Usage:
        now('date', delta_day=10, fmt="%d.%m.%y")
        now('datetime', delta_hour=-5, fmt="%Y-%m-%d %H:%M")
        now('calendar', fmt='three')  # Show three months
    """
    # Check if the first argument is actually a format string
    if '%' in s and fmt is None:
        # If s looks like a format string and no explicit fmt is provided,
        # treat s as the format and use 'datetime' as the default type
        fmt = s
        s = "datetime"
    # Alias map for inputs
    alias_map = {
        "year": ["year", "y", "yr", "years"],
        "month": ["month", "m", "mth", "months"],
        "day": ["day", "d", "days"],
        "hour": ["hour", "h", "hr", "hours"],
        "minute": ["minute", "min", "m", "minutes"],
        "second": ["second", "sec", "s", "seconds"],
        "week": ["week", "w", "wk", "weeks"],
        "weekday": ["weekday", "dayname", "weekdayname", "day_of_week"],
        "date": ["date", "dt", "daydate"],
        "time": ["time", "tm"],
        "datetime": ["datetime", "dtm", "date_time", "now"],
        "iso": ["iso", "iso8601", "iso_8601"],
        "calendar": ["calendar", "cal", "monthcalendar", "month_cal"],
        "timestamp": ["timestamp", "ts", "posix", "unix"],
    }

    all_aliases = [alias for aliases in alias_map.values() for alias in aliases]
    s_lower = s.lower()
    matched_alias = strcmp(s_lower, all_aliases)[0]

    canonical_key = None
    for key, aliases in alias_map.items():
        if matched_alias in aliases:
            canonical_key = key
            break
    if canonical_key is None:
        raise ValueError(
            f"Unsupported timepoint '{s}'. Supported keys: {list(alias_map.keys())}"
        )
    if canonical_key == "calendar":
        import calendar

        def print_calendar(calendar_data):
            """Print calendar data, handling both string and list formats."""
            if isinstance(calendar_data, list):
                for line in calendar_data:
                    print(line)
            else:
                print(calendar_data)
        def generate_single_month_view(dt: datetime) -> str:
            """Generate a single month calendar view with highlighted current day."""
            cal = calendar.TextCalendar(calendar.MONDAY)
            month_calendar = cal.formatmonth(dt.year, dt.month)
            today = dt.day

            # Highlight today in the calendar
            month_lines = month_calendar.split("\n")
            highlighted = []

            for line in month_lines:
                if any(char.isdigit() for char in line):
                    # Process line with numbers (days)
                    parts = line.split()
                    new_parts = []
                    for p in parts:
                        try:
                            day_num = int(p)
                            if day_num == today:
                                p = f"\033[1;37;44m{day_num:2d}\033[0m"  # Highlight with ANSI colors
                            else:
                                p = f"{day_num:2d}"
                        except ValueError:
                            pass
                        new_parts.append(p)
                    highlighted.append(" ".join(new_parts))
                else:
                    highlighted.append(line)

            return "\n".join(highlighted)

        def generate_three_month_view(dt: datetime) -> List[str]:
            """Generate a three-month calendar view (previous, current, next)."""
            # Calculate previous and next months
            first_day = dt.replace(day=1)
            prev_month = (first_day - timedelta(days=1)).replace(day=1)
            next_month = (first_day + timedelta(days=32)).replace(day=1)
            
            # Generate calendars for each month
            cal = calendar.TextCalendar(calendar.MONDAY)
            
            # Get calendar for each month and highlight current day in current month
            prev_cal = cal.formatmonth(prev_month.year, prev_month.month).splitlines()
            curr_cal = highlight_current_day_in_calendar(cal.formatmonth(dt.year, dt.month).splitlines(), dt.day)
            next_cal = cal.formatmonth(next_month.year, next_month.month).splitlines()
            
            # Ensure all calendars have the same number of lines
            max_lines = max(len(prev_cal), len(curr_cal), len(next_cal))
            prev_cal.extend([''] * (max_lines - len(prev_cal)))
            curr_cal.extend([''] * (max_lines - len(curr_cal)))
            next_cal.extend([''] * (max_lines - len(next_cal)))
            
            # Combine calendars side by side with proper spacing
            header = f"{prev_month.strftime('%B %Y'):^20}   {dt.strftime('%B %Y'):^20}   {next_month.strftime('%B %Y'):^20}"
            separator = "‚îÄ" * 20 + "   " + "‚îÄ" * 20 + "   " + "‚îÄ" * 20
            
            combined = [header, separator]
            
            for i in range(2, max_lines):  # Skip header lines
                prev_line = prev_cal[i].ljust(20)
                curr_line = curr_cal[i].ljust(20)
                next_line = next_cal[i].ljust(20)
                combined.append(f"{prev_line}   {curr_line}   {next_line}")
            
            return combined

        def highlight_current_day_in_calendar(calendar_lines: List[str], day: int) -> List[str]:
            """Highlight the current day in a calendar."""
            highlighted = []
            for line in calendar_lines:
                if any(char.isdigit() for char in line):
                    # Process line with numbers (days)
                    parts = line.split()
                    new_parts = []
                    for p in parts:
                        try:
                            day_num = int(p)
                            if day_num == day:
                                p = f"\033[1;37;44m{day_num:2d}\033[0m"  # Highlight with ANSI colors
                            else:
                                p = f"{day_num:2d}"
                        except ValueError:
                            pass
                        new_parts.append(p)
                    highlighted.append(" ".join(new_parts))
                else:
                    highlighted.append(line)
            return highlighted

    if return_obj is None:
        if fmt is None:
            return_obj = (
                True if canonical_key in ["datetime", "date", "time"] else False
            )
        else:
            return_obj = False

    now_dt = datetime.now()
    delta = timedelta(
        days=delta_day, hours=delta_hour, minutes=delta_minute, seconds=delta_second
    )
    adjusted_dt = now_dt + delta

    # Handle custom formatting
    def format_output(dt_obj, default_format):
        if fmt and not return_obj:
            return dt_obj.strftime(fmt)
        return dt_obj.strftime(default_format) if not return_obj else dt_obj

    if canonical_key == "year":
        return adjusted_dt.year if return_obj else format_output(adjusted_dt, "%Y")

    elif canonical_key == "month":
        if return_obj:
            return adjusted_dt.month
        return adjusted_dt.strftime(fmt) if fmt else adjusted_dt.strftime("%B")

    elif canonical_key == "day":
        return adjusted_dt.day if return_obj else format_output(adjusted_dt, "%d")

    elif canonical_key == "hour":
        return adjusted_dt.hour if return_obj else format_output(adjusted_dt, "%H")

    elif canonical_key == "minute":
        return adjusted_dt.minute if return_obj else format_output(adjusted_dt, "%M")

    elif canonical_key == "second":
        return adjusted_dt.second if return_obj else format_output(adjusted_dt, "%S")

    elif canonical_key == "week":
        week_num = adjusted_dt.isocalendar()[1]
        return week_num if return_obj else f"Week {week_num}"

    elif canonical_key == "weekday":
        if return_obj:
            return adjusted_dt.weekday()
        return adjusted_dt.strftime(fmt) if fmt else adjusted_dt.strftime("%A")

    elif canonical_key == "date":
        return (
            format_output(adjusted_dt, "%Y-%m-%d")
            if not return_obj
            else adjusted_dt.date()
        )

    elif canonical_key == "time":
        return (
            format_output(adjusted_dt, "%H:%M:%S")
            if not return_obj
            else adjusted_dt.time()
        )

    elif canonical_key == "datetime":
        return (
            format_output(adjusted_dt, "%Y-%m-%d %H:%M:%S")
            if not return_obj
            else adjusted_dt
        )

    elif canonical_key == "iso":
        return adjusted_dt.isoformat()

    elif canonical_key == "timestamp":
        ts = adjusted_dt.timestamp()
        return ts if return_obj else str(ts)

    elif canonical_key == "calendar":
        # Handle different calendar view options
        view_type = fmt if fmt else "single"

        if view_type in ["three",'3']:
            # Show three months: previous, current, and next
            calendar_data = generate_three_month_view(adjusted_dt)
        else:
            # Single month view with highlighting
            calendar_data =  generate_single_month_view(adjusted_dt)
        
        return print_calendar(calendar_data) if verbose else calendar_data

    else:
        raise ValueError(f"Unhandled key '{canonical_key}'")
 

def _yaoshi_fernet(mima="mimashigudingde",yan=b"mimashigudingde",verbose=True):
    import base64
    from cryptography.hazmat.primitives.kdf.pbkdf2 import PBKDF2HMAC
    from cryptography.hazmat.primitives import hashes
    from cryptography.hazmat.backends import default_backend 
    kdf = PBKDF2HMAC(
        algorithm=hashes.SHA256(),
        length=32,  
        salt=yan,
        iterations=100000,
        backend=default_backend()
    ) 
    return base64.urlsafe_b64encode(kdf.derive(mima.encode()))

def fenpass(fpath: str, password: str, copy: bool = True):
    """ÂØπÊñá‰ª∂ÊàñÊï¥‰∏™Êñá‰ª∂Â§πËøõË°åÂä†ÂØÜ
    Example:
        fpath = r'path'
        pwd = '****'
        copy_mode = "yes" # True/False
        fenpass(fpath, pwd, copy_mode)
        """

    def encrypt_xlsx(file_path: str, password: str, copy: bool = True):
        """‰ΩøÁî® msoffcrypto ËøõË°å Excel Âä†ÂØÜÔºå‰ΩøÂÖ∂ÈúÄË¶ÅÂØÜÁ†ÅÊâçËÉΩÊâìÂºÄ"""
        import msoffcrypto
        encrypted_path = file_path.replace(".xlsx", "_en.xlsx") if copy else file_path

        try:
            with open(file_path, "rb") as f:
                office_file = msoffcrypto.OfficeFile(f)

                if office_file.is_encrypted():
                    print(f"Excel Êñá‰ª∂Â∑≤Âä†ÂØÜÔºåË∑≥Ëøá: {file_path}")
                    return

                with open(encrypted_path, "wb") as ef:
                    office_file.encrypt(password, ef)
            
            print(f"Excel Êñá‰ª∂Â∑≤Âä†ÂØÜ: {encrypted_path}")
        except Exception as e:
            print(f"Âä†ÂØÜ Excel Êñá‰ª∂Â§±Ë¥•: {file_path}, ÈîôËØØ‰ø°ÊÅØ: {str(e)}")

    def encrypt_pdf(file_path: str, password: str, copy: bool = True):
        """Âä†ÂØÜ PDF Êñá‰ª∂Ôºå‰ΩøÂÖ∂ÈúÄË¶ÅÂØÜÁ†ÅÊâçËÉΩÊâìÂºÄ""" 
        from pypdf import PdfReader, PdfWriter
        reader = PdfReader(file_path)
        if reader.is_encrypted:
            print(f"PDF Êñá‰ª∂Â∑≤Âä†ÂØÜÔºåË∑≥Ëøá: {file_path}")
            return
        writer = PdfWriter()
        for page in reader.pages:
            writer.add_page(page)
        writer.encrypt(password)
        encrypted_path = file_path.replace(".pdf", "_en.pdf") if copy else file_path
        with open(encrypted_path, "wb") as f:
            writer.write(f)
        print(f"PDF Êñá‰ª∂Â∑≤Âä†ÂØÜ: {encrypted_path}") 
    def encrypt_txt(file_path: str, password: str, copy: bool = True):
        """Âä†ÂØÜÊñáÊú¨Êñá‰ª∂Ôºå‰ΩøÁî®ÂØπÁß∞Âä†ÂØÜ"""
        from cryptography.fernet import Fernet
        key = _yaoshi_fernet()
        cipher = Fernet(key)
        with open(file_path, "rb") as f:
            encrypted_data = cipher.encrypt(f.read())
        encrypted_path = file_path.replace(".txt", "_en.txt") if copy else file_path
        with open(encrypted_path, "wb") as f:
            f.write(encrypted_data)
        print(f"ÊñáÊú¨Êñá‰ª∂Â∑≤Âä†ÂØÜ: {encrypted_path}")
    def encrypt_zip(file_path: str, password: str, copy: bool = True): 
        try:
            import pyzipper
        except Exception as e:
            print(e)
        encrypted_path = file_path.replace(".zip", "_enc.zip") if copy else file_path

        try:
            with pyzipper.AESZipFile(
                encrypted_path,
                "w",
                compression=pyzipper.ZIP_DEFLATED,
                encryption=pyzipper.WZ_AES,
            ) as zout:
                zout.setpassword(password.encode())

                # Read the original zip file
                with pyzipper.ZipFile(file_path, "r") as zin:
                    for item in zin.infolist():
                        if not item.is_dir():
                            with zin.open(item.filename) as file:
                                zout.writestr(
                                    item.filename,
                                    file.read(),
                                    compress_type=pyzipper.ZIP_DEFLATED,
                                )

            print(f"ZIPÊñá‰ª∂Â∑≤Âä†ÂØÜ: {encrypted_path}")
            return encrypted_path

        except Exception as e:
            print(f"Âä†ÂØÜÂ§±Ë¥•: {str(e)}")
            delete(encrypted_path)
            return ""
    def encrypt_zip(file_path: str, password: str, copy: bool = True):
        """Âä†ÂØÜ ZIP Êñá‰ª∂Ôºå‰ΩøÂÖ∂ÈúÄË¶ÅÂØÜÁ†ÅÊâçËÉΩËß£Âéã"""

        import zipfile
        encrypted_path = file_path.replace(".zip", "_enc.zip") if copy else file_path
        
        try:
            with zipfile.ZipFile(file_path, 'r') as zin:
                with zipfile.ZipFile(encrypted_path, 'w', zipfile.ZIP_DEFLATED, compresslevel=9) as zout:
                    for item in zin.infolist():
                        # Skip directory entries
                        if item.is_dir():
                            continue
                        # Encrypt file contents with password
                        zout.writestr(
                            item, 
                            zin.read(item.filename, pwd=None), 
                            zipfile.ZipInfo(item.filename),
                            compress_type=zipfile.ZIP_DEFLATED,
                            compresslevel=9,
                            pwd=password.encode()
                        )
            print(f"ZIP Êñá‰ª∂Â∑≤Âä†ÂØÜ: {encrypted_path}")
        except Exception as e:
            print(f"Âä†ÂØÜÂ§±Ë¥•:{e}") 

    def encrypt_file(file_path: str, password: str, copy: bool = True):
        """Ê†πÊçÆÊñá‰ª∂Á±ªÂûãÈÄâÊã©ÂêàÈÄÇÁöÑÂä†ÂØÜÊñπÊ≥ï"""
        ext = os.path.splitext(file_path)[1].lower()
        try:
            if ext == ".xlsx":
                encrypt_xlsx(file_path, password, copy)
            elif ext == ".pdf":
                encrypt_pdf(file_path, password, copy)
            # elif ext == ".docx":
            #     print(f"‰∏çÊîØÊåÅÁöÑÊñá‰ª∂Á±ªÂûã: {ext}")
            elif ext == ".txt":
                encrypt_txt(file_path, password, copy)
            elif ext == ".zip":
                encrypt_zip(file_path, password, copy)
            else:
                try:
                    enpass_bin(file_path, password, copy)
                except:
                    print(f"‰∏çÊîØÊåÅÁöÑÊñá‰ª∂Á±ªÂûã: {ext}")
        except Exception as e:
            print(e) 
    if os.path.isdir(fpath):
        for root, _, files in os.walk(fpath):
            for file in files:
                encrypt_file(os.path.join(root, file), password, copy)
    else:
        encrypt_file(fpath, password, copy)

def generate_word_variations(args: Dict[str, Any]) -> Set[str]:
    """Worker function for word variations (now includes special chars + years)"""
    
    word_chunk = args['word_chunk']
    digits = args['digits']
    years = args['years']
    max_len = args['max_len']
    include_leet_speak = args['include_leet_speak']
    
    chunk_combos = set()
    
    @lru_cache(maxsize=5000)
    def get_variations(word: str) -> Set[str]:
        variations = {word.lower(), word.upper(), word.capitalize()}
        
        if include_leet_speak:
            leet_map = str.maketrans({
                "a": "@", "e": "3", "i": "1", "o": "0",
                "s": "$", "t": "7", "b": "8", "g": "9", "z": "2"
            })
            leet = word.lower().translate(leet_map)
            if leet != word.lower():
                variations.add(leet)
        
        return variations
    
    for w in word_chunk:
        variations = get_variations(w)
        chunk_combos.update(variations)
        
        # Add combinations with special chars and numbers
        for v in variations:
            # Simple digits (123, 007, etc.)
            for s in digits:
                if len(v + s) <= max_len:
                    chunk_combos.add(v + s)
            
            # Years (2023, 2024, etc.)
            for y in years:
                if len(v + y) <= max_len:
                    chunk_combos.add(v + y)
            
            # Special chars (_, -, ., etc.)
            for sp in ["_", "-", ".", "@", "#"]:
                # Word + special char (lengerke_)
                if len(v + sp) <= max_len:
                    chunk_combos.add(v + sp)
                
                # Word + special char + digits (lengerke_123)
                for s in digits:
                    if len(v + sp + s) <= max_len:
                        chunk_combos.add(v + sp + s)
                
                # Word + special char + year (lengerke_2023)
                for y in years:
                    if len(v + sp + y) <= max_len:
                        chunk_combos.add(v + sp + y)
    
    return chunk_combos


def generate_date_combos(args: Dict[str, Any]) -> Set[str]:
    """Worker function for date combinations"""
    months = args['months']
    years = args['years']
    days = args['days']
    months_num = args['months_num']
    max_len = args['max_len']
    
    date_combos = set()
    if not years:  # if dates are disabled
        return date_combos

    for m in months:
        for y in years:
            if len(m + y) <= max_len:
                date_combos.add(m + y)
            if len(m.capitalize() + y) <= max_len:
                date_combos.add(m.capitalize() + y)

    for d in days:
        for m in months_num:
            for y in years[-20:]:
                if len(d + m + y) <= max_len:
                    date_combos.add(d + m + y)
                if len(y + m + d) <= max_len:
                    date_combos.add(y + m + d)
    return date_combos


def generate_special_chars(args: Dict[str, Any]) -> Set[str]:
    """Worker function for special character additions"""
    combo_chunk = args['combo_chunk']
    specials = args['specials']
    max_len = args['max_len']
    
    special_combos = set()
    for w in combo_chunk:
        for sp in specials:
            if len(w + sp) <= max_len:
                special_combos.add(w + sp)
    return special_combos

def gen_common_pass(
    limit: Optional[int] = None,
    shuffle: bool = False,
    extra_words: Optional[List[str]] = None,
    include_keyboard_patterns: bool = True,
    include_dates: bool = True,
    include_names: bool = True,
    include_leet_speak: bool = True,
    min_length: Optional[int] = None,
    max_length: Optional[int] = None,
    n_cpu: Optional[int] = None,
) -> List[str]:
    """
    Multi-process version of password generator that maintains all combinations.
    Enhanced to handle cases like "Lengerke_2023" when extra_words="lengerke" is provided.
    """
    from multiprocessing import Pool, cpu_count
    import random
    from datetime import datetime

    # Determine number of n_cpu to use
    if n_cpu is None:
        n_cpu = max(1, cpu_count() - 1)  # Leave one core free

    # Common word lists
    base_words = ["password","admin","welcome","login","passw0rd",
        "secret","security","123456",
        "123456789",
        "111111",
        "654321",
        "000000",
        "121212",
        "7777777",
        "qwerty",
        "abc123",
        "1q2w3e",
        "zaq12wsx",
        "!qaz2wsx",
        "1qaz@wsx",
        "iloveyou",
        "sunshine",
        "princess",
        "superman",
        "starwars",
        "football",
        "baseball",
        "dragon",
        "monkey",
        "shadow",
        "master",
        "letmein",
        "trustno1",
        "access",
        "account",
        "backup",
        "database",
        "default",
        "system",
        "root",
        "user",
        "admin123",
        "test123",
        "temp",
        "file",
        "document",
        "data",
        "info",
    ]
    if extra_words:
        base_words.extend(extra_words)

    names = (
        [
            "john",
            "sarah",
            "michael",
            "jessica",
            "david",
            "linda",
            "james",
            "jennifer",
            "robert",
            "lisa",
            "william",
            "mary",
            "richard",
            "susan",
            "joseph",
            "karen",
        ]
        if include_names
        else []
    )

    months = [
        "jan",
        "feb",
        "mar",
        "apr",
        "may",
        "jun",
        "jul",
        "aug",
        "sep",
        "oct",
        "nov",
        "dec",
    ]

    keyboard = (
        [
            "qwerty",
            "asdfgh",
            "zxcvbn",
            "1qaz",
            "qazwsx",
            "!qaz",
            "1q2w3e",
            "1q2w3e4r",
            "123qwe",
            "12345",
            "qwertyuiop",
            "asdfghjkl",
            "zxcvbnm",
            "qwert",
            "asdf",
            "zxcv",
            "wasd",
            "uiop",
            "hjkl",
            "vbnm",
            "123456",
            "789456",
            "159753",
            "qwertz",
            "abcdef",
        ]
        if include_keyboard_patterns
        else []
    )

    words = base_words + names + months + keyboard

    # Date components
    current_year = datetime.now().year
    years = [str(y) for y in range(1945, current_year + 3)] if include_dates else []
    days = [f"{d:02d}" for d in range(1, 32)] if include_dates else []
    months_num = [f"{m:02d}" for m in range(1, 13)] if include_dates else []

    digits = [
        "",
        "1",
        "12",
        "123",
        "1234",
        "12345",
        "123456",
        "007",
        "000",
        "321",
        "111",
        "999",
        "0000",
        "1111",
        "2222",
        "3333",
        "4444",
        "5555",
        "6666",
        "7777",
        "8888",
        "9999",
    ]
    specials = [
        "",
        "!",
        "@",
        "#",
        "$",
        "%",
        "^",
        "&",
        "*",
        "?",
        "_",
        "-",
        "+",
        "=",
        "~",
        "!@",
        "!@#",
        "$%^",
        "&*",
        "()",
        "{}",
        "[]",
        "<>",
        "!@#$",
        "!@#^",
        "!@#$%",
        "!@#$&",
        "!@#$*",
        "!@#$?",
        "!@#$~",
        "!@#$%&",
        "!@#$%^",
        "!@#$%^&",
        "!@#$%^*",
        "!@#$%^?",
        "!@#$%^~",
        "!@#$%^&*",
        "!@#$%^&?",
        "!@#$%^&~",
        "!@#$%^&*~",
        "!@#$%^&*?",
        "!@#$%^&*?~",
    ]

    # Enhanced special combinations for extra words
    extra_special_combos = set()
    if extra_words:
        for word in extra_words:
            # Add original word variations
            extra_special_combos.update({word.lower(), word.upper(), word.capitalize()})

            # Add combinations with special chars and numbers
            for sp in ["", "_", "-", ".", "@", "#", "$", "!"]:
                for num in ["", *years[-5:], *digits[-5:]]:
                    combo = f"{word.capitalize()}{sp}{num}"
                    if (max_length is None or len(combo) <= max_length) and (
                        min_length is None or len(combo) >= min_length
                    ):
                        extra_special_combos.add(combo)

    # Length constraints
    max_len = max_length if max_length is not None else float("inf")
    min_len = min_length if min_length is not None else 0

    # Prepare shared arguments
    common_args = {
        "digits": digits,
        "years": years,
        "months": months,
        "days": days,
        "months_num": months_num,
        "specials": specials,
        "max_len": max_len,
        "include_leet_speak": include_leet_speak,
    }

    # --- Main parallel generation ---
    combos = set()

    # 1. Parallelize word variations
    with Pool(n_cpu) as pool:
        # Split words into chunks for parallel processing
        chunk_size = max(1, len(words) // (n_cpu * 2))
        word_chunks = [
            words[i : i + chunk_size] for i in range(0, len(words), chunk_size)
        ]

        # Prepare arguments for each chunk
        args_list = [{**common_args, "word_chunk": chunk} for chunk in word_chunks]

        # Process word variations in parallel
        for result in pool.imap_unordered(generate_word_variations, args_list):
            combos.update(result)

    # 2. Generate date combos
    date_args = {**common_args}
    combos.update(generate_date_combos(date_args))

    # 3. Parallelize special character additions
    with Pool(n_cpu) as pool:
        # Split existing combos into chunks
        combo_list = list(combos)
        chunk_size = max(1, len(combo_list) // (n_cpu * 2))
        combo_chunks = [
            combo_list[i : i + chunk_size]
            for i in range(0, len(combo_list), chunk_size)
        ]

        # Prepare arguments for each chunk
        args_list = [{**common_args, "combo_chunk": chunk} for chunk in combo_chunks]

        # Process special chars in parallel
        for result in pool.imap_unordered(generate_special_chars, args_list):
            combos.update(result)

    # Add the extra special combinations we created
    combos.update(extra_special_combos)

    # Final length filtering
    if min_len > 0 or max_len < float("inf"):
        combos = {p for p in combos if min_len <= len(p) <= max_len}

    # Convert to result
    pw_list = list(combos)

    if shuffle:
        random.shuffle(pw_list)
    else:
        base_first = sorted(set(base_words), key=lambda x: (len(x), x.lower()))
        others = sorted(combos - set(base_words), key=lambda x: (len(x), x.lower()))
        pw_list = base_first + others

    if limit is not None:
        pw_list = pw_list[:limit]

    return pw_list

def fdepass(fpath: str, password: str=None, copy: bool = True, limit:int=1000, shuffle: bool = False, extra_words=None):
    """
    ÂØπÊñá‰ª∂ÊàñÊï¥‰∏™Êñá‰ª∂Â§πËøõË°åËß£ÂØÜ
    Example:
        fpath = r'path'
        pwd = '****'
        copy_mode = "yes" # True/False
        fdepass(fpath, pwd, copy_mode)
    """
    # def decrypt_xlsx(file_path: str, password: str, copy: bool = True):
    #     """‰ΩøÁî® msoffcrypto Ëß£ÂØÜ Excel Êñá‰ª∂"""
    #     import msoffcrypto
    #     decrypted_path = file_path.replace(".xlsx", "_de.xlsx") if copy else file_path

    #     try:
    #         original_mod_time = os.path.getmtime(file_path)
    #         original_access_time = os.path.getatime(file_path)
    #         with open(file_path, "rb") as f:
    #             office_file = msoffcrypto.OfficeFile(f)
                
    #             # Check if the file is already decrypted or unsupported format
    #             if not office_file.is_encrypted():
    #                 print(f"Êñá‰ª∂Â∑≤ÁªèÊòØËß£ÂØÜÁä∂ÊÄÅÔºåË∑≥ËøáËß£ÂØÜ: {file_path}")
    #                 return

    #             # Load the password and decrypt the file
    #             office_file.load_key(password=password)

    #             # Attempt decryption
    #             with open(decrypted_path, "wb") as df:
    #                 office_file.decrypt(df) 
    #         print(f"Excel Êñá‰ª∂Â∑≤Ëß£ÂØÜ: {decrypted_path}")
    #         os.utime(decrypted_path, (original_access_time, original_mod_time))
    #     except msoffcrypto.exceptions.DecryptionError:
    #         print(f"Êó†Ê≥ïËß£ÂØÜ Excel Êñá‰ª∂ÔºåÂØÜÁ†ÅÂèØËÉΩÈîôËØØ: {file_path}")
    #     except msoffcrypto.exceptions.FileFormatError:
    #         print(f"‰∏çÊîØÊåÅÁöÑÊñá‰ª∂Ê†ºÂºèÔºåÊó†Ê≥ïÂ§ÑÁêÜÊñá‰ª∂: {file_path}")
    #     except Exception as e:
    #         print(f"Ëß£ÂØÜËøáÁ®ã‰∏≠ÂèëÁîü‰∫ÜÈîôËØØ: {str(e)}")
    def _force_pass_pdf(file_path: str, password: list, verbose=True) -> str:
        """Try to brute-force the password for a PDF file using common passwords"""
        from pypdf import PdfReader
        password_list=[password] if not isinstance(password,list) else password
        for pwd in password_list:
            try:
                reader = PdfReader(file_path)
                if reader.decrypt(pwd) and verbose:
                    print(f"ÊâæÂà∞ PDF ÂØÜÁ†Å: {pwd}")
                    return pwd
            except Exception:
                continue
        if verbose:
            print("Êó†Ê≥ïÊâæÂà∞Ê≠£Á°ÆÁöÑ PDF ÂØÜÁ†Å")
        return ""

    def decrypt_pdf(file_path: str, password: str= None, copy: bool = True, limit:int=None, shuffle: bool = True, extra_words=None):
        """Ëß£ÂØÜ PDF Êñá‰ª∂"""
        from pypdf import PdfReader, PdfWriter
        reader = PdfReader(file_path)
        if password is None:
            com_pass=gen_common_pass(limit=limit,shuffle=shuffle,extra_words=extra_words) 
            print(f"...force pass...{len(com_pass)}, shuffle={shuffle}, extra_words={extra_words}")
            password=_force_pass_pdf(file_path, password=com_pass)
            if not password:
                print("failed to get a valid pass")
                return 
        try:
            reader.decrypt(password)
        except Exception as e:
            print(f"PDF Êñá‰ª∂Ëß£ÂØÜÂ§±Ë¥•: {file_path}, ÈîôËØØ‰ø°ÊÅØ: {str(e)}")
            return

        writer = PdfWriter()
        for page in reader.pages:
            writer.add_page(page)
        
        decrypted_path = file_path.replace(".pdf", "_de.pdf") if copy else file_path
        with open(decrypted_path, "wb") as f:
            writer.write(f)
    def _force_pass_xlsx(file_path: str, password_list: list, verbose=True) -> str:
        """Try passwords against an Excel file using msoffcrypto"""
        import msoffcrypto
        password_list = [password_list] if not isinstance(password_list, list) else password_list
        
        with open(file_path, "rb") as f:
            office_file = msoffcrypto.OfficeFile(f)
            if not office_file.is_encrypted():
                if verbose:
                    print("Êñá‰ª∂Êú™Âä†ÂØÜ")
                return ""
                
            for pwd in password_list:
                try:
                    office_file.load_key(password=pwd)
                    # Test decryption with a dummy output
                    office_file.decrypt(None)  # Fast test without full decryption
                    if verbose:
                        print(f"ÊâæÂà∞ Excel ÂØÜÁ†Å: {pwd}")
                    return pwd
                except (msoffcrypto.exceptions.DecryptionError, Exception):
                    continue
        
        if verbose:
            print("Êó†Ê≥ïÊâæÂà∞Ê≠£Á°ÆÁöÑ Excel ÂØÜÁ†Å")
        return ""
    def decrypt_xlsx(
        file_path: str, 
        password: str = None, 
        copy: bool = True,
        limit: int = None,
        shuffle: bool = True,
        extra_words: list = None
    ):
        """Ëß£ÂØÜ Excel Êñá‰ª∂ÔºàÊîØÊåÅÊö¥ÂäõÁ†¥Ëß£Ôºâ"""
        import msoffcrypto
        
        
        if isinstance(password, list):
            password = _force_pass_xlsx(file_path, password, verbose=True)
        elif password is None:
            com_pass = gen_common_pass(limit=limit, shuffle=shuffle, extra_words=extra_words)
            print(f"Â∞ùËØïÊö¥ÂäõÁ†¥Ëß£... ÂØÜÁ†ÅÊï∞Èáè: {len(com_pass)}")
            password = _force_pass_xlsx(file_path, com_pass, verbose=True)
            if not password:
                print("Á†¥Ëß£Â§±Ë¥•")
                return
        decrypted_path = file_path.replace(".xlsx", "_de.xlsx") if copy else file_path

        try:
            original_mod_time = os.path.getmtime(file_path)
            original_access_time = os.path.getatime(file_path)
            
            with open(file_path, "rb") as f:
                office_file = msoffcrypto.OfficeFile(f)
                
                if not office_file.is_encrypted():
                    print(f"Êñá‰ª∂Â∑≤ÁªèÊòØËß£ÂØÜÁä∂ÊÄÅ: {file_path}")
                    return

                office_file.load_key(password=password)
                with open(decrypted_path, "wb") as df:
                    office_file.decrypt(df)
                    
            os.utime(decrypted_path, (original_access_time, original_mod_time))
            print(f"ÊàêÂäüËß£ÂØÜ: {decrypted_path}")
            return decrypted_path
            
        except msoffcrypto.exceptions.DecryptionError:
            print(f"ÂØÜÁ†ÅÈîôËØØ: {file_path}")
        except Exception as e:
            print(f"Ëß£ÂØÜÈîôËØØ: {str(e)}")
    def decrypt_zip(file_path: str, password: str, copy: bool = True):
        """Ëß£ÂØÜË¢´Âä†ÂØÜÁöÑZIPÊñá‰ª∂ÔºàÊîØÊåÅÂ§öÁßçÂéãÁº©ÊñπÊ≥ïÔºâ"""
        import zipfile
        
        
        

        # Determine output paths
        base_name = os.path.splitext(file_path)[0].replace("_enc", "")
        extract_dir = f"{base_name}_dec"
        decrypted_path = f"{base_name}_dec.zip" if copy else None

        try:
            # Create extraction directory
            os.makedirs(extract_dir, exist_ok=True)

            # First try with standard zipfile
            try:
                with zipfile.ZipFile(file_path, "r") as zin:
                    # Test password by reading first file
                    file_list = zin.namelist()
                    if file_list:
                        try:
                            zin.read(file_list[0], pwd=password.encode())
                        except RuntimeError as e:
                            if "Bad password" in str(e):
                                print(f"ÂØÜÁ†ÅÈîôËØØ: {file_path}")
                                return ""
                            raise

                    # Extract all files
                    zin.extractall(path=extract_dir, pwd=password.encode())

            except zipfile.BadZipFile:
                print(f"Êó†ÊïàÁöÑZIPÊñá‰ª∂: {file_path}")
                return ""
            except RuntimeError as e:
                if "That compression method is not supported" in str(e):
                    # Fall back to pyzipper if available
                    try:
                        import pyzipper

                        with pyzipper.AESZipFile(file_path, "r") as zin:
                            zin.extractall(path=extract_dir, pwd=password.encode())
                        print(f"Ëß£ÂØÜÊàêÂäü: {file_path}")
                    except ImportError:
                        print(f"ÈúÄË¶ÅpyzipperÂ∫ìÊù•Ëß£ÂØÜAESÂä†ÂØÜÁöÑZIPÊñá‰ª∂")
                        print("ËØ∑ÂÆâË£Ö: pip install pyzipper")
                        return ""
                    except Exception as e:
                        print(f"È´òÁ∫ßËß£ÂØÜÂ§±Ë¥•: {str(e)}")
                        return ""
                elif "Bad password" in str(e):
                    print(f"ÂØÜÁ†ÅÈîôËØØ: {file_path}")
                    return ""
                else:
                    raise

            print(f"Êñá‰ª∂Â∑≤Ëß£ÂØÜÂà∞ÁõÆÂΩï: {extract_dir}")
            # Create new decrypted zip if requested
            if copy:
                try:
                    with zipfile.ZipFile(decrypted_path, "w", zipfile.ZIP_DEFLATED) as zout:
                        for root, _, files in os.walk(extract_dir):
                            for file in files:
                                file_path_in_dir = os.path.join(root, file)
                                arcname = os.path.relpath(file_path_in_dir, extract_dir)
                                zout.write(file_path_in_dir, arcname)
                    print(f"Â∑≤ÂàõÂª∫Ëß£ÂØÜÂêéÁöÑZIPÊñá‰ª∂: {decrypted_path}")
                    return decrypted_path
                except Exception as e:
                    print(f"ÂàõÂª∫Ëß£ÂØÜZIPÂ§±Ë¥•: {str(e)}")
                    if os.path.exists(decrypted_path):
                        os.remove(decrypted_path)

            return extract_dir

        except Exception as e:
            print(f"Ëß£ÂØÜËøáÁ®ã‰∏≠ÂèëÁîüÈîôËØØ: {str(e)}")
            # Clean up
            if os.path.exists(extract_dir):
                shutil.rmtree(extract_dir)
            if decrypted_path and os.path.exists(decrypted_path):
                os.remove(decrypted_path)
            return ""
    def decrypt_txt(file_path: str, password: str, copy: bool = True):
        """Ëß£ÂØÜÊñáÊú¨Êñá‰ª∂"""
        from cryptography.fernet import Fernet 
        key = _yaoshi_fernet() 

        try:
            cipher = Fernet(key)  # Use the derived key to create the Fernet object
            with open(file_path, "rb") as f:
                encrypted_data = f.read()
                decrypted_data = cipher.decrypt(encrypted_data)

            decrypted_path = file_path.replace(".txt", "_de.txt") if copy else file_path
            with open(decrypted_path, "wb") as f:
                f.write(decrypted_data)

            print(f"ÊñáÊú¨Êñá‰ª∂Â∑≤Ëß£ÂØÜ: {decrypted_path}")

        except Exception as e:
            print(f"ÊñáÊú¨Êñá‰ª∂Ëß£ÂØÜÂ§±Ë¥•: {file_path}, ÈîôËØØ‰ø°ÊÅØ: {str(e)}")
    def decrypt_file(file_path: str, password: str, copy: bool = True):
        """Ê†πÊçÆÊñá‰ª∂Á±ªÂûãÈÄâÊã©ÂêàÈÄÇÁöÑËß£ÂØÜÊñπÊ≥ï"""
        try:
            ext = os.path.splitext(file_path)[1].lower()
            if ext == ".xlsx":
                decrypt_xlsx(file_path, password, copy,limit=limit, shuffle = shuffle, extra_words=extra_words)
            elif ext == ".pdf":
                decrypt_pdf(file_path, password, copy,limit=limit, shuffle = shuffle, extra_words=extra_words)
            # elif ext == ".docx":
            #     print(f"‰∏çÊîØÊåÅÁöÑÊñá‰ª∂Á±ªÂûã: {ext}")
            elif ext == ".txt":
                decrypt_txt(file_path, password, copy)
            elif ext == ".zip":
                decrypt_zip(file_path, password, copy)
            else:
                try:
                    depass_bin(file_path, password, copy)
                except:
                    print(f"‰∏çÊîØÊåÅÁöÑÊñá‰ª∂Á±ªÂûã: {ext}")
        except Exception as e:
            print(e) 
    if os.path.isdir(fpath):
        for root, _, files in os.walk(fpath):
            for file in files:
                decrypt_file(os.path.join(root, file), password, copy)
    else:
        decrypt_file(fpath, password, copy)
 
def derive_key(password: str, salt: bytes) -> bytes:
    """Derive cryptographic key from password using PBKDF2"""
    from cryptography.hazmat.backends import default_backend
    from cryptography.hazmat.primitives import hashes
    import base64
    from cryptography.hazmat.primitives.kdf.pbkdf2 import PBKDF2HMAC

    kdf = PBKDF2HMAC(
        algorithm=hashes.SHA256(),
        length=32,
        salt=salt,
        iterations=100000,
        backend=default_backend(),
    )
    return base64.urlsafe_b64encode(kdf.derive(password.encode()))


def enpass_bin(file_path: str, password: str, copy: bool = True) -> str:
    """Encrypt any file type using AES-256 (Fernet) with random salt"""
    from cryptography.fernet import Fernet

    salt = os.urandom(16)
    key = derive_key(password, salt)
    cipher = Fernet(key)

    original_path = file_path
    encrypted_path = original_path.replace(".","_enc.") if copy else original_path
    try:
        original_stat = os.stat(original_path)
        with open(original_path, "rb") as f_in:
            with open(encrypted_path, "wb") as f_out:
                # Write salt header
                f_out.write(salt) 
                while chunk := f_in.read(64 * 1024):  # 64KB chunks
                    f_out.write(cipher.encrypt(chunk))
        # Preserve timestamps
        os.utime(encrypted_path, (original_stat.st_atime, original_stat.st_mtime))
        print(f"Âä†ÂØÜ{encrypted_path}")
        return encrypted_path

    except Exception as e:
        print(f"Binary encryption failed: {original_path} - {str(e)}")
        delete(encrypted_path)
        return ""


def depass_bin(file_path: str, password: str, copy: bool = True) -> str:
    """Decrypt files encrypted with enpass_bin"""
    from cryptography.fernet import Fernet

    decrypted_path = file_path.replace(".", "_dec.") if copy else file_path

    try:
        with open(file_path, "rb") as f_in:
            # Read salt header
            salt = f_in.read(16)
            key = derive_key(password, salt)
            cipher = Fernet(key)

            # Preserve original metadata
            original_stat = os.stat(file_path)

            with open(decrypted_path, "wb") as f_out:
                # Decrypt in chunks
                while chunk := f_in.read(
                    64 * 1024 + 100
                ):  # Account for Fernet overhead
                    f_out.write(cipher.decrypt(chunk))

        # Restore timestamps
        os.utime(decrypted_path, (original_stat.st_atime, original_stat.st_mtime))
        print(f"Ëß£ÂØÜ{decrypted_path}")
        return decrypted_path

    except Exception as e: 
        print(f"Binary decryption failed: {file_path} - {str(e)}")
        if os.path.exists(decrypted_path):
            os.remove(decrypted_path)
        return ""

def fbackup(
    fpath: str,
    backup_dir: str,
    backup_keep_days: int = 30,
    max_backups: Optional[int] = 5,
    interval: Optional[int] = None,  # Êñ∞Â¢ûÂèÇÊï∞ÔºöÊó∂Èó¥Èó¥ÈöîÔºàÁßíÔºâÔºå‰æãÂ¶Ç 3600*12 Ë°®Á§∫ÊØè12Â∞èÊó∂ÁÆó‰∏ÄÊ¨°Â§á‰ªΩ
    compress: bool = False,
    zip_mode: bool = False,
    tar_mode: bool = False,
    checksum: bool = False,
    metadata: bool = False,
    verbose: bool = True,
    preserve_extension: bool = True,
    timestamp_format: str = "%Y%m%d_%H%M%S",
    exclude_patterns: Optional[List[str]] = None,
    include_hidden: bool = True
) -> Optional[str]:
    """
    ULTIMATE all-in-one backup function for files AND folders.
    
    Parameters:
    -----------
    fpath : str
        File or folder to back up
    backup_dir : str
        Where to store backups
    backup_keep_days : int
        Delete backups older than X days (default: 30)
    max_backups : int (optional)
        Maximum number of backups to keep
    compress : bool
        Use gzip compression (single files only)
    zip_mode : bool
        Use ZIP compression (files or folders)
    tar_mode : bool
        Use TAR compression (files or folders)
    checksum : bool
        Generate SHA256 checksum file
    metadata : bool
        Save backup metadata as JSON
    verbose : bool
        Print progress messages
    preserve_extension : bool
        Keep original file extension
    timestamp_format : str
        Custom timestamp format
    exclude_patterns : List[str]
        File patterns to exclude (e.g., ["*.tmp", "temp_*"])
    include_hidden : bool
        Include hidden files/folders (starting with .)
    
    Returns:
    --------
    str or None
        Path to the new backup, or None if failed

    # Backup a file with gzip compression
    fbackup(
        fpath="important_document.pdf",
        backup_dir="backups",
        compress=True,
        checksum=True,
        verbose=True
    )
    
    # Backup a folder with ZIP compression (excluding temp files)
    fbackup(
        fpath="project_folder",
        backup_dir="backups",
        zip_mode=True,
        exclude_patterns=["*.tmp", "temp_*"],
        metadata=True,
        backup_keep_days=60,
        max_backups=5,
        verbose=True
    )
    
    # Backup a folder with tar.gz (including hidden files)
    fbackup(
        fpath=".config",
        backup_dir="backups",
        tar_mode=True,
        include_hidden=True,
        verbose=True
    )

    """

    
    
    import gzip
    import hashlib
    import time
    from datetime import datetime
    from typing import Optional, List, Tuple
    import zipfile
    import tarfile
    import json
    
    
    # --- Setup logging ---
    def _log(msg: str, level: str = "info"):
        if verbose:
            now = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            print(f"[{now}] [{level.upper()}] {msg}")

    # --- Helper functions ---
    def _generate_checksum(file_path: str) -> str:
        sha256 = hashlib.sha256()
        with open(file_path, "rb") as f:
            for chunk in iter(lambda: f.read(4096), b""):
                sha256.update(chunk)
        return sha256.hexdigest()

    def _should_exclude(file_path: str) -> bool:
        if not exclude_patterns:
            return False
        filename = os.path.basename(file_path) 
        return (
                any(filename.endswith(pattern.lstrip('*')) for pattern in exclude_patterns) or
                any(filename.startswith(pattern.rstrip('*')) for pattern in exclude_patterns)
                )
 
    def _cleanup_old_backups(backup_prefix: str) -> None:
        try:
            now = time.time()
            cutoff = now - (backup_keep_days * 86400)
            
            backups = []
            prefix_len = len(backup_prefix)
            for f in os.listdir(backup_dir):
                if not f.startswith(backup_prefix):
                    continue
                full_path = os.path.join(backup_dir, f)
                if not os.path.isfile(full_path):
                    continue
                
                # ÊèêÂèñÊó∂Èó¥Êà≥ÈÉ®ÂàÜ
                suffix = f[prefix_len:]
                try:
                    # Âä®ÊÄÅËÆ°ÁÆóÊó∂Èó¥Êà≥ÈïøÂ∫¶
                    example_ts = datetime(2000, 1, 1, 0, 0, 0).strftime(timestamp_format)
                    ts_length = len(example_ts)
                    if len(suffix) < ts_length:
                        continue
                    timestamp_str = suffix[:ts_length]
                    file_time = datetime.strptime(timestamp_str, timestamp_format).timestamp()
                    backups.append((full_path, file_time))
                except ValueError:
                    continue  # Êó∂Èó¥Êà≥Ëß£ÊûêÂ§±Ë¥•ÂàôË∑≥Ëøá
            
            deleted = set()
            # Êåâ‰øùÁïôÂ§©Êï∞Âà†Èô§
            for path, file_time in backups:
                if file_time < cutoff:
                    try:
                        os.remove(path)
                        deleted.add(path)
                        _log(f"Deleted old backup: {path}", "debug")
                        # Âà†Èô§ÂÖ≥ËÅîÊñá‰ª∂
                        for ext in ['.sha256', '.meta']:
                            if os.path.exists(path + ext):
                                os.remove(path + ext)
                    except Exception as e:
                        _log(f"Error deleting {path}: {e}", "warning")
            
            # ÊåâÂ§á‰ªΩÊï∞ÈáèÂà†Èô§ÔºàÊñ∞Â¢û interval ÈÄªËæëÔºâ
            if max_backups and max_backups > 0:
                remaining = [b for b in backups if b[0] not in deleted]
                remaining.sort(key=lambda x: x[1])  # ÊåâÊó∂Èó¥ÂçáÂ∫èÊéíÂàóÔºàÊúÄÊóßÂú®ÂâçÔºâ
                
                if interval:
                    # ÊåâÊó∂Èó¥Èó¥ÈöîÂàÜÁªÑ
                    grouped_backups = []
                    current_group = []
                    last_time = None
                    for path, file_time in remaining:
                        if last_time is None or (file_time - last_time) >= interval:
                            if current_group:
                                grouped_backups.append(current_group)
                            current_group = [path]
                            last_time = file_time
                        else:
                            current_group.append(path)
                    if current_group:
                        grouped_backups.append(current_group)
                    
                    # ËÆ°ÁÆóÈúÄË¶ÅÂà†Èô§ÁöÑÁªÑÊï∞
                    num_to_delete = len(grouped_backups) - max_backups
                    if num_to_delete > 0:
                        for i in range(num_to_delete):
                            for path in grouped_backups[i]:
                                try:
                                    os.remove(path)
                                    _log(f"Deleted excess backup (interval-based): {path}", "debug")
                                    for ext in ['.sha256', '.meta']:
                                        if os.path.exists(path + ext):
                                            os.remove(path + ext)
                                except Exception as e:
                                    _log(f"Error deleting {path}: {e}", "warning")
                else:
                    # ÂéüÊù•ÁöÑÈÄªËæëÔºàÁõ¥Êé•ÊåâÊñá‰ª∂Êï∞Âà†Èô§Ôºâ
                    num_to_delete = len(remaining) - max_backups
                    if num_to_delete > 0:
                        for i in range(num_to_delete):
                            path = remaining[i][0]
                            try:
                                os.remove(path)
                                _log(f"Deleted excess backup: {path}", "debug")
                                for ext in ['.sha256', '.meta']:
                                    if os.path.exists(path + ext):
                                        os.remove(path + ext)
                            except Exception as e:
                                _log(f"Error deleting {path}: {e}", "warning")
        except Exception as e:
            _log(f"Cleanup error: {e}", "error")

    # --- Main backup logic ---
    try:
        # Validate source
        if not os.path.exists(fpath):
            _log(f"Source path not found: {fpath}", "error")
            return None

        is_dir = os.path.isdir(fpath)
        is_file = os.path.isfile(fpath)
        
        if not (is_dir or is_file):
            _log(f"Source is neither file nor directory: {fpath}", "error")
            return None

        # Create backup directory
        os.makedirs(backup_dir, exist_ok=True)

        # Generate backup filename
        source_name = os.path.basename(fpath.rstrip('/'))
        name, ext = os.path.splitext(source_name)
        timestamp = datetime.now().strftime(timestamp_format)
        
        if preserve_extension and is_file:
            backup_prefix = f"{name}_"
            backup_name = f"{name}_{timestamp}{ext}"
        else:
            backup_prefix = f"{source_name}_"
            backup_name = f"{source_name}_{timestamp}"

        # Add compression extension
        if tar_mode:
            backup_name += ".tar.gz"
            compress_type = "tar"
        elif zip_mode:
            backup_name += ".zip"
            compress_type = "zip"
        elif compress and is_file:
            backup_name += ".gz"
            compress_type = "gzip"
        else:
            compress_type = "none"

        backup_path = os.path.join(backup_dir, backup_name)

        # --- Perform backup ---
        _log(f"Starting backup of {fpath}...")
        
        if is_file:
            if compress_type == "gzip":
                with open(fpath, 'rb') as f_in:
                    with gzip.open(backup_path, 'wb') as f_out:
                        shutil.copyfileobj(f_in, f_out)
            elif compress_type == "zip":
                with zipfile.ZipFile(backup_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
                    zipf.write(fpath, os.path.basename(fpath))
            elif compress_type == "tar":
                with tarfile.open(backup_path, "w:gz") as tar:
                    tar.add(fpath, arcname=os.path.basename(fpath))
            else:
                shutil.copy2(fpath, backup_path)
        else:  # Directory
            if compress_type == "zip":
                with zipfile.ZipFile(backup_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
                    for root, dirs, files in os.walk(fpath):
                        if not include_hidden:
                            dirs[:] = [d for d in dirs if not d.startswith('.')]
                            files = [f for f in files if not f.startswith('.')]
                        
                        for file in files:
                            full_path = os.path.join(root, file)
                            if not _should_exclude(full_path):
                                arcname = os.path.relpath(full_path, start=fpath)
                                zipf.write(full_path, arcname)
            elif compress_type == "tar":
                with tarfile.open(backup_path, "w:gz") as tar:
                    for root, dirs, files in os.walk(fpath):
                        if not include_hidden:
                            dirs[:] = [d for d in dirs if not d.startswith('.')]
                            files = [f for f in files if not f.startswith('.')]
                        
                        for file in files:
                            full_path = os.path.join(root, file)
                            if not _should_exclude(full_path):
                                arcname = os.path.relpath(full_path, start=fpath)
                                tar.add(full_path, arcname=arcname)
            else:
                # Simple folder copy
                shutil.copytree(fpath,backup_path,
                    ignore = shutil.ignore_patterns(*exclude_patterns) if exclude_patterns else None,
                    dirs_exist_ok = False,
                    copy_function = shutil.copy2)

        _log(f"Backup created: {backup_path}")

        # Generate checksum
        if checksum and os.path.isfile(backup_path):
            checksum_value = _generate_checksum(backup_path)
            with open(backup_path + '.sha256', 'w') as f:
                f.write(checksum_value)
            _log(f"Checksum saved: {backup_path}.sha256")

        # Save metadata
        if metadata:
            meta = {
                "source": fpath,
                "backup_time": datetime.now().isoformat(),
                "type": "directory" if is_dir else "file",
                "size": os.path.getsize(backup_path) if os.path.isfile(backup_path) else 
                      sum(f.stat().st_size for f in Path(backup_path).rglob('*') if f.is_file()),
                "compression": compress_type,
                "excluded": exclude_patterns if exclude_patterns else None,
                "system": {
                    "platform": os.name,
                    "user": os.getlogin() if hasattr(os, 'getlogin') else None
                }
            }
            with open(backup_path + '.meta', 'w') as f:
                json.dump(meta, f, indent=2)
            _log(f"Metadata saved: {backup_path}.meta")

        # Clean up old backups
        if backup_keep_days > 0 or max_backups:
            _cleanup_old_backups(backup_prefix)

        return backup_path

    except Exception as e:
        _log(f"Backup FAILED: {str(e)}", "error")
        return None
 
def freplace(
    fpath: str,
    search_pattern: Union[str, Pattern],
    by: str,
    kind: Optional[List[str]] = None,
    booster: bool = True,
    dry_run: bool = False,
    backup: bool = True,
    backup_dir: str = "bak",
    encoding: str = "utf-8",
    verbose: bool = True,
) -> int:
    """
    boosterly replace a regex/string pattern with replacement in files under fpath.

    Args:
        fpath (str): Root directory to start searching files.
        search_pattern (str or compiled regex): Pattern to search for.
        by (str): Replacement string.
        kind (list of str, optional): File extensions to include (e.g., ['.py', '.txt']).
            If None, all files are processed.
        booster (bool): Whether to process subdirectories boosterly.
        dry_run (bool): If True, only print the planned changes, do not write files.
        backup (bool): Whether to create a backup copy before modifying.
        backup_suffix (str): Suffix to add for backup files.
        encoding (str): File encoding to use for reading/writing.
        verbose (bool): Print details of changes.

    Returns:
        int: Number of files changed.
    Usage:
    freplace(
    ".",
    "findAll(",
    by="find_all(")
    """
    if isinstance(search_pattern, str):
        pattern = re.compile(re.escape(search_pattern))
    else:
        pattern = search_pattern

    changed_files = 0
    for dirpath, dirnames, filenames in os.walk(fpath):
        for fname in filenames:
            if kind:
                if not any(fname.lower().endswith(ext.lower()) for ext in kind):
                    continue
            fpath = os.path.join(dirpath, fname)
            # Â§á‰ªΩ
            if backup:
                fbackup(fpath, backup_dir)
            try:
                with open(fpath, "r", encoding=encoding) as f:
                    content = f.read()
            except Exception as e:
                with open(fpath, "r", encoding=get_encoding(fpath), errors="ignore") as f:
                    content = f.read()
                if verbose:
                    print(f"[ERROR] Failed to read {fpath}: {e}")
                continue

            new_content, n_subs = pattern.subn(by, content)
            if n_subs > 0:
                changed_files += 1
                if verbose:
                    print(f"[MODIFY] {fpath}: {n_subs} replacement(s)")

                if not dry_run:
                    try:
                        with open(fpath, "w", encoding=encoding) as f:
                            f.write(new_content)
                    except Exception as e:
                        print(f"[ERROR] Failed to write {fpath}: {e}")

        if not booster:
            break

    if verbose:
        print(f"\nTotal files modified: {changed_files}")
    return changed_files

def backup(
    src="/Users/macjianfeng/Dropbox/github/python/py2ls/.venv/lib/python3.12/site-packages/py2ls/",
    tar="/Users/macjianfeng/Dropbox/github/python/py2ls/py2ls/",
    kind=None,
    depth=None,
    exclude=["_*", "*.pyc"],
    overwrite=True,
    reverse=False,
    verbose=False,
    dry_run=False,
):
    if reverse:
        src, tar = tar, src
        print(f"reversed")
    f = ls(
        src,
        kind=kind,
        exclude=["_*", "*.pyc"],
        depth=depth,
    )
    fpath_oi = f.loc[f["is_dir"] == False].path.tolist()
    [
        cp(
            i, i.replace(src, tar), overwrite=overwrite, verbose=verbose, dry_run=False
        )
        for i in fpath_oi
    ]

    print(f"[backuped] {len(fpath_oi)} items...\n[FROM] {src} \n[ TO ] {tar}")
    
def run_once_within(duration=60, reverse=False):  # default 60s
    import time

    """
    Â¶ÇÊûúreverse is True, ÂàôÂú®Á¨¨‰∏ÄÊ¨°ËøêË°åÊó∂Âπ∂‰∏çËøêË°å.‰ΩÜÊòØÂú®Á¨¨‰∫åÊ¨°ËøêË°åÊó∂ÂàôËøêË°å
    usage:
    if run_once_within():
        print("This code runs once per minute.")
    else:
        print("The code has already been run in the last minute.")
    
    """
    if not hasattr(run_once_within, "time_last"):
        run_once_within.time_last = None
    time_curr = time.time()

    if (run_once_within.time_last is None) or (
        time_curr - run_once_within.time_last >= duration
    ):
        run_once_within.time_last = time_curr  # Update the last execution time
        return False if reverse else True
    else:
        return True if reverse else False


def plt_font(dir_font: str = "/System/Library/Fonts/Hiragino Sans GB.ttc"):
    """
    Add the Chinese (default) font to the font manager
    show chinese
    Args:
        dir_font (str, optional): _description_. Defaults to "/System/Library/Fonts/Hiragino Sans GB.ttc".
    """
    import matplotlib.pyplot as plt
    from matplotlib import font_manager

    slashtype=os.sep
    if slashtype in dir_font:
        font_manager.fontManager.addfont(dir_font)
        fontname = os.path.basename(dir_font).split(".")[0]
    else:
        if "cn" in dir_font.lower() or "ch" in dir_font.lower():
            fontname = "Hiragino Sans GB"  # default Chinese font
        else:
            fontname = dir_font

    plt.rcParams["font.sans-serif"] = [fontname]
    # plt.rcParams["font.family"] = "sans-serif"
    plt.rcParams["axes.unicode_minus"] = False
    fonts_in_system = font_manager.findSystemFonts(fontpaths=None, fontext="ttf")
    fontname_in_system = [os.path.basename(i).split(".")[0] for i in fonts_in_system]
    if fontname not in fontname_in_system:
        print(f"Font '{fontname}' not found. Falling back to default.")
        plt.rcParams["font.sans-serif"] = ["Arial"]
    return fontname


# set 'dir_save'
if "dar" in sys.platform:
    dir_save = "/Users/macjianfeng/Dropbox/Downloads/"
else:
    if "win" in sys.platform:
        dir_save = "Z:\\Jianfeng\\temp\\"
    elif "lin" in sys.platform:
        dir_data = "/Users/macjianfeng/Dropbox/github/python/py2ls/confidential_data/gmail_login.json"

# ! flatten unique
def flatten(nested: Any, unique_list=True, ascending=None, ignore_case=False, verbose=False):
    """
    Recursively flattens a nested structure (lists, tuples, dictionaries, sets) into a single list.
    Parameters:
        nested : Any, Can be a list, tuple, dictionary, or set.
    Returns: list, A flattened list.
    """
    iterable_types = (list, tuple, set)
    try:
        
        iterable_types = iterable_types + (pd.Series, pd.Index)
    except ImportError:
        pass
    try:
        
        iterable_types = iterable_types + (np.ndarray,)
    except ImportError:
        pass

    flattened_list = []
    stack = [nested]
    while stack:
        current = stack.pop()
        if isinstance(current, dict):
            stack.extend(current.values())
        elif isinstance(current, iterable_types):
            stack.extend(current)
        else:
            flattened_list.append(current)
    if unique_list:
        return unique(flattened_list, ascending=ascending, ignore_case=ignore_case, verbose=verbose)
    else:
        return flattened_list

def unique(lst, ascending=None, ignore_case=False, verbose=False):
    """
    Removes duplicates from a list with optional case-insensitive sorting.

    Parameters:
    lst (list): Input list with possible duplicates.
    ascending (bool, optional): True for ascending, False for descending, None for no sort.
    ignore_case (bool, optional): Case-insensitive sorting when True.
    verbose (bool, optional): Print info about element count.

    Returns:
    list: Deduplicated list, optionally sorted.
    """
    def safe_lower(x):
        """Helper to safely convert to lowercase"""
        try:
            return x.lower()
        except AttributeError:
            return str(x).lower()

    if not lst:
        if verbose:
            print("   <in info: 0 elements>")
        return []

    if ascending is not None:
        unique_items = list(set(lst))
        key_func = (lambda x: safe_lower(x)) if ignore_case else str
        try:
            unique_items.sort(key=key_func, reverse=not ascending)
        except TypeError:
            pass  # Return unsorted if types are inconsistent
        if verbose:
            print(f"   <in info: {len(unique_items)} elements>")
        return unique_items
    else:
        seen = set()
        result = []
        for item in lst:
            if item not in seen:
                seen.add(item)
                result.append(item)
        if verbose:
            print(f"   <in info: {len(result)} elements>")
        return result

def time2do(
    when: str = "range",
    start_time: Optional[Union[str, time]] = None,
    end_time: Optional[Union[str, time]] = None,
    weekdays: Optional[Union[List[int], List[str], str, bool]] = None,
    invert: bool = False,
    timezone: Optional[str] = "Europe/Berlin",
    start_date: Optional[Union[str, date]] = None,
    end_date: Optional[Union[str, date]] = None,
    holidays: Optional[Union[List[Union[str, date]], Callable[[date], bool]]] = None,
    inclusive: str = "[]",
    *,
    cache: bool = True,
) -> bool:
    """
    Ultimate time evaluation function with complete feature set and optimized performance.

    Features:
    - Natural language time parsing ("after 2pm on weekdays")
    - Timezone awareness
    - Date ranges and holiday exclusions
    - Multiple weekday specification formats
    - Configurable time boundaries
    - Result caching for repeated calls

    Args:
        when: Time expression or keyword ("range", "never", "every day")
        start_time: Override start time (str or time object)
        end_time: Override end time (str or time object)
        weekdays: Weekday specification (list, str, or bool)
        invert: Return inverse result
        timezone: Timezone identifier
        start_date: Start date boundary
        end_date: End date boundary
        holidays: List of dates or holiday checker function
        inclusive: Time boundary inclusion ("[]", "[)", "(]", "()")
        cache: Enable result caching (default True)
    """
    from datetime import datetime, time, date, timedelta
    
    from zoneinfo import ZoneInfo

    @lru_cache(maxsize=128)
    def _cached_time2do(*args, **kwargs):
        return _time2do_impl(
            when,
            start_time,
            end_time,
            list(weekdays) if isinstance(weekdays, tuple) else weekdays,
            invert,
            timezone,
            start_date,
            end_date,
            list(holidays) if isinstance(holidays, tuple) else holidays,
            inclusive,
        )

    def _time2do_impl(
        when: str,
        start_time: Optional[Union[str, time]],
        end_time: Optional[Union[str, time]],
        weekdays: Optional[Union[List[int], List[str], str, bool]],
        invert: bool,
        timezone: Optional[str],
        start_date: Optional[Union[str, date]],
        end_date: Optional[Union[str, date]],
        holidays: Optional[Union[List[Union[str, date]], Callable[[date], bool]]],
        inclusive: str,
    ) -> bool:
        """Core implementation with all helper functions"""
        # Pre-compiled regex patterns for all parsing needs
        patterns = {
            "special": re.compile(r"^(midnight|noon)$", re.I),
            "am_pm": re.compile(r"(\d{1,2})(?::(\d{2}))?\s*([ap]m?)\b", re.I),
            "24hr": re.compile(r"(\d{1,2})(?::(\d{2}))?\b"),
            "weekday": re.compile(
                r"\b(mon|tue|wed|thu|fri|sat|sun|weekdays?|weekends?)\b", re.I
            ),
            "range_sep": re.compile(r"\b(?:to|-|and|until)\b", re.I),
            "date": re.compile(r"(\d{4})-(\d{2})-(\d{2})"),
        }

        now = _get_current_time(timezone)
        current_time = now.time()
        current_date = now.date()

        # Parse all parameters into a unified structure
        params = {
            "start_time": time(6, 0),
            "end_time": time(23, 0),
            "weekdays": None,
            "start_date": None,
            "end_date": None,
            "holidays": None,
            "never": False,
            "always": False,
        }

        # Process the 'when' string and other parameters
        _process_when_string(when, params, patterns)
        _process_time_params(start_time, end_time, params, patterns)
        _process_date_params(start_date, end_date, params, patterns)
        _process_weekdays(weekdays, params)
        _process_holidays(holidays, params, patterns)

        # Early exit conditions
        if params["never"]:
            return invert
        if params["always"]:
            return not invert

        # Check date range
        if not _check_date_range(
            current_date, params["start_date"], params["end_date"]
        ):
            return invert

        # Check holidays
        if _is_holiday(current_date, params["holidays"]):
            return invert

        # Check weekdays
        if not _check_weekday(now.weekday(), params["weekdays"]):
            return invert

        # Check time range
        in_range = _check_time_range(
            current_time, params["start_time"], params["end_time"], inclusive
        )

        return not in_range if invert else in_range

    def _get_current_time(timezone: Optional[str]) -> datetime:
        """Get current time with timezone support"""
        try:
            if timezone:
                return datetime.now(ZoneInfo(timezone))
        except Exception:
            pass
        return datetime.now()

    def _process_when_string(when: str, params: dict, patterns: dict):
        """Process the natural language 'when' string"""
        when_lower = when.lower().strip()

        # Handle special keywords
        if when_lower == "never":
            params["never"] = True
            return
        elif when_lower == "every day":
            params["always"] = True
            return

        # Extract weekdays first
        weekday_matches = patterns["weekday"].finditer(when_lower)
        for match in weekday_matches:
            if not params["weekdays"]:
                params["weekdays"] = []
            params["weekdays"].append(match.group(1))
            when_lower = when_lower.replace(match.group(), "").strip()

        # Parse time expressions
        if "between" in when_lower and "and" in when_lower:
            parts = patterns["range_sep"].split(
                when_lower.replace("between", ""), maxsplit=1
            )
            if len(parts) >= 2:
                params["start_time"] = _parse_time(parts[0], patterns)
                params["end_time"] = _parse_time(parts[1], patterns)
        elif any(sep in when_lower for sep in [" to ", "-", " until "]):
            parts = patterns["range_sep"].split(when_lower, maxsplit=1)
            if len(parts) >= 2:
                params["start_time"] = _parse_time(parts[0], patterns)
                params["end_time"] = _parse_time(parts[1], patterns)
        elif when_lower.startswith("after "):
            params["start_time"] = _parse_time(when_lower[6:], patterns)
            params["end_time"] = time(23, 59, 59)
        elif when_lower.startswith("before "):
            params["start_time"] = time(0, 0)
            params["end_time"] = _parse_time(when_lower[7:], patterns)

    def _process_time_params(start_time, end_time, params, patterns):
        """Process explicit time parameters"""
        if start_time is not None:
            params["start_time"] = _parse_time(start_time, patterns)
        if end_time is not None:
            params["end_time"] = _parse_time(end_time, patterns)

    def _parse_time(t: Union[str, time], patterns: dict) -> time:
        """Parse time from string with multiple format support"""
        if isinstance(t, time):
            return t

        t = str(t).lower().strip()

        # Handle special cases
        if match := patterns["special"].match(t):
            return time(0, 0) if match.group(1) == "midnight" else time(12, 0)

        # Parse AM/PM format
        if match := patterns["am_pm"].search(t):
            hour = int(match.group(1))
            minute = int(match.group(2) or 0)
            period = match.group(3).lower()
            if period.startswith("p") and hour != 12:
                hour += 12
            elif period.startswith("a") and hour == 12:
                hour = 0
            return time(hour, minute)

        # Parse 24-hour format
        if match := patterns["24hr"].search(t):
            hour = int(match.group(1))
            minute = int(match.group(2) or 0)
            return time(hour, minute)

        raise ValueError(f"Invalid time format: '{t}'")

    def _process_date_params(start_date, end_date, params, patterns):
        """Process date parameters"""
        if start_date is not None:
            params["start_date"] = _parse_date(start_date, patterns)
        if end_date is not None:
            params["end_date"] = _parse_date(end_date, patterns)

    def _parse_date(d: Union[str, date], patterns: dict) -> date:
        """Parse date from string or date object"""
        if isinstance(d, date):
            return d

        if match := patterns["date"].match(d):
            return date(int(match.group(1)), int(match.group(2)), int(match.group(3)))

        raise ValueError(f"Invalid date format: '{d}'. Use YYYY-MM-DD")

    def _process_weekdays(weekdays, params):
        """Process weekday specifications"""
        if weekdays is None:
            return

        if isinstance(weekdays, bool):
            params["weekdays"] = ["weekdays"] if weekdays else []
            return

        if not params["weekdays"]:
            params["weekdays"] = []

        if isinstance(weekdays, str):
            weekdays = [w.strip() for w in weekdays.split(",")]

        if isinstance(weekdays, list):
            params["weekdays"].extend(weekdays)

    def _process_holidays(holidays, params, patterns):
        """Process holiday specifications"""
        if holidays is None:
            return

        params["holidays"] = []

        if callable(holidays):
            params["holidays"] = holidays
            return

        for h in holidays:
            if isinstance(h, str):
                params["holidays"].append(_parse_date(h, patterns))
            else:
                params["holidays"].append(h)

    def _check_date_range(
        current_date: date, start_date: Optional[date], end_date: Optional[date]
    ) -> bool:
        """Check if current date is within specified range"""
        if start_date and current_date < start_date:
            return False
        if end_date and current_date > end_date:
            return False
        return True

    def _is_holiday(current_date: date, holidays) -> bool:
        """Check if date is a holiday"""
        if not holidays:
            return False
        if callable(holidays):
            return holidays(current_date)
        return current_date in [
            (
                _parse_date(h, {"date": re.compile(r"(\d{4})-(\d{2})-(\d{2})")})
                if isinstance(h, str)
                else h
            )
            for h in holidays
        ]

    def _check_weekday(current_weekday: int, weekdays_spec) -> bool:
        """Check if current weekday matches specification"""
        if not weekdays_spec:
            return True

        day_map = {
            "mon": 0,
            "tue": 1,
            "wed": 2,
            "thu": 3,
            "fri": 4,
            "sat": 5,
            "sun": 6,
            "weekday": [0, 1, 2, 3, 4],
            "weekdays": [0, 1, 2, 3, 4],
            "weekend": [5, 6],
            "weekends": [5, 6],
        }

        allowed_days = set()
        for spec in weekdays_spec:
            if isinstance(spec, int) and 0 <= spec <= 6:
                allowed_days.add(spec)
            elif isinstance(spec, str):
                spec = spec.lower()
                if spec in day_map:
                    if isinstance(day_map[spec], list):
                        allowed_days.update(day_map[spec])
                    else:
                        allowed_days.add(day_map[spec])

        return current_weekday in allowed_days if allowed_days else True

    def _check_time_range(
        current_time: time, start_time: time, end_time: time, inclusive: str
    ) -> bool:
        """Check if current time is within range with boundary options"""
        if start_time <= end_time:
            if inclusive == "[]":
                return start_time <= current_time <= end_time
            elif inclusive == "[)":
                return start_time <= current_time < end_time
            elif inclusive == "(]":
                return start_time < current_time <= end_time
            elif inclusive == "()":
                return start_time < current_time < end_time
        else:  # Crosses midnight
            if inclusive == "[]":
                return current_time >= start_time or current_time <= end_time
            elif inclusive == "[)":
                return current_time >= start_time or current_time < end_time
            elif inclusive == "(]":
                return current_time > start_time or current_time <= end_time
            elif inclusive == "()":
                return current_time > start_time or current_time < end_time

        return start_time <= current_time <= end_time  # Default

    if not cache:
        return _time2do_impl(
            when,
            start_time,
            end_time,
            weekdays,
            invert,
            timezone,
            start_date,
            end_date,
            holidays,
            inclusive,
        )

    # Convert holidays list to tuple for caching if it's a list
    cache_key = (
        when,
        start_time,
        end_time,
        (
            weekdays
            if isinstance(weekdays, (str, bool, type(None)))
            else tuple(weekdays) if weekdays else None
        ),
        invert,
        timezone,
        start_date,
        end_date,
        tuple(holidays) if isinstance(holidays, list) else holidays,
        inclusive,
    )

    return _cached_time2do(*cache_key)


# ************* below section: run_when *************
def run_when(when: str = "every 2 min", job = None, wait: int = 1):
    if "every" in when.lower():
        when = when.replace("every", "")
        run_every(when=when, job=job, wait=wait)
    elif "between" in when_lower and "and" in when_lower:
        try:
            between_part = when_lower.replace("between", "").strip()
            start, end = map(str.strip, between_part.split("and"))
            run_between(start=start, end=end, job=job, wait=wait)
        except Exception as e:
            print(f"Invalid 'between' format: {e}")
            return
    elif any([i in when.lower() for i in ["at", "@", ":", "am", "pm"]]):
        time_words = ["at", "@", ":", "am", "pm"]
        # Âà§Êñ≠'Êó∂Èó¥ËØç'ÊòØÂê¶Â≠òÂú®
        time_words_bool = [i in when.lower() for i in time_words]
        # ÊâæÂà∞'Êó∂Èó¥ËØç'ÁöÑ‰ΩçÁΩÆ
        true_indices = [index for index, value in enumerate(time_words_bool) if value]
        time_word = time_words[true_indices[0]]  # ÊâæÂà∞Á¨¨‰∏Ä‰∏™'Êó∂Èó¥ËØç'
        when = when.replace(time_word, "")  # ÂéªÈô§ Êó∂Èó¥ËØç
        run_at(when=when, job=job, wait=wait)
    else:
        print(f"Unrecognized scheduling format: '{when}'")

def run_every(when: str = None, job=None, wait: int = 1):
    """
    Schedules a job to run at the given interval.

    :param when: String specifying the interval, e.g. '2 minutes', '4 hours', '1 day'.
    :param job: The function to be scheduled.

    # usage:
        def job():
            print("1 sec")
        run_every(when="1 sec", job=job)
    """
    import schedule
    import time

    if job is None:
        print("No job provided!")
        return

    interval, unit = (
        str2num(when),
        strcmp(when.replace("every", ""), ["seconds", "minutes", "hours", "days"])[0],
    )
    print(interval, unit)
    # Mapping the scheduling based on the unit1
    if unit == "seconds":
        schedule.every(interval).seconds.do(job)
    elif unit == "minutes":
        schedule.every(interval).minutes.do(job)
    elif unit == "hours":
        schedule.every(interval).hours.do(job)
    elif unit == "days":
        schedule.every(interval).days.do(job)
    else:
        print(f"Invalid time unit: {unit}")
        return

    print(f"Scheduled job when {interval} {unit}.")

    # Keep the script running to execute the schedule
    while True:
        schedule.run_pending()
        time.sleep(wait)  # in seconds
    time.sleep(wait)  # in seconds 
def run_at(when: str, job=None, wait: int = 60):
    """
    Schedules a job to run at an exact time of the day.

    # Example usage:
    def my_job():
        print("Job executed at the exact time!")
    # Schedule the job at 14:30 when day
    run_at(when="1.30 pm", job=my_job)

    :param when: String specifying the time, e.g. '1:30 pm','1.30 am','14:30', '1:30 pm', '8:45 am'.
    :param job: The function to be scheduled.
    :param wait: The sleep interval between checks in seconds.
    """
    from datetime import datetime
    import time

    if job is None:
        print("No job provided!")
        return
    when = when.replace("A.M.", "AM").replace("P.M.", "PM")
    when = when.replace(".", ":")
    when = when.strip()

    try:
        # Attempt to parse the time in both 24-hour and 12-hour format
        if "am" in when.lower() or "pm" in when.lower():
            scheduled_time = datetime.strptime(
                when, "%I:%M %p"
            ).time()  # 12-hour format with AM/PM
        else:
            scheduled_time = datetime.strptime(when, "%H:%M").time()  # 24-hour format
    except ValueError:
        print(
            f"Invalid time format: {when}. Use 'HH:MM' (24-hour) or 'H:MM AM/PM' format."
        )
        return
    print(f"Job scheduled to run at {scheduled_time}.")
    # Keep checking the current time
    while True:
        now = datetime.now()
        # Check if current time matches the scheduled time
        if (
            now.time().hour == scheduled_time.hour
            and now.time().minute == scheduled_time.minute
        ):
            job()  # Run the job
            time.sleep(
                wait
            )  # Sleep for a minute to avoid running the job multiple times in the same minute

        time.sleep(wait)  # wait to avoid excessive CPU usage

def run_between(start: str, end: str, job=None, wait: int = 60):
    """
    Runs a job repeatedly during a specific time window each day.

    :param start: Start time (e.g., '9:00 am', '14:30')
    :param end: End time (e.g., '5:00 pm', '18:00')
    :param job: Function to execute during the window
    :param wait: Sleep interval in seconds
    """
    from datetime import datetime
    import time

    if job is None:
        print("No job provided!")
        return

    def parse_time(timestr):
        timestr = timestr.strip().lower().replace("a.m.", "am").replace("p.m.", "pm").replace(".", ":")
        try:
            if "am" in timestr or "pm" in timestr:
                return datetime.strptime(timestr, "%I:%M %p").time()
            return datetime.strptime(timestr, "%H:%M").time()
        except ValueError:
            raise ValueError(f"Invalid time format: {timestr}")

    try:
        start_time = parse_time(start)
        end_time = parse_time(end)
    except ValueError as e:
        print(e)
        return

    print(f"Running job between {start_time} and {end_time} daily.")

    while True:
        now = datetime.now().time()
        if start_time <= now <= end_time:
            job()
        time.sleep(wait)


def run_multiprocess(
    func: Callable,
    params: Iterable[Union[Tuple, Tuple[Tuple, Dict]]],
    n_cpu: int = None,
    show_progress: bool = True,
    chunksize: int = None,
    ordered: bool = True,
    context: str = None,
) -> List[Any]:
    """
    Universal multiprocessing helper that works in all environments.
    
    Parameters:
    -----------
    func : Callable
        Target function to execute
    params : Iterable of parameters
        List of arguments or (args, kwargs) tuples
    n_cpu : int, optional
        Number of worker processes
    show_progress : bool
        Show progress bar
    chunksize : int
        Task chunksize
    ordered : bool
        Maintain result order
    context : str
        Multiprocessing context ('spawn', 'fork', 'forkserver')
        
    Returns:
    --------
    List[Any]
        Results in input order
    """

    def _run_multiprocess_initializer():
        """Initialize worker processes (runs once per worker)"""
        import signal
        signal.signal(signal.SIGINT, signal.SIG_IGN)

    def _serializable_wrapper(args_kwargs: Tuple[Callable, Tuple, Dict]) -> Any:
        """Wrapper that can be pickled and handles exceptions"""
        import traceback
        func, args, kwargs = args_kwargs
        try:
            return func(*args, **kwargs)
        except Exception as e:
            return {
                "__multiprocess_error__": True,
                "type": type(e).__name__,
                "message": str(e),
                "traceback": traceback.format_exc()
            }
    
    import multiprocessing as mp
    # Validate parameters
    if not callable(func):
        raise TypeError("func must be callable")

    params_list = list(params)
    total_tasks = len(params_list)
    
    if total_tasks == 0:
        return []

    # Worker count logic
    n_cpu = min(n_cpu or mp.cpu_count(), total_tasks, 64) or 1

    # Normalize parameters
    normalized_params = []
    for item in params_list:
        if isinstance(item, tuple) and len(item) == 2 and isinstance(item[1], dict):
            normalized_params.append((func, item[0], item[1]))
        elif isinstance(item, tuple):
            normalized_params.append((func, item, {}))
        else:
            normalized_params.append((func, (item,), {}))

    # Chunksize calculation
    chunksize = chunksize or max(1, total_tasks // (n_cpu * 4))

    # Progress bar handling
    def process_tasks(pool, mapper):
        results = []
        if show_progress:
            try:
                from tqdm.auto import tqdm
                with tqdm(total=total_tasks, unit="task") as pbar:
                    for result in mapper(_serializable_wrapper, normalized_params, chunksize=chunksize):
                        results.append(result)
                        pbar.update(1)
            except ImportError:
                results = list(mapper(_serializable_wrapper, normalized_params, chunksize=chunksize))
        else:
            results = list(mapper(_serializable_wrapper, normalized_params, chunksize=chunksize))
        return results

    # Main execution
    try:
        ctx = mp.get_context(context or 'spawn')
        
        # Special handling for Jupyter notebooks
        if 'ipykernel' in sys.modules:
            import cloudpickle
            original_dumps = mp.reduction.ForkingPickler.dumps
            mp.reduction.ForkingPickler.dumps = cloudpickle.dumps

        with ctx.Pool(
            processes=n_cpu,
            initializer=_run_multiprocess_initializer
        ) as pool:
            mapper = pool.imap if ordered else pool.imap_unordered
            results = process_tasks(pool, mapper)
            
    except KeyboardInterrupt:
        print("\nInterrupt received. Terminating workers...", file=sys.stderr)
        raise
    except Exception as e:
        print(f"Parallel execution failed: {str(e)}", file=sys.stderr)
        raise
    finally:
        # Restore original dumps if we modified it
        if 'ipykernel' in sys.modules and 'original_dumps' in locals():
            mp.reduction.ForkingPickler.dumps = original_dumps

    # Process results
    final_results = []
    for res in results:
        if isinstance(res, dict) and res.get("__multiprocess_error__"):
            print(f"Task failed: {res['type']}: {res['message']}", file=sys.stderr)
            if show_progress:
                print(res["traceback"], file=sys.stderr)
            final_results.append(None)
        else:
            final_results.append(res)
    
    return final_results

# ************* above section: run_when *************
def get_timezone(timezone: str | list = None):
    if timezone is None:
        usage = """
        usage:
        datetime.now().astimezone(get_timezone("shanghai")).strftime("%H:%M")
        """
        print(usage)
        return None
    from pytz import all_timezones
    import pytz

    if isinstance(timezone, str):
        timezone = [timezone]

    # Extract the part after the "/" in time zones (if exists)
    timezones = [ssplit(i, "/")[1] if "/" in i else i for i in all_timezones]

    # Print all available time zones for debugging purposes
    # print(timezones)

    # Find and return matched time zones using strcmp
    matched_timezones = [all_timezones[strcmp(i, timezones)[1]] for i in timezone]
    if len(matched_timezones) == 1:
        return pytz.timezone(matched_timezones[0])
    else:
        return matched_timezones

def upgrade(module="py2ls", uninstall=False):
    """
    Installs or upgrades a specified Python module.

    Parameters:
    module (str): The name of the module to install/upgrade.
    uninstall (bool): If True, uninstalls the webdriver-manager before upgrading.
    """
    
    def is_package_installed(package_name):
        """Check if a package is installed."""
        import importlib.util

        package_spec = importlib.util.find_spec(package_name)
        return package_spec is not None

    if not is_package_installed(module):
        try:
            subprocess.check_call([sys.executable, "-m", "pip", "install", module])
        except subprocess.CalledProcessError as e:
            print(f"An error occurred while installing {module}: {e}")
    if uninstall:
        subprocess.check_call(["pip", "uninstall", "-y", "webdriver-manager"])
    try:
        subprocess.check_call(
            [sys.executable, "-m", "pip", "install", "--upgrade", module]
        )
    except subprocess.CalledProcessError as e:
        print(f"An error occurred while upgrading py2ls: {e}")


def get_version(pkg):
    import importlib.metadata

    def get_v(pkg_name):
        try:
            version = importlib.metadata.version(pkg_name)
            print(f"version {pkg_name} == {version}")
        except importlib.metadata.PackageNotFoundError:
            print(f"Package '{pkg_name}' not found")

    if isinstance(pkg, str):
        get_v(pkg)
    elif isinstance(pkg, list):
        [get_v(pkg_) for pkg_ in pkg]

def rm_folder(folder_path, verbose=True):
    try:
        shutil.rmtree(folder_path)
        if verbose:
            print(f"Successfully deleted {folder_path}")
    except Exception as e:
        if verbose:
            print(f"Failed to delete {folder_path}. Reason: {e}")


def fremove(path, verbose=True):
    """
    Remove a folder and all its contents or a single file.
    Parameters:
    path (str): The path to the folder or file to remove.
    verbose (bool): If True, print success or failure messages. Default is True.
    """
    try:
        if os.path.isdir(path):
            

            shutil.rmtree(path)
            if verbose:
                print(f"Successfully deleted folder {path}")
        elif os.path.isfile(path):
            os.remove(path)
            if verbose:
                print(f"Successfully deleted file {path}")
        else:
            if verbose:
                print(f"Path {path} does not exist")
    except Exception as e:
        if verbose:
            print(f"Failed to delete {path}. Reason: {e}")
 
# def get_cwd():
#     # Get the current script's directory as a Path object
#     current_directory = Path(__file__).resolve().parent
#     return current_directory

def get_cwd():
    """
    Returns the directory of the current Jupyter notebook if running in a notebook,
    otherwise returns the current working directory.
    """
    try:
        # Try to detect if running in IPython/Jupyter
        shell = get_ipython().__class__.__name__
        if shell == 'ZMQInteractiveShell':
            # Jupyter notebook or qtconsole
            try:
                import ipykernel
                import json
                from notebook import notebookapp
                import urllib.request
                import os

                # Get kernel ID from connection file
                connection_file = Path(ipykernel.get_connection_file()).resolve()
                kernel_id = connection_file.stem.split('-')[-1]

                # Find the running notebook servers
                for srv in notebookapp.list_running_servers():
                    url = srv['url']
                    token = srv.get('token', '')
                    # Request sessions from server
                    req = urllib.request.urlopen(f"{url}api/sessions?token={token}")
                    sessions = json.load(req)
                    for sess in sessions:
                        if sess['kernel']['id'] == kernel_id:
                            # Return notebook directory
                            return Path(sess['notebook']['path']).parent.resolve()
            except Exception:
                # fallback to cwd if API fails
                return Path.cwd()
        else:
            # Terminal or other IPython shells
            return Path.cwd()
    except NameError:
        # Standard Python script
        return Path.cwd()

def get_dir(paths: Union[str, Path, List[Union[str, Path]]]=None, n: int = 0, return_obj:bool = True) -> Union[Path, List[Path]]:
    """
    Get the nth parent directory of a path or list of paths.

    Args:
        paths: A single path (str or Path) or a list of paths.
        n: Number of levels to go up (default 1).

    Returns:
        Path object (or list of Path objects) representing the nth parent directory.
    """
    def parent_dir(p: Union[str, Path], n: int) -> Path:
        p = Path(p).resolve()  # convert to absolute Path
        if n < 0:
            raise ValueError("n must be >= 0")
        if n == 0:
            return p if return_obj else str(p)
        parent = p
        for _ in range(n):
            if parent.parent == parent:
                break # Already at root
            parent = parent.parent
        return Path(parent) if return_obj else parent

    if paths is None: paths = get_cwd()

    if isinstance(paths, list):
        return [parent_dir(p, n) for p in paths]
    else:
        return parent_dir(paths, n)

def detect_lang(text, output="lang", verbose=True):
    from langdetect import detect

    dir_curr_script = os.path.dirname(os.path.abspath(__file__))
    dir_lang_code = dir_curr_script + "/data/lang_code_iso639.json"
    print(dir_curr_script, os.getcwd(), dir_lang_code)
    lang_code_iso639 = fload(dir_lang_code)
    l_lang, l_code = [], []
    [[l_lang.append(v), l_code.append(k)] for v, k in lang_code_iso639.items()]
    try:
        if isa(text,"text"):
            code_detect = detect(text)
            if "c" in output.lower():  # return code
                return l_code[strcmp(code_detect, l_code, verbose=verbose)[1]]
            else:
                return l_lang[strcmp(code_detect, l_code, verbose=verbose)[1]]
        else:
            print(f"{text} is not supported")
            return "no"
    except:
        return "no"

def shared(*args, n_shared=None, verbose=True, **kwargs):
    """
    Find shared elements among multiple lists.

    Parameters:
    *args: Variable number of lists or a single list of lists
    n_shared (int, optional): Minimum number of lists an element must appear in
    verbose (bool): Whether to print progress and results

    Returns:
    list: Shared elements meeting the n_shared threshold
    """
    if verbose:
        print("\n********* checking shared elements *********")

    # Handle input types (single list of lists vs multiple lists)
    lists = args[0] if len(args) == 1 and isinstance(args[0], list) else args
    if not lists:
        if verbose:
            print("   No lists provided")
            print("********* checking shared elements *********")
        return []
    # Flatten and deduplicate input lists
    flattened_lists = [flatten(lst) for lst in lists]
    if n_shared is None:
        n_shared = len(flattened_lists)
    # Find shared elements using set operations
    if n_shared == len(flattened_lists):
        # Strict mode: elements must appear in all lists
        shared_elements = set.intersection(*(set(lst) for lst in flattened_lists))
    else:
        # Non-strict mode: elements appear in at least n_shared lists
        element_sets = [set(lst) for lst in flattened_lists]
        all_elements = set.union(*element_sets)
        shared_elements = {
            elem for elem in all_elements
            if sum(1 for s in element_sets if elem in s) >= n_shared
        }
    result = list(shared_elements)

    if verbose:
        display_count = min(5, len(result))
        display_items = result[:display_count]
        more_indicator = "..." if len(result) > display_count else ""
        
        print(f"   {len(result)} elements shared (appearing in ‚â•{n_shared} lists):")
        print(f"   Sample: {display_items}{more_indicator}")
        print("********* checking shared elements *********")

    return result

def share_not(*args, n_shared=None, verbose=False):
    """
    To find the elements in list1 that are not shared with list2 while maintaining the original order of list1
    usage:
        list1 = [1, 8, 3, 3, 4, 5]
        list2 = [4, 5, 6, 7, 8]
        not_shared(list1,list2)# output [1,3]
    """
    _common = shared(*args, n_shared=n_shared, verbose=verbose)
    list1 = flatten(args[0], verbose=verbose)
    _not_shared = [item for item in list1 if item not in _common]
    return _not_shared


def not_shared(*args, n_shared=None, verbose=False):
    """
    To find the elements in list1 that are not shared with list2 while maintaining the original order of list1
    usage:
        list1 = [1, 8, 3, 3, 4, 5]
        list2 = [4, 5, 6, 7, 8]
        not_shared(list1,list2)# output [1,3]
    """
    _common = shared(*args, n_shared=n_shared, verbose=verbose)
    list1 = flatten(args[0], verbose=verbose)
    _not_shared = [item for item in list1 if item not in _common]
    return _not_shared


#! ===========extract_text===========
def extract_text(
    text: Union[str, List[str]],
    patterns: Union[str, List[str]],
    *,
    mode: Literal["between", "split", "extract"] = "between",
    keep: Literal["none", "left", "right", "both", "markers"] = "none",
    case: Literal["sensitive", "insensitive"] = "insensitive",
    all_matches: bool = False,
    positions: bool = False,
    regex: bool = False,
    delimiter: Optional[str] = None,
    trim: bool = True,
    as_dict: bool = False,
    verbose: bool = False,
    **kwargs,
) -> Union[List[str], Tuple[int, str], Dict[str, Any], List[Dict[str, Any]], None]:
    """
    Ultimate text extraction tool with enhanced reliability and features.

    Key improvements:
    - Robust split mode with proper delimiter handling
    - Consistent return types across all modes
    - Improved pattern matching logic
    - Better edge case handling

    print(extract_text("A,B,C", ",", mode="split", keep="none", all_matches=True))
    # Correctly returns: ['A', 'B', 'C']

    print(extract_text("A,B,C", ",", mode="split", keep="left"))
    # Returns: ['A,', 'B,', 'C']

    print(extract_text("A,B,C", ",", mode="split", keep="right"))
    # Returns: [',B', ',C']

    print(extract_text("A,B,C", ",", mode="split", keep="both"))
    # Returns: ['A', ',', 'B', ',', 'C']
    """
    if verbose:
        print("""
                extract_text(
                    text: Union[str, List[str]],
                    patterns: Union[str, List[str]],
                    *,
                    mode: Literal["between", "split", "extract"] = "between",
                    keep: Literal["none", "left", "right", "both", "markers"] = "none",
                    case: Literal["sensitive", "insensitive"] = "insensitive",
                    all_matches: bool = False,
                    positions: bool = False,
                    regex: bool = False,
                    delimiter: Optional[str] = None,
                    trim: bool = True,
                    as_dict: bool = False,
                    verbose: bool = False,
                    **kwargs,
                ) 
              """)
    # Normalization and validation
    text = _normalize_text(text, delimiter)
    patterns,is_regex = _validate_patterns(patterns,mode)
    flags = re.IGNORECASE if case == "insensitive" else 0

    # Find all matches with enhanced validation
    matches = _find_matches(text, patterns, is_regex, flags)
    if not matches:
        return None
    mode=strcmp(mode,["between", "split", "extract"])[0]
    # Mode-specific processing
    if mode == "extract":
        return _handle_extract(matches, all_matches, as_dict, positions, trim)
    elif mode == "split":
        return _handle_split(text, matches, keep, all_matches, as_dict, positions, trim)
    elif mode == "between":
        return _handle_between(text, matches, patterns, keep, as_dict, positions, trim)
    else:
        raise ValueError(f"Invalid mode: {mode}")


def _normalize_text(text: Union[str, List[str]], delimiter: Optional[str]) -> str:
    """Normalize text input to single string"""
    if isinstance(text, list):
        return delimiter.join(text) if delimiter else " ".join(text)
    return text
def _validate_patterns(pattern: Union[str, List[str]],mode=None) -> List[str]:
    if isinstance(pattern, re.Pattern):
        is_regex = True
    elif mode in {"re", "not re"}:
        is_regex = True
        pattern = re.compile(pattern)
    else:
        is_regex = (
            bool(re.search(r"[.^$*+?{}\[\]|()\\]", pattern))
            if mode == "auto"
            else False
        )
        if is_regex:
            pattern = re.compile(pattern)
    return pattern,is_regex
# def _validate_patterns(patterns: Union[str, List[str]]) -> List[str]:
#     """Validate and normalize patterns"""
#     if isinstance(patterns, str):
#         return [patterns]
#     if not patterns:
#         raise ValueError("At least one pattern required")
#     return patterns


def _find_matches(
    text: str, patterns: List[str], regex: bool, flags: int
) -> List[dict]:
    """Find all pattern matches with enhanced regex handling"""
    matches = []
    for pattern in patterns:
        try:
            search_pattern = pattern if regex else re.escape(pattern)
            for match in re.finditer(search_pattern, text, flags=flags):
                matches.append(
                    {
                        "text": match.group(),
                        "start": match.start(),
                        "end": match.end(),
                        "pattern": pattern,
                        "full_match": match,
                    }
                )
        except re.error as e:
            raise ValueError(f"Invalid pattern '{pattern}': {e}")
    return sorted(matches, key=lambda x: x["start"])


def _handle_extract(
    matches: List[dict], all_matches: bool, as_dict: bool, positions: bool, trim: bool
) -> Union[List, dict]:
    """Handle text extraction of matched patterns"""
    results = []
    for match in matches if all_matches else [matches[0]]:
        content = match["text"].strip() if trim else match["text"]
        result = (
            {
                "text": content,
                "start": match["start"],
                "end": match["end"],
                "pattern": match["pattern"],
            }
            if as_dict
            else content
        )
        if positions and as_dict:
            result["positions"] = [(match["start"], match["end"])]
        results.append(result)

    return results[0] if not all_matches else results


def _create_part(
    content: str,
    start: int,
    end: int,
    match: Optional[dict],
    as_dict: bool,
    positions: bool,
    trim: bool,
) -> Union[str, dict]:
    """Create a standardized result part"""
    content = content.strip() if trim else content
    if not as_dict:
        return content

    part = {
        "text": content,
        "start": start,
        "end": end,
        "pattern": match["pattern"] if match else None,
    }
    if positions and match:
        part["positions"] = [(match["start"], match["end"])]
    return part


def _handle_between(
    text: str,
    matches: List[dict],
    patterns: List[str],
    keep: str,
    as_dict: bool,
    positions: bool,
    trim: bool,
) -> Union[Tuple, dict]:
    """Reliable between-mode implementation with boundary checks"""
    first_pattern, last_pattern = patterns[0], patterns[-1]
    first_matches = [m for m in matches if m["pattern"] == first_pattern]
    last_matches = [m for m in matches if m["pattern"] == last_pattern]

    if not first_matches or not last_matches:
        return None

    first = first_matches[0]
    last = last_matches[-1]

    if first["start"] > last["start"]:
        return None

    # Calculate extraction window
    start, end = first["start"], last["end"]
    if keep == "none":
        start, end = first["end"], last["start"]
    elif keep == "left":
        end = last["start"]
    elif keep == "right":
        start = first["end"]

    extracted = text[start:end].strip() if trim else text[start:end]

    if as_dict:
        result = {
            "text": extracted,
            "start": start,
            "end": end,
            "patterns": patterns,
            "match_positions": [(m["start"], m["end"]) for m in matches],
        }
        return result

    return (
        (start, extracted)
        if not positions
        else (start, extracted, [(m["start"], m["end"]) for m in matches])
    )


def _handle_split(
    text: str,
    matches: List[dict],
    keep: str,
    all_matches: bool,
    as_dict: bool,
    positions: bool,
    trim: bool,
) -> Union[List, dict]:
    """Split text with proper handling of keep='both' to include delimiters on both sides"""
    if not matches:
        return (
            [text]
            if not as_dict
            else [{"text": text, "start": 0, "end": len(text), "pattern": None}]
        )

    parts = []
    prev_end = 0
    process_matches = matches if all_matches else [matches[0]]

    # Special handling for keep="both"
    if keep == "both":
        for i, match in enumerate(process_matches):
            start, end = match["start"], match["end"]
            matched_text = text[start:end]

            # First segment (text before first delimiter + first delimiter)
            if i == 0:
                segment = text[prev_end:end]  # From start to end of first delimiter
                if trim:
                    segment = segment.strip()
                if segment or not trim:
                    if as_dict:
                        parts.append(
                            {
                                "text": segment,
                                "start": prev_end,
                                "end": end,
                                "pattern": match["pattern"],
                                **({"positions": [(start, end)]} if positions else {}),
                            }
                        )
                    else:
                        parts.append(segment)
                prev_end = end

            # Middle segments (delimiter + text + next delimiter)
            if i > 0 and i < len(process_matches):
                next_match = process_matches[i]
                next_start, next_end = next_match["start"], next_match["end"]
                segment = text[
                    prev_end:next_end
                ]  # From prev_end to end of next delimiter
                if trim:
                    segment = segment.strip()
                if segment or not trim:
                    if as_dict:
                        parts.append(
                            {
                                "text": segment,
                                "start": prev_end,
                                "end": next_end,
                                "pattern": next_match["pattern"],
                                **(
                                    {"positions": [(next_start, next_end)]}
                                    if positions
                                    else {}
                                ),
                            }
                        )
                    else:
                        parts.append(segment)
                prev_end = next_end

        # Last segment (last delimiter + remaining text)
        if process_matches and prev_end < len(text):
            last_match = process_matches[-1]
            segment = text[
                last_match["start"] : len(text)
            ]  # From last delimiter to end
            if trim:
                segment = segment.strip()
            if segment or not trim:
                if as_dict:
                    parts.append(
                        {
                            "text": segment,
                            "start": last_match["start"],
                            "end": len(text),
                            "pattern": last_match["pattern"],
                            **(
                                {
                                    "positions": [
                                        (last_match["start"], last_match["end"])
                                    ]
                                }
                                if positions
                                else {}
                            ),
                        }
                    )
                else:
                    parts.append(segment)

        return parts

    # Original handling for other keep modes
    for i, match in enumerate(process_matches):
        start, end = match["start"], match["end"]
        matched_text = text[start:end]

        # Handle text before the match
        if prev_end < start:
            before = text[prev_end:start]
            if trim:
                before = before.strip()
            if before or not trim:
                if as_dict:
                    parts.append(
                        {
                            "text": before,
                            "start": prev_end,
                            "end": start,
                            "pattern": None,
                            **({"positions": []} if positions else {}),
                        }
                    )
                else:
                    parts.append(before)

        # Handle the match based on keep mode
        if keep == "none":
            pass  # Skip the delimiter
        elif keep == "left":
            if parts:
                if as_dict:
                    parts[-1]["text"] += matched_text
                    parts[-1]["end"] = end
                else:
                    parts[-1] += matched_text
            else:
                if as_dict:
                    parts.append(
                        {
                            "text": matched_text,
                            "start": start,
                            "end": end,
                            "pattern": match["pattern"],
                            **({"positions": [(start, end)]} if positions else {}),
                        }
                    )
                else:
                    parts.append(matched_text)
        elif keep == "right":
            if i < len(process_matches) - 1:
                next_start = process_matches[i + 1]["start"]
                if end < next_start:
                    between = text[end:next_start]
                    if as_dict:
                        parts.append(
                            {
                                "text": matched_text + between,
                                "start": start,
                                "end": next_start,
                                "pattern": match["pattern"],
                                **({"positions": [(start, end)]} if positions else {}),
                            }
                        )
                    else:
                        parts.append(matched_text + between)
                    prev_end = next_start
                    continue

        prev_end = end

    # Handle remaining text after last match
    if prev_end < len(text):
        remaining = text[prev_end:]
        if trim:
            remaining = remaining.strip()
        if remaining or not trim:
            if keep == "right" and parts and process_matches:
                last_match = process_matches[-1]
                matched_text = text[last_match["start"] : last_match["end"]]
                if as_dict:
                    parts.append(
                        {
                            "text": matched_text + remaining,
                            "start": last_match["start"],
                            "end": len(text),
                            "pattern": last_match["pattern"],
                            **(
                                {
                                    "positions": [
                                        (last_match["start"], last_match["end"])
                                    ]
                                }
                                if positions
                                else {}
                            ),
                        }
                    )
                else:
                    parts.append(matched_text + remaining)
            else:
                if as_dict:
                    parts.append(
                        {
                            "text": remaining,
                            "start": prev_end,
                            "end": len(text),
                            "pattern": None,
                            **({"positions": []} if positions else {}),
                        }
                    )
                else:
                    parts.append(remaining)

    # Filter empty parts if trimming
    if trim:
        parts = [p for p in parts if (p["text"].strip() if as_dict else p.strip())]

    return parts


def _merge_parts(
    parts: List[Union[str, dict]], text: str, as_dict: bool, trim: bool
) -> Union[str, dict]:
    """Merge adjacent parts for keep=left mode"""
    if as_dict:
        merged_text = "".join(p["text"] for p in parts)
        return {
            "text": merged_text.strip() if trim else merged_text,
            "start": parts[0]["start"],
            "end": parts[-1]["end"],
            "patterns": list(set(p["pattern"] for p in parts if p["pattern"])),
        }
    return "".join(parts).strip() if trim else "".join(parts)
#! ===========extract_text===========
def fetch_text(
    source: Any,
    pattern: Optional[Union[str, Pattern, Callable]] = None,
    *,
    key: Optional[Union[str, int, List[Union[str, int]]]] = None,
    default: Any = None,
    ignore_case: bool = False,
    multiline: bool = False,
    dotall: bool = False,
    recursive: bool = True,
    max_depth: int = 10,
    flatten: bool = True,
    unique: bool = False,
    strip: bool = True,
    allow_empty: bool = False,
    as_string: bool = False,
    verbose: bool = False,
    **kwargs):
    r"""Extract text from various sources with flexible pattern matching.
            Parameters:
                source: Input data (str, list, dict, or nested structures)
                pattern: Optional regex pattern or callable to match content
                key: Key(s) to extract from dictionaries
                default: Default value when no match is found
                ignore_case: Case-insensitive matching
                multiline: Multiline regex mode
                dotall: Dot matches newline in regex
                recursive: Search recursively in nested structures
                max_depth: Maximum recursion depth
                flatten: Flatten nested results into a single list
                unique: Return only unique values
                strip: Strip whitespace from extracted strings
                allow_empty: Include empty strings in results
                as_string: Join list results into a string
                **kwargs: Additional regex flags or processing options

            Returns:
                Extracted content as list (default) or based on other options

        # Example 1: Simple string extraction
        text = "Contact us at support@example.com or sales@company.org"
        emails = fetch_text(text, r"\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b")
        print("Emails:", emails)

        # Example 2: Nested dictionary extraction
        data = {
            "user": {
                "name": "John Doe",
                "contacts": [
                    {"type": "email", "value": "john@example.com"},
                    {"type": "phone", "value": "+1234567890"},
                ],
                "metadata": {"tags": ["VIP", "premium", "active"]},
            }
        }

        # Extract all contact values
        contacts = fetch_text(data, key="value")
        print("Contact values:", contacts)

        # Extract only email contacts
        email_contacts = fetch_text(data, key="value", pattern=r"@")
        print("Email contacts:", email_contacts)

        # Example 3: Complex nested structure with custom matching
        nested_data = {
            "results": [
                {
                    "id": 1,
                    "text": "The quick brown fox",
                    "notes": ["jumps over", "the lazy dog"],
                },
                {"id": 2, "text": "Sample text 123", "notes": ["456", "789"]},
            ]
        }

        # Extract all words longer than 3 characters
        long_words = fetch_text(nested_data, pattern=r"\b\w{4,}\b", recursive=True)
        print("Long words:", long_words)


        # Example 4: Using a custom matcher function
        def is_number(s, path):
            try:
                float(s)
                return True
            except (ValueError, TypeError):
                return False


        numbers = fetch_text(nested_data, pattern=is_number, recursive=True)
        print("Numbers found:", numbers)

    """
    # Initialize result container
    results = []

    # Handle pattern compilation
    regex = None
    if pattern is not None:
        if callable(pattern):
            # Use callable directly for matching
            matcher = pattern
        else:
            # Compile regex pattern
            if HAS_REGEX:
                if verbose:
                    print("[str2pattern] Using advanced regex engine")
                flags = re.VERSION1  # Start with regex's extended mode
                
                if ignore_case:
                    flags |= re.IGNORECASE
                if multiline:
                    flags |= re.MULTILINE
                if dotall:
                    flags |= re.DOTALL
            else:
                if verbose:
                    print("[str2pattern] Using standard re engine")
                flags = 0
                if ignore_case:
                    flags |= re.IGNORECASE
                if multiline:
                    flags |= re.MULTILINE
                if dotall:
                    flags |= re.DOTALL
            if isinstance(pattern, str):
                regex = re.compile(pattern, flags=flags)
            else:
                regex = pattern

    # Helper function for recursive extraction
    def _extract(
        data: Any, current_depth: int = 0, path: Optional[List[Union[str, int]]] = None
    ) -> None:
        from collections.abc import Iterable

        if path is None:
            path = []

        if current_depth > max_depth:
            return

        # Handle dictionary
        if isinstance(data, dict):
            if key is not None:
                keys_to_check = [key] if not isinstance(key, list) else key
                for k in keys_to_check:
                    if k in data:
                        _process_value(data[k], current_depth, path + [k])
            elif recursive:
                for k, v in data.items():
                    _extract(v, current_depth + 1, path + [k])
            return

        # Handle list/tuple/set or other iterables
        if isinstance(data, (list, tuple, set)) or (
            not isinstance(data, (str, bytes, dict)) and isinstance(data, Iterable)
        ):
            if not recursive and current_depth > 0:
                return

            for i, item in enumerate(data):
                _extract(item, current_depth + 1, path + [i])
            return

        # Process single value
        _process_value(data, current_depth, path)

    # Helper function to process individual values
    def _process_value(
        value: Any, current_depth: int, path: List[Union[str, int]]
    ) -> None:
        nonlocal results

        # Convert bytes to string if needed
        if isinstance(value, bytes):
            try:
                value = value.decode("utf-8")
            except UnicodeDecodeError:
                return

        # Apply pattern matching
        matched = True
        if regex is not None:
            if not isinstance(value, str):
                return
            match = regex.search(value)
            if not match:
                return
            # If we have groups, use them, otherwise use the whole match
            if match.groups():
                matched_values = list(match.groups())
            else:
                matched_values = [match.group(0)]
        elif callable(pattern):
            try:
                matched = pattern(value, path=path)
                if matched is None:
                    return
                if matched is True:
                    matched_values = [value]
                else:
                    matched_values = [matched]
            except Exception:
                return
        else:
            matched_values = [value]

        # Process each matched value
        for val in matched_values:
            if val is None:
                continue

            # Convert to string if not already
            if not isinstance(val, str):
                val = str(val)

            # Apply string processing
            if strip:
                val = val.strip()

            # Check for empty strings
            if not allow_empty and not val:
                continue

            # Add to results
            results.append(val)

    # Start extraction
    _extract(source)

    # Handle empty results
    if not results:
        if as_string and default is not None:
            return str(default)
        return default if default is not None else ("" if as_string else [])

    # Post-processing
    if flatten:
        flat_results = []
        for item in results:
            if isinstance(item, (list, tuple, set)):
                flat_results.extend(item)
            else:
                flat_results.append(item)
        results = flat_results

    if unique:
        seen = set()
        unique_results = []
        for item in results:
            if item not in seen:
                seen.add(item)
                unique_results.append(item)
        results = unique_results

    if as_string:
        return " ".join(str(x) for x in results)

    return results


@lru_cache(maxsize=128)
def compile_pattern(pattern_str: str, 
        ignore_case: bool = False,
        multiline: bool = False,
        dotall: bool = False,):
    if HAS_REGEX:
        flags = re.VERSION1  # Start with regex's extended mode
        
        if ignore_case:
            flags |= re.IGNORECASE
        if multiline:
            flags |= re.MULTILINE
        if dotall:
            flags |= re.DOTALL
    else: 
        flags = 0
        if ignore_case:
            flags |= re.IGNORECASE
        if multiline:
            flags |= re.MULTILINE
        if dotall:
            flags |= re.DOTALL
    return re.compile(pattern_str, flags)
#! ===========extract_text===========
def stext(
    source: Any,
    pattern: Optional[Union[str, Pattern, Callable, dict]] = None,
    *,
    key: Optional[Union[str, int, List[Union[str, int]]]] = None,
    return_pattern: bool= False,
    default: Any = None,
    ignore_case: bool = False,
    multiline: bool = False,
    dotall: bool = False,
    recursive: Union[bool, int] = True,
    apply_flatten: bool = False,
    apply_unique: bool = False,
    strip: bool = True,
    allow_empty: bool = False,
    as_string: bool = False,
    return_list: bool = None,
    verbose: bool = False,
    # Keyword context parameters
    keyword: Optional[Union[str, List[str]]] = None,
    level: str = "window",
    window: int = 2,
    whole_word: bool = True,
    tail:str="both",# 'left','right','both'
    **kwargs
) -> Union[List[str], str, Any]:
    r"""Extract text from various sources with flexible pattern matching.

    Parameters:
        source: Input data (str, list, dict, or nested structures)
        pattern: Optional regex pattern, common alias ("email", "url", etc.),
                 callable, or keyword context config
        key: Key(s) to extract from dictionaries
        return_pattern: bool, reutnr the corresponding pattern
        default: Default value when no match is found
        ignore_case: Case-insensitive matching
        multiline: Multiline regex mode
        dotall: Dot matches newline in regex
        recursive: True for default depth (10), int for custom depth, False for no recursion
        flatten: Flatten nested results into a single list
        unique: Return only unique values
        strip: Strip whitespace from extracted strings
        allow_empty: Include empty strings in results
        as_string: Join list results into a string
        return_list: None, auto decide whether to keep the []

        # Keyword context parameters
        keyword: Keyword(s) to find context around (requires pattern="keyword")
        level: Context level ("sentence", "paragraph", or "window")
        window: Number of words around keyword (for "window" level)
        whole_word: Match whole words only

        **kwargs: Additional regex flags or processing options

    Keyword Context Pattern and Examples:
        Use dictionary format for keyword context extraction:
        Multiple context levels:
            - sentence level: Extracts full sentences containing keywords
            - paragraph level: Extracts entire paragraphs containing keywords
            - window level: Extracts configurable word windows around keywords
        # example1: patter is Dict
        text = 
        The mice were kept at 22  ¬∞C for 12h under a 12 h light/dark cycle. 
        Each animal received 200 ¬µL of saline solution by intraperitoneal injection. 
        Blood was collected at 5 min, 30 min, and 2 h after injection, and plasma glucose was measured in mg/dL. 
        Protein concentrations were determined using a BCA assay, and the samples were standardized to 1 mg/mL. The extracted DNA had an average length of 2 kb, while the sequencing run produced 3 Gb of data. The centrifuge was operated at 12,000 rpm for 10 min at 4 ¬∞C. The enzyme reaction rate was expressed in ¬µmol/min, and the protein mass was estimated at ~50 kDa.
        
        stext(text, pattern={"keyword": "under", "level": "window", "window": 3}, verbose=1)
        stext(text, "keyword", keyword="and", window=1, verbose=1)
        # xample2: Find sentences containing "error"
        stext(log_data, pattern="keyword", keyword="error")

        # xample3: Extract 5-word windows around "warning"
        stext(text_content, pattern="keyword", keyword="warning", level="window", window=5)

        # xample4: Get paragraphs mentioning "security"
        stext(document, pattern="keyword", keyword="security", level="paragraph")

    Returns:
        Extracted content as list (default) or based on other options
    """
    VERBOSE = verbose
    COMMON_PATTERNS = load_common_patterns()
    try:
        import regex as re
    except ImportError:
        import re

    # Initialize result container
    results = []

    # Handle recursion depth
    max_depth = 10
    if isinstance(recursive, int):
        max_depth = recursive
    elif not recursive:
        max_depth = 0

    # Handle pattern compilation
    regex = None
    matcher = None
    keyword_context_config = None
    # Handle keyword context extraction
    if pattern == "keyword":
        if not keyword:
            raise ValueError("Keyword must be provided when using pattern='keyword'")
        
        print(f"Applying keyword context extraction for: {keyword}") if VERBOSE else None
        keyword_context_config = {
            "keyword": keyword,
            "level": strcmp(level,["window","sentence","word","paragraph"])[0],
            "window": window,
            "ignore_case": ignore_case,
            "whole_word": whole_word,
            "tail":tail
        }
    if pattern is not None:
        # Handle keyword context pattern (special dictionary format)
        if keyword_context_config or isinstance(pattern, dict):            
            if isinstance(pattern, dict):
                keyword_context_config = pattern
            # filter useful param
            config = dict(keyword=None, level="window", window=1, ignore_case=True, whole_word=True,tail="both")
            var_valid = ["keyword", "level", "window", "word", "ignore_case", "whole_word","tail"]
            for k, v in keyword_context_config.items():
                k_val, _, sc = strcmp(k, var_valid, return_scores=1)
                if sc > 80:
                    config[k_val] = v
                else:
                    print(f"\tdetected a invalid param:'{k}:{v}'") if VERBOSE else None

            # Create the keyword context extractor
            def keyword_matcher(text, **kwargs):
                return extract_keyword_context(
                    text, 
                    keyword=config.get("keyword"),
                    level=config.get("level", "window"),
                    window=config.get("window", 2),
                    ignore_case=config.get("ignore_case", ignore_case),
                    whole_word=config.get("whole_word", True),
                    tail=config.get("tail","both")
                )
            matcher = keyword_matcher

        elif callable(pattern):
            print(f"apply Callable: {pattern}") if VERBOSE else None
            matcher = pattern
        else:
            # Handle common pattern aliases
            pattern_str = None
            if isinstance(pattern, (re.Pattern, RegexPattern)) if RegexPattern else isinstance(pattern, re.Pattern):
                print(f"apply Pattern: {pattern}") if VERBOSE else None
                pattern_str = pattern.pattern
            elif contains_regex_special(pattern) and isinstance(pattern, str):
                pattern_str= pattern
                print(f"contains_regex_special: {contains_regex_special(pattern)}") if VERBOSE else None
            else:
                # Check for pattern variants
                pattern_=list(COMMON_PATTERNS)[strcmp(pattern, [i.replace("_", "").replace("-", "") for i in list(COMMON_PATTERNS)],method="token_sort_ratio")[1]]
                print(pattern_,"after cmp all pre-config-patterns") if VERBOSE else None
                pattern_str = COMMON_PATTERNS.get(pattern_, pattern)
            print(f"pattern_str: '{pattern_str}'") if VERBOSE else None
            regex = compile_pattern(pattern_str, ignore_case, multiline, dotall)
        # reutnr the corresponding pattern
        if return_pattern:
            if keyword_context_config:
                if VERBOSE:
                    print(f"Keyword context pattern with config: {keyword_context_config}")
                # Could return config dict or a callable function wrapping extract_keyword_context
                return keyword_context_config  # or a callable if you want
            elif regex:
                if VERBOSE:
                    print(f"Returning compiled regex pattern: {regex}")
                return regex  # Return the compiled pattern (executable)
            else:
                # If regex not compiled yet, try compiling the pattern string
                try:
                    compiled = re.compile(pattern if isinstance(pattern, str) else str(pattern))
                    if VERBOSE:
                        print(f"Compiled pattern on the fly: {compiled}")
                    return compiled
                except re.error:
                    if VERBOSE:
                        print(f"Pattern not compilable, returning raw pattern string")
                    return pattern if isinstance(pattern, str) else str(pattern)

    # Helper function for recursive extraction
    def _extract(
        data: Any, 
        current_depth: int = 0, 
        path: Optional[List[Union[str, int]]] = None
    ) -> None:
        if path is None:
            path = []

        # Stop if max depth reached
        if current_depth > max_depth:
            return

        # Handle dictionaries
        if isinstance(data, dict):
            if key is not None:
                keys_to_check = [key] if not isinstance(key, list) else key
                for k in keys_to_check:
                    if k in data:
                        _process_value(data[k], current_depth, path + [k])
            elif max_depth > current_depth:
                for k, v in data.items():
                    _extract(v, current_depth + 1, path + [k])
            return

        # Handle lists, tuples, sets, generators
        if isinstance(data, (list, tuple, set, GeneratorType)):
            if max_depth > current_depth:
                for i, item in enumerate(data):
                    _extract(item, current_depth + 1, path + [i])
            return

        # Handle strings and bytes
        if isinstance(data, (str, bytes)):
            _process_value(data, current_depth, path)
            return

        # Handle other iterables
        try:
            if max_depth > current_depth and not isinstance(data, (str, bytes)):
                for i, item in enumerate(iter(data)):
                    _extract(item, current_depth + 1, path + [i])
                return
        except TypeError:
            pass

        # Process single value
        _process_value(data, current_depth, path)

    # Helper function to process individual values
    def _process_value(
        value: Any, 
        current_depth: int, 
        path: List[Union[str, int]]
    ) -> None:
        nonlocal results

        # Convert bytes to string
        if isinstance(value, bytes):
            try:
                value = value.decode("utf-8")
            except UnicodeDecodeError:
                return

        # Process based on pattern type
        if pattern is None:
            # No pattern - add value directly
            matched_values = [value]
        elif matcher is not None:
            # Custom matcher function (including keyword context)
            try:
                result = matcher(value, path=path)
                if result is True:
                    matched_values = [value]
                elif result is False or result is None:
                    return
                else:
                    matched_values = (
                        [result]
                        if not isinstance(result, (list, tuple))
                        else list(result)
                    )
            except Exception as e:
                if VERBOSE:
                    print(f"Matcher error: {e}")
                return
        else:
            # Regex pattern matching
            if not isinstance(value, str):
                return

            matched_values = []
            for match in regex.finditer(value):
                if match.groups():
                    # Add all captured groups
                    matched_values.extend(g for g in match.groups() if g is not None)
                else:
                    # Add entire match
                    matched_values.append(match.group(0))

        # Process each matched value
        for val in matched_values:
            if val is None:
                continue

            # Convert to string if needed
            if not isinstance(val, str):
                val = str(val)

            # Apply string processing
            if strip:
                val = val.strip()

            # Skip empty strings if not allowed
            if not allow_empty and not val:
                continue

            # Add to results
            results.append(val)
    def extract_keyword_context(
        text: str,
        keyword: Union[str, List[str]],
        level: str = "sentence",
        window: int = 1,
        ignore_case: bool = True,
        whole_word: bool = True,
        tail: str = "both"  # New parameter: "left", "right", or "both"
    ) -> List[str]:
        """Extract context around keywords in text.
        
        Parameters:
            text: Input text to search
            keyword: Keyword or list of keywords to find
            level: Context level ("sentence", "paragraph", or "window")
            window/word: Number of words around keyword (for "window" level)
            ignore_case: Case-insensitive search
            whole_word: Match whole words only
            tail: Which part of context to return:
                "left" - context before keyword
                "right" - context after keyword
                "both" - both sides of keyword (default)
            
        Returns:
            List of context strings
        """
        if not text or not keyword:
            return []
        
        flags = re.IGNORECASE if ignore_case else 0
        keywords = [keyword] if isinstance(keyword, str) else keyword
        # Normalize tail parameter
        tail = tail.lower()
        if tail in ["two", "all"]:
            tail = "both"
        # Escape keywords and handle whole-word matching
        if whole_word:
            keyword_pattern = r'\b(?:' + '|'.join(re.escape(k) for k in keywords) + r')\b'
        else:
            keyword_pattern = '|'.join(re.escape(k) for k in keywords)
            
        level = strcmp(level,["window","sentence","word","paragraph"])[0]
        try:
            if level == "sentence":
                # Match sentences containing keywords
                pattern = rf'[^.!?]*?{keyword_pattern}[^.!?]*[.!?]'
                return [
                    match.strip() 
                    for match in re.findall(pattern, text, flags=flags)
                ]
            
            elif level == "paragraph":
                # Split text into paragraphs and find those containing keywords
                paragraphs = re.split(r'\n\s*\n', text)
                return [
                    para.strip() 
                    for para in paragraphs
                    if re.search(keyword_pattern, para, flags=flags)
                ]
            
            elif level in ["window","word"]:
                pattern = rf'''
                    (?P<before>(?:[\w'-]+\W+){{0,{window}}})   # Up to N words before
                    (?P<keyword>{keyword_pattern})               # The keyword match
                    (?P<after>(?:\W+[\w'-]+){{0,{window}}})     # Up to N words after
                '''
                
                context_parts = []
                for match in re.finditer(pattern, text, flags=flags|re.VERBOSE):
                    if tail == "left":
                        # Context before keyword
                        context = match.group('before').strip()
                    elif tail == "right":
                        # Context after keyword
                        context = match.group('after').strip()
                    else:  # "both"
                        # Full context including keyword
                        context = match.group(0).strip()
                    
                    # Normalize whitespace and add to results
                    context_parts.append(re.sub(r'\s+', ' ', context))
                
                return context_parts
            
            else:
                raise ValueError(f"Invalid context level: {level}. Use 'sentence', 'paragraph', or 'window'")
                
        except re.error as e:
            raise ValueError(f"Invalid regex pattern: {e}") from e
    # * Main stext Func
    # Start extraction
    _extract(source)

    # Handle empty results
    if not results:
        if as_string and default is not None:
            return str(default)
        return default if default is not None else ("" if as_string else source)

    # Post-processing
    if apply_flatten:
        results = flatten(results, verbose=VERBOSE)

    if apply_unique:
        results = unique(results, verbose=VERBOSE)

    if as_string:
        return " ".join(str(x) for x in results)
 
    if return_list is None:
        if len(results) == 1:
            return results[0]
        else:
            return results
    elif return_list:
        return results
    else:
        return results[0]


def strcmp(
    search_term: str,
    candidates: List[str],
    ignore_case: bool = True,
    get_rank: bool = True,
    return_scores: bool = False,
    verbose: bool = False,
    scorer: str = "auto",
    method: Optional[str] = None,
    top_n: Optional[int] = 1,
    exact_match_first: bool = False
) -> Union[Tuple[str, int], List[str], List[Tuple[str, int, int]]]:
    """
    ‰ΩøÁî®Â§öÁßçÊ®°Á≥äÂåπÈÖçÁÆóÊ≥ïÂØπ search_term ‰∏éÂÄôÈÄâÂ≠óÁ¨¶‰∏≤ËøõË°åÁõ∏‰ººÊÄßÊØîËæÉÔºåÂπ∂ËøîÂõûÊúÄ‰Ω≥ÂåπÈÖç„ÄÇ

    ÂèÇÊï∞ËØ¥ÊòéÔºö
    - search_term: Ë¶ÅÊêúÁ¥¢ÁöÑÁõÆÊ†áÂ≠óÁ¨¶‰∏≤
    - candidates: ÂÄôÈÄâÂ≠óÁ¨¶‰∏≤ÂàóË°®
    - ignore_case: ÊòØÂê¶ÂøΩÁï•Â§ßÂ∞èÂÜô
    - get_rank: ÊòØÂê¶ËøîÂõûÊâÄÊúâÂÄôÈÄâÁöÑÁõ∏‰ººÂ∫¶ÊéíÂ∫èÁªìÊûú
    - return_scores: ÊòØÂê¶ÂêåÊó∂ËøîÂõûÁõ∏‰ººÂ∫¶ÂàÜÊï∞
    - verbose: ÊòØÂê¶ÊâìÂç∞ËØ¶ÁªÜ‰ø°ÊÅØ
    - scorer: ÈªòËÆ§ËØÑÂàÜÊñπÊ≥ï ("WR", "part", "ratio" Á≠â)
    - method: ÊòéÁ°ÆÊåáÂÆö‰ΩøÁî®ÁöÑËØÑÂàÜÊñπÊ≥ïÔºàË¶ÜÁõñ scorerÔºâ
    - top_n: ËøîÂõûÂâç top_n ‰∏™ÁªìÊûúÔºà‰ªÖÂú® get_rank ‰∏∫ True Êó∂ÁîüÊïàÔºâ
    - exact_match_first: Ëã•ÂèëÁé∞ÂÆåÂÖ®ÂåπÈÖçÔºåÂàôÁõ¥Êé•ËøîÂõû

    ÊîØÊåÅÁöÑÂåπÈÖçÊñπÊ≥ïËØ¥ÊòéÔºö
    - ratio: Â≠óÁ¨¶Á∫ßÈÄê‰ΩçÊØîËæÉÔºåÊúÄ‰∏•Ê†ºÔºåÈÄÇÂêàÂ≠óÁ¨¶ÂèòÂä®‰∏çÂ§ßÁöÑÊÉÖÂÜµ„ÄÇ
    - partial_ratio: ÈÄÇÁî®‰∫éÁü≠Â≠óÁ¨¶‰∏≤Âú®ÈïøÂ≠óÁ¨¶‰∏≤‰∏≠Â≠êÈõÜÂåπÈÖçÔºå‰æãÂ¶Ç ‚Äúapple‚Äù Âíå ‚Äúgreen apple‚Äù„ÄÇ
    - token_sort_ratio: ÂØπËØçËØ≠ÊéíÂ∫èÂêéÂÜçÊØîËæÉÔºåÈÄÇÂêàËØçÂ∫è‰∏çÂêå‰ΩÜÂÜÖÂÆπÁõ∏‰ººÁöÑÂè•Â≠ê„ÄÇ
    - token_set_ratio: ÊØîËæÉËØçÈõÜÂêàÔºåÈÄÇÁî®‰∫é‰∏ÄÊñπÂåÖÂê´Âè¶‰∏ÄÊñπÁöÑÊÉÖÂÜµÔºàÈáçÂ§çËØç‰∏çÂΩ±ÂìçÔºâ„ÄÇ
    - partial_token_sort_ratio: ËØçÊéíÂ∫è + ÈÉ®ÂàÜÂåπÈÖçÔºåÈÄÇÂêàÁü≠ËØçÂµåÂ•óÂú®ÈïøÂè•‰∏≠„ÄÇ
    - WRatio: ÁªºÂêà‰ΩøÁî®Â§öÁßçÊñπÊ≥ïÁöÑÂä†ÊùÉÁªìÊûúÔºåÈÄÇÂêàÊ≥õÁî®Âú∫ÊôØ„ÄÇ
    """
    from fuzzywuzzy import fuzz, process
    if verbose:
        method_str="""
        - ratio: Â≠óÁ¨¶Á∫ßÈÄê‰ΩçÊØîËæÉÔºåÊúÄ‰∏•Ê†ºÔºåÈÄÇÂêàÂ≠óÁ¨¶ÂèòÂä®‰∏çÂ§ßÁöÑÊÉÖÂÜµ„ÄÇ
        - partial_ratio: ÈÄÇÁî®‰∫éÁü≠Â≠óÁ¨¶‰∏≤Âú®ÈïøÂ≠óÁ¨¶‰∏≤‰∏≠Â≠êÈõÜÂåπÈÖçÔºå‰æãÂ¶Ç ‚Äúapple‚Äù Âíå ‚Äúgreen apple‚Äù„ÄÇ
        - token_sort_ratio: ÂØπËØçËØ≠ÊéíÂ∫èÂêéÂÜçÊØîËæÉÔºåÈÄÇÂêàËØçÂ∫è‰∏çÂêå‰ΩÜÂÜÖÂÆπÁõ∏‰ººÁöÑÂè•Â≠ê„ÄÇ
        - token_set_ratio: ÊØîËæÉËØçÈõÜÂêàÔºåÈÄÇÁî®‰∫é‰∏ÄÊñπÂåÖÂê´Âè¶‰∏ÄÊñπÁöÑÊÉÖÂÜµÔºàÈáçÂ§çËØç‰∏çÂΩ±ÂìçÔºâ„ÄÇ
        - partial_token_sort_ratio: ËØçÊéíÂ∫è + ÈÉ®ÂàÜÂåπÈÖçÔºåÈÄÇÂêàÁü≠ËØçÂµåÂ•óÂú®ÈïøÂè•‰∏≠„ÄÇ
        - WRatio: ÁªºÂêà‰ΩøÁî®Â§öÁßçÊñπÊ≥ïÁöÑÂä†ÊùÉÁªìÊûúÔºåÈÄÇÂêàÊ≥õÁî®Âú∫ÊôØ„ÄÇ
        """
        print(method_str)
    def to_lower(s):
        if ignore_case:
            if isinstance(s, str):
                return s.lower()
            elif isinstance(s, list):
                return [str(i).lower() for i in s]
        return s

    if not candidates or not isinstance(candidates, list):
        raise ValueError("Candidates must be a non-empty list of strings.")

    # Clean input
    search_term = str(search_term)
    candidates = [str(c) for c in candidates]

    str1 = to_lower(search_term)
    str2 = to_lower(candidates)

    scoring = (method or scorer).lower()
    get_rank = True if any([top_n is not None, get_rank]) else False
    if exact_match_first:
        for idx, cand in enumerate(str2):
            if str1 == cand:
                if verbose:
                    print(f"Exact match found: {candidates[idx]}")
                if return_scores:
                    return candidates[idx], idx
                return candidates[idx]
    # ‰∏≠ÊñáÂà´ÂêçÊò†Â∞Ñ + ÊñπÊ≥ïÂåπÈÖçÈÄªËæë
    methods_map = {
        "auto": fuzz.WRatio,
        "balance": fuzz.WRatio,
        "wr": fuzz.WRatio,
        "subset_match": fuzz.token_set_ratio,
        "order_free_match": fuzz.token_sort_ratio,
        "free": fuzz.token_sort_ratio,
        "partial_phrase_match": fuzz.partial_token_sort_ratio,
        "partial_match": fuzz.partial_ratio,
        "strict_match": fuzz.ratio,
        "strict": fuzz.ratio,
    } 

    # ‰ΩøÁî® fuzzywuzzy ÁöÑ process.extractOne ÊâæÂá∫ÊúÄÊé•ËøëÁöÑ method key
    available_keys = list(methods_map.keys())
    scoring_key, score = process.extractOne(scoring, available_keys)
    print(f"use {scoring_key} method") if verbose else None
    # ËÆæÁΩÆÂæóÂàÜÈòàÂÄºÔºàÈÅøÂÖçËØØÂåπÈÖçÔºâ
    if score < 60:
        raise ValueError(f"Êó†Ê≥ïËØÜÂà´ÁöÑÂåπÈÖçÊñπÊ≥ï: {scoring}. ÂèØÁî®ÊñπÊ≥ï‰∏∫: {available_keys}") 
    score_func = methods_map.get(scoring, fuzz.WRatio)  # ÈªòËÆ§ WRatio

    similarity_scores = [score_func(str1, c) for c in str2]

    if get_rank:
        sorted_indices = sorted(range(len(similarity_scores)), key=lambda i: similarity_scores[i], reverse=True)
        if top_n:
            sorted_indices = sorted_indices[:top_n]
        results = []
        for i in sorted_indices:
            if return_scores:
                results.append((candidates[i], i, similarity_scores[i]))
            else:
                results.append((candidates[i], i))
        if verbose:
            print("Top matches:")
            for r in results:
                print(r)
        return results[0] if top_n==1 else results

    else:
        best_idx = similarity_scores.index(max(similarity_scores))
        if verbose:
            print(f"Best match: {candidates[best_idx]} (Score: {similarity_scores[best_idx]})")
            suggestions = process.extract(search_term, candidates, limit=5)
            print("Suggestions:", suggestions)
        
        if return_scores:
            return candidates[best_idx], best_idx,similarity_scores[best_idx]
        return candidates[best_idx],best_idx

def shuffle(
    data: Any,
    seed: Optional[int] = 1,
    inplace: bool = False,
    subset: Optional[Union[int, List[int]]] = None,
    keep_indices: Optional[List[int]] = None,
    algorithm: str = "fisher_yates",
    nested: bool = False,
    shuffle_fn: Optional[callable] = None,
) -> Any:
    """
    Universal shuffle function supporting list, tuple, string, set, numpy array, pandas Series.

    Returns the same type as input unless inplace=True for mutable types.

    Parameters: same as list_shuffle with added type detection.
    """
    import copy
    import random
    original_type = type(data)

    # Convert to list
    if isinstance(data, (list, tuple, str)):
        data_list = list(data)
    elif isinstance(data, set):
        data_list = list(data)
        # sets have no order ‚Üí shuffling is essentially meaningless but allowed
    elif isinstance(data, np.ndarray):
        data_list = data.tolist()
    elif isinstance(data, pd.Series):
        data_list = data.tolist()
    else:
        raise TypeError(f"Unsupported type: {original_type}")

    # Optionally seed
    if seed is not None:
        random.seed(seed)

    n = len(data_list)
    indices = list(range(n))

    # Apply subset if provided
    if subset is not None:
        if isinstance(subset, int):
            indices = indices[:min(subset, n)]
        elif isinstance(subset, list):
            indices = [i for i in subset if 0 <= i < n]
        else:
            raise ValueError("subset must be int, list, or None")

    if keep_indices:
        indices = [i for i in indices if i not in keep_indices]

    values = [data_list[i] for i in indices]

    # Shuffle using chosen algorithm
    if shuffle_fn:
        values = shuffle_fn(values)
    elif algorithm == "fisher_yates":
        for i in range(len(values)-1, 0, -1):
            j = random.randint(0, i)
            values[i], values[j] = values[j], values[i]
    elif algorithm == "builtin":
        random.shuffle(values)
    else:
        raise ValueError("Unsupported algorithm")

    for idx, val in zip(indices, values):
        data_list[idx] = val

    if inplace and isinstance(data, list):
        data.clear()
        data.extend(data_list)
        return None

    # Convert back to original type
    if isinstance(data, str):
        return ''.join(data_list)
    elif isinstance(data, tuple):
        return tuple(data_list)
    elif isinstance(data, set):
        return set(data_list)  # no guarantee of order!
    elif isinstance(data, np.ndarray):
        return np.array(data_list)
    elif isinstance(data, pd.Series):
        return pd.Series(data_list, index=data.index)
    return data_list
 
def list_filter(
    list_: List[Optional[Union[str, int, float]]],
    pattern: Optional[Union[str, re.Pattern]] = None,
    mode: str = "auto",
    inverse: bool = False,
    ignore_case: bool = True,
    return_idx: bool = False,
    verbose: bool = False,
) -> Union[List[str], Tuple[List[str], List[int]]]:
    """
    Filters a list of strings (or convertible values) based on a pattern and matching mode.

    Parameters:
        list_ (List): The list to filter.
        pattern (str or re.Pattern): The search pattern.
        mode (str): Match mode: 'contains', 'startswith', 'endswith', 'not contains',
                    'not startswith', 'not endswith', 're', 'not re', or 'auto'.
        inverse (bool): If True, inverts the match.
        return_idx (bool): If True, also returns the indices of the matched items.

    Returns:
        List of matched items, or (items, indices) if return_idx is True.
    """
    if pattern is None:
        result = list(filter(lambda x: x is not None, list_))
        return (result, list(range(len(result)))) if return_idx else result

    # Detect if it's a regex pattern (compiled or pattern string with special characters)
    if isinstance(pattern, re.Pattern):
        is_regex = True
    elif mode in {"re", "not re"}:
        is_regex = True
        pattern = re.compile(pattern)
    else:
        is_regex = bool(re.search(r"[.^$*+?{}\[\]|()\\]", pattern)) if mode == "auto" else False
        if is_regex:
            pattern = re.compile(pattern)
    if ignore_case:
        # list_ = [str(i).lower().strip() for i in list_]
        mode = mode.lower().strip()
        if not is_regex:
            pattern=str(pattern).lower() 
    mode = strcmp(mode,[
            "contains","startswith","endswith","not contains","include","exclude","inclusive",
            "exclusive","not startswith","not endswith","re","not re",
            "auto","in","out","not in","not out","re","<>","><"])[0]
    if verbose:
        print(f"is_regex:{is_regex},ignore_case={ignore_case}, mode={mode} ")

    def match(item: Optional[Union[str, int, float]]) -> bool:
        if item is None:
            return False
        s = str(item).lower().strip() if ignore_case else str(item).strip() 

        if is_regex or mode in {"re"}:
            result = pattern.search(s) is not None
        elif mode in {"contains","auto","include","inclusive","in","not out","><",}:
            result = str(pattern) in s
        elif mode == "startswith":
            result = s.startswith(str(pattern))
        elif mode == "endswith":
            result = s.endswith(str(pattern))
        elif mode in ["not contains", "exclusive", "exclude", "out", "not in", "<>"]:
            result = str(pattern) not in s
        elif mode == "not startswith":
            result = not s.startswith(str(pattern))
        elif mode == "not endswith":
            result = not s.endswith(str(pattern))
        elif mode == "not re":
            result = not pattern.search(s)
        else:
            raise ValueError(f"Unsupported mode: {mode}")

        return not result if inverse else result

    filtered = [(i, item) for i, item in enumerate(list_) if match(item)]

    if return_idx:
        return [item for i, item in filtered], [i for i, item in filtered]
    return [item for _, item in filtered]

def imgcmp(
    img: list,
    method: str = "knn",
    thr: float = 0.75,
    detector: str = "sift",
    plot_: bool = True,
    figsize=[12, 6],
    grid_size=10,  # only for grid detector
    **kwargs,
):
    """
    Compare two images using SSIM, Feature Matching (SIFT), or KNN Matching.

    Parameters:
    - img (list): List containing two image file paths [img1, img2] or two numpy arrays.
    - method (str): Comparison method ('ssim', 'match', or 'knn').
    - detector (str): Feature detector ('sift', 'grid', 'pixel').
    - thr (float): Threshold for filtering matches.
    - plot_ (bool): Whether to display the results visually.
    - figsize (list): Size of the figure for plots.

    Returns:
    - For 'ssim': (diff, score): SSIM difference map and similarity score.
    - For 'match' or 'knn': (good_matches, len(good_matches), similarity_score): Matches and similarity score.
    """
    import cv2
    import matplotlib.pyplot as plt
    from skimage.metrics import structural_similarity as ssim

    # Load images
    if isinstance(img, list) and isinstance(img[0], str):
        image1 = cv2.imread(img[0])
        image2 = cv2.imread(img[1])
        bool_cvt = True
    else:
        image1, image2 = np.array(img[0]), np.array(img[1])
        bool_cvt = False

    if image1 is None or image2 is None:
        raise ValueError("Could not load one or both images. Check file paths.")
    methods = ["ssim", "match", "knn"]
    method = strcmp(method, methods)[0]
    if method == "ssim":
        # Convert images to grayscale
        gray1 = cv2.cvtColor(image1, cv2.COLOR_BGR2GRAY)
        gray2 = cv2.cvtColor(image2, cv2.COLOR_BGR2GRAY)

        # Compute SSIM
        score, diff = ssim(gray1, gray2, full=True)
        print(f"SSIM Score: {score:.4f}")

        # Convert diff to 8-bit for visualization
        diff = (diff * 255).astype("uint8")

        # Plot if needed
        if plot_:
            fig, ax = plt.subplots(1, 3, figsize=figsize)
            ax[0].imshow(gray1, cmap="gray")
            ax[0].set_title("Image 1")
            ax[1].imshow(gray2, cmap="gray")
            ax[1].set_title("Image 2")
            ax[2].imshow(diff, cmap="gray")
            ax[2].set_title("Difference (SSIM)")
            plt.tight_layout()
            plt.show()

        return diff, score

    elif method in ["match", "knn"]:
        # Convert images to grayscale
        gray1 = cv2.cvtColor(image1, cv2.COLOR_BGR2GRAY)
        gray2 = cv2.cvtColor(image2, cv2.COLOR_BGR2GRAY)

        if detector == "sift":
            # SIFT detector
            sift = cv2.SIFT_create()
            keypoints1, descriptors1 = sift.detectAndCompute(gray1, None)
            keypoints2, descriptors2 = sift.detectAndCompute(gray2, None)

        elif detector == "grid":
            # Grid-based detection
            keypoints1, descriptors1 = [], []
            keypoints2, descriptors2 = [], []

            for i in range(0, gray1.shape[0], grid_size):
                for j in range(0, gray1.shape[1], grid_size):
                    patch1 = gray1[i : i + grid_size, j : j + grid_size]
                    patch2 = gray2[i : i + grid_size, j : j + grid_size]
                    if patch1.size > 0 and patch2.size > 0:
                        keypoints1.append(
                            cv2.KeyPoint(
                                j + grid_size // 2, i + grid_size // 2, grid_size
                            )
                        )
                        keypoints2.append(
                            cv2.KeyPoint(
                                j + grid_size // 2, i + grid_size // 2, grid_size
                            )
                        )
                        descriptors1.append(np.mean(patch1))
                        descriptors2.append(np.mean(patch2))

            descriptors1 = np.array(descriptors1).reshape(-1, 1)
            descriptors2 = np.array(descriptors2).reshape(-1, 1)

        elif detector == "pixel":
            # Pixel-based direct comparison
            descriptors1 = gray1.flatten()
            descriptors2 = gray2.flatten()
            keypoints1 = [
                cv2.KeyPoint(x, y, 1)
                for y in range(gray1.shape[0])
                for x in range(gray1.shape[1])
            ]
            keypoints2 = [
                cv2.KeyPoint(x, y, 1)
                for y in range(gray2.shape[0])
                for x in range(gray2.shape[1])
            ]

        else:
            raise ValueError("Invalid detector. Use 'sift', 'grid', or 'pixel'.")

        # Handle missing descriptors
        if descriptors1 is None or descriptors2 is None:
            raise ValueError("Failed to compute descriptors for one or both images.")
        # Ensure descriptors are in the correct data type
        if descriptors1.dtype != np.float32:
            descriptors1 = descriptors1.astype(np.float32)
        if descriptors2.dtype != np.float32:
            descriptors2 = descriptors2.astype(np.float32)

        # BFMatcher initialization
        bf = cv2.BFMatcher()
        if method == "match":  # Cross-check matching
            bf = cv2.BFMatcher(cv2.NORM_L2, crossCheck=True)
            matches = bf.match(descriptors1, descriptors2)
            matches = sorted(matches, key=lambda x: x.distance)

            # Filter good matches
            good_matches = [
                m for m in matches if m.distance < thr * matches[-1].distance
            ]

        elif method == "knn":  # KNN matching with ratio test
            bf = cv2.BFMatcher()
            matches = bf.knnMatch(descriptors1, descriptors2, k=2)
            # Apply Lowe's ratio test
            good_matches = [m for m, n in matches if m.distance < thr * n.distance]

        # Calculate similarity score
        similarity_score = len(good_matches) / min(len(keypoints1), len(keypoints2))
        print(f"Number of good matches: {len(good_matches)}")
        print(f"Similarity Score: {similarity_score:.4f}")
        # Handle case where no good matches are found
        if len(good_matches) == 0:
            print("No good matches found.")
            return good_matches, 0.0, None

        # Identify matched keypoints
        src_pts = np.float32([keypoints1[m.queryIdx].pt for m in good_matches]).reshape(
            -1, 1, 2
        )
        dst_pts = np.float32([keypoints2[m.trainIdx].pt for m in good_matches]).reshape(
            -1, 1, 2
        )
        # Apply the homography to image2
        try:
            # Calculate Homography using RANSAC
            homography_matrix, mask = cv2.findHomography(
                src_pts, dst_pts, cv2.RANSAC, 5.0
            )
            h, w = image1.shape[:2]
            warped_image2 = cv2.warpPerspective(image2, homography_matrix, (w, h))

            # Plot result if needed
            if plot_:
                fig, ax = plt.subplots(1, 2, figsize=figsize)
                (
                    ax[0].imshow(cv2.cvtColor(image1, cv2.COLOR_BGR2RGB))
                    if bool_cvt
                    else ax[0].imshow(image1)
                )
                ax[0].set_title("Image 1")
                (
                    ax[1].imshow(cv2.cvtColor(warped_image2, cv2.COLOR_BGR2RGB))
                    if bool_cvt
                    else ax[1].imshow(warped_image2)
                )
                ax[1].set_title("Warped Image 2")
                plt.tight_layout()
                plt.show()
        except Exception as e:
            print(e)

        # Plot matches if needed
        if plot_:
            result = cv2.drawMatches(
                image1, keypoints1, image2, keypoints2, good_matches, None, flags=2
            )
            plt.figure(figsize=figsize)
            (
                plt.imshow(cv2.cvtColor(result, cv2.COLOR_BGR2RGB))
                if bool_cvt
                else plt.imshow(result)
            )
            plt.title(
                f"Feature Matches ({len(good_matches)} matches, Score: {similarity_score:.4f})"
            )
            plt.axis("off")
            plt.show()
        # Identify unmatched keypoints
        matched_idx1 = [m.queryIdx for m in good_matches]
        matched_idx2 = [m.trainIdx for m in good_matches]
        matched_kp1 = [kp for i, kp in enumerate(keypoints1) if i in matched_idx1]
        matched_kp2 = [kp for i, kp in enumerate(keypoints2) if i in matched_idx2]
        unmatched_kp1 = [kp for i, kp in enumerate(keypoints1) if i not in matched_idx1]
        unmatched_kp2 = [kp for i, kp in enumerate(keypoints2) if i not in matched_idx2]

        # Mark keypoints on the images
        img1_match = cv2.drawKeypoints(
            image1,
            matched_kp1,
            None,
            color=(0, 0, 255),
            flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS,
        )
        img2_match = cv2.drawKeypoints(
            image2,
            matched_kp2,
            None,
            color=(0, 0, 255),
            flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS,
        )
        img1_unmatch = cv2.drawKeypoints(
            image1,
            unmatched_kp1,
            None,
            color=(0, 0, 255),
            flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS,
        )
        img2_unmatch = cv2.drawKeypoints(
            image2,
            unmatched_kp2,
            None,
            color=(0, 0, 255),
            flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS,
        )

        if plot_:
            fig, ax = plt.subplots(1, 2, figsize=figsize)
            (
                ax[0].imshow(cv2.cvtColor(img1_unmatch, cv2.COLOR_BGR2RGB))
                if bool_cvt
                else ax[0].imshow(img1_unmatch)
            )
            ax[0].set_title("Unmatched Keypoints (Image 1)")
            (
                ax[1].imshow(cv2.cvtColor(img2_unmatch, cv2.COLOR_BGR2RGB))
                if bool_cvt
                else ax[1].imshow(img2_unmatch)
            )
            ax[1].set_title("Unmatched Keypoints (Image 2)")
            ax[0].axis("off")
            ax[1].axis("off")
            plt.tight_layout()
            plt.show()
        if plot_:
            fig, ax = plt.subplots(1, 2, figsize=figsize)
            (
                ax[0].imshow(cv2.cvtColor(img1_match, cv2.COLOR_BGR2RGB))
                if bool_cvt
                else ax[0].imshow(img1_match)
            )
            ax[0].set_title("Matched Keypoints (Image 1)")
            (
                ax[1].imshow(cv2.cvtColor(img2_match, cv2.COLOR_BGR2RGB))
                if bool_cvt
                else ax[1].imshow(img2_match)
            )
            ax[1].set_title("Matched Keypoints (Image 2)")
            ax[0].axis("off")
            ax[1].axis("off")
            plt.tight_layout()
            plt.show()
        return good_matches, similarity_score  # , homography_matrix

    else:
        raise ValueError("Invalid method. Use 'ssim', 'match', or 'knn'.")

def fcmp(file1, file2, kind= None, verbose=True, **kwargs):
    
    
    from concurrent.futures import ThreadPoolExecutor
    from datetime import datetime
    import json

    # --- Compare excel files ---
    def cmp_excel(
        file1,# base
        file2,  # new
        sheet_name=None,  # list or strings; default:"common" sheet
        key_columns=None,
        ignore_columns=None,
        numeric_tolerance=0,
        ignore_case=False,
        detect_reordered_rows=False,
        verbose=True,
        **kwargs,
    ):
        """
        Compare two Excel files and identify differences across specified sheets.

        Parameters:
        - file1 (Base/Reference): str, path to the first Excel file.
        - file2: str, path to the second Excel file.
        - sheet_name: list of str, specific sheets to compare (default: all common sheets).
        - key_columns: list of str, columns to use as unique identifiers (default: None, compares all columns).
        - ignore_columns: list of str, columns to exclude from comparison (default: None).
        - numeric_tolerance: float, tolerance for numeric column differences (default: 0, exact match).
        - ignore_case: bool, whether to ignore case differences (default: False).  # Changed here
        - detect_reordered_rows: bool, whether to detect reordered rows (default: False).
        - verbose: bool, whether to print progress messages (default: True).

        Returns:
        - dict, summary of differences for each sheet.
        """
        # Define output directory based on file1 basename
        file1_basename = os.path.splitext(os.path.basename(file1))[0]
        output_dir = f"CMP_{file1_basename}"
        if not os.path.exists(output_dir):
            os.makedirs(output_dir)

        # Load both files into a dictionary of DataFrames
        xl1 = pd.ExcelFile(file1)
        xl2 = pd.ExcelFile(file2)

        # Get the sheets to compare
        sheets1 = set(xl1.sheet_names)
        sheets2 = set(xl2.sheet_names)
        if sheet_name is None:
            sheet_name = list(sheets1 & sheets2)  # Compare only common sheets
        else:
            sheet_name = [sheet for sheet in sheet_name if sheet in sheets1 and sheets2]

        summary = {}
        print(f"Reference file: '{os.path.basename(file1)}'")
        def compare_sheet(sheet):
            
            if verbose:
                print(f"Comparing sheet: {sheet}...")

            # Read sheets as DataFrames
            df1 = xl1.parse(sheet).fillna("NA")
            df2 = xl2.parse(sheet).fillna("NA")

            # Handle case insensitivity
            if ignore_case:
                df1.columns = [col.lower() for col in df1.columns]
                df2.columns = [col.lower() for col in df2.columns]
                df1 = df1.applymap(lambda x: x.lower() if isinstance(x, str) else x)
                df2 = df2.applymap(lambda x: x.lower() if isinstance(x, str) else x)

            # Drop ignored columns
            if ignore_columns:
                df1 = df1.drop(
                    columns=[col for col in ignore_columns if col in df1.columns],
                    errors="ignore",
                )
                df2 = df2.drop(
                    columns=[col for col in ignore_columns if col in df2.columns],
                    errors="ignore",
                )

            # Normalize column order for comparison
            common_cols = df1.columns.intersection(df2.columns)
            df1 = df1[common_cols]
            df2 = df2[common_cols]

            # Specify key columns for comparison
            if key_columns:
                df1 = df1.set_index(key_columns)
                df2 = df2.set_index(key_columns)
            # Identify added and deleted rows based on entire row comparison, not just index
            added_rows = df2[~df2.apply(tuple, 1).isin(df1.apply(tuple, 1))]
            deleted_rows = df1[~df1.apply(tuple, 1).isin(df2.apply(tuple, 1))]

            # Detect reordered rows
            reordered_rows = pd.DataFrame()
            if detect_reordered_rows:
                # Find rows that exist in both DataFrames but are in different positions
                for idx in df1.index:
                    if idx in df2.index:
                        if not df1.loc[idx].equals(df2.loc[idx]):
                            reordered_rows = reordered_rows.append(df1.loc[idx])

            # Detect modified rows (in case of exact matches between the two files)
            aligned_df1 = df1[df1.index.isin(df2.index)]
            aligned_df2 = df2[df2.index.isin(df1.index)]

            if numeric_tolerance > 0:
                modified_rows = aligned_df1.compare(
                    aligned_df2,
                    keep_shape=False,
                    keep_equal=False,
                    result_names=["left", "right"],
                ).pipe(
                    lambda df: df[
                        ~df.apply(
                            lambda row: (
                                abs(row["left"] - row["right"]) <= numeric_tolerance
                                if pd.api.types.is_numeric_dtype(row["left"])
                                else False
                            ),
                            axis=1,
                        )
                    ]
                )
            else:
                modified_rows = aligned_df1.compare(
                    aligned_df2, keep_shape=False, keep_equal=False
                )

            # Save differences to Excel files
            sheet_dir = os.path.join(output_dir, sheet)
            os.makedirs(sheet_dir, exist_ok=True)
            added_path = os.path.join(sheet_dir, f"{sheet}_added.xlsx")
            deleted_path = os.path.join(sheet_dir, f"{sheet}_deleted.xlsx")
            modified_path = os.path.join(sheet_dir, f"{sheet}_modified.xlsx")
            reordered_path = os.path.join(sheet_dir, f"{sheet}_reordered.xlsx")

            if not added_rows.empty:
                added_rows.to_excel(added_path)
            if not deleted_rows.empty:
                deleted_rows.to_excel(deleted_path)
            if not modified_rows.empty:
                modified_rows.to_excel(modified_path)
            if not reordered_rows.empty:
                reordered_rows.to_excel(reordered_path)

            # Return the summary
            return {
                "added_rows": len(added_rows),
                "deleted_rows": len(deleted_rows),
                "modified_rows": len(modified_rows),
                "reordered_rows": len(reordered_rows),
                "added_file": added_path if not added_rows.empty else None,
                "deleted_file": deleted_path if not deleted_rows.empty else None,
                "modified_file": modified_path if not modified_rows.empty else None,
                "reordered_file": reordered_path if not reordered_rows.empty else None,
            }

        # Use ThreadPoolExecutor for parallel processing
        with ThreadPoolExecutor() as executor:
            results = executor.map(compare_sheet, sheet_name)

        # Collect results
        summary = {sheet: result for sheet, result in zip(sheet_name, results)}

        # Save JSON log
        json_path = os.path.join(output_dir, "comparison_summary.json")
        if os.path.exists(json_path):
            with open(json_path, "r") as f:
                existing_data = json.load(f)
        else:
            existing_data = {}

        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        existing_data[timestamp] = summary
        # Sort the existing data by the timestamp in descending order (latest first)
        existing_data = dict(sorted(existing_data.items(), reverse=True))

        with open(json_path, "w") as f:
            json.dump(existing_data, f, indent=4)
        if verbose:
            print(f"Comparison complete. Results saved in '{output_dir}'")

        return summary

    # --- Compare CSV files ---
    def cmp_csv(
        file1,
        file2,
        ignore_case=False,
        numeric_tolerance=0,
        ignore_columns=None,
        verbose=True,
        **kwargs,
    ):
        

        # Load data and fill NaNs
        df1 = pd.read_csv(file1).fillna("NA")
        df2 = pd.read_csv(file2).fillna("NA")

        # Standardize case if needed
        if ignore_case:
            df1.columns = df1.columns.str.lower()
            df2.columns = df2.columns.str.lower()
            df1 = df1.applymap(lambda x: x.lower() if isinstance(x, str) else x)
            df2 = df2.applymap(lambda x: x.lower() if isinstance(x, str) else x)

        # Drop ignored columns
        if ignore_columns:
            ignore_columns = [col.lower() if ignore_case else col for col in ignore_columns]
            df1.drop(columns=[col for col in ignore_columns if col in df1.columns], errors="ignore", inplace=True)
            df2.drop(columns=[col for col in ignore_columns if col in df2.columns], errors="ignore", inplace=True)

        # Reset index to ensure alignment
        df1.reset_index(drop=True, inplace=True)
        df2.reset_index(drop=True, inplace=True)

        # Align DataFrames by columns
        df1, df2 = df1.align(df2, join="inner", axis=1)

        # Compare rows
        added_rows = df2[~df2.apply(tuple, axis=1).isin(df1.apply(tuple, axis=1))]
        deleted_rows = df1[~df1.apply(tuple, axis=1).isin(df2.apply(tuple, axis=1))]

        # Compare modified rows
        if numeric_tolerance > 0:
            def numeric_diff(row):
                if pd.api.types.is_numeric_dtype(row["left"]):
                    return abs(row["left"] - row["right"]) > numeric_tolerance
                return row["left"] != row["right"]

            modified_rows = df1.compare(df2, keep_shape=True, keep_equal=False)
            modified_rows = modified_rows[modified_rows.apply(numeric_diff, axis=1)]
        else:
            modified_rows = df1.compare(df2, keep_shape=True, keep_equal=False)

        # Return results
        return {
            "added_rows": len(added_rows),
            "deleted_rows": len(deleted_rows),
            "modified_rows": len(modified_rows),
            "added_file": added_rows if not added_rows.empty else pd.DataFrame(),
            "deleted_file": deleted_rows if not deleted_rows.empty else pd.DataFrame(),
            "modified_file": modified_rows if not modified_rows.empty else pd.DataFrame(),
        }

    # --- Compare JSON files ---
    def cmp_json(
        file1, file2, ignore_case=False, numeric_tolerance=0, verbose=True, **kwargs
    ):
        import json

        with open(file1, "r") as f1:
            json1 = json.load(f1)
        with open(file2, "r") as f2:
            json2 = json.load(f2)

        # Normalize case and compare JSONs
        if ignore_case:

            def normalize(obj):
                if isinstance(obj, dict):
                    return {k.lower(): normalize(v) for k, v in obj.items()}
                elif isinstance(obj, list):
                    return [normalize(item) for item in obj]
                elif isinstance(obj, str):
                    return obj.lower()
                else:
                    return obj

            json1 = normalize(json1)
            json2 = normalize(json2)

        # Compare JSONs
        def compare_json(obj1, obj2):
            if isinstance(obj1, dict) and isinstance(obj2, dict):
                added_keys = {k: obj2[k] for k in obj2 if k not in obj1}
                deleted_keys = {k: obj1[k] for k in obj1 if k not in obj2}
                modified_keys = {
                    k: (obj1[k], obj2[k])
                    for k in obj1
                    if k in obj2 and obj1[k] != obj2[k]
                }
                return added_keys, deleted_keys, modified_keys

            elif isinstance(obj1, list) and isinstance(obj2, list):
                added_items = [item for item in obj2 if item not in obj1]
                deleted_items = [item for item in obj1 if item not in obj2]
                modified_items = [
                    (item1, item2) for item1, item2 in zip(obj1, obj2) if item1 != item2
                ]
                return added_items, deleted_items, modified_items

            else:
                if obj1 != obj2:
                    return obj1, obj2, None
                else:
                    return None, None, None

        added, deleted, modified = compare_json(json1, json2)

        return {"added_keys": added, "deleted_keys": deleted, "modified_keys": modified}

    # --- Compare Text files ---
    def cmp_txt(
        file1, file2, ignore_case=False, numeric_tolerance=0, verbose=True, **kwargs
    ):
        def read_lines(file):
            with open(file, "r") as f:
                return f.readlines()

        lines1 = read_lines(file1)
        lines2 = read_lines(file2)

        if ignore_case:
            lines1 = [line.lower() for line in lines1]
            lines2 = [line.lower() for line in lines2]

        added_lines = [line for line in lines2 if line not in lines1]
        deleted_lines = [line for line in lines1 if line not in lines2]

        modified_lines = []
        if numeric_tolerance > 0:
            for line1, line2 in zip(lines1, lines2):
                if abs(float(line1) - float(line2)) > numeric_tolerance:
                    modified_lines.append((line1, line2))
        else:
            for line1, line2 in zip(lines1, lines2):
                if line1 != line2:
                    modified_lines.append((line1, line2))

        return {
            "added_lines": added_lines,
            "deleted_lines": deleted_lines,
            "modified_lines": modified_lines,
        }

    if kind is None:
        kind = os.path.splitext(file1)[1].lower()[1:]
    # Compare based on the file type
    if kind == "xlsx":
        return cmp_excel(file1=file1, file2=file2, verbose=verbose, **kwargs)

    elif kind == "csv":
        return cmp_csv(file1=file1, file2=file2, verbose=verbose, **kwargs)

    elif kind == "json":
        return cmp_json(file1=file1, file2=file2, verbose=verbose, **kwargs)

    elif kind == "txt":
        return cmp_txt(file1=file1, file2=file2, verbose=verbose, **kwargs)

    else:
        raise ValueError(f"Unsupported file type: {kind}")
def cn2pinyin(
    cn_str: Union[str, list] = None,
    sep: str = " ",
    fmt: str = "normal",  # which style you want to set
):
    from pypinyin import pinyin, Style

    """
    Converts Chinese characters to Pinyin.
    usage: 
        cn2pinyin(cn_str, sep="_", fmt="tone")
    Args:
        cn_str (str): Chinese string to convert.
        sep (str): Separator for the output Pinyin string.
        fmt (Style): "normal","tone", "tone2","tone3","finals","finals_tone","finals_tone2","finals_tone3","initials","bopomofo","bopomofo_first","cyrillic","pl",
    Returns:
        cn_str: The Pinyin representation of the Chinese string.
    """
    fmts = [
        "normal",
        "tone",
        "tone2",
        "tone3",
        "finals",
        "finals_tone",
        "finals_tone2",
        "finals_tone3",
        "initials",
        "bopomofo",
        "bopomofo_first",
        "cyrillic",
        "pl",
    ]
    fmt = strcmp(fmt, fmts)[0]
    if fmt == "normal":
        style = Style.NORMAL
    elif fmt == "tone":
        style = Style.TONE
    elif fmt == "tone2":
        style = Style.TONE2
    elif fmt == "tone3":
        style = Style.TONE3
    elif fmt == "finals":
        style = Style.FINALS
    elif fmt == "finals_tone":
        style = Style.FINALS_TONE
    elif fmt == "finals_tone2":
        style = Style.FINALS_TONE2
    elif fmt == "finals_tone3":
        style = Style.FINALS_TONE3
    elif fmt == "initials":
        style = Style.INITIALS
    elif fmt == "bopomofo":
        style = Style.BOPOMOFO
    elif fmt == "bopomofo_first":
        style = Style.BOPOMOFO_FIRST
    elif fmt == "cyrillic":
        style = Style.CYRILLIC
    elif fmt == "pl":
        style = Style.PL
    else:
        style = Style.NORMAL
    if not isinstance(cn_str, list):
        cn_str = [cn_str]
    pinyin_flat = []
    for cn_str_ in cn_str:
        pinyin_string = pinyin(cn_str_, style=style)
        pinyin_flat.append(sep.join([item[0] for item in pinyin_string]))
    if len(pinyin_flat) == 1:
        return pinyin_flat[0]
    else:
        return pinyin_flat


def counter(list_, verbose=True):
    from collections import Counter

    c = Counter(list_)
    # Print the name counts
    for item, count in c.items():
        if verbose:
            print(f"{item}: {count}")
    return c


# usage:
# print(f"Return an iterator over elements repeating each as many times as its count:\n{sorted(c.elements())}")
# print(f"Return a list of the n most common elements:\n{c.most_common()}")
# print(f"Compute the sum of the counts:\n{c.total()}")

def dict2df(dict_, fill=None, axis=0):
    """
    Convert a dictionary to a DataFrame with flexible axis and padding options.

    Parameters:
    - dict_: The dictionary to convert (keys are columns or index).
    - fill: Value to fill in case of shorter lists.
    - axis: Axis for DataFrame construction (0 for columns, 1 for rows).

    Returns:
    - DataFrame created from the dictionary.
    """
    for key, value in dict_.items():
        if not isinstance(value, list):
            dict_[key] = [value]
            print(f"'{key}' is not a list. trying to convert it to 'list'")

    # Get the maximum length of values
    len_max = max(len(value) for value in dict_.values())

    # Extend lists to match the length of the longest list
    for key, value in dict_.items():
        if isinstance(value, list):
            value.extend([fill] * (len_max - len(value)))  # Fill shorter lists
        dict_[key] = value

    # If axis=0, the dictionary keys will be treated as column names
    if axis == 0:
        return pd.DataFrame(dict_)
    # If axis=1, the dictionary keys will be treated as index names (rows)
    else:
        return pd.DataFrame(dict_).transpose()

def text2audio(
    text,
    method=None,  # "pyttsx3","gTTS"
    rate=200,
    slow=False,  # "gTTS"
    volume=1.0,
    voice=None,
    lang=None,
    gender=None,
    age=None,
    dir_save=None,
):
    """
    # sample_text = "Hello! This is a test of the pyttsx3 text-to-speech system."
    # sample_text = "Ëøô‰∏™ÊòØ‰∏≠Êñá, ÊµãËØï"
    # sample_text = "Hallo, ich bin echo, Wie Heissen Sie"

    # text2audio(
    #     text=sample_text,
    #     rate=150,
    #     volume=0.9,
    #     # voice=None,  # Replace with a voice name or ID available on your system
    # )
    """
    if method is not None:
        methods = ["gTTS", "pyttsx3", "google"]
        method = strcmp(method, methods)[0]
    else:
        try:
            text2audio(
                text,
                method="google",
                rate=rate,
                slow=slow,
                volume=volume,
                voice=voice,
                lang=lang,
                gender=gender,
                age=age,
                dir_save=dir_save,
            )
        except Exception as e:
            print(e)
            text2audio(
                text,
                method="pyttsx3",
                rate=rate,
                slow=slow,
                volume=volume,
                voice=voice,
                lang=lang,
                gender=gender,
                age=age,
                dir_save=dir_save,
            )

    if method == "pyttsx3":
        import pyttsx3

        try:
            engine = pyttsx3.init()
            engine.setProperty("rate", rate)
            if 0.0 <= volume <= 1.0:
                engine.setProperty("volume", volume)
            else:
                raise ValueError("Volume must be between 0.0 and 1.0")

            if gender is not None:
                gender = strcmp(gender, ["male", "female"])[0]
            if age is not None:
                if isinstance(age, (float, int)):
                    if age <= 10:
                        age = "child"
                    elif 10 < age < 18:
                        age = "senior"
                    else:
                        age = "adult"
                elif isinstance(age, str):
                    age = strcmp(age, ["child", "adult", "senior"])[0]
                else:
                    raise ValueError("age: should be in ['child', 'adult', 'senior']")
            voices = engine.getProperty("voices")
            if voice is None:
                if lang is None:
                    voice = strcmp(detect_lang(text), [v.name for v in voices])[0]
                else:
                    if run_once_within():
                        print([v.name for v in voices])
                    print(f"lang:{lang}")
                    voice = strcmp(lang, [v.name for v in voices])[0]
            selected_voice = None

            for v in voices:
                # Check if the voice matches the specified gender or age
                if voice and (voice.lower() in v.name.lower() or voice in v.id):
                    selected_voice = v
                    break
                if gender and gender.lower() in v.name.lower():
                    selected_voice = v
                if age and age.lower() in v.name.lower():
                    selected_voice = v

            if selected_voice:
                engine.setProperty("voice", selected_voice.id)
            else:
                if voice or gender or age:
                    raise ValueError(
                        f"No matching voice found for specified criteria. Available voices: {[v.name for v in voices]}"
                    )
            # Generate audio
            if dir_save:
                engine.save_to_file(text, dir_save)
                print(f"Audio saved to {dir_save}")
            else:
                engine.say(text)

            engine.runAndWait()
        except Exception as e:
            print(f"An error occurred: {e}")
            #     # Explicitly terminate the pyttsx3 engine to release resources
            try:
                engine.stop()
            except RuntimeError:
                pass
                # Safely exit the script if running interactively to avoid kernel restarts
            try:
                

                sys.exit()
            except SystemExit:
                pass
    elif method in ["google", "gtts"]:
        from gtts import gTTS

        try:
            if lang is None:
                from langdetect import detect

                lang = detect(text)
            # Initialize gTTS with the provided parameters
            tts = gTTS(text=text, lang=lang, slow=slow)
        except Exception as e:
            print(f"An error occurred: {e}")

        print("not realtime reading...")
        if dir_save:
            if "." not in dir_save:
                dir_save = dir_save + ".mp3"
            tts.save(dir_save)
            print(f"Audio saved to {dir_save}")
        else:
            dir_save = "temp_audio.mp3"
            if "." not in dir_save:
                dir_save = dir_save + ".mp3"
            tts.save(dir_save)
        try:
            fopen(dir_save)
        except Exception as e:
            print(f"Error opening file: {e}")
    print("done")

# from datetime import datetime
def str2time(
    time_str: str, 
    fmt: str = "24", 
    return_obj: bool = False,
    raise_errors: bool = True,
    default: Optional[Any] = None
) -> Union[str, datetime]:
    """
    Convert a time string into the specified format with enhanced parsing capabilities.
    
    Parameters:
    - time_str (str): The time string to be converted.
    - fmt (str): The format to convert the time to. Can be:
        - '12' or '12h' for 12-hour format (02:30:45 PM)
        - '24' or '24h' for 24-hour format (14:30:45)
        - Any valid strftime format string
    - return_obj (bool): If True, returns datetime.time object instead of string
    - raise_errors (bool): If False, returns default value on error instead of raising
    - default: Value to return on error when raise_errors=False
    
    Returns:
    - str/datetime.time: The converted time string or time object
    
    Supported Input Formats:
    - 14:30:45
    - 02:30:45 PM
    - 2:30 PM
    - 2PM
    - 1430 (military time)
    - 2.30.45
    - 14-30-45
    - and many more
    
    Example Usage:
    str2time("14:30:45", fmt='12')  # returns '02:30:45 PM'
    str2time("02:30:45 PM", fmt='24')  # returns '14:30:45'
    str2time("2PM", return_obj=True)  # returns datetime.time(14, 0)
    """
    from dateutil import parser
    def preprocess_time_string(ts: str) -> str:
        """Normalize various time string formats to a parseable format"""
        if not isinstance(ts, str):
            if raise_errors:
                raise ValueError(f"Expected string, got {type(ts)}")
            return default
        
        # Clean the string
        ts = ts.strip().lower()
        
        # Handle military time (e.g., "1430" -> "14:30")
        if re.fullmatch(r'^[0-2]?\d[0-5]\d$', ts):
            ts = f"{ts[:2]}:{ts[2:]}"
        
        # Handle formats without separators (e.g., "2pm" -> "2 pm")
        ts = re.sub(r'([0-9])([a-z])', r'\1 \2', ts)
        
        # Replace various separators with colons
        ts = re.sub(r'[\.\-_]', ':', ts)
        
        # Handle AM/PM without space (e.g., "2:30pm" -> "2:30 pm")
        ts = re.sub(r'([0-9])(am|pm)', r'\1 \2', ts, flags=re.IGNORECASE)
        
        # Add seconds if missing (e.g., "14:30" -> "14:30:00")
        if re.fullmatch(r'^\d{1,2}:\d{2}$', ts):
            ts = f"{ts}:00"
        
        # Capitalize AM/PM for parsing
        ts = re.sub(r'am|pm', lambda x: x.group().upper(), ts)
        
        return ts.strip()
    
    # Determine output format
    fmt = fmt.lower()
    if fmt in ('12', '12h'):
        output_fmt = "%I:%M:%S %p"
    elif fmt in ('24', '24h'):
        output_fmt = "%H:%M:%S"
    else:
        output_fmt = fmt  # allow custom format strings
    
    try:
        processed_str = preprocess_time_string(time_str)
        
        # Try multiple parsing strategies
        for pattern in ["%H:%M:%S", "%I:%M:%S %p", "%H:%M", "%I:%M %p"]:
            try:
                time_obj = datetime.strptime(processed_str, pattern).time()
                break
            except ValueError:
                continue
        else:
            # Fallback to dateutil's parser for complex cases
            time_obj = parser.parse(processed_str).time()
        
        if return_obj:
            return time_obj
        return time_obj.strftime(output_fmt)
    
    except Exception as e:
        if raise_errors:
            raise ValueError(f"Unable to parse time string: '{time_str}'. Error: {e}")
        return default
def century_corr(dt,fmt,century_threshold=None,verbose=False):
    if century_threshold is None:
        century_threshold=int(str(datetime.today().year)[-2:])+5
    # Â§ÑÁêÜ‰∏§‰ΩçÊï∞Âπ¥‰ªΩÁöÑÊÉÖÂÜµ
    if '%y' in fmt.lower() and not '%Y' in fmt.lower():
        current_year_last = datetime.now().year % 100
        year_full = dt.year
        
        # ËÆ°ÁÆóÊ≠£Á°ÆÁöÑ‰∏ñÁ∫™
        if (dt.year % 100) < century_threshold:
            year_full = 2000 + (dt.year % 100)
        else:
            year_full = 1900 + (dt.year % 100)
        if verbose:
            print(f"Handle 2-digit years: {dt.year} -> {year_full} (century_threshold={century_threshold})")
        
        # Á°Æ‰øùÂπ¥‰ªΩÂú®ÂêàÁêÜËåÉÂõ¥ÂÜÖ
        if 1900 <= year_full <= 2100:
            dt = dt.replace(year=year_full)
    return dt

# def str2date(
#     date_str: Union[str, int, float], 
#     fmt: Optional[str] = "%Y-%m-%d",
#     original_fmt: Optional[str] = None,
#     century_threshold=None,
#     return_obj: bool = False,
#     verbose: bool = False,
#     **parser_kwargs
# ) -> Union[str, datetime, Dict[str, Any]]:
#     """
#     Convert a date string to the desired format with enhanced parsing capabilities.
    
#     Parameters:
#     - date_str: The input date string or numeric value (will be stringified)
#     - original_fmt: The original format of the date string (optional)
#     - fmt: The desired output format string. None returns datetime object
#     - return_obj: If True, always returns datetime object (overrides fmt)
#     - parser_kwargs: Additional kwargs for dateutil.parser.parse
    
#     Returns:
#     - str/datetime: The converted date string or datetime object
    
#     Supported Input Formats:
#     - 2023-05-15
#     - 15/05/2023
#     - May 15, 2023
#     - 15 May 2023
#     - 20230515
#     - and many more
    
#     Example Usage:
#     str2date("15/05/2023", fmt="%Y-%m-%d")  # returns '2023-05-15'
#     str2date("May 15, 2023", return_obj=True)  # returns datetime object
#     str2date("2023-05-15", fmt="%d.%m.%Y")  # returns '15.05.2023'
#     """
#     from dateutil import parser
#     try:
#         if is_nan(date_str):
#             return date_str
#         if century_threshold is None:
#             century_threshold=int(str(datetime.today().year)[-2:])+5
#         date_str_copy = date_str
#         # if isinstance(date_str, bytes): 
#         #     date_str = date_str.decode('utf-8', errors='replace').strip()

#         # elif isinstance(date_str, str):
#         #     # already a string, just strip whitespace
#         #     date_str = date_str.strip()
#         # else:
#         #     # handle unexpected types
#         #     date_str = str(date_str).strip()
#         if not isinstance(date_str, str):
#             date_str = str(date_str).strip()

#         common_formats = [
#             # --- DMY (European, Asia, etc.) ---
#             "%d-%m-%Y", "%d/%m/%Y", "%d.%m.%Y", "%d %m %Y",
#             "%d-%m-%y", "%d/%m/%y", "%d.%m.%y",
#             "%d-%b-%Y", "%d-%b-%y", "%d/%b/%Y", "%d/%b/%y",
#             "%d %b %Y", "%d %B %Y", "%d-%B-%Y",
#             "%A, %d %B %Y", "%a, %d %b %Y", "%a %b %d %H:%M:%S %Y",
#             "%d-%m-%Y %H:%M:%S", "%d/%m/%Y %H:%M:%S",
#             "%d-%m-%Y %H:%M", "%d/%m/%Y %H:%M",
            
#             # --- ISO & Standard ---
#             "%Y-%m-%d", "%Y/%m/%d", "%Y.%m.%d", "%Y_%m_%d", "%Y %m %d",
#             "%Y%m%d", "%Y %b %d", "%Y %B %d",
#             "%Y-%m-%d %H:%M:%S", "%Y/%m/%d %H:%M:%S", "%Y.%m.%d %H:%M:%S",
#             "%Y-%m-%dT%H:%M:%S", "%Y-%m-%dT%H:%M:%SZ", "%Y-%m-%dT%H:%M:%S.%fZ",
#             "%Y-%m-%dT%H:%M:%S.%f", "%Y-%m-%d %H:%M:%S.%f",
#             "%Y-%m-%d %H:%M", "%Y/%m/%d %H:%M", "%Y.%m.%d %H:%M",
#             "%Y-%m-%d %H:%M:%S%z", "%Y-%m-%dT%H:%M:%S%z",


#             # --- MDY (US style) ---
#             "%m-%d-%Y", "%m/%d/%Y", "%m.%d.%Y", "%m %d %Y",
#             "%m-%d-%y", "%m/%d/%y", "%m.%d.%y",
#             "%b %d, %Y", "%B %d, %Y", "%b %d %Y", "%B %d %Y",
#             "%b-%d-%Y", "%B-%d-%Y", "%b-%d-%y", "%B-%d-%y",
#             "%A, %B %d, %Y", "%a, %b %d, %Y",
#             "%m-%d-%Y %H:%M:%S", "%m/%d/%Y %H:%M:%S",
#             "%m-%d-%Y %H:%M", "%m/%d/%Y %H:%M",

#             # --- YDM / less common hybrids ---
#             "%Y-%d-%m", "%Y/%d/%m", "%Y.%d.%m",
#             "%Y %d %b", "%Y %d %B",
#             "%Y %d-%b", "%Y %d-%B",

#             # --- Weekday + Time (logs, email headers) ---
#             "%a, %d %b %Y %H:%M:%S", "%a, %d %b %Y %H:%M:%S %Z",
#             "%a %d %b %Y %H:%M:%S", "%a %d %b %Y %H:%M:%S %Z",
#             "%A %d %B %Y %H:%M:%S", "%a %b %d %H:%M:%S %Y",

#             # --- Year & Month Only ---
#             "%Y-%m", "%Y/%m", "%Y.%m", "%m/%Y", "%m-%Y", "%m.%Y",
#             "%b %Y", "%B %Y", "%Y %B", "%Y %b",
#             "%YÂπ¥%mÊúà", "%YÂπ¥%m",
#             "%YÂπ¥", "%Y",

#             # --- Chinese / Japanese / Korean (CJK) ---
#             "%YÂπ¥%mÊúà%dÊó•", "%YÂπ¥%mÊúà%dÂè∑", "%YÂπ¥%mÊúà%d", "%YÂπ¥%mÊúà",
#             "%YÂπ¥%m", "%YÂπ¥",
#             "%YÂπ¥%mÊúà%dÊó• %HÊó∂%MÂàÜ%SÁßí",
#             "%YÂπ¥%mÊúà%dÊó• %H:%M:%S",
#             "%YÂπ¥%mÊúà%dÂè∑ %H:%M:%S",
#             "%YÂπ¥%mÊúà%d %H:%M",
#             "%YÂπ¥%mÊúà%d",
#             "%YÎÖÑ %mÏõî %dÏùº", "%YÎÖÑ %mÏõî", "%YÎÖÑ",   # Korean
#             "%YÂπ¥%mÊúà%dÊó•(%a)", "%YÂπ¥%mÊúà%dÊó•(%A)",   # Japanese weekday suffix

#             # --- Partial / Loose formats ---
#             "%Y", "%y", "%b %y", "%B %y", "%Y %b", "%Y %B",
#             "%d %b", "%b %d", "%d %B", "%B %d",
#             "%m/%d", "%d/%m", "%m-%d", "%d-%m",

#             # --- Time only (in case of incomplete timestamps) ---
#             "%H:%M:%S", "%H:%M", "%H:%M:%S.%f",
#         ]

#         # Clean the string
#         date_str = re.sub(r'[\s_]+', ' ', date_str.strip())
        
#         date_candidates=fetch_text(date_str,str2pattern('date'))
#         # If no date candidates found, return the original string
#         if not date_candidates:
#             if verbose:
#                 print(f"No date patterns found in: '{date_str}'")
#             return date_str
#         cand=date_candidates[0] # only take the first candicate
#         # Try parsing with original format first
#         if original_fmt:
#             try:
#                 date_obj = datetime.strptime(cand, original_fmt)
#                 date_obj = century_corr(date_obj,fmt,century_threshold=century_threshold)
#                 if return_obj:
#                     return date_obj
#                 return date_obj.strftime(fmt) if fmt else date_obj
#             except ValueError: 
#                 for original_fmt_ in common_formats:
#                     try: 
#                         date_obj = datetime.strptime(cand, original_fmt_)
#                         if return_obj:
#                             return date_obj
#                         return date_obj.strftime(fmt) if fmt else date_obj
#                     except Exception as e:
#                         print(f"with trying common_formats:\n{original_fmt_} Error: {e}") if verbose else None
        
#         # Fall back to dateutil's parser 
#         try:
#             date_obj = parser.parse(cand, **parser_kwargs)
#             date_obj = century_corr(date_obj,fmt,century_threshold=century_threshold)
#             if return_obj:
#                 return date_obj
#             return date_obj.strftime(fmt) if fmt else date_obj
#         except Exception as e:
#             for date_format in common_formats:
#                 try:
#                     date_obj = datetime.strptime(cand, date_format)
#                     date_obj = century_corr(date_obj,date_format,century_threshold=century_threshold)
#                     if return_obj:
#                         return date_obj
#                     return date_obj.strftime(fmt) if fmt else date_obj
#                 except Exception as e:
#                     print(f"with trying common_formats:\n{date_format} Error: {e}") if verbose else None
#                     continue
#             print(f"Unable to process date string: '{cand}',type={type(cand)}. Error: {e}") if verbose else None
#             return date_str_copy
#     except Exception as e:
#         print(f"Unable to process date string: '{date_str}',type={type(date_str)}. Error: {e}") if verbose else None
#         return date_str_copy

def str2date(
    date_str: Union[str, int, float], 
    fmt: Optional[str] = "%Y-%m-%d",
    original_fmt: Optional[str] = None,
    century_threshold=None,
    return_obj: bool = False,
    verbose: bool = False,
    **parser_kwargs
) -> Union[str, datetime, Dict[str, Any]]:
    """
    Convert a date string to the desired format with enhanced parsing capabilities.
    
    Parameters:
    - date_str: The input date string or numeric value (will be stringified)
    - original_fmt: The original format of the date string (optional)
    - fmt: The desired output format string. None returns datetime object
    - return_obj: If True, always returns datetime object (overrides fmt)
    - parser_kwargs: Additional kwargs for dateutil.parser.parse
    
    Returns:
    - str/datetime: The converted date string or datetime object
    
    Supported Input Formats:
    - 2023-05-15
    - 15/05/2023
    - May 15, 2023
    - 15 May 2023
    - 20230515
    - and many more
    
    Example Usage:
    str2date("15/05/2023", fmt="%Y-%m-%d")  # returns '2023-05-15'
    str2date("May 15, 2023", return_obj=True)  # returns datetime object
    str2date("2023-05-15", fmt="%d.%m.%Y")  # returns '15.05.2023'
    """
    from dateutil import parser
    try:
        if is_nan(date_str):
            return date_str
        if century_threshold is None:
            century_threshold=int(str(datetime.today().year)[-2:])+5
        if not isinstance(date_str, str):
            date_str = str(date_str).strip()
        
        # Clean the string
        date_str = re.sub(r'[\s_]+', ' ', date_str.strip())
        
        common_formats = [
            # --- DMY (European, Asia, etc.) ---
            "%d-%m-%Y", "%d/%m/%Y", "%d.%m.%Y", "%d %m %Y",
            "%d-%m-%y", "%d/%m/%y", "%d.%m.%y",
            "%d-%b-%Y", "%d-%b-%y", "%d/%b/%Y", "%d/%b/%y",
            "%d %b %Y", "%d %B %Y", "%d-%B-%Y",
            "%A, %d %B %Y", "%a, %d %b %Y", "%a %b %d %H:%M:%S %Y",
            "%d-%m-%Y %H:%M:%S", "%d/%m/%Y %H:%M:%S",
            "%d-%m-%Y %H:%M", "%d/%m/%Y %H:%M",
            
            # --- ISO & Standard ---
            "%Y-%m-%d", "%Y/%m/%d", "%Y.%m.%d", "%Y_%m_%d", "%Y %m %d",
            "%Y%m%d", "%Y %b %d", "%Y %B %d",
            "%Y-%m-%d %H:%M:%S", "%Y/%m/%d %H:%M:%S", "%Y.%m.%d %H:%M:%S",
            "%Y-%m-%dT%H:%M:%S", "%Y-%m-%dT%H:%M:%SZ", "%Y-%m-%dT%H:%M:%S.%fZ",
            "%Y-%m-%dT%H:%M:%S.%f", "%Y-%m-%d %H:%M:%S.%f",
            "%Y-%m-%d %H:%M", "%Y/%m/%d %H:%M", "%Y.%m.%d %H:%M",
            "%Y-%m-%d %H:%M:%S%z", "%Y-%m-%dT%H:%M:%S%z",


            # --- MDY (US style) ---
            "%m-%d-%Y", "%m/%d/%Y", "%m.%d.%Y", "%m %d %Y",
            "%m-%d-%y", "%m/%d/%y", "%m.%d.%y",
            "%b %d, %Y", "%B %d, %Y", "%b %d %Y", "%B %d %Y",
            "%b-%d-%Y", "%B-%d-%Y", "%b-%d-%y", "%B-%d-%y",
            "%A, %B %d, %Y", "%a, %b %d, %Y",
            "%m-%d-%Y %H:%M:%S", "%m/%d/%Y %H:%M:%S",
            "%m-%d-%Y %H:%M", "%m/%d/%Y %H:%M",

            # --- YDM / less common hybrids ---
            "%Y-%d-%m", "%Y/%d/%m", "%Y.%d.%m",
            "%Y %d %b", "%Y %d %B",
            "%Y %d-%b", "%Y %d-%B",

            # --- Weekday + Time (logs, email headers) ---
            "%a, %d %b %Y %H:%M:%S", "%a, %d %b %Y %H:%M:%S %Z",
            "%a %d %b %Y %H:%M:%S", "%a %d %b %Y %H:%M:%S %Z",
            "%A %d %B %Y %H:%M:%S", "%a %b %d %H:%M:%S %Y",

            # --- Year & Month Only ---
            "%Y-%m", "%Y/%m", "%Y.%m", "%m/%Y", "%m-%Y", "%m.%Y",
            "%b %Y", "%B %Y", "%Y %B", "%Y %b",
            "%YÂπ¥%mÊúà", "%YÂπ¥%m",
            "%YÂπ¥", "%Y",

            # --- Chinese / Japanese / Korean (CJK) ---
            "%YÂπ¥%mÊúà%dÊó•", "%YÂπ¥%mÊúà%dÂè∑", "%YÂπ¥%mÊúà%d", "%YÂπ¥%mÊúà",
            "%YÂπ¥%m", "%YÂπ¥",
            "%YÂπ¥%mÊúà%dÊó• %HÊó∂%MÂàÜ%SÁßí",
            "%YÂπ¥%mÊúà%dÊó• %H:%M:%S",
            "%YÂπ¥%mÊúà%dÂè∑ %H:%M:%S",
            "%YÂπ¥%mÊúà%d %H:%M",
            "%YÂπ¥%mÊúà%d",
            "%YÎÖÑ %mÏõî %dÏùº", "%YÎÖÑ %mÏõî", "%YÎÖÑ",   # Korean
            "%YÂπ¥%mÊúà%dÊó•(%a)", "%YÂπ¥%mÊúà%dÊó•(%A)",   # Japanese weekday suffix

            # --- Partial / Loose formats ---
            "%Y", "%y", "%b %y", "%B %y", "%Y %b", "%Y %B",
            "%d %b", "%b %d", "%d %B", "%B %d",
            "%m/%d", "%d/%m", "%m-%d", "%d-%m",

            # --- Time only (in case of incomplete timestamps) ---
            "%H:%M:%S", "%H:%M", "%H:%M:%S.%f",
        ]
        # Try parsing with original format first
        if original_fmt:
            try:
                date_obj = datetime.strptime(date_str, original_fmt)
                date_obj = century_corr(date_obj,fmt,century_threshold=century_threshold)
                if return_obj:
                    return date_obj
                return date_obj.strftime(fmt) if fmt else date_obj
            except ValueError:
                # common_formats = [
                #     "%Y%m%d",  # 20230515
                #     "%d%m%Y",  # 15052023
                #     "%m%d%Y",  # 05152023
                #     "%Y-%m-%d",
                #     "%d-%m-%Y",
                #     "%m/%d/%Y",
                #     "%d/%m/%Y",
                #     "%b %d, %Y",  # May 15, 2023
                #     "%d %b %Y",  # 15 May 2023
                # ] 
                for original_fmt_ in common_formats:
                    try: 
                        date_obj = datetime.strptime(date_str, original_fmt_)
                        if return_obj:
                            return date_obj
                        return date_obj.strftime(fmt) if fmt else date_obj
                    except ValueError:
                        continue 
        # Fall back to dateutil's parser
        try:
            date_obj = parser.parse(date_str, **parser_kwargs)
            date_obj=century_corr(date_obj,fmt,century_threshold=century_threshold)
            if return_obj:
                return date_obj
            return date_obj.strftime(fmt) if fmt else date_obj
        except parser.ParserError as e:
            # Try some common alternative formats 
            for date_format in common_formats:
                try:
                    date_obj = datetime.strptime(date_str, date_format)
                    date_obj = century_corr(date_obj,date_format,century_threshold=century_threshold)
                    if return_obj:
                        return date_obj
                    return date_obj.strftime(fmt) if fmt else date_obj
                except ValueError:
                    continue
            print(f"Unable to process date string: '{date_str}',type={type(date_str)}. Error: {e}") if verbose else None
            return date_str
    
    except Exception as e:
        print(f"Unable to process date string: '{date_str}',type={type(date_str)}. Error: {e}") if verbose else None
        return date_str

def datetime_parser(
    datetime_str: str,
    time_fmt: Optional[str] = None,
    date_fmt: Optional[str] = None,
    return_obj: bool = False,
    raise_errors: bool = True,
    default: Optional[Any] = None,
    **parser_kwargs
) -> Union[str, datetime, Dict[str, Any]]:
    """
    Parse a combined date and time string with flexible format handling.
    
    Parameters:
    - datetime_str: String containing both date and time
    - time_fmt: Format for time portion (uses str2time rules)
    - date_fmt: Format for date portion (uses str2date rules)
    - return_obj: If True, returns datetime object
    - raise_errors: If False, returns default on error
    - default: Value to return on error when raise_errors=False
    - parser_kwargs: Passed to dateutil.parser
    
    Returns:
    - Parsed datetime string or object
    
    Example Usage:
    >>> datetime_parser("May 15, 2023 2:30 PM")
    '2023-05-15 14:30:00'
    >>> datetime_parser("15.05.2023 14:30", date_fmt="%d.%m.%Y", time_fmt="12")
    '15.05.2023 02:30:00 PM'
    """
    try:
        # First try parsing as complete datetime
        dt_obj = parser.parse(datetime_str, **parser_kwargs)
        
        # Apply formatting if needed
        if return_obj:
            return dt_obj
        
        date_part = str2date(dt_obj.date(), fmt=date_fmt or "%Y-%m-%d", return_obj=False)
        time_part = str2time(dt_obj.time(), fmt=time_fmt or "24", return_obj=False)
        
        return f"{date_part} {time_part}"
    
    except Exception as e:
        if raise_errors:
            raise ValueError(f"Unable to parse datetime string: '{datetime_str}'. Error: {e}")
        return default



def datetime2str(
    dt_input: Union[datetime, date, time, str, int, float],
    fmt: str = "auto",
    target: str = "both",
    raise_errors: bool = True,
    default: Optional[Any] = None,
    **format_kwargs,
) -> str:
    """
    Convert datetime/date/time objects or strings to formatted strings with extensive options.

    Parameters:
    - dt_input: Input to format (datetime, date, time, or parsable string/timestamp)
    - fmt: Output format specification:
        - For dates:
            - 'iso' = ISO8601 (YYYY-MM-DD)
            - 'sql' = YYYY-MM-DD
            - 'euro' = DD.MM.YYYY
            - 'us' = MM/DD/YYYY
            - 'full' = Weekday, Month Day, Year (e.g., "Monday, January 01, 2023")
            - Any strftime format string
        - For times:
            - 'iso' = ISO8601 (HH:MM:SS)
            - '12'/'12h' = 12-hour with AM/PM
            - '24'/'24h' = 24-hour format
            - 'sql' = HH:MM:SS
            - 'short' = HH:MM
            - Any strftime format string
        - 'auto' (default) = smart formatting based on input type
    - target: What to format ('date', 'time', or 'both')
    - raise_errors: If False, returns default value on error
    - default: Value to return on error when raise_errors=False
    - format_kwargs: Additional formatting options:
        - month_format: 'full' (January), 'short' (Jan), or 'number' (01)
        - weekday_format: 'full' (Monday), 'short' (Mon), or None
        - ampm: 'lower' (am/pm), 'upper' (AM/PM), or 'title' (Am/Pm)
        - precision: for times - 'seconds', 'minutes', 'hours', or 'milliseconds'

    Returns:
    - Formatted string representation of the input

    Example Usage:
    now = datetime.now()
    datetime2str(now, fmt='full')  # "Monday, January 01, 2023 02:30:45 PM"
    datetime2str(now.date(), fmt='euro')  # "01.01.2023"
    datetime2str("14:30:45", fmt='12', target='time')  # "02:30:45 PM"
    datetime2str(1672539045, fmt='us')  # "01/01/2023" (from timestamp)
    """

    def _resolve_format(fmt: str, obj_type: type) -> str:
        """Determine the appropriate format string based on shorthand options"""
        fmt = fmt.lower()

        # Date formats
        if obj_type in (datetime, date):
            if fmt in ("auto", "iso", "sql"):
                return "%Y-%m-%d"
            elif fmt == "euro":
                return "%d.%m.%Y"
            elif fmt == "us":
                return "%m/%d/%Y"
            elif fmt == "full":
                return "%A, %B %d, %Y"
            elif fmt == "short":
                return "%b %d, %Y"

        # Time formats
        elif obj_type == time:
            if fmt in ("auto", "iso", "sql"):
                return "%H:%M:%S"
            elif fmt in ("12", "12h"):
                return "%I:%M:%S %p"
            elif fmt in ("24", "24h"):
                return "%H:%M:%S"
            elif fmt == "short":
                return "%H:%M"

        return fmt  # Return as-is if not a recognized shorthand

    def _apply_format_options(format_str: str, obj_type: type, options: dict) -> str:
        """Modify format string based on additional options"""
        # Handle month formatting
        if obj_type in (datetime, date):
            if options.get("month_format") == "short":
                format_str = format_str.replace("%B", "%b")
            elif options.get("month_format") == "number":
                format_str = format_str.replace("%B", "%m").replace("%b", "%m")

            # Handle weekday formatting
            if options.get("weekday_format") == "short":
                format_str = format_str.replace("%A", "%a")
            elif options.get("weekday_format") is None:
                format_str = re.sub(r"%[Aa],?\s*", "", format_str)

        # Handle AM/PM formatting
        if obj_type in (datetime, time) and "%p" in format_str:
            ampm = options.get("ampm", "upper")
            if ampm == "lower":
                format_str = format_str.replace("%p", "%p").lower()
            elif ampm == "title":
                format_str = format_str.replace("%p", "%p")

        # Handle time precision
        if obj_type in (datetime, time):
            precision = options.get("precision")
            if precision == "minutes":
                format_str = re.sub(r":%S(\s*%p)?$", r"\1", format_str)
            elif precision == "hours":
                format_str = re.sub(r":%M:%S(\s*%p)?$", r"\1", format_str)
            elif precision == "milliseconds":
                format_str = (
                    format_str.replace("%S", "%S.%f")
                    if "%S" in format_str
                    else format_str + ".%f"
                )

        return format_str.strip()

    try:
        # Convert input to appropriate type if it's a string or timestamp
        if isinstance(dt_input, (int, float)):
            dt_input = datetime.fromtimestamp(dt_input)
        elif isinstance(dt_input, str):
            try:
                # Try parsing as datetime first
                dt_input = parser.parse(dt_input)
            except (ValueError, TypeError):
                try:
                    # Try parsing as time
                    dt_input = parser.parse(dt_input).time()
                except (ValueError, TypeError):
                    try:
                        # Try parsing as date
                        dt_input = parser.parse(dt_input).date()
                    except (ValueError, TypeError) as e:
                        if raise_errors:
                            raise ValueError(
                                f"Could not parse input string: {dt_input}"
                            ) from e
                        return default

        # Determine what we're formatting
        if target == "auto":
            if isinstance(dt_input, time):
                target = "time"
            elif isinstance(dt_input, date) and not isinstance(dt_input, datetime):
                target = "date"
            else:
                target = "both"

        # Get the appropriate format string
        if fmt == "auto":
            if target == "time":
                fmt = "24h"
            elif target == "date":
                fmt = "iso"
            else:
                fmt = "iso"  # Will be combined later

        format_str = _resolve_format(fmt, type(dt_input))
        format_str = _apply_format_options(format_str, type(dt_input), format_kwargs)

        # Handle different target types
        if target == "date" and isinstance(dt_input, (datetime, date)):
            return dt_input.strftime(format_str)
        elif target == "time" and isinstance(dt_input, (datetime, time)):
            if isinstance(dt_input, datetime):
                return dt_input.time().strftime(format_str)
            return dt_input.strftime(format_str)
        elif target == "both" and isinstance(dt_input, datetime):
            date_part = datetime2str(
                dt_input.date(), fmt=fmt, target="date", **format_kwargs
            )
            time_part = datetime2str(
                dt_input.time(), fmt=fmt, target="time", **format_kwargs
            )
            return f"{date_part} {time_part}"
        else:
            if raise_errors:
                raise ValueError(f"Cannot format {type(dt_input)} as {target}")
            return default

    except Exception as e:
        if raise_errors:
            raise ValueError(f"Failed to format {dt_input}: {str(e)}")
        return default


# Additional convenience functions
def date2str(
    date_input: Union[date, datetime, str, int, float], fmt: str = "iso", **kwargs
) -> str:
    """Convert date objects or date strings to formatted date strings"""
    return datetime2str(date_input, fmt=fmt, target="date", **kwargs)


def time2str(
    time_input: Union[time, datetime, str, int, float], fmt: str = "24h", **kwargs
) -> str:
    """Convert time objects or time strings to formatted time strings"""
    return datetime2str(time_input, fmt=fmt, target="time", **kwargs)

# def str2num(
#     s: str,
#     *args,
#     sep: Optional[Union[str, List[str]]] = None,
#     round_digits: Optional[int] = None,
#     return_list: bool = None,
#     handle_text: bool = True):
#     """
#     # Examples
#     print(str2num("123"))                # Output: 123
#     print(str2num("123.456", 2))         # Output: 123.46
#     print(str2num("one hundred and twenty three"))  # Output: 123
#     print(str2num("seven million"))      # Output: 7000000
#     print(str2num('one thousand thirty one',','))  # Output: 1,031
#     print(str2num("12345.6789", ","))    # Output: 12,345.6789
#     print(str2num("12345.6789", " ", 2)) # Output: 12 345.68
#     print(str2num('111113.34555',3,',')) # Output: 111,113.346
#     print(str2num("123.55555 sec miniuets",3)) # Output: 1.3
#     print(str2num("every 3,300.55 hours and 5.045555 min", sep=",", round=1))
#     print(str2num("five hundred fourty one"), str2num(
#         "this is 5.9435 euros for 10.04499 killograme", round=3
#     )[0])
#     Convert a string containing numeric or textual data into an integer, float, or list of numbers.

#     Parameters:
#     - s (str): Input string containing a number or textual representation of a number.
#     - *args: Additional arguments for delimiter or rounding digits.
#     - sep (str or list): Delimiter(s) to remove from the string (e.g., ',' or ['.', ',']).
#     - round_digits (int): Number of decimal places to round the result to.
#     - return_list (bool): Whether to return a list of numbers if multiple are found.
#     - handle_text (bool): Whether to process textual numbers using the numerizer library.

#     Returns:
#     - Union[float, int, List[Union[float, int]], None]: Converted number(s) or None if conversion fails.
#     """
#     if is_nan(s):
#         return s
#     if isinstance(s,list):
#         return [str2num(i, sep=sep,round_digits=round_digits,return_list=return_list,handle_text=handle_text) for i in s]
#     elif isinstance(s, str):
#         pass
#     else:
#         # return None
#         s=str(s)

#     # Merge args with explicit parameters
#     if sep is None:
#         sep = []
#     elif isinstance(sep, str):
#         sep = [sep]
#     for arg in args:
#         if isinstance(arg, str):
#             sep.append(arg)
#         elif isinstance(arg, int) and round_digits is None:
#             round_digits = arg

#     # # Remove all specified delimiters
#     # for delimiter in sep:
#     #     s = s.replace(delimiter, "")
#     # Remove delimiters efficiently using regex in one pass
#     if sep:
#         delimiter_pattern = re.compile('|'.join(map(re.escape, sep)))
#         s = delimiter_pattern.sub('', s)
#     else:
#         s = s
#     # Also strip common currency words/symbols before extraction to help Decimal later
#     s = (
#         s.replace("EUR", "")
#         .replace("eur", "")
#         .replace("‚Ç¨", "")
#         .replace("USD", "")
#         .replace("$", "")
#         .replace("GBP", "")
#         .replace("¬£", "")
#         .strip()
#     )
#     # Precompile regex for number extraction
#     number_pattern = re.compile(r"[-+]?\d*\.\d+|\d+")

#     # Attempt conversion
#     def try_convert(segment: str) -> Union[float, int, None]:
#         try:
#             return int(segment)
#         except ValueError:
#             try:
#                 return float(segment)
#             except ValueError:
#                 return None

#     # Use numerize ONLY if handle_text is enabled AND string has text
#     if handle_text and any(c.isalpha() for c in s):
#         try:
#             from numerizer import numerize
#             s = numerize(s)
#         except (ImportError, Exception):
#             pass

#     # Extract numeric segments
#     number_segments = number_pattern.findall(s)#re.findall(r"[-+]?\d*\.\d+|\d+", s)
#     # numbers = [try_convert(seg) for seg in number_segments if seg]
#     # numbers = [num for num in numbers if num is not None]
#     numbers = [num for seg in number_segments if (num := try_convert(seg)) is not None]
#     if not numbers:
#         return None  # No valid numbers found

#     # # Single or multiple numbers
#     # if len(numbers) == 1 and not return_list:
#     #     result = numbers[0]
#     # else:
#     #     result = (
#     #         numbers[0] if len(numbers) == 1 else numbers if return_list else numbers[0]
#     #     )
 
#     if return_list is None:
#         if len(numbers) == 1:
#             return numbers[0]
#         else:
#             return numbers
#     elif return_list:
#         return numbers
#     else:
#         return numbers[0]

#     # Apply rounding if necessary
#     if round_digits is not None:
#         if isinstance(result, list):
#             result = [round(num + 1e-10, round_digits) for num in result]
#         else:
#             result = round(result + 1e-10, round_digits)

#         # Convert to int if rounding to 0 digits
#         if round_digits == 0:
#             if isinstance(result, list):
#                 result = [int(num) for num in result]
#             else:
#                 result = int(result)
#     return result


from decimal import Decimal, ROUND_HALF_UP, InvalidOperation

def _is_nan(x):
    return pd.isna(x)


def _to_decimal_safe(x) -> Decimal:
    """Like your safe_decimal: return a Decimal or raise InvalidOperation."""
    if _is_nan(x):
        raise InvalidOperation("NaN")
    if isinstance(x, Decimal):
        d = x
    else:
        s = str(x).strip()
        # Remove common currency symbols / thousand separators
        s = (
            s.replace(",", "")
            .replace("EUR ", "")
            .replace("‚Ç¨", "")
            .replace("¬£", "")
            .replace("¬•", "")
            .replace("USD ", "")
        )
        if s == "":
            raise InvalidOperation("empty")
        try:
            d = Decimal(s)
        except InvalidOperation:
            # fallback to float -> Decimal (less precise but safe)
            d = Decimal(str(float(s)))
    return d


def str2num(
    s: Union[str, list, int, float, Decimal],
    *args,
    sep: Optional[Union[str, List[str]]] = None,
    round_digits: Optional[int] = None,
    return_list: Optional[bool] = None,
    handle_text: bool = True,
):
    """
    Convert a string (or list) into number(s). Uses Decimal for safe parsing/rounding.
    - If return_list is None: returns single value if only one number found, else list.
    - If return_list is True: always returns list.
    - If return_list is False: returns first found number.
    - round_digits (int): number of decimal places to round to using ROUND_HALF_UP.
 
    # Examples
    print(str2num("123"))                # Output: 123
    print(str2num("123.456", 2))         # Output: 123.46
    print(str2num("one hundred and twenty three"))  # Output: 123
    print(str2num("seven million"))      # Output: 7000000
    print(str2num('one thousand thirty one',','))  # Output: 1,031
    print(str2num("12345.6789", ","))    # Output: 12,345.6789
    print(str2num("12345.6789", " ", 2)) # Output: 12 345.68
    print(str2num('111113.34555',3,',')) # Output: 111,113.346
    print(str2num("123.55555 sec miniuets",3)) # Output: 1.3
    print(str2num("every 3,300.55 hours and 5.045555 min", sep=",", round=1))
    print(str2num("five hundred fourty one"), str2num(
        "this is 5.9435 euros for 10.04499 killograme", round=3
    )[0])
    Convert a string containing numeric or textual data into an integer, float, or list of numbers.

    Parameters:
    - s (str): Input string containing a number or textual representation of a number.
    - *args: Additional arguments for delimiter or rounding digits.
    - sep (str or list): Delimiter(s) to remove from the string (e.g., ',' or ['.', ',']).
    - round_digits (int): Number of decimal places to round the result to.
    - return_list (bool): Whether to return a list of numbers if multiple are found.
    - handle_text (bool): Whether to process textual numbers using the numerizer library.

    Returns:
    - Union[float, int, List[Union[float, int]], None]: Converted number(s) or None if conversion fails.
    """
    # Basic NaN handling
    if _is_nan(s):
        return s

    # Handle lists by recursion
    if isinstance(s, list):
        return [
            str2num(
                i,
                sep=sep,
                round_digits=round_digits,
                return_list=return_list,
                handle_text=handle_text,
            )
            for i in s
        ]

    # Normalize non-string inputs
    if not isinstance(s, str):
        s = str(s)

    # args handling: collect sep items and possibly round_digits
    if sep is None:
        sep = []
    elif isinstance(sep, str):
        sep = [sep]
    for arg in args:
        if isinstance(arg, str):
            sep.append(arg)
        elif isinstance(arg, int) and round_digits is None:
            round_digits = arg

    # If textual numbers exist and handling enabled, try numerize
    if handle_text and any(c.isalpha() for c in s):
        try:
            from numerizer import numerize  # optional dependency

            s = numerize(s)
        except Exception:
            # fallback: ignore if numerizer unavailable
            pass

    # Remove specified delimiters in one pass
    if sep:
        delimiter_pattern = re.compile("|".join(map(re.escape, sep)))
        s_clean = delimiter_pattern.sub("", s)
    else:
        s_clean = s

    # Also strip common currency words/symbols before extraction to help Decimal later
    s_clean = (
        s_clean.replace("EUR", "")
        .replace("eur", "")
        .replace("‚Ç¨", "")
        .replace("USD", "")
        .replace("$", "")
        .replace("GBP", "")
        .replace("¬£", "")
        .strip()
    )

    # Extract numeric segments (integers and decimals, with optional sign)
    number_pattern = re.compile(r"[-+]?\d*\.\d+|[-+]?\d+")
    number_segments = number_pattern.findall(s_clean)

    # Convert segments to Decimal safely
    numbers_dec = []
    for seg in number_segments:
        if not seg:
            continue
        try:
            dec = _to_decimal_safe(seg)
            numbers_dec.append(dec)
        except InvalidOperation:
            # skip non-convertible segments
            continue

    if not numbers_dec:
        return None

    # Apply rounding via Decimal.quantize if requested
    def apply_round(d: Decimal) -> Union[int, float]:
        if round_digits is None:
            # don't change precision; return int if integral else float
            if d == d.to_integral_value():
                return int(d)
            else:
                return float(d)
        else:
            q = (
                Decimal("1")
                if round_digits == 0
                else Decimal("1." + "0" * round_digits).quantize(Decimal("1"))
            )  # placeholder
            # Build quantize target like Decimal('0.01') for 2 digits
            quant_target = (
                Decimal("1")
                if round_digits == 0
                else Decimal("0." + "0" * (round_digits - 1) + "1")
            )
            rounded = (
                d.quantize(quant_target, rounding=ROUND_HALF_UP)
                if round_digits > 0
                else d.quantize(Decimal("1"), rounding=ROUND_HALF_UP)
            )
            # Return int when digits==0 else float
            if round_digits == 0:
                return int(rounded)
            # If the rounded value is integral (e.g., 2.00) and round_digits is 0 it's handled above;
            # otherwise return float with the rounded decimal.
            # Convert using float() is fine because we already rounded.
            return float(rounded)

    # Map decimals to numeric python types with rounding
    numbers = [apply_round(d) for d in numbers_dec]

    # Decide return format
    if return_list is None:
        if len(numbers) == 1:
            return numbers[0]
        else:
            return numbers
    elif return_list:
        return numbers
    else:
        return numbers[0]

def num2str(
    num: Union[int, float],
    *args,
    decimal_sep: str = ".",
    thousand_sep: str = ",",
    round_digits: Optional[int] = None,
    interval:int=3,
    force_decimal: bool = False,
    unit: Optional[str] = None,
    **kwargs,
) -> str:
    """
    Convert a number to a formatted string with customizable separators.

    Parameters:
    - num: Number to convert (int or float)
    - decimal_sep: Decimal point character (default ".")
    - thousand_sep: Thousands separator character (default ",")
    - round_digits: Number of decimal places to round to
    - force_decimal: Always show decimal places even if zero (e.g., "123.00")
    - Additional args/kwargs for backward compatibility:
      - sep: Alternate way to specify thousand_sep
      - round: Alternate way to specify round_digits

    Returns:
    - Formatted number string
    """
    # Backward compatibility with old sep/round parameters
    if "sep" in kwargs:
        thousand_sep = kwargs["sep"]
    if "round" in kwargs:
        round_digits = kwargs["round"]

    # Parse additional positional arguments (legacy support)
    for arg in args:
        if isinstance(arg, str):
            thousand_sep = arg
        elif isinstance(arg, int):
            round_digits = arg

    # Apply rounding if specified
    if round_digits is not None:
        num = round(num, round_digits)

    # Handle special cases
    if isinstance(num, float):
        if num == float("inf"):
            return "‚àû"
        if num == float("-inf"):
            return "-‚àû"
        if num != num:  # NaN check
            return "NaN"

    # Initialize variables
    integer_part = ""
    decimal_part = ""

    # Split into integer and decimal parts
    if isinstance(num, int):
        integer_part = str(num)
    else:
        num_str = (
            f"{num:.10f}".rstrip("0").rstrip(".")
            if not force_decimal
            else f"{num:.10f}"
        )
        if "." in num_str:
            integer_part, decimal_part = num_str.split(".")
            if round_digits is not None:
                decimal_part = decimal_part[:round_digits]
        else:
            integer_part = num_str

    # Add thousand separators to integer part
    if thousand_sep:
        sign = "-" if integer_part.startswith("-") else ""
        digits = integer_part.lstrip("-")

        if digits:  # Only process if we have digits
            reversed_digits = digits[::-1]
            chunks = [
                reversed_digits[i : i + interval] for i in range(0, len(reversed_digits), interval)
            ]
            formatted_reversed = thousand_sep.join(chunks)
            integer_part = sign + formatted_reversed[::-1]

    # Combine parts
    result = ""
    if decimal_part or (force_decimal and round_digits is not None and round_digits > 0):
        if force_decimal and round_digits is not None:
            decimal_part = decimal_part.ljust(round_digits, '0')[:round_digits]
        result = f"{integer_part}{decimal_sep}{decimal_part}"
    else:
        result = integer_part

    # Append unit if specified
    if unit:
        result = f"{result} {unit}"

    return result
# Examples
# print(num2str(123), type(num2str(123)))  # Output: "123"
# print(num2str(123.456, round=2), type(num2str(123.456, 2)))  # Output: "123.46"
# print(num2str(7000.125, round=1), type(num2str(7000.125, 2)))  # Output: "7000.13"
# print(
#     num2str(12345333.6789, sep=","), type(num2str(12345.6789, ","))
# )  # Output: "12,345.6789"
# print(num2str(7000.00, sep=","), type(num2str(7000.00, ",")))  # Output: "7,000.00"


# Helper to convert text or list of text to HTML
def str2html(text_list, strict=False):
    if not isinstance(text_list, list):
        text_list = [text_list]

    # Define a mapping for escape sequences to their HTML representations
    escape_mapping = {
        "\\": "&bsol;",  # Backslash
        "'": "&apos;",  # Single quote
        '"': "&quot;",  # Double quote
        "\n": "<br>",  # Newline
        "\r": "",  # Carriage return (not represented in HTML)
        "\t": "&nbsp;&nbsp;&nbsp;&nbsp;",  # Tab (4 spaces)
        "\b": "",  # Backspace (not typically represented)
        "\f": "",  # Form feed (not typically represented)
        "\v": "",  # Vertical tab (not typically represented)
        "\a": "",  # Bell/alert sound (not typically represented)
        "\0": "",  # Null character (not typically represented)
    }

    # Convert text by replacing escape sequences with their HTML equivalents
    html_content = ""
    for text in text_list:
        for escape_seq, html_rep in escape_mapping.items():
            text = text.replace(escape_seq, html_rep)
        html_content += text.replace("\n", "<br>")  # Add line breaks for newlines

    if strict:
        html_content = "<html><body>\n" + html_content + "\n</body></html>"

    return html_content


def cm2px(*cm, dpi=300) -> list:
    # Case 1: When the user passes a single argument that is a list or tuple, such as cm2px([8, 5]) or inch2cm((8, 5))
    if len(cm) == 1 and isinstance(cm[0], (list, tuple)):
        # If the input is a single list or tuple, we unpack its elements and convert each to cm
        return [i / 2.54 * dpi for i in cm[0]]
    # Case 2: When the user passes multiple arguments directly, such as cm2px(8, 5)
    else:
        return [i / 2.54 * dpi for i in cm]


def px2cm(*px, dpi=300) -> list:
    # Case 1: When the user passes a single argument that is a list or tuple, such as px2cm([8, 5]) or inch2cm((8, 5))
    if len(px) == 1 and isinstance(px[0], (list, tuple)):
        # If the input is a single list or tuple, we unpack its elements and convert each to cm
        return [i * 2.54 / dpi for i in px[0]]
    # Case 2: When the user passes multiple arguments directly, such as px2cm(8, 5)
    else:
        # Here, we convert each individual argument directly to cm
        return [i * 2.54 / dpi for i in px]


def px2inch(*px, dpi=300) -> list:
    """
    px2inch: converts pixel measurements to inches based on the given dpi.
    Usage:
    px2inch(300, 600, dpi=300); px2inch([300, 600], dpi=300)
    Returns:
        list: in inches
    """
    # Case 1: When the user passes a single argument that is a list or tuple, such as px2inch([300, 600]) or px2inch((300, 600))
    if len(px) == 1 and isinstance(px[0], (list, tuple)):
        # If the input is a single list or tuple, we unpack its elements and convert each to inches
        return [i / dpi for i in px[0]]
    # Case 2: When the user passes multiple arguments directly, such as px2inch(300, 600)
    else:
        # Here, we convert each individual argument directly to inches
        return [i / dpi for i in px]


def inch2cm(*cm) -> list:
    """
    cm2inch: converts centimeter measurements to inches.
    Usage:
    cm2inch(10, 12.7); cm2inch((10, 12.7)); cm2inch([10, 12.7])
    Returns:
        list: in inches
    """
    # Case 1: When the user passes a single argument that is a list or tuple, such as cm2inch([10, 12.7]) or cm2inch((10, 12.7))
    if len(cm) == 1 and isinstance(cm[0], (list, tuple)):
        # If the input is a single list or tuple, we unpack its elements and convert each to inches
        return [i * 2.54 for i in cm[0]]
    # Case 2: When the user passes multiple arguments directly, such as cm2inch(10, 12.7)
    else:
        # Here, we convert each individual argument directly to inches
        return [i * 2.54 for i in cm]


def inch2px(*inch, dpi=300) -> list:
    """
    inch2px: converts inch measurements to pixels based on the given dpi.

    Usage:
    inch2px(1, 2, dpi=300); inch2px([1, 2], dpi=300)

    Parameters:
    inch : float, list, or tuple
        Single or multiple measurements in inches to convert to pixels.
    dpi : int, optional (default=300)
        Dots per inch (DPI), representing the pixel density.

    Returns:
        list: Converted measurements in pixels.
    """
    # Case 1: When the user passes a single argument that is a list or tuple, e.g., inch2px([1, 2]) or inch2px((1, 2))
    if len(inch) == 1 and isinstance(inch[0], (list, tuple)):
        return [i * dpi for i in inch[0]]

    # Case 2: When the user passes multiple arguments directly, e.g., inch2px(1, 2)
    else:
        return [i * dpi for i in inch]


def cm2inch(*inch) -> list:
    """
    Usage:
    inch2cm(8,5); inch2cm((8,5)); inch2cm([8,5])
    Returns:
        list: in centimeters
    """
    # Case 1: When the user passes a single argument that is a list or tuple, such as inch2cm([8, 5]) or inch2cm((8, 5))
    if len(inch) == 1 and isinstance(inch[0], (list, tuple)):
        # If the input is a single list or tuple, we unpack its elements and convert each to cm
        return [i / 2.54 for i in inch[0]]
    # Case 2: When the user passes multiple arguments directly, such as inch2cm(8, 5)
    else:
        # Here, we convert each individual argument directly to cm
        return [i / 2.54 for i in inch]

def img2svg(fpath):
    """
    Convert an image file to SVG format using pixels2svg.

    e.g., img2svg(fpath)
    """
    
    from pixels2svg import pixels2svg
    fpath = Path(fpath)  # Ensure it's a Path object
    
    if not fpath.exists():
        raise FileNotFoundError(f"File not found: {fpath}")
    
    if not fpath.is_file():
        raise ValueError(f"Invalid file path: {fpath} is not a file")

    ftype = fpath.suffix.lstrip(".").lower()
    output_path = fpath.with_suffix(".svg")

    try:
        pixels2svg(fpath, output_path)
        print(f"saved @: {output_path}")
    except Exception as e:
        print(f"Â§±Ë¥•‰∫Ü!{e}")

    return output_path

def sqlite2sql(db_path, sql_path):
    """
    Export an SQLite database to an SQL file, including schema and data for all tables.

    :param db_path: Path to the SQLite .db file
    :param output_file: Path to the output .sql file

    # Usage
        db_path = "your_database.db"  # Replace with the path to your SQLite database
        sql_path = "output.sql"  # Replace with your desired output file name
        export_sqlite_to_sql(db_path, sql_path)

    """
    import sqlite3
    try:
        # Connect to the SQLite database
        conn = sqlite3.connect(db_path)
        cursor = conn.cursor()

        with open(sql_path, 'w') as f:
            # Write a header for the SQL dump
            f.write("-- SQLite Database Dump\n")
            f.write(f"-- Source: {db_path}\n\n")
            
            # Retrieve all table names
            cursor.execute("SELECT name FROM sqlite_master WHERE type='table' AND name NOT LIKE 'sqlite_%';")
            tables = [row[0] for row in cursor.fetchall()]

            for table in tables:
                # Write the schema for the table
                cursor.execute(f"SELECT sql FROM sqlite_master WHERE type='table' AND name='{table}';")
                schema = cursor.fetchone()
                if schema:
                    f.write(f"{schema[0]};\n\n")
                
                # Write data for the table
                cursor.execute(f"SELECT * FROM {table};")
                rows = cursor.fetchall()
                if rows:
                    cursor.execute(f"PRAGMA table_info({table});")
                    column_names = [info[1] for info in cursor.fetchall()]
                    column_list = ', '.join(f'"{col}"' for col in column_names)

                    for row in rows: 
                        values = ', '.join(
                            "'{}'".format(str(val).replace("'", "''")) if val is not None else 'NULL'
                            for val in row
                        )

                        f.write(f"INSERT INTO {table} ({column_list}) VALUES ({values});\n")
                
                f.write("\n")
            
            print(f"Database exported successfully to {sql_path}")
    
    except sqlite3.Error as e:
        print(f"SQLite error: {e}")
    except Exception as e:
        print(f"Unexpected error: {e}")
    finally:
        # Ensure the connection is closed
        if conn:
            conn.close()


def sreplace(*args, **kwargs):
    """
    sreplace(text, by=None, robust=True)
    Replace specified substrings in the input text with provided replacements.
    Args:
        text (str): The input text where replacements will be made.
        by (dict, optional): A dictionary containing substrings to be replaced as keys
            and their corresponding replacements as values. Defaults to {".com": "..come", "\n": " ", "\t": " ", "  ": " "}.
        robust (bool, optional): If True, additional default replacements for newline and tab characters will be applied.
                                Default is False.
    Returns:
        str: The text after replacements have been made.
    """
    text = None
    by = kwargs.get("by", None)
    robust = kwargs.get("robust", True)

    for arg in args:
        if isinstance(arg, str):
            text = arg
        elif isinstance(arg, dict):
            by = arg
        elif isinstance(arg, bool):
            robust = arg
        else:
            Error(f"{type(arg)} is not supported")

    # Default replacements for newline and tab characters
    default_replacements = {
        "\a": "",
        "\b": "",
        "\f": "",
        "\n": "",
        "\r": "",
        "\t": "",
        "\v": "",
        "\\": "",  # Corrected here
        # "\?": "",
        "ÔøΩ": "",
        "\\x": "",  # Corrected here
        "\\x hhhh": "",
        "\\ ooo": "",  # Corrected here
        "\xa0": "",
        "  ": " ",
    }

    # If dict_replace is None, use the default dictionary
    if by is None:
        by = {}
    # If robust is True, update the dictionary with default replacements
    if robust:
        by.update(default_replacements)

    # Iterate over each key-value pair in the dictionary and replace substrings accordingly
    for k, v in by.items():
        text = text.replace(k, v)
    return text


# usage:
# sreplace(text, by=dict(old_str='new_str'), robust=True)


def paper_size(paper_type_str="a4"):
    df = pd.DataFrame(
        {
            "a0": [841, 1189],
            "a1": [594, 841],
            "a2": [420, 594],
            "a3": [297, 420],
            "a4": [210, 297],
            "a5": [148, 210],
            "a6": [105, 148],
            "a7": [74, 105],
            "b0": [1028, 1456],
            "b1": [707, 1000],
            "b2": [514, 728],
            "b3": [364, 514],
            "b4": [257, 364],
            "b5": [182, 257],
            "b6": [128, 182],
            "letter": [215.9, 279.4],
            "legal": [215.9, 355.6],
            "business card": [85.6, 53.98],
            "photo china passport": [33, 48],
            "passport single": [125, 88],
            "visa": [105, 74],
            "sim": [25, 15],
        }
    )
    for name in df.columns:
        if paper_type_str in name.lower():
            paper_type = name
    if not paper_type:
        paper_type = "a4"  # default
    return df[paper_type].tolist()
 
def docx2pdf(dir_docx, dir_pdf=None):
    """
    Converts .docx to .pdf. Works on Windows using docx2pdf and on Linux/macOS using LibreOffice.
    
    Parameters:
    - dir_docx: path to .docx file or directory containing .docx files
    - dir_pdf: optional output directory; if None, uses same directory as input
    """

    system = platform.system()
    is_file = os.path.isfile(dir_docx)
    is_dir = os.path.isdir(dir_docx)

    if not is_file and not is_dir:
        raise FileNotFoundError(f"Input path '{dir_docx}' does not exist.")

    if system == "Windows":
        try:
            from docx2pdf import convert
        except ImportError:
            raise ImportError("docx2pdf is not installed. Run: pip install docx2pdf")

        convert(dir_docx, dir_pdf) if dir_pdf else convert(dir_docx)

    elif system in {"Linux", "Darwin"}:
        # Check if libreoffice is available
        if shutil.which("libreoffice") is None:
            raise EnvironmentError("LibreOffice is not installed or not in PATH. Install it with: sudo apt install libreoffice")

        # Determine the output directory
        output_dir = dir_pdf or os.path.dirname(dir_docx) if is_file else dir_docx

        if is_file:
            subprocess.run([
                "libreoffice", "--headless", "--convert-to", "pdf", "--outdir", output_dir, dir_docx
            ], check=True)
        elif is_dir:
            for filename in os.listdir(dir_docx):
                if filename.lower().endswith(".docx"):
                    full_path = os.path.join(dir_docx, filename)
                    subprocess.run([
                        "libreoffice", "--headless", "--convert-to", "pdf", "--outdir", dir_pdf or dir_docx, full_path
                    ], check=True)
    else:
        raise OSError(f"Unsupported OS: {system}")


def img2pdf(dir_img, kind=None, page=None, dir_save=None, page_size="a4", dpi=300):
    import img2pdf as image2pdf

    def mm_to_point(size):
        return (image2pdf.mm_to_pt(size[0]), image2pdf.mm_to_pt(size[1]))

    def set_dpi(x):
        dpix = dpiy = x
        return image2pdf.get_fixed_dpi_layout_fun((dpix, dpiy))

    if kind is None:
        _, kind = os.path.splitext(dir_img)
    if not kind.startswith("."):
        kind = "." + kind
    if dir_save is None:
        dir_save = dir_img.replace(kind, ".pdf")
    imgs = []
    if os.path.isdir(dir_img):
        if not dir_save.endswith(".pdf"):
            dir_save += "#merged_img2pdf.pdf"
        if page is None:
            select_range = ls(dir_img, kind=kind).fpath
        else:
            if not isinstance(page, (np.ndarray, list, range)):
                page = [page]
            select_range = ls(dir_img, kind=kind)["fpath"][page]
        for fname in select_range:
            if not fname.endswith(kind):
                continue
            path = os.path.join(dir_img, fname)
            if os.path.isdir(path):
                continue
            imgs.append(path)
    else:
        imgs = [
            # os.path.isdir(dir_img),
            dir_img
        ]
    print(imgs)
    if page_size:
        if isinstance(page_size, str):
            pdf_in_mm = mm_to_point(paper_size(page_size))
        else:
            print("default: page_size = (210,297)")
            pdf_in_mm = mm_to_point(page_size)
            print(f"page size was set to {page_size}")
        p_size = image2pdf.get_layout_fun(pdf_in_mm)
    else:
        p_size = set_dpi(dpi)
    with open(dir_save, "wb") as f:
        f.write(image2pdf.convert(imgs, layout_fun=p_size))

def ssplit(text, 
           by = "space", 
           regex: bool = None,
           strict: bool = False,
           keep_by: bool = False,
           strip_results: bool = True,
           verbose: bool = False,
           **kws
           ):
    r""" Determines the splitting strategy:
    #         - "space", "whitespace", "sp": split by whitespace (default)
    #         - "word": split into words using NLTK's word_tokenize
    #         - "sentence", "sent": split into sentences using NLTK's sent_tokenize
    #         - "sent_num", "sent_n": split every N sentences (use n=10 by default)
    #         - "char", "character": split into individual characters
    #         - "camel", "camelcase": split camelCase words
    #         - "upper", "capital": split at uppercase letters
    #         - "upper_lower", "ul": split at uppercase followed by lowercase
    #         - "lower_upper", "lu": split at lowercase followed by uppercase
    #         - "digits", "numbers", "num": split at numeric sequences
    #         - "number_words", "num_str": split at spelled-out numbers
    #         - "punctuation", "punct": split at punctuation marks
    #         - "lines", "line", "li": split by lines
    #         - "regex", "re": use custom regex pattern (provide in 'pattern' kwarg)
    #         - "fixed_len", "len": split into fixed-length chunks (provide 'length' kwarg)
    #         - "non_ascii", "lang": split at non-ASCII characters
    #         - "non_alphanum": split at consecutive non-alphanumeric chars
    #         - str: split by the exact string provided
    #         - list: split by any of the strings in the list

    Example:
    # Split by whitespace (default)
    print(split("Hello world! How are you?", verbose=1))
    # Split by specific string
    print(split("apple,orange,banana", by=",", verbose=1))
    # Split by multiple delimiters
    print(split("apple,orange;banana-grape", by=[",", ";", "-"], verbose=1))
    print(split("apple,orange,banana", by=",", verbose=1))
    print(
        split("1. First. 2. Second. 3. Third. 4. Fourth.",
            by="sent_num",
            n=2,
            strip_results=1,
            verbose=1))
    # Split into words (using NLTK)
    print(split("Can't stop won't stop!", by="word"))
    # Split into sentences
    print(split("First sentence. Second sentence! Third one?", by="sentence"))
    # Split every N sentences
    print(split("1. First. 2. Second. 3. Third. 4. Fourth.", by="sent_num", n=2))
    # Split into characters
    print(split("hello", by="char"))
    # Split by fixed length chunks
    print(split("abcdefghijklmnop", by="len", length=4))
    # Split at uppercase letters
    print(split("CamelCaseString", by="upper"))
    # Split camelCase words
    print(split("camelCaseVariableName", by="camel"))
    # Split at numbers (your requested example)
    print(split("Item1Price20Quantity5", by="digits"))
    # Split at number words
    print(split("Order twenty-five items", by="number_words"))
    # Split by punctuation
    print(split("Hello! How are you? I'm fine.", by="punctuation"))
    # Split by lines
    print(split("Line 1\nLine 2\r\nLine 3", by="lines"))
    # Split at non-ASCII characters
    print(split("EnglishÊó•Êú¨Ë™û–†—É—Å—Å–∫–∏–π", by="non_ascii"))
    # Split at non-alphanumeric characters
    print(split("word1@word2#word3$word4", by="non_alphanum"))
    # Split with regex pattern
    print(split("a1b2c3d4", by="regex", pattern=r"\d"))
    # Keep delimiters
    print(split("a,b;c.d", by=[",", ";", "."], keep_by=True))
    # Case-insensitive splitting
    print(split("Apple ORANGE banana", by="orange", ignore_case=True))
    # Limit number of splits
    print(split("one two three four", by="space", maxsplit=2))
    # Empty string
    print(split("", by="space"))
    # Only delimiters
    print(split(",,,", by=","))
    # Mixed content
    print(split("ID:123-Name:John Doe-Age:30", by=["ID:", "-Name:", "-Age:"]))
    # Process CSV-like string
    print(split("Name;John|Age;30||City;New York", by=[";", "|"]))
    # Extract API endpoint parts
    print(split("getUserByIdAndName", by="camel"))
    """
    VERBOSE = verbose
    if is_nan(text):
        return text
    COMMON_PATTERNS = load_common_patterns()

    if isinstance(text, list):
        nested_list = [ssplit(i, by=by, regex=regex, strict=strict, keep_by=keep_by, strip_results=strip_results, verbose=verbose, **kws) for i in text]
        flat_list = [item for sublist in nested_list for item in sublist]
        return flat_list

    def split_by_word_length(text, length):
        return [word for word in text.split() if len(word) == length]

    def split_by_multiple_delimiters(text, delimiters, keep_by=False):
        if keep_by:
            regex_pattern = "|".join(map(re.escape, delimiters))
        else:
            regex_pattern = "(?:" + "|".join(map(re.escape, delimiters)) + ")"
        return re.split(regex_pattern, text)

    def split_by_camel_case(text):
        return re.findall(r"[A-Z](?:[a-z]+|[A-Z]*(?=[A-Z]|$))", text)

    def split_at_upper_fl_lower(text):
        return re.findall(r"[A-Z](?:[a-z]+|[A-Z]+(?=[A-Z|$))", text)

    def split_at_lower_fl_upper(text):
        split_text = re.split(r"(?<=[a-z])(?=[A-Z])", text)
        return split_text

    def split_at_upper(text):
        split_text = re.split(r"(?=[A-Z])", text)
        split_text = [part for part in split_text if part]
        return split_text

    def split_by_regex_lookahead(text, pattern):
        return re.split(f"(?<={pattern})", text)

    def split_by_regex_end(text, pattern):
        return re.split(f"(?={pattern})", text)

    def split_non_ascii(text, keep_by=False):
        """
        Split text at non-ASCII characters.
        Args:
            text: Input string to split
            keep_by: If True, keeps the non-ASCII delimiters in the output
        Returns:
            List of strings split at non-ASCII characters
        """

        if keep_by:
            # Split and keep non-ASCII delimiters
            parts = re.split(r"([^\x00-\x7F]+)", text)
            # Combine adjacent parts and delimiters
            result = []
            for i in range(0, len(parts)-1, 2):
                if parts[i]:
                    result.append(parts[i])
                if i+1 < len(parts) and parts[i+1]:
                    result.append(parts[i+1])
            if len(parts) % 2 == 1 and parts[-1]:
                result.append(parts[-1])
            return result
        else:
            # Split and discard non-ASCII delimiters
            return [part for part in re.split(r"[^\x00-\x7F]+", text) if part]

    def split_by_consecutive_non_alphanumeric(text, keep_by=False):
        if keep_by:
            return re.split(r"(\W+)", text)
        else:
            return re.split(r"\W+", text)

    def split_by_fixed_length_chunks(text, length):
        return [text[i : i + length] for i in range(0, len(text), length)]

    def split_by_sent_num(text, n=10):
        from nltk.tokenize import sent_tokenize
        from itertools import pairwise

        # split text into sentences
        text_split_by_sent = sent_tokenize(text)
        cut_loc_array = np.arange(0, len(text_split_by_sent), n)
        if cut_loc_array[-1] != len(text_split_by_sent):
            cut_loc = np.append(cut_loc_array, len(text_split_by_sent))
        else:
            cut_loc = cut_loc_array
        # get text in section (e.g., every 10 sentences)
        text_section = []
        for i, j in pairwise(cut_loc):
            text_section.append(" ".join(text_split_by_sent[i:j]))
        return text_section

    def split_general(text, by, verbose=False, ignore_case=False, keep_by=False):
        pattern_str, pattern = None, by
        if isinstance(pattern, (re.Pattern, RegexPattern)) if RegexPattern else isinstance(pattern, re.Pattern):
            print(f"apply Pattern: {pattern}") if VERBOSE else None
            pattern_str = pattern.pattern
        elif contains_regex_special(pattern) and isinstance(pattern, str):
            pattern_str = pattern
            print(f"contains_regex_special: {contains_regex_special(pattern)}") if VERBOSE else None
        else:
            print(f"pattern before strcmp: {pattern}") if VERBOSE else None
            # safer fuzzy matching result extraction
            cmp_list = [i.replace("_", "").replace("-", "") for i in list(COMMON_PATTERNS)]
            res = strcmp(pattern, cmp_list, method="token_sort_ratio")
            if res is None:
                raise ValueError(f"No close pattern match found for '{pattern}'")
            pattern_ = list(COMMON_PATTERNS)[res[1]]
            print(pattern_, "after cmp all pre-config-patterns") if VERBOSE else None
            pattern_str = COMMON_PATTERNS.get(pattern_, pattern)
        
        # Handle keep_by parameter
        if keep_by: 
            if not (pattern_str.startswith("(") and pattern_str.endswith(")")):
                pattern_str = f"({pattern_str})"
        
        print(f"pattern_str: '{pattern_str}'") if VERBOSE else None
        regex = compile_pattern(pattern_str, ignore_case)
        return regex.split(text)

    def reg_split(text, pattern, keep_by=False):
        if keep_by:
            return re.split(pattern, text)
        else:
            # Convert capture groups to non-capturing groups
            non_capturing_pattern = re.sub(r'\((?!\?)', '(?:', pattern)
            return re.split(non_capturing_pattern, text)
    
    def _helper_split(text, by, strict, keep_by,**kws):
        
        # elif ("pun" in by) and not strict:
        #     print(f"splited by Ê†áÁÇπ('.!?;')") if VERBOSE else None
        #     if keep_by:
        #         result = re.split(r"([.!?;])", text)
        #     else:
        #         result = re.split(r"[.!?;]", text) 
        if (("pun" in by) and not strict) or (by in '.,!?;|:()[]{}"\'<>/-_=+*&^%$#@!~`'):
            import string
            print(f"splited by punctuation") if VERBOSE else None
            
            # Combine custom punctuation with Python's string.punctuation
            custom_punct = '.,!?;|:()[]{}"\'<>/-_=+*&^%$#@!~`'
            all_punct = custom_punct + string.punctuation
            # Remove duplicates by converting to set and back to string
            all_punct = ''.join(sorted(set(all_punct)))
            
            # Determine what pattern to use
            if by in all_punct:
                # Use the specific punctuation character provided
                pattern = by
            else:
                # Use all punctuation characters
                pattern = all_punct
            
            if keep_by:
                result = re.split(f"([{re.escape(pattern)}])", text)
            else:
                result = re.split(f"[{re.escape(pattern)}]+", text)

        elif ("sp" in by or "white" in by) and not strict:
            print(f"splited by space") if VERBOSE else None
            result = text.split()
        elif ("word" in by and "len" in by) and not strict:
            print(f"split_by_word_length(text, length)") if VERBOSE else None
            result = split_by_word_length(text, **kws)
        # elif "," in by:
        #     if verbose:
        #         print(f"splited by ','")
        #     return text.split(",") 
        elif (
            all([("digi" in by or "num" in by), not "sent" in by, not "str" in by])
            and not strict
        ):
            if verbose:
                print(f"splited by digital (numbers)")
            result = re.split(r"(\d+)", text)
        elif isinstance(by, list):
            print(f"split_by_multiple_delimiters: ['|','&']") if VERBOSE else None
            result = split_by_multiple_delimiters(text, by, keep_by)
        elif all([("digi" in by or "num" in by), "str" in by]) and not strict:
            print(f"Splitting by (number strings)") if VERBOSE else None
            pattern = re.compile(
                r"\b((?:one|two|three|four|five|six|seven|eight|nine|ten|eleven|twelve|thirteen|fourteen|fifteen|sixteen|seventeen|eighteen|nineteen|twenty|thirty|forty|fifty|sixty|seventy|eighty|ninety|hundred|thousand|million|billion|trillion|and|[\d,]+(?:\.\d+)?)(?:[-\s]?(?:one|two|three|four|five|six|seven|eight|nine|ten|eleven|twelve|thirteen|fourteen|fifteen|sixteen|seventeen|eighteen|nineteen|twenty|thirty|forty|fifty|sixty|seventy|eighty|ninety|hundred|thousand|million|billion|trillion|and|[\d,]+(?:\.\d+)?))*)\b",
                re.IGNORECASE,
            )
            result = re.split(pattern, text)
        elif ("\n" in by or "li" in by) and not strict:
            print(f"splited by lines('\n')") if VERBOSE else None
            result = text.splitlines()
        elif ("cam" in by) and not strict:
            print(f"splited by camel_case") if VERBOSE else None
            result = split_by_camel_case(text)
        elif ("word" in by) and not strict:
            from nltk.tokenize import word_tokenize
            print(f"splited by word") if VERBOSE else None
            result = word_tokenize(text)
        elif ("sen" in by and not "num" in by) and not strict:
            from nltk.tokenize import sent_tokenize
            print(f"splited by sentence") if VERBOSE else None
            result = sent_tokenize(text)
        elif ("sen" in by and "num" in by) and not strict:
            result = split_by_sent_num(text, **kws)
        elif ("cha" in by) and not strict:
            print(f"splited by chracters") if VERBOSE else None
            result = list(text)
        elif ("up" in by or "cap" in by) and ("l" not in by) and not strict:
            print(f"splited by upper case") if VERBOSE else None
            result = split_at_upper(text)
        elif ("u" in by and "l" in by) and not strict:
            if by.find("u") < by.find("l"):
                print(f"splited by upper followed by lower case") if VERBOSE else None
                result = split_at_upper_fl_lower(text)
            else:
                print(f"splited by lower followed by upper case") if VERBOSE else None
                result = split_at_lower_fl_upper(text)
        elif ("start" in by or "head" in by) and not strict:
            print(f"splited by lookahead") if VERBOSE else None
            result = split_by_regex_lookahead(text, **kws)
        elif ("end" in by or "tail" in by) and not strict:
            print(f"splited by endings") if VERBOSE else None
            result = split_by_regex_end(text, **kws)
        elif ("other" in by or "non_alp" in by) and not strict:
            print(f"splited by non_alphanumeric") if VERBOSE else None
            result = split_by_consecutive_non_alphanumeric(text, keep_by)
        elif ("len" in by) and not strict:
            print(f"splited by fixed length") if VERBOSE else None
            result = split_by_fixed_length_chunks(text, **kws)
        elif ("re" in by or "cus" in by or "cos" in by) and not strict:
            print(f"splited by customed, re; => {by}") if VERBOSE else None
            result = reg_split(text, **kws)
        elif any(["lang" in by, "eng" in by,"non_ascii" in by]) and not strict:
            result = split_non_ascii(text, keep_by)
        else:
            print(f"direct split") if VERBOSE else None
            # For direct string splitting, implement keep_by support
            if keep_by:
                # Use regex to split but keep the delimiter
                result = re.split(f"({re.escape(by)})", text)
            else:
                # Default behavior - remove delimiter
                result = text.split(by)

        return result

    # MAIN Split Func
    if is_pattern(by):
        try:
            if regex is None:
                result = split_general(text, by, verbose=verbose, keep_by=keep_by, **kws)
            else:
                if regex:
                    result = split_general(text, by, verbose=verbose, keep_by=keep_by, **kws)
                else:
                    result = _helper_split(text, by, strict, keep_by=keep_by, **kws)
        except Exception as e:
            print(f"Fall back to normal split, {e}") if VERBOSE else None
            result = _helper_split(text, by, strict, keep_by, **kws)
    else:
        result = _helper_split(text, by, strict, keep_by, **kws)

    if strip_results:
        result = [item.strip() for item in result if item.strip()] 
    else:
        result = [item for item in result if item]

    return result

# ! =========== PdfAnnotator below ===========
try:
    
    import io
    import pymupdf
    from pypdf import PdfReader, PdfWriter
    from reportlab.pdfgen import canvas
    from reportlab.lib.pagesizes import letter
    from reportlab.pdfbase import pdfmetrics
    from reportlab.pdfbase.ttfonts import TTFont
    from reportlab.lib.colors import HexColor
    from reportlab.lib.utils import ImageReader

    class PdfAnnotator:
        """
        Powerful PDF annotation tool with robust font fallback handling.
        Features:
        - Automatic fallback to platform-standard fonts when custom fonts fail
        - Full text styling support (bold/italic) even with fallback fonts
        - Comprehensive error handling for font operations
        - support both text extraction and re pattern search
        """

        # Platform-standard fonts that work everywhere
        STANDARD_FONTS = {
            "sans": "Helvetica",
            "serif": "Times-Roman",
            "mono": "Courier",
            "default": "Helvetica",
        }

        # Standard font variants
        STANDARD_VARIANTS = {
            ("Helvetica", False, False): "Helvetica",
            ("Helvetica", True, False): "Helvetica-Bold",
            ("Helvetica", False, True): "Helvetica-Oblique",
            ("Helvetica", True, True): "Helvetica-BoldOblique",
            ("Times-Roman", False, False): "Times-Roman",
            ("Times-Roman", True, False): "Times-Bold",
            ("Times-Roman", False, True): "Times-Italic",
            ("Times-Roman", True, True): "Times-BoldItalic",
            ("Courier", False, False): "Courier",
            ("Courier", True, False): "Courier-Bold",
            ("Courier", False, True): "Courier-Oblique",
            ("Courier", True, True): "Courier-BoldOblique",
        }


        EXAMPLE_STR = """
        fpath = "pdf_ori.pdf"

        annotator = PdfAnnotator(fpath)

        # This will fail gracefully and fall back to Helvetica
        # annotator.register_font("FancyFont", "nonexistent_font.ttf")

        # Text with custom font that will fall back to standard
        annotator.add_text(
            text="Important Notice",
            loc=(50, 700),
            font_name="FancyFont",  # Will fall back
            font_size=24,
            bold=True,
            fallback_style="serif",  # Prefer Times if fallback needed
        )

        # Text that will use standard font with bold/italic
        annotator.add_text(
            text="Confidential",
            loc=(100, 650),
            font_name="Courier",  # Standard font
            font_size=8,
            bold=True,
            italic=True,
            font_color=(221, 5, 49),
        )

        # Add text relative to anchor
        annotator.add_text(
            text="test maus",
            page_index=0,
            loc="Genotype",
            # offset=(100, 20),
            # font_name="DejaVuSans-Bold",
            font_size=14,
            font_color="#94BB75",
            anchor_match_mode="all",
        )

        # Add signature image
        annotator.add_image(
            image_path="/Users/macjianfeng/Dropbox/github/python/py2ls/tests/xample_imgsets/signature_ÂàòÂÅ•Èîã.png",
            page_index=0,
            loc="ToGenotype",
            offset=(150, -30),
            height=18,
            rotation=-5,
            anchor_match_mode=3,
        )

        # Add watermark on all pages
        for page_idx in [1, 4]:
            annotator.add_text(
                text="test watermark",
                page_index=page_idx - 1,
                loc=(70, 400),
                font_name="Helvetica",
                font_size=72,
                font_color="#C8CCD4FF",
                rotation=30,
            )
        annotator.save()
        # annotator.save(dir_save=fpath.replace(".pdf", "_annotated.pdf"))"""

        def __init__(
            self,
            dir_pdf: str = None,
            dir_save: str = None,
            overwrite: bool = False,
            verbose: bool = False,
            raise_error: bool =False
        ):

            if dir_pdf is None:
                print(
                    f"Warning: dir_pdf cannot be None, you can use it like this:\n{self.EXAMPLE_STR}"
                )
                return None
            self.dir_pdf = dir_pdf
            if dir_save is None:
                dir_save = self.dir_pdf if overwrite else self.dir_pdf[:-4] + "_annot.pdf"
            self.annotations = []
            self.registered_fonts = {}
            self.overwrite = overwrite
            self.dir_save = dir_save
            self.raise_error=raise_error
            self._init_standard_fonts()
            self._init_font_system()
            if verbose:
                print(self.EXAMPLE_STR)
                return None

        def _init_standard_fonts(self) -> None:
            """Ensure standard fonts are always available"""
            for font_name in set(self.STANDARD_VARIANTS.values()):
                self.registered_fonts[font_name] = "standard"
        
        def _init_font_system(self):
            """Initialize the font system by registering all available fonts"""
            # Standard PDF fonts that don't need registration
            self.builtin_fonts = ["Courier", "Helvetica", "Times", "Symbol", "ZapfDingbats"]
            
            # Platform-specific font directories
            self.font_dirs = self._get_platform_font_dirs()
            
            # Register all found fonts at initialization
            self._register_all_available_fonts()
            
        def _get_platform_font_dirs(self):
            """Get platform-specific font directories""" 
            return [d for d in get_dir_font() if os.path.exists(d)]
        
        def _register_all_available_fonts(self):
            """Register all available fonts from font directories"""
            self.available_fonts = {}
            self.font_name_mapping = {}
            
            # First register built-in fonts
            for font in self.builtin_fonts:
                self.available_fonts[font.lower()] = {
                    'name': font,
                    'path': None,
                    'type': 'builtin'
                }
            
            # Scan all font directories for supported fonts
            supported_extensions = ['.ttf', '.otf', '.ttc']
            
            for font_dir in self.font_dirs:
                for root, _, files in os.walk(font_dir):
                    for file in files:
                        if any(file.lower().endswith(ext) for ext in supported_extensions):
                            font_path = os.path.join(root, file)
                            try:
                                font_name = os.path.splitext(file)[0]
                                
                                # Register with ReportLab
                                pdfmetrics.registerFont(TTFont(font_name, font_path))
                                
                                # Store in available fonts registry
                                self.available_fonts[font_name.lower()] = {
                                    'name': font_name,
                                    'path': font_path,
                                    'type': 'custom'
                                }
                                
                                # Create mapping without extensions or style suffixes
                                base_name = font_name.split('-')[0].lower()
                                if base_name not in self.font_name_mapping:
                                    self.font_name_mapping[base_name] = []
                                self.font_name_mapping[base_name].append(font_name)
                                
                            except Exception as e:
                                continue
        def register_font(
            self, font_name: str, font_path: str=None, silent_fail: bool = True
        ) -> bool:
            """
            Register a custom font with automatic fallback

            Args:
                font_name: Internal name for the font
                font_path: Path to .ttf or .otf font file
                silent_fail: If True, falls back silently to standard font

            Returns:
                True if registration succeeded, False if fell back to standard
            """

            # Common fallback fonts for different platforms
            FALLBACK_FONTS = [ ]
            try:
                # Try to add user fonts if available
                FALLBACK_FONTS.extend(
                    ls(
                        get_dir_font(),
                        ["ttf", "otf", "ttc"],
                        booster=1,
                        depth=3,
                        sort_by="size",
                        ascending=False,
                    ).path.tolist()
                )
            except:
                pass
            font_name = "Helvetica" if font_name is None else font_name 
            if font_name not in self.builtin_fonts  and not os.path.exists(font_name):
                font_path=FALLBACK_FONTS[strcmp(font_name,[os.path.basename(i).split(".")[0] for i in FALLBACK_FONTS])[1]]
                font_name=strcmp(font_name,[os.path.basename(i).split(".")[0] for i in FALLBACK_FONTS])[0]
            
            if not os.path.exists(font_path):
                if not silent_fail:
                    raise FileNotFoundError(f"Font file not found: {font_path}")
                return False

            try:
                pdfmetrics.registerFont(TTFont(font_name, font_path))
                self.registered_fonts[font_name] = font_path
                print(f"Registered font: {font_name}")
                return True
            except Exception as e:
                if not silent_fail:
                    raise RuntimeError(f"Failed to register font {font_name}: {str(e)}")
                return False

        def _get_fallback_font(self, style: str = "sans") -> str:
            """Get appropriate fallback font based on style preference"""
            return self.STANDARD_FONTS.get(style, self.STANDARD_FONTS["default"])

        def _resolve_font_key(self, font_name: str, bold: bool, italic: bool) -> str:
            """
            Resolve font name with style variants, with automatic fallback
            to standard fonts if the requested font isn't available
            """
            # First check if exact font is registered
            if font_name in self.registered_fonts:
                base_font = font_name
                # print(f"Using registered font: {font_name}")
            else:
                print(f"{font_name} in {self.registered_fonts}")
                # Fallback to standard font family
                if font_name.lower() in ["times", "times-roman", "times new roman"]:
                    base_font = self.STANDARD_FONTS["serif"]
                elif font_name.lower() in ["courier", "monospace"]:
                    base_font = self.STANDARD_FONTS["mono"]
                else:
                    base_font = self.STANDARD_FONTS["sans"]

            # Handle standard font variants
            variant_key = (base_font, bold, italic)
            return self.STANDARD_VARIANTS.get(variant_key, base_font)

        # Ëé∑ÂæóÊåáÂÆöÈ°µÁ†ÅÁöÑsize, in mm
        @staticmethod
        def page_size(self, index: int = 0) -> Tuple[float, float]:
            """Get dimensions (width, height) of a specific page (in points)."""
            try:
                pdf = PdfReader(self.dir_pdf)
                page = pdf.pages[index]
                return self._get_page_size(page)
            except Exception as e:
                if self.raise_error:
                    raise RuntimeError(
                        f"Failed to get page size at index {index}: {str(e)}"
                    )
                else:
                    print(f"[ERROR] Failed to get page size at index {index}: {e}")
                    return (0.0, 0.0)
        @property
        def pages(self) -> int:
            """Return total number of pages in the PDF."""
            try:
                pdf = PdfReader(self.dir_pdf)
                return len(pdf.pages)
            except Exception as e:
                if self.raise_error:
                    raise RuntimeError(f"Failed to load PDF to count pages: {str(e)}")
                else:
                    print(f"[ERROR] Failed to get total pages: {e}")
                    return 0

        def add_text(
            self,
            text: str,
            page_index: int = 0,
            loc: Union[Tuple[float, float], str] = None,
            position: Tuple[float, float] = None,
            anchor_text: str = None,
            offset: Tuple[float, float] = (0, 0),
            font_name: str = "Helvetica",
            font_size: int = 12,
            font_color: str = "#000000",
            bold: bool = False,
            italic: bool = False,
            rotation: int = 0,
            fallback_style: str = "sans",
            anchor_match_mode: Union[str, int] = "last",  # "first","last", "all" or index
        ) -> None:
            """
            Add text annotation with automatic font fallback handling
            Args:
                loc: Unified location specifier:
                    - Tuple (x,y): Absolute position
                    - String: Anchor text for relative positioning
                    - None: (0,0) position
                anchor_match_mode: How to handle multiple anchor text matches:
                    "first" - use first occurrence
                    "last" - use last occurrence (default)
                    "all" - create annotation for each match
                    int - use specific index (0-based, falls back to last if out of range)
                fallback_style: Preferred fallback family ('sans', 'serif', 'mono')
            """

            if font_name not in self.registered_fonts:
                # Common fallback fonts for different platforms
                FALLBACK_FONTS = [ ]
                try:
                    # Try to add user fonts if available
                    FALLBACK_FONTS.extend(
                        ls(
                            get_dir_font(),
                            ["ttf", "otf", "ttc"],
                            booster=1,
                            depth=3,
                            sort_by="size",
                            ascending=False,
                        ).path.tolist()
                    )
                except:
                    pass
                font_name = "Helvetica" if font_name is None else font_name 
                if not os.path.exists(font_name):
                    font_path=FALLBACK_FONTS[strcmp(font_name,[os.path.basename(i).split(".")[0] for i in FALLBACK_FONTS])[1]]
                    font_name=strcmp(font_name,[os.path.basename(i).split(".")[0] for i in FALLBACK_FONTS])[0]
                try:
                    pdfmetrics.registerFont(TTFont(font_name, font_path))
                    self.registered_fonts[font_name] = font_path
                except Exception as e:
                    print(e)
            # Resolve font with fallback
            font_key = self._resolve_font_key(font_name, bold, italic)
            if run_once_within():
                if font_key!=font_name:
                    print(f"Warning: Font {font_name} not available,not in self.registered_fonts {self.registered_fonts}. Falling back to {font_key}")
                else:
                    print(f"Font {font_name} used")
            # Verify font is available (fallback to standard if not)
            if font_key not in self.registered_fonts:
                fallback_base = self._get_fallback_font(fallback_style)
                font_key = self._resolve_font_key(fallback_base, bold, italic)
                print(
                    f"Warning: Font {font_name} not available,not in self.registered_fonts {self.registered_fonts}. Falling back to {font_key}"
                )
            # Convert color to 6-digit hex format
            hex_color = color2hex(font_color, keep_alpha=False)
            if hex_color is None:
                print(f"Warning: Invalid color '{font_color}'. Falling back to black")
                hex_color = "#000000"
            # Determine position/anchor based on loc type
            position = None
            anchor_text = None

            if loc is None:
                position = (0, 0)  # Default position
            elif isinstance(loc, tuple):
                position = loc
            elif isinstance(loc, str):
                anchor_text = loc
            else:
                raise TypeError(
                    "loc must be either tuple (position) or string (anchor text)"
                )
            self.annotations.append(
                {
                    "type": "text",
                    "text": text,
                    "page_index": page_index,
                    "position": position,
                    "anchor_text": anchor_text,
                    "offset": offset,
                    "font_name": font_key,
                    "font_size": font_size,
                    "font_color": hex_color,
                    "rotation": rotation,
                    "anchor_match_mode": anchor_match_mode,
                }
            )

        def add_image(
            self,
            image_path: str,
            page_index: int = 0,
            loc: Union[Tuple[float, float], str] = None,
            position: Tuple[float, float] = None,
            anchor_text: str = None,
            offset: Tuple[float, float] = (150, -30),
            height: float = 18,
            width: float = None,
            rotation: int = 0,
            anchor_match_mode: Union[str, int] = "last",  # "first","last", "all" or index
        ) -> None:
            """
            Add image annotation to PDF

            Args:
                image_path: Path to image file
                page_index: Target page index (0-based)
                position: Absolute (x, y) position (points)
                anchor_text: Reference text for relative positioning
                offset: (dx, dy) offset from anchor position
                width: Desired image width (maintains aspect if height=None)
                height: Desired image height (maintains aspect if width=None)
                rotation: Image rotation in degrees
            """
            if not os.path.exists(image_path):
                raise FileNotFoundError(f"Image file not found: {image_path}")
            # Determine position/anchor based on loc type
            position = None
            anchor_text = None

            if loc is None:
                position = (0, 0)  # Default position
            elif isinstance(loc, tuple):
                position = loc
            elif isinstance(loc, str):
                anchor_text = loc
            else:
                raise TypeError(
                    "loc must be either tuple (position) or string (anchor text)"
                )

            if position is None and anchor_text is None:
                raise ValueError(
                    "Either loc (as position) or loc (as anchor text) must be specified"
                )
            self.annotations.append(
                {
                    "type": "image",
                    "image_path": image_path,
                    "page_index": page_index,
                    "position": position,
                    "anchor_text": anchor_text,
                    "offset": offset,
                    "width": width,
                    "height": height,
                    "rotation": rotation,
                    "anchor_match_mode": anchor_match_mode,
                }
            )

        def save(self, dir_save: str = None) -> None:
            """
            Apply annotations and save to output PDF

            Args:
                dir_save: Path for output PDF file
            """
            # Process pages
            pdf_writer = PdfWriter()
            base_pdf = PdfReader(self.dir_pdf)
            total_pages = len(base_pdf.pages)

            # Group annotations by page
            page_annotations = {}
            for anno in self.annotations:
                idx = anno["page_index"]
                if idx not in page_annotations:
                    page_annotations[idx] = []
                page_annotations[idx].append(anno)

            # Process each page
            for page_idx in range(total_pages):
                page = base_pdf.pages[page_idx]
                page_size = self._get_page_size(page)

                # Create overlay if annotations exist
                if page_idx in page_annotations:
                    overlay = self._create_overlay(
                        page_annotations[page_idx], page_size, page_idx
                    )
                    if overlay:
                        page.merge_page(overlay)
                    else:
                        print(f"[INFO] Page {page_idx}: no valid annotation rendered.")

                pdf_writer.add_page(page)

            self.dir_save = dir_save if dir_save is not None else self.dir_save
            with open(self.dir_save, "wb") as f:
                pdf_writer.write(f)

        def _get_page_size(self, page) -> Tuple[float, float]:
            """Get page dimensions in points"""
            media_box = page.mediabox
            return (float(media_box.width), float(media_box.height))
        def _find_anchor_positions(
            self, anchor_text: str, page_index: int, page_size: Tuple[float, float]
        ) -> List[Tuple[float, float]]:
            """Find all anchor text positions using PyMuPDF, supporting:
            - Exact text matches ("text")
            - Regex patterns (r"pattern" or /pattern/)
            """
            doc = pymupdf.open(self.dir_pdf)
            page = doc.load_page(page_index)
            
            # Check for regex patterns in either format
            is_regex = False
            pattern = anchor_text
            
            # Case 1: /pattern/ syntax
            if (anchor_text.startswith('/') and 
                anchor_text.endswith('/') and 
                len(anchor_text) > 2):
                is_regex = True
                pattern = anchor_text[1:-1]  # Remove the slashes
            
            # Case 2: Raw string syntax (r"pattern")
            elif (anchor_text.startswith('r"') and anchor_text.endswith('"')) or (
                anchor_text.startswith("r'") and anchor_text.endswith("'")):
                is_regex = True
                pattern = anchor_text[2:-1]  # Remove r" and "
            
            try:
                if is_regex:
                    text_instances = page.search_for(r'(%s)' % pattern)
                else:
                    text_instances = page.search_for(anchor_text)
            except re.error as e:
                if self.raise_error:
                    raise ValueError(f"Invalid regex pattern '{pattern}': {str(e)}")
                else:
                    print(f"Invalid regex pattern '{pattern}': {str(e)}")
                    return []
            finally:
                doc.close()

            if not text_instances:
                if self.raise_error:
                    raise ValueError(
                        f"Anchor text '{anchor_text}' not found on page {page_index+1}"
                    )
                else:
                    print(f"Anchor text '{anchor_text}' not found on page {page_index+1}")
                return []

            # Convert positions to ReportLab coordinates
            page_height = page_size[1]
            return [(rect.x0, page_height - rect.y0) for rect in text_instances]

        def _create_overlay(
            self, annotations: List[Dict], page_size: Tuple[float, float], page_index: int
        ) -> PdfReader:
            """Create PDF overlay with support for multiple anchor matches"""
            packet = io.BytesIO()
            can = canvas.Canvas(packet, pagesize=page_size)
            width, height = page_size
            drew_anything = False

            for anno in annotations:
                positions = []

                # Absolute positioning
                if anno.get("position"):
                    positions = [anno["position"]]

                # Anchor-based positioning
                elif anno.get("anchor_text"):
                    try:
                        anchor_positions = self._find_anchor_positions(
                            anno["anchor_text"], page_index, page_size
                        )
                    except Exception as e:
                        print(f"[ERROR] Anchor resolution failed: {e}")
                        continue

                    if not anchor_positions:
                        print(f"[WARN] Skipped page={page_index+1}: No anchor match for '{anno['anchor_text']}'")
                        continue

                    match_mode = anno.get("anchor_match_mode", "last")
                    if match_mode == "first":
                        anchor_positions = [anchor_positions[0]]
                    elif match_mode == "last":
                        anchor_positions = [anchor_positions[-1]]
                    elif match_mode == "all":
                        pass
                    else:
                        try:
                            if isinstance(match_mode, int):
                                anchor_positions = [anchor_positions[match_mode]]
                        except Exception as e:
                            print(f"[WARN] Invalid anchor_match_mode {match_mode}, defaulting to last. {e}")
                            anchor_positions = [anchor_positions[-1]]

                    dx, dy = anno["offset"]
                    positions = [(x + dx, y + dy) for x, y in anchor_positions]

                for x, y in positions:
                    if x < 0 or y < 0 or x > width or y > height:
                        print(f"[WARN] Position out of bounds ({x:.1f}, {y:.1f}) on page {page_index}")
                        continue

                    try:
                        can.saveState()

                        if anno.get("rotation", 0):
                            can.translate(x, y)
                            can.rotate(anno["rotation"])
                            can.translate(-x, -y)

                        if anno["type"] == "text":
                            can.setFont(anno["font_name"], anno["font_size"])
                            can.setFillColor(HexColor(anno["font_color"]))
                            can.drawString(x, y, anno["text"])
                            drew_anything = True

                        elif anno["type"] == "image":
                            img = ImageReader(anno["image_path"])
                            img_width, img_height = img.getSize()
                            target_width = anno["width"] or img_width
                            target_height = anno["height"] or img_height

                            if anno["width"] and not anno["height"]:
                                target_height = img_height * (target_width / img_width)
                            elif anno["height"] and not anno["width"]:
                                target_width = img_width * (target_height / img_height)

                            y_adjusted = y - target_height
                            can.drawImage(
                                img,
                                x,
                                y_adjusted,
                                width=target_width,
                                height=target_height,
                                preserveAspectRatio=True,
                                mask="auto",
                            )
                            drew_anything = True

                        can.restoreState()
                    except Exception as e:
                        print(f"[ERROR] Failed to draw annotation: {e}")

            can.save()
            packet.seek(0)
            reader = PdfReader(packet)

            if not drew_anything or len(reader.pages) == 0:
                if self.raise_error:
                    raise RuntimeError(f"[ERROR] No content written to overlay on page {page_index}. "
                                    f"Check annotation settings, positions, or anchor text.")
                else:
                    print((f"[ERROR] No content written to overlay on page {page_index}. "
                                    f"Check annotation settings, positions, or anchor text."))
                    return None
            return reader.pages[0] 
except Exception as e:
    print(e)
# ! =========== PdfAnnotator above ===========


def pdf2img(dir_pdf, dir_save=None, page=None, kind="png", verbose=True, **kws):
    from pdf2image import convert_from_path, pdfinfo_from_path

    df_dir_img_single_page = pd.DataFrame()
    dir_single_page = []
    if verbose:
        from pprint import pp

        pp(pdfinfo_from_path(dir_pdf))
    if isinstance(page, tuple) and page:
        page = list(page)
    if isinstance(page, int):
        page = [page]
    if page is None:
        page = [pdfinfo_from_path(dir_pdf)["Pages"]]
    if len(page) == 1 and page != [pdfinfo_from_path(dir_pdf)["Pages"]]:
        page = [page[0], page[0]]
    else:
        page = [1, page[0]]
    print(page)
    pages = convert_from_path(dir_pdf, first_page=page[0], last_page=page[-1], **kws)
    if dir_save is None:
        dir_save = mkdir(dirname(dir_pdf), basename(dir_pdf).split(".")[0] + "_img")
    for i, page in enumerate(pages):
        if verbose:
            print(f"processing page: {i+1}")
        if i < 9:
            dir_img_each_page = dir_save + f"page_0{i+1}.png"
        else:
            dir_img_each_page = dir_save + f"page_{i+1}.png"
        dir_single_page.append(dir_img_each_page)
        page.save(dir_img_each_page, kind.upper())
    df_dir_img_single_page["fpath"] = dir_single_page
    return df_dir_img_single_page


# dir_pdf = "/Users/macjianfeng/Dropbox/github/python/240308_Python BASE Handbook.pdf"
# df_page = pdf2img(dir_pdf, page=[1, 5],dpi=300)
def get_encoding(fpath, alternative_encodings=None, verbose=False):
    """
    Attempt to determine the encoding of a file by trying multiple encodings.

    Parameters:
    fpath (str): The path to the file.
    alternative_encodings (list): List of encodings to try. If None, uses a default list.
    verbose (bool): If True, print detailed information about each attempted encoding.

    Returns:
    str: The encoding that successfully read the file, or None if no encoding worked.
    """
    if alternative_encodings is None:
        alternative_encodings = [
            "utf-8",
            "latin1",
            "windows-1252",
            "iso-8859-1",
            "iso-8859-2",
            "iso-8859-3",
            "iso-8859-4",
            "iso-8859-5",
            "iso-8859-6",
            "iso-8859-7",
            "iso-8859-8",
            "iso-8859-9",
            "windows-1250",
            "windows-1251",
            "windows-1253",
            "windows-1254",
            "windows-1255",
            "windows-1256",
            "windows-1257",
            "windows-1258",
            "big5",
            "gb18030",
            "shift_jis",
            "euc_jp",
            "koi8_r",
            "mac_roman",
            "mac_central_europe",
            "mac_greek",
            "mac_cyrillic",
            "mac_arabic",
            "mac_hebrew",
        ]

    if not os.path.isfile(fpath):
        raise FileNotFoundError(f"The file {fpath} does not exist.")

    for enc in alternative_encodings:
        try:
            with open(fpath, mode="r", encoding=enc) as file:
                file.read()  # Try to read the file
            if verbose:
                print(f"Successfully detected encoding: {enc}")
            return enc
        except UnicodeDecodeError:
            if verbose:
                print(f"Failed to decode with encoding: {enc}")
            continue

    # If no encoding worked
    print("No suitable encoding found.")
    return None


def unzip(dir_path, output_dir=None):
    """
    Unzips or extracts various compressed file formats (.gz, .zip, .7z, .tar, .bz2, .xz, .rar).
    If the output directory already exists, it will be replaced.

    # Example usage:
    output_dir = unzip('data.tar.gz')
    output_file = unzip('file.csv.gz')
    output_dir_zip = unzip('archive.zip')
    output_dir_7z = unzip('archive.7z')

    Parameters:
    dir_path (str): Path to the compressed file.
    output_dir (str): Directory where the extracted files will be saved.
                      If None, it extracts to the same directory as the file, with the same name.

    Returns:
    str: The path to the output directory where files are extracted.
    """

    # Set default output directory to the same as the input file
    if output_dir is None:
        output_dir = os.path.splitext(dir_path)[0]

    # If the output directory already exists, remove it and replace it
    if os.path.exists(output_dir):
        if os.path.isdir(output_dir):  # check if it is a folder
            

            shutil.rmtree(output_dir)  # remove folder
        else:
            os.remove(output_dir)  # remove file

    # Handle .tar.gz files
    if dir_path.endswith(".tar.gz") or dir_path.endswith(".tgz"):
        import tarfile

        with tarfile.open(dir_path, "r:gz") as tar_ref:
            tar_ref.extractall(output_dir)
        return output_dir
    # Handle .gz files
    if dir_path.endswith(".gz") or dir_path.endswith(".gzip"):
        import gzip

        output_file = os.path.splitext(dir_path)[0]  # remove the .gz extension
        try:
            

            with gzip.open(dir_path, "rb") as gz_file:
                with open(output_file, "wb") as out_file:
                    shutil.copyfileobj(gz_file, out_file)
            print(f"unzipped '{dir_path}' to '{output_file}'")
        except FileNotFoundError:
            print(f"Error: The file '{dir_path}' was not found.")
        except PermissionError:
            print(
                f"Error: Permission denied when accessing '{dir_path}' or writing to '{output_file}'."
            )
        except Exception as e:
            try:
                import tarfile

                with tarfile.open(dir_path, "r:gz") as tar:
                    tar.extractall(path=output_file)
            except Exception as final_e:
                print(f"An final unexpected error occurred: {final_e}")
        return output_file

    # Handle .zip files
    elif dir_path.endswith(".zip"):
        import zipfile

        with zipfile.ZipFile(dir_path, "r") as zip_ref:
            zip_ref.extractall(output_dir)
        return output_dir

    # Handle .7z files (requires py7zr)
    elif dir_path.endswith(".7z"):
        import py7zr

        with py7zr.SevenZipFile(dir_path, mode="r") as z:
            z.extractall(path=output_dir)
        return output_dir

    # Handle .tar files
    elif dir_path.endswith(".tar"):
        import tarfile

        with tarfile.open(dir_path, "r") as tar_ref:
            tar_ref.extractall(output_dir)
        return output_dir

    # Handle .tar.bz2 files
    elif dir_path.endswith(".tar.bz2"):
        import tarfile

        with tarfile.open(dir_path, "r:bz2") as tar_ref:
            tar_ref.extractall(output_dir)
        return output_dir

    # Handle .bz2 files
    elif dir_path.endswith(".bz2"):
        import bz2

        output_file = os.path.splitext(dir_path)[0]  # remove the .bz2 extension
        with bz2.open(dir_path, "rb") as bz_file:
            with open(output_file, "wb") as out_file:
                shutil.copyfileobj(bz_file, out_file)
        return output_file

    # Handle .xz files
    elif dir_path.endswith(".xz"):
        import lzma

        output_file = os.path.splitext(dir_path)[0]  # remove the .xz extension
        with lzma.open(dir_path, "rb") as xz_file:
            with open(output_file, "wb") as out_file:
                shutil.copyfileobj(xz_file, out_file)
        return output_file

    # Handle .rar files (requires rarfile)
    elif dir_path.endswith(".rar"):
        import rarfile

        with rarfile.RarFile(dir_path) as rar_ref:
            rar_ref.extractall(output_dir)
        return output_dir

    else:
        raise ValueError(f"Unsupported file format: {os.path.splitext(dir_path)[1]}")

def is_df_abnormal(df: pd.DataFrame,thr=4, verbose=False) -> bool:
    """
    Usage
    is_abnormal = is_df_abnormal(df, verbose=1)
    True: abnormal
    False: normal
    """
    if not isinstance(df, pd.DataFrame):
        if verbose:
            print("not pd.DataFrame")
        return False
    df.columns = df.columns.astype(str)  # ÊääÂÆÉÂèòÊàêstr, ËøôÊ†∑Â∞±ÂèØ‰ª•ËøõË°åcountsËøêÁÆó‰∫Ü
    # Initialize a list to hold messages about abnormalities
    messages = []
    is_abnormal = False
    # Check the shape of the DataFrame
    actual_shape = df.shape
    messages.append(f"Shape of DataFrame: {actual_shape}")

    # Check column names
    column_names = df.columns.tolist()

    # Count of delimiters and their occurrences
    delimiter_counts = {"\t": 0, ",": 0, "\n": 0, "": 0}  # Count of empty strings

    for name in column_names:
        # Count occurrences of each delimiter
        delimiter_counts["\t"] += name.count("\t")
        delimiter_counts[","] += name.count(",")
        delimiter_counts["\n"] += name.count("\n")
        if name.strip() == "":
            delimiter_counts[""] += 1

    # Check for abnormalities based on delimiter counts
    if len(column_names) == 1 and delimiter_counts["\t"] > 1:
        messages.append("Abnormal: Column names are not split correctly.")
        is_abnormal = True
        if verbose:
            print(f'len(column_names) == 1 and delimiter_counts["\t"] > 1')
    if verbose:
        print("1", is_abnormal)
    if any(delimiter_counts[d] > thr for d in delimiter_counts if d != ""):
        messages.append("Abnormal: Too many delimiters in column names.")
        is_abnormal = True
        if verbose:
            print(f'any(delimiter_counts[d] > {thr} for d in delimiter_counts if d != "")')
    if verbose:
        print("2", is_abnormal)
    if delimiter_counts[""] > thr:
        messages.append("Abnormal: There are empty column names.")
        is_abnormal = True
        if verbose:
            print(f'delimiter_counts[""] > {thr}')
    if verbose:
        print("3", is_abnormal)
    if any(delimiter_counts[d] > thr for d in ["\t", ",", "\n"]):
        messages.append("Abnormal: Some column names contain unexpected characters.")
        is_abnormal = True
        if verbose:
            print(f'any(delimiter_counts[d] > 3 for d in ["\t", ",", "\n"])')
    if verbose:
        print("4", is_abnormal)
    # # Check for missing values
    # missing_values = df.isnull().sum()
    # if missing_values.any():
    #     messages.append("Missing values in columns:")
    #     messages.append(missing_values[missing_values > 0].to_string())
    #     is_abnormal = True
    #     print(f'missing_values.any()')

    # Check data types
    data_types = df.dtypes
    # messages.append(f"Data types of columns:\n{data_types}")

    # Check for an unreasonable number of rows or columns
    if actual_shape[0] < 2 or actual_shape[1] < 2:
        messages.append(
            "Abnormal: DataFrame is too small (less than 2 rows or columns)."
        )
        is_abnormal = True
        if verbose:
            print(f"actual_shape[0] < 2 or actual_shape[1] < 2")
    if verbose:
        print("6", is_abnormal)
    # Compile results
    if verbose:
        print("\n".join(messages))
    return is_abnormal  # Data is abnormal

def decrypt_excel(fpath, password):
    # * needs a password?
    import msoffcrypto  # pip install msoffcrypto-tool
    from io import BytesIO

    # Open the encrypted Excel file
    with open(fpath, "rb") as f:
        try:
            office_file = msoffcrypto.OfficeFile(f)
            office_file.load_key(password=password)  # Provide the password
            decrypted = BytesIO()
            office_file.decrypt(decrypted)
        except:
            office_file = msoffcrypto.OfficeFile(f)
            office_file.load_key(password=depass(password))  # Provide the password
            decrypted = BytesIO()
            office_file.decrypt(decrypted)
    decrypted.seek(0) # reset pointer to start
    return decrypted

# ! for Excel formating   
def _backup_validations(sheet, verbose=False):
    """
    Comprehensive validation backup with multiple verification layers
    """
    from openpyxl.utils import range_to_tuple, get_column_letter
    from openpyxl.worksheet.datavalidation import DataValidation
    
    backup = {
        "validations": [],
        "conditional_formatting": [],
        "merged_cells": [str(mr) for mr in sheet.merged_cells.ranges],
        "_metadata": {
            "validated_cells": set(),
            "validated_columns": set(),
            "validation_types": set()
        }
    }

    # METHOD 1: Official data_validations collection (primary source)
    for dv in sheet.data_validations.dataValidation:
        validation_data = {
            "type": dv.type,
            "formula1": dv.formula1,
            "formula2": dv.formula2,
            "allow_blank": dv.allow_blank,
            "showDropDown": dv.showDropDown,
            "showInputMessage": getattr(dv, 'showInputMessage', True),
            "showErrorMessage": getattr(dv, 'showErrorMessage', False),
            "errorTitle": dv.errorTitle,
            "error": dv.error,
            "promptTitle": dv.promptTitle,
            "prompt": dv.prompt,
            "ranges": [],
            "_source": "data_validations"
        }
        
        # Process ranges
        for rng in dv.cells.ranges:
            range_str = str(rng)
            print(range_str)
            try:
                coord = range_to_tuple(range_str)
                min_col, min_row, max_col, max_row = coord
                is_single = (min_col == max_col and min_row == max_row)
                
                # Track coverage
                for col in range(min_col, max_col + 1):
                    col_letter = get_column_letter(col)
                    backup["_metadata"]["validated_columns"].add(col_letter)
                    for row in range(min_row, max_row + 1):
                        backup["_metadata"]["validated_cells"].add(f"{col_letter}{row}")
                
                validation_data["ranges"].append({
                    "str": range_str,
                    "coord": coord,
                    "is_single": is_single
                })
            except Exception as e:
                validation_data["ranges"].append({"str": range_str, "error": str(e)})
        
        backup["validations"].append(validation_data)
        backup["_metadata"]["validation_types"].add(dv.type)

    # METHOD 2: Cell-by-cell verification (fallback)
    missing_validations = []
    for row in sheet.iter_rows():
        for cell in row:
            # Version-agnostic cell validation check
            cell_has_validation = False
            try:
                # OpenPyXL 3.0+ style
                if hasattr(cell, 'data_validation') and cell.data_validation:
                    cell_has_validation = True
                # Older versions style
                elif hasattr(cell, 'has_data_validation') and cell.has_data_validation:
                    cell_has_validation = True
            except Exception:
                continue
                
            if cell_has_validation:
                cell_ref = f"{get_column_letter(cell.column)}{cell.row}"
                if cell_ref not in backup["_metadata"]["validated_cells"]:
                    missing_validations.append(cell_ref)

    # METHOD 3: Handle any missing validations
    if missing_validations:
        if verbose:
            print(f"Found {len(missing_validations)} validations not in data_validations collection")
        
        # Group by column for more efficient processing
        from collections import defaultdict
        column_groups = defaultdict(list)
        for ref in missing_validations:
            col = ''.join(filter(str.isalpha, ref))
            column_groups[col].append(ref)
        
        # Create supplemental validations
        for col, cell_refs in column_groups.items():
            try:
                sample_cell = sheet[cell_refs[0]]
                dv = sample_cell.data_validation if hasattr(sample_cell, 'data_validation') else None
                
                if dv:
                    validation_data = {
                        "type": dv.type,
                        "formula1": getattr(dv, 'formula1', None),
                        "formula2": getattr(dv, 'formula2', None),
                        "allow_blank": getattr(dv, 'allow_blank', False),
                        "showDropDown": getattr(dv, 'showDropDown', False),
                        "showInputMessage": getattr(dv, 'showInputMessage', True),
                        "showErrorMessage": getattr(dv, 'showErrorMessage', False),
                        "ranges": cell_refs,
                        "_source": "cell_validation_fallback",
                        "_recovered": True
                    }
                    backup["validations"].append(validation_data)
                    backup["_metadata"]["validation_types"].add(dv.type)
                    
                    # Update coverage tracking
                    backup["_metadata"]["validated_columns"].add(col)
                    backup["_metadata"]["validated_cells"].update(cell_refs)
            except Exception as e:
                if verbose:
                    print(f"Failed to backup validation for {col}: {str(e)}")
 
    # METHOD 4: Comprehensive cross-sheet dropdown detection
    for row in sheet.iter_rows():
        for cell in row:
            try:
                # Skip if no data validation or not a list type
                if not hasattr(cell, 'data_validation') or not cell.data_validation:
                    continue
                    
                dv = cell.data_validation
                if dv.type != 'list':
                    continue
                    
                formula = dv.formula1
                if not formula:
                    continue

                # Standard cleaning and pattern detection
                clean_formula = formula.strip('"\'').lstrip('=')
                cell_ref = f"{get_column_letter(cell.column)}{cell.row}"
                
                # Detection patterns (ordered by priority)
                patterns = [
                    (r'^[\w\s]+!\$?[A-Za-z]+\$?\d+(?::\$?[A-Za-z]+\$?\d+)?$', "direct sheet reference"),
                    (r'INDIRECT\(["\'][\w\s]+![A-Za-z]+\d+(?::[A-Za-z]+\d+)?["\']\)', "INDIRECT sheet reference"),
                    (r'^[\w\s]+$', "potential named range"),
                ]

                detected_type = None
                for pattern, description in patterns:
                    if re.match(pattern, clean_formula, re.IGNORECASE):
                        detected_type = description
                        break

                if detected_type:
                    # Special handling for direct sheet references
                    if detected_type == "direct sheet reference":
                        # Extract sheet name for verification
                        sheet_name = clean_formula.split('!')[0]
                        # Verify the sheet exists in the workbook
                        if sheet_name not in sheet.parent.sheetnames:
                            detected_type = f"broken reference (sheet '{sheet_name}' not found)"
                    
                    validation_data = {
                        "type": "list",
                        "formula1": formula,
                        "formula2": getattr(dv, 'formula2', None),
                        "allow_blank": getattr(dv, 'allow_blank', True),
                        "showDropDown": not getattr(dv, 'showDropDown', False),  # Correct dropdown display
                        "showInputMessage": getattr(dv, 'showInputMessage', True),
                        "showErrorMessage": getattr(dv, 'showErrorMessage', False),
                        "errorTitle": getattr(dv, 'errorTitle', ""),
                        "error": getattr(dv, 'error', ""),
                        "promptTitle": getattr(dv, 'promptTitle', ""),
                        "prompt": getattr(dv, 'prompt', ""),
                        "ranges": [cell_ref],
                        "_source": "cross_sheet_detection",
                        "_detection_method": detected_type,
                        "_is_cross_sheet": True,
                        "_formula_clean": clean_formula,
                        "_sheet_name": clean_formula.split('!')[0] if '!' in clean_formula else None
                    }

                    # Check for duplicates before adding
                    is_duplicate = any(
                        v.get("_source") == "cross_sheet_detection" and
                        cell_ref in v.get("ranges", []) and
                        v.get("formula1") == formula
                        for v in backup["validations"]
                    )
                    
                    if not is_duplicate:
                        backup["validations"].append(validation_data)
                        backup["_metadata"]["validated_cells"].add(cell_ref)
                        backup["_metadata"]["validated_columns"].add(get_column_letter(cell.column))
                        backup["_metadata"]["validation_types"].add(dv.type)
                        
            except Exception as e:
                if verbose:
                    print(f"Error processing cell {cell.coordinate}: {str(e)}")

    return backup

def _restore_validations(sheet, backup,verbose=False):
    """
    ÊÅ¢Â§çÊï∞ÊçÆÈ™åËØÅÂíåÊù°‰ª∂Ê†ºÂºèËßÑÂàôÂà∞Â∑•‰ΩúË°®
    
    Args:
        sheet: openpyxlÁöÑÂ∑•‰ΩúË°®ÂØπË±°
        backup: ‰ªé_backup_validations()Ëé∑ÂèñÁöÑÂ§á‰ªΩÂ≠óÂÖ∏
    """
    from openpyxl.worksheet.datavalidation import DataValidation, DataValidationList
    from openpyxl.formatting.rule import Rule, ColorScaleRule, DataBarRule, IconSetRule
    from openpyxl.utils import get_column_letter

    # 1. Ê∏ÖÈô§Áé∞ÊúâÈ™åËØÅÂíåÊ†ºÂºèÔºàÊõ¥ÂèØÈù†ÁöÑÊñπÊ≥ïÔºâ
    sheet.data_validations = DataValidationList()
    
    # Ê∏ÖÈô§Êù°‰ª∂Ê†ºÂºèÁöÑÊõ¥Â•ΩÊñπÊ≥ï
    if hasattr(sheet.conditional_formatting, '_rules'):
        sheet.conditional_formatting._rules.clear()
    elif hasattr(sheet.conditional_formatting, 'cf_rules'):
        sheet.conditional_formatting.cf_rules.clear()
    else:
        # ÊúÄÂΩªÂ∫ïÁöÑÊ∏ÖÈô§ÊñπÊ≥ï 
        cf_ranges = list(sheet.conditional_formatting)
        for cf_range in cf_ranges:
            try:
                del sheet.conditional_formatting[cf_range]
            except TypeError:
                # Skip problematic ranges that can't be deleted
                continue

    # 2. ÂÖàÊÅ¢Â§çÂêàÂπ∂ÂçïÂÖÉÊ†º
    for mr in backup.get("merged_cells", []):
        try:
            if mr and mr not in sheet.merged_cells:
                sheet.merge_cells(mr)
        except Exception as e:
            if "already merged" not in str(e):
                print(f"[ÂêàÂπ∂ÂçïÂÖÉÊ†º] Ë≠¶Âëä: {mr} - {str(e)}")

    # 3. ÊÅ¢Â§çÊï∞ÊçÆÈ™åËØÅËßÑÂàôÔºà‰øÆÂ§ç‰∫ÜÂÖ≥ÈîÆÂèÇÊï∞Ôºâ
    for i, val in enumerate(backup.get("validations", [])):
        try:
            dv = DataValidation(
                type=val["type"],
                formula1=val["formula1"],
                formula2=val.get("formula2"),
                allow_blank=val["allow_blank"],
                showDropDown=False,  # Âº∫Âà∂ÂºÄÂêØ, # Dropdown control (False = show dropdown)
                showInputMessage=val.get("showInputMessage", True),  
                showErrorMessage=val.get("showErrorMessage", False), 
                errorTitle=val.get("errorTitle"),
                error=val.get("error"),
                promptTitle=val.get("promptTitle"),
                prompt=val.get("prompt")
            )
            for rng in val["ranges"]:
                try:
                    # Handle both string ranges and coordinate dicts from backup
                    if isinstance(rng, dict):
                        # Restore from coordinate backup if available
                        if "coord" in rng:
                            min_col, min_row, max_col, max_row = rng["coord"]
                            if min_col == max_col and min_row == max_row:
                                dv.add(f"{get_column_letter(min_col)}{min_row}")
                            else:
                                dv.add(f"{get_column_letter(min_col)}{min_row}:"
                                      f"{get_column_letter(max_col)}{max_row}")
                        else:
                            dv.add(rng["str"])
                    else:
                        # Direct string range
                        dv.add(rng)
                except Exception as e:
                    print(f"Warning: Could not add range {rng} to validation: {str(e)}")
            sheet.add_data_validation(dv)
        except Exception as e:
            if verbose:
                print(f"Warning: Could not restore validation: {str(e)}")
 
    # 4. ÊÅ¢Â§çÊù°‰ª∂Ê†ºÂºèËßÑÂàôÔºàÂÆåÂÖ®ÈáçÂÜôÁöÑÈÄªËæëÔºâ
    for i, rule_data in enumerate(backup.get("conditional_formatting", [])):
        try:
            rule = None
            rule_type = rule_data.get("type", "")
            
            # Ë∞ÉËØï‰ø°ÊÅØ
            debug_info = f"[Êù°‰ª∂Ê†ºÂºè#{i}] Á±ªÂûã: {rule_type}"
            if "colorScale" in rule_data:
                debug_info += " (È¢úËâ≤ÊØî‰æã)"
            elif "dataBar" in rule_data:
                debug_info += " (Êï∞ÊçÆÊù°)"
            elif "iconSet" in rule_data:
                debug_info += " (ÂõæÊ†áÈõÜ)"
            print(debug_info)

            # ÂàõÂª∫ÂØπÂ∫îÁ±ªÂûãÁöÑËßÑÂàô
            if rule_data.get("colorScale"):
                cs = rule_data["colorScale"]
                rule = ColorScaleRule(
                    start_type=cs.get("start_type", "min"),
                    start_value=cs.get("start_value"),
                    start_color=cs.get("start_color"),
                    mid_type=cs.get("mid_type", "percentile"),
                    mid_value=cs.get("mid_value", 50),
                    mid_color=cs.get("mid_color"),
                    end_type=cs.get("end_type", "max"),
                    end_value=cs.get("end_value"),
                    end_color=cs.get("end_color")
                )
            elif rule_data.get("dataBar"):
                db = rule_data["dataBar"]
                rule = DataBarRule(
                    start_type=db.get("start_type", "min"),
                    start_value=db.get("start_value"),
                    end_type=db.get("end_type", "max"),
                    end_value=db.get("end_value"),
                    color=db.get("color", "FF638EC6"),
                    showValue=db.get("showValue", True)
                )
            elif rule_data.get("iconSet"):
                icon = rule_data["iconSet"]
                rule = IconSetRule(
                    iconSet=icon.get("iconSet", "3TrafficLights"),
                    showValue=icon.get("showValue", True),
                    values=icon.get("values", None),
                    type=icon.get("type", None),
                    reverse=icon.get("reverse", False)
                )
            else:
                # Ê†áÂáÜËßÑÂàô
                rule = Rule(
                    type=rule_type,
                    dxf=rule_data.get("dxf"),
                    stopIfTrue=rule_data.get("stopIfTrue", False)
                )
                # Âä®ÊÄÅËÆæÁΩÆÂ±ûÊÄß
                for attr in ['formula', 'formula1', 'formula2', 'operator', 'text']:
                    if attr in rule_data and rule_data[attr] is not None:
                        try:
                            setattr(rule, attr, rule_data[attr])
                        except AttributeError:
                            if verbose:
                                print(f"[Êù°‰ª∂Ê†ºÂºè#{i}] Ë≠¶Âëä: Êó†Ê≥ïËÆæÁΩÆÂ±ûÊÄß {attr}")

            if rule:
                # Â∫îÁî®ËßÑÂàôÂà∞ÊâÄÊúâËåÉÂõ¥
                range_count = 0
                for rng in rule_data.get("ranges", []):
                    try:
                        if not isinstance(rng, str):
                            rng = str(rng)
                        if rng:  # ÈùûÁ©∫Â≠óÁ¨¶‰∏≤Ê£ÄÊü•
                            sheet.conditional_formatting.add(rng, rule)
                            range_count += 1
                    except Exception as e:
                        if verbose:
                            print(f"[Êù°‰ª∂Ê†ºÂºè#{i}] ËåÉÂõ¥ÈîôËØØ: {rng} - {str(e)}")
                
                if range_count == 0:
                    if verbose:
                        print(f"[Êù°‰ª∂Ê†ºÂºè#{i}] Ë≠¶Âëä: Ê≤°ÊúâÊúâÊïàËåÉÂõ¥ÔºåËßÑÂàôÊú™Â∫îÁî®")
            else:
                if verbose:     
                    print(f"[Êù°‰ª∂Ê†ºÂºè#{i}] ÈîôËØØ: Êó†Ê≥ïÂàõÂª∫ËßÑÂàôÂØπË±°")
                
        except Exception as e:
            if verbose:
                print(f"[Êù°‰ª∂Ê†ºÂºè#{i}] ‰∏•ÈáçÈîôËØØ: {str(e)}")

def load_excel(fpath, **kwargs):
    engine = kwargs.get("engine", "openpyxl")
    verbose = kwargs.pop("verbose", False)
    password = kwargs.pop("password", None)
    output = kwargs.pop("output", "DataFrame").lower()
    sheet_name = kwargs.pop("sheet_name", None)
    def print_sheet_info(fpath):
        try:
            meta = pd.ExcelFile(fpath)
            print(f"n_sheet={len(meta.sheet_names)},\t'sheetname = 0 (default)':")
            [print(f"{i}:\t{name}") for i, name in enumerate(meta.sheet_names)]
        except Exception as e:
            if verbose:
                print(f"Error retrieving sheet info: {e}")



    if output in ["dataframe", "df"]:
        if verbose:
            print("loading data as a DataFrame")
        if not bool(password):
            if verbose:
                print("Reading Excel without password protection...")
            df = pd.read_excel(fpath, engine=engine, sheet_name=sheet_name, **kwargs)
            if verbose:
                print_sheet_info(fpath)
            return df
        # Handle password-protected DataFrame case
        else:
            if verbose:
                print("Decrypting and loading DataFrame...")
            try:
                decrypted = decrypt_excel(fpath, password=password)
                df = pd.read_excel(decrypted, engine=engine,sheet_name=sheet_name, **kwargs)
            except:
                df = pd.read_excel(fpath, engine=engine,sheet_name=sheet_name, **kwargs)
            if verbose:
                print_sheet_info(fpath)
            return df
    # Handle cases for non-dataframe output
    else:
        if verbose:
            print("loading data as a formatted workbook")
        from openpyxl import load_workbook
        try:
            if verbose:
                print("Returning worksheet (non-DataFrame output)...")
            if password:
                try:
                    decrypted = decrypt_excel(fpath, password=password)
                    workbook = load_workbook(decrypted, data_only=False)
                except:
                    workbook = load_workbook(fpath, data_only=False)
                
            else:
                workbook = load_workbook(fpath, data_only=False) 

            # Handle sheet selection
            if sheet_name:
                if sheet_name not in workbook.sheetnames:
                    raise ValueError(f"Sheet '{sheet_name}' not found. Available sheets: {workbook.sheetnames}")
                    
                # # Remove other sheets while preserving case sensitivity
                # for sheet in [s for s in workbook.sheetnames if s != sheet_name]:
                    # del workbook[sheet]
            return workbook

        except Exception as e:
            raise Exception(f"Error loading Workbook: {str(e)}")
# ! fload master
@decorators.Timer()
def fload(fpath, kind=None, **kwargs):
    """
    Load content from a file with specified file type.
    Parameters:
        fpath (str): The file path from which content will be loaded.
        kind (str): The file type to load. Supported options: 'docx', 'txt', 'md', 'html', 'json', 'yaml', 'xml', 'csv', 'xlsx', 'pdf'.
        **kwargs: Additional parameters for 'csv' and 'xlsx' file types.
    Returns:
        content: The content loaded from the file.
    """

    def load_mplstyle(style_file):
        """
how to adjust a mplstyle: 

    import matplotlib.pyplot as plt 
    plt.style.use("paper")
    from py2ls.ips import * 

    paper_style = fload("styles/stylelib/paper.mplstyle")

    # these changes
    paper_style["axes.spines.right"] = True
    paper_style["axes.spines.top"] = True

    fsave("paper2.mplstyle",paper_style)
        """
        import matplotlib.pyplot as plt
        
        # Load the style file
        plt.style.use(style_file)
        
        # Get only simple, serializable rcParams
        simple_rcparams = {}
        
        # List of rcParam categories that are generally safe
        safe_categories = {
            'axes.', 'backend.', 'boxplot.', 'contour.', 'date.',
            'errorbar.', 'figure.', 'font.', 'grid.', 'hatch.',
            'hist.', 'image.', 'legend.', 'lines.', 'markers.',
            'mathtext.', 'patch.', 'path.', 'pdf.', 'pgf.',
            'ps.', 'savefig.', 'scatter.', 'svg.', 'text.',
            'time.', 'tk.', 'toolbar.', 'webagg.', 'xtick.', 'ytick.', 'axes3d.'
        }
        
        for key, value in plt.rcParams.items():
            # Skip complex objects and only keep simple types
            if isinstance(value, (str, int, float, bool)):
                simple_rcparams[key] = value
            # Also handle simple lists (like color cycles)
            elif isinstance(value, list) and all(isinstance(item, (str, int, float, bool)) for item in value):
                simple_rcparams[key] = value
        
        return simple_rcparams
    def load_txt_md(fpath):
        with open(fpath, "r") as file:
            content = file.read()
        return content
    def load_html(fpath, **kwargs):
        return pd.read_html(fpath, **kwargs)

    def load_json(fpath, **kwargs):
        """
        Load a JSON file into a Python object or pandas DataFrame.
        Uses orjson if available, otherwise falls back to json.
        
        Parameters
        ----------
        fpath : str
            Path to the JSON file.
        output : str, default="json"
            If "json", returns a Python object.
            If "df", returns a pandas DataFrame using pd.read_json().
        **kwargs : 
            Additional arguments passed to pd.read_json() if output != "json".
        """
        output = kwargs.pop("output", "json")

        if output == "json":
            try:
                import orjson as _json
                with open(fpath, "rb") as f:
                    content = _json.loads(f.read())
            except ImportError:
                import json as _json
                with open(fpath, "r") as f:
                    content = _json.load(f)
            return content
        else:
            return pd.read_json(fpath, **kwargs)

    def load_yaml(fpath):
        import yaml

        with open(fpath, "r") as file:
            content = yaml.safe_load(file)
        return content

    def load_xml(fpath, fsize_thr: int = 100):
        from lxml import etree

        def load_small_xml(fpath):
            tree = etree.parse(fpath)
            root = tree.getroot()
            return etree.tostring(root, pretty_print=True).decode()

        def load_large_xml(fpath):
            xml_parts = []
            context = etree.iterparse(
                fpath, events=("start", "end"), recover=True, huge_tree=True
            )

            for event, elem in context:
                if event == "end":
                    xml_parts.append(etree.tostring(elem, pretty_print=True).decode())
                    elem.clear()
                    while elem.getprevious() is not None:
                        del elem.getparent()[0]
            del context
            return "".join(xml_parts)

        file_size = os.path.getsize(fpath) / 1024 / 1024  # in MB

        if file_size > fsize_thr:
            print(f"reading a small file:{file_size} Mb")
            return load_large_xml(fpath)
        else:
            print(f"reading a big file:{file_size} Mb")
            return load_small_xml(fpath)

    def get_comment(fpath, comment=None, encoding="utf-8", lines_to_check=5):
        """
        Detect comment characters in a file.

        Parameters:
        - fpath: str, the file path of the CSV file.
        - encoding: str, the encoding of the file (default is 'utf-8').
        - lines_to_check: int, number of lines to check for comment characters (default is 5).

        Returns:
        - str or None: the detected comment character, or None if no comment character is found.
        """
        comment_chars = [
            "#",
            "!",
            "//",
            ";",
        ]  # can use any character or string as a comment
        try:
            with open(fpath, "r", encoding=encoding) as f:
                lines = [next(f) for _ in range(lines_to_check)]
        except (UnicodeDecodeError, ValueError):
            with open(fpath, "r", encoding=get_encoding(fpath)) as f:
                lines = [next(f) for _ in range(lines_to_check)]
        for line in lines:
            for char in comment_chars:
                if line.startswith(char):
                    return char
        return None

    def _get_chunks(df_fake):
        """
        helper func for 'load_csv'
        """
        chunks = []
        for chunk in df_fake:
            chunks.append(chunk)
        return pd.concat(chunks, ignore_index=True)

    def load_csv(fpath, **kwargs):
        from pandas.errors import EmptyDataError

        engine = kwargs.pop("engine", "pyarrow")  # default: None
        sep = kwargs.pop("sep", None)  # default: ','
        index_col = kwargs.pop("index_col", None)  # default: None
        memory_map = kwargs.pop("memory_map", False)  # default: False
        skipinitialspace = kwargs.pop("skipinitialspace", False)  # default: False
        encoding = kwargs.pop("encoding", "utf-8")  # default: "utf-8"
        on_bad_lines = kwargs.pop("on_bad_lines", "skip")  # default: 'error'
        comment = kwargs.pop("comment", None)  # default: None
        fmt = kwargs.pop("fmt", False)  # default:
        chunksize = kwargs.pop("chunksize", None)  # default: None

        # check filesize
        f_size = round(os.path.getsize(fpath) / 1024 / 1024, 3)
        if f_size >= 50:  # 50 MB
            if chunksize is None:
                chunksize = 5000
                print(
                    f"file size: ~{f_size}MB, chunksize with {chunksize}"
                )
        engine = "c" if chunksize else engine  # when chunksize, recommend 'c'
        low_memory = kwargs.pop("low_memory", True)  # default: True
        low_memory = (
            False if chunksize else True
        )  # when chunksize, recommend low_memory=False # default:
        verbose = kwargs.pop("verbose", False)
        if run_once_within(reverse=True) and verbose:
            use_pd("read_csv", verbose=verbose)

        if comment is None:  # default: None
            comment = get_comment(
                fpath, comment=None, encoding="utf-8", lines_to_check=5
            )
            
        # Detect and fix escaped "\t" before reading
        try:
            with open(fpath, "r", encoding=encoding) as f:
                first_line = f.readline()
            if "\\t" in first_line and (sep in [None, "\t"]):
                if verbose:
                    print(":warning: Detected literal '\\t' sequences ‚Äî converting to real tabs...")
                with open(fpath, "r", encoding=encoding) as f:
                    text = f.read().replace("\\t", "\t")
                fpath = StringIO(text)  # use memory buffer instead of disk file
                sep = "\t"
        except Exception as e:
            if verbose:
                print(f"Tab check failed: {e}")
        # Detect and fix escaped "\t" before reading
        use_buffer = False
        buffer_obj = None
        try:
            with open(fpath, "r", encoding=encoding) as f:
                first_line = f.readline()
            if "\\t" in first_line and (sep in [None, "\t"]):
                if verbose:
                    print(":warning: Detected literal '\\t' sequences ‚Äî converting to real tabs...")
                with open(fpath, "r", encoding=encoding) as f:
                    text = f.read().replace("\\t", "\t")
                buffer_obj = StringIO(text)  # keep as a separate object
                sep = "\t"
                use_buffer = True
        except Exception as e:
            if verbose:
                print(f"Tab check failed: {e}") 
        try:
            df = pd.read_csv(
                fpath,
                engine=engine,
                index_col=index_col,
                memory_map=memory_map,
                encoding=encoding,
                comment=comment,
                skipinitialspace=skipinitialspace,
                sep=sep,
                on_bad_lines=on_bad_lines,
                chunksize=chunksize,
                low_memory=low_memory,
                **kwargs,
            )
            if chunksize:
                df = _get_chunks(df)
                print(df.shape)
            if is_df_abnormal(df, thr=5, verbose=0):  # raise error
                raise ValueError("the df is abnormal")
        except Exception as e:
            if verbose:
                print(f"Tab check failed: {e}, trying pyarrow engine") 
            try:
                try:
                    if engine == "pyarrow" and not chunksize:
                        df = pd.read_csv(
                            fpath,
                            engine=engine,
                            index_col=index_col,
                            encoding=encoding,
                            sep=sep,
                            on_bad_lines=on_bad_lines,
                            comment=comment,
                            low_memory=low_memory,
                            **kwargs,
                        )
                    else:
                        df = pd.read_csv(
                            fpath,
                            engine=engine,
                            index_col=index_col,
                            memory_map=memory_map,
                            encoding=encoding,
                            sep=sep,
                            skipinitialspace=skipinitialspace,
                            on_bad_lines=on_bad_lines,
                            comment=comment,
                            chunksize=chunksize,
                            low_memory=low_memory,
                            **kwargs,
                        )
                    if chunksize:
                        df = _get_chunks(df)
                        print(df.shape)
                    if is_df_abnormal(df, thr=5, verbose=0):
                        raise ValueError("the df is abnormal")
                except (UnicodeDecodeError, ValueError):
                    encoding = get_encoding(fpath)
                    # print(f"utf-8 failed. Retrying with detected encoding: {encoding}")
                    if engine == "pyarrow" and not chunksize:
                        df = pd.read_csv(
                            fpath,
                            engine=engine,
                            index_col=index_col,
                            encoding=encoding,
                            sep=sep,
                            on_bad_lines=on_bad_lines,
                            comment=comment,
                            low_memory=low_memory,
                            **kwargs,
                        )
                    else:
                        df = pd.read_csv(
                            fpath,
                            engine=engine,
                            index_col=index_col,
                            memory_map=memory_map,
                            encoding=encoding,
                            sep=sep,
                            skipinitialspace=skipinitialspace,
                            on_bad_lines=on_bad_lines,
                            comment=comment,
                            chunksize=chunksize,
                            low_memory=low_memory,
                            **kwargs,
                        )
                    if chunksize:
                        df = _get_chunks(df)
                        print(df.shape)
                    if is_df_abnormal(df, thr=5, verbose=0):
                        raise ValueError("the df is abnormal")
            except Exception as e:
                if verbose:
                    print(f"trying with: engine=pyarrow, but different separators") 
                separators = [",",";", "\t", ";", "|", " "]
                for sep in separators:
                    sep2show = sep if sep != "\t" else "\\t"
                    if verbose:
                        print(f'trying with: engine=pyarrow, sep="{sep2show}"')
                    try:
                        df = pd.read_csv(
                            fpath,
                            engine=engine,
                            skipinitialspace=skipinitialspace,
                            sep=sep,
                            on_bad_lines=on_bad_lines,
                            comment=comment,
                            chunksize=chunksize,
                            low_memory=low_memory,
                            **kwargs,
                        )
                        if chunksize:
                            df = _get_chunks(df)
                            print(df.shape)
                        if not is_df_abnormal(df, thr=5, verbose=0) and verbose:  # normal
                            display(df.head(2))
                            if verbose:
                                print(f"shape: {df.shape}")
                            return df
                    except:
                        pass
                else:
                    if not chunksize:
                        engines = [None, "c", "python"]
                        for engine in engines:
                            separators = [",",";", "\t", ";", "|", " "]
                            for sep in separators:
                                try:
                                    sep2show = sep if sep != "\t" else "\\t"
                                    if verbose:
                                        print(
                                            f"trying with: engine={engine}, sep='{sep2show}'"
                                        )
                                    # print(".")
                                    df = pd.read_csv(
                                        fpath,
                                        engine=engine,
                                        sep=sep,
                                        on_bad_lines=on_bad_lines,
                                        comment=comment,
                                        chunksize=chunksize,
                                        low_memory=low_memory,
                                        **kwargs,
                                    )
                                    # display(df.head(2))
                                    # print(f"is_df_abnormal:{is_df_abnormal(df, thr=5, verbose=0)}")
                                    if chunksize:
                                        df = _get_chunks(df)
                                        print(df.shape)
                                    if not is_df_abnormal(df, thr=5, verbose=0):
                                        if verbose:
                                            (
                                                display(df.head(2))
                                                if isinstance(df, pd.DataFrame)
                                                else desisplay("it is not a DataFrame")
                                            )
                                            (
                                                print(f"shape: {df.shape}")
                                                if isinstance(df, pd.DataFrame) and verbose
                                                else display("it is not a DataFrame")
                                            )
                                        return df
                                except EmptyDataError as e:
                                    continue
                            else:
                                pass
        # print(kwargs)
        # if is_df_abnormal(df,verbose=verbose):
        #     df=pd.read_csv(fpath,**kwargs)
        if verbose:
            display(df.head(2))
            print(f"shape: {df.shape}")
        return df


    def load_parquet(fpath, **kwargs):
        """
        Load a Parquet file into a Pandas DataFrame with advanced options.

        Parameters:
        - fpath (str): The file path to the Parquet file.
        - engine (str): The engine to use for reading the Parquet file (default is 'pyarrow').
        - columns (list): List of columns to load. If None, loads all columns.
        - verbose (bool): If True, prints additional information about the loading process.
        - filters (list): List of filter conditions for predicate pushdown.
        - **kwargs: Additional keyword arguments for `pd.read_parquet`.

        Returns:
        - df (DataFrame): The loaded DataFrame.
        """

        engine = kwargs.get("engine", "pyarrow")
        verbose = kwargs.pop("verbose", False)

        if run_once_within(reverse=True) and verbose:
            use_pd("read_parquet", verbose=verbose)
        try:
            df = pd.read_parquet(fpath, engine=engine, **kwargs)
            if verbose:
                if "columns" in kwargs:
                    print(f"Loaded columns: {kwargs['columns']}")
                else:
                    print("Loaded all columns.")
            if verbose:
                print(f"shape: {df.shape}")
        except Exception as e:
            print(f"An error occurred while loading the Parquet file: {e}")
            df = None

        return df

    def load_ipynb(fpath, **kwargs):
        import nbformat
        from nbconvert import MarkdownExporter

        as_version = kwargs.get("as_version", 4)
        with open(fpath, "r") as file:
            nb = nbformat.read(file, as_version=as_version)
            md_exporter = MarkdownExporter()
            md_body, _ = md_exporter.from_notebook_node(nb)
        return md_body

    def load_pdf(fpath, page="all", verbose=False, **kwargs):
        """
        Parameters:
        fpath: The path to the PDF file to be loaded.
        page (optional):
            Specifies which page or pages to extract text from. By default, it's set to "all", which means text from all
            pages will be returned. It can also be an integer to specify a single page number or a list of integers to
            specify multiple pages.
        verbose (optional):
            If True, prints the total number of pages processed.
        Functionality:
        It initializes an empty dictionary text_dict to store page numbers as keys and their corresponding text as values.
        It iterates through each page of the PDF file using a for loop.
        For each page, it extracts the text using PyPDF2's extract_text() method and stores it in text_dict with the page number incremented by 1 as the key.
        If the page parameter is an integer, it converts it into a list containing that single page number to ensure consistency in handling.
        If the page parameter is a NumPy array, it converts it to a list using the tolist() method to ensure compatibility with list operations.
        If verbose is True, it prints the total number of pages processed.
        If page is a list, it combines the text of the specified pages into a single string combined_text and returns it.
        If page is set to "all", it returns the entire text_dict containing text of all pages.
        If page is an integer, it returns the text of the specified page number.
        If the specified page is not found, it returns the string "Page is not found".
        """
        from pypdf import PdfReader 
        

        text_dict = {}
        with open(fpath, "rb") as file:
            pdf_reader = PdfReader(file)
            num_pages = len(pdf_reader.pages)
            for page_num in range(num_pages):
                if verbose:
                    print(f"processing page {page_num}")
                page_ = pdf_reader.pages[page_num]
                text_dict[page_num + 1] = page_.extract_text()
        if isinstance(page, int):
            page = [page]
        elif isinstance(page, np.ndarray):
            page = page.tolist()
        if verbose:
            print(f"total pages: {page_num}")
        if isinstance(page, list):
            combined_text = ""
            for page_num in page:
                combined_text += text_dict.get(page_num, "")
            return combined_text
        elif "all" in page.lower():
            combined_text = ""
            for i in text_dict.values():
                combined_text += i
            return combined_text
        else:
            return text_dict.get(int(page), "Page is not found")

    def load_docx(
        fpath: str,
        detailed: bool = False,
        include_empty_paragraphs: bool = False,
        strip_text: bool = True,
        extract_images: bool = False,
        max_recursion_depth: int = 5,
        include_meta: bool = False,
    ):
        """
        Ultimate DOCX parser with enhanced features and robust error handling.

        Args:
            fpath: Path to DOCX file
            detailed: Return rich object structure (default: False)
            include_empty_paragraphs: Keep empty paragraphs (default: False)
            strip_text: Remove leading/trailing whitespace (default: True)
            extract_images: Return images as base64 (default: False)
            max_recursion_depth: For nested tables (default: 5)
            include_meta: Include document metadata (default: False)

        Returns:
            List of elements (detailed=False) or rich objects (detailed=True)
        """
        from docx import Document
        from docx.oxml.ns import qn
        from docx.oxml import parse_xml
        from docx.shared import Inches, RGBColor
        import base64
        import io
        from PIL import Image
        from collections import defaultdict

        def safe_extract_hyperlinks(doc: Document) -> Dict[str, str]:
            """Extract hyperlinks with error handling"""
            try:
                links = {}
                if hasattr(doc.part, "rels") and doc.part.rels:
                    for rel in doc.part.rels.values():
                        if "hyperlink" in rel.reltype:
                            links[rel.rId] = rel._target
                return links
            except Exception:
                return {}

        def safe_detect_numbering_formats(doc: Document) -> Dict[str, str]:
            """Extract numbering formats with comprehensive error handling"""
            numbering = {}
            try:
                # Check if numbering part exists without triggering creation
                if not hasattr(doc.part, "_numbering_part"):
                    return numbering

                # Access numbering part if it exists
                if hasattr(doc.part, "numbering_part"):
                    try:
                        num_xml = doc.part.numbering_part._element
                        for num in num_xml.xpath(".//w:num"):
                            try:
                                num_id = num.get(qn("w:numId"))
                                abstract_id = num.xpath("./w:abstractNumId")[0].get(
                                    qn("w:val")
                                )

                                abstract = num_xml.xpath(
                                    f'.//w:abstractNum[@w:abstractNumId="{abstract_id}"]'
                                )
                                if abstract:
                                    for lvl in abstract[0].xpath(".//w:lvl"):
                                        try:
                                            level = lvl.get(qn("w:ilvl"))
                                            fmt = lvl.xpath("./w:numFmt")[0].get(
                                                qn("w:val")
                                            )
                                            numbering[f"{num_id}_{level}"] = fmt
                                        except (IndexError, AttributeError):
                                            continue
                            except (IndexError, AttributeError):
                                continue
                    except Exception:
                        pass
            except Exception:
                pass
            return numbering

        def process_body(parent, ctx: Dict, depth: int):
            """Process document body recursively with error handling"""
            blocks = []
            if depth > ctx["max_depth"]:
                return blocks

            try:
                for child in parent.iterchildren():
                    try:
                        tag = child.tag
                        if tag.endswith("p"):  # Paragraph
                            para = safe_process_paragraph(child, ctx, depth)
                            if para is not None:
                                blocks.append(para)
                        elif tag.endswith("tbl"):  # Table
                            table = safe_process_table(child, ctx, depth)
                            if table is not None:
                                blocks.append(table)
                        elif tag.endswith("sectPr"):  # Section break
                            if ctx["doc"]._element.body is parent:
                                try:
                                    blocks.append(
                                        {
                                            "type": "section_break",
                                            "properties": parse_section_properties(child),
                                        }
                                    )
                                except Exception:
                                    pass
                    except Exception:
                        continue
            except Exception:
                pass
            return blocks

        def safe_process_paragraph(p, ctx: Dict, depth: int):
            """Process paragraph with comprehensive error handling"""
            try:
                from docx.text.paragraph import Paragraph

                para = Paragraph(p, ctx["doc"])

                # Process runs with formatting
                runs = []
                for run in para.runs:
                    try:
                        run_data = {
                            "text": run.text.strip() if ctx["strip"] else run.text,
                            "bold": run.bold,
                            "italic": run.italic,
                            "underline": run.underline,
                        }

                        # Add font info if available
                        try:
                            run_data["font"] = {
                                "name": run.font.name,
                                "size": run.font.size,
                                "color": (
                                    parse_color(run.font.color.rgb)
                                    if hasattr(run.font.color, "rgb") and run.font.color.rgb
                                    else None
                                ),
                            }
                        except Exception:
                            run_data["font"] = None

                        # Handle hyperlinks
                        try:
                            hyperlink = safe_detect_hyperlink(run, ctx)
                            if hyperlink:
                                run_data["hyperlink"] = hyperlink
                        except Exception:
                            pass

                        runs.append(run_data)
                    except Exception:
                        continue

                # Check for images
                images = []
                if ctx["extract_images"]:
                    try:
                        for shape in para._element.xpath(".//pic:pic"):
                            try:
                                img_data = safe_extract_image_data(shape, ctx)
                                if img_data:
                                    images.append(img_data)
                            except Exception:
                                continue
                    except Exception:
                        pass

                # Check for lists
                list_info = None
                try:
                    list_info = safe_detect_list_properties(para, ctx)
                except Exception:
                    pass

                # Build paragraph object
                full_text = "".join(r.get("text", "") for r in runs)
                is_empty = not full_text.strip()

                if not ctx["detailed"]:
                    return (
                        full_text
                        if not is_empty or ctx["include_empty_paragraphs"]
                        else None
                    )

                para_data = {
                    "type": "paragraph",
                    "text": full_text,
                    "style": para.style.name if para.style else None,
                    "runs": runs,
                    "alignment": str(para.alignment) if para.alignment else None,
                    "list": list_info,
                }

                if images:
                    para_data["images"] = images

                return para_data
            except Exception:
                return None

        def safe_process_table(
            tbl, ctx: Dict, depth: int
        ) -> Optional[Union[List[List[str]], Dict]]:
            """Process table with comprehensive error handling"""
            try:
                from docx.table import Table

                table = Table(tbl, ctx["doc"])
                rows = []

                for row in table.rows:
                    row_data = []
                    for cell in row.cells:
                        try:
                            # Process cell content recursively
                            cell_content = process_body(cell._tc, ctx, depth + 1)

                            # Get cell properties
                            cell_props = {
                                "rowspan": get_span(cell, "row"),
                                "colspan": get_span(cell, "col"),
                                "shading": parse_shading(cell),
                                "vertical_alignment": get_vertical_alignment(cell),
                                "width": get_cell_width(cell),
                            }

                            if ctx["detailed"]:
                                cell_data = {
                                    "content": cell_content,
                                    **cell_props,
                                }
                            else:
                                # Flatten cell content for simple output
                                text_parts = []
                                for item in cell_content:
                                    if isinstance(item, str):
                                        text_parts.append(item)
                                    elif (
                                        isinstance(item, dict)
                                        and item.get("type") == "paragraph"
                                    ):
                                        text_parts.append(item.get("text", ""))
                                    elif isinstance(item, list):  # Nested table
                                        text_parts.append("[TABLE]")
                                cell_data = "\n".join(text_parts)

                            row_data.append(cell_data)
                        except Exception:
                            row_data.append(
                                "[ERROR PROCESSING CELL]" if ctx["detailed"] else ""
                            )
                    rows.append(row_data)

                if not ctx["detailed"]:
                    return rows

                return {
                    "type": "table",
                    "rows": rows,
                    "style": table.style.name if table.style else None,
                    "width": get_table_width(table),
                    "layout": get_table_layout(table),
                    "borders": get_table_borders(table),
                }
            except Exception:
                return None

        def get_span(cell, span_type: str):
            """Get rowspan or colspan for a cell"""
            try:
                if span_type == "row":
                    return int(cell._tc.get(qn("w:rowSpan"), 1))
                elif span_type == "col":
                    return int(cell._tc.get(qn("w:gridSpan"), 1))
            except Exception:
                return 1

        def parse_shading(cell) -> Optional[Dict]:
            """Parse cell shading/background color"""
            try:
                shading = cell._tc.xpath("./w:tcPr/w:shd")[0]
                return {
                    "fill": shading.get(qn("w:fill")),
                    "color": shading.get(qn("w:color")),
                    "val": shading.get(qn("w:val")),
                }
            except Exception:
                return None

        def get_vertical_alignment(cell) -> Optional[str]:
            """Get cell vertical alignment"""
            try:
                align = cell._tc.xpath("./w:tcPr/w:vAlign")[0]
                return align.get(qn("w:val"))
            except Exception:
                return None

        def get_cell_width(cell) -> Optional[Dict]:
            """Get cell width properties"""
            try:
                width = cell._tc.xpath("./w:tcPr/w:tcW")[0]
                return {
                    "width": width.get(qn("w:w")),
                    "type": width.get(qn("w:type")),
                }
            except Exception:
                return None

        def get_table_width(table) -> Optional[Dict]:
            """Get table width properties"""
            try:
                width = table._tbl.xpath("./w:tblPr/w:tblW")[0]
                return {
                    "width": width.get(qn("w:w")),
                    "type": width.get(qn("w:type")),
                }
            except Exception:
                return None

        def get_table_layout(table) -> Optional[str]:
            """Get table layout (autofit/fixed)"""
            try:
                layout = table._tbl.xpath("./w:tblPr/w:tblLayout")[0]
                return layout.get(qn("w:type"))
            except Exception:
                return None

        def get_table_borders(table) -> Dict:
            """Get table border properties"""
            borders = {}
            try:
                tbl_borders = table._tbl.xpath("./w:tblPr/w:tblBorders")[0]
                for side in ["top", "left", "bottom", "right", "insideH", "insideV"]:
                    try:
                        border = tbl_borders.xpath(f"./w:{side}")[0]
                        borders[side] = {
                            "color": border.get(qn("w:color")),
                            "size": border.get(qn("w:sz")),
                            "space": border.get(qn("w:space")),
                            "val": border.get(qn("w:val")),
                        }
                    except Exception:
                        borders[side] = None
            except Exception:
                pass
            return borders

        def safe_detect_hyperlink(run, ctx: Dict) -> Optional[str]:
            """Check if run contains hyperlink with error handling"""
            try:
                hyperlinks = run._element.xpath(".//w:hyperlink")
                if hyperlinks:
                    link_id = hyperlinks[0].get(qn("r:id"))
                    return ctx["hyperlinks"].get(link_id, None)
            except Exception:
                pass
            return None

        def safe_extract_image_data(shape, ctx: Dict) -> Optional[Dict]:
            """Extract image as base64 string with error handling"""
            try:
                blip = shape.xpath(".//a:blip")[0]
                img_id = blip.get(qn("r:embed"))
                image_part = ctx["doc"].part.related_parts[img_id]

                img_bytes = image_part._blob
                img_format = image_part.content_type.split("/")[-1]

                # Convert to base64
                b64 = base64.b64encode(img_bytes).decode("utf-8")
                ctx["image_counter"] += 1

                return {
                    "id": f"img_{ctx['image_counter']}",
                    "format": img_format,
                    "data": f"data:image/{img_format};base64,{b64}",
                    "dimensions": get_image_dimensions(shape),
                }
            except Exception:
                return None

        def get_image_dimensions(shape) -> Dict[str, float]:
            """Get image dimensions in inches"""
            try:
                ext = shape.xpath(".//a:xfrm/a:ext")[0]
                return {
                    "width": Inches(float(ext.get("cx", 0)) / 914400),
                    "height": Inches(float(ext.get("cy", 0)) / 914400),
                }
            except Exception:
                return {"width": 0, "height": 0}

        def safe_detect_list_properties(para, ctx: Dict) -> Optional[Dict]:
            """Detect list numbering format with error handling"""
            try:
                num_id = para._p.xpath("./w:pPr/w:numPr/w:numId")
                if not num_id:
                    return None

                num_id = num_id[0].get(qn("w:val"))
                level = para._p.xpath("./w:pPr/w:numPr/w:ilvl")[0].get(qn("w:val"))
                fmt = ctx["list_info"].get(f"{num_id}_{level}")

                if fmt:
                    return {
                        "num_id": num_id,
                        "level": level,
                        "format": fmt,
                    }
            except Exception:
                pass
            return None

        def parse_section_properties(sect_pr) -> Dict:
            """Parse section properties"""
            props = {}
            try:
                # Page size and margins
                pg_sz = sect_pr.xpath("./w:pgSz")[0]
                props["page_size"] = {
                    "width": pg_sz.get(qn("w:w")),
                    "height": pg_sz.get(qn("w:h")),
                    "orientation": pg_sz.get(qn("w:orient")),
                }

                pg_mar = sect_pr.xpath("./w:pgMar")[0]
                props["margins"] = {
                    "top": pg_mar.get(qn("w:top")),
                    "right": pg_mar.get(qn("w:right")),
                    "bottom": pg_mar.get(qn("w:bottom")),
                    "left": pg_mar.get(qn("w:left")),
                    "header": pg_mar.get(qn("w:header")),
                    "footer": pg_mar.get(qn("w:footer")),
                    "gutter": pg_mar.get(qn("w:gutter")),
                }
            except Exception:
                pass
            return props

        def parse_color(color_rgb: Optional[RGBColor]) -> Optional[str]:
            """Convert color to hex string"""
            if color_rgb is None:
                return None
            return f"#{color_rgb:06x}"

        def get_document_metadata(ctx: Dict) -> Dict:
            """Extract document properties"""
            doc = ctx["doc"]
            core_props = doc.core_properties
            return {
                "title": core_props.title,
                "author": core_props.author,
                "created": core_props.created,
                "modified": core_props.modified,
                "pages": len(doc.sections),
                "paragraphs": sum(1 for _ in doc.paragraphs),
                "tables": len(doc.tables),
                "images": ctx["image_counter"],
            }

        try:
            doc = Document(fpath)
        except Exception as e:
            raise ValueError(f"Failed to open DOCX file: {str(e)}")

        ctx = {
            "doc": doc,
            "strip": strip_text,
            "extract_images": extract_images,
            "image_counter": 0,
            "max_depth": max_recursion_depth,
            "hyperlinks": safe_extract_hyperlinks(doc),
            "list_info": safe_detect_numbering_formats(doc),
            "detailed": detailed,
            "include_empty_paragraphs": include_empty_paragraphs,
        }

        try:
            content = process_body(doc._element.body, ctx, 0)
            meta = get_document_metadata(ctx) if include_meta else {}

            if include_meta and detailed:
                return {"content": content, "metadata": meta}
            return content
        except Exception as e:
            raise RuntimeError(f"Error processing DOCX content: {str(e)}")

    def load_rtf(file_path):
        from striprtf.striprtf import rtf_to_text

        try:
            with open(file_path, "r") as file:
                rtf_content = file.read()
                text = rtf_to_text(rtf_content)
                return text
        except Exception as e:
            print(f"Error loading RTF file: {e}")


    def load_npy(filepath, **kwargs):
        """
        Load a .npy file (NumPy binary format).

        This format stores a single NumPy array efficiently.
        Example:
            >>> arr = np.array([1, 2, 3])
            >>> np.save("example.npy", arr)
            >>> data = load_npy("example.npy")
            >>> print(data)  # Output: [1 2 3]
        """
        return np.load(filepath, **kwargs)


    def load_npz(filepath, **kwargs):
        """
        Load a .npz file (NumPy zip archive).

        Stores multiple NumPy arrays in one file, accessed as a dict.
        Example:
        arr1 = np.array([1,2,3])
        arr2 = np.array([4,5,6])
        np.savez("arrays.npz", a=arr1, b=arr2)
        loaded = np.load("arrays.npz")
        print(loaded['a'])  # array([1,2,3])
        print(loaded['b'])  # array([4,5,6])

            >>> np.savez("example.npz", x=[1, 2, 3], y=[4, 5, 6])
            >>> data = load_npz("example.npz")
            >>> print(data["x"])  # Output: [1 2 3]
        """
        return dict(np.load(filepath, **kwargs))


    def load_arff(filepath, **kwargs):
        """
        Load a .arff file (Attribute-Relation File Format).

        Common in machine learning (e.g. Weka datasets).
        Example:
            >>> result = load_arff("iris.arff")
            >>> print(result['meta'])  # Shows feature names and types
        """
        from scipy.io import arff
        data, meta = arff.loadarff(filepath, **kwargs)
        return {'data': data, 'meta': meta}


    def load_pt(filepath, **kwargs):
        """
        Load a .pt file (PyTorch checkpoint or tensor).

        Can contain tensors, models, or dictionaries.
        Example:
            >>> import torch
            >>> torch.save({"a": torch.tensor([1, 2])}, "example.pt")
            >>> data = load_pt("example.pt")
            >>> print(data["a"])  # Output: tensor([1, 2])
        """
        import torch
        return torch.load(filepath, map_location=torch.device('cpu'), **kwargs)


    def load_pth(filepath, **kwargs):
        """
        Load a .pth file (PyTorch model state_dict).

        Often used for saving only the model parameters.
        Example:
            >>> model_state = {"layer1": torch.randn(3, 3)}
            >>> torch.save(model_state, "model.pth")
            >>> data = load_pth("model.pth")
            >>> print(data)  # Dictionary of tensors
        """
        import torch
        return torch.load(filepath, map_location=torch.device('cpu'), **kwargs)


    def load_dcm(filepath, **kwargs):
        """
        Load a .dcm file (DICOM medical image).

        Standard for medical imaging (MRI, CT).
        Example:
            >>> dcm_data = load_dcm("example.dcm")
            >>> print(dcm_data.PatientName)
        """
        import pydicom
        return pydicom.dcmread(filepath, **kwargs)
 
    def load_idx_file(filename):
        
        import struct
        with open(filename, "rb") as f:
            zero, data_type, dims = struct.unpack(">HBB", f.read(4))
            shape = tuple(struct.unpack(">I", f.read(4))[0] for _ in range(dims))

            dtype_map = {
                0x08: np.uint8,
                0x09: np.int8,
                0x0B: np.int16,
                0x0C: np.int32,
                0x0D: np.float32,
                0x0E: np.float64,
            }

            if data_type not in dtype_map:
                raise ValueError(f"Unsupported data type: 0x{data_type:02X}")

            data = np.frombuffer(f.read(), dtype=dtype_map[data_type])
            return data.reshape(shape)

    def load_gmt(filepath: str) -> Dict[str, Dict[str, List[str]]]:
        """
        Load a GMT file.

        Returns format:
        {
            "SetA": {
                "description": "...",
                "genes": [...]
            },
            ...
        }
        """

        if not os.path.exists(filepath):
            raise FileNotFoundError(filepath)

        gene_sets = {}

        with open(filepath, "r") as f:
            for line in f:
                line = line.strip()
                if not line or line.startswith("#"):
                    continue  # skip empty lines / comments

                parts = line.split("\t")
                if len(parts) < 2:
                    raise ValueError(f"Malformed GMT line: {line}")

                name = parts[0]
                desc = parts[1]
                genes = parts[2:] if len(parts) > 2 else []

                gene_sets[name] = {
                    "description": desc,
                    "genes": genes,
                }

        return gene_sets


    def load_gmx(filepath: str) -> Dict[str, Dict[str, List[str]]]:
        """
        Load a GMX file.

        Returns format:
        {
            "SetA": {
                "description": "...",
                "genes": [...]
            },
            ...
        }
        """

        if not os.path.exists(filepath):
            raise FileNotFoundError(filepath)

        with open(filepath, "r") as f:
            lines = [l.strip() for l in f if l.strip()]

        if len(lines) < 2:
            raise ValueError("GMX file must have at least 2 rows.")

        names = lines[0].split("\t")
        descs = lines[1].split("\t")
        data_lines = lines[2:]  # gene rows

        num_sets = len(names)
        if len(descs) != num_sets:
            raise ValueError("GMX name line and description line length mismatch.")

        # Prepare empty lists
        gene_sets = {
            names[i]: {"description": descs[i], "genes": []}
            for i in range(num_sets)
        }

        # Fill genes
        for line in data_lines:
            cols = line.split("\t")
            # pad with "" if needed
            cols = cols + [""] * (num_sets - len(cols))
            for i, gene in enumerate(cols):
                if gene.strip():
                    gene_sets[names[i]]["genes"].append(gene.strip())

        return gene_sets



    # main function
    if kind is None:
        _, kind = os.path.splitext(fpath)
        if kind:
            kind = kind.lower()
        else:
            if all([i.lower() in os.path.basename(fpath).lower() for i in ["idx", "ubyte"]]):
                kind="idx"
    kind = kind.lstrip(".").lower()
    img_types = [
        "bmp",
        "eps",
        "gif",
        "png",
        "jpg",
        "jpeg",
        "jpeg2000",
        "tiff",
        "tif",
        "icns",
        "ico",
        "im",
        "msp",
        "pcx",
        "ppm",
        "sgi",
        "spider",
        "tga",
        "webp" 
    ]
    doc_types = [
        "docx",
        "pdf",
        "txt",
        "csv",
        "xlsx",
        "tsv","gtf","gff","bed",
        "parquet",
        "snappy",
        "md",
        "html",
        "json",
        "yaml",
        "xml",
        "ipynb",
        "mtx",
        "rtf",
    ]
    zip_types = [
        "gz",
        "zip",
        "7z",
        "rar",
        "tgz",
        "tar",
        "tar.gz",
        "tar.bz2",
        "bz2",
        "xz",
        "gzip",
    ]
    data_types=["npy","npz","arff","pt","pth","idx"]
    other_types = ["fcs","gmt","gmx"]
    supported_types = [*doc_types, *img_types, *zip_types, *other_types,*data_types]
    if kind not in supported_types:
        print(
            f'Warning:\n"{kind}" is not in the supported list '
        )  # {supported_types}')

    if kind == "docx":
        return load_docx(fpath)
    elif kind == "txt" or kind == "md":
        return load_txt_md(fpath)
    elif kind == "html":
        return load_html(fpath, **kwargs)
    elif kind == "json":
        return load_json(fpath, **kwargs)
    elif kind == "yaml":
        return load_yaml(fpath)
    elif kind == "xml":
        return load_xml(fpath)
    elif kind in ["csv", "tsv","gtf","gff","bed"]:
        # verbose = kwargs.pop("verbose", False)
        # if run_once_within(reverse=True) and verbose:
        #     use_pd("read_csv")
        content = load_csv(fpath, **kwargs)
        return content
    elif kind == "pkl":
        verbose = kwargs.pop("verbose", False)
        if run_once_within(reverse=True) and verbose:
            use_pd("read_pickle")
        try:
            res_ = pd.read_pickle(fpath, **kwargs)
        except Exception as e:
            import pickle

            with open("sgd_classifier.pkl", "rb") as f:
                res_ = pickle.load(f)
        return res_
    elif kind in ["ods", "ods", "odt"]:
        engine = kwargs.get("engine", "odf")
        kwargs.pop("engine", None)
        return load_excel(fpath, engine=engine, **kwargs)
    elif kind == "xls":
        verbose = kwargs.pop("verbose", False)
        engine = kwargs.get("engine", "xlrd")
        kwargs.pop("engine", None)
        content = load_excel(fpath, engine=engine, **kwargs)
        (
            print(f"shape: {content.shape}")
            if isinstance(content, pd.DataFrame) and verbose
            else None
        )
        display(content.head(3)) if isinstance(content, pd.DataFrame) else None
        return content
    elif kind == "xlsx":
        verbose = kwargs.pop("verbose", False)
        content = load_excel(fpath, verbose=verbose,**kwargs)
        # (
        #     display(content.head(3))
        #     if isinstance(content, pd.DataFrame) and verbose
        #     else None
        # )
        print(f"shape: {content.shape}") if isinstance(content, pd.DataFrame) and verbose else None
        return content
    elif kind == "mtx":
        from scipy.io import mmread

        verbose = kwargs.pop("verbose", False)
        dat_mtx = mmread(fpath)
        content = pd.DataFrame.sparse.from_spmatrix(dat_mtx, **kwargs)
        (
            display(content.head(3))
            if isinstance(content, pd.DataFrame) and verbose
            else None
        )
        print(f"shape: {content.shape}")
        return content
    elif kind == "ipynb":
        return load_ipynb(fpath, **kwargs)
    elif kind in ["parquet", "snappy"]:
        verbose = kwargs.pop("verbose", False)
        if run_once_within(reverse=True):
            use_pd("read_parquet")
        return load_parquet(fpath, **kwargs)
    elif kind == "feather":
        verbose = kwargs.pop("verbose", False)
        if run_once_within(reverse=True):
            use_pd("read_feather")
        content = pd.read_feather(fpath, **kwargs)
        return content
    elif kind == "h5":
        content = pd.read_hdf(fpath, **kwargs)
        return content 
    elif kind == "pdf":
        return load_pdf(fpath, **kwargs)
    elif kind.lower() in img_types:
        print(f'Image ".{kind}" is loaded.')
        return load_img(fpath)
    elif kind == "gz" and fpath.endswith(".soft.gz"):
        import GEOparse

        return GEOparse.get_GEO(filepath=fpath)
    elif kind.lower() in zip_types:
        from pprint import pp

        keep = kwargs.get("keep", False)
        fpath_unzip = unzip(fpath)
        if os.path.isdir(fpath_unzip):
            print(f"{fpath_unzip} is a folder. fload stoped.")
            return ls(fpath_unzip)
        elif os.path.isfile(fpath_unzip):
            print(f"{fpath_unzip} is a file.")
            content_unzip = fload(fpath_unzip, **kwargs)
            if not keep:
                os.remove(fpath_unzip)
            return content_unzip
        else:
            print(f"{fpath_unzip} does not exist or is a different type.")

    elif kind.lower() == "gmt":
        import gseapy as gp

        gene_sets = gp.read_gmt(fpath)
        return gene_sets

    elif kind.lower() == "fcs":
        import fcsparser

        # https://github.com/eyurtsev/fcsparser
        meta, data = fcsparser.parse(fpath, reformat_meta=True)
        print("meta, data = fload(*.fcs)")
        return meta, data
    elif kind.lower()=="gmt":
        return load_gmt(fpath)
    elif kind.lower()=="gmx":
        return load_gmx(fpath)
    elif kind == "mplstyle":
        return load_mplstyle(fpath)
    elif kind == "rtf":
        return load_rtf(fpath) 
    elif kind=="npy":
        return load_npy(fpath, **kwargs)
    elif kind=="npz":
        return load_npz(fpath, **kwargs)
    elif kind=="arff":
        return load_arff(fpath, **kwargs)
    elif kind=="pt":
        return load_pt(fpath, **kwargs)
    elif kind=="pth":
        return load_pth(fpath, **kwargs)
    elif kind=="dcm":
        return load_dcm(fpath, **kwargs) 
    elif kind=="idx":
        return load_idx_file(fpath) 
    else:
        print("direct reading...")
        try:
            try:
                with open(fpath, "r", encoding="utf-8") as f:
                    content = f.readlines()
            except UnicodeDecodeError:
                print("Failed to read as utf-8, trying different encoding...")
                with open(fpath, "r", encoding=get_encoding(fpath)) as f:
                    content = f.readlines()
        except:
            try:
                with open(fpath, "r", encoding="utf-8") as f:
                    content = f.read()
            except UnicodeDecodeError:
                print("Failed to read as utf-8, trying different encoding...")
                with open(fpath, "r", encoding=get_encoding(fpath)) as f:
                    content = f.read()
        return content

#* ========get_dir_font==========
def get_dir_font():
    """Get all relevant system and user font directories across platforms."""
    home = os.path.expanduser("~")
    font_dirs = []

    if sys.platform.startswith("darwin"):  # macOS
        font_dirs += [
            os.path.join(home, "Library", "Fonts"),              # User fonts
            "/Library/Fonts",                                    # System fonts
            "/System/Library/Fonts",                             # System fonts
        ]
    elif sys.platform.startswith("linux"):  # Linux
        font_dirs += [
            os.path.join(home, ".fonts"),                        # Legacy user fonts
            os.path.join(home, ".local", "share", "fonts"),      # Modern user fonts
            "/usr/share/fonts",                                  # System-wide fonts
            "/usr/local/share/fonts",                            # Additional system fonts
        ]
    elif sys.platform.startswith("win"):  # Windows
        font_dirs += [
            os.path.join(home, "AppData", "Local", "Microsoft", "Windows", "Fonts"),  # User fonts
            "C:\\Windows\\Fonts"                                                      # System fonts
        ]

    # Filter out non-existent paths
    return [d for d in font_dirs if os.path.exists(d)]


def fopen(fpath):
    try:
        # Check if the file exists
        if not os.path.isfile(fpath):
            print(f"Error: The file does not exist - {fpath}")
            return

        # Get the system platform
        system = platform.system()

        # Platform-specific file opening commands
        if system == "Darwin":  # macOS
            os.system(f'open "{fpath}"')
        elif system == "Windows":  # Windows
            # Ensure the path is handled correctly in Windows, escape spaces
            os.system(f'start "" "{fpath}"')
        elif system == "Linux":  # Linux
            os.system(f'xdg-open "{fpath}"')
        elif system == "Java":  # Java (or other unhandled systems)
            print(f"Opening {fpath} on unsupported system.")
        else:
            print(f"Unsupported OS: {system}")

        print(f"Successfully opened {fpath} with the default application.")
    except Exception as e:
        print(f"Error opening file {fpath}: {e}")


def fupdate(fpath, content=None, how="head"):
    """
    Update a file by adding new content at the top and moving the old content to the bottom.
    If the file is a JSON file, merge the new content with the old content.

    Parameters
    ----------
    fpath : str
        The file path where the content should be updated.
    content : str or dict, optional
        The new content to add at the top of the file (for text) or merge (for JSON).
        If not provided, the function will not add any new content.

    Notes
    -----
    - If the file at `fpath` does not exist, it will be created.
    - For text files, the new content will be added at the top, followed by the old content.
    - For JSON files, the new content will be merged with the existing JSON content.
    """
    content = content or ""
    file_ext = os.path.splitext(fpath)[1]
    how_s = ["head", "tail", "start", "end", "beginning", "stop", "last", "before"]
    how = strcmp(how, how_s)[0]
    print(how)
    add_where = "head" if how in ["head", "start", "beginning", "before"] else "tail"
    if "json" in file_ext.lower():
        old_content = fload(fpath, kind="json") if os.path.exists(fpath) else {}
        updated_content = (
            {**content, **old_content}
            if add_where == "head"
            else (
                {**old_content, **content} if isinstance(content, dict) else old_content
            )
        )
        fsave(fpath, updated_content)
    else:
        # Handle text file
        if os.path.exists(fpath):
            with open(fpath, "r") as file:
                old_content = file.read()
        else:
            old_content = ""

        # Write new content at the top followed by old content
        with open(fpath, "w") as file:
            if add_where == "head":
                file.write(content + "\n")
                file.write(old_content)
            else:
                file.write(old_content)
                file.write(content + "\n")


def fappend(fpath, content=None, sheet_name=None, mode="a", **kwargs):
    """
    Append new content to the end of a file while preserving existing formatting.
    Supports multiple file types using fload and fsave functions.

    Parameters:
    - fpath: str, path to the file
    - content: content to append. Format depends on file type:
        - Text files: str or list of strings
        - CSV/Excel: list of dicts, DataFrame, or dict (single row)
        - JSON: dict or list to merge/append
        - XML: dict or ElementTree to merge
        - YAML: dict to merge
    - sheet_name: str, sheet name for Excel files
    - mode: str, append mode - 'a' (append), 'i' (insert at beginning), 'o' (overwrite)
    - **kwargs: additional arguments passed to fload/fsave

    Returns:
    - bool: True if successful, False otherwise
    """
    import os
    import pandas as pd
    from collections.abc import MutableMapping, MutableSequence

    mode = strcmp(mode, ["a", "append", "i", "insert", "o", "overwrite", "overlay"])[0]
    if mode in ["a", "append"]:
        mode = "a"
    elif mode in ["i", "insert"]:
        mode = "i"
    elif mode in ["o", "overwrite", "overlay"]:
        mode = "o"

    # Helper functions for different file types
    def _append_text(fpath, content, mode, file_exists, **kwargs):
        """Append to text files"""
        if isinstance(content, (list, tuple)):
            content = "\n".join(str(item) for item in content)
        elif not isinstance(content, str):
            content = str(content)

        encoding = kwargs.get("encoding", "utf-8")

        if mode == "i" and file_exists:  # Insert at beginning
            existing_content = fload(fpath, kind="txt", **kwargs)
            content = content + "\n" + existing_content
            fsave(fpath, content, kind="txt", how="overwrite", **kwargs)
        elif mode == "o":  # Overwrite
            fsave(fpath, content, kind="txt", how="overwrite", **kwargs)
        else:  # Append (default)
            with open(fpath, "a", encoding=encoding) as f:
                f.write(content + "\n")

    def _append_csv(fpath, content, mode, file_exists, **kwargs):
        """Append to CSV files"""
        # Convert content to DataFrame
        if isinstance(content, dict):  # Single row
            df_new = pd.DataFrame([content])
        elif isinstance(content, (list, tuple)):
            if content and isinstance(content[0], dict):
                df_new = pd.DataFrame(content)
            else:
                df_new = pd.DataFrame(content)
        elif isinstance(content, pd.DataFrame):
            df_new = content
        else:
            raise ValueError(
                "For CSV files, content must be dict, list of dicts, or DataFrame"
            )

        if not file_exists or mode == "o":
            # Save new content directly
            fsave(fpath, df_new, kind="csv", **kwargs)
        else:
            # Load existing data and append
            try:
                df_existing = fload(fpath, kind="csv", **kwargs)
                if mode == "i":  # Insert at beginning
                    df_combined = pd.concat([df_new, df_existing], ignore_index=True)
                else:  # Append at end (default)
                    df_combined = pd.concat([df_existing, df_new], ignore_index=True)
            except Exception:
                # If loading fails, use new content only
                df_combined = df_new

            fsave(fpath, df_combined, kind="csv", **kwargs)

    def _append_excel(fpath, content, sheet_name, mode, file_exists, **kwargs):
        """Append to Excel files with formatting preservation"""
        # Convert content to DataFrame
        if isinstance(content, dict):
            df_new = pd.DataFrame([content])
        elif isinstance(content, (list, tuple)):
            if content and isinstance(content[0], dict):
                df_new = pd.DataFrame(content)
            else:
                df_new = pd.DataFrame(content)
        elif isinstance(content, pd.DataFrame):
            df_new = content
        else:
            raise ValueError(
                "For Excel files, content must be dict, list of dicts, or DataFrame"
            )

        if not file_exists or mode == "o":
            # Save new content directly
            fsave(fpath, df_new, kind="xlsx", sheet_name=sheet_name, **kwargs)
        else:
            try:
                # Try to load existing workbook to preserve formatting
                workbook = fload(
                    fpath,
                    kind="xlsx",
                    output="workbook",
                    sheet_name=sheet_name,
                    **kwargs,
                )

                # Load existing data as DataFrame
                df_existing = fload(
                    fpath, kind="xlsx", output="df", sheet_name=sheet_name, **kwargs
                )

                if mode == "i":  # Insert at beginning
                    df_combined = pd.concat([df_new, df_existing], ignore_index=True)
                else:  # Append at end (default)
                    df_combined = pd.concat([df_existing, df_new], ignore_index=True)

                # Save with formatting preserved
                from openpyxl import load_workbook

                if isinstance(workbook, load_workbook.__globals__["Workbook"]):
                    # Use openpyxl to preserve formatting
                    _append_to_excel_workbook(
                        fpath, df_combined, workbook, sheet_name, **kwargs
                    )
                else:
                    # Fallback to regular save
                    fsave(
                        fpath, df_combined, kind="xlsx", sheet_name=sheet_name, **kwargs
                    )

            except Exception as e:
                print(f"Warning: Could not preserve Excel formatting: {e}")
                # Fallback to simple append
                if file_exists:
                    df_existing = fload(
                        fpath, kind="xlsx", output="df", sheet_name=sheet_name, **kwargs
                    )
                    if mode == "i":
                        df_combined = pd.concat(
                            [df_new, df_existing], ignore_index=True
                        )
                    else:
                        df_combined = pd.concat(
                            [df_existing, df_new], ignore_index=True
                        )
                else:
                    df_combined = df_new
                fsave(fpath, df_combined, kind="xlsx", sheet_name=sheet_name, **kwargs)

    def _append_to_excel_workbook(fpath, df_combined, workbook, sheet_name, **kwargs):
        """Append data to Excel workbook while preserving formatting"""
        try:
            from openpyxl.utils.dataframe import dataframe_to_rows
            from openpyxl.worksheet.worksheet import Worksheet

            sheet_name = sheet_name or workbook.sheetnames[0]

            if sheet_name in workbook.sheetnames:
                ws = workbook[sheet_name]
                # Clear existing data but preserve formatting
                ws.delete_rows(2, ws.max_row)  # Keep header row
            else:
                ws = workbook.create_sheet(sheet_name)

            # Write DataFrame to worksheet
            for r_idx, row in enumerate(
                dataframe_to_rows(df_combined, index=False, header=True), 1
            ):
                for c_idx, value in enumerate(row, 1):
                    ws.cell(row=r_idx, column=c_idx, value=value)

            workbook.save(fpath)
        except Exception as e:
            print(f"Error preserving Excel formatting: {e}")
            # Fallback to regular save
            fsave(fpath, df_combined, kind="xlsx", sheet_name=sheet_name, **kwargs)

    def _append_json(fpath, content, mode, file_exists, **kwargs):
        """Append to JSON files"""
        if not file_exists or mode == "o":
            fsave(fpath, content, kind="json", **kwargs)
        else:
            existing_data = fload(fpath, kind="json", **kwargs)

            if mode == "i":  # Insert at beginning
                if isinstance(existing_data, list) and isinstance(content, list):
                    combined = content + existing_data
                elif isinstance(existing_data, dict) and isinstance(content, dict):
                    combined = {**content, **existing_data}  # content takes precedence
                else:
                    combined = [content] + (
                        [existing_data]
                        if not isinstance(existing_data, list)
                        else existing_data
                    )
            else:  # Append at end
                if isinstance(existing_data, list) and isinstance(content, list):
                    combined = existing_data + content
                elif isinstance(existing_data, dict) and isinstance(content, dict):
                    combined = {**existing_data, **content}  # content takes precedence
                else:
                    combined = (
                        [existing_data]
                        if not isinstance(existing_data, list)
                        else existing_data
                    ) + [content]

            fsave(fpath, combined, kind="json", **kwargs)

    def _append_xml(fpath, content, mode, file_exists, **kwargs):
        """Append to XML files (basic implementation)"""
        print(
            "Warning: XML append support is limited. Consider using other formats for complex data."
        )
        return _append_text(fpath, content, mode, file_exists, **kwargs)

    def _append_yaml(fpath, content, mode, file_exists, **kwargs):
        """Append to YAML files"""
        if not file_exists or mode == "o":
            fsave(fpath, content, kind="yaml", **kwargs)
        else:
            existing_data = fload(fpath, kind="yaml", **kwargs)

            if isinstance(existing_data, dict) and isinstance(content, dict):
                if mode == "i":
                    combined = {**content, **existing_data}
                else:
                    combined = {**existing_data, **content}
            else:
                # Handle as list or other types
                if not isinstance(existing_data, list):
                    existing_data = [existing_data]
                if not isinstance(content, list):
                    content = [content]

                if mode == "i":
                    combined = content + existing_data
                else:
                    combined = existing_data + content

            fsave(fpath, combined, kind="yaml", **kwargs)

    def _append_html(fpath, content, mode, file_exists, **kwargs):
        """Append to HTML files"""
        if isinstance(content, pd.DataFrame):
            # Convert DataFrame to HTML table
            content = content.to_html(**kwargs)

        return _append_text(fpath, content, mode, file_exists, **kwargs)

    def _append_markdown(fpath, content, mode, file_exists, **kwargs):
        """Append to Markdown files"""
        return _append_text(fpath, content, mode, file_exists, **kwargs)

    def _append_parquet(fpath, content, mode, file_exists, **kwargs):
        """Append to Parquet files"""
        if isinstance(content, dict):
            df_new = pd.DataFrame([content])
        elif isinstance(content, (list, tuple)):
            df_new = pd.DataFrame(content)
        elif isinstance(content, pd.DataFrame):
            df_new = content
        else:
            raise ValueError(
                "For Parquet files, content must be dict, list of dicts, or DataFrame"
            )

        if not file_exists or mode == "o":
            fsave(fpath, df_new, kind="parquet", **kwargs)
        else:
            df_existing = fload(fpath, kind="parquet", **kwargs)
            if mode == "i":
                df_combined = pd.concat([df_new, df_existing], ignore_index=True)
            else:
                df_combined = pd.concat([df_existing, df_new], ignore_index=True)
            fsave(fpath, df_combined, kind="parquet", **kwargs)

    def _append_feather(fpath, content, mode, file_exists, **kwargs):
        """Append to Feather files"""
        return _append_parquet(fpath, content, mode, file_exists, **kwargs)

    def _append_pickle(fpath, content, mode, file_exists, **kwargs):
        """Append to Pickle files (limited support)"""
        if not file_exists or mode == "o":
            fsave(fpath, content, kind="pkl", **kwargs)
        else:
            existing_data = fload(fpath, kind="pkl", **kwargs)

            if isinstance(existing_data, list) and isinstance(content, list):
                if mode == "i":
                    combined = content + existing_data
                else:
                    combined = existing_data + content
            elif isinstance(existing_data, dict) and isinstance(content, dict):
                if mode == "i":
                    combined = {**content, **existing_data}
                else:
                    combined = {**existing_data, **content}
            else:
                # Can't reliably combine other types
                print(
                    "Warning: Pickle append only supports lists and dicts. Using new content."
                )
                combined = content

            fsave(fpath, combined, kind="pkl", **kwargs)

    # ! main function: fappend
    if content is None:
        return False

    ext = os.path.splitext(fpath)[1].lower()
    file_exists = os.path.exists(fpath)

    try:
        if ext == ".txt":
            return _append_text(fpath, content, mode, file_exists, **kwargs)

        elif ext == ".csv":
            return _append_csv(fpath, content, mode, file_exists, **kwargs)

        elif ext in (".xlsx", ".xls", ".xlsm", ".xlsb"):
            return _append_excel(
                fpath, content, sheet_name, mode, file_exists, **kwargs
            )

        elif ext == ".json":
            return _append_json(fpath, content, mode, file_exists, **kwargs)

        elif ext == ".xml":
            return _append_xml(fpath, content, mode, file_exists, **kwargs)

        elif ext in (".yaml", ".yml"):
            return _append_yaml(fpath, content, mode, file_exists, **kwargs)

        elif ext == ".html":
            return _append_html(fpath, content, mode, file_exists, **kwargs)

        elif ext == ".md":
            return _append_markdown(fpath, content, mode, file_exists, **kwargs)

        elif ext in (".parquet", ".pq"):
            return _append_parquet(fpath, content, mode, file_exists, **kwargs)

        elif ext in (".feather", ".ftr"):
            return _append_feather(fpath, content, mode, file_exists, **kwargs)

        elif ext in (".pkl", ".pickle"):
            return _append_pickle(fpath, content, mode, file_exists, **kwargs)

        else:
            print(f"Warning: Unsupported file type for append: {ext}")
            # Try generic text append for unknown file types
            return _append_text(fpath, content, mode, file_exists, **kwargs)

    except Exception as e:
        print(f"Error appending to {fpath}: {str(e)}")
        return False

def filter_kwargs(kws, valid_kwargs):
    if isinstance(valid_kwargs, dict):
        kwargs_filtered = {
            key: value for key, value in kws.items() if key in valid_kwargs.keys()
        }
    elif isinstance(valid_kwargs, list):
        kwargs_filtered = {
            key: value for key, value in kws.items() if key in valid_kwargs
        }
    return kwargs_filtered

def fsearch(
    folder: str,
    search_pattern: str,
    search_type: str = "fuzzy",  # "text", "regex", "word", "fuzzy"
    case_sensitive: bool = False,
    file_patterns: Optional[List[str]] = None,
    exclude_dirs: Optional[List[str]] = None,
    exclude_files: Optional[List[str]] = None,
    max_file_size: int = 50 * 1024 * 1024,  # 10MB
    encoding: str = "utf-8",
    fallback_encodings: Optional[List[str]] = None,
    context_lines: int = 0,
    max_results: int = 1000,
    progress_callback: Optional[Callable[[str, int, int], None]] = None,
    result_callback: Optional[Callable[[Tuple], None]] = None,
    max_workers: int = None,
    include_archives: bool = False,
    follow_symlinks: bool = False,
    file_content_types: Optional[
        List[str]
    ] = None,  # ["text", "code", "document", "data"]
    min_line_length: int = 0,
    max_line_length: int = 10000,
    output_format: str = "detailed",  # "detailed", "minimal", "summary"
) -> Dict[str, Any]:
    """
    file search function with comprehensive features.

    Args:
        folder: Root directory to search
        search_pattern: Pattern to search for
        search_type: Type of search - "text", "regex", "word", "fuzzy"
        case_sensitive: Whether search should be case sensitive
        file_patterns: List of file patterns to include
        exclude_dirs: Directories to exclude from search
        exclude_files: File patterns to exclude
        max_file_size: Maximum file size to search (bytes)
        encoding: Primary encoding to try
        fallback_encodings: Fallback encodings if primary fails
        context_lines: Number of context lines around matches
        max_results: Maximum number of results to return
        progress_callback: Callback for progress (file_path, files_processed, total_files)
        result_callback: Callback for each match found
        max_workers: Maximum threads for parallel processing
        include_archives: Whether to search inside zip/gzip archives
        follow_symlinks: Whether to follow symbolic links
        file_content_types: Types of files to search - ["text", "code", "document", "data"]
        min_line_length: Minimum line length to consider
        max_line_length: Maximum line length to consider
        output_format: Output format - "detailed", "minimal", "summary"

    Returns:
        Dictionary with search results and metadata

    Usage:
        results = fsearch(
        folder=".../",
        search_pattern="echo",
        search_type="word",
    )
    print(
        f"Found {results['summary']['total_matches']} matches in {results['summary']['files_with_matches']} files"
    )
    """
    import re
    from pathlib import Path
    from concurrent.futures import ThreadPoolExecutor, as_completed

    # Initialize default parameters
    if fallback_encodings is None:
        fallback_encodings = ["utf-8", "latin-1", "cp1252", "iso-8859-1"]

    if exclude_dirs is None:
        exclude_dirs = [
            ".git",
            "__pycache__",
            ".idea",
            "node_modules",
            "venv",
            "env",
            ".svn",
            ".hg",
        ]

    if exclude_files is None:
        exclude_files = ["*.pyc", "*.class", "*.so", "*.dll", "*.exe", "*.bin"]

    if file_patterns is None:
        file_patterns = ["*"]

    if file_content_types is None:
        file_content_types = ["text", "code", "document"]

    if max_workers is None:
        max_workers = min(32, (os.cpu_count() or 1) + 4)

    # Compile search pattern based on type
    def compile_searcher():
        if search_type == "text":
            pattern = re.escape(search_pattern)
            if not case_sensitive:
                return re.compile(pattern, re.IGNORECASE)
            return re.compile(pattern)

        elif search_type == "regex":
            flags = 0 if case_sensitive else re.IGNORECASE
            return re.compile(search_pattern, flags)

        elif search_type == "word":
            pattern = r"\b" + re.escape(search_pattern) + r"\b"
            flags = 0 if case_sensitive else re.IGNORECASE
            return re.compile(pattern, flags)

        elif search_type == "fuzzy":
            # Simple fuzzy matching (all characters must appear in order)
            pattern = ".*".join(map(re.escape, search_pattern))
            flags = 0 if case_sensitive else re.IGNORECASE
            return re.compile(pattern, flags)

        else:
            raise ValueError(f"Unsupported search type: {search_type}")

    searcher = compile_searcher()

    # File type categorization
    def get_file_category(file_path: str) -> str:
        ext = Path(file_path).suffix.lower()

        # Code files
        code_extensions = {
            ".py",
            ".js",
            ".java",
            ".cpp",
            ".c",
            ".h",
            ".hpp",
            ".cs",
            ".php",
            ".rb",
            ".go",
            ".rs",
            ".swift",
            ".kt",
            ".scala",
            ".m",
            ".mm",
            ".r",
            ".pl",
            ".pm",
            ".tcl",
            ".lua",
            ".sql",
            ".sh",
            ".bash",
            ".zsh",
            ".fish",
        }

        # Document files
        doc_extensions = {
            ".txt",
            ".md",
            ".rst",
            ".tex",
            ".docx",
            ".odt",
            ".rtf",
            ".tex",
        }

        # Data files
        data_extensions = {
            ".csv",
            ".json",
            ".xml",
            ".yaml",
            ".yml",
            ".ini",
            ".cfg",
            ".conf",
            ".toml",
            ".properties",
            ".log",
        }

        if ext in code_extensions:
            return "code"
        elif ext in doc_extensions:
            return "document"
        elif ext in data_extensions:
            return "data"
        else:
            return "text"

    def should_search_file(file_path: str) -> bool:
        """Check if file should be searched based on multiple criteria."""
        import fnmatch

        try:
            # Check file size
            if max_file_size and os.path.getsize(file_path) > max_file_size:
                return False

            # Check file patterns
            if not any(

                fnmatch.fnmatch(os.path.basename(file_path), pattern)
                for pattern in file_patterns
            ):
                return False

            # Check excluded files
            if any(
                fnmatch.fnmatch(os.path.basename(file_path), pattern)
                for pattern in exclude_files
            ):
                return False

            # Check file content types
            file_category = get_file_category(file_path)
            if file_content_types and file_category not in file_content_types:
                # Additional MIME type check for unclassified files
                import mimetypes

                mime_type, _ = mimetypes.guess_type(file_path)
                if mime_type and not mime_type.startswith("text/"):
                    return False

            return True

        except (OSError, ValueError):
            return False

    def read_file_lines(file_path: str) -> Optional[List[str]]:
        """Read file with encoding fallback and archive support."""
        # Handle compressed files
        import gzip

        if file_path.endswith(".gz"):
            try:
                with gzip.open(file_path, "rt", encoding=encoding) as f:
                    return f.readlines()
            except Exception:
                pass

        # Handle zip files
        elif file_path.endswith(".zip") and include_archives:
            return read_zip_file_lines(file_path)

        # Regular files
        encodings = [encoding] + fallback_encodings
        for enc in encodings:
            try:
                with open(file_path, "r", encoding=enc) as f:
                    return f.readlines()
            except (UnicodeDecodeError, UnicodeError):
                continue
            except Exception as e:
                logging.debug(f"Error reading {file_path} with {enc}: {e}")
                continue
        return None

    def read_zip_file_lines(zip_path: str) -> Optional[List[str]]:
        """Extract and read files from zip archive."""
        import zipfile

        try:
            with zipfile.ZipFile(zip_path, "r") as zf:
                for file_info in zf.infolist():
                    if not file_info.is_dir() and should_search_file(
                        file_info.filename
                    ):
                        with zf.open(file_info.filename) as f:
                            content = f.read().decode("utf-8", errors="ignore")
                            return content.splitlines()
        except Exception as e:
            logging.debug(f"Error reading zip file {zip_path}: {e}")
        return None

    def get_context_lines(
        lines: List[str], line_num: int, context_lines: int
    ) -> Optional[List[str]]:
        """Get context lines around the match."""
        if context_lines == 0:
            return None

        start = max(0, line_num - 1 - context_lines)
        end = min(len(lines), line_num + context_lines)
        return [
            f"{start + i + 1}: {line.rstrip()}"
            for i, line in enumerate(lines[start:end])
        ]

    def search_in_file(file_path: str) -> List[Tuple]:
        """Search for pattern in a single file."""
        file_results = []

        try:
            lines = read_file_lines(file_path)
            if lines is None:
                return file_results

            for line_num, line in enumerate(lines, 1):
                # Skip lines that don't meet length criteria
                line_length = len(line.strip())
                if line_length < min_line_length or line_length > max_line_length:
                    continue

                # Search in line
                if searcher.search(line):
                    context = get_context_lines(lines, line_num, context_lines)

                    result = (
                        file_path,
                        line_num,
                        line.rstrip(),
                        context,
                        len(line.strip()),
                    )
                    file_results.append(result)

                    if result_callback:
                        result_callback(result)

        except Exception as e:
            logging.debug(f"Error searching in {file_path}: {e}")

        return file_results

    # Statistics and state
    stats = {
        "files_searched": 0,
        "files_with_matches": 0,
        "total_matches": 0,
        "total_files_found": 0,
        "search_time": 0,
        "errors": 0,
    }

    # Collect all files to search
    all_files = []
    try:
        for root, dirs, files in os.walk(folder, followlinks=follow_symlinks):
            # Remove excluded directories from traversal
            dirs[:] = [d for d in dirs if d not in exclude_dirs]

            for file in files:
                file_path = os.path.join(root, file)
                if should_search_file(file_path):
                    all_files.append(file_path)

    except Exception as e:
        logging.error(f"Error walking directory: {e}")
        stats["errors"] += 1

    stats["total_files_found"] = len(all_files)
    results = []

    # Search files (parallel or sequential)
    import time

    start_time = time.time()

    def search_wrapper(file_path):
        try:
            file_results = search_in_file(file_path)
            stats["files_searched"] += 1
            if file_results:
                stats["files_with_matches"] += 1
                stats["total_matches"] += len(file_results)

            if progress_callback:
                progress_callback(
                    file_path, stats["files_searched"], stats["total_files_found"]
                )

            return file_results
        except Exception as e:
            stats["errors"] += 1
            logging.debug(f"Error in search wrapper for {file_path}: {e}")
            return []

    if max_workers > 1:
        # Parallel search
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            future_to_file = {
                executor.submit(search_wrapper, file_path): file_path
                for file_path in all_files[: max_results * 10]
            }  # Limit for safety

            for future in as_completed(future_to_file):
                try:
                    file_results = future.result()
                    results.extend(file_results)
                    if len(results) >= max_results:
                        break
                except Exception as e:
                    stats["errors"] += 1
                    logging.debug(f"Error in parallel search: {e}")
    else:
        # Sequential search
        for file_path in all_files:
            if len(results) >= max_results:
                break
            file_results = search_wrapper(file_path)
            results.extend(file_results)

    stats["search_time"] = time.time() - start_time

    # Format results based on output format
    def format_results():
        if output_format == "minimal":
            return [(fp, ln, line) for fp, ln, line, ctx, _ in results[:max_results]]
        elif output_format == "summary":
            return {
                "matches_per_file": len([r for r in results if r[0] == fp])
                for fp in set(r[0] for r in results)
            }
        else:  # detailed
            return results[:max_results]

    # Final result assembly
    return {
        "results": format_results(),
        "statistics": stats,
        "search_parameters": {
            "folder": folder,
            "search_pattern": search_pattern,
            "search_type": search_type,
            "case_sensitive": case_sensitive,
            "max_results": max_results,
            "context_lines": context_lines,
        },
        "summary": {
            "files_searched": stats["files_searched"],
            "files_with_matches": stats["files_with_matches"],
            "total_matches": stats["total_matches"],
            "search_time_seconds": round(stats["search_time"], 2),
        },
    }


def notify(
    message: str,
    title: str = "Notification",
    app_name: str = "Notification",
    timeout: int = 8,
    sound: bool = True,
    log_path: str = None,
    print_console: bool = True,
    max_length: int = 500,
    verbose:bool = True
):
    """
    a cross-platform notification system.

    Features:
    - Native system notifications on Windows, macOS, Linux
    - Automatic fallback if system API fails
    - Optional sound alerts
    - Optional log file recording
    - Optional console display
    - Message length protection
    """
    import platform
    import subprocess
    import sys
    from plyer import notification
    from datetime import datetime
    # ----- Safety: prevent overly large notifications -----
    if len(message) > max_length:
        message = message[:max_length - 3] + "..."

    VERBOSE = True if verbose else False
    
    # -----------------------------
    # 1. Try Plyer (Primary Engine)
    # -----------------------------
    try:
        notification.notify(
            title=title,
            message=message,
            app_name=app_name,
            timeout=timeout
        )
        plyer_ok = True
    except Exception:
        plyer_ok = False

    # ----------------------------------------------------
    # 2. If Plyer fails ‚Üí fallback to OS-native commands
    # ----------------------------------------------------
    if not plyer_ok:
        system = platform.system().lower()

        try:
            if "windows" in system:
                # Windows Toast fallback (PowerShell)
                cmd = [
                    "powershell.exe",
                    "-Command",
                    f"[Windows.UI.Notifications.ToastNotificationManager, Windows.UI.Notifications, ContentType = WindowsRuntime];"
                    f"$template = [Windows.UI.Notifications.ToastNotificationManager]::GetTemplateContent([Windows.UI.Notifications.ToastTemplateType]::ToastText02);"
                    f"$textNodes = $template.GetElementsByTagName('text');"
                    f"$textNodes.Item(0).AppendChild($template.CreateTextNode('{title}')) | Out-Null;"
                    f"$textNodes.Item(1).AppendChild($template.CreateTextNode('{message}')) | Out-Null;"
                    f"$toast = [Windows.UI.Notifications.ToastNotification]::new($template);"
                    f"[Windows.UI.Notifications.ToastNotificationManager]::CreateToastNotifier('{app_name}').Show($toast);"
                ]
                subprocess.Popen(cmd, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)

            elif "darwin" in system:
                # macOS notification

                tn_path = shutil.which("terminal-notifier")
                if tn_path:
                    # use terminal-notifier (homebrew: brew install terminal-notifier)
                    cmd = [
                        tn_path,
                        "-title",
                        title,
                        "-message",
                        message,
                    ]
                    # optionally set the bundle id / app name
                    if app_name:
                        cmd += ["-appBundleID", app_name]
                    subprocess.run(cmd, check=False, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
                else:
                    # Fallback to osascript; use json.dumps to escape strings correctly
                    # Example: display notification "Hello" with title "T"
                    safe_msg = json.dumps(message)
                    safe_title = json.dumps(title)
                    script = f'display notification {safe_msg} with title {safe_title}'
                    subprocess.run(["osascript", "-e", script], check=False, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)

            elif "linux" in system:
                # Linux (libnotify)
                subprocess.run([
                    "notify-send",
                    title,
                    message
                ])
        except Exception as e:
            print(f"Failed to fallback to os-native engine: {e}") if VERBOSE else None 

    # -----------------------------
    # 3. Optional Sound Alert
    # -----------------------------
    if sound:
        try:
            if platform.system() == "Windows":
                import winsound
                winsound.MessageBeep(winsound.MB_ICONEXCLAMATION)
            elif platform.system() == "Darwin":
                subprocess.Popen(["afplay", "/System/Library/Sounds/Ping.aiff"])
            else:
                subprocess.Popen(["paplay", "/usr/share/sounds/freedesktop/stereo/complete.oga"])
        except Exception as e:
            print(f"Failed to play the sound: {e}") if VERBOSE else None

    # -----------------------------
    # 4. Optional Console Logging
    # -----------------------------
    if print_console:
        print(f"\nNOTIFY: {title}\n{message}\n") if VERBOSE else None

    # -----------------------------
    # 5. Optional File Logging
    # -----------------------------
    if log_path:
        try:
            with open(log_path, "a", encoding="utf-8") as f:
                f.write(f"[{datetime.now()}] {title}: {message}\n")
        except Exception as e:
            print(f"Failed to write log: {e}") if VERBOSE else None

str_space_speed = 'sapce cmp:parquet(0.56GB)<feather(1.14GB)<csv(6.55GB)<pkl=h5("26.09GB")\nsaving time: pkl=feather("13s")<parquet("35s")<h5("2m31s")<csv("58m")\nloading time: pkl("6.9s")<parquet("16.1s")=feather("15s")<h5("2m 53s")<csv(">>>30m")'

def fsave(
    fpath,
    content,
    how="overwrite",
    kind=None,
    font_name="Times",
    font_size=10,
    spacing=6,
    retry:int= 1,
    retry_delay:int=10,
    **kwargs,
):
    """
    Save content into a file with specified file type and formatting.
    Parameters:
        fpath (str): The file path where content will be saved.
        content (list of str or dict): The content to be saved, where each string represents a paragraph or a dictionary for tabular data.
        kind (str): The file type to save. Supported options: 'docx', 'txt', 'md', 'html', 'pdf', 'csv', 'xlsx', 'json', 'xml', 'yaml'.
        font_name (str): The font name for text formatting (only applicable for 'docx', 'html', and 'pdf').
        font_size (int): The font size for text formatting (only applicable for 'docx', 'html', and 'pdf').
        spacing (int): The space after each paragraph (only applicable for 'docx').
        **kwargs: Additional parameters for 'csv', 'xlsx', 'json', 'yaml' file types.
    Returns:
        None
    """
    # check if wether the parent directory exists?
    if not os.path.isdir(os.path.dirname(fpath)):
        mkdir(os.path.dirname(fpath))
    
    def save_content(fpath, content, mode="w", how="overwrite"):
        if "wri" in how.lower():
            with open(fpath, mode, encoding="utf-8") as file:
                file.write(content)
        elif "upd" in how.lower():
            fupdate(fpath, content=content)
        elif "app" in how.lower():
            fappend(fpath, content=content)

    def save_docx(fpath, content, font_name, font_size, spacing):
        import docx

        if isinstance(content, str):
            content = content.split(". ")
        doc = docx.Document()
        for i, paragraph_text in enumerate(content):
            paragraph = doc.add_paragraph()
            run = paragraph.add_run(paragraph_text)
            font = run.font
            font.name = font_name
            font.size = docx.shared.Pt(font_size)
            if i != len(content) - 1:  # Add spacing for all but the last paragraph
                paragraph.space_after = docx.shared.Pt(spacing)
        doc.save(fpath)

    def save_txt_md(fpath, content, sep="\n", mode="w"):
        # Ensure content is a single string
        if isinstance(content, list):
            content = sep.join(content)
        save_content(fpath, sep.join(content), mode)

    def save_html(fpath, content, font_name, font_size, mode="w"):
        html_content = "<html><body>"
        for paragraph_text in content:
            html_content += f'<p style="font-family:{font_name}; font-size:{font_size}px;">{paragraph_text}</p>'
        html_content += "</body></html>"
        save_content(fpath, html_content, mode)

        def write_heading(level, text):
            """Write formatted heading"""
            sizes = {
                1: font_size + 8,
                2: font_size + 6,
                3: font_size + 4,
                4: font_size + 2,
                5: font_size,
                6: font_size - 2,
            }
            safe_set_font("B")
            pdf.set_font_size(sizes.get(level, font_size + 4))
            pdf.cell(0, 10, text, ln=True)
            pdf.ln(line_space)
            pdf.set_font_size(font_size)
            safe_set_font("")

        def write_list_item(indent, text, marker):
            """Write formatted list item"""
            pdf._list_depth = max(pdf._list_depth, indent + 1)
            indent_size = 10 * pdf._list_depth
            pdf.cell(indent_size)

            # Handle ordered vs unordered lists
            if marker.isdigit():
                pdf.write(font_size / 2, f"{marker}. ")
            else:
                pdf.write(font_size / 2, "‚Ä¢ ")

            # Process the list item text
            write_formatted_text(text)
            pdf.ln(line_space / 2)

        def write_code_block(content):
            """Write formatted code block"""
            safe_set_font("")
            pdf.set_font("Courier", "", font_size - 1)
            pdf.set_fill_color(240, 240, 240)
            pdf.multi_cell(0, line_space, content, border=0, fill=True)
            pdf.ln(line_space)
            pdf.set_font(font_name, "", font_size)
            safe_set_font("")

        def draw_horizontal_rule():
            """Draw a horizontal rule"""
            y = pdf.get_y()
            pdf.line(margins[0], y + line_space / 2, pdf.w - margins[0], y + line_space / 2)
            pdf.ln(line_space * 2)

        def write_blockquote(text):
            """Write formatted blockquote"""
            pdf.set_left_margin(pdf.l_margin + 10)
            pdf.set_text_color(90, 90, 90)
            pdf.multi_cell(0, line_space, text)
            pdf.set_left_margin(margins[0])
            pdf.set_text_color(0, 0, 0)
            pdf.ln(line_space)

        # Document content
        if show_title and title:
            safe_set_font("B")
            pdf.set_font_size(font_size + 4)
            pdf.cell(0, 10, title, ln=True, align="C")
            pdf.ln(line_space * 2)
            pdf.set_font_size(font_size)
            safe_set_font()

        if content:
            content_list = [content] if isinstance(content, str) else content
            for paragraph in content_list:
                write_formatted_text(str(paragraph).strip())

        # Output
        if isinstance(fpath, (str, bytes, os.PathLike)):
            pdf.output(fpath, "F")
        elif isinstance(fpath, BytesIO):
            pdf.output(fpath)
        else:
            raise ValueError("fpath must be a file path or BytesIO object")

 
    def save_gmt(
        filepath: str,
        gene_sets: Union[Dict[str, List[str]], List[Dict]],
        default_description: str = "NA",
    ):
        """
        Save gene sets to a GMT file.

        Parameters
        ----------
        gene_sets : dict or list
            dict format:
                {"set_name": ["G1", "G2", ...]}
            list format:
                [{"name": "...", "description": "...", "genes": [...]}, ...]
        filepath : str
        default_description : str
            Used when no description is provided.
        """
        def _sanitize_token(token: str) -> str:
            """Remove illegal characters for GMT/GMX."""
            if token is None:
                return ""
            return str(token).replace("\t", " ").strip()
        def _ensure_list(obj):
            """Ensure obj is a list."""
            if obj is None:
                return []
            if isinstance(obj, list):
                return obj
            return [obj]
        if not filepath.endswith(".gmt"):
            filepath += ".gmt"

        with open(filepath, "w") as f:
            # dict format
            if isinstance(gene_sets, dict):
                for name, genes in gene_sets.items():
                    name = _sanitize_token(name)
                    desc = _sanitize_token(default_description)
                    genes = [_sanitize_token(g) for g in _ensure_list(genes)]
                    f.write("\t".join([name, desc] + genes) + "\n")

            # list of dict format
            elif isinstance(gene_sets, list):
                for item in gene_sets:
                    if "name" not in item:
                        raise ValueError("Missing 'name' field in one of the entries.")
                    name = _sanitize_token(item["name"])
                    desc = _sanitize_token(item.get("description", default_description))
                    genes = [_sanitize_token(g) for g in _ensure_list(item.get("genes"))]
                    f.write("\t".join([name, desc] + genes) + "\n")

            else:
                raise ValueError("gene_sets must be a dict or list of dicts.")

        return filepath


    def save_gmx(
        filepath: str,
        gene_sets: Union[Dict[str, List[str]], List[Dict]],
        default_description: str = "NA",
    ):
        """
        Save gene sets to a GMX file (matrix format).

        GMX format:
        - Each column = one gene set
        - Row 1 = set names
        - Row 2 = descriptions
        - Rows >=3 = genes (empty allowed)

        """
        def _sanitize_token(token: str) -> str:
            """Remove illegal characters for GMT/GMX."""
            if token is None:
                return ""
            return str(token).replace("\t", " ").strip()
        def _ensure_list(obj):
            """Ensure obj is a list."""
            if obj is None:
                return []
            if isinstance(obj, list):
                return obj
            return [obj]

        if not filepath.endswith(".gmx"):
            filepath += ".gmx"

        # Normalize input to list-of-dicts
        if isinstance(gene_sets, dict):
            gene_sets = [{"name": k, "description": default_description, "genes": v}
                        for k, v in gene_sets.items()]

        if not isinstance(gene_sets, list):
            raise ValueError("gene_sets must be dict or list-of-dict.")

        # Sanitize & normalize
        names = [_sanitize_token(gs["name"]) for gs in gene_sets]
        descs = [_sanitize_token(gs.get("description", default_description)) for gs in gene_sets]
        gene_lists = [(_ensure_list(gs.get("genes"))) for gs in gene_sets]

        # Find maximum gene column length
        max_len = max(len(gl) for gl in gene_lists)

        # Write GMX
        with open(filepath, "w") as f:
            # First row: names
            f.write("\t".join(names) + "\n")

            # Second row: descriptions
            f.write("\t".join(descs) + "\n")

            # More rows: genes
            for i in range(max_len):
                row = []
                for gl in gene_lists:
                    if i < len(gl):
                        row.append(_sanitize_token(gl[i]))
                    else:
                        row.append("")  # empty cell allowed
                f.write("\t".join(row) + "\n")

        return filepath
    
    def save_pdf(
        fpath,
        content=None,
        font_name="Helvetica",
        font_size=12,
        line_space=5,
        margins=(15, 15, 15),
        page_orientation="P",
        page_size="A4",
        custom_page_size=None,
        title=None,
        author=None,
        enable_page_numbers=True,
        toc:bool = False,
        show_line_numbers=True,
        pygments_style="monokai",
        auto_detect_links=True,
        auto_format_text=True,
        align="L",
        is_html:bool=True,
        show_border=False,
        show_title=True,
        header_text=None,
        footer_text=None,
        sanitize_text=True,
        text_color=None,
        markdown_extensions=None,
        markdown_extension_configs=None,
    ):
        """Save content to a PDF file using HTML conversion for better formatting."""

        from fpdf import FPDF,FontFace, TextStyle
        
        from io import BytesIO
        import markdown
        from fpdf.enums import XPos, YPos

        # Supported page sizes
        PAGE_SIZES = {
            "A3": (297, 420),
            "A4": (210, 297),
            "A5": (148, 210),
            "A6": (105, 148),
            "Letter": (216, 279),
            "Legal": (216, 356),
            "Tabloid": (279, 432),
        }

        # Default color scheme
        DEFAULT_COLORS = {
            "title": "#000000",
            "h1": "#000000",
            "h2": "#000000",
            "h3": "#000000",
            "h4": "#000000",
            "h5": "#000000",
            "h6": "#000000",
            "p": "#000000",
            "a": "#0645AD",
            "li": "#0B0552FF",
            "dd": "#0B0552FF",
            "blockquote":"#e9ff32",
            "code": "#000000",
            "quote": "#e9ff32",
        }
        # Process color settings
        
        colors = DEFAULT_COLORS.copy()
        if text_color is not None:
            if isinstance(text_color, str):
                colors.update({k: color2hex(text_color) for k in DEFAULT_COLORS})
            else:
                colors.update({k: color2hex(v) for k, v in text_color.items() if v})
        tag_styles={
            # Inline tags are FontFace instances :
            "a": FontFace(color="#00f", emphasis="UNDERLINE"),
            "b": FontFace(emphasis="BOLD"),
            "code": FontFace(family="Courier"),
            "del": FontFace(emphasis="STRIKETHROUGH"),
            "em": FontFace(emphasis="ITALICS"),
            "font": FontFace(),
            "i": FontFace(emphasis="ITALICS"),
            "s": FontFace(emphasis="STRIKETHROUGH"),
            "strong": FontFace(emphasis="BOLD"),
            "u": FontFace(emphasis="UNDERLINE"),
            "blockquote": TextStyle(color=colors["blockquote"], t_margin=3, b_margin=3),
            "center": TextStyle(t_margin=4 + 7 / 30),
            "dd": TextStyle(),
            "dt": TextStyle(),
            "title": TextStyle(
                        b_margin=0.4,
                        font_size_pt=font_size + 8,
                        t_margin=6, 
                        l_margin="Center",
                        font_style="B",
                    ),
            "h1": TextStyle(
                color=colors['h1'], l_margin="Center",font_style="B",b_margin=0.4, font_size_pt=font_size + 4, t_margin=5 + 834 / 900
            ),
            "h2": TextStyle(
                color=colors['h2'],font_style="B",b_margin=0.4, font_size_pt=font_size + 3, t_margin=5 + 453 / 900
            ),
            "h3": TextStyle(
                color=colors['h3'],font_style="B",b_margin=0.4, font_size_pt=font_size + 2, t_margin=5 + 199 / 900
            ),
            "h4": TextStyle(
                color=colors['h4'],font_style="B",b_margin=0.4, font_size_pt=font_size + 2, t_margin=5 + 72 / 900
            ),
            "h5": TextStyle(
                color=colors['h5'],font_style="B",b_margin=0.4, font_size_pt=font_size + 2, t_margin=5 - 55 / 900
            ),
            "h6": TextStyle(
                color=colors['h6'],font_style="B",b_margin=0.4, font_size_pt=font_size + 2, t_margin=5 - 182 / 900
            ),
            "li": TextStyle(l_margin=5, t_margin=2,color=colors["li"]),
            "p": TextStyle(color=colors['p']),
            "pre": TextStyle(t_margin=4 + 7 / 30, font_family="Courier"),
            "ol": TextStyle(t_margin=2,color=colors["li"]),
            "ul": TextStyle(t_margin=2,color=colors["li"]),
        }
        # Default markdown extensions
        if markdown_extensions is None: 
            markdown_extensions = [
                "extra",           # Adds tables, fenced code, abbreviations, and more
                "toc" if toc else None,             # Table of contents
                "nl2br",           # Convert newlines to <br>
                "sane_lists",      # Better list handling
                # "tables",          # Redundant if 'extra' is present, but safe
                "fenced_code",     # Code blocks (redundant in 'extra', but okay)
                # "codehilite",      # Syntax highlighting
                "md_in_html",      # Allow markdown in HTML blocks
                # "attr_list",       # Allow attributes on Markdown elements
                "admonition",      # Adds support for !!! note, !!! warning, etc.
                # "smarty",          # Convert quotes, dashes to typographically correct symbols
                "abbr",            # Abbreviations
                "def_list",        # Definition lists
                "footnotes",       # Footnote support
            ]

        if markdown_extension_configs is None:
            markdown_extension_configs = {
                "toc": {"permalink": True,         # Adds anchor links (üîó) next to headings
                        "toc_depth": "2-4",        # Include headings from H2 to H4
                        },
                "codehilite": {
                        "linenums": True,          # Show line numbers in code blocks
                        "guess_lang": False,       # Don't guess language
                        "pygments_style": pygments_style,  # Choose syntax theme (e.g., "default", "monokai", "friendly")
                        "noclasses": True,         # Use inline styles, better for PDF rendering
                        },
                "smarty": {"smart_quotes": True,
                        "smart_dashes": True,
                        },
                }

        try:
            # Try to add user fonts if available
            FALLBACK_FONTS=ls(
                    get_dir_font(),
                    ["ttf", "otf", "ttc"],
                    booster=1,
                    depth=3,
                    sort_by="size",
                    ascending=False,
                ).path.tolist()

        except:
            FALLBACK_FONTS = [ ]
            pass

        # Font loading logic
        builtin_fonts = ["Courier", "Helvetica", "Times", "Symbol", "ZapfDingbats"]
        if font_name not in builtin_fonts and not os.path.exists(font_name):
            font_name = FALLBACK_FONTS[
                strcmp(
                    font_name, [os.path.basename(i).split(".")[0] for i in FALLBACK_FONTS]
                )[1]
            ]

        class PDFWithHeaderFooter(FPDF):
            def __init__(
                self,
                orientation="P",
                format="A4",
                unit="mm",
                enable_page_numbers=True,
                footer_text=None,
                header_text=None,
                font_name="Helvetica",
                font_size=12,
            ):
                # Handle page size
                if isinstance(format, str):
                    format = PAGE_SIZES.get(format, PAGE_SIZES["A4"])
                elif isinstance(format, (tuple, list)) and len(format) == 2:
                    format = tuple(format)
                else:
                    format = PAGE_SIZES["A4"]

                super().__init__(orientation=orientation, unit=unit, format=format)
                self.enable_page_numbers = enable_page_numbers
                self.footer_text = footer_text
                self.header_text = header_text
                self._footer_font_size = font_size - 2
                self._header_font_size = font_size - 1
                self._default_font = "Helvetica"
                self._custom_font_alias = None

            def header(self):
                if self.header_text:
                    font = (
                        self._custom_font_alias
                        if self._custom_font_alias
                        else self._default_font
                    )
                    self.set_font(font, "", self._header_font_size)
                    self.cell(
                        0,
                        10,
                        self.header_text,
                        new_x=XPos.LMARGIN,
                        new_y=YPos.NEXT,
                        align="C",
                        border=0,
                    )

            def footer(self):
                self.set_y(-15)
                font = (
                    self._custom_font_alias
                    if self._custom_font_alias
                    else self._default_font
                )
                self.set_font(font, "", self._footer_font_size)

                if self.footer_text and self.enable_page_numbers:
                    # Left footer text cell, no line break
                    self.cell(
                        0,
                        10,
                        self.footer_text,
                        new_x=XPos.RIGHT,  # Move cursor to right edge after cell
                        new_y=YPos.TOP,  # Keep same line vertically
                        align="L",
                        border=0,
                    )
                    # Page number cell aligned right on same line
                    self.cell(
                        0,
                        10,
                        f"{self.page_no()}/{{nb}}",
                        new_x=XPos.LMARGIN,  # Move cursor back to left margin after cell (optional)
                        new_y=YPos.TOP,
                        align="R",
                        border=0,
                    )
                elif self.footer_text:
                    self.cell(
                        0,
                        10,
                        self.footer_text,
                        new_x=XPos.LMARGIN,
                        new_y=YPos.NEXT,
                        align="C",
                        border=0,
                    )
                elif self.enable_page_numbers:
                    self.cell(
                        0,
                        10,
                        f"{self.page_no()}/{{nb}}",
                        new_x=XPos.LMARGIN,
                        new_y=YPos.NEXT,
                        align="C",
                        border=0,
                    )

        # Create PDF instance
        pdf = PDFWithHeaderFooter(
            orientation=page_orientation,
            format=custom_page_size if custom_page_size else page_size,
            enable_page_numbers=enable_page_numbers,
            font_name=font_name,
            font_size=font_size,
            footer_text=footer_text,
            header_text=header_text,
        )

        # PDF setup
        pdf.set_auto_page_break(auto=True, margin=margins[2])
        pdf.set_margins(margins[0], margins[1], margins[2])
        pdf.alias_nb_pages()

        custom_font_loaded = False

        # Case 1: User provided a font path
        if os.path.exists(font_name):
            try:
                font_alias = "CustomFont"
                pdf.add_font(font_alias, "", font_name)
                # Try to add variants
                try:
                    pdf.add_font(font_alias, "B", font_name)
                except:
                    pass
                try:
                    pdf.add_font(font_alias, "I", font_name)
                except:
                    pass
                try:
                    pdf.add_font(font_alias, "BI", font_name)
                except:
                    pass
                pdf._custom_font_alias = font_alias
                font_name = font_alias
                custom_font_loaded = True
            except Exception as e:
                print(f"Warning: Couldn't load custom font {font_name}: {str(e)}")

        # Case 2: Not a built-in font and not a path - try fallback fonts
        if not custom_font_loaded and font_name not in builtin_fonts:
            for fallback_font in FALLBACK_FONTS:
                if os.path.exists(fallback_font):
                    try:
                        font_alias = "FallbackFont"
                        pdf.add_font(font_alias, "", fallback_font)
                        # Try to add variants
                        try:
                            pdf.add_font(font_alias, "B", fallback_font)
                        except:
                            pass
                        try:
                            pdf.add_font(font_alias, "I", fallback_font)
                        except:
                            pass
                        try:
                            pdf.add_font(font_alias, "BI", fallback_font)
                        except:
                            pass
                        pdf._custom_font_alias = font_alias
                        font_name = font_alias
                        custom_font_loaded = True
                        if run_once_within():
                            print(f"Using fallback font: {fallback_font}")
                        break
                    except Exception as e:
                        continue

        # Final fallback to Helvetica if no other font loaded
        if not custom_font_loaded:
            font_name = pdf._default_font

        # Now add the page after font is loaded
        pdf.add_page()
        pdf.set_font(font_name, "", font_size)

        if title:
            pdf.set_title(title)
        if author:
            pdf.set_author(author)

        def safe_set_font(style=""):
            """Safely set font with fallback if style not available"""
            try:
                pdf.set_font(font_name, style, font_size)
                pdf._current_style = style
            except:
                pdf.set_font(font_name, "", font_size)
                pdf._current_style = "" 
        # Output saving
        def _save_pdf(fpath):
            if isinstance(fpath, (str, bytes, os.PathLike)):
                pdf.output(fpath)
            elif isinstance(fpath, BytesIO):
                pdf.output(fpath)
            else:
                raise ValueError("fpath must be a file path or BytesIO object")
        def _sanitize_text(text):
            """Handle various Unicode characters that may cause PDF encoding issues"""
            if not isinstance(text, str):
                text = str(text)

            # First ensure proper UTF-8 encoding
            text = text.encode("utf-8", errors="replace").decode("utf-8")

            # Single character replacements
            char_replacements = {
                # Bullets
                "‚Ä¢": "*",  # Standard bullet
                "‚Ä£": "*",  # Triangular bullet
                "‚ÅÉ": "-",  # Hyphen bullet
                "‚ó¶": "*",  # White bullet
                "‚¶ø": "*",  # Circled bullet
                "‚Åå": "*",  # Reversed bullet
                "‚Åç": "*",  # Combine with reversed bullet
                "‚àô": "*",  # Bullet operator
                "‚òû": "->",  # Hand pointing
                "‚òõ": "->",  # Hand pointing
                "‚òü": "->",  # Hand pointing down
                "‚úì": "[Y]",  # Check mark
                "‚úî": "[Y]",  # Heavy check mark
                "‚úó": "[N]",  # Ballot X
                "‚úò": "[N]",  # Heavy ballot X
                "‚Äì": "-",  # En dash
                "‚Äî": "--",  # Em dash
                "‚Äú": '"',  # Left double quote
                "‚Äù": '"',  # Right double quote
                "‚Äò": "'",  # Left single quote
                "‚Äô": "'",  # Right single quote
                "¬´": '<=',  # Left angle quote
                "¬ª": '=>',  # Right angle quote
            }

            # Create translation table for single-character replacements
            translation_table = str.maketrans(char_replacements)
            text = text.translate(translation_table)

            # Multi-character replacements (handled separately)
            multi_replacements = {
                "*   ": "-   ",  # Convert asterisk bullets to hyphens
                "‚Ä¢   ": "-   ",  # Convert bullet points to hyphens
                "‚Ä£   ": "-   ",  # Convert triangular bullets to hyphens
            }

            for pattern, replacement in multi_replacements.items():
                text = text.replace(pattern, replacement)

            return text
        def _get_current_font_style():
            # Save current font settings
            current_font = pdf.font_family
            current_style = pdf.font_style
            current_size = pdf.font_size_pt
            return [current_font,current_style,current_size]
        def _restore_font_style(list_font_style:list=None):
            # Restore original font settings
            pdf.set_font(list_font_style[0], list_font_style[1], list_font_style[2])
        def write_formatted_line(line):
            """Helper function to process a single line with formatting"""
            # Enhanced pattern to support more markdown features
            parts = re.split(
                r"(\*\*\*.*?\*\*\*|\*\*.*?\*\*|__.*?__|\*.*?\*|_.*?_|`.*?`|~~.*?~~|"
                r"!?\[.*?\]\(.*?\)|https?://\S+|www\.\S+|^\s*[-*+]\s+|^\s*\d+\.\s+)",
                line,
            )

            for part in parts:
                if not part:
                    continue

                # Handle lists (unordered and ordered)
                if re.match(r"^\s*[-*+]\s+", part) or re.match(r"^\s*\d+\.\s+", part):
                    pdf.cell(10)  # Indent
                    pdf.write(
                        font_size / 2, "‚Ä¢ " if "*" in part or "-" in part else part
                    )
                    continue

                # Handle links [text](url)
                link_match = re.match(r"\[(.*?)\]\((.*?)\)", part)
                if link_match:
                    text, url = link_match.groups()
                    pdf.set_text_color(colors["a"])
                    tmp_font_style = _get_current_font_style()
                    safe_set_font("U")
                    pdf.write(font_size / 2, text + " ", link=url)
                    _restore_font_style(tmp_font_style)  # restore font style BEFORE resetting color
                    pdf.set_text_color(colors["p"])
                    continue

                # Handle strikethrough ~~text~~
                if part.startswith("~~") and part.endswith("~~") and len(part) > 3:
                    pdf.set_text_color(colors["p"])
                    y = pdf.get_y()
                    text_width = pdf.get_string_width(part[2:-2])
                    pdf.write(font_size / 2, part[2:-2] + " ")
                    pdf.line(
                        pdf.get_x() - text_width - 2,
                        y + font_size / 2,
                        pdf.get_x() - 2,
                        y + font_size / 2,
                    )
                    continue

                # Handle inline code `code`
                if part.startswith("`") and part.endswith("`") and len(part) > 1:
                    code_text = part[1:-1]
                    pdf.set_text_color(colors["code"])
                    pdf.set_fill_color(colors["quote"])

                    # Save current position
                    x = pdf.get_x()
                    y = pdf.get_y()

                    # Calculate width needed for the code block
                    text_width = pdf.get_string_width(f" {code_text} ")
                    available_width = pdf.w - pdf.r_margin - x

                    # Use multi_cell if text is wider than available space
                    if text_width > available_width:
                        pdf.multi_cell(
                            0,
                            font_size + 1,
                            f" {code_text} ",
                            border=0,
                            ln=True,
                            fill=True,
                            align="L",
                        )
                    else:
                        pdf.cell(
                            text_width,
                            font_size + 1,
                            f" {code_text} ",
                            ln=False,
                            fill=True,
                        )

                    pdf.set_text_color(colors["p"])
                    continue

                # Handle bold-italic ***text***
                if part.startswith("***") and part.endswith("***") and len(part) > 5:
                    tmp_font_style = _get_current_font_style()
                    safe_set_font("BI")
                    pdf.write(font_size / 2, part[3:-3] + " ")
                    _restore_font_style(tmp_font_style)  # restore font style
                    continue

                # Handle bold **text** or __text__
                if (part.startswith("**") and part.endswith("**")) or (
                    part.startswith("__") and part.endswith("__")
                ):
                    tmp_font_style = _get_current_font_style()
                    safe_set_font("B")
                    pdf.write(font_size / 2, part[2:-2] + " ")
                    _restore_font_style(tmp_font_style)  # restore font style
                    continue

                # Handle italic *text* or _text_
                if (part.startswith("*") and part.endswith("*")) or (
                    part.startswith("_") and part.endswith("_")
                ):
                    tmp_font_style = _get_current_font_style()
                    safe_set_font("I")
                    pdf.write(font_size / 2, part[1:-1] + " ")
                    _restore_font_style(tmp_font_style)  # restore font style
                    continue

                # Handle regular URLs
                if re.match(r"(https?://|www\.)\S+", part):
                    pdf.set_text_color(colors["a"])
                    tmp_font_style = _get_current_font_style()
                    safe_set_font("U")
                    url = part if part.startswith("http") else f"http://{part}"
                    pdf.write(font_size / 2, part + " ", link=url)
                    _restore_font_style(tmp_font_style)  # restore font style BEFORE resetting color
                    pdf.set_text_color(colors["p"])
                    continue

                # Normal text
                pdf.write(font_size / 2, part + " ")

        def write_formatted_text(text):
            if not text.strip():
                pdf.ln(line_space)
                return
            if sanitize_text:
                text = _sanitize_text(text)

            text = (
                str(text).encode("utf-8").decode("utf-8")
                if not isinstance(text, str)
                else text
            )

            # Handle multi-line content properly
            lines = text.split("\n")
            for line in lines:
                if not line.strip():  # Empty line
                    pdf.ln(line_space)
                    continue

                # HEADER DETECTION AND PROCESSING
                header_match = re.match(r'^(#{1,7})\s+(.*)', line,re.UNICODE)
                if header_match:
                    header_level = len(header_match.group(1))
                    header_text = header_match.group(2).strip()
                    # header_text = header_text.replace('\t', ' ').strip()

                    
                    # Save current font settings
                    current_font = pdf.font_family
                    current_style = pdf.font_style
                    current_size = pdf.font_size_pt
                    
                    # Set header style based on level
                    header_size = font_size + (8 - header_level) * 2  # h1: largest, h7: smallest
                    safe_set_font("B")
                    pdf.set_font_size(header_size)
                    # add bookmark
                    pdf.bookmark(title=header_text, level=header_level - 1)

                    # Write header text with formatting
                    write_formatted_line(header_text)
                    
                    # Restore original font settings
                    pdf.set_font(current_font, current_style, current_size)
                    pdf.ln(line_space * 1.5)  # Extra space after header
                    continue

                # Process regular line
                write_formatted_line(line)
                pdf.ln(line_space)
        
        # Document title
        if show_title and title:
            pdf.set_font(font_name, "B", font_size + 4)
            pdf.set_text_color(colors["title"])
            pdf.cell(0, 10, title, new_x=XPos.LMARGIN, new_y=YPos.NEXT, align="C")
        
        # Process content
        is_html=isa(content, "html") if is_html is None else is_html

        def render_as_markdown():
            content_list = [content] if isinstance(content, str) else content
            for paragraph in content_list:
                if sanitize_text:
                    paragraph = _sanitize_text(str(paragraph).strip())
                write_formatted_text(paragraph)
            _save_pdf(fpath)

        def render_as_html():
            # Convert content to string if it's not already
            if not isinstance(content, str):
                if isinstance(content, (list, tuple)):
                    content_str = "\n\n".join(str(item) for item in content)
                else:
                    content_str = str(content)
            else:
                content_str = content

            # Convert markdown to HTML
            md = markdown.Markdown(
                extensions=markdown_extensions,
                extension_configs=markdown_extension_configs,
                output_format="html5",
            )
            # Convert Markdown to HTML
            html_body = md.convert(content_str)

            # Insert TOC if needed
            if toc and hasattr(md, "toc"):
                if "[TOC]" in content_str:
                    html_content = content_str.replace("[TOC]", md.toc) + "\n\n" + html_body
                else:
                    html_content = md.toc + "\n\n" + html_body
            else:
                html_content = html_body  
            pdf.write_html(html_content, tag_styles=tag_styles)
            _save_pdf(fpath)
        if content:
            if not is_html:
                try:
                    render_as_markdown()
                except Exception as e:
                    print(f"[Markdown rendering failed]: {e}")
                    # Start new PDF document
                    pdf = PDFWithHeaderFooter(
                        orientation=page_orientation,
                        format=custom_page_size if custom_page_size else page_size,
                        enable_page_numbers=enable_page_numbers,
                        font_name=font_name,
                        font_size=font_size,
                        footer_text=footer_text,
                        header_text=header_text,
                    )
                    pdf.set_auto_page_break(auto=True, margin=margins[2])
                    pdf.set_margins(margins[0], margins[1], margins[2])
                    pdf.alias_nb_pages()
                    pdf.add_page()
                    pdf.set_font(font_name, "", font_size)
                    if show_title and title:
                        pdf.set_font(font_name, "B", font_size + 4)
                        pdf.set_text_color(colors["title"])
                        pdf.cell(0, 10, title, new_x=XPos.LMARGIN, new_y=YPos.NEXT, align="C")
                    render_as_html()
            else:
                render_as_html()

    def save_csv(fpath, data, **kwargs):
        # https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_csv.html

        verbose = kwargs.pop("verbose", False)
        if run_once_within():
            use_pd("to_csv", verbose=verbose)
        kwargs_csv = dict(
            path_or_buf=None,
            sep=",",
            na_rep="",
            float_format=None,
            columns=None,
            header=True,
            index=False, # ‰∏ç‰øùÂ≠òÁ¥¢Âºï,
            index_label=None,
            mode="w",
            encoding="UTF-8",
            compression="infer",
            quoting=None,
            quotechar='"',
            lineterminator=None,
            chunksize=None,
            date_format=None,
            doublequote=True,
            escapechar=None,
            decimal=".",
            errors="strict",
            storage_options=None,
        )
        kwargs_valid = filter_kwargs(kwargs, kwargs_csv)
        df = pd.DataFrame(data)
        # set index: False
        index = kwargs_valid.pop("index",False)
        df.to_csv(fpath, index=index, **kwargs_valid)
    @debug
    def save_xlsx(fpath, data, password=None,apply_format=None, **kwargs):
        import msoffcrypto
        from io import BytesIO
        import openpyxl
        from openpyxl import Workbook
        from openpyxl.worksheet.worksheet import Worksheet
        import pandas.io.formats.style

        verbose = kwargs.pop("verbose", False)
        sheet_name = kwargs.pop("sheet_name", "Sheet1")
        engine = kwargs.pop("engine", "xlsxwriter")
        mode = kwargs.pop("mode","a")
        if_sheet_exists = strcmp(kwargs.get("if_sheet_exists","overwrite"),['error', 'new', 'replace', 'overlay','overwrite'])[0]
        if_sheet_exists="overlay" if if_sheet_exists=="overwrite" else if_sheet_exists
        kwargs.pop("if_sheet_exists",None)
        if run_once_within():
            use_pd("to_excel", verbose=verbose)
 
        if apply_format is None:
            kwargs_format=list(extract_kwargs(format_excel).keys())[4:]
            apply_format=True if any([i in kwargs_format for i in kwargs]) else False

        if apply_format or any([
                isinstance(data, Worksheet), 
                isinstance(data, Workbook),
                isinstance(data, pd.io.formats.style.Styler)
             ]): 
            if isinstance(data, pd.io.formats.style.Styler):
                try:
                    with pd.ExcelWriter(fpath, mode=mode, engine="openpyxl") as writer:
                        # First save raw data
                        data.data.to_excel(writer, sheet_name=sheet_name, index=False)
                        # Then save style on top
                        data.to_excel(writer, sheet_name=sheet_name, index=False)
                except Exception as e:
                    print(f"Cannot save the styles, only saving raw data! Because: {e}")
                    with pd.ExcelWriter(fpath, mode=mode, engine="openpyxl", if_sheet_exists='overlay') as writer:
                        data.data.to_excel(writer, sheet_name=sheet_name, index=False)
                        data.to_excel(writer, sheet_name=sheet_name, index=False)

            format_excel(df=data, 
                        filename=fpath,
                        sheet_name=sheet_name,
                        #  password=password,
                        if_sheet_exists=if_sheet_exists,
                        mode=mode, 
                        engine=engine,
                        verbose=verbose,
                        **kwargs)
        else:
            # Remove non-relevant kwargs
            irrelevant_keys=list(extract_kwargs(format_excel).keys())[4:]
            [kwargs.pop(key, None) for key in irrelevant_keys]
            df = pd.DataFrame(data)
            # Write to Excel without password first
            temp_file = BytesIO()
            
            df.to_excel(
                temp_file,
                sheet_name=sheet_name,
                index=False,
                engine="xlsxwriter",
                **kwargs,
            )
            # If a password is provided, encrypt the file
            if password:
                temp_file.seek(0)
                office_file = msoffcrypto.OfficeFile(temp_file) 
                with open(fpath, "wb") as encrypted_file:
                    office_file.encrypt(outfile=encrypted_file,password=password)
            else: # Save the file without encryption if no password is provided
                try:
                    # Use ExcelWriter with append mode if the file exists
                    engine="openpyxl" if mode=="a" else "xlsxwriter"
                    if mode=="a":
                        with pd.ExcelWriter(fpath, engine=engine, mode=mode,if_sheet_exists=if_sheet_exists) as writer:
                            df.to_excel(writer, sheet_name=sheet_name, index=False, **kwargs)
                    else:
                        with pd.ExcelWriter(fpath, engine=engine, mode=mode) as writer:
                            df.to_excel(writer, sheet_name=sheet_name, index=False, **kwargs)
                except FileNotFoundError:
                    # If file doesn't exist, create a new one
                    df.to_excel(fpath, sheet_name=sheet_name, index=False, **kwargs)

    def save_ipynb(fpath, data, **kwargs):
        # Split the content by code fences to distinguish between code and markdown
        import nbformat

        parts = data.split("```")
        cells = []

        for i, part in enumerate(parts):
            if i % 2 == 0:
                # Even index: markdown content
                cells.append(nbformat.v4.new_markdown_cell(part.strip()))
            else:
                # Odd index: code content
                cells.append(nbformat.v4.new_code_cell(part.strip()))
        # Create a new notebook
        nb = nbformat.v4.new_notebook()
        nb["cells"] = cells
        # Write the notebook to a file
        with open(fpath, "w", encoding="utf-8") as ipynb_file:
            nbformat.write(nb, ipynb_file)

    def save_json(fpath_fname, var_dict_or_df):
        """
        Save Python objects, pandas DataFrames, or numpy arrays to a JSON file.
        Uses orjson if available (faster), otherwise falls back to json.
        """
        try:
            import orjson as _json
            use_orjson = True
        except ImportError:
            import json as _json
            use_orjson = False

        def _convert_js(data):
            if isinstance(data, pd.DataFrame):
                return data.to_dict(orient="list")
            elif isinstance(data, np.ndarray):
                return data.tolist()
            elif isinstance(data, dict):
                return {key: _convert_js(value) for key, value in data.items()}
            return data

        serializable_data = _convert_js(var_dict_or_df)

        if use_orjson:
            # orjson.dumps returns bytes, so we need to decode
            with open(fpath_fname, "wb") as f_json:
                f_json.write(_json.dumps(serializable_data, option=_json.OPT_INDENT_2))
        else:
            with open(fpath_fname, "w") as f_json:
                _json.dump(serializable_data, f_json, indent=4)

    def save_yaml(fpath, data, **kwargs):
        import yaml

        with open(fpath, "w") as file:
            yaml.dump(data, file, **kwargs)

    def save_xml(fpath, data):
        from lxml import etree

        root = etree.Element("root")
        if isinstance(data, dict):
            for key, val in data.items():
                child = etree.SubElement(root, key)
                child.text = str(val)
        else:
            raise ValueError("XML saving only supports dictionary data")
        tree = etree.ElementTree(root)
        tree.write(fpath, pretty_print=True, xml_declaration=True, encoding="UTF-8")

    def save_parquet(fpath: str, data: pd.DataFrame, **kwargs):
        engine = kwargs.pop(
            "engine", "auto"
        )  # autoÂÖàËØïpyarrow, ‰∏çË°åÂ∞±ËΩ¨‰∏∫fastparquet, {‚Äòauto‚Äô, ‚Äòpyarrow‚Äô, ‚Äòfastparquet‚Äô}
        compression = kwargs.pop(
            "compression", None
        )  # Use None for no compression. Supported options: ‚Äòsnappy‚Äô, ‚Äògzip‚Äô, ‚Äòbrotli‚Äô, ‚Äòlz4‚Äô, ‚Äòzstd‚Äô
        try:
            # Attempt to save with "pyarrow" if engine is set to "auto"
            data.to_parquet(fpath, engine=engine, compression=compression, **kwargs)
            print(
                f"DataFrame successfully saved to {fpath} with engine '{engine}' and {compression} compression."
            )
        except Exception as e:
            print(
                f"Error using with engine '{engine}' and {compression} compression: {e}"
            )
            if "Sparse" in str(e):
                try:
                    # Handle sparse data by converting columns to dense
                    print("Attempting to convert sparse columns to dense format...")
                    data = data.apply(
                        lambda x: (
                            x.sparse.to_dense() if pd.api.types.is_sparse(x) else x
                        )
                    )
                    save_parquet(fpath, data=data, **kwargs)
                except Exception as last_e:
                    print(
                        f"After converted sparse columns to dense format, Error using with engine '{engine}' and {compression} compression: {last_e}"
                    )
    def save_mplstyle(fpath, style_dict, auto_install=True):
        from matplotlib import rcParamsDefault
        
        # Only save rcParams that can be serialized to .mplstyle format
        # Skip complex objects like path effects, transforms, etc.
        skip_keys = {
            "figure.hooks",
            "path.effects",
            "path.sketch",
            "pdf.fonttype", 
            "svg.fonttype",
            "mathtext.fontset",
            "backend",
            "backend_fallback","docstring.hardcopy","tk.window_focus","webagg.address","webagg.open_in_browser","webagg.port","webagg.port_retries",
            "interactive",
            "toolbar",
            "webagg",
            "tk.",
            "macosx.",
            "qt.",
            "gtk.",
            "timezone",
            "savefig.directory",
            "figure.raise_window",
            "figure.max_open_warning",
            "docstring.",
            "date.epoch",
            "lines.dash_capstyle","lines.dash_joinstyle","lines.solid_joinstyle","lines.solid_capstyle",
        }
        
        # Filter only valid keys and skip problematic ones
        valid_keys = set(rcParamsDefault.keys())
        style_dict_clean = {}
        
        for k, v in style_dict.items():
            if (
                k not in valid_keys
                or k in skip_keys
                or any([i.lower() in k.lower() for i in skip_keys])
            ):
                continue
                
            # Only serialize simple types that work in .mplstyle files
            if isinstance(v, (str, int, float, bool)):
                style_dict_clean[k] = v
            elif isinstance(v, list):
                # Skip lists that contain non-serializable objects
                if all(isinstance(item, (str, int, float, bool)) for item in v):
                    style_dict_clean[k] = v
                else:
                    print(f"Warning: Skipping list with complex objects: {k}")
            else:
                # Skip other complex types
                print(f"Warning: Skipping non-serializable key: {k} (type: {type(v)})")
        
        if not str(fpath).endswith(".mplstyle"):
            fpath += ".mplstyle"
        
        # Write to file with proper formatting
        lines = []
        for k, v in style_dict_clean.items():
            if isinstance(v, bool):
                lines.append(f"{k}: {str(v).lower()}")
            elif isinstance(v, list):
                # Format lists properly for .mplstyle
                if len(v) == 0:
                    lines.append(f"{k}: ")
                else:
                    items = ", ".join(str(item) for item in v)
                    lines.append(f"{k}: {items}")
            else:
                lines.append(f"{k}: {v}")
        
        with open(fpath, "w") as f:
            f.write("\n".join(lines))
        
        # Auto-install logic remains the same
        if auto_install:
            import matplotlib as mpl
            stylelib_dir = os.path.join(mpl.get_configdir(), "stylelib")
            os.makedirs(stylelib_dir, exist_ok=True)
            dest_path = os.path.join(stylelib_dir, os.path.basename(fpath))
            cp(fpath, dest_path, overwrite=True)
            print(f"Style saved to: {fpath}")
            print(f"Installed to: {dest_path}")
            print("You can now use it via: plt.style.use('<filename_without_mplstyle>')")

    # main func
    if kind is None:
        _, kind = os.path.splitext(fpath)
        kind = kind.lower()

    kind = kind.lstrip(".").lower()
    if kind not in [
        "docx",
        "txt",
        "md",
        "html",
        "pdf",
        "csv",
        "xlsx",
        "json",
        "xml",
        "yaml",
        "ipynb",
        "mplstyle"
    ]:
        print(
            f"Warning:\n{kind} is not in the supported list ['docx', 'txt', 'md', 'html', 'pdf', 'csv', 'xlsx', 'json', 'xml', 'yaml']"
        )
    mode = kwargs.get("mode", "w")
    print(f"Saving '{kind}' => {fpath}")
    # print(f"Saving '{kind}' => {os.path.basename(fpath)}")
    # for i_try in tqdm(range(retry), desc=f"Saving '{os.path.basename(fpath)}'", ncols=100, bar_format='{desc} {bar} | {n_fmt}/{total_fmt}\n'):
    for i_try in range(retry):
        try:
            if kind == "docx" or kind == "doc":
                save_docx(fpath, content, font_name, font_size, spacing)
            elif kind == "txt":
                save_txt_md(fpath, content, sep="", mode=mode)
            elif kind == "md":
                save_txt_md(fpath, content, sep="", mode=mode)
            elif kind == "":
                save_txt_md(fpath, content, sep="", mode=mode)
            elif kind == "html":
                save_html(fpath, content, font_name, font_size)
            elif kind == "pdf":
                verbose = kwargs.pop("verbose", False)
                if verbose:
                    print('save_pdf(fpath,content=None,font_name="Â§ßÊ¶ÇÂêçÂ≠ó, ÊàñËÄÖÂ≠ó‰ΩìË∑ØÂæÑ,ÊàñËÄÖÂ≠ó‰ΩìÂêçÁß∞",font_size=12,line_space=5,margins=(15, 15, 15),page_orientation="P",page_size="A4",custom_page_size=None,title=None,author=None,enable_page_numbers=True,auto_detect_links=True,auto_format_text=True,align="L",show_border=False,show_title=True,header_text=None,footer_text=None)')
                save_pdf(fpath, content, font_name, font_size, **kwargs)
            elif kind == "csv":
                save_csv(fpath, content, **kwargs)
            elif kind == "xlsx":
                save_xlsx(fpath, content, **kwargs)
            elif kind == "json":
                save_json(fpath, content)
            elif kind == "xml":
                save_xml(fpath, content)
            elif kind == "yaml":
                save_yaml(fpath, content, **kwargs)
            elif kind == "ipynb":
                save_ipynb(fpath, content, **kwargs)
            elif kind == "mplstyle":
                save_mplstyle(fpath, content)
            elif kind.lower() in ["parquet", "pq", "big", "par"]:
                verbose = kwargs.pop("verbose", False)
                if verbose:
                    print(str_space_speed)
                    use_pd("to_parquet")
                    return None
                compression = kwargs.pop(
                    "compression", None
                )  # Use None for no compression. Supported options: ‚Äòsnappy‚Äô, ‚Äògzip‚Äô, ‚Äòbrotli‚Äô, ‚Äòlz4‚Äô, ‚Äòzstd‚Äô
                # fix the fpath ends
                _fpath, _ext = os.path.splitext(fpath)
                fpath = _fpath + _ext.replace(kind, "parquet")
                if compression is not None:
                    if not fpath.endswith(compression):
                        fpath = fpath + f".{compression}"
                save_parquet(fpath=fpath, data=content, compression=compression, **kwargs)
            elif kind.lower() in ["pkl", "pk", "pickle", "pick"]:
                # Pickle: Although not as efficient in terms of I/O speed and storage as Parquet or Feather,
                # Pickle is convenient if you want to preserve exact Python object types.
                verbose = kwargs.pop("verbose", False)
                if verbose:
                    print(str_space_speed)
                    use_pd("to_pickle")
                    return None
                _fpath, _ext = os.path.splitext(fpath)
                fpath = _fpath + _ext.replace(kind, "pkl")
                compression = kwargs.pop("compression", None)
                if compression is not None:
                    if not fpath.endswith(compression["method"]):
                        fpath = fpath + f".{compression['method']}"
                if isinstance(content, pd.DataFrame):
                    content.to_pickle(fpath, **kwargs)
                else:
                    try:
                        content = pd.DataFrame(content)
                        content.to_pickle(fpath, **kwargs)
                    except Exception as e:
                        try:
                            import pickle

                            with open(fpath, "wb") as f:
                                pickle.dump(content, f)
                            print("done!", fpath)
                        except Exception as e:
                            raise ValueError(
                                f"content is not a DataFrame, cannot be saved as a 'pkl' format: {e}"
                            )
            elif kind.lower() in ["fea", "feather", "ft", "fe", "feat", "fether"]:
                # Feather: The Feather format, based on Apache Arrow, is designed for fast I/O operations. It's
                # optimized for data analytics tasks and is especially fast when working with Pandas.

                verbose = kwargs.pop("verbose", False)
                if verbose:
                    print(str_space_speed)
                    use_pd("to_feather")
                    return None
                _fpath, _ext = os.path.splitext(fpath)
                fpath = _fpath + _ext.replace(kind, "feather")
                if isinstance(content, pd.DataFrame):
                    content.to_feather(fpath, **kwargs)
                else:
                    try:
                        print("trying to convert it as a DataFrame...")
                        content = pd.DataFrame(content)
                        content.to_feather(fpath, **kwargs)
                    except Exception as e:
                        raise ValueError(
                            f"content is not a DataFrame, cannot be saved as a 'pkl' format: {e}"
                        )
            elif kind.lower() in ["gmt"]:
                return save_gmt(fpath,content)
            elif kind.lower() in ["gmx"]:
                return save_gmx(fpath,content)
            elif kind.lower() in ["hd", "hdf", "h", "h5"]:
                # particularly useful for large datasets and can handle complex data structures
                verbose = kwargs.pop("verbose", False)
                if verbose:
                    print(str_space_speed)
                    use_pd("to_hdf")
                _fpath, _ext = os.path.splitext(fpath)
                fpath = _fpath + _ext.replace(kind, "h5")
                compression = kwargs.pop("compression", None)
                if compression is not None:
                    if not fpath.endswith(compression):
                        fpath = fpath + f".{compression}"
                if isinstance(content, pd.DataFrame):
                    content.to_hdf(fpath, key="content", **kwargs)
                else:
                    try:
                        print("trying to convert it as a DataFrame...")
                        content = pd.DataFrame(content)
                        content.to_hdf(fpath, **kwargs)
                    except Exception as e:
                        raise ValueError(
                            f"content is not a DataFrame, cannot be saved as a 'pkl' format: {e}"
                        )
            else:
                from . import netfinder

                try:
                    netfinder.downloader(url=content, dir_save=dirname(fpath), kind=kind)
                except:
                    print(
                        f"Error:\n{kind} is not in the supported list ['docx', 'txt', 'md', 'html', 'pdf', 'csv', 'xlsx', 'json', 'xml', 'yaml']"
                    )
            return
        except Exception as e:
            print(f" ‚Äî retrying in {retry_delay}s ({i_try+1}/{retry}): {e}")
            traceback.print_exc()
            time.sleep(retry_delay) 
    print("All retry attempts failed.")


def addpath(fpath):
    sys.path.insert(0, dir)

def _flist(fpath, contains="all"):
    all_files = [
        os.path.join(fpath, f)
        for f in os.listdir(fpath)
        if os.path.isfile(os.path.join(fpath, f))
    ]
    if isinstance(contains, list):
        filt_files = []
        for filter_ in contains:
            filt_files.extend(_flist(fpath, filter_))
        return filt_files
    else:
        if "all" in contains.lower():
            return all_files
        else:
            filt_files = [f for f in all_files if isa(f, contains)]
            return filt_files


def sort_kind(df, by="name", ascending=True):
    if df[by].dtype == "object":  # Check if the column contains string values
        if ascending:
            sorted_index = df[by].str.lower().argsort()
        else:
            sorted_index = df[by].str.lower().argsort()[::-1]
    else:
        if ascending:
            sorted_index = df[by].argsort()
        else:
            sorted_index = df[by].argsort()[::-1]
    sorted_df = df.iloc[sorted_index].reset_index(drop=True)
    return sorted_df


def contains_wildcards(p: str) -> bool:
    """Check if string contains fnmatch-style wildcards (*, ?, [])."""
    try:
        return bool(re.search(r"[\*\?\[\]]", p))
    except:
        return False

def contains_regex_special(p: str) -> bool:
    """Check if string contains regex special characters (excluding fnmatch)."""
    try:
        return bool(re.search(r"[.^$+{}\\|()]", p))
    except:
        return False

def is_simple_string(p: str) -> bool:
    """Return True if string has no regex specials and no fnmatch wildcards."""
    try:
        return not (contains_wildcards(p) or contains_regex_special(p))
    except:
        return False

def is_pattern(p) -> bool:
    """Return True if input is a compiled regex or a string with special pattern chars."""
    if isinstance(p, RegexPattern):
        return True
    if isinstance(p, str):
        # Single punctuation characters should NOT be treated as patterns
        if len(p) == 1 and p in '.,!?;|:()[]{}"\'<>/-_=+*&^%$#@!~`':
            return False
        return contains_wildcards(p) or contains_regex_special(p)
    return False

def is_nan(x):
    """
    Returns True if x is considered "missing" or "empty":
    - np.nan, pd.NaT, None
    - Empty strings (all whitespace)
    - Empty containers (list, tuple, set, dict)
    Otherwise returns False.
    """
    # If x is array-like, vectorize
    if isinstance(x, (list, tuple, np.ndarray, pd.Series)):
        arr = np.array(x, dtype=object)
        vcheck = np.vectorize(is_nan)  # vectorized scalar check
        return vcheck(arr)
    # Check None
    if x is None:
        return True

    # Check NaN and NaT (works for floats, timestamps, etc)
    if isinstance(x, float) and np.isnan(x):
        return True
    if isinstance(x, (pd._libs.tslibs.nattype.NaTType, pd.Timestamp)) and pd.isna(x):
        return True

    # Check empty strings (strip whitespace)
    if isinstance(x, str) and x.strip() == "":
        return True

    # Check empty containers: list, tuple, set, dict
    if isinstance(x, (list, tuple, set, dict)) and len(x) == 0:
        return True
    return False

def is_false(x):
    """
    Returns True if x is considered False-like:
    - False
    - None
    - Numeric zero
    - Empty strings (all whitespace)
    - Empty containers (list, tuple, set, dict)
    Otherwise returns False.
    """
    # If x is array-like, vectorize
    if isinstance(x, (list, tuple, np.ndarray, pd.Series)):
        arr = np.array(x, dtype=object)
        vcheck = np.vectorize(is_false)  # vectorized scalar check
        return vcheck(arr)

    # scalar checks
    if x is False:
        return True
    if x is None:
        return True
    if isinstance(x, (int, float)) and x == 0:
        return True
    if isinstance(x, str) and x.strip() == "":
        return True
    if isinstance(x, (list, tuple, set, dict)) and len(x) == 0:
        return True

    return False

def is_date(value, errors='raise'):
    """
    Check if value is a date or can be parsed as a date (without time).
    """
    if is_nan(value) or isinstance(value,(float,int)):
        return False 
    from datetime import datetime, date as dt_date, time as dt_time
    if isinstance(value, dt_date) and not isinstance(value, datetime):
        return True
    if isinstance(value, datetime):
        return value.time() == dt_time(0, 0) 
    if value is None or (isinstance(value, str) and value.strip() == ""):
        return False
    try:
        parsed = pd.to_datetime(value, errors=errors)
        # A pure date will have time at midnight
        return parsed.time() == dt_time(0, 0)
    except (ValueError, TypeError):
        return False

def is_time(value, errors='raise'):
    """
    Check if value is a time or can be parsed as a time (without date).
    Supports formats like '14:30:00', '2 pm', 'noon', 'midnight'.
    """
    if is_nan(value) or isinstance(value,(float,int)):
        return False 
    from datetime import datetime, date as dt_date, time as dt_time
    if isinstance(value, dt_time):
        return True
    if isinstance(value, datetime):
        return value.date() == datetime(1900, 1, 1).date()    
    if value is None or (isinstance(value, str) and value.strip() == ""):
        return False    # Common keywords for time
    if isinstance(value, str) and value.strip().lower() in {"noon", "midnight"}:
        return True    # Try parsing as time
    try:
        # Pandas will add today's date ‚Äî detect if original looks like time
        parsed = pd.to_datetime(value, errors=errors)
        if parsed.date() == datetime.today().date() or parsed.date() == datetime(1900, 1, 1).date():
            return True
    except (ValueError, TypeError):
        pass    # Regex fallback for HH:MM or HH:MM:SS
    if isinstance(value, str) and re.fullmatch(r"\d{1,2}:\d{2}(:\d{2})?", value.strip()):
        return True    
    return False

def is_datetime(value, errors='raise'):
    """
    Check if value is a datetime or can be parsed as a datetime (date + time).
    Accepts timestamps, strings, and datetime-like objects.
    """
    if is_nan(value) or isinstance(value,(float,int)):
        return False 
    from datetime import datetime, date as dt_date, time as dt_time
    if isinstance(value, datetime):
        return True
    if isinstance(value, (dt_date, dt_time)):
        return False  # pure date/time is not full datetime    
    if value is None or (isinstance(value, str) and value.strip() == ""):
        return False    
    try:
        pd.to_datetime(value, errors=errors)
        return True
    except (ValueError, TypeError):
        return False

def is_phonenumber(
    raw_phone: str,
    return_obj: bool = False,
    regions: Union[str, List[str]] = ["de", "cn"],
    allow_extractions: bool = True,
    strict: bool = False,
    try_common_prefixes: bool = True,
) -> Union[bool, str, None]:
    """
    Smart phone number validation with multi-region support.

    Features:
    - Validates numbers in any format while preserving original formatting
    - Supports single region or multiple regions for validation
    - Extracts numbers from text containing other content
    - Handles numbers with extensions
    - Strict mode for exact match validation
    - Automatic handling of common international prefixes

    Parameters:
        raw_phone: Input string containing phone number
        return_obj: Return original string if valid instead of boolean
        regions: Region(s) to validate against (e.g., 'DE', ['US', 'CN', 'DE'])
        allow_extractions: Attempt to extract number from text if direct parse fails
        strict: Require exact match of entire input string
        try_common_prefixes: Attempt to fix common prefix issues (00, +, etc.)

    Returns:
        bool: True if valid phone number found (when return_obj=False)
        str: Original valid phone string (when return_obj=True)
        None: If no valid number found
    """
    try:
        import phonenumbers
        from phonenumbers import PhoneNumberMatcher, NumberParseException, PhoneNumber
    except ImportError:
        raise ImportError(
            "The 'phonenumbers' package is required. Install with 'pip install phonenumbers'"
        )

    # Clean input while preserving original for return
    cleaned = raw_phone.strip()
    if not cleaned:
        return False if not return_obj else None

    # Normalize regions parameter
    region_list = []
    if regions:
        if isinstance(regions, str):
            region_list = [regions.upper()]
        else:
            region_list = [r.upper() for r in regions if isinstance(r, str)]

    # Helper function to validate a parsed number
    def validate_number(num: PhoneNumber) -> bool:
        """Validate number with additional checks"""
        return phonenumbers.is_valid_number(num)

    # Helper function to try parsing with multiple regions
    def try_parse_with_regions(
        number_str: str, regions: List[str]
    ) -> Tuple[Optional[PhoneNumber], Optional[str]]:
        """Try parsing with multiple regions, return first valid parse"""
        # First try without specific region (international format)
        try:
            parsed = phonenumbers.parse(number_str, None)
            if validate_number(parsed):
                return parsed, None
        except NumberParseException:
            pass

        # Try with each specified region
        for region in regions:
            try:
                parsed = phonenumbers.parse(number_str, region)
                if validate_number(parsed):
                    return parsed, region
            except NumberParseException:
                continue

        return None, None

    # Helper function to extract numbers from text
    def extract_from_text(text: str, regions: List[str]) -> Optional[str]:
        """Extract phone numbers from text using multiple regions"""
        # First try without specific region
        try:
            for match in PhoneNumberMatcher(text, None):
                if validate_number(match.number):
                    if not strict or (
                        strict and match.start == 0 and match.end == len(text)
                    ):
                        return match.raw_string
        except Exception:
            pass

        # Try with each specified region
        for region in regions:
            try:
                for match in PhoneNumberMatcher(text, region):
                    if validate_number(match.number):
                        if not strict or (
                            strict and match.start == 0 and match.end == len(text)
                        ):
                            return match.raw_string
            except Exception:
                continue

        return None

    # Stage 1: Direct parsing with region list
    parsed, used_region = try_parse_with_regions(cleaned, region_list)
    if parsed:
        return cleaned if return_obj else True

    # Stage 2: Try extraction from text
    if allow_extractions:
        extracted = extract_from_text(cleaned, region_list)
        if extracted:
            return extracted if return_obj else True

    # Stage 3: Try common prefix adjustments
    if try_common_prefixes:
        # Handle double zero prefix
        if cleaned.startswith("00"):
            modified = "+" + cleaned[2:].lstrip()
            parsed, _ = try_parse_with_regions(modified, region_list)
            if parsed:
                return cleaned if return_obj else True

        # Handle double plus prefix
        if cleaned.startswith("++"):
            modified = "+" + cleaned[2:].lstrip()
            parsed, _ = try_parse_with_regions(modified, region_list)
            if parsed:
                return cleaned if return_obj else True

        # Handle missing plus with known regions
        if region_list and not cleaned.startswith("+"):
            for region in region_list:
                try:
                    country_code = phonenumbers.country_code_for_region(region)
                    modified = f"+{country_code}{cleaned.lstrip('0')}"
                    parsed, _ = try_parse_with_regions(modified, region_list)
                    if parsed:
                        return cleaned if return_obj else True
                except KeyError:
                    continue

    # Stage 4: Try removing non-digit characters for strict formats
    if not strict:
        digits_only = re.sub(r"[^\d+]", "", cleaned)
        if digits_only != cleaned:
            parsed, _ = try_parse_with_regions(digits_only, region_list)
            if parsed:
                return cleaned if return_obj else True

    return False if not return_obj else None

def isa(content, kind, errors='coerce'):
    """
    content, kind='img'
    kinds file paths based on the specified kind.
    Args:
        content (str): Path to the file.
        kind (str): kind of file to kind. Default is 'img' for images. Other options include 'doc' for documents,
                    'zip' for ZIP archives, and 'other' for other types of files.
    Returns:
        bool: True if the file matches the kind, False otherwise.
    """
    if is_nan(content):
        return False 

    def is_image(fpath):
        """
        Determine if a given file is an image based on MIME type and file extension.

        Args:
            fpath (str): Path to the file.

        Returns:
            bool: True if the file is a recognized image, False otherwise.
        """
        from PIL import Image

        if isinstance(fpath, str):
            import mimetypes

            # Known image MIME types
            image_mime_types = {
                "image/jpeg",
                "image/png",
                "image/gif",
                "image/bmp",
                "image/webp",
                "image/tiff",
                "image/x-icon",
                "image/svg+xml",
                "image/heic",
                "image/heif",
            }

            # Known image file extensions
            image_extensions = {
                ".jpg",
                ".jpeg",
                ".png",
                ".gif",
                ".bmp",
                ".webp",
                ".tif",
                ".tiff",
                ".ico",
                ".svg",
                ".heic",
                ".heif",
                ".fig",
                ".jpg",
            }

            # Get MIME type using mimetypes
            mime_type, _ = mimetypes.guess_type(fpath)

            # Check MIME type
            if mime_type in image_mime_types:
                return True

            # Fallback: Check file extension
            ext = os.path.splitext(fpath)[
                -1
            ].lower()  # Get the file extension and ensure lowercase
            if ext in image_extensions:
                return True

            return False

        elif isinstance(fpath, Image.Image):
            # If the input is a PIL Image object
            return True

        return False


    def is_video(fpath):
        """
        Determine if a given file is a video based on MIME type and file extension.

        Args:
            fpath (str): Path to the file.

        Returns:
            bool: True if the file is a recognized video, False otherwise.
        """
        import mimetypes

        # Known video MIME types
        video_mime_types = {
            "video/mp4",
            "video/quicktime",
            "video/x-msvideo",
            "video/x-matroska",
            "video/x-flv",
            "video/webm",
            "video/ogg",
            "video/x-ms-wmv",
            "video/x-mpeg",
            "video/3gpp",
            "video/avi",
            "video/mpeg",
            "video/x-mpeg2",
            "video/x-ms-asf",
        }

        # Known video file extensions
        video_extensions = {
            ".mp4",
            ".mov",
            ".avi",
            ".mkv",
            ".flv",
            ".webm",
            ".ogv",
            ".wmv",
            ".mpg",
            ".mpeg",
            ".3gp",
            ".mpeg2",
            ".asf",
            ".ts",
            ".m4v",
            ".divx",
        }

        # Get MIME type using mimetypes
        mime_type, _ = mimetypes.guess_type(fpath)

        # Check MIME type
        if mime_type in video_mime_types:
            return True

        # Fallback: Check file extension
        ext = os.path.splitext(fpath)[
            -1
        ].lower()  # Get the file extension and ensure lowercase
        if ext in video_extensions:
            return True

        return False


    def is_document(fpath):
        """
        Determine if a given file is a document based on MIME type and file extension.

        Args:
            fpath (str): Path to the file.

        Returns:
            bool: True if the file is a recognized document, False otherwise.
        """
        import mimetypes

        # Define known MIME types for documents
        document_mime_types = {
            "text/",
            "application/pdf",
            "application/msword",
            "application/vnd.openxmlformats-officedocument.wordprocessingml.document",
            "application/vnd.ms-excel",
            "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
            "application/vnd.ms-powerpoint",
            "application/vnd.openxmlformats-officedocument.presentationml.presentation",
            "application/rtf",
            "application/x-latex",
            "application/vnd.oasis.opendocument.text",
            "application/vnd.oasis.opendocument.spreadsheet",
            "application/vnd.oasis.opendocument.presentation",
        }

        # Define extensions for fallback
        document_extensions = {
            ".txt",
            ".log",
            ".csv",
            ".json",
            ".xml",
            ".pdf",
            ".doc",
            ".docx",
            ".xls",
            ".xlsx",
            ".ppt",
            ".pptx",
            ".odt",
            ".ods",
            ".odp",
            ".rtf",
            ".tex",
        }

        # Get MIME type
        mime_type, _ = mimetypes.guess_type(fpath)

        # Check MIME type
        if mime_type and any(
            mime_type.startswith(doc_type) for doc_type in document_mime_types
        ):
            return True

        # Fallback: Check file extension
        ext = os.path.splitext(fpath)[
            -1
        ].lower()  # Get the extension, ensure it's lowercase
        if ext in document_extensions:
            return True

        return False

    def is_office_file(fpath):
        """
        Determine if a given file is an Office document (Microsoft Office, Apple Keynote, or WPS Office).

        Args:
            fpath (str): Path to the file.

        Returns:
            bool: True if the file is an Office-type file, False otherwise.
        """
        import mimetypes
        office_mime_keywords = [
            "application/msword",
            "application/vnd.ms-",
            "application/vnd.openxmlformats-officedocument",
            "application/vnd.apple.keynote",
            "application/vnd.kingsoft",
        ]

        office_extensions = {
            # Microsoft Office
            ".doc", ".docx", ".xls", ".xlsx", ".ppt", ".pptx", ".csv",
            # WPS Office (Kingsoft)
            ".wps", ".et", ".dps",
            # Apple iWork
            ".key", ".numbers", ".pages",
        }

        mime_type, _ = mimetypes.guess_type(fpath)

        if mime_type and any(keyword in mime_type for keyword in office_mime_keywords):
            return True

        ext = os.path.splitext(fpath)[-1].lower()
        return ext in office_extensions
    def is_audio(fpath):
        """
        Determine if a given file is an audio file based on MIME type and file extension.

        Args:
            fpath (str): Path to the file.

        Returns:
            bool: True if the file is a recognized audio file, False otherwise.
        """
        import mimetypes

        # Known audio MIME types
        audio_mime_types = {
            "audio/mpeg",
            "audio/wav",
            "audio/ogg",
            "audio/aac",
            "audio/flac",
            "audio/midi",
            "audio/x-midi",
            "audio/x-wav",
            "audio/x-flac",
            "audio/pcm",
            "audio/x-aiff",
            "audio/x-m4a",
        }

        # Known audio file extensions
        audio_extensions = {
            ".mp3",
            ".wav",
            ".ogg",
            ".aac",
            ".flac",
            ".midi",
            ".m4a",
            ".aiff",
            ".pcm",
            ".wma",
            ".ape",
            ".alac",
            ".opus",
        }

        # Get MIME type using mimetypes
        mime_type, _ = mimetypes.guess_type(fpath)

        # Check MIME type
        if mime_type in audio_mime_types:
            return True

        # Fallback: Check file extension
        ext = os.path.splitext(fpath)[
            -1
        ].lower()  # Get the file extension and ensure lowercase
        if ext in audio_extensions:
            return True

        return False


    def is_code(fpath):
        """
        Determine if a given file is a code file based on file extension and optionally MIME type.

        Args:
            fpath (str): Path to the file.
            check_mime (bool): Whether to perform a MIME type check in addition to file extension check.

        Returns:
            bool: True if the file is a recognized code file, False otherwise.
        """
        # Known programming and scripting file extensions
        code_extensions = {
            ".m",
            ".py",
            ".ipynb",
            ".js",
            ".html",
            ".css",
            ".java",
            ".cpp",
            ".h",
            ".cs",
            ".go",
            ".rs",
            ".sh",
            ".rb",
            ".swift",
            ".ts",
            ".json",
            ".xml",
            ".yaml",
            ".toml",
            ".bash",
            ".r",
        }

        # Check file extension
        ext = os.path.splitext(fpath)[-1].lower()
        if ext in code_extensions:
            return True
        return False
    def is_num(s):
        """
        Check if a string can be converted to a number (int or float).
        Parameters:
        - s (str): The string to check.
        Returns:
        - bool: True if the string can be converted to a number, False otherwise.
        """
        try:
            float(s)  # Try converting the string to a float
            return True
        except ValueError:
            return False

    def is_zip(fpath):
        import mimetypes

        mime_type, _ = mimetypes.guess_type(fpath)
        if mime_type == "application/zip":
            return True
        else:
            return False
    def is_text(s):
        has_alpha = any(char.isalpha() for char in s)
        has_non_alpha = any(not char.isalpha() for char in s)
        # no_special = not re.search(r'[^A-Za-z0-9\s]', s)
        return has_alpha and has_non_alpha
    
    # Main ISA FUNC
    if any([kind.lower() in i.lower() for i in ["nan","empty","blank","nat","pd.nat","np.nan"]]):
        return is_nan(content)
    elif "img" in kind.lower() or "image" in kind.lower():
        return is_image(content)
    elif any([i in kind.lower() for i in ["vid","mov","film"] ]):
        return is_video(content)
    elif any([i in kind.lower() for i in ["offi","ms","microsoft", "word","excel","powerpoint","ppt","doc","document","documents"] ]):
        return is_office_file(content)
    elif "aud" in kind.lower():
        return is_audio(content)
    elif "doc" in kind.lower():
        return is_document(content)
    elif "zip" in kind.lower():
        return is_zip(content)
    elif "dir" in kind.lower() or ("f" in kind.lower() and "d" in kind.lower()):
        return os.path.isdir(content)
    elif "code" in kind.lower():  # file
        return is_code(content)
    elif "fi" in kind.lower():  # file
        return os.path.isfile(content)
    elif "num" in kind.lower():  # file
        return is_num(content)
    elif "text" in kind.lower() or "txt" in kind.lower():  # file
        return is_text(content)
    elif "color" in kind.lower():  # file
        return is_str_color(content)
    elif 'dat' in kind.lower() and not 'tim' in kind.lower():
        return is_date(content, errors=errors)
    elif 'tim' in kind.lower() and not 'dat' in kind.lower():
        return is_time(content, errors=errors)
    elif 'tim' in kind.lower() and 'dat' in kind.lower():
        return is_datetime(content, errors=errors)
    elif 'pattern' in kind.lower():
        return is_pattern(content)
    elif "html" in kind.lower(): 
        if content is None or not isinstance(content, str):
            return False
        # Remove leading and trailing whitespace
        content = content.strip()
        # Check for common HTML tags using regex
        # This pattern matches anything that looks like an HTML tag
        tag_pattern = r"<[a-zA-Z][^>]*>(.*?)</[a-zA-Z][^>]*>"
        # Check for any opening and closing tags
        if re.search(tag_pattern, content):
            return True
        # Additional checks for self-closing tags
        self_closing_tags = ["img", "br", "hr", "input", "meta", "link"]
        for tag in self_closing_tags:
            if f"<{tag}" in content:
                return True
        return False
    else:
        print(f"{kind} was not set up correctly")
        return False

@lru_cache(maxsize=8)
def get_ip():
    """
    Master IP discovery function.

    Returns:
        dict with keys:
        - public.ipv4 / public.ipv6
        - default_route.ipv4 / default_route.ipv6
        - interfaces.{iface} -> list of IPs
    """
    
    from collections import defaultdict
    import socket

    def _get_public_ip(urls):
        import urllib.request
        for url in urls:
            try:
                with urllib.request.urlopen(url, timeout=5) as r:
                    return r.read().decode().strip()
            except Exception:
                continue
        return None

    def _get_default_route_ip(family):
        try:
            sock = socket.socket(family, socket.SOCK_DGRAM)
            if family == socket.AF_INET:
                sock.connect(("8.8.8.8", 80))
            else:
                sock.connect(("2001:4860:4860::8888", 80))
            ip = sock.getsockname()[0]
            sock.close()
            return ip
        except Exception:
            return None


    def _get_interface_ips():
        interfaces = defaultdict(list)

        try:
            output = subprocess.check_output(
                ["ip", "-json", "addr"], stderr=subprocess.DEVNULL
            ).decode()
            data = json.loads(output)

            for iface in data:
                name = iface["ifname"]
                for addr in iface.get("addr_info", []):
                    interfaces[name].append(addr["local"])
        except Exception:
            # macOS fallback
            try:
                output = subprocess.check_output(
                    ["ifconfig"], stderr=subprocess.DEVNULL
                ).decode()
                current = None
                for line in output.splitlines():
                    if not line.startswith("\t") and ":" in line:
                        current = line.split(":")[0]
                    elif "inet " in line and current:
                        interfaces[current].append(line.split()[1])
                    elif "inet6 " in line and current:
                        interfaces[current].append(line.split()[1].split("%")[0])
            except Exception:
                pass

        return dict(interfaces)

    return {
        "public": {
            "ipv4": _get_public_ip([
                "https://api.ipify.org",
                "https://ifconfig.me/ip",
                "https://ipv4.icanhazip.com"
            ]),
            "ipv6": _get_public_ip([
                "https://ifconfig.me",
                "https://ipv6.icanhazip.com"
            ]),
        },
        "default_route": {
            "ipv4": _get_default_route_ip(socket.AF_INET),
            "ipv6": _get_default_route_ip(socket.AF_INET6),
        },
        "interfaces": _get_interface_ips()
    }


@lru_cache(maxsize=8)
def get_os(full=False, verbose=False):
    """Collects comprehensive system information.
    full(bool): True, get more detailed info
    verbose(bool): True, print it
    usage:
        info = get_os(full=True, verbose=False)
    """
    import psutil
    # import GPUtil
    import socket
    import uuid
    import cpuinfo
    from datetime import datetime, timedelta
    @lru_cache(maxsize=1)
    def get_os_type():
        os_name = sys.platform
        if "dar" in os_name:
            return "macOS"
        else:
            if "win" in os_name:
                return "Windows"
            elif "linux" in os_name:
                return "Linux"
            else:
                print(f"{os_name}, returned 'None'")
                return None

    if not full:
        return get_os_type()
    @lru_cache(maxsize=1)
    def get_os_info():
        """Get the detailed OS name, version, and other platform-specific details."""

        def get_mac_os_info():
            """Get detailed macOS version and product name."""
            try:
                sw_vers = subprocess.check_output(["sw_vers"]).decode("utf-8")
                product_name = (
                    [
                        line
                        for line in sw_vers.split("\n")
                        if line.startswith("ProductName")
                    ][0]
                    .split(":")[1]
                    .strip()
                )
                product_version = (
                    [
                        line
                        for line in sw_vers.split("\n")
                        if line.startswith("ProductVersion")
                    ][0]
                    .split(":")[1]
                    .strip()
                )
                build_version = (
                    [
                        line
                        for line in sw_vers.split("\n")
                        if line.startswith("BuildVersion")
                    ][0]
                    .split(":")[1]
                    .strip()
                )

                # Return the formatted macOS name, version, and build
                return f"{product_name} {product_version} (Build {build_version})"
            except Exception as e:
                return f"Error retrieving macOS name: {str(e)}"

        def get_windows_info():
            """Get detailed Windows version and edition."""
            try:
                # Get basic Windows version using platform
                windows_version = platform.version()
                release = platform.release()
                version = platform.win32_ver()[0]

                # Additional information using Windows-specific system commands
                edition_command = "wmic os get caption"
                edition = (
                    subprocess.check_output(edition_command, shell=True)
                    .decode("utf-8")
                    .strip()
                    .split("\n")[1]
                )

                # Return Windows information
                return f"Windows {version} {release} ({edition})"
            except Exception as e:
                return f"Error retrieving Windows information: {str(e)}"

        def get_linux_info():
            """Get detailed Linux version and distribution info."""
            try:
                # Check /etc/os-release for modern Linux distros
                with open("/etc/os-release") as f:
                    os_info = f.readlines()

                os_name = (
                    next(line for line in os_info if line.startswith("NAME"))
                    .split("=")[1]
                    .strip()
                    .replace('"', "")
                )
                os_version = (
                    next(line for line in os_info if line.startswith("VERSION"))
                    .split("=")[1]
                    .strip()
                    .replace('"', "")
                )

                # For additional info, check for the package manager (e.g., apt, dnf)
                package_manager = "Unknown"
                if os.path.exists("/usr/bin/apt"):
                    package_manager = "APT (Debian/Ubuntu)"
                elif os.path.exists("/usr/bin/dnf"):
                    package_manager = "DNF (Fedora/RHEL)"

                # Return Linux distribution, version, and package manager
                return f"{os_name} {os_version} (Package Manager: {package_manager})"
            except Exception as e:
                return f"Error retrieving Linux information: {str(e)}"

        os_name = platform.system()

        if os_name == "Darwin":
            return get_mac_os_info()
        elif os_name == "Windows":
            return get_windows_info()
        elif os_name == "Linux":
            return get_linux_info()
        else:
            return f"Unknown OS: {os_name} {platform.release()}"

    def get_os_name_and_version():
        os_name = platform.system()
        if os_name == "Darwin":
            try:
                # Run 'sw_vers' command to get macOS details like "macOS Sequoia"
                sw_vers = subprocess.check_output(["sw_vers"]).decode("utf-8")
                product_name = (
                    [
                        line
                        for line in sw_vers.split("\n")
                        if line.startswith("ProductName")
                    ][0]
                    .split(":")[1]
                    .strip()
                )
                product_version = (
                    [
                        line
                        for line in sw_vers.split("\n")
                        if line.startswith("ProductVersion")
                    ][0]
                    .split(":")[1]
                    .strip()
                )

                # Return the formatted macOS name and version
                return f"{product_name} {product_version}"

            except Exception as e:
                return f"Error retrieving macOS name: {str(e)}"

        # For Windows, we use platform to get the OS name and version
        elif os_name == "Windows":
            os_version = platform.version()
            return f"Windows {os_version}"

        # For Linux, check for distribution info using platform and os-release file
        elif os_name == "Linux":
            try:
                # Try to read Linux distribution info from '/etc/os-release'
                with open("/etc/os-release") as f:
                    os_info = f.readlines()

                # Find fields like NAME and VERSION
                os_name = (
                    next(line for line in os_info if line.startswith("NAME"))
                    .split("=")[1]
                    .strip()
                    .replace('"', "")
                )
                os_version = (
                    next(line for line in os_info if line.startswith("VERSION"))
                    .split("=")[1]
                    .strip()
                    .replace('"', "")
                )
                return f"{os_name} {os_version}"

            except Exception as e:
                return f"Error retrieving Linux name: {str(e)}"

        # Default fallback (for unknown OS or edge cases)
        return f"{os_name} {platform.release()}"

    def get_system_uptime():
        """Returns system uptime as a human-readable string."""
        try:
            boot_time = datetime.fromtimestamp(psutil.boot_time())
            uptime = datetime.now() - boot_time
            return str(uptime).split(".")[0]  # Remove microseconds
        except:
            return None

    def get_active_processes(limit=10):
        try:
            processes = []
            for proc in psutil.process_iter(
                ["pid", "name", "cpu_percent", "memory_percent"]
            ):
                try:
                    processes.append(proc.info)
                except psutil.NoSuchProcess:
                    pass
            # Handle NoneType values by treating them as 0
            processes.sort(key=lambda x: x["cpu_percent"] or 0, reverse=True)
            return processes[:limit]
        except:
            return None
    @lru_cache(maxsize=1)
    def get_virtual_environment_info():
        """Checks if the script is running in a virtual environment and returns details."""
        try:
            # Check if running in a virtual environment
            if hasattr(sys, "real_prefix") or (
                hasattr(sys, "base_prefix") and sys.base_prefix != sys.prefix
            ):
                return {
                    "Virtual Environment": sys.prefix,
                    "Site-Packages Path": os.path.join(
                        sys.prefix,
                        "lib",
                        "python{}/site-packages".format(sys.version_info.major),
                    ),
                }
            else:
                return {"Virtual Environment": "Not in a virtual environment"}
        except Exception as e:
            return {"Error": str(e)}

    def get_temperatures():
        """Returns temperature sensor readings."""
        try:
            return psutil.sensors_temperatures(fahrenheit=False)
        except AttributeError:
            return {"Error": "Temperature sensors not available"}

    def get_battery_status():
        """Returns battery status."""
        try:
            battery = psutil.sensors_battery()
            if battery:
                time_left = (
                    str(timedelta(seconds=battery.secsleft))
                    if battery.secsleft != psutil.POWER_TIME_UNLIMITED
                    else "Charging/Unlimited"
                )
                return {
                    "Percentage": battery.percent,
                    "Plugged In": battery.power_plugged,
                    "Time Left": time_left,
                }
            return {"Status": "No battery detected"}
        except:
            return {"Status": "No battery detected"}

    def get_disk_io():
        """Returns disk I/O statistics."""
        disk_io = psutil.disk_io_counters()
        return {
            "Read (GB)": disk_io.read_bytes / (1024**3),
            "Write (GB)": disk_io.write_bytes / (1024**3),
            "Read Count": disk_io.read_count,
            "Write Count": disk_io.write_count,
        }

    def get_network_io():
        """Returns network I/O statistics."""
        net_io = psutil.net_io_counters()
        return {
            "Bytes Sent (GB)": net_io.bytes_sent / (1024**3),
            "Bytes Received (GB)": net_io.bytes_recv / (1024**3),
            "Packets Sent": net_io.packets_sent,
            "Packets Received": net_io.packets_recv,
        }
    @lru_cache(maxsize=32)
    def run_shell_command(command):
        """Runs a shell command and returns its output."""
        try:
            result = subprocess.run(
                command,
                shell=True,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                text=True,
            )
            return (
                result.stdout.strip()
                if result.returncode == 0
                else result.stderr.strip()
            )
        except Exception as e:
            return f"Error running command: {e}"

    system_info = {
        "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "os": get_os_type(),
        "system": {
            "os": get_os_info(),
            "platform": f"{platform.system()} {platform.release()}",
            "version": platform.version(),
            "machine": platform.machine(),
            "processor": platform.processor(),
            "architecture": platform.architecture()[0],
            "hostname": socket.gethostname(),
            "ip address": get_ip(),#socket.gethostbyname(socket.gethostname()),
            "mac address": ":".join(
                ["{:02x}".format((uuid.getnode() >> i) & 0xFF) for i in range(0, 48, 8)]
            ),
            "cpu brand": cpuinfo.get_cpu_info().get("brand_raw", "Unknown"),
            "python version": platform.python_version(),
            "uptime": get_system_uptime(),
        },
        "cpu": {
            "physical cores": psutil.cpu_count(logical=False),
            "logical cores": psutil.cpu_count(logical=True),
            "max frequency (MHz)": psutil.cpu_freq().max,
            "min frequency (MHz)": psutil.cpu_freq().min,
            "current frequency (MHz)": psutil.cpu_freq().current,
            "usage per core (%)": psutil.cpu_percent(percpu=True),
            "total cpu Usage (%)": psutil.cpu_percent(),
            "load average (1m, 5m, 15m)": (
                os.getloadavg() if hasattr(os, "getloadavg") else "N/A"
            ),
        },
        "memory": {
            "total memory (GB)": psutil.virtual_memory().total / (1024**3),
            "available memory (GB)": psutil.virtual_memory().available / (1024**3),
            "used memory (GB)": psutil.virtual_memory().used / (1024**3),
            "memory usage (%)": psutil.virtual_memory().percent,
            "swap total (GB)": psutil.swap_memory().total / (1024**3),
            "swap free (GB)": psutil.swap_memory().free / (1024**3),
            "swap used (GB)": psutil.swap_memory().used / (1024**3),
            "swap usage (%)": psutil.swap_memory().percent,
        },
        "disk": {},
        "disk io": get_disk_io(),
        "network": {},
        "network io": get_network_io(),
        "gpu": [],
        # "temperatures": get_temperatures(),
        # "battery": get_battery_status(),
        "active processes": get_active_processes(),
        "environment": {
            "user": os.getenv("USER", "Unknown"),
            "environment variables": dict(os.environ),
            "virtual environment info": get_virtual_environment_info(),  # Virtual env details
            "docker running": os.path.exists("/.dockerenv"),  # Check for Docker
            "shell": os.environ.get("SHELL", "Unknown"),
            "default terminal": run_shell_command("echo $TERM"),
            "kernel version": platform.uname().release,
            "virtualization type": run_shell_command("systemd-detect-virt"),
        },
        "additional info": {
            "Shell": os.environ.get("SHELL", "Unknown"),
            "default terminal": run_shell_command("echo $TERM"),
            "kernel version": platform.uname().release,
            "virtualization type": run_shell_command("systemd-detect-virt"),
            "running in docker": os.path.exists("/.dockerenv"),
        },
    }

    # Disk Information

    system_info["disk"] = {}

    for partition in psutil.disk_partitions():
        try:
            usage = psutil.disk_usage(partition.mountpoint)
            system_info["disk"][partition.device] = {
                "mountpoint": partition.mountpoint,
                "file system type": partition.fstype,
                "total space (GB)": usage.total / (1024 ** 3),
                "used space (GB)": usage.used / (1024 ** 3),
                "free space (GB)": usage.free / (1024 ** 3),
                "usage (%)": usage.percent,
            }
        except Exception as e:
            # Catch all exceptions including SystemError, OSError, PermissionError, etc.
            system_info["disk"][partition.device] = {
                "mountpoint": partition.mountpoint,
                "error": repr(e),  # Use repr to capture exact error
            }
    for partition in psutil.disk_partitions():
        try:
            usage = psutil.disk_usage(partition.mountpoint)
            system_info["disk"][partition.device] = {
                "mountpoint": partition.mountpoint,
                "file system type": partition.fstype,
                "total size (GB)": usage.total / (1024**3),
                "used (GB)": usage.used / (1024**3),
                "free (GB)": usage.free / (1024**3),
                "usage (%)": usage.percent,
            }
        except (PermissionError, OSError, SystemError) as e:
            system_info["disk"][partition.device] = {
                "mountpoint": partition.mountpoint,
                "error": str(e),
            } 

    # Network Information
    if_addrs = psutil.net_if_addrs()
    for interface_name, interface_addresses in if_addrs.items():
        system_info["network"][interface_name] = []
        for address in interface_addresses:
            if str(address.family) == "AddressFamily.AF_INET":
                system_info["network"][interface_name].append(
                    {
                        "ip address": address.address,
                        "netmask": address.netmask,
                        "broadcast ip": address.broadcast,
                    }
                )
            elif str(address.family) == "AddressFamily.AF_PACKET":
                system_info["network"][interface_name].append(
                    {
                        "mac address": address.address,
                        "netmask": address.netmask,
                        "broadcast mac": address.broadcast,
                    }
                ) 
    res = system_info if full else get_os_type()
    if verbose:
        try:
            preview(res)
        except Exception as e:
            pnrint(e)
    return res
def get_creation_time(stats):
    """Return file creation time cross-platform.
    want true creation time in Python, it‚Äôs tricky ‚Äî on macOS, os.stat().st_birthtime works, 
    but on Linux you‚Äôd need to check the filesystem type and may have to use platform-specific tools."""
    system = platform.system()
    if system == "Darwin":  # macOS
        return getattr(stats, "st_birthtime", stats.st_ctime)
    elif system == "Windows":
        return stats.st_ctime
    else:  # Linux/Unix without birthtime support
        try:
            # Python 3.12+ with statx might provide it
            return stats.st_birthtime
        except AttributeError:
            return None  # Birth time not available

def fpath_norm(
    fpath: Optional[Union[str, os.PathLike]],
    absolute: bool = True,
    expanduser: bool = True,
    expandvars: bool = True,
    lowercase: Optional[bool] = None,
    strip_trailing: bool = True,
    unicode_normalize: str = "NFC",
) -> str:
    """
    Normalize a file path for consistent cross-platform handling.

    Args:
        fpath: Input path to normalize
        absolute: Convert to absolute path (default: True)
        expanduser: Expand ~ to home directory (default: True)
        expandvars: Expand environment variables (default: True)
        lowercase: Force lowercase (None=auto based on OS)
        strip_trailing: Remove trailing slashes (default: True)
        unicode_normalize: Unicode normalization form (default: "NFC")

    Returns:
        Normalized Unix-style path string

    Examples:
        >>> fpath_norm("~/Documents//test.txt")
        '/home/user/Documents/test.txt'
        >>> fpath_norm("C:\\Users\\Test", lowercase=True)
        'c:/users/test'
    """
    if not fpath:
        return ""

    fpath = str(fpath)
    if expandvars:
        fpath = os.path.expandvars(fpath)
    
    p = Path(fpath)
    if expanduser:
        p = p.expanduser()
    if absolute:
        try:
            p = p.resolve(strict=False)
        except Exception:
            p = p.absolute()

    norm = str(p).replace("\\", "/")
    if unicode_normalize:
        import unicodedata
        norm = unicodedata.normalize(unicode_normalize, norm)
    if strip_trailing and len(norm) > 1 and norm.endswith("/"):
        norm = norm.rstrip("/")
    if lowercase or (lowercase is None and (os.name == "nt" or sys.platform == "darwin")):
        norm = norm.lower()
    
    return norm



def ls(
    rootdir,
    kind=None,
    ignore_case:bool=True,
    filter=None,
    exclude=None,
    sort_by="name",
    ascending=True,
    contains=None,
    booster='auto', 
    depth=0,
    hidden=False,
    orient="list",
    output="df",
    verbose=False,
    suffix_list=None,
    workers=None,
    root_anchor=None,
): 
    """
    Scan directories and return structured file information with advanced filtering.

    Features:
    - Recursive directory scanning with depth control
    - Advanced inclusion/exclusion filters
    - Parallel processing for large directories
    - Multiple output formats (DataFrame, dicts, etc.)
    - Cross-platform path handling
    - Comprehensive metadata collection

    Args:
        - rootdir: Path(s) to scan (str or list of str)
        - kind: File extensions to include (e.g. '.txt' or ['jpg','png'])
        - ignore_case: Case-insensitive matching (default: True)
        - filter: Wildcard patterns to include (e.g. '*.csv' or ['report*','data?'])
        - exclude: Patterns to exclude (supports ** recursive wildcards)
        - sort_by: Column to sort by ('name', 'size', 'modified_time', etc.)
        - ascending: Sort order (default: True=ascending)
        - contains: Regex pattern to match in filenames
        - booster: Enable parallel processing ('auto', True, False)
        - depth: Max recursion depth (0=current dir only)
        - hidden: Include hidden files (default: False)
        - orient: Output format for non-DataFrame ('list', 'dict', 'records', etc.)
        - output: Output type ('df'=DataFrame, 'list'=raw list)
        - verbose: Print progress messages
        - suffix_list: Custom file suffix definitions
        - workers: Number of parallel workers (when booster=True)
        - root_anchor: Base path for relative calculations
    Returns:
        DataFrame or other format containing file metadata
    Examples:
        # Test 1: Basic listing
        print("\n[Test 1] Basic directory scan:")
        result = ls(TEST_ROOT)
        print(f"Found {len(result)} items")
        display(result.head(2))

        # Test 2: Kind filtering
        print("\n[Test 2] PDF files only:")
        pdfs = ls(TEST_ROOT, kind=".pdf")
        print(f"Found {len(pdfs)} PDFs")
        display(pdfs)

        # Test 3: Filter + exclude
        print("\n[Test 3] CSV files excluding Mail:")
        csvs = ls(TEST_ROOT, filter="*.csv", exclude="*Mail*")
        print(f"Found {len(csvs)} CSVs (without Mail)")
        display(csvs)

        # Test 4: Contains regex
        print("\n[Test 4] Files containing '2023':")
        year_files = ls(TEST_ROOT, contains="2023", output="list")
        print(f"Found {len(year_files)} matching files")
        for f in year_files[:3]:
            print(f["path"])

        # Test 5: Hidden files
        print("\n[Test 5] Include hidden files:")
        hidden = ls(TEST_ROOT, hidden=True, sort_by="size", ascending=False)
        print(f"Found {len(hidden)} items (including hidden)")
        display(hidden.head(3))

        # Test 6: Depth control
        print("\n[Test 6] Depth=1 (no subfolders):")
        shallow = ls(TEST_ROOT, depth=1)
        print(f"Found {len(shallow)} items at depth 1")

        # Test 7: Multiple filters
        print("\n[Test 7] Multiple filters:")
        filtered = ls(
            TEST_ROOT,
            kind=[".xlsx", ".csv"],
            filter=["*2023*", "*log*"],
            exclude=["*temp*", "*backup*"],
            verbose=True,
        )
        print(f"Found {len(filtered)} matching files")
        display(filtered)

        # Test 8: Parallel processing
        print("\n[Test 8] Parallel scan:")
        parallel = ls(
            [TEST_ROOT, os.path.join(TEST_ROOT, "temp")],
            booster=True,
            workers=2,
            verbose=True,
        )
        print(f"Found {len(parallel)} items in parallel")
    """

    import fnmatch
    from concurrent.futures import ProcessPoolExecutor, as_completed
    
    if root_anchor is None:# to preserve the true root
        root_anchor = rootdir

    # Preprocess exclude patterns
    exclude_patterns = [exclude] if isinstance(exclude, str) else list(exclude or [])

    @lru_cache(maxsize=1024)
    def should_exclude(path, name):
        """Cached exclusion check with pattern pre-processing"""
        normalized_path = _normalize_path(path)
        if ignore_case:
            normalized_path = normalized_path.lower()
            name = name.lower()

        for pattern in exclude_patterns:
            # Convert pattern to normalized form once
            norm_pattern = _normalize_pattern(pattern)
            
            # Check both path and name
            if fnmatch.fnmatch(normalized_path, norm_pattern):
                return True
            if fnmatch.fnmatch(name, norm_pattern):
                return True
        return False

    @lru_cache(maxsize=1024)
    def _normalize_path(path):
        """Normalize path with caching"""
        return os.path.normpath(os.path.abspath(path)).replace('\\', '/')

    @lru_cache(maxsize=512)
    def _normalize_pattern(pattern):
        """Ensure consistent pattern handling"""
        pattern = pattern.replace('\\', '/')  # Normalize slashes
        if ignore_case:
            pattern = pattern.lower()
        # Handle special cases for * and ?
        pattern = pattern.replace('.*', '*').replace('.?', '?')
        return pattern
    # Ê®°ÂùóÁ∫ßËæÖÂä©ÂáΩÊï∞ (ÂèØpickle)
    @lru_cache(maxsize=2048)
    def _is_hidden(filepath):
        """Ê£ÄÊü•Êñá‰ª∂/Êñá‰ª∂Â§πÊòØÂê¶ÈöêËóè"""
        if platform.system() == "Windows":
            import ctypes

            attribute = ctypes.windll.kernel32.GetFileAttributesW(filepath)
            return attribute != -1 and bool(attribute & 2)
        return os.path.basename(filepath).startswith(".")

    def _match_filter(name, filters, ignore_case=True):
        """Enhanced filter matching supporting wildcards and regex"""
        if not filters:
            return True
        if isinstance(filters, str):
            filters = [filters]
        # Prepare name based on ignore_case setting
        target_name = name.lower() if ignore_case else name
        for pattern in filters:
            # If ignore_case is True, convert pattern to lowercase
            processed_pattern = pattern.lower() if ignore_case else pattern
            # Wildcard matching (legacy support)
            if "*" in processed_pattern or "?" in processed_pattern:
                if fnmatch.fnmatch(target_name, processed_pattern):
                    return True
            # Regex matching
            try:
                # Add case-insensitive flag to regex if needed
                if ignore_case:
                    pattern_obj = re.compile(processed_pattern, re.IGNORECASE)
                else:
                    pattern_obj = re.compile(processed_pattern)
                if pattern_obj.search(name):
                    return True
            except re.error:
                # Fallback to exact match if invalid regex
                match_name = target_name if ignore_case else name
                if processed_pattern == match_name:
                    return True
        return False

    @lru_cache(maxsize=1024)
    def _get_depth(dirpath, root):
        dirpath = os.path.normpath(os.path.abspath(dirpath))
        root = os.path.normpath(os.path.abspath(root))
        rel_path = os.path.relpath(dirpath, root)
        return 0 if rel_path == "." else rel_path.count(os.sep)
    @lru_cache(maxsize=1024)
    def _get_file_extension(filename, suffix_list=None):
        """Ëé∑ÂèñÊñá‰ª∂Êâ©Â±ïÂêç"""
        if suffix_list:
            for ext in sorted(suffix_list, key=len, reverse=True):
                if filename.endswith(ext):
                    return filename[: -len(ext)], ext
        return os.path.splitext(filename)


    # Ê†∏ÂøÉÂ§ÑÁêÜÂáΩÊï∞
    def _process_item(item_path, kind_list, filter_list, contains, hidden, suffix_list,ignore_case):
        """Â§ÑÁêÜÂçï‰∏™Êñá‰ª∂/ÁõÆÂΩïÈ°π"""
        try:
            name = os.path.basename(item_path)
            full_path = os.path.abspath(item_path)
            is_dir = os.path.isdir(item_path)
            # first apply Hidden filter
            if not hidden and _is_hidden(item_path):
                return None 
            # exclusion check
            if should_exclude(full_path, name):
                return None

            # apply CONTAINS filter
            if contains and not re.search(contains, name):
                return None
            
            # apply INCLUSION Filter
            if not _match_filter(name, filter_list,ignore_case):
                return None

            # Ëé∑ÂèñÊñá‰ª∂Áä∂ÊÄÅÂíåÂÖÉÊï∞ÊçÆ
            stats = os.stat(item_path)

            category_kind=[".document",".documents",".office",".ms",".img",".image",".images",".zip",".code",".scripts",".script",".file",".video",".audio",".movie"]
            # Â§ÑÁêÜÊñá‰ª∂Â§πÁ±ªÂûã
            folder_exts = [".fd", ".fld", ".fol", ".folder"] 
            if any([i.lower() in category_kind for i in kind_list if i is not None]):
                if any([isa(item_path, kind_list_.lower()) for kind_list_ in kind_list]):
                    name_part, ext = _get_file_extension(name, suffix_list)
                else:
                    return None
            else:
                if is_dir:
                    if kind_list and kind_list[0] not in folder_exts:
                        return None
                    name_part, ext = name, None
                # Â§ÑÁêÜÊñá‰ª∂Á±ªÂûã
                else:
                    if kind_list and any([i.lower() in folder_exts for i in kind_list if i is not None]):
                        return None
                    name_part, ext = _get_file_extension(name, suffix_list)
                    if kind_list:
                        if ignore_case:
                            kind_set = {k.lower() for k in kind_list}
                            if ext.lower() not in kind_set:
                                return None
                        else:
                            if ext not in kind_list:
                                return None
            # ÊûÑÂª∫ÂÖÉÊï∞ÊçÆÂ≠óÂÖ∏
            return {
                "name": name_part,
                "kind": ext,
                "size": stats.st_size / (1024 * 1024),
                "basename": name,
                "path": item_path,
                "ctime": get_creation_time(stats),#stats.st_ctime,
                "mtime": stats.st_mtime,
                "atime": stats.st_atime,
                "permission": stat.filemode(stats.st_mode), 
                "rootdir": os.path.dirname(item_path),
                "is_dir": is_dir,
                "os": get_os(),
            }
        except Exception as e:
            return None

    # È¢ÑÂ§ÑÁêÜÂèÇÊï∞
    rootdir_list = [rootdir] if not isinstance(rootdir, list) else rootdir
    kind_list = [kind] if kind and not isinstance(kind, list) else (kind or [])
    mapping = {'doc': 'docx','docx': 'doc','xls': 'xlsx','xlsx': 'xls','ppt': 'pptx','pptx': 'ppt'}# handle special cases 
    kind_list_lower = [i.lower() for i in kind_list]# Convert kind_list to lowercase once
    kind_set = set(kind_list) # Use a set for O(1) lookup
    # Apply mapping
    for key, val in mapping.items():
        if any(key in s for s in kind_list_lower) and val not in kind_set:
            kind_list.append(val)
            kind_set.add(val)
    kind_list = [k if k.startswith(".") else f".{k}" for k in kind_list]
    filter_list = ([filter] if filter and not isinstance(filter, list) else (filter or []))
    # ÁªìÊûúÊî∂ÈõÜÂô®
    all_items = []
    fd_exts = [".fd", ".fld", ".fol", ".folder"]
    # Âπ∂Ë°åÂ§ÑÁêÜÈÄªËæë
    if isinstance(booster, str):
        booster= False if 'auto' in booster.lower() and len(rootdir_list) < 4 else True
    if booster:
        workers = workers or max(4, os.cpu_count() or 1)
        with ProcessPoolExecutor(max_workers=workers) as executor:
            futures = {}
            for root in rootdir_list:
                future = executor.submit(
                    ls,
                    root,
                    kind=kind,
                    ignore_case=ignore_case,
                    filter=filter,
                    sort_by=sort_by,
                    ascending=ascending,
                    contains=contains,
                    booster=False,  # Â≠êËøõÁ®ãÁ¶ÅÁî®ÈÄíÂΩíÂπ∂Ë°å
                    depth=depth,
                    hidden=hidden,
                    orient="list",
                    output="list",
                    verbose=verbose,
                    suffix_list=suffix_list,
                    workers=1,
                    root_anchor=root_anchor,
                )
                futures[future] = root

            for future in as_completed(futures):
                try:
                    result = future.result()
                    all_items.extend(result)
                except Exception as e:
                    if verbose:
                        print(f"Error processing {futures[future]}: {str(e)}")
    # ÂçïËøõÁ®ãÂ§ÑÁêÜÈÄªËæë
    else:
        for root in rootdir_list:
            for dirpath, dirs, files in os.walk(root):
                # Apply directory exclusions during traversal
                dirs[:] = [d for d in dirs if not should_exclude(os.path.join(dirpath, d),d)]
                # Ê∑±Â∫¶ÊéßÂà∂
                if depth is not None:
                    rel_path = os.path.relpath(dirpath, root)
                    if rel_path == ".":
                        current_depth = 0
                    else:
                        current_depth = len(rel_path.split(os.sep))
                    if current_depth > depth:
                        del dirs[:]  # ÂÅúÊ≠¢Ê∑±ÂÖ•
                        continue
                # Â§ÑÁêÜÁõÆÂΩï
                for d in dirs[:]:  # ‰ΩøÁî®ÂâØÊú¨ÈÅçÂéÜ
                    item_path = os.path.join(dirpath, d)
                    if not hidden and _is_hidden(item_path):
                        dirs.remove(d)  # Êõ¥Êñ∞ÂéüÂßãÂàóË°®
                        continue
                    item = _process_item(
                        item_path, kind_list, filter_list, contains, hidden, suffix_list,ignore_case
                    )
                    if item:
                        all_items.append(item)

                # Â§ÑÁêÜÊñá‰ª∂
                for f in files:
                    item_path = os.path.join(dirpath, f)
                    item = _process_item(
                        item_path, kind_list, filter_list, contains, hidden, suffix_list,ignore_case
                    )
                    if item:
                        all_items.append(item)
    # ÊûÑÂª∫ÁªìÊûú
    if not all_items:
        return pd.DataFrame() if output == "df" else []
    df = pd.DataFrame(all_items) # ËΩ¨Êç¢‰∏∫DataFrame
    
    # ÂêéÂ§ÑÁêÜ
    if len(df) > 0:
        df["owner"] = df["path"].map(fowner)
        df["length"] = df["name"].map(len)

        # Convert time columns if they exist
        time_cols = ["ctime", "mtime", "atime"]
        existing_time_cols = [col for col in time_cols if col in df.columns]
        # df.loc[:, existing_time_cols] = df.loc[:, existing_time_cols].apply(pd.to_datetime, unit="s")
        df[existing_time_cols] = df[existing_time_cols].apply(lambda col: pd.to_datetime(col, unit="s", errors="coerce"))

        # Create alias columns only for existing time cols 
        alias_map = {
            "created_time": "ctime",
            "modified_time": "mtime",
            "last_open_time": "atime",
            "latest_time": "atime",
            "last_access_time": "atime",
        }
        for alias, orig in alias_map.items():
            if orig in df.columns:
                df[alias] = df[orig]

        f_col_names = [
            "name", "kind", "length", "size", "basename","ctime","mtime","atime","rootdir", "permission","path", 
            "created_time", "modified_time", "last_open_time","latest_time","last_access_time",
            "is_dir", "owner", "os",
        ]
        final_cols = [col for col in f_col_names if col in df.columns]
        df = df[final_cols]
        sort_by = strcmp(sort_by, f_col_names)[0]# ÊéíÂ∫è  
        df = sort_kind(df, by=sort_by, ascending=ascending)
    # ËøîÂõûÊåáÂÆöÊ†ºÂºè
    if output == "df":
        if verbose:
            print(f"Found {len(df)} items")
        return df
    # ÂÖ∂ÂÆÉËæìÂá∫Ê†ºÂºè
    orient = orient.lower()
    if orient == "list":
        return df.to_dict("records")
    elif orient == "dict":
        return df.to_dict()
    elif orient == "records":
        return df.to_dict("records")
    elif orient == "index":
        return df.to_dict("index")
    elif orient == "series":
        return df.to_dict("series")
    return df

def tree(
    rootdir,
    kind=None,
    ignore_case=True,
    filter=None,
    sort_by="path",
    ascending=True,
    contains=None,
    booster="auto",
    depth=2,# 3rh level, 'None', showing all levels
    hidden=False,
    suffix_list=None,
    workers=None,
    show_size=True,
    show_time=False,
    show_permission=False,
    verbose=True,
    relative_to=None,
    collect_strings=True,
    return_contents=False,
):
    """
    Display or return a hierarchical folder/file tree with filtering and metadata.

    Parameters
    ----------
    collect_strings : bool, default=False
        If True, returns a list of strings representing the tree structure.
        If False, prints the tree and returns None or structured data.
    """ 
    def build_file_tree(df, rootdir, relative_to=None):
        """Build hierarchical file tree structure with consistent metadata format"""
        from collections import defaultdict
        import os
        import pandas as pd
        
        # Ensure all root directories are included, even if empty after filtering
        roots = [rootdir] if isinstance(rootdir, str) else rootdir
        all_roots = []
        
        for root in roots:
            abs_root = os.path.abspath(root)
            
            # Add root directory to the dataframe if not present
            root_entry = {
                'abs_path': abs_root,
                'path': root,
                'is_dir': True,
                'name': os.path.basename(abs_root)
            }
            
            # Check if root is already in df
            root_in_df = False
            if not df.empty:
                root_paths = df['path'].apply(lambda x: os.path.abspath(x))
                root_in_df = any(root_paths == abs_root)
            
            all_roots.append(abs_root)
        
        # Create display paths
        if relative_to:
            relative_to = os.path.abspath(relative_to)
            df['display_path'] = df['abs_path'].apply(lambda x: os.path.relpath(x, relative_to))
        else:
            df['display_path'] = df['abs_path']
        
        # Ensure all directories in the path chain are included
        all_paths = set(df['abs_path'].tolist())
        all_dirs = set()
        
        # Add all parent directories
        for path in all_paths:
            parent = os.path.dirname(path)
            while parent and parent not in all_dirs:
                all_dirs.add(parent)
                parent = os.path.dirname(parent)
        
        # Add missing directories to the dataframe
        missing_dirs = []
        for dir_path in all_dirs:
            if dir_path not in all_paths:
                # Check if this directory is within one of our roots
                is_under_root = any(dir_path.startswith(root) for root in all_roots)
                if is_under_root:
                    missing_dirs.append({
                        'abs_path': dir_path,
                        'path': dir_path,
                        'is_dir': True,
                        'name': os.path.basename(dir_path),
                        'display_path': os.path.relpath(dir_path, relative_to) if relative_to else dir_path
                    })
        
        if missing_dirs:
            missing_df = pd.DataFrame(missing_dirs)
            df = pd.concat([df, missing_df], ignore_index=True)
        
        # Ensure required columns exist
        if 'is_dir' not in df.columns:
            df['is_dir'] = df['abs_path'].apply(os.path.isdir)
        
        def create_item(row):
            """Create item with properly formatted metadata"""
            metadata = {}
            if show_size and 'size' in row and not row['is_dir']:
                metadata['size'] = row.get('size', 0)
            if show_time and 'modified_time' in row:
                mtime = row['modified_time']
                if pd and isinstance(mtime, pd.Timestamp):
                    mtime = mtime.to_pydatetime()
                metadata['modified'] = mtime
            if show_permission and 'permission' in row:
                metadata['permission'] = row.get('permission', '')
            
            return {
                'name': os.path.basename(row['display_path']),
                'path': row['display_path'],
                'abs_path': row['abs_path'],
                'is_dir': row['is_dir'],
                'metadata': metadata
            }
        
        # Build parent-child mapping
        tree = defaultdict(list)
        for _, row in df.iterrows():
            item = create_item(row)
            parent = os.path.dirname(item['abs_path'])
            tree[parent].append(item)
        
        # Sort children: directories first, then by name
        for parent in tree:
            tree[parent].sort(key=lambda x: (not x['is_dir'], x['name'].lower()))
        
        # Build hierarchical structure
        def build_branch(directory):
            children = []
            for child in tree.get(directory, []):
                if child['is_dir']:
                    child['contents'] = build_branch(child['abs_path'])
                children.append(child)
            return children
        
        # Process root directories
        root_items = []
        for root in all_roots:
            display_path = os.path.relpath(root, relative_to) if relative_to else root
            
            root_item = {
                'name': os.path.basename(root),
                'path': display_path,
                'abs_path': root,
                'is_dir': True,
                'metadata': {},
                'contents': build_branch(root)
            }
            root_items.append(root_item)

        # ---------- FILE COUNT (recursive) ----------
        def count_files(node):
            if not node.get("contents"):
                node["metadata"]["n_files"] = 0
                return 0

            total = 0
            for c in node["contents"]:
                if c["is_dir"]:
                    total += count_files(c)
                else:
                    total += 1

            node["metadata"]["n_files"] = total
            return total

        for r in root_items:
            count_files(r)
        
        return root_items 
    def print_tree(items, prefix="", is_last=False):
        """Recursively print directory tree structure with optional color"""
        for i, item in enumerate(items):
            is_last_child = i == len(items) - 1
            connector = "‚îî‚îÄ‚îÄ " if is_last_child else "‚îú‚îÄ‚îÄ "

            # Format metadata for display 
            meta_parts = []
            if item["is_dir"] and "n_files" in item["metadata"]:
                meta_parts.append(f"{item['metadata']['n_files']}") 
            if "size" in item["metadata"] and not item["is_dir"]:
                size = item["metadata"]["size"]
                if size >= 1024:
                    meta_parts.append(f"{size/(1024):.2f}GB")
                elif size >= 1:
                    meta_parts.append(f"{size:.2f}MB")
                elif size >= 1/1024:
                    meta_parts.append(f"{size*1024:.0f}KB")
                else:
                    meta_parts.append(f"{size*1024*1024:.0f}B")
            if "modified" in item["metadata"]:
                meta_parts.append(item["metadata"]["modified"].strftime("%Y-%m-%d %H:%M"))
            if "permission" in item["metadata"]:
                meta_parts.append(item["metadata"]["permission"])
            meta_str = f" [{' | '.join(meta_parts)}]" if meta_parts else ""

            # Color directories vs files
            if item["is_dir"]:
                display_name = color_text(item["name"], c="b", bold=True)  # blue bold
            else:
                display_name = color_text(item["name"], c="#9F463E")  # green for files

            print(f"{prefix}{connector}{display_name}{color_text(meta_str,c='y')}")

            if item.get("contents"):
                new_prefix = prefix + ("    " if is_last_child else "‚îÇ   ")
                print_tree(item["contents"], new_prefix, is_last_child)

    # Recursive function to collect tree strings
    def collect_tree_strings(items, prefix="", is_last=False):
        """Recursively collect directory tree structure as strings"""
        lines = []
        for i, item in enumerate(items):
            is_last_child = i == len(items) - 1
            connector = "‚îî‚îÄ‚îÄ " if is_last_child else "‚îú‚îÄ‚îÄ "

            # Format metadata for display
            meta_str = ""
            if item["metadata"]:
                meta_parts = []
                if "size" in item["metadata"] and not item["is_dir"]:
                    size = item["metadata"]["size"]
                    if size >= 1024:  # More than 1GB
                        meta_parts.append(f"{size/(1024):.2f}GB")
                    elif size >= 1:
                        meta_parts.append(f"{size:.2f}MB")
                    elif size >= 1/1024:
                        meta_parts.append(f"{size*1024:.0f}KB")
                    else:
                        meta_parts.append(f"{size*1024*1024:.0f}B") 

                if "modified" in item["metadata"]:
                    meta_parts.append(
                        item["metadata"]["modified"].strftime("%Y-%m-%d %H:%M")
                    )

                if "permission" in item["metadata"]:
                    meta_parts.append(item["metadata"]["permission"])

                meta_str = " [" + " | ".join(meta_parts) + "]"

            lines.append(f"{prefix}{connector}{item['name']}{meta_str}")

            if item.get("contents"):
                new_prefix = prefix + ("    " if is_last_child else "‚îÇ   ")
                lines.extend(
                    collect_tree_strings(item["contents"], new_prefix, is_last_child)
                )
        return lines

    # Initialize string collector if needed
    output_lines = [] if collect_strings else None

    # Get filtered directory listing
    df = ls(
        rootdir=rootdir,
        kind=kind,
        ignore_case=ignore_case,
        filter=filter,
        sort_by=sort_by,
        ascending=ascending,
        contains=contains,
        booster=booster,
        depth=depth,
        hidden=hidden,
        orient="list",
        output="df",
        verbose=verbose,
        suffix_list=suffix_list,
        workers=workers,
    )
    if df.empty:
        if return_contents:
            return {}
        if collect_strings:
            return ["[No matching files or folders found]"]
        print("[No matching files or folders found]")
        return None
    if kind:
        if not kind.startswith("."):
            kind = "." + kind
        df = df[df["is_dir"] | df["path"].str.endswith(kind)]

    # Convert all paths to absolute for consistent processing
    df["abs_path"] = df["path"].apply(lambda x: os.path.abspath(x))

    root_items = build_file_tree(df, rootdir, relative_to)
    if verbose:
        for i, root_item in enumerate(root_items):
            print(f"\n{root_item['path']}")
            print_tree(root_item["contents"], is_last=(i == len(root_items) - 1))
    if collect_strings:
        all_lines = []
        for i, root_item in enumerate(root_items):
            all_lines.append(f"\n{root_item['path']}")
            all_lines.extend(collect_tree_strings(root_item["contents"]))
        if return_contents:
            return all_lines 

    if return_contents:
        if len(root_items) == 1:
            return root_items[0]
        return root_items 

# tree = cp_func(".","ftree")

def frecent(
    rootdir: Union[str, List[str]],
    recent: Union[int, float, str, timedelta] = 1,  # Supports natural language
    kind: Optional[Union[str, List[str]]] = None,
    ignore_case: bool = True,
    filter: Optional[Union[str, List[str]]] = None,
    sort_by: str = "mtime",
    ascending: bool = True,
    contains: Optional[str] = None,
    booster: Union[str, bool] = 'auto',
    depth: int = 0,
    hidden: Optional[bool] = False,
    orient: str = "list",
    output: str = "df",
    verbose: bool = False,
    suffix_list: Optional[List[str]] = None,
    workers: Optional[int] = None,
    root_anchor: Optional[str] = None,
    time_column: str = None,#"mtime",  # New: specify time column for filtering
    time_comparison: str = "newer",  # New: 'newer', 'older', or 'range'
    time_range: Optional[tuple] = None,  # New: for range comparisons
    min_size: Optional[float] = None,  # New: size in MB
    max_size: Optional[float] = None,  # New: size in MB
) -> Union[pd.DataFrame, List[Dict]]:
    """
    Advanced file search with natural language time filtering and enhanced capabilities.
    
    Parameters:
    recent (int/float/str/timedelta): Time period to search. Supports natural language
        (e.g., "2 days", "1 week 3 hours", "yesterday", "last month")
    time_column (str): Time column to use for filtering ('mtime', 'atime', 'ctime')
    time_comparison (str): Type of time comparison ('newer', 'older', 'range')
    time_range (tuple): Custom time range as (start, end) datetime objects
    min_size/max_size (float): Size limits in megabytes
    """
    # Parse natural language time expressions
    def parse_time_expression(expr: str) -> timedelta:
        """Convert natural language time expressions to timedelta"""
        now = datetime.now()
        expr = expr.lower().strip()
        
        # Predefined expressions
        predefined = {
            "now": timedelta(0),
            "today": timedelta(hours=now.hour, minutes=now.minute, seconds=now.second),
            "yesterday": timedelta(days=1),
            "this week": timedelta(days=now.weekday()),
            "last week": timedelta(days=now.weekday() + 7),
            "this month": timedelta(days=now.day-1),
            "last month": timedelta(days=now.day-1 + 30),  # Approximate
        }
        
        if expr in predefined:
            return predefined[expr]
        
        # Relative time parser
        pattern = r"(\d+)\s*(\w+)"
        matches = re.findall(pattern, expr)
        if not matches:
            raise ValueError(f"Unrecognized time expression: {expr}")
        
        delta = timedelta()
        units_map = {
            "sec": "seconds", "second": "seconds", "seconds": "seconds",
            "min": "minutes", "minute": "minutes", "minutes": "minutes",
            "hr": "hours", "hour": "hours", "hours": "hours",
            "day": "days", "days": "days",
            "wk": "weeks", "week": "weeks", "weeks": "weeks",
            "mon": "days", "month": "days", "months": "days",  # Approx 30 days
            "yr": "days", "year": "days", "years": "days",  # Approx 365 days
        }
        
        for value, unit in matches:
            value = int(value)
            unit = units_map.get(unit.lower(), "days")
            
            if unit == "days" and ("mon" in unit or "month" in unit):
                value *= 30
            elif unit == "days" and ("yr" in unit or "year" in unit):
                value *= 365
            
            delta += timedelta(**{unit: value})
        
        return delta

    # Calculate cutoff time
    cutoff = None
    custom_range = None
    
    if isinstance(recent, (int, float)):
        cutoff = datetime.now() - timedelta(days=recent)
    elif isinstance(recent, timedelta):
        cutoff = datetime.now() - recent
    elif isinstance(recent, str):
        try:
            time_delta = parse_time_expression(recent)
            cutoff = datetime.now() - time_delta
        except ValueError:
            raise ValueError(f"Invalid time expression: '{recent}'. Valid examples: "
                            "'2 days', '1 week 3 hours', 'yesterday'")
    
    # Handle custom time ranges
    if time_range is not None:
        time_comparison = "range"
    if time_comparison == "range" and time_range:
        if len(time_range) == 2:
            if any(isinstance(t, str) for t in time_range):
                time_range=(str2date(t,return_obj=True) for t in time_range)
        else:
            raise ValueError("time_range must be tuple of two datetime objects")
        custom_range = time_range
    
    # Get file listing
    df = ls(
        rootdir,
        kind=kind,
        ignore_case=ignore_case,
        filter=filter,
        sort_by=sort_by,
        ascending=ascending,
        contains=contains,
        booster=booster,
        depth=depth,
        hidden=hidden,
        orient="df" if output == "df" else "list",
        output="df",
        verbose=verbose,
        suffix_list=suffix_list,
        workers=workers,
        root_anchor=root_anchor,
    )
    
    # Return empty result if no files found
    if df.empty if isinstance(df, pd.DataFrame) else not df:
        return pd.DataFrame() if output == "df" else []
    
    # Convert to DataFrame if needed
    if not isinstance(df, pd.DataFrame):
        df = pd.DataFrame(df)
    
    # Time-based filtering 
    if time_column is None:
        time_column=df.column(sort_by)[0] 
    
    if time_comparison == "newer" and cutoff:
        df = df[df[time_column] >= cutoff]
    elif time_comparison == "older" and cutoff:
        df = df[df[time_column] <= cutoff]
    elif time_comparison == "range" and custom_range:
        start, end = custom_range
        df = df[(df[time_column] >= start) & (df[time_column] <= end)]
    
    # Size-based filtering
    if min_size is not None:
        df = df[df["size"] >= min_size]
    if max_size is not None:
        df = df[df["size"] <= max_size]
    
    # Final sorting
    if sort_by:
        valid_sort_cols = set(df.columns)
        if sort_by not in valid_sort_cols:
            raise ValueError(f"Invalid sort column '{sort_by}'. Valid columns: {valid_sort_cols}")
        df = df.sort_values(sort_by, ascending=ascending)
    
    # Convert to requested output format
    if output == "df":
        return df
    
    orient = orient.lower()
    if orient == "list":
        return df.to_dict("records")
    elif orient == "dict":
        return df.to_dict()
    elif orient == "records":
        return df.to_dict("records")
    elif orient == "index":
        return df.to_dict("index")
    elif orient == "series":
        return df.to_dict("series")
    
    return df


def listpkg(where="env", verbose=False):
    """list all pacakages"""

    def listfunc_(lib_name, opt="call"):
        """list functions in specific lib"""
        if opt == "call":
            funcs = [
                func
                for func in dir(lib_name)
                if callable(getattr(lib_name, func))
                if not func.startswith("__")
            ]
        else:
            funcs = dir(lib_name)
        return funcs

    if any([i in where.lower() for i in ["all", "env"]]):
        import pkg_resources

        lst_pkg = [pkg.key for pkg in pkg_resources.working_set]
    else:
        lst_pkg = listfunc_(where)
    print(lst_pkg) if verbose else None
    return lst_pkg

def local_path(fpath, station=r"Q:\IM\IM2_AML\sandbox\dev\tmp\\"):
    """copy file to a specific folder first, to aviod file conflict"""
    try:
        new_path=fbackup(fpath, station, interval=12*3600,verbose=0)
    except Exception as e:
        print(f"Path did not update because: Error:{e}")
        new_path=fpath
    return new_path

def mv(
    src: Union[str, Path],
    dst: Union[str, Path],
    overwrite: bool = False,
    verbose: bool = True,
    dry_run : bool =True,
    filter: Optional[Union[str, List[str]]] = None,
    booster: bool = False
) -> None:
    """
    Enhanced move function with filter and booster support.
    
    Args:
        src: Source path (file/folder) or directory when using filter
        dst: Destination path
        overwrite: Whether to overwrite existing files
        verbose: Show operation details
        filter: Pattern(s) for selective moving:
            - None: Original behavior (move exact path)
            - str: File extension (e.g., '.txt') or name pattern (e.g., 'temp*')
            - List[str]: Multiple patterns
        booster: Search subdirectories when using filter
    """
    
    def _move_single(src: Path, dst: Path, overwrite: bool, verbose: bool,dry_run: bool) -> bool:
        """Original move logic for single file/folder"""
        try:
            dir_par_dst = os.path.dirname(str(dst))
            if not os.path.isdir(dir_par_dst):
                os.makedirs(dir_par_dst, exist_ok=True)
                
            if dst.is_dir():
                dst = dst / src.name
                
            if dst.exists():
                if overwrite:
                    if dst.is_file():
                        dst.unlink()
                    else:
                        shutil.rmtree(dst)
                else:
                    dst = dst.with_name(
                        f"{dst.stem}_{datetime.now().strftime('_%H%M%S')}{dst.suffix}"
                    )
            
            if dry_run:
                print(f"'dry_run' is True:\n\t{src} => {dst}")
                return 
            shutil.move(str(src), str(dst))
            if verbose:
                print(f"\nDone! Moved to {dst}\n")
            return True
            
        except Exception as e:
            logging.error(f"Failed to move {src} to {dst}: {str(e)}")
            return False

    try:
        src_path = Path(src).resolve()
        dst_path = Path(dst).resolve()
        # Prevent infinite loop check
        if booster:
            try:
                if dst_path.is_relative_to(src_path):
                    logging.error("Destination cannot be inside source directory when using booster mode")
                    return None
            except ValueError:
                pass  # Different drive case
        if filter is None:
            return _move_single(src_path, dst_path, overwrite, verbose, dry_run)
            
        if not src_path.exists():
            print(f"Source path '{src_path}' does not exist")
            return
            
        if not src_path.is_dir():
            print("Filter mode requires source to be a directory")
            return
            
        filters = [filter] if isinstance(filter, str) else filter
        moved_count = 0
        
        for pattern in filters:
            # Handle extension patterns (starting with .)
            if pattern.startswith('.'):
                search_pattern = f"*{pattern}"
            else:
                search_pattern = pattern
                
            matches = src_path.rglob(search_pattern) if booster else src_path.glob(search_pattern)
            
            for item in matches:
                if not item.exists():  # Skip broken symlinks
                    continue
                    
                # Calculate relative path and create destination path
                rel_path = item.relative_to(src_path)
                target_path = dst_path / rel_path
                
                # Ensure parent directory exists
                target_path.parent.mkdir(parents=True, exist_ok=True)
                
                if _move_single(item, target_path, overwrite, verbose):
                    moved_count += 1
        
        if verbose:
            print(f"\nDone! Moved {moved_count} items matching filter(s) {filters}\n")

    except Exception as e:
        logging.error(f"Move error: {str(e)}")

def sort_folder(fpath, dir_save = None, blacklist=None):
    f=ls(fpath, verbose=False)
    
    if dir_save is None:
        dir_save= os.path.join(f.rootdir[0], "Finder_Sorted")
    elif isa(dir_save,"dir") and isinstance(dir_save, str):
        dir_save= os.path.join(f.rootdir[0], dir_save)
        
    for type in [i[1:].upper() for i in flatten(f.kind) if "." in str(i)]:
        if blacklist is not None and any([str(i).lower() in type.lower() for i in blacklist]):
            print(f"type: {type}")
            continue 

        dir_base = mkdir(dir_save, type, overwrite=False)
        try:
            [mv(ii, dir_base,overwrite=True, verbose=False) for ii in ls(fpath, type, verbose=False).path]
        except Exception as e:
            print(f"Error: here1: {e}")


# ============================================================
# rsync backend
# ============================================================
def _is_remote_path(path: Union[str, Path]) -> bool:
    """
    Detect if a path is a remote path (SSH style or cloud).
    Examples:
        'gata2:/home/user/file'  -> True
        's3:bucket/path'          -> True
        '/home/user/file'         -> False
    """
    if isinstance(path, Path):
        path = str(path)

    # Cloud storage prefixes
    cloud_prefixes = ("s3:", "gcs:", "azure:", "onedrive:", "remote:")
    if path.startswith(cloud_prefixes):
        return True

    # SSH-style remote path: host:path
    if ":" in path:
        parts = path.split(":", 1)
        # Ignore Windows drive letters like C:\
        if not (len(parts[0]) == 1 and parts[0].isalpha()):
            return True

    return False


def _detect_engine(src: Union[str, Path], dst: Union[str, Path]) -> str:
    """Detect which copy engine to use based on destination path."""
    src_str = str(src)
    dst_str = str(dst)
    
    # Check for cloud storage
    cloud_prefixes = ("s3:", "gcs:", "azure:", "onedrive:", "remote:")
    
    if any(src_str.startswith(prefix) for prefix in cloud_prefixes) or \
       any(dst_str.startswith(prefix) for prefix in cloud_prefixes):
        return "rclone"

    # Check SSH-style remote path
    if _is_remote_path(src) or _is_remote_path(dst):
        return "rsync"
    
    return "local"


def _safe_mkdir(path: Path, dry_run: bool = False, verbose: bool = False):
    """Safely create directory with dry-run support."""
    if not path.exists():
        if dry_run and verbose:
            print(f"[DRY RUN] Would create directory: {path}")
        elif not dry_run:
            path.mkdir(parents=True, exist_ok=True)
 
def _apply_sleep(sleep_duration: float, verbose: bool = True, dry_run: bool = False):
    """Apply sleep with optional verbose output."""
    if sleep_duration > 0:
        prefix = "[DRY RUN] " if dry_run else ""
        if verbose:
            print(f"{prefix}Sleeping for {sleep_duration} seconds...")
        if not dry_run:
            time.sleep(sleep_duration)

def copy_rsync(
    src: Union[str, Path],
    dst: str,
    *,
    filter: Optional[list[str]] = None,
    bwlimit: Optional[int] = None,  # default MB/s
    chunk_size: Optional[int] = None,
    chunk_mode: str = "file",      # "file" | "dir"
    low_priority: bool = False,
    whole_file: bool = True,
    inplace: bool = False,
    sleep: float = 0.0,
    dry_run: bool = False,
    verbose: bool = True,
):
    """
    rsync-based copy backend with throttling & chunking support.

    Designed for:
    - SSH targets
    - S3-mounted filesystems (s3fs, goofys)
    - Large FASTQ / BAM files
    rsync backend with:
      - file-level chunking
      - directory-level chunking
      - safe single-file passthrough
    """

    IS_LINUX = sys.platform.startswith("linux")
    IS_MAC = sys.platform == "darwin"
    src_path = Path(src).resolve()

    if chunk_size is not None and chunk_size <= 0:
        raise ValueError("chunk_size must be > 0")
    # Validate bwlimit
    if bwlimit is not None:
        if bwlimit <= 0:
            raise ValueError("bwlimit must be > 0 (in MB/s)")
        if bwlimit < 0.1:
            print(f"Warning: Very low bwlimit ({bwlimit} MB/s). Using 0.1 MB/s minimum.")
            bwlimit_kbps = 100  # 0.1 MB/s * 1000 = 100 KB/s
        else:
            # Convert MB/s to KB/s (rsync uses KB/s)
            bwlimit_kbps = int(bwlimit * 1024)
    # ----------------------------
    # rsync base command
    # ----------------------------
    base_cmd = ["rsync", "-avh", "--progress", "--partial"]

    if whole_file:
        base_cmd.append("--whole-file")
    if inplace:
        base_cmd.append("--inplace")
    if bwlimit:
        base_cmd.append(f"--bwlimit={bwlimit_kbps}")
    if dry_run:
        base_cmd.append("--dry-run")

    if filter:
        base_cmd.append("--include=*/")
        for f in (filter if isinstance(filter, list) else [filter]):
            base_cmd.append(f"--include=*{f}" if f.startswith(".") else f"--include={f}")
        base_cmd.append("--exclude=*")

    def run(cmd):
        # Wrap with nice/ionice if low_priority is True
        if low_priority:
            if IS_LINUX:
                cmd = ["ionice", "-c2", "-n7", "nice", "-n", "19"] + cmd
            elif IS_MAC:
                cmd = ["nice", "-n", "19"] + cmd

        if verbose:
            print("Running:", " ".join(cmd))
        if not dry_run:
            subprocess.run(cmd, check=True)

    # ========================================================
    # CHUNK MODE
    # ========================================================
    if chunk_size:
        # ---------- FILE MODE ----------
        if chunk_mode == "file":
            if src_path.is_file():
                run(base_cmd + [str(src_path), dst])
                return Path(dst) if ":" not in dst else dst

            files = [p for p in src_path.rglob("*") if p.is_file()]
            if not files:
                if verbose:
                    print("[INFO] No files to copy")
                return None

            for i in range(0, len(files), chunk_size):
                batch = files[i:i + chunk_size]

                with tempfile.NamedTemporaryFile("w", delete=False) as f:
                    for p in batch:
                        f.write(str(p.relative_to(src_path)) + "\n")
                    filelist = f.name

                cmd = base_cmd + [
                    "--files-from", filelist,
                    "--relative",
                    str(src_path),
                    dst,
                ]
                run(cmd)
                
                # Apply sleep after each chunk
                _apply_sleep(sleep, verbose, dry_run)

            return Path(dst) if ":" not in dst else dst

        # ---------- DIR MODE ----------
        if chunk_mode == "dir":
            if src_path.is_file():
                raise ValueError("chunk_mode='dir' requires directory source")

            dirs = [p for p in src_path.iterdir() if p.is_dir()]
            if not dirs:
                if verbose:
                    print("[INFO] No subdirectories found")
                return None

            for i in range(0, len(dirs), chunk_size):
                batch = dirs[i:i + chunk_size]

                cmd = base_cmd + [*(str(d) for d in batch), dst]
                run(cmd)
                
                _apply_sleep(sleep, verbose, dry_run)

            return Path(dst) if ":" not in dst else dst

        raise ValueError("chunk_mode must be 'file' or 'dir'")

    # ========================================================
    # NORMAL rsync
    # ========================================================
    run(base_cmd + [str(src_path), dst])
    return Path(dst) if ":" not in dst else dst


# ============================================================
# rclone backend
# ============================================================

def copy_rclone(
    src: Union[str, Path],
    dst: str,
    *,
    filter: Optional[list[str]] = None,
    sleep: float = 0.0,
    dry_run: bool = False,
    verbose: bool = True,
):
    """
    rclone-based backend for cloud storage.

    Use this for:
    - S3, GCS, Azure, OneDrive
    - Object storage semantics

    Parameters:
        src: Local directory
        dst: rclone remote path (e.g. s3:bucket/path)
        sleep: Sleep duration in seconds between operations
        dry_run: rclone --dry-run
        verbose: Print command
    """
    def rclone_filter_args(filters: list[str]) -> list[str]:
        args = []
        for f in filters:
            if f.startswith("."):
                args.append(f"--include=*{f}")
            else:
                args.append(f"--include={f}")
        return args

    cmd = ["rclone", "copy", str(src), dst, "--progress"]

    if filter:
        if not isinstance(filter, list):
            filter = [filter]
        cmd.extend(rclone_filter_args(filter))

    if dry_run:
        cmd.append("--dry-run")

    if verbose:
        print("Running:", " ".join(cmd))

    if dry_run:
        print("[DRY RUN] Would run:", " ".join(cmd))
        return None
    else:
        subprocess.run(cmd, check=True)
        _apply_sleep(sleep, verbose, dry_run)

        return dst


def cp(
    src: Union[str, Path],
    dst: Union[str, Path],
    engine: str = None,
    overwrite: bool = False,
    exist_ok: bool = False,
    filter: Optional[Union[str, List[str]]] = None,
    booster: bool = False,
    *,
    bwlimit: Optional[int] = None,
    chunk_size: Optional[int] = None,
    chunk_mode: str = "file",
    sleep: float = 0.0,
    low_priority: bool = False,
    dry_run: bool = False,
    verbose: bool = True,
) -> Optional[Path]:
    """
    Enhanced file/folder copy utility with advanced features like filtering, recursive copying,
    conflict resolution, and dry-run simulation. 
    Args:
        src: Source path (file or directory)
        dst: Destination path
        overwrite: Overwrite existing files (default: False)
        exist_ok: Skip existing files instead of renaming (default: False)
        verbose: Show detailed operation logs (default: True)
        filter: Pattern(s) for selective copying:
            - None: Copy exact path (default)
            - str: Single pattern (e.g., '.txt', 'temp*')
            - List[str]: Multiple patterns
        booster: Search subdirectories when using filter (default: False)
        dry_run: Simulate operation without actual copying (default: False)
        sleep: Sleep duration in seconds between operations (default: 0.0)

    Returns:
        Path: Path to last copied item (or what would be copied in dry-run)
        None: If operation failed or nothing was copied

    Examples:

        1. Basic file copy:
           >>> cp('file.txt', 'backups/')
           Done! Copied to /path/to/backups/file.txt

        2. Folder copy with overwrite:
           >>> cp('project/', 'backups/project/', overwrite=True)
           Done! Copied to /path/to/backups/project/

        3. Skip existing files:
           >>> cp('data/', 'archive/', exist_ok=True)
           Skipped existing file: /path/to/archive/data/file1.txt
           Done! Copied 3 items

        4. Filter by extension:
           >>> cp('docs/', 'pdf_backups/', filter='.pdf')
           Done! Copied 5 items matching filter(s) ['.pdf']

        5. Multiple filters with booster (recursive):
           >>> cp('project/', 'backups/', 
           ...       filter=['.py', 'README*'], booster=True)
           Done! Copied 12 items matching filter(s) ['.py', 'README*']

        6. Dry-run simulation:
           >>> cp('data/', 'backup/', filter='.csv', dry_run=True)
           [DRY RUN] Would copy file: data/orders.csv -> backup/orders.csv
           [DRY RUN] Would copy 3 items matching filter(s) ['.csv']

        7. Complex example with all features:
           >>> cp('source/', 'destination/',
           ...       filter=['.jpg', '.png'],
           ...       overwrite=True,
           ...       booster=True,
           ...       verbose=False,
           ...       dry_run=True)
           [Would copy 42 items matching filter(s) ['.jpg', '.png']]

    Notes:
        - When overwrite=False and exist_ok=False, conflicted files get timestamp suffixes
        - booster=True enables recursive searching when using filters
        - dry_run=True shows what would happen without modifying filesystem
        - Return value is always None in dry-run mode
        - sleep parameter adds delays between operations (useful for rate limiting)
    """

    VERBOSE = verbose

    #  below is 'local'
    def _copy_single(src: Path, dst: Path, overwrite: bool, exist_ok: bool, verbose: bool) -> Optional[Path]:
        """Original copy logic for single file/folder"""
        try:
            dst_parent = dst.parent  # Ensure parent directory exists
            _safe_mkdir(dst_parent, dry_run, VERBOSE)
            
            # Apply sleep before copy operation
            _apply_sleep(sleep, VERBOSE, dry_run)
            
            if not src.is_dir():
                # If destination is a directory, append source filename
                if dst.exists() and dst.is_dir():
                    dst = dst / src.name
                    
                if dst.exists():
                    if overwrite:
                        if dry_run:
                            if VERBOSE:
                                print(f"[DRY RUN] Would delete existing file: {dst}")
                        else:
                            dst.unlink()
                    elif exist_ok:
                        if VERBOSE:
                            print(f"{'[DRY RUN] ' if dry_run else ''}Skipped existing file: {dst}")
                        return None
                    else:
                        # Add timestamp suffix
                        timestamp = datetime.now().strftime('%y%m%d_%H%M%S')
                        new_name = f"{dst.stem}_{timestamp}{dst.suffix}"
                        dst = dst.with_name(new_name)
                        if dry_run and VERBOSE:
                            print(f"[DRY RUN] Would rename destination to: {dst}")
                if dry_run:
                    if VERBOSE:
                        print(f"[DRY RUN] Would copy file: {src} -> {dst}")
                    return dst
                        
                shutil.copy2(str(src), str(dst))
                if VERBOSE:
                    print(f"\nDone! Copied to {dst}\n")
                return dst
                
            else:
                # If destination exists and is a directory, append source directory name
                if dst.exists() and dst.is_dir():
                    dst = dst / src.name
                if dst.exists():
                    if overwrite:
                        if dry_run:
                            if VERBOSE:
                                print(f"[DRY RUN] Would delete existing directory: {dst}")
                        else:
                            shutil.rmtree(str(dst))
                    elif exist_ok:
                        if VERBOSE:
                            print(f"{'[DRY RUN] ' if dry_run else ''}Skipped existing directory: {dst}")
                        return None
                    else:
                        # Add timestamp suffix
                        timestamp = datetime.now().strftime('%y%m%d_%H%M%S')
                        new_name = f"{dst.stem}_{timestamp}"
                        dst = dst.with_name(new_name)
                        if dry_run and VERBOSE:
                            print(f"[DRY RUN] Would rename destination to: {dst}")
                if dry_run:
                    if VERBOSE:
                        print(f"[DRY RUN] Would copy directory: {src} -> {dst}")
                    return dst
                
                # Actually copy the directory
                shutil.copytree(str(src), str(dst), copy_function=shutil.copy2, dirs_exist_ok=overwrite)
                if VERBOSE:
                    print(f"\nDone! Copied to {dst}\n")
                return dst
                
        except Exception as e:
            print(f"{'[DRY RUN] ' if dry_run else ''}Failed to copy {src} to {dst}: {str(e)}")
            return None

    # main func
    if engine is None:
        engine = _detect_engine(src, dst)

    if engine == "rsync":
        return copy_rsync(
            src,
            str(dst),
            filter=filter,
            bwlimit=bwlimit,
            chunk_size=chunk_size,
            chunk_mode=chunk_mode,
            sleep=sleep,
            low_priority=low_priority,
            dry_run=dry_run,
            verbose=VERBOSE,
        )

    if engine == "rclone":
        return copy_rclone(
            src, 
            str(dst), 
            filter=filter, 
            sleep=sleep,
            dry_run=dry_run, 
            verbose=VERBOSE
        )
    
    try:
        src_path = Path(src).resolve()
        dst_path = Path(dst).resolve()
        
        if not src_path.exists():
            print(f"Source path '{src_path}' does not exist")
            return None
        
        if dry_run and VERBOSE:
            print("\n[DRY RUN] Running in simulation mode - no files will be actually copied\n")
            
        # Prevent infinite loop check
        if booster:
            try:
                if dst_path.is_relative_to(src_path):
                    print("Destination cannot be inside source directory when using booster mode")
                    return None
            except ValueError:
                pass  # Different drive case
                
        if filter is None:
            return _copy_single(src_path, dst_path, overwrite, exist_ok, VERBOSE)

        if not src_path.is_dir():
            print("Filter mode requires source to be a directory")
            return None
            
        filters = [filter] if isinstance(filter, str) else filter
        copied_count = 0
        last_copied = None
        
        for pattern in filters:
            # Handle extension patterns (starting with .)
            if pattern.startswith('.'):
                search_pattern = f"*{pattern}"
            else:
                search_pattern = pattern
            
            # Use rglob for recursive search if booster is True, otherwise glob 
            matches = src_path.rglob(search_pattern) if booster else src_path.glob(search_pattern)
            
            for item in matches:
                if not item.exists():  # Skip broken symlinks
                    continue
                # Skip directories when filtering (unless pattern explicitly matches them)
                if item.is_dir():
                    continue
                # Calculate relative path and create destination path
                rel_path = item.relative_to(src_path)
                target_path = dst_path / rel_path
                _safe_mkdir(target_path.parent, dry_run, VERBOSE)
                result = _copy_single(item, target_path, overwrite, exist_ok, VERBOSE)
                if result:
                    copied_count += 1
                    last_copied = result
        
        if VERBOSE:
            action = "Would copy" if dry_run else "Copied"
            print(f"\n{dry_run and '[DRY RUN] ' or ''}{action} {copied_count} items matching filter(s) {filters}\n")
        return last_copied
        
    except Exception as e:
        print(f"{'[DRY RUN] ' if dry_run else ''}Copy error: {str(e)}")
        return None

def delete(
    fpath: Union[str, Path],
    filter: Optional[Union[str, List[str]]] = None,
    backup: bool = False,
    backup_dir: str = "bak", 
    booster: bool = False,
    dry_run: bool = False,
    verbose: bool = True
) -> None:
    """
    Powerful deletion function with booster mode for deep searching.

    Args:
        fpath: Path to file/folder OR parent directory when using filter
        filter: Optional filter pattern(s) for selective deletion:
            - None: Delete exact path (original behavior)
            - str:
                - If contains '.' treat as extension (e.g., '.tmp')
                - Else treat as exact filename (e.g., 'tempfile')
            - List[str]: Multiple filters to apply
        booster: Enable deep searching through all subdirectories (supercharged mode)
        dry_run: If True, show what would be deleted but don't delete anything


    Behavior:
    1. When filter is None: Original behavior (delete exact path)
    2. With filter: Delete matching items in fpath's parent directory
    3. With booster=True: Search deeply through all subdirectories
    """


    def _delete_single_path(path: Path,verbose) -> bool:
        """Delete a single file or folder, returns success status"""
        try:
            if not path.exists():
                if verbose: print(f"Path '{path}' does not exist")
                return False
            if dry_run:
                if verbose: print(f"[DRY RUN] Would delete: {path}")
                return True
            if backup: fbackup(path, backup_dir=backup_dir)
            if path.is_file():
                path.unlink()
                if verbose: print(f"Deleted file: {path}")
                return True
            elif path.is_dir():
                shutil.rmtree(path)
                if verbose: print(f"Deleted folder: {path}")
                return True
            return False
        except Exception as e:
            logging.error(f"Failed to delete {path}: {str(e)}")
            return False
    if isinstance(fpath,list):
        [delete(fpath_,filter=filter,backup=backup,backup_dir=backup_dir, booster=booster,dry_run=dry_run) for fpath_ in fpath]
        return
    try:
        fpath = Path(fpath).resolve()

        # Original behavior when no filter
        if filter is None:
            _delete_single_path(fpath,verbose)
            return

        # Convert single filter to list for uniform processing
        filters = [filter] if isinstance(filter, str) else filter

        # Determine search root directory
        parent = fpath.parent if fpath.is_file() else fpath
        if not parent.exists():
            print(f"Path '{parent}' does not exist")
            return

        deleted_count = 0

        # Process each filter
        for pattern in filters:
            # Determine search method based on booster mode
            if booster:
                matches = parent.rglob(f"*{pattern}" if "." in pattern else pattern)
            else:
                matches = parent.glob(f"*{pattern}" if "." in pattern else pattern)

            for item in matches:
                if _delete_single_path(item,verbose):
                    deleted_count += 1
        action = "Will delete" if dry_run else "Done! Deleted"
        print(f"\n{action} {deleted_count} items matching filter(s): {filters}\n") 

    except Exception as e:
        logging.error(f"Deletion error: {str(e)}") 

def rm(
    fpath: Union[str, Path],
    dry_run: bool = False,
    filter: Optional[Union[str, List[str]]] = None,
    backup: bool = False,
    backup_dir: str = "bak", 
    booster: bool = False,
    verbose: bool = True
) -> None:
    """
    Powerful deletion function with booster mode for deep searching.

    Args:
        fpath: Path to file/folder OR parent directory when using filter
        filter: Optional filter pattern(s) for selective deletion:
            - None: Delete exact path (original behavior)
            - str:
                - If contains '.' treat as extension (e.g., '.tmp')
                - Else treat as exact filename (e.g., 'tempfile')
            - List[str]: Multiple filters to apply
        booster: Enable deep searching through all subdirectories (supercharged mode)
        dry_run: If True, show what would be deleted but don't delete anything


    Behavior:
    1. When filter is None: Original behavior (delete exact path)
    2. With filter: Delete matching items in fpath's parent directory
    3. With booster=True: Search deeply through all subdirectories
    """
    def _delete_single_path(path: Path,verbose) -> bool:
        """Delete a single file or folder, returns success status"""
        try:
            if not path.exists():
                print(f"Path '{path}' does not exist") if verbose else None
                return False
            if dry_run:
                print(f"[DRY RUN] Would delete: {path}") if verbose else None
                return True
            if backup:
                fbackup(path, backup_dir=backup_dir)
            if path.is_file():
                path.unlink()
                print(f"Deleted file: {path}") if verbose else None
                return True
            elif path.is_dir():
                shutil.rmtree(path)
                print(f"Deleted folder: {path}") if verbose else None
                return True
            return False
        except Exception as e:
            logging.error(f"Failed to delete {path}: {str(e)}")
            return False
    if isinstance(fpath,list):
        [rm(fpath_,filter=filter,backup=backup,backup_dir=backup_dir, booster=booster,dry_run=dry_run) for fpath_ in fpath]
        return
    try:
        fpath = Path(fpath).resolve()

        # Original behavior when no filter
        if filter is None:
            _delete_single_path(fpath,verbose)
            return

        # Convert single filter to list for uniform processing
        filters = [filter] if isinstance(filter, str) else filter

        # Determine search root directory
        parent = fpath.parent if fpath.is_file() else fpath
        if not parent.exists():
            print(f"Path '{parent}' does not exist")
            return

        deleted_count = 0

        # Process each filter
        for pattern in filters:
            # Determine search method based on booster mode
            if booster:
                matches = parent.rglob(f"*{pattern}" if "." in pattern else pattern)
            else:
                matches = parent.glob(f"*{pattern}" if "." in pattern else pattern)

            for item in matches:
                if _delete_single_path(item,verbose):
                    deleted_count += 1
        action = "Will delete" if dry_run else "Done! Deleted"
        print(f"\n{action} {deleted_count} items matching filter(s): {filters}\n") 

    except Exception as e:
        logging.error(f"Deletion error: {str(e)}")

BATCH_SIZE = 64 * 1024 * 1024  # 64 MB chunk
# Picklable top-level worker for multiprocessing 
# def _parallel_worker_picklable(file, pattern, ignore_case):
#     try:
#         # Call grep but collect results properly
#         results = []
#         for line in _grep(file, pattern, engine="substring", ignore_case=ignore_case, verbose=False):
#             results.append(line)
#         return results
#     except Exception as e:
#         # Return empty list on any error
#         return []
def _parallel_worker_picklable(file, pattern, ignore_case):
    """Simple substring search for parallel processing"""
    try:
        from pathlib import Path
        import os
        
        file = Path(file) if not isinstance(file, Path) else file
        pat_bytes = pattern.lower().encode() if ignore_case else pattern.encode()
        results = []
        
        with file.open("rb") as f:
            for lineno, line_bytes in enumerate(f, 1):
                # Case-insensitive comparison if needed
                if ignore_case:
                    line_to_search = line_bytes.lower()
                else:
                    line_to_search = line_bytes
                    
                if pat_bytes in line_to_search:
                    try:
                        line_text = line_bytes.decode('utf-8', errors='ignore').rstrip()
                    except:
                        line_text = line_bytes.decode('latin-1', errors='ignore').rstrip()
                    results.append(f"{file}:{lineno}:{line_text}")
        return results
    except Exception as e:
        # Log error if debugging
        # print(f"Error processing {file}: {e}")
        return []
def _grep(
    path,
    pattern,
    *,
    engine: str = None,
    ignore_case: bool = True,
    mmap_threshold: int = 50 * 1024 * 1024,
    parallel_file_threshold: int = None,
    workers: int = None,
    verbose: bool = True
): 
    """
    a grep-like search with auto-selected engines.

    Engines:
      - None (auto)
      - "line"
      - "substring"
      - "mmap"
      - "parallel"

    Returns: generator of "file:line:content"

    Usage:
    # 1. Search a single file
    for line in grep("app.log","ERROR"):
        print(line)
    # 2. Search a directory recursively
    for line in grep("/var/log", "timeout"):
        print(line)

    ‚ö° Force a specific engine
    3. Line-by-line (classic grep behavior)
    for line in grep("logs/", "^WARN", engine="line"):
        print(line)
    Use when:
        Small text files
        Regex-heavy patterns
        Predictable output order
    4.  Substring engine (FASTEST for plain text)
    for line in grep( "logs/", "SUCCESS",engine="substring"):
        print(line)
    Use when:
        No regex
        Large files
        You want maximum speed
    Rule: No regex ‚Üí substring beats grep
    5. mmap engine (big files)
    for line in grep("huge.log", "Exception", engine="mmap"):
        print(line)
    Use when:
        File > 50MB
        SSD available
        You want fewer syscalls
    """
    import mmap
    from concurrent.futures import ThreadPoolExecutor
    
    path = Path(path)
    # Handle both files and directories
    if path.is_file():
        files = [path]
    else:
        files = [f for f in path.rglob("*") if f.is_file()]
    
    if not files:
        if verbose:
            print(f"No files found at {path}")
        return

    total_size = sum(f.stat().st_size for f in files)
    is_regex = bool(re.search(r"[.^$*+?{}\[\]|()]", pattern))

    parallel_file_threshold = parallel_file_threshold or (os.cpu_count() or 4) * 2

    if engine is not None:
        engine = strcmp(engine, ["line", "substring", "mmap", "parallel"])[0]
    # Auto engine selection
    if engine is None:
        if len(files) >= parallel_file_threshold:
            engine = "parallel"
        elif total_size >= mmap_threshold:
            engine = "mmap"
        elif not is_regex:
            engine = "substring"
        else:
            engine = "line"
    
    def engine_line(file):
        flags = re.IGNORECASE if ignore_case else 0
        rx = re.compile(pattern, flags)
        with file.open("r", errors="ignore") as f:
            for lineno, line in enumerate(f, 1):
                if rx.search(line):
                    yield f"{file}:{lineno}:{line.rstrip()}"

    def engine_substring_thread(chunk, offset, pat_bytes):
        """
        Search a single chunk for substring matches.
        Returns list of (line_number, line_text)
        """
        results = []
        lines = chunk.split(b"\n")
        line_offset = offset
        for i, line in enumerate(lines):
            if pat_bytes in line:
                try:
                    decoded_line = line.decode(errors="ignore").rstrip()
                except:
                    decoded_line = ""
                results.append((line_offset + i + 1, decoded_line))
        return results

    def engine_substring(file):
        pat_bytes = pattern.encode()
        file_size = file.stat().st_size
        results = []

        # Use threads if file is huge
        if file_size >= mmap_threshold:
            n_workers = workers or (os.cpu_count() or 4)
            offsets = []
            chunks = []
            with file.open("rb") as f:
                offset = 0
                while offset < file_size:
                    read_size = min(BATCH_SIZE, file_size - offset)
                    f.seek(offset)
                    chunk = f.read(read_size)
                    # Make sure chunk ends at newline
                    if offset + read_size < file_size:
                        extra = f.readline()
                        chunk += extra
                    chunks.append(chunk)
                    offsets.append(offset)
                    offset += read_size

            with ThreadPoolExecutor(n_workers) as pool:
                futures = []
                for chunk, off in zip(chunks, offsets):
                    futures.append(pool.submit(engine_substring_thread, chunk, off, pat_bytes))
                for future in futures:
                    for lineno, line in future.result():
                        yield f"{file}:{lineno}:{line}"
        else:
            # Small file, normal substring scan
            with file.open("rb") as f:
                for lineno, line in enumerate(f, 1):
                    if pat_bytes in line:
                        yield f"{file}:{lineno}:{line.decode(errors='ignore').rstrip()}"

    def engine_mmap(file):
        flags = re.IGNORECASE if ignore_case else 0
        rx = re.compile(pattern.encode(), flags)
        with file.open("rb") as f:
            mm = mmap.mmap(f.fileno(), 0, access=mmap.ACCESS_READ)
            for m in rx.finditer(mm):
                start = mm.rfind(b"\n", 0, m.start()) + 1
                end = mm.find(b"\n", m.end())
                line = mm[start:end].decode(errors="ignore")
                lineno = mm[:start].count(b"\n") + 1
                yield f"{file}:{lineno}:{line.rstrip()}"
            mm.close()

    # -------------------------
    # Execution
    # -------------------------
    if engine == "parallel":
        n_workers = workers or (os.cpu_count() or 4)
        args_list = [(f, pattern, ignore_case) for f in files]
        from multiprocessing import Pool

        # Use multiprocessing pool for multiple files
        with Pool(n_workers) as pool:
            results = pool.starmap(_parallel_worker_picklable, args_list)
            for lines in results:
                for line in lines:
                    # if verbose:
                    #     print(line)
                    yield line
    else:
        engine_func = {
            "line": engine_line,
            "substring": engine_substring,
            "mmap": engine_mmap,
        }[engine]

        for f in files:
            for line in engine_func(f):
                # if verbose:
                #     print(line)
                yield line
def grep(
    fpath,
    keyword,
    kind = None,
    filter = None,
    engine: str = None,
    ignore_case: bool = True,
    mmap_threshold: int = 50 * 1024 * 1024,
    parallel_file_threshold: int = None,
    workers: int = None,
    verbose: bool = True,
    **kwargs
):

    r"""
        a = grep(
            "/Users/macjianfeng/Dropbox/00-Personal/@Pappelweg 8/",
            "euros",
            kind="eml",
            filter="*Final Transfer*",
            depth=None,
            verbose=False,
        )

        for i, j in enumerate(a):
            s1, s2 = ssplit(j, r":\d+:")
            print(f"{i+1}. {os.path.basename(s1)}\n\t{s2}")
    """
    if os.path.isdir(fpath):
        f=ls(fpath,kind=kind, filter=filter,**kwargs)
        if f.empty:
            print(f"cannot find a {kind} file") if kind is not None else print("cannot find a file")
            return 
        fpaths=f.path
    else:
        fpaths=[fpath]
    res_all=[]
    for path in fpaths:
        res=_grep(
            path=path,
            pattern=keyword,
            engine = engine,
            ignore_case = ignore_case,
            mmap_threshold=mmap_threshold,
            parallel_file_threshold=parallel_file_threshold,
            workers=workers,
            verbose=verbose
        )
        if res:
            res_all.extend(unique([i for i in res if i]))

    if verbose: 
        for i, j in enumerate(res_all):
            try:
                s1, s2 = ssplit(j, r":\d+:")
                print(
                    f"{i+1}. [FOLDER] {os.path.dirname(s1)}\n\t[FILE] {os.path.basename(s1)}\n[DETAILS] {s2}\n"
                )
            except Exception as e:
                print(e)
    return res_all

def flist(
    fpath: Union[str, Path],
    filter: Optional[Union[str, List[str]]] = None,
    booster: bool = False, 
) -> None:
    """
    Powerful detection function with booster mode for deep searching.

    Args:
        fpath: Path to file/folder OR parent directory when using filter
        filter: Optional filter pattern(s) for selective deletion:
            - None: Delete exact path (original behavior)
            - str:
                - If contains '.' treat as extension (e.g., '.tmp')
                - Else treat as exact filename (e.g., 'tempfile')
            - List[str]: Multiple filters to apply
        booster: Enable deep searching through all subdirectories (supercharged mode)
        dry_run: If True, show what would be deleted but don't delete anything


    Behavior:
    1. When filter is None: Original behavior (delete exact path)
    2. With filter: Delete matching items in fpath's parent directory
    3. With booster=True: Search deeply through all subdirectories
    """

    def _detect_single_path(path: Path) -> bool:
        """Delete a single file or folder, returns success status"""
        try:
            if not path.exists():
                print(f"Path '{path}' does not exist")
                return False 
            return True
        except Exception as e:
            logging.error(f"Failed to delete {path}: {str(e)}")
            return False

    try:
        fpath = Path(fpath).resolve()

        # Original behavior when no filter
        if filter is None:
            _detect_single_path(fpath)
            return

        # Convert single filter to list for uniform processing
        filters = [filter] if isinstance(filter, str) else filter

        # Determine search root directory
        parent = fpath.parent if fpath.is_file() else fpath
        if not parent.exists():
            print(f"Path '{parent}' does not exist")
            return

        _count,f_found = 0,[]

        # Process each filter
        for pattern in filters:
            # Determine search method based on booster mode
            if booster:
                matches = parent.rglob(f"*{pattern}" if "." in pattern else pattern)
            else:
                matches = parent.glob(f"*{pattern}" if "." in pattern else pattern)

            for item in matches:
                if _detect_single_path(item):
                    _count += 1
                    f_found.append(item)
        print(f"\nFound {_count} items matching filter(s): {filters}\n")
        return f_found
    except Exception as e:
        logging.error(f"Detection error: {str(e)}")
        return

def frename(fpath, dst, smart=True,dry_run=True):
    """Rename a file or folder."""
    try:
        src_kind, dst_kind = None, None
        if smart:
            dir_name_src = os.path.dirname(fpath)
            dir_name_dst = os.path.dirname(dst)
            src_kind = os.path.splitext(fpath)[1]
            dst_kind = os.path.splitext(dst)[1]
            if dir_name_dst != dir_name_src:
                dst = os.path.join(dir_name_src, dst)
            if dst_kind is not None and src_kind is not None:
                if dst_kind != src_kind:
                    dst = dst + src_kind
        if os.path.exists(fpath):
            if dry_run:
                print(f"dry_run is True:\t{fpath} => {dst}")
                return 
            os.rename(fpath, dst)
            print(f"[RENAME]:\t{fpath}\t=>\t{dst}")
        else:
            print(f"Failed: {fpath} does not exist.")
    except Exception as e:
        logging.error(f"Failed to rename {fpath} to {dst}: {e}")

def frename_batch(rootdir,
                kind="pdf",
                by="modified_time",
                pattern=r'^(?!2)(?!\d{6})\S.*', #r'^(?!\d)\S.*', 
                depth=None,
                dry_run=True,
            ):
    r"""
    Batch rename files in a directory based on file metadata and a regex pattern.

    This function renames files in a given root directory (and optionally its subdirectories)
    by prepending a date string (e.g., modified time) to filenames. It only renames files that
    match a specified regex pattern.

    Parameters:
    ----------
    rootdir : str
        Root directory to search for files.
    
    kind : str, default='pdf'
        File extension/type to filter (e.g., 'pdf', 'txt').
    
    by : str, default='modified_time'
        Column in the DataFrame (returned by `ls`) to use for generating the date prefix.
        Common options: 'modified_time', 'created_time'.
    
    pattern : str, default=r'^(?!2)(?!\d{6})\S.*'
        Regular expression pattern used to filter filenames before renaming.
        - Example: r'^(?!2)(?!\\d{6})\\S.*' matches names that do not start with '2'
          and whose first 6 characters are not all digits.
    
    depth : int or None, default=None
        Directory depth to scan. If None, scans all levels recursively.
    
    dry_run : bool, default=True
        If True, prints the intended renaming operations without actually performing them.

    Notes:
    -----
    - Files are renamed to: `<timestamp>_<original_filename>`, where timestamp is
      derived from the selected `by` column (in `YYYYMMDD` format).
    - Uses an external `ls` function to scan and return file metadata as a DataFrame.
    - Uses an external `list_filter` function to apply the regex filter and return indices.
    - Relies on a `rename` function to perform the actual renaming (unless `dry_run` is True).

    Example:
    --------
    >>> rename_batch('/data/docs', kind='pdf', by='modified_time',
                     pattern=r'^(?!2)(?!\\d{6})\\S.*', dry_run=True)
    dry_run is True:    /data/docs/report.pdf /data/docs/20250805_report.pdf
    """
    try:
        df = ls(rootdir, kind="pdf", depth=None) 
        # ^ ‚Äî start of line
        # (?!\d) ‚Äî negative lookahead: line must not start with a digit
        # \S.* ‚Äî match a non-whitespace character followed by anything (i.e., non-empty line)
        idxs=list_filter(df.name.tolist(),
                        pattern = pattern,
                        return_idx=True) [1]
        mtimes=df[by].apply(lambda x: str(x)[:10].replace("-","")).tolist()
        for a,b,c,d in zip([df.rootdir.tolist()[i] for i in idxs],[df.basename.tolist()[i] for i in idxs],[mtimes[i] for i in idxs],[df.path.tolist()[i] for i in idxs]):
            frename(d, os.path.join(a,c+"_"+b),dry_run=dry_run)
    except Exception as e:
        print(e)


def mkdir_old(pardir: str = None, 
          chdir: str | list = None, 
          overwrite: bool = False, 
          return_obj: bool = False
          ):
    """
    Create a directory.

    Parameters:
    - pardir (str): Parent directory where the new directory will be created. If None, uses the current working directory.
    - chdir (str | list): Name of the new directory or a list of directories to create.
                          If None, a default name 'new_directory' will be used.
    - overwrite (bool): If True, overwrite the directory if it already exists. Defaults to False.

    Returns:
    - str: The path of the created directory or an error message.

    Usage:
    # Create a parent directory only
    mkdir("/tmp/my_project")
    # Create parent + one child directory
    mkdir("/tmp/my_project", "data")
    # Create parent + multiple subdirectories
    mkdir(
            pardir="/tmp/my_project",
            chdir=["data", "results", "figures"]
        )
    # Subdirectories already exist (default behavior)
    mkdir("/tmp/my_project", ["data", "results"])
    # Force overwrite existing subdirectories
    mkdir(
            pardir="/tmp/my_project",
            chdir=["data", "results"],
            overwrite=True
        )
    # Using current working directory as parent
    mkdir(None, "logs")
    # project_dirs = mkdir(
            pardir="/data/rnaseq/project_X",
            chdir=[
                "raw_fastq",
                "trimmed_fastq",
                "alignment",
                "counts",
                "qc",
                "figures"
            ]
        )

    """
    if return_obj: 
        from pathlib import Path

    def _to_path(x):
        if isinstance(x, list):
            return [Path(p) for p in x]
        return Path(x) 

    def _mkdir_nest(fpath: str) -> str:
        """
        Create nested directories based on the provided file path.

        Parameters:
        - fpath (str): The full file path for which the directories should be created.

        Returns:
        - str: The path of the created directory.
        """
        # Split the full path into directories
        f_slash = os.sep
        if os.path.isdir(fpath):
            fpath = fpath + f_slash if not fpath.endswith(f_slash) else fpath
            return fpath
        dir_parts = fpath.split(f_slash)  # Split the path by the OS-specific separator

        # Start creating directories from the root to the desired path
        root_dir = os.path.splitdrive(fpath)[
            0
        ]  # Get the root drive on Windows (e.g., 'C:')
        current_path = (
            root_dir if root_dir else f_slash
        )  # Start from the root directory or POSIX '/'

        for part in dir_parts:
            if part:
                current_path = os.path.join(current_path, part)
                if not os.path.isdir(current_path):
                    os.makedirs(current_path)
        if not current_path.endswith(f_slash):
            current_path += f_slash
        return current_path

    rootdir = []
    # pardir is None
    if pardir is None:
        pardir = os.getcwd()
    pardir = _mkdir_nest(pardir)
    if chdir is None: 
        return pardir if not return_obj else _to_path(pardir)
    else:
        pass
    print(pardir)
    if isinstance(chdir, str):
        chdir = [chdir]
    chdir = list(set(chdir))
    if isinstance(pardir, str):  # Dir_parents should be 'str' type
        pardir = os.path.normpath(pardir)
    stype = os.sep

    if os.path.isdir(pardir):
        os.chdir(pardir)  # Set current path
        # Check if subdirectories are not empty
        if chdir:
            chdir.sort()
            for folder in chdir:
                child_tmp = os.path.join(pardir, folder)
                if not os.path.isdir(child_tmp):
                    os.mkdir("./" + folder)
                    print(f"\n {folder} was created successfully!\n")
                else:
                    if overwrite:
                        shutil.rmtree(child_tmp)
                        os.mkdir("./" + folder)
                        print(f"\n {folder} overwrite! \n")
                    else:
                        print(f"\n {folder} already exists! \n")
                rootdir.append(child_tmp + stype)  # Note down
        else:
            print("\nWarning: Dir_child doesn't exist\n")
    else:
        print("\nWarning: Dir_parent is not a directory path\n")
    # Dir is the main output, if only one dir, then str type is inconvenient
    if len(rootdir) == 1:
        rootdir = rootdir[0]
        rootdir = rootdir + stype if not rootdir.endswith(stype) else rootdir

    return rootdir if not return_obj else _to_path(rootdir)


# Path Version
def mkdir(
    parent: Union[str, Path, None] = None,
    children: Union[str, Path, List[Union[str, Path]], None] = None,
    *,
    overwrite: bool = False,
    return_obj: bool = False,
    verbose: bool = False,
) -> Optional[Union[Path, List[Path]]]:
    """
    Enhanced directory creation with parent + child pattern.

    Parameters:
        parent: Parent directory (optional, can be None for absolute paths)
        children: Child directory name(s) or nested path
        exist_ok: Don't raise error if directory exists (default: True)
        overwrite: Overwrite directory if it exists (remove and recreate)
        return_obj: Return Path object(s) (default: False)
        verbose: Print status messages (default: False)

    Returns:
        None, Path, or List[Path] depending on parameters

    Examples:
        # Single directory (parent + child)
        mkdir("/parent", "child", return_obj=True)

        # Single absolute path (parent=None)
        mkdir("/absolute/path", return_obj=True)

        # Multiple children
        mkdir("/parent", ["child1", "child2"], return_obj=True)

        # Nested structure (automatically creates all parents)
        mkdir("/parent", "a/b/c", return_obj=True)

        # Overwrite existing
        mkdir("/parent", "child", overwrite=True)
    """

    def _create_single_dir(p: Path) -> Optional[Path]:
        """Helper to create single directory with proper error handling"""
        try:
            # Handle overwrite/clean mode
            if overwrite and p.exists():
                if p.is_dir():
                    if verbose:
                        print(f"[Removing Folder]: {p}")
                    shutil.rmtree(str(p))
                else:
                    if verbose:
                        print(f"[Removing File]: {p}")
                    p.unlink()
                # After removal, we need to create it
                p.mkdir(parents=True, exist_ok=True)
                if verbose:
                    print(f"[DIR (overwritten)]: {p}")
                return p if return_obj else None

            # Normal creation (no overwrite)
            if not p.exists():
                p.mkdir(parents=True, exist_ok=True)
                if verbose:
                    print(f"Created directory: {p}")
            elif verbose:
                print(f"Directory already exists: {p}")

            return p if return_obj else None

        except PermissionError as e:
            print(f"Permission denied creating {p}: {e}")
            return None
        except FileExistsError as e:
            if not exist_ok:
                print(f"Directory already exists (exist_ok=False): {p}")
                raise
            return None
        except Exception as e:
            print(f"Error creating {p}: {e}")
            return None

    # ===== Parameter parsing =====
    # Support both calling styles:
    # 1. mkdir(path) or mkdir([path1, path2])
    # 2. mkdir(parent, child) or mkdir(parent, [child1, child2])

    actual_parent = None
    actual_children = None

    if children is None:
        # Single parameter call: mkdir(path) or mkdir([paths])
        if isinstance(parent, (list, tuple)):
            actual_children = parent  # List of paths
        else:
            actual_children = parent  # Single path
            actual_parent = None
    else:
        # Two parameter call: mkdir(parent, children)
        actual_parent = parent
        actual_children = children

    # Convert parent to Path if provided
    parent_path = Path(actual_parent) if actual_parent is not None else None

    # Check if we have anything to process
    if actual_children is None:
        return None

    # ===== Process paths =====
    # Convert children to list
    if isinstance(actual_children, (str, Path)):
        children_list = [actual_children]
    else:
        children_list = list(actual_children)

    children_list = list(set(children_list))

    # Build full paths
    full_paths = []
    for child in children_list:
        child_path = Path(child)
        if parent_path:
            full_path = parent_path / child_path
        else:
            full_path = child_path
        full_paths.append(full_path)

    # ===== Create directories =====
    created_paths = []
    for path in full_paths:
        result = _create_single_dir(path)
        if result is not None and return_obj:
            created_paths.append(result)

    # ===== Return appropriate result =====
    if return_obj:
        if not created_paths:
            return None
        elif len(created_paths) == 1:
            return created_paths[0]
        else:
            return created_paths

    return None

def split_path(fpath):
    f_slash = os.sep
    dir_par = f_slash.join(fpath.split(f_slash)[:-1])
    dir_ch = "".join(fpath.split(f_slash)[-1:])
    return dir_par, dir_ch

def figsave(*args, dpi=300, **kwargs):
    """
    Save a Matplotlib figure or image file in various formats.

    This function automatically determines whether to save a Matplotlib figure 
    or an image (PIL or NumPy array) and handles different file formats, including:
    - PDF, EPS, PNG, JPG, TIFF, ICO, EMF

    Parameters:
    -----------
    *args : str, PIL.Image, np.ndarray
        - File path (directory and/or filename) to save the figure.
        - If an image is provided (PIL or NumPy), it will be saved accordingly.
    
    dpi : int, optional (default=300)
        - Resolution (dots per inch) for saved figures.
    
    **kwargs : dict
        - Additional keyword arguments for `matplotlib.pyplot.savefig()`, including:
            - bbox_inches (str, default="tight"): Bounding box for figure.
            - pad_inches (float, default=0): Padding around figure.
            - facecolor (str, default="white"): Background color.
            - edgecolor (str, default="auto"): Edge color.

    Supported Formats:
    ------------------
    - Vector: `pdf`, `eps`, `emf`
    - Raster: `png`, `jpg`, `jpeg`, `tiff`, `tif`, `ico`
    
    Example Usage:
    --------------
    >>> figsave("output_plot.pdf")
    >>> figsave("figs/plot.png", dpi=600)
    >>> figsave("./results/figure", format="pdf")
    >>> figsave("icons/logo.ico", image)  # Save an image file as an icon

    Returns:
    --------
    None
    """
    import matplotlib.pyplot as plt
    from PIL import Image
    
    
    bbox_inches=kwargs.pop("bbox_inches","tight")
    pad_inches=kwargs.pop("pad_inches",0)
    facecolor=kwargs.pop("facecolor",'white')
    edgecolor=kwargs.pop("edgecolor",'auto')
    
    dir_save = None
    fname = None
    img = None
    fig = None
    arg_unknown=[]
    for arg in args:
        if isinstance(arg, str):
            path = Path(arg)
            if path.suffix:  # Has file extension
                fname = path.name
                dir_save = path.parent
            else:
                dir_save = path
        elif isinstance(arg, (Image.Image, np.ndarray)):
            img = arg  # Store PIL image or numpy array
        elif isinstance(arg, plt.Figure):
            fig = arg  # Store the figure if passed
        else:
            arg_unknown.append(arg)
    print(arg_unknown)

    dir_save = Path(dir_save) if dir_save else Path(".")
    dir_save.mkdir(parents=True, exist_ok=True)
    # Handle filename and extension
    if fname is None:
        fname = dir_save
    else:
        fname = dir_save / fname
    if fname.suffix == "":
        fname = fname.with_suffix(".pdf")  # Default format

    ftype = fname.suffix.lstrip(".").lower()
    if fig is None:
        fig = plt.gcf() 
    # Save figure based on file type
    if ftype == "eps":
        fig.savefig(fname, format="eps", bbox_inches=bbox_inches)
        fig.savefig(fname.with_suffix(".pdf"), format="pdf", dpi=dpi,
                    pad_inches=pad_inches, bbox_inches=bbox_inches,
                    facecolor=facecolor, edgecolor=edgecolor) 
    elif ftype.lower() in ["jpg", "jpeg", "png", "tiff", "tif"]:
        if img is not None:  # If a PIL image is provided
            if isinstance(img, Image.Image):
                if img.mode == "RGBA":
                    img = img.convert("RGB")
                img.save(fname, format=ftype.upper(), dpi=(dpi, dpi))
            elif isinstance(img, np.ndarray):
                import cv2
                if img.ndim == 2:
                    # Grayscale image
                    Image.fromarray(img).save(fname, format=ftype.upper(), dpi=(dpi, dpi))
                elif img.ndim == 3:
                    if img.shape[2] == 3:
                        # RGB image
                        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
                        Image.fromarray(img).save(fname, format=ftype.upper(), dpi=(dpi, dpi))
                    elif img.shape[2] == 4:
                        # RGBA image
                        img = cv2.cvtColor(img, cv2.COLOR_BGRA2RGBA)  # Convert BGRA to RGBA
                        Image.fromarray(img).save(fname, format=ftype.upper(), dpi=(dpi, dpi))
                    else:
                        raise ValueError("Unexpected number of channels in the image array.")
                else:
                    raise ValueError("Image array has an unexpected number of dimensions.")
        else:
            fig.savefig(fname,format=ftype.lower(),dpi=dpi,pad_inches=pad_inches,bbox_inches=bbox_inches,facecolor=facecolor,edgecolor=edgecolor)
    elif ftype.lower() in ["emf","pdf","fig"]:
        try:
            fig.savefig(fname,format=ftype.lower(),dpi=dpi,pad_inches=pad_inches,bbox_inches=bbox_inches,facecolor=facecolor,edgecolor=edgecolor)
        except Exception as e:
            fig.savefig(fname,format=ftype.lower(),dpi=dpi,pad_inches=0,bbox_inches=None,facecolor=facecolor,edgecolor=edgecolor)

    elif ftype.lower() == "ico":
        # Ensure the image is in a format that can be saved as an icon (e.g., 32x32, 64x64, etc.)
        if img is None:  # If no image is provided, use the matplotlib figure
            img = plt.figure() 
            print(fname)
            img.savefig(fname, 
                        format="png",
                        dpi=dpi,
                        pad_inches=pad_inches,
                        bbox_inches=bbox_inches,
                        facecolor=facecolor,
                        edgecolor=edgecolor )
            img = Image.open(fname)  # Load the saved figure image

        # Resize the image to typical icon sizes and save it as .ico
        icon_sizes = [(32, 32), (64, 64), (128, 128), (256, 256)]
        img = img.convert("RGBA") 
        img.save(fname, format="ICO", sizes=icon_sizes)
        print(f"Icon saved @: {fname} with sizes: {icon_sizes}")
    print(f"\nSaved @: dpi={dpi}\n{fname}")
 
def is_str_color(s):
    """
    Determine whether a value can be interpreted as a valid Python/Matplotlib color.
    
    Supports:
    - Named colors ('red', 'skyblue')
    - Hex codes (#RGB, #RRGGBB, #RRGGBBAA)
    - Grayscale strings ('0.5')
    - RGB/RGBA tuples (0‚Äì1 or 0‚Äì255 range)
    - CSS rgb()/rgba() strings
    """
    from matplotlib.colors import is_color_like
    # None or NaN are invalid
    if s is None:
        return False

    # Try Matplotlib's built-in validator (handles almost everything)
    try:
        return is_color_like(s)
    except Exception:
        pass

    # Additional fallback for rgb()/rgba() CSS-like strings
    if isinstance(s, str):
        css_rgb_pattern = r"^rgba?\(\s*(\d{1,3}%?\s*,\s*){2,3}(\d{1,3}%?|\d*\.?\d+)\s*\)$"
        return re.match(css_rgb_pattern, s) is not None

    # Tuple or list of numbers (RGB or RGBA)
    if isinstance(s, (tuple, list)) and 3 <= len(s) <= 4:
        try:
            vals = [float(v) for v in s]
            return all(0 <= v <= 1 or 0 <= v <= 255 for v in vals)
        except Exception:
            return False

    return False

def isnum(s):
    return is_num(s)

def is_unique(list_:List,**kwarg):
    """
    to see if the list is unique
    """
    if len(list_)==len(unique(list_,**kwarg)):
        return True
    return False

def adjust_spines(ax=None, spines=["left", "bottom"], distance=2):
    import matplotlib.pyplot as plt

    if ax is None:
        ax = plt.gca()
    for loc, spine in ax.spines.items():
        if loc in spines:
            spine.set_position(("outward", distance))  # outward by 2 points
            # spine.set_smart_bounds(True)
        else:
            spine.set_color("none")  # don't draw spine
    # turn off ticks where there is no spine
    if "left" in spines:
        ax.yaxis.set_ticks_position("left")
    else:
        ax.yaxis.set_ticks([])
    if "bottom" in spines:
        ax.xaxis.set_ticks_position("bottom")
    else:
        # no xaxis ticks
        ax.xaxis.set_ticks([])

def add_colorbar(im, width=None, pad=None, **kwargs):
    # usage: add_colorbar(im, width=0.01, pad=0.005, label="PSD (dB)", shrink=0.8)
    l, b, w, h = im.axes.get_position().bounds  # get boundaries
    width = width or 0.1 * w  # get width of the colorbar
    pad = pad or width  # get pad between im and cbar
    fig = im.axes.figure  # get figure of image
    cax = fig.add_axes([l + w + pad, b, width, h])  # define cbar Axes
    return fig.colorbar(im, cax=cax, **kwargs)  # draw cbar

def list2slice(indices: Union[List[int], np.ndarray]) -> List[slice]:
    """
    Convert a list of indices to a list of slice objects for Excel formatting.

    This function takes a list of indices (which may or may not be consecutive)
    and converts them into slice objects that represent consecutive ranges.

    Args:
        indices: A list or numpy array of integers representing indices.
                Must be non-empty and contain only integers.

    Returns:
        A list of slice objects representing consecutive ranges in the input indices.

    Raises:
        ValueError: If input is empty or contains non-integer values.
        TypeError: If input is not a list or numpy array.

    Examples:
        >>> list_to_slice([1, 2, 3, 5, 6, 8])
        [slice(1, 4, None), slice(5, 7, None), slice(8, 9, None)]

        >>> list_to_slice([])
        ValueError: Input indices cannot be empty
    """
    # Input validation
    if not isinstance(indices, (list, np.ndarray)):
        raise TypeError("Input must be a list or numpy array")

    if len(indices) == 0:
        raise ValueError("Input indices cannot be empty")

    # Convert to numpy array and validate all elements are integers
    try:
        indices_arr = np.array(indices, dtype=int)
    except (ValueError, TypeError):
        raise ValueError("All elements in indices must be integers")

    # Handle single element case
    if len(indices_arr) == 1:
        return [slice(int(indices_arr[0]), int(indices_arr[0]) + 1)]

    # Sort and remove duplicates while preserving order
    unique_sorted = np.unique(indices_arr)

    # Find where the difference between consecutive elements is greater than 1
    break_points = np.where(np.diff(unique_sorted) != 1)[0] + 1

    # Split into consecutive ranges
    ranges = np.split(unique_sorted, break_points)

    # Create slice objects
    slices = []
    for r in ranges:
        start = int(r[0])
        end = int(r[-1]) + 1  # slices are exclusive of the end value
        slices.append(slice(start, end))

    return slices

def list2str(x_str):
    s = "".join(str(x) for x in x_str)
    return s


def str2list(str_):
    l = []
    [l.append(x) for x in str_]
    return l


def str2words(
    content,
    method="combined",
    custom_dict=None,
    sym_spell_params=None,
    use_threading=True,
):
    """
    Ultimate text correction function supporting multiple methods,
    lists or strings, and domain-specific corrections.

    Parameters:
        content (str or list): Input text or list of strings to correct.
        method (str): Correction method ('textblob', 'sym', 'combined').
        custom_dict (dict): Custom dictionary for domain-specific corrections.
        sym_spell_params (dict): Parameters for initializing SymSpell.

    Returns:
        str or list: Corrected text or list of corrected strings.
    """
    from textblob import TextBlob
    from symspellpy import SymSpell, Verbosity
    
    import pkg_resources
    from concurrent.futures import ThreadPoolExecutor

    def initialize_symspell(max_edit_distance=2, prefix_length=7):
        """Initialize SymSpell for advanced spelling correction."""
        sym_spell = SymSpell(max_edit_distance, prefix_length)
        dictionary_path = pkg_resources.resource_filename(
            "symspellpy",
            # "frequency_bigramdictionary_en_243_342.txt",
            "frequency_dictionary_en_82_765.txt",
        )

        sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1)
        return sym_spell

    def segment_words(text, sym_spell):
        """Segment concatenated words into separate words."""
        segmented = sym_spell.word_segmentation(text)
        return segmented.corrected_string

    @lru_cache(maxsize=1000)  # Cache results for repeated corrections
    def advanced_correction(word, sym_spell):
        """Correct a single word using SymSpell."""
        suggestions = sym_spell.lookup(word, Verbosity.CLOSEST, max_edit_distance=2)
        return suggestions[0].term if suggestions else word

    def apply_custom_corrections(word, custom_dict):
        """Apply domain-specific corrections using a custom dictionary."""
        return custom_dict.get(word.lower(), word)

    def preserve_case(original_word, corrected_word):
        """
        Preserve the case of the original word in the corrected word.
        """
        if original_word.isupper():
            return corrected_word.upper()
        elif original_word[0].isupper():
            return corrected_word.capitalize()
        else:
            return corrected_word.lower()

    def process_string(text, method, sym_spell=None, custom_dict=None):
        """
        Process a single string for spelling corrections.
        Handles TextBlob, SymSpell, and custom corrections.
        """
        if method in ("sym", "combined") and sym_spell:
            text = segment_words(text, sym_spell)

        if method in ("textblob", "combined"):
            text = str(TextBlob(text).correct())

        corrected_words = []
        for word in text.split():
            original_word = word
            if method in ("sym", "combined") and sym_spell:
                word = advanced_correction(word, sym_spell)

            # Step 3: Apply custom corrections
            if custom_dict:
                word = apply_custom_corrections(word, custom_dict)
            # Preserve original case
            word = preserve_case(original_word, word)
            corrected_words.append(word)

        return " ".join(corrected_words)

    # Initialize SymSpell if needed
    sym_spell = None
    if method in ("sym", "combined"):
        if not sym_spell_params:
            sym_spell_params = {"max_edit_distance": 2, "prefix_length": 7}
        sym_spell = initialize_symspell(**sym_spell_params)

    # Process lists or strings
    if isinstance(content, list):
        if use_threading:
            with ThreadPoolExecutor() as executor:
                corrected_content = list(
                    executor.map(
                        lambda x: process_string(x, method, sym_spell, custom_dict),
                        content,
                    )
                )
            return corrected_content
        else:
            return [
                process_string(item, method, sym_spell, custom_dict) for item in content
            ]
    else:
        return process_string(content, method, sym_spell, custom_dict)

def str2raw(s: Union[str, re.Pattern], verbose: bool = False) -> str:
    """
    Convert a string to its raw/escaped representation.
    If given a compiled regex.Pattern, return its pattern string instead.
    """
    if hasattr(s, 'pattern'):  # Works for both re.Pattern and regex.Pattern
        s_raw = s.pattern
    elif isinstance(s, str):
        s_raw = s.encode("unicode_escape").decode("ascii")
    else:
        raise TypeError(f"Unsupported type for str2raw: {type(s)}. Expected str or regex pattern")
    if verbose:
        print(f"=> original:\n{s}\n=> raw string (escaped):\n{s_raw}")
    return s_raw

def str2pattern(s: str, verbose: bool = False, ignore_case: bool = False,
                multiline: bool = False, dotall: bool = False) -> re.Pattern:
    """
    Convert a string or alias into a compiled regex Pattern.

    - If s matches an alias in COMMON_PATTERNS, use that regex.
    - Otherwise, treat s as a regex pattern string.
    - Compile the regex with optional flags.
    - Provide verbose debug info.

    Usage:
    str2pattern('digits' ).findall("Station 565/325\nJako, 23 year-old. 65.8 kg, b-Wei√ü ie (W, 36)") 
    """
    patterns = load_common_patterns(verbose=verbose)

    # Normalize alias matching (ignore underscores, dashes, lowercase)
    def normalize_key(key):
        return key.lower().replace("_", "").replace("-", "")

    s_norm = normalize_key(s)

    # Try to find matching alias in COMMON_PATTERNS keys (fuzzy matching simplified)
    matched_key = None
    for key in patterns:
        if normalize_key(key) == s_norm:
            matched_key = key
            break

    if matched_key:
        pattern_str = patterns[matched_key]
        if verbose:
            print(f"Alias '{s}' matched pattern key '{matched_key}': {pattern_str}")
    else:
        pattern_str = s
        if verbose:
            print(f"No alias matched for '{s}', using raw pattern: {str2raw(pattern_str, verbose=verbose)}")
    if HAS_REGEX:
        if verbose:
            print("[str2pattern] Using advanced regex engine")
        flags = re.VERSION1  # Start with regex's extended mode
        
        if ignore_case:
            flags |= re.IGNORECASE
        if multiline:
            flags |= re.MULTILINE
        if dotall:
            flags |= re.DOTALL
    else:
        if verbose:
            print("[str2pattern] Using standard re engine")
        flags = 0
        if ignore_case:
            flags |= re.IGNORECASE
        if multiline:
            flags |= re.MULTILINE
        if dotall:
            flags |= re.DOTALL

    try:
        compiled = re.compile(pattern_str, flags=flags)
        if verbose:
            print(f"Compiled pattern: {compiled.pattern}")
        return compiled
    except re.error as e:
        if verbose:
            print(f"Regex compilation error for pattern '{pattern_str}': {e}")
        raise


# !convert strings to qrcode
def str2qrcode(
    data: str,
    dir_save: str = "qrcode.png",
    size: int = 10,#Good balance between size and quality
    border: int = 4,#Minimum required by QR code standard
    error_correction: str = "M",
    fill_color: str = "black",
    back_color: str = "white",
    logo_path: str = None,
    logo_size_ratio: float = 0.2,
    transparent: bool = False,
):
    """
        Generate a QR Code with advanced options.

        Args:
            data (str): The data/text/url to encode in the QR code.
            dir_save (str): Output file path. Format is inferred from extension.
            size (int): Controls the size of each QR box (default: 10).
            border (int): Width (in boxes) of the QR border (default: 4).
            error_correction (str): 'L', 'M', 'Q', 'H'. Higher allows bigger logos.
            fill_color (str): QR code color.
            back_color (str): Background color.
            logo_path (str): Optional path to a logo image.
            logo_size_ratio (float): Logo size as fraction of QR code size.
            transparent (bool): Make background transparent (if back_color is white).
            return_image (bool): If True, return the PIL image instead of saving.

    str2qrcode(
        data="openai_qrs.jep",
        dir_save="openai_qrs.png",
        size=50,
        border=4,
        error_correction="H",
        fill_color="white",
        back_color="#263C4D",
        logo_path="/Users/macjianfeng/Dropbox/Downloads/iShot_2025-07-14_22.00.51.jpg",
        # logo_size_ratio=0.6,
        transparent=0,
    )
    """
    from PIL import Image, ImageDraw
    import qrcode

    # Map error correction level
    ec_levels = {
        "L": qrcode.constants.ERROR_CORRECT_L,#~7%
        "M": qrcode.constants.ERROR_CORRECT_M,#~15%
        "Q": qrcode.constants.ERROR_CORRECT_Q,#~25%
        "H": qrcode.constants.ERROR_CORRECT_H,#~30%
    }
    ec_level = ec_levels.get(error_correction.upper(), qrcode.constants.ERROR_CORRECT_M)

    # Build QR code
    qr = qrcode.QRCode(
        version=None,  # Auto fit
        error_correction=ec_level,
        box_size=size,
        border=border,
    )
    qr.add_data(data)
    qr.make(fit=True)

    img = qr.make_image(
        fill_color=fill_color, back_color=None if transparent else back_color
    ).convert("RGBA")

    if transparent and back_color == "white":
        datas = img.getdata()
        newData = []
        for item in datas:
            if item[:3] == (255, 255, 255):  # white
                newData.append((255, 255, 255, 0))  # transparent
            else:
                newData.append(item)
        img.putdata(newData)

    # Add logo
    if logo_path and os.path.exists(logo_path):
        logo = Image.open(logo_path).convert("RGBA")
        # Resize logo
        qr_width, qr_height = img.size
        logo_size_ratio=min(logo_size_ratio,0.25) # Keep logo_size_ratio ‚â§ 0.25 for wide compatibility.
        logo_size = int(min(qr_width, qr_height) * logo_size_ratio)
        logo = logo.resize((logo_size, logo_size), Image.LANCZOS)

        # Paste logo at the center
        pos = ((qr_width - logo_size) // 2, (qr_height - logo_size) // 2)
        img.paste(logo, pos, mask=logo)

    # Save or return
    if isinstance(dir_save,str):
        figsave(dir_save, img)
        return 
    else:
        return img

def load_img(fpath):
    """
    Load an image from the specified file path.
    Args:
        fpath (str): The file path to the image.
    Returns:
        PIL.Image: The loaded image.
    Raises:
        FileNotFoundError: If the specified file is not found.
        OSError: If the specified file cannot be opened or is not a valid image file.
    """
    from PIL import Image

    try:
        img = Image.open(fpath)
        return img
    except FileNotFoundError:
        raise FileNotFoundError(f"The file '{fpath}' was not found.")
    except OSError:
        raise OSError(f"Unable to open file '{fpath}' or it is not a valid image file.")


def apply_filter(img, *args, verbose=True):
    # def apply_filter(img, filter_name, filter_value=None):
    """
    Apply the specified filter to the image.
    Args:
        img (PIL.Image): The input image.
        filter_name (str): The name of the filter to apply.
        **kwargs: Additional parameters specific to the filter.
    Returns:
        PIL.Image: The filtered image.
    """
    from PIL import ImageFilter

    def correct_filter_name(filter_name):
        if all(
            [
                "b" in filter_name.lower(),
                "ur" in filter_name.lower(),
                "box" not in filter_name.lower(),
            ]
        ):
            return "BLUR"
        elif "cont" in filter_name.lower():
            return "Contour"
        elif "det" in filter_name.lower():
            return "Detail"
        elif (
            "edg" in filter_name.lower()
            and "mo" not in filter_name.lower()
            and "f" not in filter_name.lower()
        ):
            return "EDGE_ENHANCE"
        elif "edg" in filter_name.lower() and "mo" in filter_name.lower():
            return "EDGE_ENHANCE_MORE"
        elif "emb" in filter_name.lower():
            return "EMBOSS"
        elif "edg" in filter_name.lower() and "f" in filter_name.lower():
            return "FIND_EDGES"
        elif "sh" in filter_name.lower() and "mo" not in filter_name.lower():
            return "SHARPEN"
        elif "sm" in filter_name.lower() and "mo" not in filter_name.lower():
            return "SMOOTH"
        elif "sm" in filter_name.lower() and "mo" in filter_name.lower():
            return "SMOOTH_MORE"
        elif "min" in filter_name.lower():
            return "MIN_FILTER"
        elif "max" in filter_name.lower():
            return "MAX_FILTER"
        elif "mod" in filter_name.lower():
            return "MODE_FILTER"
        elif "mul" in filter_name.lower():
            return "MULTIBAND_FILTER"
        elif "gau" in filter_name.lower():
            return "GAUSSIAN_BLUR"
        elif "box" in filter_name.lower():
            return "BOX_BLUR"
        elif "med" in filter_name.lower():
            return "MEDIAN_FILTER"
        else:
            supported_filters = [
                "BLUR",
                "CONTOUR",
                "DETAIL",
                "EDGE_ENHANCE",
                "EDGE_ENHANCE_MORE",
                "EMBOSS",
                "FIND_EDGES",
                "SHARPEN",
                "SMOOTH",
                "SMOOTH_MORE",
                "MIN_FILTER",
                "MAX_FILTER",
                "MODE_FILTER",
                "MULTIBAND_FILTER",
                "GAUSSIAN_BLUR",
                "BOX_BLUR",
                "MEDIAN_FILTER",
            ]
            raise ValueError(
                f"Unsupported filter: {filter_name}, should be one of: {supported_filters}"
            )

    for arg in args:
        if isinstance(arg, str):
            filter_name = correct_filter_name(arg)
        else:
            filter_value = arg
    if verbose:
        print(f"processing {filter_name}")
    filter_name = filter_name.upper()  # Ensure filter name is uppercase

    # Supported filters
    supported_filters = {
        "BLUR": ImageFilter.BLUR,
        "CONTOUR": ImageFilter.CONTOUR,
        "DETAIL": ImageFilter.DETAIL,
        "EDGE_ENHANCE": ImageFilter.EDGE_ENHANCE,
        "EDGE_ENHANCE_MORE": ImageFilter.EDGE_ENHANCE_MORE,
        "EMBOSS": ImageFilter.EMBOSS,
        "FIND_EDGES": ImageFilter.FIND_EDGES,
        "SHARPEN": ImageFilter.SHARPEN,
        "SMOOTH": ImageFilter.SMOOTH,
        "SMOOTH_MORE": ImageFilter.SMOOTH_MORE,
        "MIN_FILTER": ImageFilter.MinFilter,
        "MAX_FILTER": ImageFilter.MaxFilter,
        "MODE_FILTER": ImageFilter.ModeFilter,
        "MULTIBAND_FILTER": ImageFilter.MultibandFilter,
        "GAUSSIAN_BLUR": ImageFilter.GaussianBlur,
        "BOX_BLUR": ImageFilter.BoxBlur,
        "MEDIAN_FILTER": ImageFilter.MedianFilter,
    }
    # Check if the filter name is supported
    if filter_name not in supported_filters:
        raise ValueError(
            f"Unsupported filter: {filter_name}, should be one of: {[i.lower() for i in supported_filters.keys()]}"
        )

    # Apply the filter
    if filter_name.upper() in [
        "BOX_BLUR",
        "GAUSSIAN_BLUR",
        "MEDIAN_FILTER",
        "MIN_FILTER",
        "MAX_FILTER",
        "MODE_FILTER",
    ]:
        radius = filter_value if filter_value is not None else 2
        return img.filter(supported_filters[filter_name](radius))
    elif filter_name in ["MULTIBAND_FILTER"]:
        bands = filter_value if filter_value is not None else None
        return img.filter(supported_filters[filter_name](bands))
    else:
        if filter_value is not None and verbose:
            print(
                f"{filter_name} doesn't require a value for {filter_value}, but it remains unaffected"
            )
        return img.filter(supported_filters[filter_name])


def detect_angle(image, by="median", template=None):
    """Detect the angle of rotation using various methods."""
    from sklearn.decomposition import PCA
    from skimage import transform, feature, filters, measure
    from skimage.color import rgb2gray
    from scipy.fftpack import fftshift, fft2
    
    import cv2

    # Convert to grayscale
    if np.array(image).shape[-1] > 3:
        image = np.array(image)[:, :, :3]
    gray_image = rgb2gray(image)

    # Detect edges using Canny edge detector
    edges = feature.canny(gray_image, sigma=2)

    # Use Hough transform to detect lines
    lines = transform.probabilistic_hough_line(edges)
    if isinstance(by, bool):
        by="mean" if by else 0
    if not lines and any(["me" in by, "pca" in by]):
        print("No lines detected. Adjust the edge detection parameters.")
        return 0
    methods = [
        "mean",
        "median",
        "pca",
        "gradient orientation",
        "template matching",
        "moments",
        "fft",
    ]
    by = strcmp(by, methods)[0]
    # Hough Transform-based angle detection (Median/Mean)
    if "me" in by.lower():
        angles = []
        for line in lines:
            (x0, y0), (x1, y1) = line
            angle = np.arctan2(y1 - y0, x1 - x0) * 180 / np.pi
            if 80 < abs(angle) < 100:
                angles.append(angle)
        if not angles:
            return 0
        if "di" in by:
            median_angle = np.median(angles)
            rotation_angle = (
                90 - median_angle if median_angle > 0 else -90 - median_angle
            )

            return rotation_angle
        else:
            mean_angle = np.mean(angles)
            rotation_angle = 90 - mean_angle if mean_angle > 0 else -90 - mean_angle

            return rotation_angle

    # PCA-based angle detection
    elif "pca" in by.lower():
        y, x = np.nonzero(edges)
        if len(x) == 0:
            return 0
        pca = PCA(n_components=2)
        pca.fit(np.vstack((x, y)).T)
        angle = np.arctan2(pca.components_[0, 1], pca.components_[0, 0]) * 180 / np.pi
        return angle

    # Gradient Orientation-based angle detection
    elif "gra" in by.lower():
        gx, gy = np.gradient(gray_image)
        angles = np.arctan2(gy, gx) * 180 / np.pi
        hist, bin_edges = np.histogram(angles, bins=360, range=(-180, 180))
        return bin_edges[np.argmax(hist)]

    # Template Matching-based angle detection
    elif "temp" in by.lower():
        if template is None:
            # Automatically extract a template from the center of the image
            height, width = gray_image.shape
            center_x, center_y = width // 2, height // 2
            size = (
                min(height, width) // 4
            )  # Size of the template as a fraction of image size
            template = gray_image[
                center_y - size : center_y + size, center_x - size : center_x + size
            ]
        best_angle = None
        best_corr = -1
        for angle in range(0, 180, 1):  # Checking every degree
            rotated_template = transform.rotate(template, angle)
            res = cv2.matchTemplate(gray_image, rotated_template, cv2.TM_CCOEFF)
            _, max_val, _, _ = cv2.minMaxLoc(res)
            if max_val > best_corr:
                best_corr = max_val
                best_angle = angle
        return best_angle

    # Image Moments-based angle detection
    elif "mo" in by.lower():
        moments = measure.moments_central(gray_image)
        angle = (
            0.5
            * np.arctan2(2 * moments[1, 1], moments[0, 2] - moments[2, 0])
            * 180
            / np.pi
        )
        return angle

    # Fourier Transform-based angle detection
    elif "fft" in by.lower():
        f = fft2(gray_image)
        fshift = fftshift(f)
        magnitude_spectrum = np.log(np.abs(fshift) + 1)
        rows, cols = magnitude_spectrum.shape
        r, c = np.unravel_index(np.argmax(magnitude_spectrum), (rows, cols))
        angle = np.arctan2(r - rows // 2, c - cols // 2) * 180 / np.pi
        return angle

    else:
        print(f"Unknown method {by}: supported methods: {methods}")
        return 0


def imgsets(
    img,
    auto: bool = True,
    size=None,
    figsize=None,
    dpi: int = 200,
    show_axis: bool = False,
    plot_: bool = True,
    verbose: bool = False,
    model: str = "isnet-general-use",
    **kwargs,
):
    """
    Apply various enhancements and filters to an image using PIL's ImageEnhance and ImageFilter modules.

    Args:
        img (PIL.Image): The input image.
        sets (dict): A dictionary specifying the enhancements, filters, and their parameters.
        show (bool): Whether to display the enhanced image.
        show_axis (bool): Whether to display axes on the image plot.
        size (tuple): The size of the thumbnail, cover, contain, or fit operation.
        dpi (int): Dots per inch for the displayed image.
        figsize (tuple): The size of the figure for displaying the image.
        auto (bool): Whether to automatically enhance the image based on its characteristics.

    Returns:
        PIL.Image: The enhanced image.

    Supported enhancements and filters:
        - "sharpness": Adjusts the sharpness of the image. Values > 1 increase sharpness, while values < 1 decrease sharpness.
        - "contrast": Adjusts the contrast of the image. Values > 1 increase contrast, while values < 1 decrease contrast.
        - "brightness": Adjusts the brightness of the image. Values > 1 increase brightness, while values < 1 decrease brightness.
        - "color": Adjusts the color saturation of the image. Values > 1 increase saturation, while values < 1 decrease saturation.
        - "rotate": Rotates the image by the specified angle.
        - "crop" or "cut": Crops the image. The value should be a tuple specifying the crop box as (left, upper, right, lower).
        - "size": Resizes the image to the specified dimensions.
        - "thumbnail": Resizes the image to fit within the given size while preserving aspect ratio.
        - "cover": Resizes and crops the image to fill the specified size.
        - "contain": Resizes the image to fit within the specified size, adding borders if necessary.
        - "fit": Resizes and pads the image to fit within the specified size.
        - "filter": Applies various filters to the image (e.g., BLUR, CONTOUR, EDGE_ENHANCE).

    Note:
        The "color" and "enhance" enhancements are not implemented in this function.
    Usage: 
    imgsets(dir_img, auto=1, color=1.5, plot_=0)
    imgsets(dir_img, color=2)
    imgsets(dir_img, pad=(300, 300), bgcolor=(73, 162, 127), plot_=0)
    imgsets(dir_img, contrast=0, color=1.2, plot_=0)
    imgsets(get_clip(), flip="tb")# flip top and bottom
    imgsets(get_clip(), contrast=1, rm=[100, 5, 2]) #'foreground_threshold', 'background_threshold' and 'erode_structure_size'
    imgsets(dir_img, rm="birefnet-portrait") # with using custom model
    """

    import matplotlib.pyplot as plt
    from PIL import ImageEnhance, ImageOps, Image

    supported_filters = [
        "BLUR",
        "CONTOUR",
        "DETAIL",
        "EDGE_ENHANCE",
        "EDGE_ENHANCE_MORE",
        "EMBOSS",
        "FIND_EDGES",
        "SHARPEN",
        "SMOOTH",
        "SMOOTH_MORE",
        "MIN_FILTER",
        "MAX_FILTER",
        "MODE_FILTER",
        "MULTIBAND_FILTER",
        "GAUSSIAN_BLUR",
        "BOX_BLUR",
        "MEDIAN_FILTER",
    ]
    # *Rembg is a tool to remove images background.
    # https://github.com/danielgatis/rembg
    rem_models = {
        "u2net": "general use cases.",
        "u2netp": "A lightweight version of u2net model.",
        "u2net_human_seg": "human segmentation.",
        "u2net_cloth_seg": "Cloths Parsing from human portrait. Here clothes are parsed into 3 category: Upper body, Lower body and Full body.",
        "silueta": "Same as u2net but the size is reduced to 43Mb.",
        "isnet-general-use": "A new pre-trained model for general use cases.",
        "isnet-anime": "A high-accuracy segmentation for anime character.",
        "sam": "any use cases.",
        "birefnet-general": "general use cases.",
        "birefnet-general-lite": "A light pre-trained model for general use cases.",
        "birefnet-portrait": "human portraits.",
        "birefnet-dis": "dichotomous image segmentation (DIS).",
        "birefnet-hrsod": "high-resolution salient object detection (HRSOD).",
        "birefnet-cod": "concealed object detection (COD).",
        "birefnet-massive": "A pre-trained model with massive dataset.",
    }
    models_support_rem = list(rem_models.keys())
    str_usage = """
    imgsets(dir_img, auto=1, color=1.5, plot_=0)
    imgsets(dir_img, color=2)
    imgsets(dir_img, pad=(300, 300), bgcolor=(73, 162, 127), plot_=0)
    imgsets(dir_img, contrast=0, color=1.2, plot_=0)
    imgsets(get_clip(), flip="tb")# flip top and bottom
    imgsets(get_clip(), contrast=1, rm=[100, 5, 2]) #'foreground_threshold', 'background_threshold' and 'erode_structure_size'
    imgsets(dir_img, rm="birefnet-portrait") # with using custom model
    """
    if run_once_within():
        print(str_usage)

    def gamma_correction(image, gamma=1.0, v_max=255):
        # adjust gama value
        inv_gamma = 1.0 / gamma
        lut = [
            int((i / float(v_max)) ** inv_gamma * int(v_max)) for i in range(int(v_max))
        ]
        return lut  # image.point(lut)

    def auto_enhance(img):
        """
        Automatically enhances the image based on its characteristics, including brightness,
        contrast, color range, sharpness, and gamma correction.

        Args:
            img (PIL.Image): The input image.

        Returns:
            dict: A dictionary containing the optimal enhancement values applied.
            PIL.Image: The enhanced image.
        """
        from PIL import Image, ImageEnhance, ImageOps, ImageFilter
        

        # Determine the bit depth based on the image mode
        try:
            if img.mode in ["1", "L", "P", "RGB", "YCbCr", "LAB", "HSV"]:
                bit_depth = 8
            elif img.mode in ["RGBA", "CMYK"]:
                bit_depth = 8
            elif img.mode in ["I", "F"]:
                bit_depth = 16
            else:
                raise ValueError("Unsupported image mode")
        except:
            bit_depth = 8

        # Initialize enhancement factors
        enhancements = {
            "brightness": 1.0,
            "contrast": 0,  # autocontrasted
            "color": 1.35,
            "sharpness": 1.0,
            "gamma": 1.0,
        }

        # Calculate brightness and contrast for each channel
        num_channels = len(img.getbands())
        brightness_factors = []
        contrast_factors = []
        for channel in range(num_channels):
            channel_histogram = img.split()[channel].histogram()
            total_pixels = sum(channel_histogram)
            brightness = (
                sum(i * w for i, w in enumerate(channel_histogram)) / total_pixels
            )
            channel_min, channel_max = img.split()[channel].getextrema()
            contrast = channel_max - channel_min
            # Adjust calculations based on bit depth
            normalization_factor = 2**bit_depth - 1
            brightness_factor = (
                1.0 + (brightness - normalization_factor / 2) / normalization_factor
            )
            contrast_factor = (
                1.0 + (contrast - normalization_factor / 2) / normalization_factor
            )
            brightness_factors.append(brightness_factor)
            contrast_factors.append(contrast_factor)

        # Calculate average brightness and contrast factors across channels
        enhancements["brightness"] = sum(brightness_factors) / num_channels
        # Adjust brightness and contrast
        img = ImageEnhance.Brightness(img).enhance(enhancements["brightness"])
        sharpness_enhancer = ImageEnhance.Sharpness(img)
        # Use edge detection to estimate sharpness need
        edges = img.filter(ImageFilter.FIND_EDGES).convert("L")
        avg_edge_intensity = np.mean(np.array(edges))
        enhancements["sharpness"] = min(
            2.0, max(0.5, 1.0 + avg_edge_intensity / normalization_factor)
        )
        return enhancements

    # Load image if input is a file path
    if isinstance(img, str):
        img = load_img(img)
    img_update = img.copy()

    if auto:
        kwargs = {**auto_enhance(img_update), **kwargs}
    params = [
        "sharp",
        "color",
        "contrast",
        "bright",
        "crop",
        "rotate",
        "size",
        "resize",
        "thumbnail",
        "cover",
        "contain",
        "filter",
        "fit",
        "pad",
        "rem",
        "rm",
        "back",
        "bg_color",
        "cut",
        "gamma",
        "flip",
        "booster",
    ]
    for k, value in kwargs.items():
        k = strcmp(k, params)[0]  # correct the param name
        if "shar" in k.lower():
            enhancer = ImageEnhance.Sharpness(img_update)
            img_update = enhancer.enhance(value)
        elif all(
            ["col" in k.lower(), "bg" not in k.lower(), "background" not in k.lower()]
        ):
            # *color
            enhancer = ImageEnhance.Color(img_update)
            img_update = enhancer.enhance(value)
        elif "contr" in k.lower():
            if value and isinstance(value, (float, int)):
                enhancer = ImageEnhance.Contrast(img_update)
                img_update = enhancer.enhance(value)
            else:
                try:
                    img_update = ImageOps.autocontrast(img_update)
                    print("autocontrasted")
                except Exception as e:
                    print(f"Failed 'auto-contrasted':{e}")
        elif "bri" in k.lower():
            enhancer = ImageEnhance.Brightness(img_update)
            img_update = enhancer.enhance(value)
        elif "cro" in k.lower() or "cut" in k.lower():
            img_update = img_update.crop(value)
        elif "rota" in k.lower():
            if isinstance(value, (str,bool)):
                value = detect_angle(img_update, by=value)
                print(f"rotated by {value}¬∞")
            img_update = img_update.rotate(value)
        elif "flip" in k.lower():
            if "l" in value and "r" in value:
                # left/right
                img_update = img_update.transpose(Image.FLIP_LEFT_RIGHT)
            elif any(["u" in value and "d" in value, "t" in value and "b" in value]):
                # up/down or top/bottom
                img_update = img_update.transpose(Image.FLIP_TOP_BOTTOM)
        elif "si" in k.lower():
            if isinstance(value, tuple):
                value = list(value)
            value = [int(i) for i in value]
            img_update = img_update.resize(value)
        elif "thum" in k.lower():
            img_update.thumbnail(value)
        elif "cover" in k.lower():
            img_update = ImageOps.cover(img_update, size=value)
        elif "contain" in k.lower():
            img_update = ImageOps.contain(img_update, size=value)
        elif "fi" in k.lower() and "t" in k.lower():  # filter
            if isinstance(value, dict):
                if verbose:
                    print(f"supported filter: {supported_filters}")
                for filter_name, filter_value in value.items():
                    img_update = apply_filter(
                        img_update, filter_name, filter_value, verbose=verbose
                    )
            else:
                img_update = ImageOps.fit(img_update, size=value)
        elif "pad" in k.lower():
            # *ImageOps.pad ensures that the resized image has the exact size specified by the size parameter while maintaining the aspect ratio.
            # size: A tuple specifying the target size (width, height).
            img_update = ImageOps.pad(img_update, size=value)
        elif "rem" in k.lower() or "rm" in k.lower() or "back" in k.lower():
            from rembg import remove, new_session

            if verbose:
                preview(rem_models)

            print(f"supported modles: {models_support_rem}")
            model = strcmp(model, models_support_rem)[0]
            session = new_session(model)
            if isinstance(value, bool):
                print(f"using model:{model}")
                img_update = remove(img_update, session=session)
            elif value and isinstance(value, (int, float, list)):
                if verbose:
                    print("https://github.com/danielgatis/rembg/blob/main/USAGE.md")
                    print(
                        f"rm=True # using default setting;\nrm=(240,10,10)\n'foreground_threshold'(240) and 'background_threshold' (10) values used to determine foreground and background pixels. \nThe 'erode_structure_size'(10) parameter specifies the size of the erosion structure to be applied to the mask."
                    )
                if isinstance(value, int):
                    value = [value]
                if len(value) < 2:
                    img_update = remove(
                        img_update,
                        alpha_matting=True,
                        alpha_matting_background_threshold=value,
                        session=session,
                    )
                elif 2 <= len(value) < 3:
                    img_update = remove(
                        img_update,
                        alpha_matting=True,
                        alpha_matting_background_threshold=value[0],
                        alpha_matting_foreground_threshold=value[1],
                        session=session,
                    )
                elif 3 <= len(value) < 4:
                    img_update = remove(
                        img_update,
                        alpha_matting=True,
                        alpha_matting_background_threshold=value[0],
                        alpha_matting_foreground_threshold=value[1],
                        alpha_matting_erode_size=value[2],
                        session=session,
                    )
            elif isinstance(value, tuple):  # replace the background color
                if len(value) == 3:
                    value += (255,)
                img_update = remove(img_update, bgcolor=value, session=session)
            elif isinstance(value, str):
                # use custom model
                print(f"using model:{strcmp(value, models_support_rem)[0]}")
                img_update = remove(
                    img_update,
                    session=new_session(strcmp(value, models_support_rem)[0]),
                )
        elif "bg" in k.lower() and "color" in k.lower():
            from rembg import remove

            if isinstance(value, list):
                value = tuple(value)
            if isinstance(value, tuple):  # replace the background color
                if len(value) == 3:
                    value += (255,)
                img_update = remove(img_update, bgcolor=value)
        elif "boost" in k.lower():
            import torch
            from realesrgan import RealESRGANer

            if verbose:
                print("Applying Real-ESRGAN for image reconstruction...")
            if isinstance(value, bool):
                scale = 4
            elif isinstance(value, (float, int)):
                scale = value
            else:
                scale = 4
            device = "cuda" if torch.cuda.is_available() else "cpu"
            dir_curr_script = os.path.dirname(os.path.abspath(__file__))
            model_path = dir_curr_script + "/data/RealESRGAN_x4plus.pth"
            model_RealESRGAN = RealESRGANer(
                device=device,
                scale=scale,
                model_path=model_path,
                model="RealESRGAN_x4plus",
            )
            # https://github.com/xinntao/Real-ESRGAN?tab=readme-ov-file#python-script
            img_update = model_RealESRGAN.enhance(np.array(img_update))[0]
    # Display the image if requested
    if plot_:
        if figsize is None:
            plt.figure(dpi=dpi)
        else:
            plt.figure(figsize=figsize, dpi=dpi)
        plt.imshow(img_update)
        if show_axis:
            plt.axis("on")  # Turn on axis
            plt.minorticks_on()
            plt.grid(
                which="both", linestyle="--", linewidth=0.5, color="gray", alpha=0.7
            )

        else:
            plt.axis("off")  # Turn off axis
    return img_update


def thumbnail(dir_img_list, figsize=(10, 10), dpi=100, dir_save=None, kind=".png"):
    """
    Display a thumbnail figure of all images in the specified directory.
    Args:
        dir_img_list (list): List of the Directory containing the images.
    """
    import matplotlib.pyplot as plt
    from PIL import Image

    num_images = len(dir_img_list)
    if not kind.startswith("."):
        kind = "." + kind

    if num_images == 0:
        print("No images found to display.")
        return
    grid_size = int(num_images**0.5) + 1  # Determine grid size
    fig, axs = plt.subplots(grid_size, grid_size, figsize=figsize, dpi=dpi)
    for ax, image_file in zip(axs.flatten(), dir_img_list):
        try:
            img = Image.open(image_file)
            ax.imshow(img)
            ax.axis("off")
        except:
            continue

    [ax.axis("off") for ax in axs.flatten()]
    plt.tight_layout()
    if dir_save is None:
        plt.show()
    else:
        if os.path.basename(dir_save):
            fname = os.path.basename(dir_save) + kind
        else:
            fname = "_thumbnail_" + os.path.basename(os.path.dirname(dir_save)[:-1]) + ".png"
        if os.path.dirname(dir_img_list[0]) == os.path.dirname(dir_save):
            figsave(os.path.dirname(dir_save[:-1]), fname)
        else:
            figsave(os.path.dirname(dir_save), fname)

def dir_lib(lib_oi):
    """
    # example usage:
    # dir_lib("seaborn")
    """
    import site

    # Get the site-packages directory
    f = ls(site.getsitepackages()[0], "folder")

    # Find Seaborn directory within site-packages
    dir_list = []
    for directory in f.fpath:
        if lib_oi in directory.lower():
            dir_list.append(directory)

    if dir_list != []:
        print(f"{lib_oi} directory:", dir_list)
    else:
        print(f"Cannot find the {lib_oi} in site-packages directory.")
    return dir_list


class FileInfo:
    def __init__(self, size, creation_time, ctime, mod_time, mtime, parent_dir, fname, kind, owner, extra_info=None):
        self.size = size
        self.creation_time = creation_time
        self.ctime = ctime
        self.mod_time = mod_time
        self.mtime = mtime
        self.atime = atime
        self.parent_dir = parent_dir
        self.fname = fname
        self.kind = kind
        self.owner = owner
        if extra_info:
            for key, value in extra_info.items():
                setattr(self, key, value)
        print("To show the result: 'finfo(fpath).show()'")

    def __repr__(self):
        return (
            f"FileInfo(size={self.size} MB,  "
            f"ctime='{self.ctime}',  mtime='{self.mtime}', "
            f"parent_dir='{self.parent_dir}', fname='{self.fname}', kind='{self.kind}', owner='{self.owner}')"
        )

    def __str__(self):
        return (
            f"FileInfo:\n"
            f"  Size: {self.size} MB\n" 
            f"  CTime: {self.ctime}\n" 
            f"  MTime: {self.mtime}\n"
            f"  ATime: {self.atime}\n"
            f"  Parent Directory: {self.parent_dir}\n"
            f"  File Name: {self.fname}\n"
            f"  Kind: {self.kind}\n"
            f"  Owner: {self.owner}"
        )

    def show(self):
        return {
            "size": self.size, 
            "ctime": self.ctime, 
            "mtime": self.mtime,
            "parent_dir": self.parent_dir,
            "fname": self.fname,
            "kind": self.kind,
            "owner": self.owner,
            **{
                key: getattr(self, key)
                for key in vars(self)
                if key not in ["size",  "ctime", "mtime", "parent_dir", "fname", "kind", "owner"]
            },
        }



@lru_cache(maxsize=256)
def fowner(file_path):
    """Retrieve file owner in a robust and cross-platform way."""
    try:
        if platform.system() == "Windows":
            import win32security
            sd = win32security.GetFileSecurity(file_path, win32security.OWNER_SECURITY_INFORMATION)
            owner_sid = sd.GetSecurityDescriptorOwner()
            name, domain, _ = win32security.LookupAccountSid(None, owner_sid)
            return f"{domain}\\{name}"
        else:
            import pwd
            stat_info = os.stat(file_path)
            return pwd.getpwuid(stat_info.st_uid).pw_name
    except Exception as e:
        logging.warning(f"Failed to get owner of {file_path}: {e}")
        return None

def finfo(fpath, output='json', verbose=False):
    """Retrieve detailed metadata about a file."""

    def _format_timestamp(timestamp,fmt='%Y-%m-%d %H:%M:%S'):
        """Convert timestamp to a human-readable format."""
        import datetime
        return datetime.datetime.fromtimestamp(timestamp).strftime(fmt)
    
    if not os.path.exists(fpath):
        raise FileNotFoundError(f"File '{fpath}' not found.")
    data = {
        "size": round(os.path.getsize(fpath) / 1024 / 1024, 3), 
        "ctime": _format_timestamp(os.path.getctime(fpath)), 
        "mtime": _format_timestamp(os.path.getmtime(fpath)),
        "atime": _format_timestamp(os.path.getatime(fpath)),
        "parent_dir": os.path.dirname(fpath) + os.sep,
        "fname": os.path.basename(fpath),
        "kind": os.path.splitext(fpath)[1].lower(),
        "owner": fowner(fpath),
    }

    # Extract PDF metadata if applicable
    extra_info = {}
    if data["kind"].lower() == ".pdf":
        try:
            from pdf2image import pdfinfo_from_path
            extra_info = pdfinfo_from_path(fpath)
        except ImportError:
            extra_info = {"Warning": "pdf2image module not installed, cannot extract PDF metadata."}

    if verbose:
        import json
        print(json.dumps(data, indent=2))

    if output == 'json':
        return data
    else:
        return FileInfo(**data)

#! convert to colorful text
def color_text(txt:str, c:Union[str,tupple]='r', bold=False):
    """
    Convert a string to colored text using ANSI codes or RGB truecolor.

    Parameters
    ----------
    txt : str
        The text to colorize.
    c : str | tuple
        Color specification:
        - Short/long ANSI names ('r', 'red', 'g', 'green', ...)
        - RGB tuple/list with 0-1 floats or 0-255 integers
        - Hex string ('#RRGGBB', '#RGB')
        - HTML/CSS color names or CSS formats (rgb(), rgba(), hsl(), hsla())
    bold : bool, default False
        If True, make the text bold.

    Returns
    -------
    str
        ANSI-encoded string suitable for terminal output.
    """

    import re

    # --- Helper 1: map ANSI names to color codes ---
    color_map = {
        'k': 30, 'black': 30,
        'r': 31, 'red': 31,
        'g': 32, 'green': 32,
        'y': 33, 'yellow': 33,
        'b': 34, 'blue': 34,
        'm': 35, 'magenta': 35,
        'c': 36, 'cyan': 36,
        'w': 37, 'white': 37
    }

    # --- Helper 2: convert RGB to ANSI truecolor ---
    def rgb_text(txt, rgb):
        """
        Convert RGB/RGBA tuple (0-1 or 0-255) to ANSI 24-bit escape sequence.
        Ignores alpha if present (terminals don't support it)
        """
        r, g, b = rgb[:3]
        # Scale to 0-255 if 0-1
        if max(r, g, b) <= 1:
            r, g, b = int(r*255), int(g*255), int(b*255)
        return f"\033[38;2;{r};{g};{b}m{txt}\033[0m"

    # --- Helper 3: convert ANSI code to escape ---
    def code_text(txt, code, bold=False):
        style = '1;' if bold else ''
        return f"\033[{style}{code}m{txt}\033[0m"

    # --- Main logic ---
    if isinstance(c, str):
        c_lower = c.strip().lower()
        if c_lower in color_map:
            # Use simple ANSI code
            return code_text(txt, color_map[c_lower], bold=bold)
        else:
            # Try to convert any color string to RGB
            try:
                rgb = color2rgb(c)
                if rgb is not None:
                    return rgb_text(txt, rgb)
            except:
                pass
            # Fallback to white if unrecognized
            return code_text(txt, 37, bold=bold)

    elif isinstance(c, (tuple, list)):
        # Treat as RGB/RGBA
        if len(c) >= 3:
            return rgb_text(txt, c)
        else:
            # Fallback
            return code_text(txt, 37, bold=bold)

    else:
        # Unknown type, fallback
        return code_text(txt, 37, bold=bold)
ctext = cp_func(".", "color_text")

def color2rgb(
    color_input: str | tuple | list | None, 
    alpha: float | None = None
) -> tuple | None:
    """
    Ultimate color conversion utility with support for multiple formats and transparency.
    
    Parameters:
    -----------
    color_input : str | tuple | list | None
        Supported formats:
        - Hex strings ("#RRGGBB", "#RGB")
        - Named colors ("red", "blue")
        - RGB tuples ((0.2, 0.4, 0.6))
        - RGBA tuples ((0.2, 0.4, 0.6, 0.8))
        - HTML/CSS colors ("cornflowerblue")
        - CSS formats:
          - rgb(100,200,50)
          - rgba(100,200,50,0.8)
          - hsl(120,60%,70%)
          - hsla(120,60%,70%,0.8)
    alpha : float | None, optional
        Opacity value (0.0-1.0). If provided, adds/overrides alpha channel.
    
    Returns:
    --------
    tuple | None
        (R, G, B) or (R, G, B, A) tuple in 0-1 range, or None if invalid
    """
    from matplotlib import colors as mcolors
    
    
    if color_input is None:
        return None
    
    # Case 1: Already in RGB/RGBA tuple format
    if isinstance(color_input, (tuple, list)):
        if 3 <= len(color_input) <= 4:
            if all(0 <= x <= 1 for x in color_input):
                if alpha is not None and len(color_input) == 3:
                    return (*color_input, alpha)
                return tuple(color_input)
    
    # Case 2: String input
    if isinstance(color_input, str):
        # Remove whitespace and make lowercase
        color_str = color_input.strip().lower()
        
        # Handle CSS rgb/rgba format
        if color_str.startswith(('rgb(', 'rgba(')):
            try:
                nums = list(map(float, re.findall(r"[\d.]+", color_str)))
                if 3 <= len(nums) <= 4:
                    rgb = tuple(x/255 if i < 3 else x for i, x in enumerate(nums))
                    if alpha is not None:
                        return (*rgb[:3], alpha)
                    return rgb[:4] if len(rgb) == 4 else rgb[:3]
            except:
                pass
        
        # Handle CSS hsl/hsla format
        elif color_str.startswith(('hsl(', 'hsla(')):
            try:
                nums = list(map(float, re.findall(r"[\d.]+", color_str)))
                if 3 <= len(nums) <= 4:
                    h, s, l = nums[0]/360, nums[1]/100, nums[2]/100
                    rgb = mcolors.hsv_to_rgb((h, s, l))
                    if len(nums) == 4:
                        rgb += (nums[3],)
                    if alpha is not None:
                        return (*rgb[:3], alpha)
                    return rgb[:4] if len(rgb) == 4 else rgb[:3]
            except:
                pass
        
        # Standard hex/named color processing
        try:
            rgb = mcolors.to_rgba(color_str)
            if alpha is not None:
                return (*rgb[:3], alpha)
            return rgb if len(rgb) == 4 and rgb[3] != 1 else rgb[:3]
        except ValueError:
            pass
    
    # Fallback for invalid colors
    print(f"Warning: Invalid color format '{color_input}'")
    return None

def color2hex(
    color_input: str | tuple | list | dict | int | None,
    keep_alpha: bool = False,
    force_long: bool = False,
    uppercase: bool = True,
    prefix: str = "#",
    allow_short: bool = True
) -> str | None:
    """
    Ultimate color to hex converter with comprehensive format support.
    
    Parameters:
    -----------
    color_input : str | tuple | list | dict | int | None
        Input color in any of these formats:
        - Hex strings ("#RRGGBB", "#RGB", "RRGGBB", "RGB")
        - Named colors ("red", "blue", "transparent")
        - RGB/RGBA tuples ((0.2, 0.4, 0.6), (255, 0, 0), (100, 100, 100, 0.5))
        - CSS formats:
          - rgb(100,200,50)
          - rgba(100,200,50,0.8)
          - hsl(120,60%,70%)
          - hsla(120,60%,70%,0.8)
        - Integer RGB (0xFF0000 for red)
        - Dictionary {"r": 255, "g": 0, "b": 0} or {"h": 0, "s": 100, "l": 50}
    keep_alpha : bool, optional
        Whether to include alpha channel in hex format (#RRGGBBAA)
    force_long : bool, optional
        Force 6/8-digit hex even when 3/4-digit would be possible
    uppercase : bool, optional
        Use uppercase hex characters (False for lowercase)
    prefix : str, optional
        Prefix for hex string ("#" for CSS, "0x" for programming, "" for raw)
    allow_short : bool, optional
        Allow shortened 3/4-digit hex when possible
    
    Returns:
    --------
    str | None
        Hex color string or None if invalid
        
    Usage:
    color2hex((0.5, 0.2, 0.8)) ‚Üí "#7f33cc"
    color2hex("rgb(127, 51, 204)") ‚Üí "#7f33cc"
    color2hex((0.2, 0.4, 0.6, 0.8), True) ‚Üí "#336699cc"
    color2hex(0xFF0000, uppercase=True) ‚Üí "#FF0000"
    color2hex({"r": 255, "g": 165, "b": 0}, prefix="") ‚Üí "ffa500"
    color2hex("hsl(120, 100%, 50%)") ‚Üí "#00ff00"
    """
    from matplotlib import colors as mcolors
    import math
    def is_short_candidate(x_float):
        """
        Check whether a float channel was originally EXACTLY at a short-hex level.
        Short hex digits represent multiples of 17: 0,17,34,...255.
        This function prevents accidental "#DDD" caused by rounding noise.
        """
        x255 = x_float * 255
        nearest = round(x255 / 17) * 17
        return abs(x255 - nearest) < 1e-6  # tight tolerance

    def to_rgba(color) -> tuple | None:
        """Internal conversion to RGBA tuple"""
        if color is None:
            return None
        if isinstance(color, int):
            if color < 0:
                return None
            return (
                (color >> 16) & 0xFF,
                (color >> 8) & 0xFF,
                color & 0xFF,
                255
            )
        if isinstance(color, dict):
            keys = set(color.keys())
            if {'r','g','b'}.issubset(keys):
                r = color['r']; g = color['g']; b = color['b']
                r = r/255 if isinstance(r, int) and r > 1 else float(r)
                g = g/255 if isinstance(g, int) and g > 1 else float(g)
                b = b/255 if isinstance(b, int) and b > 1 else float(b)
                a = color.get('a', 1.0)
                return (r, g, b, float(a)) 
            if {'h','s','l'}.issubset(keys):
                h = color['h'] / 360
                s = color['s'] / 100
                l = color['l'] / 100
                rgb = mcolors.hsv_to_rgb((h, s, l))
                a = color.get('a', 1.0)
                return (*rgb, float(a))
            return None
        # ---------- String input ----------
        if isinstance(color, str):
            color = color.strip()

            # raw hex without '#'
            if re.fullmatch(r"[0-9a-fA-F]{3,8}", color):
                return mcolors.to_rgba("#" + color)

            # CSS rgb(), rgba(), hsl(), hsla()
            if color.lower().startswith(("rgb(", "rgba(", "hsl(", "hsla(")):
                try:
                    return mcolors.to_rgba(color)
                except ValueError:
                    return None

            # named colors
            try:
                return mcolors.to_rgba(color)
            except ValueError:
                return None
        # ---------- Tuple / list ----------
        if isinstance(color, (tuple, list)):
            if len(color) in (3, 4):
                normalized = []
                for i, v in enumerate(color):
                    if i < 3:  # RGB channels
                        if isinstance(v, int):
                            normalized.append(v / 255 if v > 1 else v)
                        else:
                            normalized.append(float(v))
                    else:  # Alpha channel
                        normalized.append(float(v))
                return tuple(normalized)
        return None

    rgba = to_rgba(color_input)
    if rgba is None:
        return None

    # convert RGBA floats ‚Üí integer channels
    comp = []
    for i, c in enumerate(rgba):
        if i == 3 and not keep_alpha:
            break  # remove alpha
        comp.append(round(c * 255))

    # determine if short-hex is allowed
    allow_short_hex = (
        allow_short
        and not force_long
        and len(comp) in (3, 4)
        and all(is_short_candidate(rgba[i]) for i in range(3))  # use raw floats
    )
    # build hex string
    if allow_short_hex:
        # convert multiples of 17 ‚Üí single hex digit (x//17)
        short_vals = [ (round(rgba[i]*255) // 17) for i in range(3) ]
        if keep_alpha:
            short_vals.append(round(rgba[3] * 255))
        hex_str = "".join(f"{v:1x}" for v in short_vals)
    else:
        hex_str = "".join(f"{v:02x}" for v in comp)

    if uppercase:
        hex_str = hex_str.upper()

    return f"{prefix}{hex_str}" 

# ! format excel file
def hex2argb(color):
    """
    Convert a color name or hex code to aARGB format required by openpyxl.

    :param color: A color in the format: 'blue', '#RRGGBB', 'RRGGBB', 'aARRGGBB'
    :return: A hex color code in the format aARRGGBB.

    Example:
        print(hex2argb("blue"))      # Output: FF0000FF
        print(hex2argb("FFFF00"))    # Output: FFFFFF00
        print(hex2argb("#DF4245"))   # Output: FFDf4245
        print(hex2argb("FF00FF00"))  # Output: FF00FF00 (already in aARGB format)
    """
    import matplotlib.colors as mcolors
    
    color = color.lower().replace(" ", "") # 'light blue'
    # Convert color name (e.g., "blue") to hex
    if color.lower() in mcolors.CSS4_COLORS:
        color = mcolors.CSS4_COLORS[color.lower()].lstrip("#")
    color = color.lstrip("#").upper()# Remove '#' if present
    
    # Validate hex format
    if not re.fullmatch(r"[A-F0-9]{6,8}", color):
        raise ValueError(f"Ê†ºÂºèÈîôËØØ: {color}, Â∫îËØ•‰ΩøÁî® RRGGBB, #RRGGBB, or aARRGGBB format.")

    # If already in aARRGGBB format (8 chars), return as is
    if len(color) == 8:
        return color
    
    # If in RRGGBB format, add FF (full opacity) as alpha
    return f"FF{color}"

def extract_kwargs(func):
    # Get the signature of the function
    signature = inspect.signature(func)
    # Extract parameters that are kwargs (parameters with default values or **kwargs)
    kwargs = {
        param.name: param.default
        for param in signature.parameters.values()
        if param.default is not inspect.Parameter.empty
    }

    return kwargs

def handle_kwargs(
    params: Dict[str, Any],
    func: Optional[Callable] = None,
    how: str = "pop",
    strict: bool = False,
    ignore_private: bool = True,
    keep_extra: bool = False,
    exclude_list: List = None,
    default_values: Dict[str, Any] = None,
    prefix: str = "",
    suffix: str = "",
    verbose: bool = False,
    **kwargs,
) -> Dict[str, Any]:
    """
    Handle keyword arguments for a function by validating, filtering, or transforming them.

    Parameters:
    -----------
    params : Dict[str, Any]
        Dictionary of keyword arguments to handle
    func : Callable, optional
        Target function to validate parameters against
    how : str, default='pop'
        Operation mode:
        - 'pop': Remove invalid parameters (default)
        - 'filter': Return only valid parameters
        - 'raise': Raise error on invalid parameters
        - 'warn': Warn about invalid parameters but proceed
        - 'prefix': Add prefix to invalid parameters
        - 'suffix': Add suffix to invalid parameters
        - 'map': Use mapping dictionary to rename parameters
        - 'defaults': Fill missing parameters with defaults
        - 'validate': Validate types if type hints are available
    strict : bool, default=False
        If True, require exact parameter matches (including positional-only)
    ignore_private : bool, default=True
        If True, ignore parameters starting with underscore
    keep_extra : bool, default=False
        If True, keep extra parameters in the returned dict
    default_values : Dict[str, Any], optional
        Default values to use for missing parameters
    prefix : str, default=''
        Prefix to add to invalid parameters when how='prefix'
    suffix : str, default=''
        Suffix to add to invalid parameters when how='suffix'
    **kwargs :
        Additional arguments for specific operations:
        - mapping: Dict for parameter renaming when how='map'
        - warn_func: Custom warning function when how='warn'

    Returns:
    --------
    Dict[str, Any]
        Processed parameters dictionary
    """
    from inspect import signature, Parameter

    if verbose:
        print(
            """
        # Basic usage (your original use case)
        processed = handle_kwargs(params, my_function, how='pop')

        # Filter to only valid parameters
        filtered = handle_kwargs(params, my_function, how='filter')

        # Raise error on invalid parameters
        try:
            validated = handle_kwargs(params, my_function, how='raise')
        except TypeError as e:
            print(e)

        # Warn about invalid parameters
        warned = handle_kwargs(params, my_function, how='warn')

        # Use as decorator
        @handle_kwargs_decorator(how='filter')
        def my_function(a, b, c=10):
            return a + b + c

        # Fill defaults
        defaults = {'x': 1, 'y': 2}
        result = handle_kwargs(params, my_function, how='defaults', default_values=defaults)

        # Validate types
        validated = handle_kwargs(params, my_function, how='validate')
    """
        )
    if not isinstance(params, dict):
        raise TypeError(f"params must be a dictionary, got {type(params).__name__}")

    if func is None:
        # If no function provided, return params as-is or apply basic operations
        if how in ["pop", "filter", "raise", "warn"]:
            return params.copy()
        return params

    # Get valid parameters from the function
    params_valid = man(func, return_obj=1, verbose=0)

    if not params_valid:
        if strict:
            raise ValueError(f"No parameters found for function {func.__name__}")
        return params.copy()

    # Handle private parameters
    if ignore_private:
        params_valid = [p for p in params_valid if not p.startswith("_")]

    # Handle positional-only parameters in strict mode
    if strict:
        try:
            sig = signature(func)
            pos_only_params = [
                name
                for name, param in sig.parameters.items()
                if param.kind in [Parameter.POSITIONAL_ONLY, Parameter.VAR_POSITIONAL]
            ]
            params_valid = [p for p in params_valid if p not in pos_only_params]
        except:
            pass

    result = params.copy()
    extra_params = {}
    invalid_params = {}

    # Identify valid and invalid parameters
    for key, value in params.items():
        if exclude_list is not None and key in exclude_list:
            invalid_params[key] = value
        elif key in params_valid:
            extra_params[key] = value
        else:
            invalid_params[key] = value

    # Apply the requested operation
    if how == "pop":
        # Remove invalid parameters
        for key in invalid_params:
            result.pop(key, None)

    elif how == "filter":
        # Return only valid parameters
        result = {k: v for k, v in params.items() if k in params_valid}

    elif how == "raise":
        # Raise error if invalid parameters exist
        if invalid_params:
            invalid_keys = list(invalid_params.keys())
            raise TypeError(
                f"Invalid parameter(s) for function {func.__name__}: {invalid_keys}. "
                f"Valid parameters are: {params_valid}"
            )

    elif how == "warn":
        # Warn about invalid parameters
        if invalid_params:
            warn_func = kwargs.get("warn_func", print)
            warning_msg = (
                f"Warning: Invalid parameter(s) for function {func.__name__}: "
                f"{list(invalid_params.keys())}. They will be ignored."
            )
            if callable(warn_func):
                warn_func(warning_msg)
            else:
                print(warning_msg)
        # Remove invalid parameters after warning
        for key in invalid_params:
            result.pop(key, None)

    elif how == "prefix":
        # Add prefix to invalid parameters
        renamed = {}
        for key, value in invalid_params.items():
            new_key = f"{prefix}{key}"
            renamed[new_key] = value
            result.pop(key)
        result.update(renamed)

    elif how == "suffix":
        # Add suffix to invalid parameters
        renamed = {}
        for key, value in invalid_params.items():
            new_key = f"{key}{suffix}"
            renamed[new_key] = value
            result.pop(key)
        result.update(renamed)

    elif how == "map":
        # Rename parameters using mapping
        mapping = kwargs.get("mapping", {})
        renamed = {}
        for key, value in invalid_params.items():
            if key in mapping:
                new_key = mapping[key]
                renamed[new_key] = value
                result.pop(key)
            elif not keep_extra:
                result.pop(key, None)
        result.update(renamed)

    elif how == "defaults":
        # Fill missing parameters with defaults
        if default_values:
            for key, default_val in default_values.items():
                if key in params_valid and key not in result:
                    result[key] = default_val

    elif how == "validate":
        # Validate parameter types if type hints are available
        try:
            sig = signature(func)
            annotations = func.__annotations__

            for key, value in result.items():
                if key in annotations:
                    expected_type = annotations[key]
                    # Allow None for Optional types
                    if (
                        value is not None
                        and not isinstance(value, expected_type)
                        and expected_type != type(None)
                    ):
                        # Check for Optional[type] pattern
                        if (
                            hasattr(expected_type, "__origin__")
                            and expected_type.__origin__ is Union
                        ):
                            # Handle Union types including Optional
                            if type(None) in expected_type.__args__:
                                valid_types = [
                                    t for t in expected_type.__args__ if t != type(None)
                                ]
                                if not any(isinstance(value, t) for t in valid_types):
                                    raise TypeError(
                                        f"Parameter '{key}' should be one of {valid_types} or None, "
                                        f"got {type(value).__name__}"
                                    )
                            else:
                                if not isinstance(value, expected_type):
                                    raise TypeError(
                                        f"Parameter '{key}' should be {expected_type}, "
                                        f"got {type(value).__name__}"
                                    )
                        else:
                            raise TypeError(
                                f"Parameter '{key}' should be {expected_type}, "
                                f"got {type(value).__name__}"
                            )
        except (AttributeError, KeyError):
            # If type validation fails, continue without validation
            pass

    else:
        raise ValueError(
            f"Invalid 'how' parameter: {how}. "
            f"Valid options are: pop, filter, raise, warn, prefix, suffix, map, defaults, validate"
        )

    # Apply keep_extra flag
    if not keep_extra and how not in ["prefix", "suffix", "map"]:
        # Remove any remaining invalid parameters
        for key in invalid_params:
            result.pop(key, None)

    return result

def df2workbook(dataframes: dict, filename: str, mode: str = 'w',**kwargs):
    """
    Â∞ÜÂçï‰∏™DataFrameÂåñÊàêworkbook

    Parameters:
    - dataframes: dict of {sheet_name: DataFrame}
    - filename: str, path to the Excel file
    - mode: 'w' = write new file, 'a' = append to existing file
    """
    from openpyxl import load_workbook
    if mode not in ['w', 'a']:
        raise ValueError("Mode must be 'w' (write) or 'a' (append).")

    if mode == 'a' and os.path.exists(filename):
        # Append mode using openpyxl
        with pd.ExcelWriter(filename, engine='openpyxl', mode='a', if_sheet_exists='replace') as writer:
            for sheet_name, df in dataframes.items():
                df.to_excel(writer, sheet_name=sheet_name, index=False)
    else:
        # Write mode or file does not exist
        with pd.ExcelWriter(filename, engine='xlsxwriter') as writer:
            for sheet_name, df in dataframes.items():
                df.to_excel(writer, sheet_name=sheet_name, index=False)

# ! excel formatting
def copy_format(
    fpath: str, target_path: str, sheet_name, copy_values: bool = True, save:bool=True
):
    """
    ULTIMATE worksheet formatting copier with complete validation and conditional formatting support

    Features:
    - Full style copying (fonts, borders, fills, alignment)
    - Number formats and comments
    - Merged cells and freeze panes
    - Auto-filters
    - Data validations (including cross-sheet)
    - Conditional formatting
    - Optimized performance

    Parameters:
    - fpath: Path to source workbook
    - target_path: Path to target workbook
    - sheet_name: Single string (same name) or tuple/list (source_name, target_name)
    - copy_values: Whether to copy cell values (default: True)

    Returns: Modified target worksheet

    ‰æãÂ≠ê:
    1. copy_format("source.xlsx", "target.xlsx", sheet_name="same sheet_name")
    2. copy_format("source.xlsx", "target.xlsx", sheet_name=["sheet_name1", "sheet_name2"])
    3. copy_format("source.xlsx", "target.xlsx", sheet_name=("sheet_name1", "sheet_name2"))
    """
    import openpyxl
    from openpyxl import load_workbook,Workbook
    from openpyxl.worksheet.worksheet import Worksheet

    from openpyxl.styles import Font, Border, PatternFill, Alignment
    from copy import copy
    

    # --- Helper Functions ---
    def safe_copy_style(obj):
        """Deep copy style objects without recursion issues"""
        if obj is None:
            return None

        if isinstance(obj, (Font, Border, PatternFill, Alignment)):
            new_obj = obj.__class__()
            for attr in dir(obj):
                if not attr.startswith("_") and not callable(getattr(obj, attr)):
                    try:
                        setattr(new_obj, attr, getattr(obj, attr))
                    except (AttributeError, TypeError):
                        pass
            return new_obj
        return copy(obj)

    # --- Main Execution ---
    try:
        # Load workbooks with optimized settings
        wb_source = load_workbook(
            fpath, read_only=False, keep_vba=False, data_only=True
        )
        if isinstance(target_path, Worksheet) or isinstance(target_path, Workbook):
            wb_target=target_path
        else:
            wb_target = load_workbook(target_path, read_only=False, keep_vba=False)

        # Handle sheet names
        if isinstance(sheet_name, (list, tuple)) and len(sheet_name) == 2:
            src_name, tgt_name = sheet_name
        else:
            src_name = tgt_name = str(sheet_name)

        # Verify sheet existence
        if src_name not in wb_source.sheetnames:
            raise ValueError(f"Source sheet '{src_name}' not found")
        if tgt_name not in wb_target.sheetnames:
            raise ValueError(f"Target sheet '{tgt_name}' not found")

        src_ws = wb_source[src_name]
        tgt_ws = wb_target[tgt_name]

        # Backup validations and conditional formatting FIRST
        backup = _backup_validations(src_ws)

        # --- Copy Core Formatting ---
        # 1. Column dimensions
        for col, dim in src_ws.column_dimensions.items():
            tgt_dim = tgt_ws.column_dimensions[col]
            tgt_dim.width = dim.width
            tgt_dim.hidden = dim.hidden
            if dim.has_style:
                tgt_dim.font = safe_copy_style(dim.font)
                tgt_dim.fill = safe_copy_style(dim.fill)
                tgt_dim.border = safe_copy_style(dim.border)
                tgt_dim.alignment = safe_copy_style(dim.alignment)

        # 2. Row dimensions
        for row, dim in src_ws.row_dimensions.items():
            tgt_dim = tgt_ws.row_dimensions[row]
            tgt_dim.height = dim.height
            tgt_dim.hidden = dim.hidden
            if dim.has_style:
                tgt_dim.font = safe_copy_style(dim.font)
                tgt_dim.fill = safe_copy_style(dim.fill)
                tgt_dim.border = safe_copy_style(dim.border)
                tgt_dim.alignment = safe_copy_style(dim.alignment)

        # 3. Cell styles and values
        for row in src_ws.iter_rows():
            for cell in row:
                tgt_cell = tgt_ws.cell(
                    row=cell.row,
                    column=cell.column,
                    value=cell.value if copy_values else None,
                )
                if cell.has_style:
                    tgt_cell.font = safe_copy_style(cell.font)
                    tgt_cell.fill = safe_copy_style(cell.fill)
                    tgt_cell.border = safe_copy_style(cell.border)
                    tgt_cell.alignment = safe_copy_style(cell.alignment)
                    tgt_cell.number_format = cell.number_format
                    if cell.comment:
                        tgt_cell.comment = copy(cell.comment)

        # --- Copy Structural Elements ---
        # 1. Merged cells
        tgt_ws.merged_cells.ranges = []
        for merged_range in src_ws.merged_cells.ranges:
            tgt_ws.merge_cells(str(merged_range))

        # 2. Freeze panes and auto-filter
        tgt_ws.freeze_panes = src_ws.freeze_panes
        if src_ws.auto_filter.ref:
            tgt_ws.auto_filter.ref = src_ws.auto_filter.ref

        # --- Restore Advanced Features ---
        _restore_validations(tgt_ws, backup)

        # Save with optimization
        if save:
            wb_target.save(target_path)
            return tgt_ws

    except Exception as e:
        warnings.warn(f"Format copying failed: {str(e)}")
        raise
    finally:
        # Ensure workbooks are closed
        if "wb_source" in locals():
            wb_source.close()
        if "wb_target" in locals():
            wb_target.close()

# ! =========(below) interact with worrkbook and DataFrame=========== 

from openpyxl import load_workbook 
from openpyxl.workbook.workbook import Workbook 
from openpyxl.utils import get_column_letter 

class ColumnNotFoundError(ValueError):
    """Exception raised when a required column is not found."""
    pass
class MultipleMatchError(ValueError):
    """Exception raised when multiple matches are found and strategy is 'error'."""
    pass
class DataFrameAlignExcel:
    """
    A powerful tool for updating Excel files with data from DataFrames with various matching strategies.

    Features:
    - Accepts either file path or open Workbook object
    - Multiple matching strategies (exact, contains, starts_with, ends_with, regex)
    - Multiple value update strategies (overwrite, add, subtract, multiply, divide, append)
    - Support for multiple worksheets
    - Automatic column creation
    - Value normalization options
    - Detailed logging and dry-run mode
    - Progress reporting
    - Data validation
    - make_backup functionality
    """

    def __init__(self, fpath: Union[str, Workbook], df: pd.DataFrame = None):
        """
        Initialize the DataFrameAlignExcel.

        Args:
            fpath: Path to the Excel file (str) or open Workbook object
            df: Optional DataFrame to use for updates
        """
        self.fpath_or_wb = fpath
        self.df = df.copy() if df is not None else None
        self.wb = None
        self.backup_path = None
        self.log = []
        self.owns_workbook = False  # Track whether we created the workbook or it was passed in
        self.compound_key_separator = unicodedata.lookup("VERTICAL LINE")  # Rare Unicode separator


    def load_workbook(self) -> None:
        """Load the Excel workbook if a path was provided."""
        if isinstance(self.fpath_or_wb, str):
            if not os.path.exists(self.fpath_or_wb):
                raise FileNotFoundError(f"Excel file not found: {self.fpath_or_wb}")
            self.wb = load_workbook(self.fpath_or_wb)
            self.owns_workbook = True
        elif isinstance(self.fpath_or_wb, Workbook):
            self.wb = self.fpath_or_wb
            self.owns_workbook = False
        else:
            raise TypeError("fpath must be either a string path or an openpyxl Workbook object")

    def create_backup(self) -> None:
        """Create a make_backup of the original Excel file (only if we loaded from a file)."""
        if not isinstance(self.fpath_or_wb, str):
            self.log.append("Skipping make_backup - working with Workbook object directly")
            return

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.backup_path = os.path.join(
            os.path.dirname(self.fpath_or_wb),
            f"backup_{timestamp}_{os.path.basename(self.fpath_or_wb)}",
        )
        self.wb.save(self.backup_path)
        self.log.append(f"Created make_backup at: {self.backup_path}")

    def save_workbook(self, dir_save: str = None) -> None:
        """
        Save the workbook to a file.

        Args:
            dir_save: Optional path to save to. If None and we loaded from a file,
                       saves to the original path.
        """
        if self.wb is None:
            raise ValueError("No workbook loaded")

        if dir_save is None:
            if isinstance(self.fpath_or_wb, str):
                dir_save = self.fpath_or_wb
            else:
                dir_save = datetime.now().strftime("%Y%m%d_%H%M%S") + ".xlsx"
                self.log.append(f"No save path provided. Using: {dir_save}")

        # Ensure directory exists
        os.makedirs(os.path.dirname(dir_save), exist_ok=True)
        self.wb.save(dir_save)
        self.log.append(f"Saved workbook to: {dir_save}")
        return dir_save

    def normalize_value(self, value, clean_keys: str = "strip_split_first") -> str:
        """
        Normalize a value based on the specified method.

        Args:
            value: Value to normalize
            clean_keys: One of:
                - 'strip': just strip whitespace
                - 'strip_lower': strip and lowercase
                - 'strip_split_first': strip and take first part before comma
                - 'strip_split_last': strip and take last part after comma
                - None: no normalization

        Returns:
            Normalized value
        """
        if value is None:
            return None

        value = str(value)

        if clean_keys is None:
            return value

        if clean_keys == "strip":
            return value.strip()
        elif clean_keys == "strip_lower":
            return value.strip().lower()
        elif clean_keys == "strip_split_first":
            return value.strip().split(",")[0].strip()
        elif clean_keys == "strip_split_last":
            parts = value.strip().split(",")
            return parts[-1].strip() if len(parts) > 1 else value.strip()
        else:
            warnings.warn(f"Unknown clean_keys: {clean_keys}. Using 'strip'.")
            return value.strip() 
    def find_column_index(
        self, ws, header_row: int, column_name: str, max_search_columns: int = 100
    ) -> int:
        """
        Efficiently find the column index (1-based) for a given column name.

        Args:
            ws: Worksheet object
            header_row: Row number containing headers (1-based)
            column_name: Column name to find
            max_search_columns: Max number of columns to search

        Returns:
            Column index (1-based), or -1 if not found
        """
        for col_idx in range(1, max_search_columns + 1):
            cell = ws.cell(row=header_row, column=col_idx)
            if cell.value is None:
                continue
            if str(cell.value).strip().lower() == column_name.lower():
                return col_idx
        return -1


    def apply_match_method(self, value: str, pattern: str, method: str) -> bool:
        """
        Apply the specified matching method to compare values.

        Args:
            value: Value to test
            pattern: Pattern to match against
            method: Matching method (exact, contains, starts_with, ends_with)

        Returns:
            True if match, False otherwise
        """
        if method == "exact":
            return value == pattern
        elif method == "contains":
            return pattern in value
        elif method == "starts_with":
            return value.startswith(pattern)
        elif method == "ends_with":
            return value.endswith(pattern)
        elif method == "regex":
            return bool(re.search(pattern, value))
        else:
            raise ValueError(f"Unsupported match method: {method}")

    # def build_compound_key(row, columns):
    #     parts = [self.normalize_value(row[col], clean_keys) for col in columns]
    #     return "||".join(parts)

    def build_compound_key(self, parts: List[str]) -> str:
        """
        Build a compound key from multiple parts using a rare Unicode separator.
        Args:
            parts: List of string parts to combine
        Returns:
            Compound key string
        """
        safe_parts = []
        for p in parts:
            if p is None:
                safe_parts.append("‚àÖ")  # Unicode null symbol
            else:
                safe_parts.append(str(p).strip())
        return self.compound_key_separator.join(safe_parts)

    def handle_multiple_matches(
        self,
        df_matches: pd.DataFrame,
        df_col: str,
        strategy: str,
        concat_separator: str = ", "
    ) -> any:
        """
        Handle multiple matches according to the specified strategy.
        
        Args:
            df_matches: DataFrame with multiple matching rows
            df_col: Column name to extract values from
            strategy: Strategy for handling multiple matches
            concat_separator: Separator for concatenation strategies
            
        Returns:
            Aggregated or selected value
        """
        if strategy == "first":
            return df_matches[df_col].iloc[0]
        elif strategy == "last":
            return df_matches[df_col].iloc[-1]
        elif strategy == "sum":
            return df_matches[df_col].sum()
        elif strategy == "mean":
            return df_matches[df_col].mean()
        elif strategy == "concat":
            return concat_separator.join([str(x) for x in df_matches[df_col]])
        elif strategy == "min":
            return df_matches[df_col].min()
        elif strategy == "max":
            return df_matches[df_col].max()
        else:
            raise ValueError(f"Unknown multiple match strategy: {strategy}")

    def update_values(
        self,
        df: pd.DataFrame = None,
        sheet_name: Union[str, int, List[Union[str, int]]] = 0,
        header_row: int = 1,
        column_match: Union[Dict[str, str], List[Tuple[str, str]]] = None,
        column_mapping: Union[Dict[str, str], List[Tuple[str, str]]] = None,
        clean_keys: str = "strip_split_first",
        match_method: str = "exact",
        update_strategy: str = "overwrite",
        create_missing_columns: bool = True,
        preview_only: bool = False,
        show_progress: bool = True,
        skip_no_match: bool = True,
        make_backup: bool = True,
        save_path: str = None,
        verbose: bool = False,
        on_multiple_matches: Union[str, Dict[str, str]] = "first",
        concat_separator: str = ", "
    ) -> Dict[str, int]:
        """
        Update Excel with DataFrame data using various matching and update strategies.
        
        Args:
            on_multiple_matches: Strategy for handling multiple matches:
                - 'first': use first match (default)
                - 'last': use last match
                - 'error': raise error
                - 'warn': warn and use first
                - 'sum': sum numeric values
                - 'mean': average numeric values
                - 'concat': concatenate text values
                - 'min': minimum value
                - 'max': maximum value
                - Dict: per-column strategies
            concat_separator: Separator for concatenation strategies
        """
        start_time = datetime.now()

        # Validate input parameters
        if df is None:
            df = self.df
        if df is None:
            raise ValueError("No DataFrame provided")

        if not column_match:
            raise ValueError("column_match must be specified")

        # Prepare column mappings
        if column_mapping is None:
            column_mapping = {col: col for col in df.columns}
        if not column_mapping:
            raise ValueError("column_mapping must be specified")

        # Convert to list of tuples for consistent processing
        if isinstance(column_match, dict):
            column_match = list(column_match.items())
        if isinstance(column_mapping, dict):
            column_mapping = list(column_mapping.items())

        # Load workbook if needed
        if self.wb is None:
            self.load_workbook()

        # Create backup if requested
        if not preview_only and make_backup:
            self.create_backup()

        # Initialize statistics
        stats = {
            "processed_sheet_names": [],
            "processed_sheets": 0,
            "total_updates": 0,
            "skipped_rows": 0,
            "created_columns": 0,
            "matched_rows": 0,
            "multiple_matches": 0,
            "multiple_match_warnings": 0
        }

        # Prepare sheet names
        if not isinstance(sheet_name, list):
            sheet_names = [sheet_name]
        else:
            sheet_names = sheet_name

        # Precompute compound keys for exact matching
        if match_method == "exact":
            match_df_cols = [df_col for df_col, _ in column_match]
            df["__compound_key__"] = df.apply(
                lambda row: self.build_compound_key([
                    self.normalize_value(row[col], clean_keys)
                    for col in match_df_cols
                ]),
                axis=1,
            )

        # Process each sheet
        for sheet in sheet_names:
            # Get worksheet
            if isinstance(sheet, str):
                ws = self.wb[sheet]
            elif isinstance(sheet, int):
                ws = self.wb.worksheets[sheet]
            else:
                ws = self.wb.active

            sheet_title = ws.title
            self.log.append(f"\nProcessing sheet: {sheet_title}")
            if verbose:
                print(f"Processing sheet: {sheet_title}")

            # Find or create update columns
            update_col_indices = {}
            for df_col, excel_col in column_mapping:
                col_idx = self.find_column_index(ws, header_row, excel_col)
                if col_idx == -1:
                    if create_missing_columns:
                        # Find last column with content
                        last_col = 1
                        for cell in ws[header_row]:
                            if cell.value is not None:
                                last_col = cell.column
                        col_idx = last_col + 1
                        ws.cell(row=header_row, column=col_idx, value=excel_col)
                        stats["created_columns"] += 1
                        self.log.append(f"Created new column '{excel_col}' at position {col_idx}")
                    else:
                        raise ColumnNotFoundError(
                            f"Column '{excel_col}' not found and create_missing_columns=False"
                        )
                update_col_indices[excel_col] = col_idx

            # Cache match column indices
            match_col_indices = {}
            for df_col, excel_col in column_match:
                col_idx = self.find_column_index(ws, header_row, excel_col)
                if col_idx == -1:
                    if skip_no_match:
                        self.log.append(
                            f"Warning: Match column '{excel_col}' not found. Skipping."
                        )
                        continue
                    else:
                        raise ColumnNotFoundError(
                            f"Match column '{excel_col}' not found"
                        )
                match_col_indices[excel_col] = col_idx

            # Prepare row iterator with progress bar
            total_rows = ws.max_row - header_row
            row_iterator = ws.iter_rows(min_row=header_row + 1)
            if show_progress:
                row_iterator = tqdm(
                    row_iterator,
                    total=total_rows,
                    desc=f"Processing {sheet_title}",
                    unit="row",
                )

            # Process each row
            for row_idx, row in enumerate(row_iterator, start=header_row + 1):
                # Collect patterns for matching
                patterns = {}
                for df_col, excel_col in column_match:
                    col_idx = match_col_indices.get(excel_col)
                    if col_idx is None:  # Skipped column
                        continue
                    cell_value = row[col_idx - 1].value
                    norm_value = self.normalize_value(cell_value, clean_keys) or ""
                    patterns[df_col] = norm_value

                # Skip if no valid match columns
                if not patterns:
                    stats["skipped_rows"] += 1
                    continue

                # Find matching rows in DataFrame
                if match_method == "exact":
                    # Build compound key for current Excel row
                    compound_key_parts = [patterns[df_col] for df_col, _ in column_match]
                    compound_key = self.build_compound_key(compound_key_parts)
                    df_matches = df[df["__compound_key__"] == compound_key]
                else:
                    # Apply non-exact matching
                    df_matches = df.copy()
                    for df_col, pattern in patterns.items():
                        df_matches = df_matches[
                            df_matches[df_col].apply(
                                lambda x: self.apply_match_method(
                                    self.normalize_value(x, clean_keys) or "",
                                    pattern,
                                    match_method
                                )
                            )
                        ]

                # Handle match results
                n_matches = len(df_matches)
                if n_matches == 0:
                    stats["skipped_rows"] += 1
                    continue
                elif n_matches == 1:
                    # Single match - use the first row
                    matched_row = df_matches.iloc[0]
                    is_multiple = False
                else:
                    # Multiple matches - handle according to strategy
                    stats["multiple_matches"] += 1
                    is_multiple = True
                    
                    # Determine strategy
                    if on_multiple_matches == "error":
                        raise MultipleMatchError(
                            f"Found {n_matches} matches for row {row_idx} in sheet {sheet_title}"
                        )
                    elif on_multiple_matches == "warn":
                        warnings.warn(
                            f"Found {n_matches} matches for row {row_idx} in sheet {sheet_title}. Using first."
                        )
                        stats["multiple_match_warnings"] += 1
                        matched_row = df_matches.iloc[0]
                    else:
                        # Create a synthetic row with aggregated values
                        matched_row = {}
                        for df_col, _ in column_mapping:
                            # Get column-specific strategy if provided as dict
                            if isinstance(on_multiple_matches, dict):
                                strategy = on_multiple_matches.get(df_col, "first")
                            else:
                                strategy = on_multiple_matches
                            
                            # Apply the strategy
                            matched_row[df_col] = self.handle_multiple_matches(
                                df_matches, df_col, strategy, concat_separator
                            )

                stats["matched_rows"] += 1

                # Update values for matched row
                for df_col, excel_col in column_mapping:
                    col_idx = update_col_indices.get(excel_col)
                    if col_idx is None:  # Column not found or skipped
                        continue

                    cell = ws.cell(row=row_idx, column=col_idx)
                    
                    # Get new value (handle both Series and dict)
                    if isinstance(matched_row, pd.Series):
                        new_value = matched_row[df_col]
                    else:
                        new_value = matched_row.get(df_col)

                    # Skip None values
                    if new_value is None:
                        continue

                    # Apply update strategy
                    if update_strategy == "overwrite":
                        cell.value = new_value
                    elif update_strategy in ("add", "subtract", "multiply", "divide"):
                        try:
                            old_value = (
                                float(cell.value) if cell.value is not None else 0
                            )
                            new_num = float(new_value) if new_value is not None else 0

                            if update_strategy == "add":
                                cell.value = old_value + new_num
                            elif update_strategy == "subtract":
                                cell.value = old_value - new_num
                            elif update_strategy == "multiply":
                                cell.value = old_value * new_num
                            elif update_strategy == "divide":
                                if new_num == 0:
                                    cell.value = old_value
                                else:
                                    cell.value = old_value / new_num
                        except (ValueError, TypeError):
                            if skip_no_match:
                                continue
                            raise ValueError(
                                f"Could not perform {update_strategy} on non-numeric values"
                            )
                    elif update_strategy == "append":
                        separator = ", " if cell.value else ""
                        cell.value = (
                            f"{cell.value}{separator}{new_value}"
                            if cell.value
                            else new_value
                        )
                    else:
                        raise ValueError(f"Unknown update_strategy: {update_strategy}")

                    stats["total_updates"] += 1

            stats["processed_sheets"] += 1
            stats["processed_sheet_names"].append(sheet_title)

        # Save workbook after all sheets processed
        if not preview_only:
            saved_path = self.save_workbook(save_path)
            self.log.append(f"Final workbook saved to: {saved_path}")
        else:
            self.log.append("\nDry run complete - no changes saved")

        # Clean up compound key if created
        if "__compound_key__" in df.columns:
            df.drop(columns="__compound_key__", inplace=True, errors="ignore")

        # Remove backup if in preview mode
        if preview_only and self.backup_path and os.path.exists(self.backup_path):
            try:
                os.remove(self.backup_path)
            except Exception:
                pass

        # Prepare summary
        duration = datetime.now() - start_time
        summary = (
            f"\nUpdate Summary ({duration.total_seconds():.2f}s):\n"
            f"\tSheets processed: {stats['processed_sheets']} ({', '.join(stats['processed_sheet_names'])})\n"
            f"\tRows matched: {stats['matched_rows']}\n"
            f"\tCells updated: {stats['total_updates']}\n"
            f"\tSkipped rows: {stats['skipped_rows']}\n"
            f"\tNew columns created: {stats['created_columns']}\n"
            f"\tMultiple matches encountered: {stats['multiple_matches']}\n"
            f"\tMultiple match warnings: {stats['multiple_match_warnings']}"
        )

        self.log.append(summary)
        if show_progress:
            print(summary)

        return stats

    def get_log(self) -> str:
        """Get the operation log as a string."""
        return "\n".join(self.log)

    def close(self) -> None:
        """Close the workbook if we own it."""
        if self.wb is not None and self.owns_workbook:
            self.wb.close()
            self.wb = None

    def __enter__(self):
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        self.close()

# Alias for mapping type
DFToExcelMapping = Union[Dict[str, str], List[Tuple[str, str]]]

def df_align(
    fpath: Union[str, Workbook],
    df: pd.DataFrame,
    sheet_name: Union[str, int, List[Union[str, int]]] = 0,
    header_row: int = 1,
    column_match: DFToExcelMapping = None,
    column_mapping: DFToExcelMapping = None,
    clean_keys: str = "strip_split_first",
    match_method: str = "exact",
    update_strategy: str = "overwrite",
    create_missing_columns: bool = True,
    preview_only: bool = False,
    show_progress: bool = True,
    skip_no_match: bool = True,
    make_backup: bool = True,
    save_path: str = None,
    verbose: bool = False,
    on_multiple_matches: Union[str, Dict[str, str]] = "first",
    concat_separator: str = ", "
) -> Dict[str, int]:
    """
    High-level function to align DataFrame data with an Excel workbook.

    Example:
        wb = load_workbook("data.xlsx")
        df_align(
            fpath=wb,
            df=df,
            sheet_name="Sheet1",
            header_row=2,
            column_match={"SampleID": "SampleID"},
            column_mapping={"Vials": "Vials", "Total": "Total Vials"},
            on_multiple_matches={"Vials": "sum", "Notes": "concat"},
            concat_separator="; "
        )

    Args:
        on_multiple_matches: Strategy for handling multiple matches:
            - 'first': use first match (default)
            - 'last': use last match
            - 'error': raise error
            - 'warn': warn and use first
            - 'sum': sum numeric values
            - 'mean': average numeric values
            - 'concat': concatenate text values
            - 'min': minimum value
            - 'max': maximum value
            - Dict: per-column strategies
        concat_separator: Separator for concatenation strategies

    Returns:
        Dictionary with operation statistics

    # Basic non-exact matching (contains):
    df_align(
        "data.xlsx",
        df_updates,
        match_method="contains",
        column_match={"Product": "Item Name"}
    )
    # Multiple match handling with warnings:
    df_align(
        "inventory.xlsx",
        df_restock,
        match_method="starts_with",
        column_match={"SKU": "Product Code"},
        on_multiple_matches="warn"
    )
    # Advanced per-column strategies:
    df_align(
        "sales.xlsx",
        df_transactions,
        match_method="ends_with",
        column_match={"CustomerID": "Client ID"},
        column_mapping={
            "Amount": "Total",
            "Notes": "Comments"
        },
        on_multiple_matches={
            "Amount": "sum",  # Sum amounts for multiple matches
            "Notes": "concat"  # Concatenate notes
        },
        concat_separator="; "
    )
    # Strict error handling:
    try:
    df_align(
            "financials.xlsx",
            df_payments,
            match_method="exact",
            column_match={"PaymentID": "Transaction ID"},
            on_multiple_matches="error"
        )
    except MultipleMatchError as e:
        print(f"Update aborted: {e}")

    """
    with DataFrameAlignExcel(fpath, df) as updater:
        result = updater.update_values(
            sheet_name=sheet_name,
            header_row=header_row,
            column_match=column_match,
            column_mapping=column_mapping,
            clean_keys=clean_keys,
            match_method=match_method,
            update_strategy=update_strategy,
            create_missing_columns=create_missing_columns,
            preview_only=preview_only,
            show_progress=show_progress,
            skip_no_match=skip_no_match,
            make_backup=make_backup,
            save_path=save_path,
            verbose=verbose,
            on_multiple_matches=on_multiple_matches,
            concat_separator=concat_separator
        )
        if verbose:
            print(updater.get_log())
        return result

def set_sheet_visible(
    fpath: str,
    sheet_name: Union[int, str, None, list] = None,
    show: Union[bool, str] = True,
    exclude: Union[List[Union[str, int]], None] = None,
    delete: Union[List[Union[str, int]], None] = None,
    verbose: bool = False,
) -> None:
    """
    Modify sheet visibility and order in an Excel workbook.

    Args:
        fpath (str): Path to the Excel workbook.
        sheet_name (int | str | None | list): Index(es) or name(s) of the sheet(s) to apply visibility/order to.
                                              If None, all sheets are considered.
        show (bool | str): Visibility mode. Can be:
                           - True -> visible
                           - False -> veryHidden
                           - 'visible', 'hidden', 'veryHidden' as string
        exclude (list[str | int] | None): List of sheet names or indices to exclude from changes. 
        delete (str | int | list[str | int] | None): Sheet(s) to delete by name or index.
        verbose (bool): If True, logs actions.
    """
    try:
        wb = fload(fpath, output="bit", get_validations=1)
    except Exception as e:
        raise FileNotFoundError(f"Unable to load workbook: {e}")

    sheet_names = wb.sheetnames
    if all([verbose,delete is not None]):
        print("Workbook loaded with sheets:")
        for i, name in enumerate(sheet_names):
            print(f"  [{i}] {name}")
    # --------------------------------------
    #  Delete specified sheets
    # --------------------------------------
    if delete is not None:
        delete_list = delete if isinstance(delete, list) else [delete]
        for del_item in delete_list:
            if isinstance(del_item, int):
                if 0 <= del_item < len(sheet_names):
                    title = sheet_names[del_item]
                else:
                    raise IndexError(f"delete index {del_item} out of range (0~{len(sheet_names)-1})")
            elif isinstance(del_item, str):
                title, idx = strcmp(del_item, sheet_names)
                if idx == -1:
                    raise ValueError(f"Sheet '{del_item}' not found to delete.")
            else:
                raise TypeError(f"Unsupported delete type: {type(del_item)}")

            if len(wb.sheetnames) <= 1:
                raise ValueError(f"Cannot delete the last remaining sheet: '{title}'")
            wb.remove(wb[title])
            if verbose:
                print(f"Deleted sheet: '{title}'")

        # Refresh sheet names after deletion
        sheet_names = wb.sheetnames
        if verbose:
            print("Workbook loaded with sheets (updated):")
            for i, name in enumerate(sheet_names):
                print(f"  [{i}] {name}")

    # Normalize exclude
    excludes = []
    if exclude is None:
        exclude = []
    if not isinstance(exclude, list):
        exclude = [exclude] 
    for exclude_ in exclude:
        if isinstance(exclude_, str):
            excludes.append(strcmp(exclude_, sheet_names)[0])
        elif isinstance(exclude_, int):
            if 0 <= exclude_ < len(sheet_names):
                excludes.append(sheet_names[exclude_])
            else:
                raise IndexError(
                    f"exclude index {exclude_} out of range (0~{len(sheet_names)-1})"
                )

    # Normalize and resolve sheet_name(s)
    target_indices = []
    sheet_name_list = sheet_name if isinstance(sheet_name, list) else [sheet_name] 
    for sheet_name_ in sheet_name_list: 
        if sheet_name_ is None:
            target_indices = list(range(len(sheet_names)))
            break
        elif isinstance(sheet_name_, int):
            if 0 <= sheet_name_ < len(sheet_names):
                target_indices.append(sheet_name_)
            else:
                raise IndexError(
                    f"sheet_name index {sheet_name_} is out of range (0~{len(sheet_names)-1})"
                )
        elif isinstance(sheet_name_, str):
            idx = strcmp(sheet_name_, sheet_names)[1]
            if idx == -1:
                raise ValueError(f"Sheet '{sheet_name_}' not found.")
            target_indices.append(idx)

    # Map show argument to valid state
    valid_states = ["veryHidden", "visible", "hidden"]
    if isinstance(show, str):
        if show not in valid_states:
            raise ValueError(
                f"Invalid show value '{show}'. Must be one of {valid_states}"
            )
        state = show
    else:
        state = "visible" if show else "veryHidden"

    # Modify sheet visibility
    for idx in target_indices:
        ws = wb[sheet_names[idx]]
        if ws.title in excludes:
            if verbose:
                print(f"Skipping excluded sheet: '{ws.title}'")
            continue
        ws.sheet_state = state

    # Ensure at least one sheet is visible
    visible_sheets = [s for s in wb.worksheets if s.sheet_state == "visible"]
    if not visible_sheets:
        fallback_sheet = wb.worksheets[0]
        fallback_sheet.sheet_state = "visible"
        if verbose:
            print(
                f"No visible sheets found. Setting '{fallback_sheet.title}' to visible."
            )

    # Reorder sheets according to sheet_name order
    try:
        ordered_titles = []
        for idx in target_indices:
            title = sheet_names[idx]
            if title not in excludes:
                ordered_titles.append(title)

        # Move each target sheet to its new position in order
        if verbose:
            print(f"\n{'*'*5} SHEET NAME {'*'*5} ")
        for i, title in enumerate(ordered_titles):
            sheet = wb[title]
            wb._sheets.remove(sheet)
            wb._sheets.insert(i, sheet)
            if verbose:
                print(f"\t[{i}] : '{title}'")
        if verbose:
            print(f"{'*'*5} SHEET NAME {'*'*5} \n")
    except Exception as e:
        raise RuntimeError(f"Failed to reorder sheets: {e}")

    if verbose:
        print(f"Final sheet order: {[s.title for s in wb.worksheets]}")

    try:
        wb.save(fpath)
    except Exception as e:
        raise IOError(f"Error saving workbook: {e}")
    
# ! =========(Above) interact with worrkbook and DataFrame===========
def wb_sheet_visible(
    fpath: str,
    sheet_name: Union[int, str, None, list] = 1,
    show: Union[bool, str] = True,
    exclude: Union[List[Union[str, int]], None] = None,
    verbose: bool = False,
) -> None:
    """
    Modify sheet visibility and order in an Excel workbook.

    Args:
        fpath (str): Path to the Excel workbook.
        sheet_name (int | str | None | list): Index(es) or name(s) of the sheet(s) to apply visibility/order to.
                                              If None, all sheets are considered.
        show (bool | str): Visibility mode. Can be:
                           - True -> visible
                           - False -> veryHidden
                           - 'visible', 'hidden', 'veryHidden' as string
        exclude (list[str | int] | None): List of sheet names or indices to exclude from changes.
        verbose (bool): If True, logs actions.
    """
    try:
        wb = fload(fpath, output="bit", get_validations=1)
    except Exception as e:
        raise FileNotFoundError(f"Unable to load workbook: {e}")

    sheet_names = wb.sheetnames
    if verbose:
        print("Workbook loaded with sheets:")
        for i, name in enumerate(sheet_names):
            print(f"  [{i}] {name}")

    # Normalize exclude
    excludes = []
    if exclude is None:
        exclude = []
    if not isinstance(exclude, list):
        exclude = [exclude]
    for exclude_ in exclude:
        if isinstance(exclude_, str):
            excludes.append(strcmp(exclude_, sheet_names)[0])
        elif isinstance(exclude_, int):
            if 0 <= exclude_ < len(sheet_names):
                excludes.append(sheet_names[exclude_])
            else:
                raise IndexError(
                    f"exclude index {exclude_} out of range (0~{len(sheet_names)-1})"
                )

    # Normalize and resolve sheet_name(s)
    target_indices = []
    sheet_name_list = sheet_name if isinstance(sheet_name, list) else [sheet_name]
    for sheet_name_ in sheet_name_list:
        if sheet_name_ is None:
            target_indices = list(range(len(sheet_names)))
            break
        elif isinstance(sheet_name_, int):
            if 0 <= sheet_name_ < len(sheet_names):
                target_indices.append(sheet_name_)
            else:
                raise IndexError(
                    f"sheet_name index {sheet_name_} is out of range (0~{len(sheet_names)-1})"
                )
        elif isinstance(sheet_name_, str):
            idx = strcmp(sheet_name_, sheet_names)[1]
            if idx == -1:
                raise ValueError(f"Sheet '{sheet_name_}' not found.")
            target_indices.append(idx)

    # Map show argument to valid state
    valid_states = ["veryHidden", "visible", "hidden"]
    if isinstance(show, str):
        if show not in valid_states:
            raise ValueError(
                f"Invalid show value '{show}'. Must be one of {valid_states}"
            )
        state = show
    else:
        state = "visible" if show else "veryHidden"

    # Modify sheet visibility
    for idx in target_indices:
        ws = wb[sheet_names[idx]]
        if ws.title in excludes:
            if verbose:
                print(f"Skipping excluded sheet: '{ws.title}'")
            continue
        ws.sheet_state = state

    # Ensure at least one sheet is visible
    visible_sheets = [s for s in wb.worksheets if s.sheet_state == "visible"]
    if not visible_sheets:
        fallback_sheet = wb.worksheets[0]
        fallback_sheet.sheet_state = "visible"
        if verbose:
            print(
                f"No visible sheets found. Setting '{fallback_sheet.title}' to visible."
            )

    # Reorder sheets according to sheet_name order
    try:
        ordered_titles = []
        for idx in target_indices:
            title = sheet_names[idx]
            if title not in excludes:
                ordered_titles.append(title)

        # Move each target sheet to its new position in order
        for i, title in enumerate(ordered_titles):
            sheet = wb[title]
            wb._sheets.remove(sheet)
            wb._sheets.insert(i, sheet)
            if verbose:
                print(f"Moved sheet '{title}' to position {i}")
    except Exception as e:
        raise RuntimeError(f"Failed to reorder sheets: {e}")

    if verbose:
        print(f"Final sheet order: {[s.title for s in wb.worksheets]}")

    try:
        wb.save(fpath)
    except Exception as e:
        raise IOError(f"Error saving workbook: {e}")

def format_excel(
    df: pd.DataFrame=None,
    filename:str=None,
    sheet_name:Union[str, int]=0,
    insert_img:dict=None,# {"A1":img_path}
    usage:bool=False,
    text_color:Union[dict,bool]=False, # dict: set the text color
    bg_color:Union[dict,bool]=False, # dict: set the back_ground color
    cell:Union[dict, list]=None,  # dict: or list for multiple locs setting:
    width:Union[bool, dict]=None,  # dict
    width_factor:int=1,# width * width_factor
    width_padding:int=2,# width + width_padding
    width_max=None,
    height:Union[bool, dict]=None,  # dict e.g., {2: 50, 3: 25},  keys are columns
    height_factor:int=1,
    height_padding:int=2,
    height_max=None,
    merge:tuple=None,  # tuple e.g.,  (slice(0, 1), slice(1, 3)),
    shade:Union[dict, list]=None,  # dict
    comment:Union[dict, list]=None,  # dict e.g., {(2, 4): "This is a comment"},
    comment_always_visible:bool=True,# always display comment
    link:Union[dict, list]=None,  # dict e.g., {(2, 2): "https://example.com"},
    protect:dict=None,  # dict, protect sheet
    protect_file:dict={},
    number_format:dict=None,  # dict: e.g., {1:"0.00", 2:"#,##0",3:"0%",4:"$#,##0.00"}
    data_validation=None,  # dict
    template:dict={},# e.g., template=dict(path="xx.xlsx",sheet_name=['sheet_name1',"sheet_name2"])
    apply_filter:bool=False, # add filter 
    freeze :str= False,#"A2",
    conditional_format:dict=None,  # dict
    verbose:bool=False,
    **kwargs,
):
    """
    Parameters:
        df : pandas.DataFrame, optional
            DataFrame to be written to the Excel file.
        filename : str, optional
            Path to the output Excel file.
        sheet_name : str or int, default 0
            Name or index of the sheet where data will be written.
        insert_img : dict, optional
            Dictionary specifying image insert locations, e.g., {"A1": "path/to/image.png"}.
        usage : bool, default False
            If True, display usage examples.
        cell : dict or list, optional
            Specifies cell formatting options.
        width : dict, optional
            Dictionary specifying column widths, e.g., {1: 20, 2: 30}.
        width_factor : int, default 2
            Additional factor to adjust column width dynamically.
        height : dict, optional
            Dictionary specifying row heights, e.g., {2: 50, 3: 25}.
        height_max : int, default 25
            Maximum row height allowed.
        merge : tuple, optional
            Specifies cell merging, e.g., (slice(0, 1), slice(1, 3)).
        shade : dict, optional
            Dictionary defining cell shading/styling.
        comment : dict, optional
            Dictionary adding comments, e.g., {(2, 4): "This is a comment"}.
        comment_always_visible : bool, default True
            Whether comments should always be visible.
        link : dict, optional
            Dictionary specifying hyperlinks, e.g., {(2, 2): "https://example.com"}.
        protect : dict, optional
            Dictionary defining cell protection settings.
        number_format : dict, optional
            Dictionary specifying number formats, e.g., {1: "0.00", 2: "#,##0"}.
        data_validation : dict, optional
            Dictionary setting data validation rules.
        apply_filter : bool, default True
            Whether to apply filters to the header row.
        freeze : str, optional
            Cell reference (e.g., "A2") to freeze rows/columns.
        conditional_format : dict, optional
            Dictionary defining conditional formatting rules.
        verbose : bool, default False
            Whether to print detailed execution logs.
        **kwargs : dict
            Additional parameters for advanced customization. 
    """

    usage_str="""
        Formats an Excel file with various styling options.
        Usage:
        fsave(
                dir_save,
                fload(dir_save, output="bit", sheet_name=sheet_name),
                sheet_name=sheet_name,
                if_sheet_exists="overlay",
                mode="a",
                width_factor=0,
                height={1: 50},
                cell=[
                    {
                        (slice(0, 1), slice(0, df_exists.shape[1])): {
                            "fill": {
                                "start_color": "61AFEF",  # Starting color
                                "end_color": "61AFEF",  # Ending color (useful for gradients)
                                "fill_type": "solid",  # Fill type (solid, gradient, etc.)
                            },
                            "font": {
                                "name": "Arial",  # Font name
                                "size": 11,  # Font size
                                "bold": True,  # Bold text
                                "italic": False,  # Italic text
                                # "underline": "single",  # Underline (single, double)
                                "color": "#000000",  # Font color
                            },
                            "alignment": {
                                "horizontal": "center",  # Horizontal alignment (left, center, right)
                                "vertical": "center",  # Vertical alignment (top, center, bottom)
                                "wrap_text": True,  # Wrap text in the cell
                                "shrink_to_fit": True,  # Shrink text to fit within cell
                                "text_rotation": 0,  # Text rotation angle
                            },
                        }
                    },
                    {
                        (
                            slice(0, df_exists.shape[0]),
                            slice(0, df_exists.shape[1]),
                        ): {
                            "alignment": {
                                "horizontal": "center",  # Horizontal alignment (left, center, right)
                                "vertical": "center",  # Vertical alignment (top, center, bottom)
                                "wrap_text": True,  # Wrap text in the cell
                                "shrink_to_fit": True,  # Shrink text to fit within cell
                                "text_rotation": 0,  # Text rotation angle
                            },
                        }
                    },
                    {
                        (slice(0, df_exists.shape[0]), slice(2, 3)): {
                            "alignment": {
                                "horizontal": "left",  # Horizontal alignment (left, center, right)
                            },
                        }
                    },
                    {
                        (slice(0, df_exists.shape[0]), slice(7, 8)): {
                            "alignment": {
                                "horizontal": "left",  # Horizontal alignment (left, center, right)
                            },
                        }
                    },
                ],
                password=False,  # depass("ogB3B7y3xR9iuH4QIQbyy6VXG14I0A8DlsTxyiGqg1U="),
            )
            """
    if verbose:
        print(usage_str)
    from datetime import datetime
    import openpyxl
    from openpyxl import load_workbook,Workbook
    from openpyxl.worksheet.worksheet import Worksheet

    from openpyxl.styles import Font, PatternFill, Alignment, Border, Side
    from openpyxl.utils import get_column_letter
    from openpyxl.worksheet.datavalidation import DataValidation
    from openpyxl.comments import Comment
    from openpyxl.formatting.rule import ColorScaleRule, DataBarRule, IconSetRule,IconSet
    from openpyxl.workbook.protection import WorkbookProtection

    def _clean_excel_str(s):
        if isinstance(s, str):
            return re.sub(r'[\x00-\x1F\x7F]', '', s)  # remove control chars
        return s
    def _escape_excel_formula(cell):
        if isinstance(cell, str) and cell.startswith(('=', '+', '-')):
            return "'" + cell  # add single quote to escape
        return cell 

    def convert_indices_to_range(row_slice, col_slice):
        """Convert numerical row and column slices to Excel-style range strings."""
        start_row = row_slice.start + 1
        end_row = row_slice.stop if row_slice.stop is not None else None
        start_col = col_slice.start + 1
        end_col = col_slice.stop if col_slice.stop is not None else None

        start_col_letter = get_column_letter(start_col)
        end_col_letter = get_column_letter(end_col) if end_col else None
        return (
            f"{start_col_letter}{start_row}:{end_col_letter}{end_row}"
            if end_col_letter
            else f"{start_col_letter}{start_row}"
        )


    def is_merged_cell(ws, cell):
        """Check if a cell is part of any merged range."""
        for merged_range in ws.merged_cells.ranges:
            if cell.coordinate in merged_range:
                return True
        return False

    def apply_auto_width(ws, width_factor=1.2, width_padding=2, width_max=50):
        """
        Automatically adjust column widths based on content length,
        with complete protection against merged cell errors.
        
        Args:
            ws: Worksheet object
            width_factor: Multiplier for content length (default 1.2)
            width_padding: Additional padding (default 2)
            width_max: Maximum column width (default 50)
        """
        # First build a set of all merged cell coordinates
        merged_coords = set()
        for merged_range in ws.merged_cells.ranges:
            for row in ws.iter_rows(min_row=merged_range.min_row,
                                max_row=merged_range.max_row,
                                min_col=merged_range.min_col,
                                max_col=merged_range.max_col):
                for cell in row:
                    merged_coords.add(cell.coordinate)
        
        for col in ws.columns:
            if not col:
                continue
                
            col_letter = get_column_letter(col[0].column)
            max_length = 0
            
            for cell in col:
                # Skip merged cells entirely
                if cell.coordinate in merged_coords:
                    continue
                    
                try:
                    if cell.value is not None:
                        # Handle both single-line and multi-line content
                        cell_value = str(cell.value)
                        lines = cell_value.split('\n')
                        current_max = max(len(line) for line in lines)
                        max_length = max(max_length, current_max)
                except Exception as e:
                    print(f"Skipping cell {cell.coordinate} due to error: {e}")
                    continue
            
            # Calculate width with constraints
            adjusted_width = min(
                max(1, (max_length * width_factor) + width_padding),
                width_max if width_max is not None else float('inf')
            )
            
            ws.column_dimensions[col_letter].width = adjusted_width

    def apply_color_to_worksheet(ws=None, sheet_name=None, conditions=None, cell_idx=None,where="text"):
        """
        Apply text color formatting to a specific cell range in an openpyxl workbook based on conditions.

        Parameters:
        ws : worrksheet
            The openpyxl workbook object to style.
        sheet_name : str
            The name of the sheet to style.
        conditions : dict
            Dictionary defining conditions for text or background coloring.
                Example:
                {
                    ">10": "#FF0000",           # Red if value is greater than 10
                    "contains:Error": "#FFFF00", # Yellow if text contains 'Error'
                    "startswith:Warn": "#FFA500" # Orange if text starts with 'Warn'
                }
        cell_idx : tuple, optional
            A tuple of slices defining the selected row and column range (only for DataFrame).
        where : str, default="text"
            "text" -> Apply color to text, "bg" -> Apply color to background.

        Returns:
        Workbook
            The workbook with applied formatting.
        """ 
        def evaluate_condition(value, condition):
            """Evaluate the condition dynamically."""
            if not isinstance(conditions, dict):
                raise ValueError(f"conditionÂøÖÈ°ªÊòØdictÊ†ºÂºè:e.g., {'x>=20':'#DD0531', 'startswith:Available':'#DD0531'}")
            try:
                if "x" in condition and re.search(r"[<>=!]=*", condition):
                    expr = condition.replace("x", str(value))
                    return eval(expr)
                elif condition.startswith("startswith:") or condition.startswith("startwith:"):
                    return value.startswith(condition.split(":", 1)[1])
                elif condition.startswith("endswith:") or condition.startswith("endwith:"):
                    return value.endswith(condition.split(":", 1)[1])
                elif condition.startswith("contains:") or condition.startswith("contain:") or condition.startswith("include:"):
                    return condition.split(":", 1)[1] in value
                elif condition.startswith("matches:") or condition.startswith("match:"):
                    return re.search(condition.split(":", 1)[1], value) is not None 
                else:
                    expr = condition
                return False
            except Exception as e: 
                return False

        def apply_condition_to_cell_text_color(cell, value):
            """Apply color to a cell if it matches any condition."""
            for condition, color in conditions.items():
                if evaluate_condition(value, condition):
                    # Apply color to font
                    cell.font = openpyxl.styles.Font(
                        color=openpyxl.styles.Color(rgb=hex2argb(color))
                    )
                    return
        def apply_condition_to_cell_bg_color(cell, value):
            """Apply background color to a cell if it matches any condition."""
            for condition, color in conditions.items():
                if evaluate_condition(value, condition):
                    if not isinstance(color,list):
                        color=[color]
                    if len(color)==1:
                        cell.fill = PatternFill(
                            start_color=hex2argb(color[0]),
                            end_color=hex2argb(color[0]),
                            fill_type="solid"
                        )
                    elif len(color)==2:
                        cell.fill = PatternFill(
                            start_color=hex2argb(color[0]),
                            end_color=hex2argb(color[1]),
                            fill_type="solid"
                        )
                    return
        if isinstance(cell_idx, tuple):
            # If cell_idx is provided, select a range based on the slice
            row_slice, col_slice = cell_idx
            rows = list(
                ws.iter_rows(
                    min_row=row_slice.start + 1,
                    max_row=row_slice.stop,
                    min_col=col_slice.start + 1,
                    max_col=col_slice.stop,
                )
            )
            for row in rows:
                for cell in row:
                    if where=="text":
                        apply_condition_to_cell_text_color(cell, cell.value)
                    elif where=="bg":
                        apply_condition_to_cell_bg_color(cell, cell.value)
        else:
            # If no cell_idx is provided, apply to all cells
            for row in ws.iter_rows():
                for cell in row:
                    if where=="text":
                        apply_condition_to_cell_text_color(cell, cell.value)
                    elif where=="bg":
                        apply_condition_to_cell_bg_color(cell,cell.value)
        return ws
    
    def apply_format(ws, cell, cell_range):
        """Apply cell formatting to a specified range."""
        # Get all merged cell coordinates first
        merged_cells = set()
        for merged_range in ws.merged_cells.ranges:
            for coord in merged_range.cells:
                merged_cells.add(coord)
        cell_font, cell_fill, cell_alignment, border = None, None, None, None
        kws_cell = ["font", "fill", "alignment", "border"]
        for K, _ in cell.items():
            if strcmp(K, kws_cell)[0] == "font":
                #! font
                font_color = "000000"
                font_name = "Arial"
                font_underline = "none"
                font_size = 11
                font_bold = False
                font_strike = False
                font_italic = False
                kws_font = [
                    "name",
                    "size",
                    "bold",
                    "underline",
                    "color",
                    "strike",
                    "italic",
                ]
                for k_, v_ in cell.get(K, {}).items():
                    if strcmp(k_, kws_font)[0] == "name":
                        font_name = v_
                    elif strcmp(k_, kws_font)[0] == "size":
                        font_size = v_
                    elif strcmp(k_, kws_font)[0] == "bold":
                        font_bold = v_
                    elif strcmp(k_, kws_font)[0] == "underline":
                        font_underline = strcmp(v_, ["none", "single", "double"])[0]
                    elif strcmp(k_, kws_font)[0] == "color":
                        font_color = hex2argb(v_)
                    elif strcmp(k_, kws_font)[0] == "strike":
                        font_strike = v_
                    elif strcmp(k_, kws_font)[0] == "italic":
                        font_italic = v_

                cell_font = Font(
                    name=font_name,
                    size=font_size,
                    bold=font_bold,
                    italic=font_italic,
                    underline=font_underline,
                    strike=font_strike,
                    color=font_color,
                )

            if strcmp(K, kws_cell)[0] == "fill":
                #! fill
                kws_fill = ["start_color", "end_color", "fill_type", "color"]
                kws_fill_type = [
                    "darkVertical",
                    "lightDown",
                    "lightGrid",
                    "solid",
                    "darkDown",
                    "lightGray",
                    "lightUp",
                    "gray0625",
                    "lightVertical",
                    "lightHorizontal",
                    "darkHorizontal",
                    "gray125",
                    "darkUp",
                    "mediumGray",
                    "darkTrellis",
                    "darkGray",
                    "lightTrellis",
                    "darkGrid",
                ]
                start_color, end_color, fill_type = (
                    "FFFFFF",
                    "FFFFFF",
                    "solid",
                )  # default
                for k, v in cell.get(K, {}).items():
                    if strcmp(k, kws_fill)[0] == "color":
                        start_color, end_color = hex2argb(v), hex2argb(v)
                        break
                for k, v in cell.get(K, {}).items():
                    if strcmp(k, kws_fill)[0] == "start_color":
                        start_color = hex2argb(v)
                    elif strcmp(k, kws_fill)[0] == "end_color":
                        end_color = hex2argb(v)
                    elif strcmp(k, kws_fill)[0] == "fill_type":
                        fill_type = strcmp(v, kws_fill_type)[0]
                cell_fill = PatternFill(
                    start_color=start_color,
                    end_color=end_color,
                    fill_type=fill_type,
                )

            if strcmp(K, kws_cell)[0] == "alignment":
                #! alignment
                # default
                align_horizontal = "general"
                align_vertical = "center"
                align_rot = 0
                align_wrap = False
                align_shrink = False
                align_indent = 0
                kws_align = [
                    "horizontal",
                    "ha",
                    "vertical",
                    "va",
                    "text_rotation",
                    "rotat",
                    "rot",
                    "wrap_text",
                    "wrap",
                    "shrink_to_fit",
                    "shrink",
                    "indent",
                ]
                for k, v in cell.get(K, {}).items():
                    if strcmp(k, kws_align)[0] in ["horizontal", "ha"]:
                        align_horizontal = strcmp(
                            v, ["general", "left", "right", "center"]
                        )[0]
                    elif strcmp(k, kws_align)[0] in ["vertical", "va"]:
                        align_vertical = strcmp(v, ["top", "center", "bottom"])[0]
                    elif strcmp(k, kws_align)[0] in ["text_rotation", "rotat", "rot"]:
                        align_rot = v
                    elif strcmp(k, kws_align)[0] in ["wrap_text", "wrap"]:
                        align_wrap = v
                    elif strcmp(k, kws_align)[0] in [
                        "shrink_to_fit",
                        "shrink",
                        "wrap_text",
                        "wrap",
                    ]:
                        align_shrink = v
                    elif strcmp(k, kws_align)[0] in ["indent"]:
                        align_indent = v
                cell_alignment = Alignment(
                    horizontal=align_horizontal,
                    vertical=align_vertical,
                    text_rotation=align_rot,
                    wrap_text=align_wrap,
                    shrink_to_fit=align_shrink,
                    indent=align_indent,
                )

            if strcmp(K, kws_cell)[0] == "border":
                #! border
                kws_border = [
                    "color_left",
                    "color_l",
                    "color_right",
                    "color_r",
                    "color_top",
                    "color_t",
                    "color_bottom",
                    "color_b",
                    "color_diagonal",
                    "color_d",
                    "color_outline",
                    "color_o",
                    "color_vertical",
                    "color_v",
                    "color_horizontal",
                    "color_h",
                    "color",
                    "style_left",
                    "style_l",
                    "style_right",
                    "style_r",
                    "style_top",
                    "style_t",
                    "style_bottom",
                    "style_b",
                    "style_diagonal",
                    "style_d",
                    "style_outline",
                    "style_o",
                    "style_vertical",
                    "style_v",
                    "style_horizontal",
                    "style_h",
                    "style",
                ]
                # * border color
                border_color_l, border_color_r, border_color_t, border_color_b = (
                    "FF000000",
                    "FF000000",
                    "FF000000",
                    "FF000000",
                )
                border_color_d, border_color_o, border_color_v, border_color_h = (
                    "FF000000",
                    "FF000000",
                    "FF000000",
                    "FF000000",
                )
                # get colors config
                for k, v in cell.get(K, {}).items():
                    print(k, v,strcmp(k, kws_border)[0])
                    if strcmp(k, kws_border)[0] in ["color"]:
                        border_color_all = hex2argb(v)
                        # Â¶ÇÊûúËÆæÁΩÆ‰∫Ücolor,Ë°®Á§∫ÂÖ∂ÂÆÉÁöÑÊâÄÊúâÁöÑÈÉΩËÆæÁΩÆÊàê‰∏∫‰∏ÄÊ†∑ÁöÑ
                        # ÁÑ∂ÂêéÂÜçÊâçÂºÄÂßãËá™Â∑±ÂÆö‰πâÂÖ∂ÂÆÉÁöÑcolor
                        (
                            border_color_l,
                            border_color_r,
                            border_color_t,
                            border_color_b,
                        ) = (
                            border_color_all,
                            border_color_all,
                            border_color_all,
                            border_color_all,
                        )
                        (
                            border_color_d,
                            border_color_o,
                            border_color_v,
                            border_color_h,
                        ) = (
                            border_color_all,
                            border_color_all,
                            border_color_all,
                            border_color_all,
                        )
                    elif strcmp(k, kws_border)[0] in ["color_left", "color_l"]:
                        border_color_l = hex2argb(v)
                    elif strcmp(k, kws_border)[0] in ["color_right", "color_r"]:
                        border_color_r = hex2argb(v)
                    elif strcmp(k, kws_border)[0] in ["color_top", "color_t"]:
                        border_color_t = hex2argb(v)
                    elif strcmp(k, kws_border)[0] in ["color_bottom", "color_b"]:
                        border_color_b = hex2argb(v)
                    elif strcmp(k, kws_border)[0] in ["color_diagonal", "color_d"]:
                        border_color_d = hex2argb(v)
                    elif strcmp(k, kws_border)[0] in ["color_outline", "color_o"]:
                        border_color_o = hex2argb(v)
                    elif strcmp(k, kws_border)[0] in ["color_vertical", "color_v"]:
                        border_color_v = hex2argb(v)
                    elif strcmp(k, kws_border)[0] in ["color_horizontal", "color_h"]:
                        border_color_h = hex2argb(v)
                # *border style
                border_styles = [
                    "thin",
                    "medium",
                    "thick",
                    "dotted",
                    "dashed",
                    "hair",
                    "mediumDashed",
                    "dashDot",
                    "dashDotDot",
                    "slantDashDot",
                    "none",
                ]
                border_style_l, border_style_r, border_style_t, border_style_b = (
                    None,
                    None,
                    None,
                    None,
                )
                border_style_d, border_style_o, border_style_v, border_style_h = (
                    None,
                    None,
                    None,
                    None,
                )
                # get styles config
                for k, v in cell.get(K, {}).items():
                    # if not "style" in k:
                    #     break
                    if strcmp(k, kws_border)[0] in ["style"]:
                        border_style_all = strcmp(v, border_styles)[0]
                        # Â¶ÇÊûúËÆæÁΩÆ‰∫Üstyle,Ë°®Á§∫ÂÖ∂ÂÆÉÁöÑÊâÄÊúâÁöÑÈÉΩËÆæÁΩÆÊàê‰∏∫‰∏ÄÊ†∑ÁöÑ
                        # ÁÑ∂ÂêéÂÜçÊâçÂºÄÂßãËá™Â∑±ÂÆö‰πâÂÖ∂ÂÆÉÁöÑstyle
                        (
                            border_style_l,
                            border_style_r,
                            border_style_t,
                            border_style_b,
                        ) = (
                            border_style_all,
                            border_style_all,
                            border_style_all,
                            border_style_all,
                        )
                        (
                            border_style_d,
                            border_style_o,
                            border_style_v,
                            border_style_h,
                        ) = (
                            border_style_all,
                            border_style_all,
                            border_style_all,
                            border_style_all,
                        )
                    elif strcmp(k, kws_border)[0] in ["style_left", "style_l"]:
                        border_style_l = strcmp(v, border_styles)[0]
                    elif strcmp(k, kws_border)[0] in ["style_right", "style_r"]:
                        border_style_r = strcmp(v, border_styles)[0]
                    elif strcmp(k, kws_border)[0] in ["style_top", "style_t"]:
                        border_style_t = strcmp(v, border_styles)[0]
                    elif strcmp(k, kws_border)[0] in ["style_bottom", "style_b"]:
                        border_style_b = strcmp(v, border_styles)[0]
                    elif strcmp(k, kws_border)[0] in ["style_diagonal", "style_d"]:
                        border_style_d = strcmp(v, border_styles)[0]
                    elif strcmp(k, kws_border)[0] in ["style_outline", "style_o"]:
                        border_style_o = strcmp(v, border_styles)[0]
                    elif strcmp(k, kws_border)[0] in ["style_vertical", "style_v"]:
                        border_style_v = strcmp(v, border_styles)[0]
                    elif strcmp(k, kws_border)[0] in ["style_horizontal", "style_h"]:
                        border_style_h = strcmp(v, border_styles)[0]
                # * apply border config
                border = Border(
                    left=Side(border_style=border_style_l, color=border_color_l),
                    right=Side(border_style=border_style_r, color=border_color_r),
                    top=Side(border_style=border_style_t, color=border_color_t),
                    bottom=Side(border_style=border_style_b, color=border_color_b),
                    diagonal=Side(border_style=border_style_d, color=border_color_d),
                    diagonal_direction=0,
                    outline=Side(border_style=border_style_o, color=border_color_o),
                    vertical=Side(border_style=border_style_v, color=border_color_v),
                    horizontal=Side(border_style=border_style_h, color=border_color_h),
                )

        #! final apply configs
        for row in ws[cell_range]:
            for cell_ in row:
                if cell_.coordinate in merged_cells:
                    continue  # Skip merged cells
                if cell_font:
                    cell_.font = cell_font
                if cell_fill:
                    cell_.fill = cell_fill
                if cell_alignment:
                    cell_.alignment = cell_alignment
                if border:
                    cell_.border = border
    def generate_unique_sheet_name(wb, sheet_name):
        """Generate a unique sheet name if the given name already exists in the workbook."""
        if sheet_name not in wb.sheetnames:
            return sheet_name
        counter = 1
        unique_name = f"{sheet_name}_{counter}"
        while unique_name in wb.sheetnames:
            counter += 1
            unique_name = f"{sheet_name}_{counter}"
        return unique_name


    # if it is already worksheet format
    if isinstance(df, pd.DataFrame):
        pass
    elif isinstance(df, Worksheet) or isinstance(df, Workbook):
        pass
    elif df is None:
        if any(filename):
            df = fload(filename, output="bit")
    else:
        try:
            print(f"is loading file {os.path.basename(df)}")
            df = fload(df)
        except Exception as e:
            print(e) 
    if isinstance(df, tuple):
        df, original_validations=df
    if filename is None:
        filename = str(datetime.now().strftime("%y%m%d_%H.xlsx"))
    
    kwargs.pop("format", None)  # Êõ¥Â•ΩÂú∞Ë∑üfsaveÁªìÂêà‰ΩøÁî®
    kwargs.pop("sheet_name", 0)  # Êõ¥Â•ΩÂú∞Ë∑üdf.to_excelÁªìÂêà‰ΩøÁî®
    # Âè™ÊúâopenpyxlÊâçÊîØÊåÅ append
    mode = strcmp(kwargs.get("mode", "a"), ["a", "w","auto"])[0]
    # print(f'mode="{mode}"')
    kwargs.pop("mode", None)
    engine = strcmp(kwargs.get("engine", "openpyxl"), ["xlsxwriter", "openpyxl"])[0]
    # corr engine
    engine="openpyxl" if mode=="a" else "xlsxwriter" 
    # print(f'engine="{engine}"')
    if_sheet_exists=kwargs.get("if_sheet_exists","replace")
    # ÈÄöÂ∏∏ÊòØ‰∏çÈúÄË¶Å‰øùÂ≠òindexÁöÑ
    index = kwargs.get("index", False)
    # header https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_excel.html
    header=kwargs.pop("header",True) # bool or list of str, default True: Write out the column names. If a list of string is given it is assumed to be aliases for the column names.
    password = kwargs.pop("password", None)  # Use kwargs if provided
    
    kwargs.pop("password", None)
    kwargs.pop("header", None)
    kwargs.pop("index", None)
    kwargs.pop("if_sheet_exists", None)
 

    if isinstance(df, Workbook):
        """ÊâìÂºÄSheet_nameÊåáÂÆöÁöÑË°®Ê†ºÔºåÂ¶ÇÊûúËØ•Ë°®‰∏çÂ≠òÂú®ÔºåÂàôÂàõÂª∫‰∏Ä‰∏™Êñ∞ÁöÑÊàñ‰ªéÁé∞ÊúâÊñá‰ª∂‰∏≠Âä†ËΩΩÊï∞ÊçÆ"""
        wb = df
        
        # Check if sheet exists in input workbook
        try:
            if isinstance(sheet_name, int):
                ws = wb.worksheets[sheet_name]
            else:
                ws = wb[sheet_name]
        except Exception as e:
            # print(f'mode="{mode}"')
            if not os.path.exists(filename) or mode=="w":
                ws = wb.create_sheet(title=sheet_name) 
            else:# file exists
                wb = load_workbook(filename) 

                with pd.ExcelWriter(filename, mode="a", engine=engine, if_sheet_exists=if_sheet_exists) as writer:
                    for ws in wb.worksheets:
                        # Convert to DataFrame
                        data = list(ws.values)
                        df = pd.DataFrame(data)
                        try:
                            # Clean up NaNs in header
                            df.columns = df.iloc[0]
                            df = df[1:]
                            df.columns = df.columns.fillna("Unnamed")

                            # Optional: Sanitize cell values
                            df = df.applymap(lambda x: '' if pd.isnull(x) else x)

                            # Write to new sheet or overwrite
                            df.to_excel(writer, sheet_name=ws.title, index=False)
                            print(3)
                        except Exception as e:
                            print(e)

                # ÈáçÊñ∞ÊâìÂºÄÂàöÊõ¥Êñ∞ËøáÁöÑÊï∞ÊçÆ
                wb = load_workbook(filename) 
                    
        # Handle case where we need to write data to file
        if os.path.exists(filename) and mode != "w":
            try:
                # Backup all sheets
                sheet_data = {}
                for sheet in wb.worksheets:
                    data = list(sheet.values)
                    df = pd.DataFrame(data[1:], columns=data[0])
                    sheet_data[sheet.title] = df
                    
                # Write all sheets back to file
                with pd.ExcelWriter(filename, engine=engine, mode=mode,if_sheet_exists=if_sheet_exists) as writer:
                    for name, df in sheet_data.items():
                        df.to_excel(writer, sheet_name=name, index=index, header=header, **kwargs)
                        
                # ÈáçÊñ∞ÊâìÂºÄÂàöÊõ¥Êñ∞ËøáÁöÑÊï∞ÊçÆ
                wb = load_workbook(filename) 
                if sheet_name in wb.sheetnames:
                    ws = wb[sheet_name]
                    if not sheet_name==sheet_name:
                        wb.remove(wb[sheet_name]) 
                else:
                    raise KeyError(f"Worksheet {sheet_name} does not exist.") 
            except Exception as e:
                print(f"Error saving workbook: {e}")

    else:
        if not os.path.exists(filename) or mode=="w": # or overwrite
            try:
                df = df.applymap(_clean_excel_str)
            except Exception as e:
                print(f"trying to _clean_excel_str:{e}")
            try:
                df = df.applymap(_escape_excel_formula)
            except Exception as e:
                print(f"trying to _escape_excel_formula:{e}")
            try:
                if isinstance(df, pd.io.formats.style.Styler):
                    try:
                        df.to_excel(filename, sheet_name=sheet_name, index=False, engine="openpyxl")
                        print(f"in format_excel: styled file: {os.path.basename(filename)}")
                    except Exception as e: 
                        print(f"in format_excel: cannot save the styles, only save the raw data!,because: {e}")
                        with pd.ExcelWriter(filename, mode="w", engine=engine) as writer:
                            df.data.to_excel(writer, sheet_name=sheet_name, index=False)
                else:
                    with pd.ExcelWriter(filename, mode="w", engine=engine) as writer:
                        if isinstance(df, dict):
                            for name_, df_ in df.items():
                                df_.to_excel(writer, sheet_name=name_, index=index, header=header, **kwargs)
                        else:
                            df.to_excel(writer, sheet_name=sheet_name, index=index, header=header,**kwargs)
            except Exception as e:
                print(f"Failed to save Excel file: {e}")
            wb = load_workbook(filename)
            if sheet_name in wb.sheetnames:
                wb[sheet_name].sheet_state = 'visible'
            else:
                wb.active.sheet_state = 'visible'  # Fallback to the active sheet
            try:
                if isinstance(sheet_name, str):
                    ws = wb[sheet_name]
                elif isinstance(sheet_name, int):
                    ws = wb.worksheets[sheet_name]
                else:
                    ws = wb.worksheets[sheet_name]  # the index of worksheets
            except Exception as e: 
                ws = wb.create_sheet(title=sheet_name)
        else:# file exists
            wb = load_workbook(filename)
            with pd.ExcelWriter(filename, mode="a", engine=engine, if_sheet_exists=if_sheet_exists) as writer:
                if isinstance(df, dict):
                    for name_, df_ in df.items():
                        df_.to_excel(writer, sheet_name=name_, index=index, header=header, **kwargs)
                else:
                    df.to_excel(writer, sheet_name=sheet_name, index=index, header=header,**kwargs)
            wb = load_workbook(filename) 
            if sheet_name in wb.sheetnames:
                ws = wb[sheet_name]
                if not sheet_name==sheet_name:
                    wb.remove(wb[sheet_name]) 
            else:
                print(e)
                ws = wb.create_sheet(title=sheet_name) 
 
    # *  Copy Formatting when available
    if template:
        if isinstance(template, dict):
            path_source=template.get("path")
            sheet_name_source=template.get("sheet_name",sheet_name)
        else:
            path_source=template
            sheet_name_source=sheet_name
        if isinstance(sheet_name_source, (list,tuple)):
            pass
        else:
            if sheet_name_source!=sheet_name:
                sheet_name_source=list(sheet_name_source).append(sheet_name)
        try: 
            copy_format(path_source, wb, sheet_name=sheet_name_source, save=False,copy_values=False)
            print(f"Copied formatting from sheet_name '{sheet_name}'in file '{os.path.basename(path_source)}'")
        except Exception as e:
            print(e)
            copy_format(path_source, wb, sheet_name=[pd.ExcelFile(path_source).sheet_names[0],sheet_name], save=False,copy_values=False)
            print(f"Alternatively, Copied formatting from the 1st Sheet '{sheet_name}'in file '{os.path.basename(path_source)}'")
    # ! Apply Text color
    if text_color:
        if verbose:
            text_color_str="""
            text_color=[
                {
                    (slice(1, 2), slice(0, 3)): {
                        "x>20": "#DD0531",  # Numbers > 20 ‚Üí red
                        "x<=8": "#35B20C",  # Numbers ‚â§ 10 ‚Üí blue
                        "'x'!='available'": "#0510DD",  # 'available' ‚Üí green
                        "10<x<=30": "#EAB107",  # 10 < value ‚â§ 30 ‚Üí orange
                        "10<=x<30": "#C615BE",  # 10 ‚â§ value < 30 ‚Üí purple
                    }
                },
                {
                    (slice(3, df.shape[0] + 1), slice(0, 3)): {
                        "x>20": "#DD0531",  # Numbers > 20 ‚Üí red
                        "x<=10": "#35B20C",  # Numbers ‚â§ 10 ‚Üí blue
                        "'x'!='available'": "#0510DD",  # 'available' ‚Üí green
                        "10<x<=30": "#EAB107",  # 10 < value ‚â§ 30 ‚Üí orange
                        "10<=x<30": "#C615BE",  # 10 ‚â§ value < 30 ‚Üí purple
                    }
                },
            ],
            """
            print(text_color_str)
        if not isinstance(text_color, list):
            text_color=[text_color]
        for text_color_ in text_color:
            for indices, dict_text_conditions in text_color_.items():
                ws = apply_color_to_worksheet(ws, sheet_name=sheet_name, conditions=dict_text_conditions, cell_idx=indices,where="text")
    # ! Apply Text color
    if bg_color:
        if verbose:
            bg_color_str="""
            bg_color=[
                {
                    (slice(1, 2), slice(0, 3)): {
                        "x>20": ["#DD0531","#35B20C"],  # Numbers > 20 ‚Üí red
                        "x<=8": "#35B20C",  # Numbers ‚â§ 10 ‚Üí blue
                        "'x'!='available'": "#0510DD",  # 'available' ‚Üí green
                        "10<x<=30": "#EAB107",  # 10 < value ‚â§ 30 ‚Üí orange
                        "10<=x<30": "#C615BE",  # 10 ‚â§ value < 30 ‚Üí purple
                    }
                },
                {
                    (slice(3, df.shape[0] + 1), slice(0, 3)): {
                        "x>20": "#DD0531",  # Numbers > 20 ‚Üí red
                        "x<=10": "#35B20C",  # Numbers ‚â§ 10 ‚Üí blue
                        "'x'!='available'": "#0510DD",  # 'available' ‚Üí green
                        "10<x<=30": "#EAB107",  # 10 < value ‚â§ 30 ‚Üí orange
                        "10<=x<30": "#C615BE",  # 10 ‚â§ value < 30 ‚Üí purple
                    }
                },
            ],
            """
            print(bg_color_str)
        if not isinstance(bg_color, list):
            bg_color=[bg_color]
        for bg_color_ in bg_color:
            for indices, dict_text_conditions in bg_color_.items():
                ws = apply_color_to_worksheet(ws, sheet_name=sheet_name, conditions=dict_text_conditions, cell_idx=indices,where="bg")
    # !Apply cell formatting
    if cell:
        if not isinstance(cell, list):
            cell = [cell]
        for cell_ in cell:
            for indices, format_options in cell_.items():
                cell_range = convert_indices_to_range(*indices)
                apply_format(ws, format_options, cell_range)

        if verbose:
            cell_tmp="""cell=[
                    {
                        (slice(0, 1), slice(0, len(df.columns))): {
                            "font": {
                                "name": "Calibri",  # Font name
                                "size": 14,  # Font size
                                "bold": True,  # Bold text
                                "italic": False,  # Italic text
                                "underline": "single",  # Underline (single, double)
                                "color": "#FFFFFF",  # Font color
                            },
                            "fill": {
                                "start_color": "a1cd1e",  # Starting color
                                "end_color": "4F81BD",  # Ending color (useful for gradients)
                                "fill_type": "solid",  # Fill type (solid, gradient, etc.)
                            },
                            "alignment": {
                                "horizontal": "center",  # Horizontal alignment (left, center, right)
                                "vertical": "center",  # Vertical alignment (top, center, bottom)
                                "wrap_text": True,  # Wrap text in the cell
                                "shrink_to_fit": True,  # Shrink text to fit within cell
                                "text_rotation": 0,  # Text rotation angle
                            },
                            "border": {
                                "left": "thin",
                                "right": "thin",
                                "top": "thin",
                                "bottom": "thin",
                                "color": "000000",  # Border color
                            },
                        }
                    },{}]"""
            print(cell_tmp)
    # !Apply cell shading
    if shade:
        if not isinstance(shade, list):
            shade = [shade]
        for shade_ in shade:
            for indices, shading in shade_.items():
                cell_range = convert_indices_to_range(*indices)
                fill = PatternFill(
                    start_color=hex2argb(shading.get("bg_color", "FFFFFF")),
                    end_color=hex2argb(shading.get("end_color", "FFFFFF")),
                    fill_type=shading.get("fill_type", "solid"),
                    patternType=shading.get("pattern_type", "solid"),
                    fgColor=hex2argb(shading.get("fg_color", "0000FF")),
                )
                for row in ws[cell_range]:
                    for cell in row:
                        cell.fill = fill
        if verbose:
            shade_temp="""shade={        
                        (slice(1, 4), slice(1, 3)): {
                        "bg_color": "#63C187",  # Background color
                        "pattern_type": "solid",  # Fill pattern (e.g., solid, darkGrid, lightGrid)
                        "fg_color": "#0000FF",  # Foreground color, used in patterns
                        "end_color": "0000FF",  # End color, useful for gradients
                        "fill_type": "solid",  # Type of fill (solid, gradient, etc.)
                    }}"""
            print(shade_temp)
    # !number formatting
    if number_format:
        if not isinstance(number_format, list):
            number_format = [number_format]
        for number_format_ in number_format:
            for col_idx, fmt in number_format_.items():
                col_letter = get_column_letter(col_idx)
                for cell in ws[col_letter][1:]:  # Skip the header
                    cell.number_format = fmt
        if verbose:
            number_format_temp="""number_format={
                                1: "0.00",  # Two decimal places for column index 1
                                2: "#,##0",  # Thousands separator
                                3: "0%",  # Percentage format
                                4: "$#,##0.00",  # Currency format
                            }""" 
            print(number_format_temp)
    
    if freeze:
        if isinstance(freeze,bool):
            freeze='A2'
        ws.freeze_panes = freeze  # Freeze everything above and to the left of A2
    if apply_filter:
        if isinstance(apply_filter, bool):
            # Default: Apply filter to the entire first row (header)
            filter_range = f"A1:{get_column_letter(ws.max_column)}1"
            ws.auto_filter.ref = filter_range
            if not freeze:
                ws.freeze_panes = "A2"  # Freeze everything above and to the left of A2
        elif isinstance(apply_filter, tuple):
            row_slice, col_slice = apply_filter
            # Extract the start and end indices for rows and columns
            start_row, end_row = row_slice.start, row_slice.stop
            start_col_idx, end_col_idx = col_slice.start, col_slice.stop

            # Ensure valid row and column indices
            if start_row < 1: start_row = 1  # Row should not be less than 1
            if end_row > ws.max_row: end_row = ws.max_row  # Ensure within sheet's row limits
            if start_col_idx < 1: start_col_idx = 1  # Column should not be less than 1
            if end_col_idx > ws.max_column: end_col_idx = ws.max_column  # Ensure within sheet's column limits
            
            # Get column letters based on indices
            start_col = get_column_letter(start_col_idx)
            end_col = get_column_letter(end_col_idx)

            # Define the filter range based on specific rows and columns
            filter_range = f"{start_col}{start_row}:{end_col}{end_row}"

            # Apply the filter
            ws.auto_filter.ref = filter_range
            if freeze:
                ws.freeze_panes = freeze  # Freeze everything above and to the left of A2
    # !widths 
    if isinstance(width, bool):
        width=None if width else False
    if isinstance(height,bool):
        height=None if height else False

    merged_cells = set()
    for merged_range in ws.merged_cells.ranges:
        for row in ws.iter_rows(min_row=merged_range.min_row,
                            max_row=merged_range.max_row,
                            min_col=merged_range.min_col,
                            max_col=merged_range.max_col):
            for cell in row:
                merged_cells.add(cell.coordinate)
    if width is None or width == {}:  # automatic adjust width
        print("auto-width")
        for col in ws.columns:
            if not col:
                continue
            try:
                col_letter = get_column_letter(col[0].column)
                
                # Skip entire column if any cell is merged
                if any(cell.coordinate in merged_cells for cell in col):
                    continue
                    
                max_length = 0
                for cell in col:
                    try:
                        if cell.value:
                            cell_value = str(cell.value)
                            if '\n' in cell_value:
                                max_line_length = max(len(line) for line in cell_value.split('\n'))
                                max_length = max(max_length, max_line_length)
                            else:
                                max_length = max(max_length, len(cell_value))
                    except:
                        pass
                        
                adjusted_width = (max_length * width_factor) + width_padding
                if width_max is not None:
                    adjusted_width = min(adjusted_width, width_max)
                ws.column_dimensions[col_letter].width = max(5, adjusted_width)
                
            except Exception as e:
                print(f"Error adjusting width for column: {e}")
                continue
    elif isinstance(width, (int, float)): # set all columns to this value
        print("set to fixed width {}".format(width))
        for col in ws.columns:
            column = get_column_letter(col[0].column)
            ws.column_dimensions[column].width = width * width_factor + width_padding
    elif isinstance(width, dict):  # custom widths per column
        for col_idx, width_ in width.items():
            col_letter = get_column_letter(col_idx)
            ws.column_dimensions[col_letter].width = width_
 
    # !heights
    if height is None or height=={}:  # automatic adust height
        for row in ws.iter_rows(min_row=1, max_row=ws.max_row):
            max_height = 0
            for cell in row:
                if cell.value:
                    lines = str(cell.value).split("\n")
                    max_line_length = max(len(line) for line in lines)
                    estimated_height = 15 * len(lines)
                    if max_line_length > 20:
                        estimated_height += 5 * (max_line_length // 20)
                    max_height = max(max_height, estimated_height)
            height_adj=max_height*height_factor+height_padding
            if height_max is not None:
                height_adj=min(height_adj, height_max)
                    
            ws.row_dimensions[row[0].row].height = height_adj
    elif isinstance(height,bool) and not height:
        pass 
    elif isinstance(height, (int, float)):
        # set all rows to the same fixed height
        for i in range(1, ws.max_row + 1):
            ws.row_dimensions[i].height = height*height_factor+height_padding

    else:
        for row, height_ in height.items():
            ws.row_dimensions[row].height = height_

    # !Merge cells using slice indices
    if merge:
        if isinstance(merge, tuple):
            merge = [merge]
        for indices in merge:
            # Ensure indices are slice objects
            if len(indices) == 2:
                row_slice, col_slice = indices
                merge_range = convert_indices_to_range(row_slice, col_slice)
                ws.merge_cells(merge_range)
            elif len(indices) == 4:
                start_row, start_col, end_row, end_col = indices
                # Convert column numbers to letters (e.g., 1 -> 'A')
                start_cell = f"{get_column_letter(start_col)}{start_row}"
                end_cell = f"{get_column_letter(end_col)}{end_row}"
                merge_range = f"{start_cell}:{end_cell}"
                ws.merge_cells(merge_range)
            else:
                raise ValueError(
                    f"‰∏§ÁßçÊñπÂºè: 1. format: (start_row, start_col, end_row, end_col), 2. format: (slice(0, 3), slice(1, 2))"
                )

    # !Add comment
    if comment:
        if not isinstance(comment, list):
            comment = [comment]
        for comment_ in comment:
            if not isinstance(comment_, dict):
                raise TypeError("Each item in the `comments` list must be a dictionary.")
        
            for (row, col), comment_str in comment_.items():
                if not isinstance(row, int) or not isinstance(col, int):
                    raise ValueError("Row and column indices must be integers.")
                if not isinstance(comment_str, str):
                    raise ValueError("Comment text must be a string.")

                comment_curr = Comment(comment_str, "Author")
                comment_curr.visible = comment_always_visible
                ws.cell(row=row + 1, column=col + 1).comment = comment_curr
        if verbose:
            comment_tmp="""comment=[
                        {(0, 0): "This is a comment for A1"},
                        {(1, 1): "This is a comment for B2"},
                        {(2, 2): "This is a comment for C3"},
                    ]"""
            print(comment_tmp)
    # !Add link
    if link:
        if not isinstance(link, list):
            link = [link]
        for link_ in link:
            for (row, col), link_str in link_.items():
                ws.cell(row=row + 1, column=col + 1).hyperlink = link_str
        if verbose:
            print('link={(2, 2): "https://example.com"}')
    # !Apply data validation
    if data_validation:
        if verbose:
            print("""data_validation={
                        (slice(1, 2), slice(2, 10)): {
                            "type": "list",
                            "formula1": '"Option1,Option2,Option3"',  # List of options
                            "allow_blank": True,
                            "showDropDown": False,
                            "showErrorMessage": True,
                            "errorTitle": "Invalid input",
                            "error": "Please select a valid option.",
                        }
                    }"""
                  )
    if data_validation:
        # Default settings based on type
        default_settings_by_type = {
            "list": {
                "allow_blank": True,
                "showDropDown": False,
                "showErrorMessage": False,
                "errorTitle": "",
                "error": "",
            },
            "whole": {
                "allow_blank": True,
                "showErrorMessage": True,
                "errorTitle": "Invalid number",
                "error": "Please enter a valid whole number.",
            },
            "decimal": {
                "allow_blank": True,
                "showErrorMessage": True,
                "errorTitle": "Invalid decimal",
                "error": "Please enter a valid decimal number.",
            },
            "date": {
                "allow_blank": True,
                "showErrorMessage": True,
                "errorTitle": "Invalid date",
                "error": "Please enter a valid date.",
            },
            "time": {
                "allow_blank": True,
                "showErrorMessage": True,
                "errorTitle": "Invalid time",
                "error": "Please enter a valid time.",
            },
            "textLength": {
                "allow_blank": True,
                "showErrorMessage": True,
                "errorTitle": "Invalid text length",
                "error": "Please enter text with correct length.",
            },
            "custom": {
                "allow_blank": True,
                "showErrorMessage": True,
                "errorTitle": "Invalid input",
                "error": "Your input does not match the required format.",
            },
        }

        for indices, validation in data_validation.items():
            cell_range = convert_indices_to_range(*indices)

            # Get default settings based on type
            validation_type = validation.get("type", "list")
            # Preprocess formula1 if it's given as a list (for list type)
            validation["type"]=validation_type
            print(validation)
            if validation.get("type") == "list" and "formula1" in validation and isinstance(validation["formula1"], list):
                print('yes')
                validation["formula1"] = '"' + ",".join(str(i) for i in validation["formula1"]) + '"'

            default_settings = default_settings_by_type.get(validation_type, {})

            # Apply defaults (only if not already set)
            for key, value in default_settings.items():
                validation.setdefault(key, value)
            print(validation)
            dv = DataValidation(**validation)
            ws.add_data_validation(dv)
            dv.add(cell_range)

    if bool(protect):  # Check if protection options are provided
        from openpyxl.worksheet.protection import SheetProtection

        if protect is None:
            protect = {}  # Ensure protect is always a dictionary

        # Default to 'protect' password if not explicitly set
        password = password or protect.get("password")

        # Apply sheet protection with password if a password is provided
        if password:
            ws.protection.sheet = True
            ws.protection.set_password(password)  # Directly set the password
        else:
            ws.protection.sheet = protect.get("sheet", False)  # Use 'protect' settings if no password 
        # ÂàõÂª∫ SheetProtection ÂØπË±°ÔºåÂπ∂‰º†ÂÖ•ÂêÑÈ°π‰øùÊä§ÈÖçÁΩÆ
        protection = SheetProtection(
            password=password,  # ËÆæÁΩÆÂØÜÁ†ÅÔºåÁî®Êà∑Âú® Excel ‰∏≠ÂèñÊ∂à‰øùÊä§Êó∂ÈúÄË¶ÅËæìÂÖ•
            sheet=True,  # ÂêØÁî®Â∑•‰ΩúË°®‰øùÊä§ÔºåÂøÖÈ°ª‰∏∫ True ÂÖ∂‰ΩôÈÄâÈ°πÊâçÁîüÊïà

            # ‰ª•‰∏ã‰∏∫‰øùÊä§ÈÄâÈ°πÔºàTrue Ë°®Á§∫ÂÖÅËÆ∏Êìç‰ΩúÔºåFalse Ë°®Á§∫Á¶ÅÊ≠¢Êìç‰ΩúÔºâÔºö
            objects=protect.get("objects", False),  # ÊòØÂê¶ÂÖÅËÆ∏ÁºñËæëÂµåÂÖ•ÂØπË±°ÔºàÂ¶ÇÂõæË°®„ÄÅÊåâÈíÆÁ≠âÔºâ
            scenarios=protect.get("scenarios", False),  # ÊòØÂê¶ÂÖÅËÆ∏ËÆøÈóÆ‚ÄúÊñπÊ°àÁÆ°ÁêÜÂô®‚ÄùÔºàExcel ÁöÑÂÅáËÆæÂàÜÊûêÂäüËÉΩÔºâ

            # ÂçïÂÖÉÊ†ºÊ†ºÂºèËÆæÁΩÆÊùÉÈôêÔºö
            formatCells=protect.get("formatCells", False),  # ÊòØÂê¶ÂÖÅËÆ∏Êõ¥ÊîπÂçïÂÖÉÊ†ºÊ†ºÂºèÔºàÂ≠ó‰Ωì„ÄÅÈ¢úËâ≤„ÄÅËæπÊ°ÜÁ≠âÔºâ
            formatColumns=protect.get("formatColumns", False),  # ÊòØÂê¶ÂÖÅËÆ∏Ë∞ÉÊï¥ÂàóÂÆΩ
            formatRows=protect.get("formatRows", False),  # ÊòØÂê¶ÂÖÅËÆ∏Ë∞ÉÊï¥Ë°åÈ´ò

            # ÊèíÂÖ•‰∏éÂà†Èô§ÊùÉÈôêÔºö
            insertColumns=protect.get("insertColumns", False),  # ÊòØÂê¶ÂÖÅËÆ∏ÊèíÂÖ•Êñ∞Âàó
            insertRows=protect.get("insertRows", False),  # ÊòØÂê¶ÂÖÅËÆ∏ÊèíÂÖ•Êñ∞Ë°å
            deleteColumns=protect.get("deleteColumns", False),  # ÊòØÂê¶ÂÖÅËÆ∏Âà†Èô§Âàó
            deleteRows=protect.get("deleteRows", False),  # ÊòØÂê¶ÂÖÅËÆ∏Âà†Èô§Ë°å

            # ÈÄâÊã©ÂçïÂÖÉÊ†ºÊùÉÈôêÔºö
            selectLockedCells=protect.get("selectLockedCells", False),  # ÊòØÂê¶ÂÖÅËÆ∏ÈÄâÊã©Ë¢´ÈîÅÂÆöÁöÑÂçïÂÖÉÊ†º
            selectUnlockedCells=protect.get("selectUnlockedCells", True)  # ÊòØÂê¶ÂÖÅËÆ∏ÈÄâÊã©Êú™Ë¢´ÈîÅÂÆöÁöÑÂçïÂÖÉÊ†º
        )
        ws.protection = protection
    # !conditional formatting
    if conditional_format:
        if not isinstance(conditional_format, list):
            conditional_format = [conditional_format]
        print(f"conditional_format dict setting: 'color_scale', 'data_bar' and 'icon_set[not-ready]'")
        for conditional_format_ in conditional_format:
            for indices, rules in conditional_format_.items():
                cell_range = convert_indices_to_range(*indices)
                if not isinstance(rules, list):
                    rules=[rules]
                for rule in rules:
                    # Handle color scale
                    if "color_scale" in rule:
                        if verbose:
                            color_scale_tmp="""
                                    conditional_format={
                                        (slice(1, df.shape[0] + 1), slice(1, 2)):
                                            {
                                                "color_scale": {
                                                    "start_type": "min",
                                                    "start_value": 0,
                                                    "start_color": "#74ADE9",
                                                    "mid_type": "percentile",
                                                    "mid_value": 50,
                                                    "mid_color": "74ADE9",
                                                    "end_type": "max",
                                                    "end_value": 100,
                                                    "end_color": "#B62833",
                                                }
                                            }}
                            """
                            print(color_scale_tmp)
                        color_scale = rule["color_scale"] 

                        color_scale_rule = ColorScaleRule(
                            start_type=color_scale.get("start_type", "min"),
                            start_value=color_scale.get("start_value",None),
                            start_color=hex2argb(color_scale.get("start_color", "#74ADE9")),
                            mid_type=color_scale.get("mid_type","percentile"),
                            mid_value=color_scale.get("mid_value",None),
                            mid_color=hex2argb(color_scale.get("mid_color", "FFFFFF")),
                            end_type=color_scale.get("end_type", "max"),
                            end_value=color_scale.get("end_value",None),
                            end_color=hex2argb(color_scale.get("end_color", "#B62833")),
                        )
                        ws.conditional_formatting.add(cell_range, color_scale_rule)
                    # Handle data bar
                    if "data_bar" in rule:
                        if verbose:
                            data_bar_tmp="""
                                    conditional_format={
                                        (slice(1, df.shape[0] + 1), slice(1, 2)):
                                            {
                                                "data_bar": {
                                                    "start_type": "min",
                                                    "start_value": None,
                                                    "end_type": "max",
                                                    "end_value": None,
                                                    "color": "F6C9CE",
                                                    "show_value": True,
                                                }
                                            }}
                            """
                            print(data_bar_tmp)
                        data_bar = rule["data_bar"]
                        bar_color = hex2argb(data_bar.get("color", "638EC6"))

                        data_bar_rule = DataBarRule(
                            start_type=data_bar.get("start_type", "min"),
                            start_value=data_bar.get("start_value",None),
                            end_type=data_bar.get("end_type", "max"),
                            end_value=data_bar.get("end_value",None),
                            color=bar_color,
                            showValue=data_bar.get("show_value", True),
                        )
                        ws.conditional_formatting.add(cell_range, data_bar_rule)

                    # Handle icon setse
                    if "icon_set" in rule:
                        icon_set = rule["icon_set"]
                        icon_set_rule = IconSet(
                            iconSet=icon_set.get("iconSet", "3TrafficLights1"),  # Corrected
                            showValue=icon_set.get("show_value", True),        # Corrected
                            reverse=icon_set.get("reverse", False)            # Corrected
                        )
                        ws.conditional_formatting.add(cell_range, icon_set_rule)
                    # Handle text-based conditions
                    if "text_color" in rule: # not work
                        from openpyxl.styles.differential import DifferentialStyle
                        from openpyxl.formatting.rule import Rule
                        from openpyxl.styles import PatternFill

                        # Extract the fill properties from the rule
                        fill = rule.get("fill", {})
                        start_color = fill.get("start_color", "FFFFFF")  # Default to white if not specified
                        end_color = fill.get("end_color", "FFFFFF")  # Default to white if not specified
                        fill_type = fill.get("fill_type", "solid")  # Default to solid fill if not specified

                        # Extract the text condition or default to a space if 'text' is not provided
                        text = rule.get("text", " ")  

                        # Create the DifferentialStyle using the extracted fill settings
                        dxf = DifferentialStyle(
                            fill=PatternFill(start_color=start_color, end_color=end_color, fill_type=fill_type)
                        )
                        
                        # Create the text rule based on the text condition
                        text_rule = Rule(
                            type="containsText",  # The type of condition
                            operator=rule.get("operator", "equal"),  # Default operator is "equal"
                            text=text,
                            dxf=dxf,  # Apply the fill color from DifferentialStyle
                        )
                        ws.conditional_formatting.add(cell_range, text_rule)
        if verbose:
            conditional_format_temp="""
                    conditional_format={
                            (slice(1, 3), slice(1, 4)): [
                                {
                                    "data_bar": {
                                        "start_type": "min",
                                        "start_value": 100,
                                        "end_type": "max",
                                        "end_value": None,
                                        "color": "F6C9CE",
                                        "show_value": True,
                                    }
                                },
                                {
                                    "color_scale": {
                                        "start_type": "min",
                                        "start_value": 0,
                                        "start_color": "#74ADE9",
                                        "mid_type": "percentile",
                                        "mid_value": 50,
                                        "mid_color": "74ADE9",
                                        "end_type": "max",
                                        "end_value": 100,
                                        "end_color": "#B62833",
                                    }
                                },
                            ]
                        }
            """
            print(conditional_format_temp)
    if insert_img:
        if not isinstance(insert_img, dict):
            raise ValueError(f'insert_img ÈúÄË¶ÅdictÊ†ºÂºè: e.g., insert_img={"A1":"example.png"}')
        try:
            from openpyxl import drawing
            from PIL import Image
            import PIL
            for img_cell, img_data in insert_img.items():
                img_width = img_height = None
                pil_img=img_path = None
                if isinstance(img_data, dict):
                    if "path" in img_data:
                        img_ = drawing.image.Image(img_data["path"])# File path
                    elif "image" in img_data:
                        img_ = drawing.image.Image(img_data["image"])# PIL Image object
                    elif "array" in img_data:
                        img_ = drawing.image.Image(Image.fromarray(img_data["array"]))# Convert NumPy array to PIL Image

                    img_width = img_data.get("width", None)
                    img_height = img_data.get("height", None)
                elif isinstance(img_data, str):
                    img_ = drawing.image.Image(img_data)# Direct file path
                elif isinstance(img_data, (PIL.Image.Image,PIL.PngImagePlugin.PngImageFile)):
                    img_ = drawing.image.Image(img_data)# Direct PIL Image object
                elif isinstance(img_data, np.ndarray):
                    img_ = drawing.image.Image(Image.fromarray(img_data))# Convert NumPy array to PIL Image
                elif pil_img:
                    img_ = drawing.image.Image(pil_img)

                # Set width and height if provided
                if img_width is not None:
                    img_.width = img_width
                if img_height is not None:
                    img_.height = img_height
                ws.add_image(img_, img_cell)  
                print(f"‚úÖ at column '{img_cell}' inserted image ==>File: {os.path.basename(filename)}")

        except Exception as e:
            print(e)
    if protect_file and isinstance(protect_file,dict):
        pass_=protect_file.get("password", None) 
        wb.security=WorkbookProtection(
                workbookPassword= enpass(pass_) if len(pass_)==len(enpass('a')) else pass_,
                lockStructure=protect_file.get("lock_structure", True),
                lockWindows=protect_file.get("lock_window", True),
                revisionsPassword=protect_file.get("warning", ""),# Add this line to suppress warnings
            )
        print(f'file is protected with password: {enpass(protect_file.get("password", None))[:6]}')

    # ungroup sheets
    for sheet in wb.worksheets:
        sheet.sheet_view.tabSelected = False
    # !Save the workbook
    try:
        wb.save(filename)
    except Exception as e:
         print(f"Error saving workbook: {str(e)}")

def preview(var):
    """Master function to preview formatted variables in Jupyter."""
    from bs4 import BeautifulSoup
    from IPython.display import display, HTML, Markdown

    if isinstance(var, str):
        if isa(var, "html"):
            display(HTML(var))  # Render as HTML
        # Check if it's a valid markdown
        elif var.startswith("#"):
            display(Markdown(var))
        else:
            # Otherwise, display as plain text
            print(var)
    elif isinstance(var, BeautifulSoup):
        preview(str(var))
    elif isinstance(var, pd.DataFrame):
        # Display pandas DataFrame
        display(var)

    elif isinstance(var, list) or isinstance(var, dict):
        import json

        # Display JSON
        json_str = json.dumps(var, indent=4)
        display(Markdown(f"```json\n{json_str}\n```"))

    elif isinstance(var, bytes):
        # Display image if it's in bytes format
        display(Image(data=var))

    elif isinstance(var, str) and (var.endswith(".png") or var.endswith(".jpg")):
        # Display image from file path
        display(Image(filename=var))

    elif isinstance(var, dict):
        import json

        # Handle dictionary formatting
        json_str = json.dumps(var, indent=4)
        display(Markdown(f"```json\n{json_str}\n```"))

    else:
        # If the format is not recognized, print a message
        print("Format not recognized or unsupported.")


def df2db(df: pd.DataFrame, db_url: str, table_name: str, if_exists: str = "replace"):
    """
    Save a pandas DataFrame to a SQL database (SQLite, MySQL, PostgreSQL).

    Usage:
    case 1. SQLite:
        df2db(df, 'sqlite:///sample_manager.db', 'users', 'replace')
    case 2. MySQL:
        df2db(df, 'mysql+mysqlconnector://user:password@localhost/mydatabase', 'users', 'replace')
    case 3. PostgreSQL:
        df2db(df, 'postgresql://user:password@localhost/mydatabase', 'users', 'replace')

    Parameters:
    - df (pd.DataFrame): The DataFrame to save.
    - db_url (str): The SQLAlchemy connection string to the database.
    - table_name (str): The name of the table to save the DataFrame to.
    - if_exists (str): What to do if the table already exists. Options:
        - 'replace': Drop the table before inserting new values.
        - 'append': Append new values to the table.
        - 'fail': Do nothing if the table exists. Default is 'replace'.
    """
    from sqlalchemy import create_engine
    from sqlalchemy.exc import SQLAlchemyError

    try:
        # Create the SQLAlchemy engine based on the provided db_url
        engine = create_engine(db_url)

        # Save the DataFrame to the SQL database (table)
        df.to_sql(table_name, con=engine, index=False, if_exists=if_exists)

        print(f"Data successfully saved to {db_url} in the {table_name} table.")

    except SQLAlchemyError as e:
        # Handle SQLAlchemy-related errors (e.g., connection issues, query issues)
        print(f"Error saving DataFrame to database: {str(e)}")

    except Exception as e:
        # Handle other unexpected errors
        print(f"An unexpected error occurred: {str(e)}")

def _df_outlier(
    data,
    columns=None,
    method=["zscore", "iqr", "percentile", "iforest"],
    min_outlier_method=3,  # Ëá≥Â∞ë‰∏§ÁßçÊñπÊ≥ïÊ£ÄÊü•Âá∫outlier
    zscore_threshold=3,
    iqr_threshold=1.5,
    lower_percentile=5,
    upper_percentile=95,
):
    from scipy.stats import zscore
    from sklearn.ensemble import IsolationForest
    from sklearn.preprocessing import StandardScaler

    # Fill completely NaN columns with a default value (e.g., 0)
    data = data.copy()
    data.loc[:, data.isna().all()] = 0
    if columns is not None:
        if isinstance(columns, (list, pd.core.indexes.base.Index)):
            data = data[columns]
    col_names_org = data.columns.tolist()
    index_names_org = data.index.tolist()
    # Separate numeric and non-numeric columns
    numeric_data = data.select_dtypes(include=[np.number])
    non_numeric_data = data.select_dtypes(exclude=[np.number])

    # if columns is not None:
    #     numeric_data = numeric_data[columns]
    if numeric_data.empty:
        raise ValueError("Input data must contain numeric columns.")

    outliers_df = pd.DataFrame(index=numeric_data.index)
    if isinstance(method, str):
        method = [method]

    # Z-score method
    if "zscore" in method:
        z_scores = np.abs(zscore(numeric_data))
        outliers_df["zscore"] = np.any(z_scores > zscore_threshold, axis=1)

    # IQR method
    if "iqr" in method:
        Q1 = numeric_data.quantile(0.25)
        Q3 = numeric_data.quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - iqr_threshold * IQR
        upper_bound = Q3 + iqr_threshold * IQR
        outliers_df["iqr"] = (
            (numeric_data < lower_bound) | (numeric_data > upper_bound)
        ).any(axis=1)

    # Percentile method
    if "percentile" in method:
        lower_bound = numeric_data.quantile(lower_percentile / 100)
        upper_bound = numeric_data.quantile(upper_percentile / 100)
        outliers_df["percentile"] = (
            (numeric_data < lower_bound) | (numeric_data > upper_bound)
        ).any(axis=1)

    # Isolation Forest method
    if "iforest" in method:
        # iforest method cannot handle NaNs, then fillna with mean
        numeric_data_ = numeric_data.fillna(numeric_data.mean())
        scaler = StandardScaler()
        scaled_data = scaler.fit_transform(numeric_data_)
        iso_forest = IsolationForest(contamination=0.05)
        outliers_df["iforest"] = iso_forest.fit_predict(scaled_data) == -1

    # Combine all outlier detections
    if len(method) == 4:  # all method are used:
        outliers_df["outlier"] = outliers_df.sum(axis=1) >= min_outlier_method
    else:
        outliers_df["outlier"] = outliers_df.any(axis=1)

    # Handling Outliers: Remove or Winsorize or Replace with NaN
    processed_data = numeric_data.copy()

    processed_data.loc[outliers_df["outlier"]] = np.nan

    return processed_data

def df_group(
    data: pd.DataFrame,
    columns: Union[str, list, None] = None,
    by: Union[str, list, None] = None,
    param: Dict[str, Any] = None,
    sep: Union[str, list] = [", ", ","],
    dropna: bool = True,
    unique: bool = False,
    astype: type = str,
    merge: List[str] = None,
    merge_symbo_column: str = " & ",
    merge_symbo_cell: str = "[]",  # ["{}","()","[]"]
) -> pd.DataFrame:
    """
    Groups a dataframe based on specified column(s) and applies aggregation functions dynamically.
    
    Parameters:
    data (pd.DataFrame): The dataframe to be grouped.
    columns (Union[str, list, None]): Columns to select; if None, all columns are selected.
    by (Union[str, list, None]): Column name(s) to group by (single or multiple).
    param (dict): A dictionary specifying custom aggregation rules.
    sep (Union[str, list]): Separator for concatenated values. If list: sep[0]=general, sep[1]=merge.
    dropna (bool): Whether to drop NaN values before aggregation.
    unique (bool): Whether to apply uniqueness before concatenation.
    astype (type): Data type to cast values before aggregation.
    merge (List[str]): List of columns to merge into a single paired column.
    merge_symbo_column (str): Symbol to join merged column names (default "&").
    merge_symbo_cell (str): Symbol to wrap merged cell values (default "[]"; options: "{}", "()", "[]").

    Returns:
    pd.DataFrame: Grouped and aggregated dataframe with original column order preserved.
    Usage:
    data = pd.DataFrame({
                        "Cage Tag": [1, 1, 2, 2, 3],
                        "Physical Tag": ["A1", "A2", "B1", "B2", "C1"],
                        "Sex": ["M", "F", "M", "M", "F"],
                        "Date of Birth": ["2021-06-01", "2021-06-02", "2021-07-01", "2021-07-02", "2021-08-01"],
                        "Age": [34, 35, 30, 31, 29],
                        "State": ["Mating", "Resting", "Mating", "Resting", "Mating"],
                        "Owner": ["Dr. Smith", "Dr. Smith", "Dr. Brown", "Dr. Brown", "Dr. Lee"],
                        })
    display(data)
    result = df_group(data,
                    #   columns=["Sex", "Date of Birth", "Age", "State"],
                    by="Cage Tag",
                    merge=["Age", "State"],
                    merge_symbo_column="|",
                    #   astype=str
                    # sep=[',',    '_'],
                    merge_symbo_cell=None
                    )
    """
    # Define aggregation function for unique or non-unique concatenation
    def _agg_func(x):
        if unique:
            values = x.dropna().unique() if dropna else x.unique()
        else:
            values = x.dropna() if dropna else x
        return sep[0].join(map(astype, values))

    if param is None:
        param = {}
    if columns is None:
        columns = data.columns.tolist()
    elif isinstance(columns, str):
        columns = [columns]
    if by is None:
        by = []
    elif isinstance(by, str):
        by = [by]
    if not isinstance(sep, list):
        sep = [sep]
    if len(sep) == 1:
        sep = [sep[0], sep[0]]

    # Create a copy of original columns for final ordering
    original_columns = columns.copy()

    # Merge specified columns into a single column
    if merge:
        merge_col_name = merge_symbo_column.join(merge)
        
        # Create merged column with optional wrapping symbols
        if merge_symbo_cell is None:
            data[merge_col_name] = data[merge].apply(
                lambda row: f"{sep[1].join(map(astype, row.dropna()))}" if dropna 
                else f"{sep[1].join(map(astype, row))}", axis=1
            )
        elif len(merge_symbo_cell) == 2:
            open_sym, close_sym = merge_symbo_cell[0], merge_symbo_cell[1]
            data[merge_col_name] = data[merge].apply(
                lambda row: f"{open_sym}{sep[1].join(map(astype, row.dropna()))}{close_sym}" if dropna 
                else f"{open_sym}{sep[1].join(map(astype, row))}{close_sym}", axis=1
            )
        else:
            data[merge_col_name] = data[merge].apply(
                lambda row: f"[{sep[1].join(map(astype, row.dropna()))}]" if dropna 
                else f"[{sep[1].join(map(astype, row))}]", axis=1
            )
        
        # Update columns list and remove merged columns
        columns = [col for col in columns if col not in merge]
        columns.append(merge_col_name)
        original_columns = columns.copy()
        
        # Remove merged columns from data
        data = data.drop(columns=merge)
        
        # Remove merged columns from param if present
        for col in merge:
            if col in param:
                del param[col]

    # Default aggregation: unique concatenation for non-group columns
    default_aggregations = {
        col: _agg_func for col in columns if col not in by
    }
    aggregation_rules = {**default_aggregations, **param}

    # Group and aggregate
    if by:
        grouped_df = data.groupby(by).agg(aggregation_rules).reset_index()
    else:
        grouped_df = data.agg(aggregation_rules).to_frame().T

    # Preserve original column order (grouping columns first)
    group_columns = [col for col in by if col in grouped_df.columns]
    non_group_columns = [
        col for col in original_columns 
        if col not in by and col in grouped_df.columns
    ]
    extra_columns = [
        col for col in grouped_df.columns 
        if col not in group_columns and col not in non_group_columns
    ]
    
    ordered_columns = group_columns + non_group_columns + extra_columns
    return grouped_df[ordered_columns]

def df_outlier(
    data,
    columns=None,
    method=["zscore", "iqr", "percentile", "iforest"],
    min_outlier_method=2,  # Ëá≥Â∞ë‰∏§ÁßçÊñπÊ≥ïÊ£ÄÊü•Âá∫outlier
    zscore_threshold=3,
    iqr_threshold=1.5,
    lower_percentile=5,
    upper_percentile=95,
):
    """
    Usage:
    data_out = df_outlier(
        data,
        columns=["income"],
        method="iforest",
        min_outlier_method=1)

    Advanced outlier detection and handling function.

    Parameters:
    - data: DataFrame, the input data (numerical).
    - method: List, the outlier detection method to use. Options: 'zscore', 'iqr', 'percentile', 'iforest'.
    - zscore_threshold: float, threshold for Z-score outlier detection (default 3).
    - iqr_threshold: float, threshold for IQR method (default 1.5).
    - lower_percentile: float, lower percentile for percentile-based outliers (default 5).
    - upper_percentile: float, upper percentile for percentile-based outliers (default 95).
    - keep_nan: bool, whether to replace outliers with NaN (default True).
    - plot: bool, whether to visualize the outliers (default False).
    - min_outlier_method: int, minimum number of method that need to flag a row as an outlier (default 2).
    - inplace: bool, whether to modify the original `data` DataFrame (default False).

    Returns:
    - processed_data: DataFrame with outliers handled based on method (if winsorize/remove is True).
    """
    col_names_org = data.columns.tolist()
    index_names_org = data.index.tolist()

    numeric_data = data.select_dtypes(include=[np.number])
    non_numeric_data = data.select_dtypes(exclude=[np.number])

    _outlier_df_tmp = pd.DataFrame()
    for col in numeric_data.columns:
        _outlier_df_tmp = pd.concat(
            [
                _outlier_df_tmp,
                _df_outlier(
                    data=data,
                    columns=[col],
                    method=method,
                    min_outlier_method=min_outlier_method,  # Ëá≥Â∞ë‰∏§ÁßçÊñπÊ≥ïÊ£ÄÊü•Âá∫outlier
                    zscore_threshold=zscore_threshold,
                    iqr_threshold=iqr_threshold,
                    lower_percentile=lower_percentile,
                    upper_percentile=upper_percentile,
                ),
            ],
            axis=1,
            # join="inner",
        )
    processed_data = pd.concat([_outlier_df_tmp, non_numeric_data], axis=1)
    processed_data = processed_data[col_names_org]
    return processed_data


def df_extend(data: pd.DataFrame, column, axis=0, sep=None, prefix="col"):
    """
    Extend a DataFrame by the list elecments in the column.

    Parameters:
    ----------
    data : pd.DataFrame
        The input DataFrame to be extended.
    column : str
        The name of the column to be split.

    axis : int, optional
        The axis along which to expand the DataFrame.
        - 0 (default): Expand the specified column into multiple rows.
        - 1: Expand the specified column into multiple columns.

    sep : str, optional
        The separator used to split the values in the specified column.
        Must be provided for the function to work correctly.
    """

    data = data.copy()
    mask = data[column].str.contains(sep, na=False)
    data = data.copy()
    if mask.any():
        data[column] = data[column].apply(
            lambda x: x.split(sep) if isinstance(x, str) else x
        )  # Only split if x is a string

        # Strip spaces from each item in the lists
        data[column] = data[column].apply(
            lambda x: [item.strip() for item in x] if isinstance(x, list) else x
        )

    data = data.explode(column, ignore_index=True)
    return data


def df_cycle(data: pd.DataFrame, columns=None, max_val=None, inplace=False):
    """
    Purpose: transforms a datetime feature (like month or day) into a cyclic encoding for use in machine learning models, particularly neural networks.
    Usage:
        data = pd.DataFrame({'month': [1, 4, 7, 10, 12]})  # Just months as an example
        # df_cycle month cyclically
        data = df_cycle(data, 'month', 12)
    """
    if columns is None:
        columns = list(
            data.select_dtypes(include=np.number).columns
        )  # If no columns specified, use all columns
    if max_val is None:
        max_val = np.max(
            data[columns]
        )  # If no max_val specified, use the maximum value across all columns
    if isinstance(columns, str):
        columns = [
            columns
        ]  # If a single column name is provided as a string, convert it to a list

    # Check if inplace is True, so we modify the original dataframe
    if inplace:
        # Modify the data in place, no return statement needed
        for col in columns:
            data[col + "_sin"] = np.sin(2 * np.pi * data[col] / max_val)
            data[col + "_cos"] = np.cos(2 * np.pi * data[col] / max_val)
    else:
        # If inplace is False, return the modified dataframe
        new_data = data.copy()
        for col in columns:
            new_data[col + "_sin"] = np.sin(2 * np.pi * new_data[col] / max_val)
            new_data[col + "_cos"] = np.cos(2 * np.pi * new_data[col] / max_val)
        return new_data


# ! DataFrame
def df_astype(
    data: pd.DataFrame,
    columns: Optional[Union[str, int, List[Union[str, int]]]] = None,
    astype: Union[str, dict] = 'datetime',
    skip_row: Union[str, list] = None,
    original_fmt: str = None,
    fmt: Optional[str] = None,
    inplace: bool = False,
    errors: str = "coerce",  # "ignore", "raise", "coerce"
    verbose: bool = False,
    **kwargs,
) -> Optional[pd.DataFrame]:
    """
    A powerful DataFrame column type converter supporting datetime, timedelta, numeric, categorical, and cyclical conversions.
    """
    VERBOSE=verbose
    def _standardize_columns(cols):
        if cols is None:
            return list(data.columns)
        if isinstance(cols, (str, int)):
            return [cols]
        return cols

    def _to_column_name(col):
        return data.columns[col] if isinstance(col, int) else col

    def _is_nanosecond_timestamp(series):
        """Check if values are in nanosecond timestamp range"""
        try:
            return series.dropna().apply(lambda x: isinstance(x, (int, float))).all() and \
                   series.dropna().between(1e18, 2e18).all()
        except:
            return False

    def convert_num_cell(val,**kwargs):
        if is_nan(val):
            return val
        try:
            return str2num(val, **kwargs)
        except Exception as e:
            print(e) if VERBOSE else None
            return val

    def _convert_num(series,**kwargs):
        try:
            series = series.apply(lambda x: convert_num_cell(str(x),**kwargs))
        except Exception as e:
            print(e) if VERBOSE else None
        return series
    def convert_if_true_cell(val):
        if is_nan(val) or isinstance(val, (int, float)):
            return val
        try:
            return str2date(val, fmt=fmt)
        except Exception as e:
            print(e) if VERBOSE else None
            return val
    def _convert_datetime(series, type_hint=None):
        if _is_nanosecond_timestamp(series):
            print(f"Converting nanosecond timestamps in column '{series.name}'") if VERBOSE else None
            series = pd.to_datetime(series, errors=errors, unit='ns')
        try:
            if type_hint:
                if type_hint == "time":
                    series = pd.to_datetime(series, format=original_fmt, errors='raise', **kwargs)
                    return series.dt.time
                elif type_hint == "date":
                    series = pd.to_datetime(series, format=original_fmt, errors='raise', **kwargs)
                    return series.dt.date
                elif type_hint == "year":
                    series = pd.to_datetime(series, format=original_fmt, errors='raise', **kwargs)
                    return series.dt.year
                elif type_hint == "month":
                    series = pd.to_datetime(series, format=original_fmt, errors='raise', **kwargs)
                    return series.dt.month
                elif type_hint == "day":
                    series = pd.to_datetime(series, format=original_fmt, errors='raise', **kwargs)
                    return series.dt.day
                elif type_hint == "hour":
                    series = pd.to_datetime(series, format=original_fmt, errors='raise', **kwargs)
                    return series.dt.hour
                elif type_hint == "minute":
                    series = pd.to_datetime(series, format=original_fmt, errors='raise', **kwargs)
                    return series.dt.minute
                elif type_hint == "second":
                    series = pd.to_datetime(series, format=original_fmt, errors='raise', **kwargs)
                    return series.dt.second
                elif type_hint == "week":
                    series = pd.to_datetime(series, format=original_fmt, errors='raise', **kwargs)
                    return series.dt.day_name()
            # Use numpy vectorize
            series=series.map(convert_if_true_cell)
        except Exception as e:
            print(f"erros: _convert_datetime: {e}") if VERBOSE else None
        return series
    def _convert_single_column(col, atype):
        col_name = _to_column_name(col)
        try:
            print(f"{atype} processing '{col_name}'") if VERBOSE else None
            if atype in datetime_types:
                data[col_name] = _convert_datetime(data[col_name], atype if atype != "datetime" else None)
            elif atype == "num":
                data[col_name] = _convert_num(data[col_name],**kwargs)
            elif atype == "numeric":
                data[col_name] = pd.to_numeric(data[col_name], errors=errors, **kwargs)
            elif atype == "timedelta":
                data[col_name] = pd.to_timedelta(data[col_name], errors=errors, **kwargs)
            elif atype == "circular":
                max_val = kwargs.get("max_val")
                data[col_name] = df_cycle(data, columns=col_name, max_val=max_val)
            elif atype == "int":
                data[col_name] = data[col_name].astype("float").astype("int")
            else:
                data[col_name] = data[col_name].astype(atype)
        except Exception as e:
            print(f"Failed to convert column '{col_name}' to '{atype}': {e}") if VERBOSE else None

    # Main df_astype Func
    if not inplace:
        data = data.copy()

    if skip_row is not None:
        data = data.drop(index=skip_row, errors="ignore")

    columns = _standardize_columns(columns)
    print(columns) if VERBOSE else None
    # supported aliases
    datetime_types = [
        "datetime", "time", "date", "year", "month", "day", "hour", "minute", "second", "week"
    ]
    if astype in datetime_types and len(columns)==data.shape[1]:
        df_bool = data.map(is_datetime)
        all_true_cols = df_bool.columns[df_bool.all()]
        columns = all_true_cols.tolist()
        print(f"processing columns: {columns}") if VERBOSE else None
    # print(columns) if VERBOSE else None
    if isinstance(astype, dict):
        for col, atype in astype.items():
            _convert_single_column(col, atype)
    else:
        for col in columns:
            print(col) if VERBOSE else None
            _convert_single_column(col, astype)

    # if verbose:
    #     print(data.info())
    return None if inplace else data

def calculate_age(birthdate,fmt="%d.%m.%Y", exact=True, default=None, return_years_months=False):
    """
    ËÆ°ÁÆóÂπ¥ÈæÑÔºàÁ≤æÁ°ÆÂà∞Âπ¥ÊàñÂπ¥ÊúàÔºâ

    ÂèÇÊï∞:
    -----------
    birthdate : datetime-like, str
        Âá∫ÁîüÊó•ÊúüÔºåÂèØ‰ª•ÊòØdatetimeÂØπË±°„ÄÅpandas.TimestampÊàñÂèØËß£ÊûêÁöÑÊó•ÊúüÂ≠óÁ¨¶‰∏≤
    exact : bool, ÂèØÈÄâ
        ÊòØÂê¶ËÆ°ÁÆóÁ≤æÁ°ÆÂπ¥ÈæÑÔºàËÄÉËôëÊúà‰ªΩÂíåÊó•ÔºâÔºåFalseÂàôÂè™ËÆ°ÁÆóÂπ¥‰ªΩÂ∑Æ (ÈªòËÆ§: True)
    default : any, ÂèØÈÄâ
        ÂΩìËæìÂÖ•Êó†ÊïàÊó∂ËøîÂõûÁöÑÈªòËÆ§ÂÄº (ÈªòËÆ§: None)
    return_years_months : bool, ÂèØÈÄâ
        ÊòØÂê¶ËøîÂõûÂåÖÂê´Âπ¥ÂíåÊúàÁöÑÂ≠óÂÖ∏ (ÈªòËÆ§: False)

    ËøîÂõû:
    --------
    int or dict or default
        ËøîÂõûÂπ¥ÈæÑÔºàÊï¥Êï∞ÔºâÔºåÊàñ{'years': int, 'months': int}Â≠óÂÖ∏ÔºåÊàñÈªòËÆ§ÂÄº
    ‰æãÂ≠ê: 
    data = {
        "birthdate": [
            "04/20/19",
            "04/20/33",
            "04/20/29",
            "04/20/30",
            "04/20/68",
            "04/20/69",
            "04/20/99",
        ],
    }
    df = pd.DataFrame(data)
    # display(df)
    # ËΩ¨Êç¢Êó•ÊúüÔºà‰ΩøÁî®ÈªòËÆ§threshold=30Ôºâ
    df["converted"] = df_date(
        df[["birthdate"]], fmt="%m/%d/%y", astype="date", verbose=True, century_threshold=25
    )["birthdate"]
    # ÊòæÁ§∫ÁªìÊûú
    # display(df)
    df["age"] = df["converted"].apply(lambda x: calculate_age(x))
    df
    """
    from datetime import datetime

    # Â§ÑÁêÜÁ©∫ÂÄº
    if pd.isnull(birthdate):
        return default

    # Áªü‰∏ÄËΩ¨Êç¢‰∏∫datetimeÂØπË±°
    try:
        if not isinstance(birthdate, (datetime, pd.Timestamp)):
            birthdate = pd.to_datetime(birthdate,format=fmt, errors="raise")
    except Exception as e:
        print(f"Êó•ÊúüËß£ÊûêÈîôËØØ: {e}")
        return default

    today = datetime.today()

    # ËÆ°ÁÆóÂü∫Êú¨Âπ¥ÈæÑ
    try:
        if not exact:
            # ÁÆÄÂçïÂπ¥‰ªΩÂ∑ÆËÆ°ÁÆó
            age = today.year - birthdate.year
        else:
            # Á≤æÁ°ÆÂπ¥ÈæÑËÆ°ÁÆóÔºàËÄÉËôëÊúà‰ªΩÂíåÊó•Ôºâ
            age = (
                today.year
                - birthdate.year
                - ((today.month, today.day) < (birthdate.month, birthdate.day))
            )

        if return_years_months:
            # ËÆ°ÁÆóÂÆåÊï¥ÁöÑÂπ¥ÂíåÊúà
            months = (today.year - birthdate.year) * 12 + today.month - birthdate.month
            if today.day < birthdate.day:
                months -= 1
            years = months // 12
            months = months % 12
            return {"years": years, "months": months}
        else:
            return age

    except Exception as e:
        print(f"Âπ¥ÈæÑËÆ°ÁÆóÈîôËØØ: {e}")
        return default


def df_date(df, keywords=["date", "datum", "Êó∂Èó¥", "Êó•Êúü"], fmt="%d.%m.%y",original_fmt="%Y-%m-%d",
                 astype="str", century_threshold=None, inplace=False, verbose=False,**kwargs):
    """
    Ëá™Âä®ËØÜÂà´Âπ∂ËΩ¨Êç¢Êï∞ÊçÆÊ°Ü‰∏≠‰∏éÊó•ÊúüÁõ∏ÂÖ≥ÁöÑÂàóÊ†ºÂºè
    
    ÂèÇÊï∞:
        df (pd.DataFrame): ËæìÂÖ•Êï∞ÊçÆÊ°Ü
        keywords (list): Áî®‰∫éËØÜÂà´Êó•ÊúüÂàóÁöÑÂÖ≥ÈîÆËØçÂàóË°®
        fmt (str): Êó•ÊúüÊ†ºÂºèÂ≠óÁ¨¶‰∏≤
        astype (str): ÁõÆÊ†áÁ±ªÂûã ("str"Êàñ"datetime")
        century_threshold (int): ‰∏§‰ΩçÊï∞Âπ¥‰ªΩÁöÑ‰∏ñÁ∫™ÈòàÂÄº, default: current year+5, e.g., in year 2025, then the thr=30
        inplace (bool): ÊòØÂê¶ÂéüÂú∞‰øÆÊîπÊï∞ÊçÆÊ°Ü
        verbose (bool): ÊòØÂê¶ÊòæÁ§∫ËØ¶ÁªÜËΩ¨Êç¢‰ø°ÊÅØ
        
    ËøîÂõû:
        pd.DataFrame: ËΩ¨Êç¢ÂêéÁöÑÊï∞ÊçÆÊ°Ü
 
    # ÊµãËØïÊï∞ÊçÆ
    data = {
        "birthdate": ["04/20/29", "04/20/30", "04/20/68", "04/20/69", "04/20/99"],
        "description": [
            "‰Ωé‰∫éÈòàÂÄº(29<30)‚Üí2029Âπ¥",
            "Á≠â‰∫éÈòàÂÄº(30=30)‚Üí1930Âπ¥",
            "68‚Üí2068Âπ¥(POSIXÊ†áÂáÜ)",
            "69‚Üí1969Âπ¥(POSIXÊ†áÂáÜ)",
            "99‚Üí1999Âπ¥",
        ],
    }
    df = pd.DataFrame(data)
    display(df)

    df_date(
        df,
        fmt="%m/%d/%y",
        keywords=["date", "conv"],
        astype="date",
        verbose=True,
        inplace=True,
        century_threshold=30,
    )
    # ÊòæÁ§∫ÁªìÊûú
    display(df)
    df["age"] = df["birthdate"].apply(lambda x: calculate_age(x))
    df
    """
    from datetime import datetime

    def parse_custom_date(date_str, fmt, century_threshold=None, verbose=False):
        """
        Â¢ûÂº∫ÁâàÊó•ÊúüËß£ÊûêÂáΩÊï∞ÔºåÊ≠£Á°ÆÂ§ÑÁêÜ‰∏§‰ΩçÊï∞Âπ¥‰ªΩÁöÑ‰∏ñÁ∫™ÈóÆÈ¢ò
        
        ÂèÇÊï∞:
            date_str (str): Ë¶ÅËß£ÊûêÁöÑÊó•ÊúüÂ≠óÁ¨¶‰∏≤
            fmt (str): Êó•ÊúüÊ†ºÂºèÂ≠óÁ¨¶‰∏≤ (‰æãÂ¶Ç "%d/%m/%y")
            century_threshold (int): ‰∏§‰ΩçÊï∞Âπ¥‰ªΩÁöÑ‰∏ñÁ∫™ÈòàÂÄº (0-99)
                                    - Â∞è‰∫éÊ≠§ÂÄºËßÜ‰∏∫20XXÂπ¥
                                    - Â§ß‰∫éÁ≠â‰∫éÊ≠§ÂÄºËßÜ‰∏∫19XXÂπ¥
            verbose (bool): ÊòØÂê¶ÊòæÁ§∫Ë∞ÉËØï‰ø°ÊÅØ
            
        ËøîÂõû:
            datetime or pd.NaT: Ëß£ÊûêÂêéÁöÑdatetimeÂØπË±°ÊàñNaT(Ëß£ÊûêÂ§±Ë¥•Êó∂)
        """
        # Ê£ÄÊü•Á©∫ÂÄº
        if pd.isnull(date_str):
            if verbose:
                print("ËæìÂÖ•‰∏∫Á©∫ÂÄº")
            return pd.NaT
        if century_threshold is None:
            century_threshold=int(str(datetime.today().year)[-2:])+5
        # Ê∏ÖÁêÜÂ≠óÁ¨¶‰∏≤
        date_str = str(date_str).strip()
        if verbose:
            print(f"\nConverting '{date_str}' (format: '{fmt}')")
        
        try:
            # È¶ñÂÖàÂ∞ùËØïÁõ¥Êé•Ëß£Êûê
            dt = datetime.strptime(date_str, fmt)
            print(dt)
            
            # Â§ÑÁêÜ‰∏§‰ΩçÊï∞Âπ¥‰ªΩÁöÑÊÉÖÂÜµ
            if '%y' in fmt.lower() and not '%Y' in fmt.lower():
                current_year_last2 = datetime.now().year % 100
                year_full = dt.year
                
                # ËÆ°ÁÆóÊ≠£Á°ÆÁöÑ‰∏ñÁ∫™
                if (dt.year % 100) < century_threshold:
                    year_full = 2000 + (dt.year % 100)
                else:
                    year_full = 1900 + (dt.year % 100)
                if verbose:
                    print(f"Handle 2-digit years: {dt.year} -> {year_full} (century_threshold={century_threshold})")
                
                # Á°Æ‰øùÂπ¥‰ªΩÂú®ÂêàÁêÜËåÉÂõ¥ÂÜÖ
                if 1900 <= year_full <= 2100:
                    dt = dt.replace(year=year_full)
                else:
                    if verbose:
                        print(f"Âπ¥‰ªΩË∂ÖÂá∫ÂêàÁêÜËåÉÂõ¥: {year_full}")
                    return pd.NaT
            
            if verbose:
                print(f"Done! {dt}\n")
            return dt
            
        except ValueError as e:
            if verbose:
                print(f"Ëß£ÊûêÂ§±Ë¥•: {e}")
            return pd.NaT
    if century_threshold is None:
        century_threshold=int(str(datetime.today().year)[-2:])+5
    if not inplace:
        df = df.copy()
    
    # ÊâæÂá∫ÂêçÁß∞‰∏≠ÂåÖÂê´ÂÖ≥ÈîÆËØçÁöÑÂàó
    date_cols = [
        col for col in df.columns
        if any(kw.lower() in col.lower() for kw in keywords)
    ]
    
    astype=strcmp(astype,["datetime","str"])[0]
    print(f"\nËØÜÂà´Âà∞Êó•ÊúüÁõ∏ÂÖ≥Âàó: {date_cols}\n")
    for col in date_cols:
        try:
            if astype == "datetime":
                # ËΩ¨Êç¢‰∏∫datetimeÂØπË±°
                if verbose:
                    print(f"Â∞ÜÂàó '{col}' ËΩ¨‰∏∫datetime (Ê†ºÂºè: '{fmt}')")
                
                # Â∫îÁî®Ëá™ÂÆö‰πâËß£ÊûêÂáΩÊï∞
                df[col] = df[col].apply(
                    lambda x: parse_custom_date(x, fmt, century_threshold, verbose)
                )
                
            elif astype == "str":
                # ËΩ¨Êç¢‰∏∫Ê†ºÂºèÂåñÂ≠óÁ¨¶‰∏≤
                if pd.api.types.is_datetime64_any_dtype(df[col]):
                    if verbose:
                        print(f"Â∞ÜÂàó '{col}' ËΩ¨‰∏∫Â≠óÁ¨¶‰∏≤ (Ê†ºÂºè: '{fmt}')")
                    df[col] = df[col].dt.strftime(fmt)
                else:
                    # ÈùûÊó•ÊúüÁ±ªÂûãÂ∞ùËØïÂÖàËΩ¨Êó•ÊúüÂÜçËΩ¨Â≠óÁ¨¶‰∏≤
                    try:
                        temp = pd.to_datetime(df[col], errors='raise')
                        df[col] = temp.dt.strftime(fmt)
                        if verbose:
                            print(f"Â∞ÜÂàó '{col}' ÈÄöËøá‰∏≠Èó¥ËΩ¨Êç¢ËΩ¨‰∏∫Â≠óÁ¨¶‰∏≤")
                    except:
                        raise ValueError(f"Âàó '{col}' Êó†Ê≥ïËΩ¨Êç¢‰∏∫Êó•ÊúüÊ†ºÂºè")
            else:
                raise ValueError(f"Êó†ÊïàÁöÑastypeÂèÇÊï∞: {astype} (Âè™Êé•Âèó'str'Êàñ'datetime')")
                
        except Exception as e:
            if verbose:
                print(f"Âàó '{col}' ËΩ¨Êç¢Â§±Ë¥•: {str(e)}")
    try: 
        df_ = df_astype(
            df, columns=date_cols, astype="date", fmt=fmt, inplace=inplace, verbose=verbose, **kwargs
        )
        return df_
    except Exception as e:
        print(f"got final errors: {e}, ‰ΩÜÊòØÂ∞ùËØïÂÖ∂ÂÆÉÂäûÊ≥ï")
    
    return df

#! pd extensions 
@pd.api.extensions.register_dataframe_accessor("column")
class Column:
    def __init__(self, pandas_obj):
        self._df = pandas_obj
    
    def __call__(
        self, 
        keywords: Optional[Union[str, int, float, range, np.ndarray, List[Union[str, int, float, range, np.ndarray]]]] = None, 
        output: Literal['columns', 'list', 'dataframe', 'df'] = 'columns',
        fuzzy: bool = False,
    ) -> Union[List[str], pd.DataFrame]:
        """
        Get columns matching specified keyword(s)
        Parameters:
        -----------
        keywords : str or list of str, optional
            Keyword(s) to search in column names. If None, returns all columns.
        fuzzy : bool, default True
            Whether to ignore case when matching keywords
        output : str, default 'columns'
            Output format:
            - 'columns' or 'list': returns list of column names
            - 'dataframe' or 'df': returns DataFrame with selected columns
            
        Returns:
        --------
        Union[List[str], pd.DataFrame]
            Either list of column names or filtered DataFrame

        df = pd.DataFrame({
            'OrderDate': ['2023-01-01', '2023-01-02'],
            'delivery_time': ['10:00', '12:00'],
            'CustomerID': [1, 2],
            'DATE_RECEIVED': ['2023-01-03', '2023-01-04']
        })

        # Get all columns
        df.column()  
        # Returns: ['OrderDate', 'delivery_time', 'CustomerID', 'DATE_RECEIVED']

        # Get date-related columns (case insensitive)
        df.column("date")  
        # Returns: ['OrderDate', 'DATE_RECEIVED']

        # Get multiple keyword matches
        df.column(["date", "time"])  
        # Returns: ['OrderDate', 'delivery_time', 'DATE_RECEIVED']

        # Exact case matching
        df.column("DATE", fuzzy=False)  
        # Returns: ['DATE_RECEIVED']
        """
        # Get matching columns
        if keywords is None:
            cols = self._df.columns.tolist()
        else:
            if not isinstance(keywords, list):
                keywords = [keywords]

            # Expand range / np.arange into individual indices
            expanded_keywords = []
            for kw in keywords:
                if isinstance(kw, range) or (isinstance(kw, np.ndarray) and np.issubdtype(kw.dtype, np.integer)):
                    expanded_keywords.extend(list(kw))
                else:
                    expanded_keywords.append(kw)

            # correct fuzzy, if kw is int/float, then fuzzy is forced to True
            fuzzy = True if any([not isinstance(kw,str) for kw in expanded_keywords]) else fuzzy
            if not fuzzy:
                # Ê®°Á≥äÂ§ÑÁêÜ
                cols=[strcmp(kw.lower(), self._df.columns.tolist())[0] for kw in expanded_keywords]
            else:
                cols = []
                for kw in expanded_keywords:
                    if isinstance(kw, str):
                        # Match columns containing keyword
                        cols.extend(
                            [col for col in self._df.columns if kw.lower() in col.lower()]
                        )
                    elif isinstance(kw, (int, float, np.integer, np.floating)):
                        # Treat as column index
                        cols.append(self._df.columns[int(kw)])
                    else:
                        raise ValueError(f"Unsupported keyword type: {type(kw)}")

        output=strcmp(output.lower(),['columns', 'list', 'dataframe', 'df','all'])[0]
        # Handle output format
        if output in ('columns', 'list'):
            return cols
        elif output in ('dataframe', 'df','all'):
            return self._df[cols]
        else:
            raise ValueError(
                f"Invalid output format '{output}'. "
                "Use 'columns'/'list' or 'dataframe'/'df'"
            )
# ! Select is literally a copy of Column with output='df' as the default.
@pd.api.extensions.register_dataframe_accessor("select")
class Select:
    def __init__(self, pandas_obj):
        self._df = pandas_obj
    
    def __call__(self, *args, **kwargs):
        # Force output to 'df' unless user overrides
        kwargs.setdefault("output", "df")
        return self._df.column(*args, **kwargs)

#! =================df.apply_style(rules)========================= 
@pd.api.extensions.register_dataframe_accessor("apply_style")
class ApplyStyleAccessor:
    def __init__(self, pandas_obj: pd.DataFrame):
        self._obj = pandas_obj

    def __call__(
        self,
        style_rules: List[Dict],
        hover: Optional[Dict] = None,
        caption: Optional[str] = None,
        table_styles: Optional[List[Dict]] = None,
        default_alignment: Optional[Dict] = None,
        verbose:bool = False
    ):
        """
        Parameters:
        - style_rules: List of conditional formatting rules
        - hover: Dict of hover effects (e.g., {'color': 'red', 'background-color': 'yellow'})
        - caption: Table caption text
        - table_styles: List of table-wide styles

        Style Rule Properties:
        - Text: color, font-size, font-weight, font-style, font-family, text-decoration
        - Background: background-color, background-gradient, opacity
        - Borders: border, border-top, border-bottom, border-left, border-right
        - Alignment: text-align, vertical-align
        - Padding: padding, padding-top, padding-bottom, padding-left, padding-right
        - Transformation: text-transform, letter-spacing
        - Special: cursor, display, white-space

        """
        return _apply_style_core(
            self._obj,
            style_rules,
            hover=hover,
            caption=caption,
            table_styles=table_styles,
            default_alignment=default_alignment,
            verbose=verbose
        )

def _apply_style_core(
    df: pd.DataFrame,
    style_rules: List[Dict],
    hover: Optional[Dict] = None,
    caption: Optional[str] = None,
    table_styles: Optional[List[Dict]] = None,
    default_alignment: Optional[Dict] = None,
    verbose: bool = False
):
    if verbose:
        print("""
data = {
    "Name": ["Alice", "Bob", "Charlie", "David"],
    "Age": [25, 30, 35, 40],
    "Score": [92, 88, 76, 95],
    "Status": ["Active", "Inactive", "Pending", "Active"],
    "Join Date": pd.to_datetime(
        ["2020-01-01", "2019-05-15", "2021-03-10", "2018-11-20"]
    ),
}
df = pd.DataFrame(data)

# Comprehensive styling example
styled_df = df.apply_style(
    [
        {
            "column": "Score",
            "operator": ">=",
            "value": 90,
            "color": "red",
            "background-color": "green",
            "font-weight": "bold",
            "border": "2px solid darkgreen",
            "text-align": "center",
            "padding": "5px 10px",
        },
        {
            "column": "Status",
            "operator": "==",
            "value": "Active",
            "background-color": "lightblue",
            "text-decoration": "underline",
            "font-style": "italic",
        },
        {
            "column": "Age",
            "operator": ">",
            "value": 30,
            "color": "darkred",
            "font-size": "14px",
            "border-left": "3px solid orange",
        },
        {
            "index": 0,
            "operator": "==",
            "value": "Alice",
            "background-color": "lightyellow",
            "font-family": "Arial",
        },
    ],
    default_alignment={
        "header": {
            "text-align": "left",
            "vertical-align": "middle",
            "font-weight": "bold",
        },
        "cells": {"text-align": "center", "vertical-align": "top"},
    },
    # hover={
    #     "background-color": "#F3F3BA",  # Light yellow on hover
    #     "transition": "all 3s ease",  # Smooth transition
    # },
    caption="Employee Performance Data",
    table_styles=[
        {
            "selector": "th",
            "props": [
                ("background-color", "#333"),
                ("color", "white"),
                ("font-weight", "bold"),
                ("text-align", "center"),
            ],
        },
        {
            "selector": "tr:nth-child(even)",
            "props": [("background-color", "#f9f9f9")],
        },
    ],
)

display(styled_df)
delete("styled9.xlsx")
styled_df.to_excel("styled9.xlsx", sheet_name="style9", index=False)


# example:
df = pd.DataFrame(
    {
        "Event": ["Conference", "Workshop", "Deadline"],
        "Date": [
            "2023-06-15",
            "2022-07-01",
            "2026-05-31",
        ],  # pd.to_datetime(["2023-06-15", "2023-07-01", "2023-05-31"]),
    }
)

styled_df = df.apply_style(
    [
        # {
        #     "column": "Date",
        #     "operator": "before",
        #     "value": "2023-06-01",
        #     "background-color": "#ffcccc",
        #     "text": "‚ö†Ô∏è Past deadline",
        # },
        # {
        #     "column": "Date",
        #     "operator": "after",
        #     "value": pd.Timestamp.now(),
        #     "background-color": "#e6ffe6",
        #     "text": "‚Üë Upcoming",
        # },
        {
            "column": "Date",
            "operator": "between",
            "value": ["2022-06-01", "2025-06-30"],
            "border": "2px solid darkred",
        },
    ]
)
styled_df
              """)
    def _preprocess_cfg(style_rules: List[Dict]) -> List[Dict]:
        """Normalize and validate style rules with comprehensive CSS support"""
        if not isinstance(style_rules, list):
            style_rules = [style_rules]

        corrected_rules = []
        for rule in style_rules:
            normalized_rule = {}
            for key, value in rule.items():
                # Convert common aliases to standard CSS properties 
                prop_aliases = {
                    # Background properties
                    "bg": "background",
                    "bg_color": "background-color",
                    "background_color": "background-color",
                    "bg_image": "background-image",
                    "bg_repeat": "background-repeat",
                    "bg_position": "background-position",
                    "bg_size": "background-size",
                    "bg_attachment": "background-attachment",
                    "bg_gradient": "background-gradient",
                    "bg_blend_mode": "background-blend-mode",
                    
                    # Text properties
                    "color":"color",
                    "text_color": "color",
                    "text_align": "text-align",
                    "text_decoration": "text-decoration",
                    "text_transform": "text-transform",
                    "text_indent": "text-indent",
                    "text_shadow": "text-shadow",
                    "text_overflow": "text-overflow",
                    "letter_spacing": "letter-spacing",
                    "word_spacing": "word-spacing",
                    "line_height": "line-height",
                    "white_space": "white-space",
                    "direction": "direction",
                    
                    # Font properties
                    "font": "font",
                    "font_size": "font-size",
                    "font_weight": "font-weight",
                    "font_style": "font-style",
                    "font_family": "font-family",
                    "font_variant": "font-variant",
                    "font_stretch": "font-stretch",
                    
                    # Border properties (full set)
                    "border": "border",
                    "border_top": "border-top",
                    "border_right": "border-right",
                    "border_bottom": "border-bottom",
                    "border_left": "border-left",
                    "border_color": "border-color",
                    "border_width": "border-width",
                    "border_style": "border-style",
                    "border_radius": "border-radius",
                    "border_top_color": "border-top-color",
                    "border_right_color": "border-right-color",
                    "border_bottom_color": "border-bottom-color",
                    "border_left_color": "border-left-color",
                    "border_top_width": "border-top-width",
                    "border_right_width": "border-right-width",
                    "border_bottom_width": "border-bottom-width",
                    "border_left_width": "border-left-width",
                    "border_top_style": "border-top-style",
                    "border_right_style": "border-right-style",
                    "border_bottom_style": "border-bottom-style",
                    "border_left_style": "border-left-style",
                    "border_top_left_radius": "border-top-left-radius",
                    "border_top_right_radius": "border-top-right-radius",
                    "border_bottom_right_radius": "border-bottom-right-radius",
                    "border_bottom_left_radius": "border-bottom-left-radius",
                    
                    # Padding properties
                    "padding": "padding",
                    "padding_top": "padding-top",
                    "padding_right": "padding-right",
                    "padding_bottom": "padding-bottom",
                    "padding_left": "padding-left",
                    "padding_x": "padding-left",  # Horizontal (both left/right)
                    "padding_y": "padding-top",   # Vertical (both top/bottom)
                    
                    # Margin properties
                    "margin": "margin",
                    "margin_top": "margin-top",
                    "margin_right": "margin-right",
                    "margin_bottom": "margin-bottom",
                    "margin_left": "margin-left",
                    "margin_x": "margin-left",    # Horizontal (both left/right)
                    "margin_y": "margin-top",     # Vertical (both top/bottom)
                    
                    # Display properties
                    "display": "display",
                    "visibility": "visibility",
                    "opacity": "opacity",
                    "overflow": "overflow",
                    "overflow_x": "overflow-x",
                    "overflow_y": "overflow-y",
                    "z_index": "z-index",
                    
                    # Position properties
                    "position": "position",
                    "top": "top",
                    "right": "right",
                    "bottom": "bottom",
                    "left": "left",
                    
                    # Flexbox properties
                    "flex": "flex",
                    "flex_direction": "flex-direction",
                    "flex_wrap": "flex-wrap",
                    "flex_flow": "flex-flow",
                    "flex_grow": "flex-grow",
                    "flex_shrink": "flex-shrink",
                    "flex_basis": "flex-basis",
                    "justify_content": "justify-content",
                    "align_items": "align-items",
                    "align_self": "align-self",
                    "align_content": "align-content",
                    
                    # Grid properties
                    "grid": "grid",
                    "grid_template": "grid-template",
                    "grid_template_columns": "grid-template-columns",
                    "grid_template_rows": "grid-template-rows",
                    "grid_template_areas": "grid-template-areas",
                    "grid_column": "grid-column",
                    "grid_row": "grid-row",
                    "grid_area": "grid-area",
                    "grid_gap": "grid-gap",
                    "grid_column_gap": "grid-column-gap",
                    "grid_row_gap": "grid-row-gap",
                    
                    # Animation/transition properties
                    "transition": "transition",
                    "transition_property": "transition-property",
                    "transition_duration": "transition-duration",
                    "transition_timing": "transition-timing-function",
                    "transition_delay": "transition-delay",
                    "animation": "animation",
                    
                    # Special parameters (non-CSS)
                    "apply_to": "apply_to",
                    "operator": "operator",
                    "value": "value"
                }
                if key.lower() in ["column","index","columns","indexs","operator","op","value","apply_to"]:
                    normalized_key = key.lower().replace("-", "_")
                else:
                    normalized_key = strcmp(key.lower().replace("-", "_"),list(prop_aliases.keys()))[0]
                # print(f"normalized_key={normalized_key}")
                normalized_key = prop_aliases.get(normalized_key, normalized_key)
                normalized_rule[normalized_key] = value
            # print(normalized_rule)
            # Validate required fields
            if not all(k in normalized_rule for k in ("operator", "value")):
                raise ValueError("Each rule must contain 'operator' and 'value'")
            if not any(k in normalized_rule for k in ("column", "index")):
                raise ValueError("Each rule must contain either 'column' or 'index'")
            if "apply_to" not in normalized_rule:
                normalized_rule["apply_to"] = "cell"
            corrected_rules.append(normalized_rule)

        return corrected_rules

    def _apply_column_styles(x: pd.Series) -> List[str]:
        """Apply styles to each cell in a column or entire column/row"""
        styles = [""] * len(x)
        column_name = x.name
        
        # Check for column-level rules first
        for rule in column_rules:
            if column_name != rule["column"]:
                continue
                
            apply_to = rule.get("apply_to", "cell")
            
            # For column-level application
            if apply_to == "column":
                # Check if any cell in column meets condition
                if any(_evaluate_condition(val, rule["operator"], rule["value"]) for val in x):
                    return [_build_style_string(rule)] * len(x)
                continue
                
            # For row-level application (applied in _apply_index_styles)
            if apply_to == "row":
                continue
                
            # Default cell-level application
            for i, value in enumerate(x):
                try:
                    if _evaluate_condition(value, rule["operator"], rule["value"]):
                        styles[i] = _build_style_string(rule)
                except Exception as e:
                    print(f"Error applying style for {column_name}: {e}")
        
        return styles

    def _apply_index_styles(x: pd.Series) -> List[str]:
        """Apply styles to each cell in an index or entire row"""
        styles = [""] * len(x)
        index_name = x.name
        
        # Check for index-level rules first
        for rule in index_rules:
            if index_name != rule["index"]:
                continue
                
            apply_to = rule.get("apply_to", "cell")
            
            # For row-level application
            if apply_to == "row":
                # Check if any cell in row meets condition
                if any(_evaluate_condition(val, rule["operator"], rule["value"]) for val in x):
                    return [_build_style_string(rule)] * len(x)
                continue
                
            # Default cell-level application
            for i, value in enumerate(x):
                try:
                    if _evaluate_condition(value, rule["operator"], rule["value"]):
                        styles[i] = _build_style_string(rule)
                except Exception as e:
                    print(f"Error applying style for {index_name}: {e}")
        
        return styles
    def _evaluate_condition(value, op: str, val) -> bool:
        """
        Ultimate condition evaluator with comprehensive list support for datetime comparisons
        """
        import datetime
        op = op.lower()
        original_value = value
        
        # Helper function to safely convert to datetime
        def try_convert_to_datetime(obj, verbose=False):
            if verbose:
                print(f"Input type: {type(obj)}")
            
            # Handle list/tuple inputs
            if isinstance(obj, (list, tuple)):
                try:
                    return [pd.to_datetime(item) for item in obj]
                except:
                    if verbose:
                        print("List conversion failed")
                    return None
            
            # Handle single value inputs
            if isinstance(obj, (pd.Timestamp, datetime.datetime, datetime.date)):
                if verbose:
                    print("Already datetime type")
                return obj
            if isinstance(obj, str):
                try:
                    return pd.to_datetime(obj)
                except:
                    if verbose:
                        print("String conversion failed")
                    return None
            return None
        
        # Convert both values to datetime if possible
        value_dt = try_convert_to_datetime(value)
        val_dt = try_convert_to_datetime(val)
        
        # Special handling for 'between' operator with lists
        if op == "between":
            if not isinstance(val, (list, tuple)) or len(val) != 2:
                return False
                
            # Try numeric comparison first
            try:
                num_value = float(original_value)
                start = float(val[0])
                end = float(val[1])
                return start <= num_value <= end
            except (ValueError, TypeError):
                pass
                
            # Try datetime comparison
            value_dt = try_convert_to_datetime(original_value)
            range_dt = try_convert_to_datetime(val)
            if value_dt and all(x is not None for x in range_dt):
                return range_dt[0] <= value_dt <= range_dt[1]
                
            # Fall back to string comparison
            try:
                return str(val[0]) <= str(original_value) <= str(val[1])
            except:
                return False 
        
        # Handle datetime comparisons if both values are datetime-like
        if value_dt is not None and val_dt is not None:
            try:
                if op in ("before", "<", "earlier"):
                    return value_dt < val_dt
                elif op in ("after", ">", "later"):
                    return value_dt > val_dt
                elif op in ("on_or_before", "<=", "before_or_on"):
                    return value_dt <= val_dt
                elif op in ("on_or_after", ">=", "after_or_on"):
                    return value_dt >= val_dt
                elif op == "same_day":
                    return value_dt.date() == val_dt.date()
            except Exception as e:
                print(f"Datetime comparison error: {e}")
        
        # Fall back to regular comparison
        try:
            if op in ("==", "=", "is"):
                return original_value == val
            elif op in ("!=", "~="):
                return original_value != val
            elif op == ">":
                return original_value > val
            elif op == "<":
                return original_value < val
            elif op == ">=":
                return original_value >= val
            elif op == "<=":
                return original_value <= val
            elif op in ("in", "include", "including"):
                if isinstance(val, (list, tuple, set)):
                    return original_value in val
                return isinstance(original_value, str) and (str(val) in original_value)
            elif op == "not in":
                if isinstance(val, (list, tuple, set)):
                    return original_value not in val
                return isinstance(original_value, str) and (str(val) not in original_value)
            elif op == "contains":
                return isinstance(original_value, str) and str(val) in original_value
            elif op == "not contains":
                return isinstance(original_value, str) and str(val) not in original_value
            elif op == "startswith":
                return isinstance(original_value, str) and original_value.startswith(str(val))
            elif op == "endswith":
                return isinstance(original_value, str) and original_value.endswith(str(val))
            elif op == "isna":
                return pd.isna(original_value)
            elif op == "notna":
                return not pd.isna(original_value)
        except Exception as e:
            print(f"Comparison error: {e}")
        
        return False
    def _build_style_string(rule: Dict) -> str:
        """Build CSS string from rule dictionary"""
        style_parts = []
        for prop, val in rule.items():
            if prop in ["column", "index", "operator", "value","apply_to"]:
                continue
            style_parts.append(f"{prop}: {val}")
        return "; ".join(style_parts)
    def _build_style_props(rule: Dict) -> Dict:
        """Convert style rule to properties dictionary"""
        return {k: v for k, v in rule.items() 
                if k not in ["column", "index", "operator", "value", "apply_to"]}
    def _apply_default_alignment(styler, alignment: Dict):
        """Apply default alignment to headers and cells"""
        if not alignment:
            return styler

        # Process header alignment
        header_align = alignment.get("header", {})
        if header_align:
            if "align" in header_align:
                # Handle combined alignment shortcut
                align = header_align.pop("align")
                header_align.setdefault("text-align", align)
                header_align.setdefault("vertical-align", align)

            header_styles = [
                {
                    "selector": "th",
                    "props": [
                        (k.replace("_", "-"), v) for k, v in header_align.items()
                    ],
                }
            ]
            styler = styler.set_table_styles(header_styles, overwrite=False)

        # Process cell alignment
        cell_align = alignment.get("cells", {})
        if cell_align:
            if "align" in cell_align:
                # Handle combined alignment shortcut
                align = cell_align.pop("align")
                cell_align.setdefault("text-align", align)
                cell_align.setdefault("vertical-align", align)

            cell_styles = [
                {
                    "selector": "td",
                    "props": [(k.replace("_", "-"), v) for k, v in cell_align.items()],
                }
            ]
            styler = styler.set_table_styles(cell_styles, overwrite=False)

        return styler
    def _apply_styles(df: pd.DataFrame, styler):
        """Core function to apply all styles"""
        processed_rules = _preprocess_cfg(style_rules)
        
        # Apply column-level styles
        for rule in [r for r in processed_rules if r.get("apply_to") == "column"]:
            col = rule["column"]
            mask = df[col].apply(lambda x: _evaluate_condition(x, rule["operator"], rule["value"]))
            if mask.any():
                props = _build_style_props(rule)
                styler = styler.set_properties(subset=pd.IndexSlice[:, col], **props)
        
        # Apply row-level styles
        for rule in [r for r in processed_rules if r.get("apply_to") == "row"]:
            if "column" in rule:
                col = rule["column"]
                mask = df[col].apply(lambda x: _evaluate_condition(x, rule["operator"], rule["value"]))
            else:  # index-based
                idx = rule["index"]
                mask = df.index == idx
                
            for i in df.index[mask]:
                props = _build_style_props(rule)
                styler = styler.set_properties(subset=pd.IndexSlice[i, :], **props)
        
        # Apply cell-level styles
        for rule in [r for r in processed_rules if r.get("apply_to", "cell") == "cell"]:
            if "column" in rule:
                col = rule["column"]
                mask = df[col].apply(lambda x: _evaluate_condition(x, rule["operator"], rule["value"]))
                for i in df.index[mask]:
                    props = _build_style_props(rule)
                    styler = styler.set_properties(subset=pd.IndexSlice[i, col], **props)
            else:  # index-based
                idx = rule["index"]
                props = _build_style_props(rule)
                styler = styler.set_properties(subset=pd.IndexSlice[idx, :], **props)
        
        return styler

    # Create base styler
    styler = df.style
    styler = _apply_styles(df, styler)
    if default_alignment:
        styler = _apply_default_alignment(styler, default_alignment)
 
    # Apply additional table styling
    if hover:
        styler = styler.set_properties(**hover, subset=pd.IndexSlice[:, :])

    if caption:
        styler = styler.set_caption(caption)

    if table_styles:
        styler = styler.set_table_styles(table_styles)

    return styler

#! =================df.search( )=========================
@pd.api.extensions.register_dataframe_accessor("search")
class SearchAccessor:
    def __init__(self, pandas_obj):
        self._df = pandas_obj

    def __call__(
        self,
        columns: Union[str, int, List[str], List[int],List[float]],
        pattern: Union[str, List[str]],
        how: str = "auto",
        engine: str = "auto",
        ignore_case: bool = True,
        pattern_any: bool = True,
        column_any: bool = True,
        na: Union[bool, object] = False,
        flags: int = 0,
        escape: bool = False,
        return_mask:bool = False,
        verbose: bool = True
    ) -> pd.Series:
        r"""
        Advanced multi-column search with regex and fnmatch support.
            If all patterns are simple strings without regex special chars or wildcards (*, ?, [, ]), use 'exact' or 'contains' depending on whether the pattern looks like a full match or substring.
            If patterns start or end with wildcards (*), use 'startswith' or 'endswith' with fnmatch engine.
            Otherwise, fallback to regex 'contains'.
            Engine 'auto':
            If patterns include wildcards (*, ?, [), choose fnmatch.
            Else if patterns contain regex special chars (. ^ $ * + ? { } [ ] \ | ( )), choose re.
            Otherwise, fnmatch or re can both work, but default to fnmatch for simpler matching.
        Parameters
        ----------
        columns : str,int or list of str/int
            Column(s) to search. Can be a single column name or list of columns.
        pattern : str or list of str
            Pattern(s) to search for. Can be a single pattern or list of patterns.
        how : {'contains', 'startswith', 'endswith', 'exact', 'match'}, default 'contains'
            Search method:
            - 'contains': Pattern contained anywhere in string
            - 'startswith': String starts with pattern
            - 'endswith': String ends with pattern
            - 'exact': Full string exact match
            - 'match': Pattern matches from start (regex only)
        engine : {'re', 'fnmatch'}, default 're'
            Pattern matching engine:
            - 're': Regular expressions (full regex support)
            - 'fnmatch': Unix shell-style wildcards (*, ?, [])
        ignore_case : bool, default True
            Case-sensitive search. When False, ignores case differences.
        pattern_any : bool, default True
            For multi-pattern searches:
            - True: Match ANY pattern (OR logic)
            - False: Match ALL patterns (AND logic)
        column_any : bool, default True
            For multi-column searches:
            - True: Match ANY column (OR logic)
            - False: Match ALL columns (AND logic)
        na : scalar, default False
            Value to return for NA values. Set to True to treat NAs as matches.
        flags : int, default 0
            Regex flags (re.IGNORECASE, re.MULTILINE, etc.) for 're' engine.
        escape : bool, default False
            Escape regex special characters automatically when using 're' engine.
        return_mask: bool, default False, return the DataFrame
        Returns
        -------
        pd.Series
            Boolean mask of rows matching the search criteria.

        Examples
        --------
            # 1. Basic text search
            print("\n1. Names containing 'John':")
            display(df[df.search("name", "A")])
            # 2. Case-insensitive search
            print("\n2. Emails containing 'GMAIL' (case-insensitive):")
            print(df[df.search("email", "org", ignore_case=False)][["email"]])
            # 3. Exact match
            print("\n3. Exactly 'IT' department:")
            print(df[df.search("department", "IT", how="exact")][["department", "name"]])
            # 4. Multi-column search (OR)
            print("\n4. 'urgent' in comments OR 'critical' in filename:")
            mask = df.search(["comments", "filename"], ["urgent", "critical"], ignore_case=False)
            print(df[mask][["comments", "filename"]])
            # 5. Multi-pattern AND search
            print("\n5. Comments containing BOTH 'urgent' AND 'client':")
            mask = df.search("comments", ["urgent", "client"], pattern_any=False, ignore_case=False)
            print(df[mask][["comments"]])

            # 6. Starts with
            print("\n6. Filenames starting with 'backup':")
            mask = df.search("filename", "backup*", how="startswith", engine="fnmatch")
            print(df[mask][["filename"]])

            # 7. Ends with
            print("\n7. Files ending with '.txt':")
            mask = df.search("filename", "*.txt", how="endswith", engine="fnmatch")
            print(df[mask][["filename"]])

            # 8. Regex pattern
            print("\n8. Phone numbers matching US pattern (###-###-####):")
            mask = df.search("phone", r"\d{3}-\d{3}-\d{4}")
            print(df[mask][["phone"]])

            # 9. Fnmatch character sets
            print("\n9. Filenames matching '[abc]*.txt':")
            mask = df.search("filename", "[abc]*.txt", engine="fnmatch")
            print(df[mask][["filename"]])

            # 10. Multi-column AND + Multi-pattern OR
            print("\n10. IT department AND ('active' OR 'pending' status):")
            dept_mask = df.search("department", "IT", how="exact")
            status_mask = df.search("status", ["active", "pending"], pattern_any=True)
            print(df[dept_mask & status_mask][["department", "status", "name"]])

            # 11. NA handling
            print("\n11. Rows with missing email OR 'pending' status:")
            email_mask = df.search("email", ".*", na=True)  # Match all including NA
            status_mask = df.search("status", "pending", how="exact")
            print(df[email_mask | status_mask][["email", "status"]])

            # 12. Complex combination
            print("\n12. Critical issues in IT or Support departments:")
            critical_mask = df.search("comments", "critical", ignore_case=False)
            dept_mask = df.search("department", ["IT", "Support"], pattern_any=True)
            print(df[critical_mask & dept_mask][["department", "comments"]])

            # 13. Escape special characters
            print("\n13. Comments containing literal '.*txt':")
            df_test = pd.DataFrame({"text": ["Backup.*txt required", "Normal text"]})
            mask = df_test.search("text", ".*txt", escape=True)
            print(df_test[mask])

            # 14. Match method
            print("\n14. Names starting with 'Dr.':")
            mask = df.search("name", "Dr\\.", how="match")  # Regex match from start
            print(df[mask][["name"]])

            # 15. Fnmatch exact pattern
            print("\n15. Filenames exactly matching 'data.csv':")
            mask = df.search("filename", "data.csv", how="exact", engine="fnmatch")
            print(df[mask][["filename"]])
        """

        VERBOSE = verbose
        engine=strcmp(str(engine).lower(), ["re","fnmatch","auto"])[0]
        how=strcmp(how, ['auto','contains', 'startswith', 'endswith', 'exact', 'match'])[0]

        # Validate parameters
        if columns!=0 and not columns:
            raise ValueError("At least one column must be specified")
        if not pattern:
            raise ValueError("At least one pattern must be specified") 

        # Normalize inputs to lists
        # columns = [columns] if isinstance(columns, str) else columns
        columns = self._df.column(columns)
        patterns = [pattern] if isinstance(pattern, str) else pattern

        # Decide engine if auto
        if engine == "auto":
            # If any pattern has fnmatch wildcards, use fnmatch
            if any(contains_wildcards(p) for p in patterns):
                engine = "fnmatch"
            # Else if any pattern has regex specials, use regex
            elif any(contains_regex_special(p) for p in patterns):
                engine = "re"
            else:
                # default simpler matching
                engine = "re"
            print(f"[auto] engine='{engine}'") if VERBOSE else None
        if engine=='fnmatch':
            try:
                import fnmatch
            except Exception as e:
                engine = "re"
                print(f"{e}, FALL-BACK to {engine}")

        if how == "auto":
            if all(contains_wildcards(p) for p in patterns):
                if all(p.startswith("*") and not p.endswith("*") for p in patterns):
                    how = "endswith"
                elif all(p.endswith("*") and not p.startswith("*") for p in patterns):
                    how = "startswith"
                elif all(p.startswith("*") and p.endswith("*") for p in patterns):
                    how = "contains"
                else:
                    how = "contains"  # Wildcards but not consistent => use contains
            elif all(not contains_wildcards(p) and not contains_regex_special(p) for p in patterns):
                how = "exact"  # Simple, literal patterns
            else:
                how = "contains"  # Fallback
            print(f"[auto] how='{how}'") if VERBOSE else None
        
        # Prepare flags
        flags = flags | re.IGNORECASE if ignore_case else flags

        # Escape patterns if requested and using regex
        if engine == "re" and escape:
            patterns = [re.escape(p) for p in patterns]

        # Convert fnmatch patterns to regex
        if engine == "fnmatch":
            patterns = [fnmatch.translate(p) for p in patterns] 

        # Process each column
        column_masks = []
        for col in columns:
            s = self._df[col].astype(str)
            pattern_masks = []

            for pat in patterns:
                if how == "contains":
                    mask = s.str.contains(pat, regex=True, flags=flags, na=na)
                elif how == "startswith":
                    mask = s.str.match(f"^{pat}", na=na)
                elif how == "endswith":
                    mask = s.str.contains(f"{pat}$", flags=flags, na=na)
                elif how == "exact":
                    mask = s.str.fullmatch(pat, flags=flags, na=na)
                elif how == "match":
                    mask = s.str.match(pat, na=na)
                pattern_masks.append(mask)

            # Combine patterns for this column
            combined = pd.concat(pattern_masks, axis=1)
            col_mask = combined.any(axis=1) if pattern_any else combined.all(axis=1)
            column_masks.append(col_mask)

        # Combine columns
        final = pd.concat(column_masks, axis=1)
        # verbose
        print(f"[FINAL] engine='{engine}', how='{how}', ignore_case={ignore_case}, pattern_any={pattern_any}, column_any={column_any}, flags={flags}, na={na}, escape={escape}\ncolumns={columns}") if VERBOSE else None
        mask = final.any(axis=1) if column_any else final.all(axis=1) 
        return mask if return_mask else self._df[mask]
    
def df_merge(
    df1: pd.DataFrame,
    df2: pd.DataFrame,
    use_index: bool = False,
    columns: list = ["col_left", "col_right"],
    how: str = "left",
    fuzz:bool= False,
    verbose:bool=True,
) -> pd.DataFrame:
    """
    Merges two DataFrames based on either the index or shared columns with matching data types.
    usage:
        #(1) if the index are the same
            df_merged = df_merge(df1, df2, use_index=True(defalut), how='outer')
        #(2) if there are shaed columns, then based on shared columns
            df_merged = df_merge(df1, df2, how='outer')
        #(3) if columns: then based on the specific columns
            df_merged = df_merge(df1, df2, columns=["col_left", "col_right"],how='outer')
    Parameters:
    - df1 (pd.DataFrame): The first DataFrame.
    - df2 (pd.DataFrame): The second DataFrame.
    - use_index (bool): If True, first try to merge by index if they are comparable; otherwise, fall back to column-based merge.
    - how (str): Type of merge to perform: 'inner', 'outer', 'left', or 'right'. Default is 'inner'.
    'inner': only the rows that have matching values in both DataFrames (intersection)
    'outer': keeps all rows from both DataFrames and fills in missing values with NaN
    'left': keeps all rows from the left DataFrame and matches rows from the right DataFrame
    'right': keeps all rows from the right DataFrame and matches rows from the left DataFrame, filling with NaN if there is no match.

    Returns:
    - pd.DataFrame: The merged DataFrame.
    """
    if fuzz:
        # try to merge them based on the columns, but not exact the same, but similar
        df1_copy=df1.copy()
        for i in df1_copy[columns[0]]:
            if verbose:
                print(f"double checking: {i}--------->{strcmp(i,df2[columns[1]].tolist())[0]}")
            df1_copy.loc[df1_copy[columns[0]]==i,columns[0]+"_corr"]=strcmp(i,df2[columns[1]].tolist() )[0]
        columns[0]=columns[0]+"_corr"
        df_=df_merge(df1_copy,df2,columns=columns)
        # drop the 'corr' column
        df_ = df_.drop(columns=[columns[0]])
        return df_
        
    # 1. Check if indices are comparable (same length and types)
    if use_index:
        print(f"Merging based on index using '{how}' join...")
        df_merged = pd.merge(df1, df2, left_index=True, right_index=True, how=how)
        return df_merged

    # 2. Find common columns with the same dtype
    common_columns = df1.columns.intersection(df2.columns)
    shared_columns = []
    for col in common_columns:
        try:
            if df1[col].dtype == df2[col].dtype:
                shared_columns.append(col)
        except Exception as e:
            print(e)
            pass
            
    if not isinstance(columns, list):
        columns = [columns]
    if len(columns) != 2:
        raise ValueError(
            "'columns':list shoule be a list: columns=['col_left','col_right']"
        )
    if all(columns):
        if verbose:
            print(f"Merging based on columns: {columns} using '{how}' join...")
        df_merged = pd.merge(df1, df2, left_on=columns[0], right_on=columns[1], how=how)
    elif shared_columns:
        if verbose:
            print(
                f"Merging based on shared columns: {shared_columns} using '{how}' join..."
            )
        df_merged = pd.merge(df1, df2, on=shared_columns, how=how)
    else:
        raise ValueError(
            "No common columns with matching data types to merge on, and indices are not comparable."
        )
    return df_merged


def df_drop_duplicates(
    data: pd.DataFrame,
    by: Union[str, List[str]] = "index",  # Options: 'index', or column name(s) for 'rows'
    keep="first",  # Options: 'first', 'last', or False (drop all duplicates)
    ignore_index=True,
    inplace: bool = False,
    verbose=True,
):
    """
    data (pd.DataFrame): DataFrame to drop duplicates from.
    by (str): Specify by to drop duplicates:
                 - 'index': Drop duplicates based on the DataFrame index.
                 - Column name(s) for row-wise duplicate checking.
    keep (str): Which duplicates to keep:
        'first',
        'last',
        False (drop all duplicates).
    inplace (bool): Whether to modify the original DataFrame in place.
    """
    original_shape = data.shape
    if by == "index":
        # Drop duplicates in the index
        result = data[~data.index.duplicated(keep=keep)]
    else:
        # Drop duplicates row-wise based on column(s)
        result = data.drop_duplicates(subset=by, keep=keep, ignore_index=ignore_index)
    if original_shape != result.shape or verbose:
        print(f"\nshape:{original_shape} (before drop_duplicates)")
        print(f"shape:{result.shape} (after drop_duplicates)")
    if inplace:
        # Modify the original DataFrame in place
        data.drop(data.index, inplace=True)  # Drop all rows first
        data[data.columns] = result  # Refill the DataFrame
        return None
    else:
        return result


#! fillna()
def df_fillna(
    data: pd.DataFrame,
    method: str = "knn",
    axis: int = 0,  # column-wise
    constant: float = None,
    n_neighbors: int = 5,  # KNN-specific
    max_iter: int = 10,  # Iterative methods specific
    inplace: bool = False,
    random_state: int = 1,
) -> pd.DataFrame:
    """
    Fill missing values in a DataFrame using specified imputation method.

    Parameters:
    data (pd.DataFrame): The DataFrame to fill missing values.
    method (str): The imputation method to use. Options are:
        - 'mean': Replace missing values with the mean of the column.
        - 'median': Replace missing values with the median of the column.
        - 'most_frequent': Replace missing values with the most frequent value in the column.
        - 'constant': Replace missing values with a constant value provided by the `constant` parameter.
        - 'knn': Use K-Nearest Neighbors imputation; replaces missing values based on the values of the nearest neighbors
        - 'iterative': Use Iterative imputation; each feature with missing values as a function of other features and estimates them iteratively
        - 'mice' (Multivariate Imputation by Chained Equations): A special case of iterative imputation.
        # - 'missforest': A random forest-based imputation method. Uses a random forest model to predict and fill missing values
        # - 'softimpute': Matrix factorization imputation.A matrix factorization technique where missing values are imputed by
        #       reconstructing the data matrix using low-rank approximation
        # - EM (Expectation-Maximization): Often used in advanced statistics to estimate missing values in a probabilistic framework.
        # - 'svd': Use IterativeSVD (matrix factorization via Singular Value Decomposition).

    axis (int): The axis along which to impute:
        - 0: Impute column-wise (default).
        - 1: Impute row-wise.
    constant (float, optional): Constant value to use for filling NaNs if method is 'constant'.
    inplace (bool): If True, modify the original DataFrame. If False, return a new DataFrame.

    """
    if isinstance(data, pd.Series):
        data = pd.DataFrame(data)
    # handle None
    for col in data.columns:
        data[col] = data[col].apply(lambda x: np.nan if x is None else x)

    # Fill completely NaN columns with a default value (e.g., 0)
    data = data.copy()
    data.loc[:, data.isna().all()] = 0

    col_names_org = data.columns.tolist()
    index_names_org = data.index.tolist()
    # Separate numeric and non-numeric columns
    numeric_data = data.select_dtypes(include=[np.number])
    non_numeric_data = data.select_dtypes(exclude=[np.number])

    if data.empty:
        raise ValueError("Input DataFrame is empty.")

    # Validate method
    methods = [
        "mean",
        "median",
        "most_frequent",
        "constant",
        "knn",
        "iterative",
    ]  # ,"missforest","softimpute","svd"]
    method = strcmp(method, methods)[0]

    # If using constant method, ask for a constant value
    if constant is not None:
        method = "constant"
        try:
            constant = float(constant)
        except ValueError:
            raise ValueError("Constant value must be a number.")

    # Initialize SimpleImputer with the chosen method
    if method == "constant":
        from sklearn.impute import SimpleImputer

        imputer = SimpleImputer(strategy=method, fill_value=constant)
    elif method == "knn":
        from sklearn.impute import KNNImputer

        imputer = KNNImputer(n_neighbors=n_neighbors)
    elif method == "iterative" or method == "mice":
        from sklearn.experimental import enable_iterative_imputer
        from sklearn.impute import IterativeImputer

        imputer = IterativeImputer(max_iter=max_iter, random_state=random_state)
    else:  # mean, median, most_frequent
        from sklearn.impute import SimpleImputer

        imputer = SimpleImputer(strategy=method)

    # Fit and transform the data
    if axis == 0:
        # Impute column-wise
        imputed_data = imputer.fit_transform(numeric_data)
    elif axis == 1:
        # Impute row-wise
        imputed_data = imputer.fit_transform(numeric_data.T)
    else:
        raise ValueError("Invalid axis. Use 0 for columns or 1 for rows.")

    imputed_data = pd.DataFrame(
        imputed_data if axis == 0 else imputed_data.T,
        index=numeric_data.index if axis == 0 else numeric_data.columns,
        columns=numeric_data.columns if axis == 0 else numeric_data.index,
    )
    for col in imputed_data.select_dtypes(include=[np.number]).columns:
        imputed_data[col] = imputed_data[col].astype(numeric_data[col].dtype)

    # Handle non-numeric data imputation
    if not non_numeric_data.empty:
        from sklearn.impute import SimpleImputer

        if method == "constant":
            non_numeric_imputer = SimpleImputer(
                strategy="constant", fill_value=constant
            )
        else:
            non_numeric_imputer = SimpleImputer(strategy="most_frequent")

        # Impute non-numeric columns column-wise (axis=0)
        imputed_non_numeric = non_numeric_imputer.fit_transform(non_numeric_data)

        # Convert imputed non-numeric array back to DataFrame with original index and column names
        imputed_non_numeric_df = pd.DataFrame(
            imputed_non_numeric,
            index=non_numeric_data.index,
            columns=non_numeric_data.columns,
        )
    else:
        imputed_non_numeric_df = pd.DataFrame(index=data.index)

    imputed_data = pd.concat([imputed_data, imputed_non_numeric_df], axis=1).reindex(
        columns=data.columns
    )

    if inplace:
        # Modify the original DataFrame
        data[:] = imputed_data[col_names_org]
        return None
    else:
        # Return the modified DataFrame
        return imputed_data[col_names_org]

def df_cut(
    df: pd.DataFrame,
    column: str,
    *,
    new_col_name: Optional[str] = None,
    bins: Optional[Union[int, List[float], Dict[str, Union[float, str, pd.Timestamp]]]] = None,
    range_start: Optional[Union[float, str, pd.Timestamp]] = None,
    range_end: Optional[Union[float, str, pd.Timestamp]] = None,
    step: Optional[Union[float, str, pd.Timedelta]] = None,
    labels: Optional[List[str]] = None,
    label_format: Optional[Union[str, Callable[[float, float], str]]] = None,
    include_overflow: bool = True,
    include_underflow: bool = False,
    right: bool = False,
    drop_original: bool = False,
    precision: int = 2,
    show_count: bool = False,
    symbol_count: str = "n=",
    show_percentage: bool = False,
    symbol_percentage: str = "%",
    show_total_count: bool = False,
    symbol_total_count: str = "‚àën=",
    sep_between: str = " | ",
    sort_labels: bool = True,
    na_action: str = "keep",
    na_fill_value: Optional[str] = None,
    dtype: Optional[Union[str, pd.CategoricalDtype]] = None,
    ordered: bool = True,
    inplace: bool = False,
    datetime_format: str = "%Y-%m-%d",
    categorical_agg: str = "count",
) -> Optional[pd.DataFrame]:
    """
            Enhanced binning function that works with numeric, datetime, and categorical columns.

            Features:
            - Automatic type detection (numeric, datetime, categorical)
            - Flexible bin specification (number of bins, explicit edges, or range+step)
            - Customizable labels with formatting
            - Count and percentage display options
            - NA value handling
            square bracket: means inclusive
            parenthesis: means exclusive
            Parameters:
            -----------
            df : pd.DataFrame
                Input DataFrame containing the column to bin
            column : str
                Name of column to bin
            new_col_name : str, optional
                Name for binned column (default: f"{column}_binned")
            bins : int, list, or dict, optional
                - int: Number of equal-width bins
                - list: Explicit bin edges
                - dict: {'start': x, 'end': y, 'step': z} for range specification
            range_start : float or datetime-like, optional
                Start value for bin range (required if bins is None or dict)
            range_end : float or datetime-like, optional
                End value for bin range (default: max of column)
            step : float or timedelta-like, optional
                Step size for bin creation (required if bins is None or dict)
            labels : list of str, optional
                Custom labels for bins (must match number of bins)
            label_format : str or callable, optional
                Format string or function for bin labels
            include_overflow : bool, default True
                Include catch-all bin for values above range_end
            include_underflow : bool, default False
                Include catch-all bin for values below range_start
            right : bool, default False
                Whether bins include the right edge
            drop_original : bool, default False
                Drop original column after binning
            precision : int, default 2
                Decimal precision for numeric bin labels
            show_count : bool, default False
                Show count of items in each bin
            show_percentage : bool, default False
                Show percentage of items in each bin
            show_total_count : bool, default False
                Show total count in labels
            na_action : str, default 'keep'
                How to handle NA values ('keep', 'drop', or 'fill')
            na_fill_value : str, optional
                Value to fill NAs with if na_action='fill'
            dtype : dtype or CategoricalDtype, optional
                Output dtype for binned column
            ordered : bool, default True
                Whether bins are ordered
            inplace : bool, default False
                Modify DataFrame in place
            datetime_format : str, default "%Y-%m-%d"
                Format string for datetime labels
            categorical_agg : str, default 'count'
                For categorical data: 'count' or 'ratio'

            Returns:
            --------
            pd.DataFrame or None
                Returns modified DataFrame unless inplace=True

            Examples:
            --------
            # Numeric binning
            df_cut(df, 'age', bins=5)
            df_cut(df, 'price', range_start=0, range_end=1000, step=100)

            # Datetime binning
            df_cut(df, 'date', bins={'start': '2023-01-01', 'end': '2023-12-31', 'step': '1M'})

            # Categorical binning
            df_cut(df, 'category', bins=5, categorical_agg='ratio')

    # Sample datetime data
    dates = pd.date_range("2020-01-01", "2023-12-31", freq="D")
    df = pd.DataFrame(
        {
            "order_date": np.random.choice(dates, 500),
            "delivery_time": np.random.randint(1, 72, 500),  # hours
        }
    )
    # Example 1: Monthly bins
    # Monthly binning with exact month boundaries
    df_cut(
        df,
        "order_date",
        bins={"start": "2019-01-01", "end": "2023-12-31", "step": "1Y"},
        datetime_format="%Y-%m-%d",
        label_format="%m-%d",
        show_count=True,
        show_percentage=True,
        show_total_count=True,
    )
    # Weekly binning
    df_cut(
        df,
        "order_date",
        bins={"start": "2019-01-01", "end": "2023-12-31", "step": "1W"},
        label_format="%Y-%m-%d",
        datetime_format="%Y-%m-%d",
        show_count=True,
        show_percentage=True,
        show_total_count=True,
    )


    # Sample numeric data
    df = pd.DataFrame(
        {"price": np.random.uniform(10, 1000, 1000), "age": np.random.randint(18, 80, 1000)}
    )

    # Example 1: Equal-width bins
    df_cut(df, "price", bins=5, show_count=True)

    # Example 2: Custom range with step
    df_cut(
        df,
        "price",
        range_start=0,
        range_end=1000,
        step=200,
        label_format="${left:.0f}-${right:.0f}",
        show_percentage=True,
    )
    df_cut(
        df,
        "price",
        bins={"start": 0, "end": 1000, "step": 200},
        # label_format="${left:.0f}-${right:.0f}",
        show_percentage=True,
    )
    """
    from pandas.api.types import is_numeric_dtype, is_datetime64_any_dtype

    def _process_time_step(step: Union[str, int, float, pd.Timedelta]) -> str:
        """Convert step to pandas frequency string."""
        if isinstance(step, pd.Timedelta):
            return step.freqstr if step.freqstr else str(step)

        if isinstance(step, (int, float)):
            return f"{step}S"  # Interpret numbers as seconds

        if isinstance(step, str):
            step = step.strip().lower()
            match = re.match(r"(\d*\.?\d+)?\s*([a-z]+)", step)
            if not match:
                raise ValueError(f"Invalid time step format: {step}")

            num_part, unit_part = match.groups()
            num = float(num_part) if num_part else 1.0

            unit_map = {
                "y": "Y",
                "yr": "Y",
                "yrs": "Y",
                "year": "Y",
                "years": "Y",
                "m": "M",
                "mo": "M",
                "mon": "M",
                "month": "M",
                "months": "M",
                "w": "W",
                "wk": "W",
                "wks": "W",
                "week": "W",
                "weeks": "W",
                "d": "D",
                "day": "D",
                "days": "D",
                "h": "H",
                "hr": "H",
                "hrs": "H",
                "hour": "H",
                "hours": "H",
                "min": "T",
                "mins": "T",
                "minute": "T",
                "minutes": "T",
                "s": "S",
                "sec": "S",
                "secs": "S",
                "second": "S",
                "seconds": "S",
            }

            if unit_part not in unit_map:
                raise ValueError(f"Unknown time unit: {unit_part}")

            freq = unit_map[unit_part]
            if num.is_integer():
                num = int(num)
            return f"{num}{freq}"

        raise TypeError(f"Unsupported step type: {type(step)}")


    def _process_datetime_column(
        col: pd.Series,
        bins: Optional[Union[int, List[pd.Timestamp]]],
        range_start: Optional[Union[str, pd.Timestamp]],
        range_end: Optional[Union[str, pd.Timestamp]],
        step: Optional[Union[str, pd.Timedelta]],
        labels: Optional[List[str]],
        label_format: Optional[Union[str, Callable]],
        datetime_format: str,
        right: bool,
        include_underflow: bool,
        include_overflow: bool,
    ) -> Tuple[pd.Categorical, List[str]]:
        """Process datetime column with accurate counting."""
        col = pd.to_datetime(col)

        # Handle bin edges
        if bins is None:
            if step is None:
                raise ValueError("Step must be provided for datetime binning")

            # Convert step to pandas frequency string
            step_freq = _process_time_step(step)

            # Set default range if needed
            range_start = (
                pd.to_datetime(range_start) if range_start is not None else col.min()
            )
            range_end = pd.to_datetime(range_end) if range_end is not None else col.max()

            # Generate bins
            try:
                bin_edges = pd.date_range(start=range_start, end=range_end, freq=step_freq)
                if len(bin_edges) == 0:
                    bin_edges = pd.date_range(start=range_start, end=range_end, periods=2)
                elif bin_edges[-1] < range_end:
                    bin_edges = bin_edges.append(pd.DatetimeIndex([range_end]))
            except ValueError as e:
                raise ValueError(f"Invalid frequency specification: {step_freq}") from e
        elif isinstance(bins, int):
            bin_edges = pd.date_range(start=col.min(), end=col.max(), periods=bins + 1)
        else:
            bin_edges = pd.to_datetime(bins)

        # Add overflow/underflow bins
        if include_underflow:
            bin_edges = bin_edges.insert(0, pd.Timestamp.min)
        if include_overflow:
            bin_edges = bin_edges.append(pd.DatetimeIndex([pd.Timestamp.max]))

        # Perform the cut - this is where we ensure proper binning
        binned = pd.cut(
            col.astype("int64"),  # Convert to nanoseconds for precise binning
            bins=bin_edges.astype("int64"),
            right=right,
            include_lowest=True,
        )

        # Generate labels if not provided
        if labels is None:
            labels = []
            for i in range(len(bin_edges) - 1):
                left = bin_edges[i]
                right_ = bin_edges[i + 1]

                # Handle special cases
                if left == pd.Timestamp.min:
                    left_str = "<"
                else:
                    left_str = left.strftime(datetime_format)

                if right_ == pd.Timestamp.max:
                    right_str = ">"
                else:
                    right_str = right_.strftime(datetime_format)

                # Apply label formatting
                if callable(label_format):
                    label = label_format(left, right_)
                elif isinstance(label_format, str):
                    try:
                        if left != pd.Timestamp.min and right_ != pd.Timestamp.max:
                            label = f"{left.strftime(label_format)}-{right_.strftime(label_format)}"
                        else:
                            label = f"{left_str}-{right_str}"
                    except (ValueError, AttributeError):
                        label = f"{left_str}-{right_str}"
                else:
                    label = f"{left_str}-{right_str}"

                labels.append(label)

        return binned, labels


    def _process_categorical_column(
        col: pd.Series,
        bins: Optional[Union[int, List[str]]],
        labels: Optional[List[str]],
        categorical_agg: str,
    ) -> Tuple[pd.Categorical, List[str]]:
        value_counts = col.value_counts(normalize=(categorical_agg == "ratio"))

        if bins is not None and isinstance(bins, int):
            top_categories = value_counts.head(bins).index
            binned = col.where(col.isin(top_categories), "Other")
        elif isinstance(bins, list):
            binned = col.where(col.isin(bins), "Other")
        else:
            binned = col

        binned = binned.astype("category")

        if labels is not None:
            binned = binned.cat.rename_categories(dict(zip(binned.cat.categories, labels)))

        return binned, list(binned.cat.categories)


    def _process_numeric_column(
        col: pd.Series,
        bins: Optional[Union[int, List[float]]],
        range_start: Optional[float],
        range_end: Optional[float],
        step: Optional[float],
        labels: Optional[List[str]],
        label_format: Optional[Union[str, Callable]],
        precision: int,
        right: bool,
        include_underflow: bool,
        include_overflow: bool,
    ) -> Tuple[pd.Categorical, List[str]]:
        if bins is None:
            if range_start is None or step is None:
                raise ValueError("If bins not provided, must set range_start and step")
            if range_end is None:
                range_end = col.max()

            bin_edges = list(np.arange(range_start, range_end + step, step))
        elif isinstance(bins, int):
            bin_edges = np.linspace(col.min(), col.max(), bins + 1).tolist()
        else:
            bin_edges = list(bins)

        # Add overflow/underflow bins if needed
        if include_underflow and not np.isinf(bin_edges[0]):
            bin_edges.insert(0, float("-inf"))
        if include_overflow and not np.isinf(bin_edges[-1]):
            bin_edges.append(float("inf"))

        # Generate labels if not provided
        if labels is None:
            labels = []
            for i in range(len(bin_edges) - 1):
                left = round(bin_edges[i], precision)
                right_ = round(bin_edges[i + 1], precision)

                if label_format:
                    label = (
                        label_format(left, right_)
                        if callable(label_format)
                        else label_format.format(left=left, right=right_)
                    )
                else:
                    if np.isinf(left) and left < 0:
                        label = f"<{right_}"
                    elif np.isinf(right_):
                        label = f">{left}"
                    else:
                        label = f"[{left}, {right_}{']' if right else ')'}"

                labels.append(label)

        binned = pd.cut(
            col, bins=bin_edges, labels=labels, right=right, include_lowest=True
        )
        return binned, labels


    def _handle_na_values(
        col: pd.Series, na_action: str, na_fill_value: Optional[str]
    ) -> pd.Series:
        if na_action == "drop":
            return col.dropna()
        elif na_action == "fill" and na_fill_value is not None:
            return col.fillna(na_fill_value)
        return col


    def _add_statistical_labels(
        binned: pd.Categorical,
        labels: List[str],
        show_count: bool,
        show_percentage: bool,
        show_total_count: bool,
        symbol_count: str,
        symbol_percentage: str,
        symbol_total_count: str,
        sep_between: str, 
    ) -> List[str]:
        """Add statistical information with accurate counts."""
        # Get counts by matching the exact bin intervals
        value_counts = binned.value_counts()
        total = len(binned.dropna())

        new_labels = []
        for i, (label, category) in enumerate(zip(labels, binned.cat.categories)):
            count = value_counts.get(category, 0)
            parts = [label]

            if show_count:
                parts.append(f"{symbol_count}{count}")
            if show_percentage:
                percentage = (count / total * 100) if total > 0 else 0
                parts.append(f"{percentage:.1f}{symbol_percentage}")
            if show_total_count:
                parts.append(f"{symbol_total_count}{total}")

            # Ensure unique labels 
            new_label = sep_between.join(parts)                
            if new_label in new_labels:
                new_label = f"{new_label}_{i}"
            new_labels.append(new_label)

        return new_labels


    def _sort_bin_labels(binned: pd.Categorical, labels: List[str]) -> pd.Categorical:
        try:
            # Attempt to sort by the underlying intervals
            sorted_categories = sorted(binned.cat.categories)
            binned = binned.cat.reorder_categories(sorted_categories, ordered=True)
        except Exception:
            # If sorting fails (e.g., string labels), fallback to given label order
            binned = binned.cat.set_categories(labels, ordered=True)
        return binned
    # Input validation
    if column not in df.columns:
        raise ValueError(f"Column '{column}' not found in DataFrame")

    if not inplace:
        df = df.copy()

    col_data = df[column]

    # Determine column type
    if is_datetime64_any_dtype(col_data):
        col_type = "datetime"
        col_data = pd.to_datetime(col_data)
    elif isinstance(col_data.dtype, pd.CategoricalDtype) or col_data.dtype == "object":
        col_type = "categorical"
    elif is_numeric_dtype(col_data):
        col_type = "numeric"
    else:
        raise TypeError(f"Unsupported column type: {col_data.dtype}")

    # Handle dictionary bin specification
    if isinstance(bins, dict):
        range_start = bins.get("start", range_start)
        range_end = bins.get("end", range_end)
        step = bins.get("step", step)
        bins = None

    # Process based on column type
    if col_type == "datetime":
        binned, bin_labels = _process_datetime_column(
            col_data,
            bins,
            range_start,
            range_end,
            step,
            labels,
            label_format,
            datetime_format,
            right,
            include_underflow,
            include_overflow,
        )
    elif col_type == "categorical":
        binned, bin_labels = _process_categorical_column(
            col_data, bins, labels, categorical_agg
        )
    else:
        binned, bin_labels = _process_numeric_column(
            col_data,
            bins,
            range_start,
            range_end,
            step,
            labels,
            label_format,
            precision,
            right,
            include_underflow,
            include_overflow,
        )

    # Handle NA values
    binned = _handle_na_values(binned, na_action, na_fill_value)

    # Add statistical information to labels if requested
    if show_count or show_percentage or show_total_count:
        bin_labels = _add_statistical_labels(
            binned,
            bin_labels,
            show_count,
            show_percentage,
            show_total_count,
            symbol_count,
            symbol_percentage,
            symbol_total_count,
            sep_between,
        )
        binned = binned.cat.rename_categories(
            dict(zip(binned.cat.categories, bin_labels))
        )

    # Sort labels if requested
    if sort_labels and not right and len(bin_labels) > 1:
        binned = _sort_bin_labels(binned, bin_labels)

    # Create final output column
    new_col = new_col_name or f"{column}_binned"
    df[new_col] = binned.astype(dtype) if dtype else binned

    if drop_original:
        df.drop(columns=[column], inplace=True)

    return None if inplace else df



def df_encoder(
    data: pd.DataFrame,
    method: str = "dummy",  #'dummy', 'onehot', 'ordinal', 'label', 'target', 'binary'
    columns=None,
    target_column=None,  # Required for 'target' encoding method
    **kwargs,
) -> pd.DataFrame:
    """
    Methods explained:
    - 'dummy': pandas' `get_dummies` to create dummy variables for categorical columns, which is another form of one-hot encoding, but with a simpler interface.

    - 'onehot': One-hot encoding is used when there is no inherent order in categories. It creates a binary column for each category and is useful for nominal categorical variables. However, it increases dimensionality significantly if there are many unique categories.

    - 'ordinal': Ordinal encoding is used when there is an inherent order in the categories. It assigns integers to categories based on their order. Use this when the categories have a ranking (e.g., 'low', 'medium', 'high').

    - 'label': Label encoding is used for converting each unique category to a numeric label. It can be useful when working with algorithms that can handle categorical data natively (e.g., decision trees). However, it might introduce unintended ordinal relationships between the categories.

    - 'target': Target encoding is used when you encode a categorical feature based on the mean of the target variable. This is useful when there is a strong correlation between the categorical feature and the target variable. It is often used in predictive modeling to capture relationships that are not directly encoded in the feature.

    - 'binary': Binary encoding is a more efficient alternative to one-hot encoding when dealing with high-cardinality categorical variables. It converts categories into binary numbers and then splits them into multiple columns, reducing dimensionality compared to one-hot encoding.
    """

    # Select categorical columns
    categorical_cols = data.select_dtypes(exclude=np.number).columns.tolist()
    methods = ["dummy", "onehot", "ordinal", "label", "target", "binary"]
    method = strcmp(method, methods)[0]

    if columns is None:
        columns = categorical_cols

    # pd.get_dummies()
    if method == "dummy":
        dtype = kwargs.pop("dtype", int)
        drop_first = kwargs.pop("drop_first", True)
        try:
            encoded_df = pd.get_dummies(
                data[columns], drop_first=drop_first, dtype=dtype, **kwargs
            )
            return pd.concat([data.drop(columns, axis=1), encoded_df], axis=1)
        except Exception as e:
            # print(f"Warning, Ê≤°ÊúâËøõË°åËΩ¨Êç¢, Âõ†‰∏∫: {e}")
            return data
    # One-hot encoding
    elif method == "onehot":
        from sklearn.preprocessing import OneHotEncoder

        encoder = OneHotEncoder(drop="first", sparse_output=False, **kwargs)
        encoded_data = encoder.fit_transform(data[columns])
        encoded_df = pd.DataFrame(
            encoded_data,
            columns=encoder.get_feature_names_out(columns),
            index=data.index,
        )
        return pd.concat([data.drop(columns, axis=1), encoded_df], axis=1)

    # Ordinal encoding
    elif method == "ordinal":
        from sklearn.preprocessing import OrdinalEncoder

        encoder = OrdinalEncoder(**kwargs)
        encoded_data = encoder.fit_transform(data[columns])
        encoded_df = pd.DataFrame(encoded_data, columns=columns, index=data.index)
        return pd.concat([data.drop(columns, axis=1), encoded_df], axis=1)

    # Label encoding
    elif method == "label":
        from sklearn.preprocessing import LabelEncoder

        encoder = LabelEncoder()
        # Apply LabelEncoder only to non-numeric columns
        non_numeric_columns = [
            col for col in columns if not pd.api.types.is_numeric_dtype(data[col])
        ]

        if not non_numeric_columns:
            return data
        encoded_data = data[non_numeric_columns].apply(
            lambda col: encoder.fit_transform(col)
        )
        return pd.concat([data.drop(non_numeric_columns, axis=1), encoded_data], axis=1)

    # Target encoding (Mean of the target for each category)
    elif method == "target":
        if target_column is None:
            raise ValueError("target_column must be provided for target encoding.")
        from category_encoders import TargetEncoder

        encoder = TargetEncoder(cols=columns, **kwargs)
        encoded_data = encoder.fit_transform(data[columns], data[target_column])
        return pd.concat([data.drop(columns, axis=1), encoded_data], axis=1)

    # Binary encoding (for high-cardinality categorical variables)
    elif method == "binary":
        from category_encoders import BinaryEncoder

        encoder = BinaryEncoder(cols=columns, **kwargs)
        encoded_data = encoder.fit_transform(data[columns])
        return pd.concat([data.drop(columns, axis=1), encoded_data], axis=1)

def df_sort_values(
    data,
    by=None,
    custom_order=None,
    ascending=True,
    ignore_index=True,
    inplace=False,
    verbose=True,
    **kwargs,
):
    """
        Sort a DataFrame by specified column(s) based on custom orders or by count.

        Parameters:
        - data: DataFrame to be sorted.
        - by: The name(s) of the column(s) to sort by (str or list).
        - custom_order: Custom sorting specification (str, list, or dict):
            - 'count': sort by frequency counts
            - list: custom order for first sort column
            - dict: {column: order} where order is 'count' or a custom list
        - ascending: Boolean or list of booleans, default True.
                     Sort ascending vs. descending per column.
        - ignore_index: Whether to reset index after sorting.
        - inplace: If True, perform operation in place and return None.
        - verbose: Whether to print success messages.
        - **kwargs: Additional arguments to pass to sort_values.

        Returns:
        - Sorted DataFrame if inplace is False, otherwise None.


    # Example usage:
    data = {
        "month": ["March", "January", "February", "April", "December"],
        "Amount": [200, 100, 150, 300, 250],
    }
    df_month = pd.DataFrame(data)

    # Define the month order
    month_order = ["January","February","March","April","May","June",
        "July","August","September","October","November","December"]
    display(df_month)
    sorted_df_month = df_sort_values(df_month, "month", month_order, ascending=True)
    display(sorted_df_month)
    df_sort_values(df_month, "month", month_order, ascending=False, inplace=True)
    display(df_month)
    """
    # Validate input
    if by is None:
        raise ValueError("The 'by' parameter is required for sorting.")

    # Convert single column to list for uniform processing
    by_columns = [by] if isinstance(by, str) else by
    if not isinstance(by_columns, list):
        raise TypeError("'by' must be a string or list of strings.")

    # Check all columns exist
    for col in by_columns:
        if col not in data.columns:
            raise ValueError(f"Column '{col}' does not exist in the DataFrame.")

    # Handle ascending parameter
    if isinstance(ascending, bool):
        ascending = [ascending] * len(by_columns)
    elif not isinstance(ascending, list) or len(ascending) != len(by_columns):
        raise ValueError("'ascending' must be a boolean or list matching 'by' length.")

    # Process custom orders
    if custom_order is not None:
        # Convert to dictionary format if needed
        if not isinstance(custom_order, dict):
            if not by_columns:
                raise ValueError("'by' must be specified when using custom_order")
            custom_order = {by_columns[0]: custom_order}

    # Create working copy if not inplace
    if inplace:
        df = data
    else:
        df = data.copy()

    # Apply custom ordering logic
    if custom_order:
        for i, col in enumerate(by_columns):
            if col in custom_order:
                order_spec = custom_order[col]

                if isinstance(order_spec, str) and "count" in order_spec.lower():
                    # Count-based sorting
                    value_counts = df[col].value_counts()
                    count_ascending = ascending[i]
                    sorted_counts = value_counts.sort_values(
                        ascending=count_ascending
                    ).index.tolist()
                    df[col] = pd.Categorical(
                        df[col], categories=sorted_counts, ordered=True
                    )
                elif isinstance(order_spec, list):
                    # Custom list order
                    df[col] = pd.Categorical(
                        df[col], categories=order_spec, ordered=True
                    )
                else:
                    raise ValueError(
                        f"Invalid custom_order for column '{col}': must be 'count' or list."
                    )

    # Perform sorting
    try:
        if inplace:
            df.sort_values(
                by=by_columns,
                ascending=ascending,
                inplace=True,
                ignore_index=ignore_index,
                **kwargs,
            )
            if verbose:
                print(f"Successfully sorted DataFrame by {by_columns}")
            return None
        else:
            sorted_df = df.sort_values(
                by_columns, ascending=ascending, ignore_index=ignore_index, **kwargs
            )
            if verbose:
                print(f"Successfully sorted DataFrame by {by_columns}")
            return sorted_df
    except Exception as e:
        print(f"Error sorting DataFrame by {by_columns}: {e}")
        return data if inplace else df

def df_scaler(
    data: pd.DataFrame,  # should be numeric dtype
    scaler="standard", # None | str | sklearn scaler
    columns=None,  # default, select all numeric col/row
    feature_range=None,  # specific for 'minmax'
    vmin=0,
    vmax=1,
    inplace=False,
    verbose=False,  # show usage
    axis=0,  # defalut column-wise
    return_scaler: bool = False,  # True: return both: return df, scaler
    **kwargs,
):
    """
    df_scaler(data, scaler="standard", inplace=False, axis=0, verbose=True)
    Bug 1 ‚Äî scaler=None does NOT mean ‚Äúno scaling‚Äù
    Parameters:
    - data: pandas DataFrame to be scaled.
    - method: Scaler type ('standard', 'minmax', 'robust'). Default is 'standard'.
    - columns: List of columns (for axis=0) or rows (for axis=1) to scale.
               If None, all numeric columns/rows will be scaled.
    - inplace: If True, modify the DataFrame in place. Otherwise, return a new DataFrame.
    - axis: Axis along which to scale. 0 for column-wise, 1 for row-wise. Default is 0.
    - verbose: If True, prints logs of the process.
    - kwargs: Additional arguments to be passed to the scaler.
    """
 
    from sklearn.utils.validation import check_is_fitted
    
    if verbose:
        print('df_scaler(data, scaler="standard", inplace=False, axis=0, verbose=True)')
    if scaler is None and method is None:
        if return_scaler:
            return data.copy(), None
        return data.copy()
    if data.columns.duplicated().any():
        raise ValueError(
            "df_scaler received duplicated column names. "
            "Scaling duplicated labels is unsafe."
        )
    if scaler is None:
        if verbose:
            print("df_scaler: scaler=None ‚Üí no scaling applied")
        return (data.copy(), None) if return_scaler else data.copy()
    if isinstance(scaler, str):
        scaler = scaler.lower()
        method = strcmp(scaler, ["standard", "minmax", "robust", "maxabs"])[0]
        if method == "standard":
            from sklearn.preprocessing import StandardScaler

            if verbose:
                print(
                    "performs z-score normalization: This will standardize each feature to have a mean of 0 and a standard deviation of 1."
                )
                print(
                    "Use when the data is approximately normally distributed (Gaussian).\nWorks well with algorithms sensitive to feature distribution, such as SVMs, linear regression, logistic regression, and neural networks."
                )
            kwargs = handle_kwargs(kwargs, StandardScaler)
            scaler = StandardScaler(**kwargs)
        elif method == "minmax":
            from sklearn.preprocessing import MinMaxScaler

            if feature_range is None:
                feature_range = (vmin, vmax)
            if verbose:
                print(
                    "don't forget to define the range: e.g., 'feature_range=(0, 1)'. "
                )
                print(
                    "scales the features to the range [0, 1]. Adjust feature_range if you want a different range, like [-1, 1]."
                )
                print(
                    "Use when the data does not follow a normal distribution and you need all features in a specific range (e.g., [0, 1]).\nIdeal for algorithms that do not assume a particular distribution, such as k-nearest neighbors and neural networks."
                )
            kwargs = handle_kwargs(kwargs, MinMaxScaler)
            scaler = MinMaxScaler(feature_range=feature_range, **kwargs)
        elif method == "robust":
            from sklearn.preprocessing import RobustScaler

            if verbose:
                print(
                    "scales the data based on the median and interquartile range, which is robust to outliers."
                )
                print(
                    "Use when the dataset contains outliers.\nThis method is useful because it scales based on the median and the interquartile range (IQR), which are more robust to outliers than the mean and standard deviation."
                )
            kwargs = handle_kwargs(kwargs, RobustScaler)
            scaler = RobustScaler(**kwargs)
        elif method == "maxabs":
            from sklearn.preprocessing import MaxAbsScaler

            if verbose:
                print(
                    "This scales each feature by its maximum absolute value, resulting in values within the range [-1, 1] for each feature."
                )
                print(
                    "Use for data that is already sparse or when features have positive or negative values that need scaling without shifting the data.\nOften used with sparse data (data with many zeros), where preserving zero entries is essential, such as in text data or recommendation systems."
                )
            kwargs = handle_kwargs(kwargs, MaxAbsScaler)
            scaler = MaxAbsScaler(**kwargs)
    if axis not in [0, 1]:
        raise ValueError("Axis must be 0 (column-wise) or 1 (row-wise).")
    if verbose:
        print(scaler)
    if axis == 0:
        # Column-wise scaling (default)
        if columns is None:
            columns = data.select_dtypes(include=np.number).columns.tolist()
        non_numeric_columns = data.columns.difference(columns)

        try:
            check_is_fitted(scaler)
            scaled_data = scaler.transform(data[columns])
            if verbose:
                print("Using fitted scaler ‚Üí transform()")
        except Exception:
            scaled_data = scaler.fit_transform(data[columns])
            if verbose:
                print("Fitting scaler ‚Üí fit_transform()")
        if inplace:
            data[columns] = scaled_data
            return (data, scaler) if return_scaler else data

        scaled_df = pd.concat(
            [
                pd.DataFrame(scaled_data, columns=columns, index=data.index),
                data[non_numeric_columns],
            ],
            axis=1,
        )[data.columns]

        if inplace:
            data[columns] = scaled_data
            print("Original DataFrame modified in place (column-wise).")
        else:
            scaled_df = pd.concat(
                [
                    pd.DataFrame(scaled_data, columns=columns, index=data.index),
                    data[non_numeric_columns],
                ],
                axis=1,
            )
            scaled_df = scaled_df[data.columns]  # Maintain column order
            if return_scaler:
                return scaled_df, scaler
            else:
                return scaled_df

    elif axis == 1:
        # Row-wise scaling

        if verbose:
                print(
                    "WARNING: axis=1 performs row-wise normalization. "
                    "This is NOT suitable for ML train/test pipelines."
                )

        numeric = data.select_dtypes(include="number")

        try:
            check_is_fitted(scaler)
            scaled = scaler.transform(numeric.T).T
        except Exception:
            scaled = scaler.fit_transform(numeric.T).T

        scaled_df = data.copy()
        scaled_df[numeric.columns] = scaled

        return (scaled_df, scaler) if return_scaler else scaled_df

########### DfTransformer Below:########### 

try:
    import scipy.stats as _st
    _HAVE_SCIPY = True
except Exception:
    _HAVE_SCIPY = False

try:
    from sklearn.base import BaseEstimator, TransformerMixin
    _HAVE_SKLEARN = True
except Exception:
    # simple fallback base classes if sklearn not installed
    class BaseEstimator:
        pass
    class TransformerMixin:
        pass
    _HAVE_SKLEARN = False


Number = Union[int, float]
ArrayLike = Union[np.ndarray, pd.Series, List[Number]]

def _to_df(data: Union[pd.DataFrame, np.ndarray]) -> pd.DataFrame:
    return data if isinstance(data, pd.DataFrame) else pd.DataFrame(data)

def _ensure_1d(arr: np.ndarray, axis: int):
    return arr

def _describe_series(s: pd.Series) -> dict:
    return {
        "count": s.count(),
        "mean": s.mean(),
        "std": s.std(ddof=1),
        "min": s.min(),
        "25%": s.quantile(0.25),
        "50%": s.median(),
        "75%": s.quantile(0.75),
        "max": s.max(),
        "skew": s.skew(),
        "kurt": s.kurtosis(),
    }

def apply_axiswise(x: np.ndarray, axis: int, fn: Callable) -> np.ndarray:
    """
    Generic axis-aware apply helper.
    fn should accept a 1D array and return a scalar or 1D array.
    """
    x = np.asarray(x)

    if axis not in (0, 1):
        raise ValueError("axis must be 0 or 1")

    # If axis=0 ‚Üí operate per column ‚Üí pass each x[:, i]
    if axis == 0:
        out = [fn(x[:, i]) for i in range(x.shape[1])]
        out = np.column_stack(out)

    # If axis=1 ‚Üí operate per row ‚Üí pass each x[i, :]
    else:
        out = [fn(x[i, :]) for i in range(x.shape[0])]
        out = np.row_stack(out)

    return out

class DfTransformer(BaseEstimator, TransformerMixin):
    """
    axis=0 means: operate column-wise, i.e. statistics are computed down the rows for each column.
        ‚Üí This matches sklearn, scipy, numpy conventions.
    axis=1 means: operate row-wise, i.e. statistic per row.

    DfTransformer class:
    - fit / transform / inverse_transform
    - many builtin transforms (zscore, minmax, robust, log, boxcox, yeojohnson, winsorize, lp-norm, sigmoid, arcsinh, whitening, ...)
    - plotting with skew/kurt overlays
    - usage text for each method
    - sklearn-compatible API (if sklearn installed)
    - persistent params for inverse transforms where possible
    Usage:
        T = DfTransformer(ddof=1)
        T.fit(df, method="z-transform", columns=["A","B"])
        X = T.transform(df, method="z-transform")
        X_inv = T.inverse_transform(X, method="z-transform")


    df = create_test_dataset()
    T = DfTransformer(verbose=True)
    print("Available methods:")
    T.methods()
    # Fit z-transform
    T.fit(df, method="z-transform", columns=None)
    df_z = T.transform(df, method="z-transform")
    print("\nZ-transform summary (first rows):")
    print(df_z[["Signal1", "Signal2"]].head())
    # plot before/after for Signal1
    T.debug(df, df_z, method="z-transform", columns=["Signal1"])
    print("\nReport (before):")
    print(T.info(df, ["Signal1", "Outlier"]))
    print("\nReport (after):")
    print(T.info(df_z, ["Signal1", "Outlier"]))
    """

    def __init__(self, ddof: int = 1, eps: float = 1e-9, verbose: bool = False, method: str = "z-transform", axis: int=0):
        self.ddof = ddof
        self.eps = eps
        self.verbose = verbose
        self.axis = axis

        # registry: method name -> function(x_array, axis, **kwargs) -> array
        self.methods: Dict[str, Callable] = {}
        self.method: str = method
        self.usage_docs: Dict[str, str] = {}
        # storage for fitted parameters per method
        # structure: { method_name: { 'columns': [...], 'params': {col: {...}} } }
        self.fitted_params: Dict[str, dict] = {}
        self.last_info: dict = {}

        # register builtin methods
        self._register_builtin_methods()

    # -------------------------
    # Registration API
    # -------------------------
    def register(self, name: str, func: Callable, usage: str = ""):
        """Register custom transform function.

        func signature: func(x: np.ndarray, axis: int, **kwargs) -> np.ndarray
        """
        self.methods[name] = func
        self.usage_docs[name] = usage.strip()

    def methods(self):
        print("Available transforms:")
        for k in sorted(self.methods.keys()):
            print(" -", k)

    def usage(self, method: Optional[str] = None):
        """Print usage docs for all or a specific method."""
        print("""
    T = DfTransformer(method="z-tran")
    df_ = T.generate_test_data()
    df_trans = T.transform(df_)
    T.debug(df_, df_trans, columns=["Signal2", "Signal1"])
              """)
        
        if method is None:
            method = self.method
        else:
            method=strcmp(method, list(self.methods))[0] 
        print(self.usage_docs.get(method, f"(no usage doc for {method})"))

    # -------------------------
    # Column selection
    # -------------------------
    def _select_columns(self, df: pd.DataFrame, columns: Optional[Union[str, List[str]]]):
        if columns is None:
            return df.select_dtypes(include=[np.number]).columns.tolist()
        if isinstance(columns, str):
            return [columns]
        return columns

    # -------------------------
    # Register builtins
    # -------------------------
    def _register_builtin_methods(self):
        # z-score (fit stores mean & std)
        def _zscore(x, axis, ddof=self.ddof, eps=self.eps, **kw):
            mean = np.nanmean(x, axis=axis, keepdims=True)
            sd = np.nanstd(x, axis=axis, ddof=ddof, keepdims=True)
            sd = np.where(sd == 0, eps, sd)
            return (x - mean) / sd

        self.register("z-transform", _zscore,
                      "Z-transform / z-score normalization. Stores mean & std for inverse_transform.")

        # minmax (fit stores min & max)
        def _minmax(x, axis, eps=self.eps, **kw):
            vmin = np.nanmin(x, axis=axis, keepdims=True)
            vmax = np.nanmax(x, axis=axis, keepdims=True)
            return (x - vmin) / (vmax - vmin + eps)
        self.register("minmax", _minmax, "Min-max scaling to [0,1]. Stores min & max for inverse_transform.")

        # robust scaling (IQR)
        def _robust(x, axis, q_low=0.25, q_high=0.75, eps=self.eps, **kw):
            q1 = np.nanquantile(x, q_low, axis=axis, keepdims=True)
            q3 = np.nanquantile(x, q_high, axis=axis, keepdims=True)
            iqr = np.where((q3 - q1) == 0, eps, (q3 - q1))
            return (x - q1) / iqr
        self.register("robust", _robust, "Robust (IQR) scaling. Uses 25%/75% quantiles by default.")

        # center only
        def _center(x, axis, **kw):
            mean = np.nanmean(x, axis=axis, keepdims=True)
            return x - mean
        self.register("center", _center, "Subtract column mean (centering).")

        # unit variance (divide by std)
        def _unitvar(x, axis, ddof=self.ddof, eps=self.eps, **kw):
            sd = np.nanstd(x, axis=axis, ddof=ddof, keepdims=True)
            sd = np.where(sd == 0, eps, sd)
            return x / sd
        self.register("unitvar", _unitvar, "Divide by standard deviation (unit variance).")

        # normalize L1/L2
        def _normalize_l1(x, axis, eps=self.eps, **kw):
            norm = np.sum(np.abs(x), axis=axis, keepdims=True)
            norm = np.where(norm == 0, eps, norm)
            return x / norm
        self.register("normalize_l1", _normalize_l1, "L1 normalization")

        def _normalize_l2(x, axis, eps=self.eps, **kw):
            norm = np.sqrt(np.sum(x ** 2, axis=axis, keepdims=True))
            norm = np.where(norm == 0, eps, norm)
            return x / norm
        self.register("normalize_l2", _normalize_l2, "L2 normalization")

        # log, log2, log10 (not invertible in presence of clipping)
        def _log(x, axis, eps=self.eps, **kw):
            return np.log(x + eps)
        self.register("log", _log, "Natural log transform: log(x + eps). Data must be > -eps")

        def _log2(x, axis, eps=self.eps, **kw):
            return np.log2(x + eps)
        self.register("log2", _log2, "Log2 transform")

        def _log10(x, axis, eps=self.eps, **kw):
            return np.log10(x + eps)
        self.register("log10", _log10, "Log10 transform")

        # sqrt, arcsinh, sigmoid
        def _sqrt(x, axis, eps=self.eps, **kw):
            return np.sqrt(np.clip(x, a_min=0, a_max=None))
        self.register("sqrt", _sqrt, "Square-root (x>=0)")

        def _arcsinh(x, axis, **kw):
            return np.arcsinh(x)
        self.register("arcsinh", _arcsinh, "Inverse-hyperbolic-sine transform (handles negatives)")

        def _sigmoid(x, axis, **kw):
            return 1.0 / (1.0 + np.exp(-x))
        self.register("sigmoid", _sigmoid, "Sigmoid: 1/(1+exp(-x))")

        # rank / quantile / percentile
        def _rank(x, axis, **kw):
            # Convert to DataFrame to use pandas ranking (stable)
            df = pd.DataFrame(x)
            ranked = df.rank(method="average", axis=axis).to_numpy()
            return ranked
        self.register("rank", _rank, "Rank transform (average ranks)")

        def _quantile(x, axis, **kw):
            df = pd.DataFrame(x)
            return df.rank(pct=True, axis=axis).to_numpy()
        self.register("quantile", _quantile, "Quantile transform -> [0,1]")

        def _percentile(x, axis, **kw):
            df = pd.DataFrame(x)
            return df.rank(pct=True, axis=axis).to_numpy() * 100
        self.register("percentile", _percentile, "Percentile rank [0,100]")

        # winsorize (fit stores percentile cutoffs; inverse is NOT possible)
        def _winsorize(x, axis, lower_pct=0.05, upper_pct=0.95, **kw):
            lower = np.nanquantile(x, lower_pct, axis=axis, keepdims=True)
            upper = np.nanquantile(x, upper_pct, axis=axis, keepdims=True)
            return np.clip(x, lower, upper)
        self.register("winsorize", _winsorize,
                      "Winsorize (clip extremes). Non-invertible. Stores cutoffs for reporting.")

        # Box-Cox and Yeo-Johnson (power transforms). require scipy for automatic lambda.
        def _boxcox(x, axis, lam: Optional[float] = None, eps=self.eps, **kw):
            # Apply Box-Cox per column (requires positive data). If lam is None and scipy available,
            # estimate lambda per column via boxcox_normmax; otherwise require lam provided.
            if not _HAVE_SCIPY and lam is None:
                raise RuntimeError("scipy required to estimate Box-Cox lambda automatically.")
            x_out = np.zeros_like(x)
            # operate column-wise (axis=0 -> operate along rows per column)
            if axis != 0:
                # unify to column-major by transposing expectation
                arr = x.T
                invert = True
            else:
                arr = x
                invert = False
            for i in range(arr.shape[1]):
                col = arr[:, i]
                col = np.clip(col, a_min=eps, a_max=None)
                if lam is None:
                    lam_i = _st.boxcox_normmax(col) #stats.boxcox_normmax(col)  # may raise if not allowed
                else:
                    lam_i = lam
                if abs(lam_i) < 1e-8:
                    transformed = np.log(col)
                else:
                    transformed = (np.power(col, lam_i) - 1.0) / lam_i
                if invert:
                    x_out[:, i] = transformed
                else:
                    x_out[:, i] = transformed
            if invert:
                return x_out.T
            return x_out
        self.register("boxcox", _boxcox, "Box-Cox power transform (positive data). Requires scipy for lambda estimation.")
        # ---------------------------------------------------------
        # CLR Transform (Centered Log-Ratio)
        # ---------------------------------------------------------
        def _centered_log_ratio(x, axis, eps=self.eps, **kw):
            """
            CLR transform for compositional data.
            clr(x) = log(x) - mean(log(x))
            """
            # avoid log(0)
            x_safe = np.clip(x, a_min=eps, a_max=None)
            log_x = np.log(x_safe)

            # geometric mean ‚Üí mean of log
            gm = np.nanmean(log_x, axis=axis, keepdims=True)
            return log_x - gm

        self.register("centered_log_ratio",_centered_log_ratio,
            "Centered Log-Ratio transform (CLR) for compositional data. Uses log(x) - mean(log(x))."
        ) 
        def _isometric_log_ratio(x, axis, eps=self.eps, **kw):
            """
            ILR(Isometric Log-Ratio) transform for compositional data.

            ILR(x) = (clr(x)) @ H^T    where H = Helmert basis.
            Only works column-wise (axis=0) because basis acts on features.
            """
            if axis != 0:
                raise ValueError("ILR transform requires axis=0 (columns = components).")

            X = np.clip(x, a_min=eps, a_max=None)
            logX = np.log(X)
            clrX = logX - np.nanmean(logX, axis=0)

            D = X.shape[1]  # number of components

            # Helmert matrix (D √ó (D-1))
            H = np.zeros((D, D-1))
            for i in range(1, D):
                a = np.ones(i) / i
                b = -1
                H[0:i, i-1] = a
                H[i,   i-1] = b
            # Orthonormalize
            for j in range(D-1):
                col = H[:, j]
                H[:, j] = col / np.linalg.norm(col)

            # store basis for inverse
            self.fitted_params["ilr"] = {"H": H}

            # ILR projection
            Z = clrX @ H
            return Z

        self.register("isometric_log_ratio",_isometric_log_ratio,
            "ILR transform (Isometric Log-Ratio). Orthogonal basis over CLR space; uses Helmert matrix."
        )

        def _yeojohnson(x, axis, lam: Optional[float] = None, **kw):
            # Yeo-Johnson can handle negative values. If lam None and scipy available, estimate.
            if not _HAVE_SCIPY and lam is None:
                raise RuntimeError("scipy required to estimate Yeo-Johnson lambda automatically.")
            x_out = np.zeros_like(x)
            if axis != 0:
                arr = x.T
                invert = True
            else:
                arr = x
                invert = False
            for i in range(arr.shape[1]):
                col = arr[:, i]
                if lam is None:
                    lam_i = stats.yeojohnson_normmax(col)
                else:
                    lam_i = lam
                transformed = stats.yeojohnson(col, lmbda=lam_i)
                x_out[:, i] = transformed
            return x_out.T if invert else x_out
        self.register("yeojohnson", _yeojohnson, "Yeo-Johnson power transform (handles negative values).")

        # Lp-norm scaling (stores norms)
        def _lp_norm(x, axis, p=2, eps=self.eps, **kw):
            # Axis-aware: if axis==0, compute per-column p-norm across rows
            norm = np.sum(np.abs(x) ** p, axis=axis, keepdims=True) ** (1.0 / p)
            norm = np.where(norm == 0, eps, norm)
            return x / norm
        self.register("lp-norm", _lp_norm, "Lp norm scaling (p default 2). Stores norms for inverse if needed.")

        # Whitening (PCA whitening) - fit stores mean, components, singular values
        def _whiten(x, axis, eps=self.eps, **kw):
            # Whitening is applied column-wise (features in columns). We expect axis=0 for columns.
            if axis != 0:
                raise ValueError("whitening currently supports axis=0 (column-wise).")
            # x shape: (n_rows, n_columns)
            X = x.copy().astype(float)
            # center
            mu = np.nanmean(X, axis=0, keepdims=True)
            Xc = X - mu
            # compute SVD for whitening
            # covariance-like
            cov = np.dot(Xc.T, Xc) / (Xc.shape[0] - 1 + eps)
            U, S, Vt = np.linalg.svd(cov)
            # Whitening matrix: W = U * diag(1/sqrt(S + eps)) * U.T
            D = np.diag(1.0 / np.sqrt(S + eps))
            W = U.dot(D).dot(U.T)
            Xw = Xc.dot(W.T)  # whitened data
            # store params for inverse (mean, W)
            # We'll allow inverse via X = Xw.dot(W_inv.T) + mu where W_inv = inv(W)
            return Xw
        self.register("whiten", _whiten, "PCA whitening (axis=0). Fit stores mean and whitening matrix for inverse.")

        # Scale to [-1, 1]
        def _scale_neg1_1(x, axis, eps=self.eps, **kw):
            vmin = np.nanmin(x, axis=axis, keepdims=True)
            vmax = np.nanmax(x, axis=axis, keepdims=True)
            return 2 * (x - vmin) / (vmax - vmin + eps) - 1
        self.register("scale_neg1_1", _scale_neg1_1, "Scale data to [-1,1].")

        # Smoothing (moving average) - operates per column/time series (axis=0)
        def _smooth(x, axis, window=5, **kw):
            df = pd.DataFrame(x)
            sm = df.rolling(window=window, axis=0, min_periods=1, center=True).mean().to_numpy()
            return sm
        self.register("smooth", _smooth, "Moving average smoothing (axis=0).")

        # add any other transforms you like similarly...

    # -------------------------
    # Fit / Transform / Inverse transform
    # -------------------------
    def fit(self,
            data: Union[pd.DataFrame, np.ndarray],
            method: str = None,
            columns: Optional[Union[str, List[str]]] = None,
            axis: int = None,
            **fit_kwargs):
        """
        Fit required parameters for a transform that requires learning (means, stds, min/max, lambdas, whitening matrix...).
        This does NOT modify data in place.
        """
        if method is None:
            method = self.method
        else:
            method=strcmp(method, list(self.methods))[0]
            self.method=method 
        if method not in self.methods:
            method=strcmp(method, list(self.methods))[0]
            print(f"Unknown method, do you mean '{method}', if not, select a method from {list(self.methods)}")
        if axis is None:
            axis=self.axis
        else:
            self.axis=axis
        
        df = _to_df(data)
        cols = self._select_columns(df, columns)
        X = df[cols].to_numpy()

        params: Dict[str, dict] = {}
        start = time.time()

        # Which methods need fitted parameters? We'll handle common ones:
        if method in ("z-transform", "zscore"):
            mean = np.nanmean(X, axis=0)
            std = np.nanstd(X, axis=0, ddof=self.ddof)
            std[std == 0] = self.eps
            for i, c in enumerate(cols):
                params[c] = {"mean": float(mean[i]), "std": float(std[i])}

        elif method == "minmax":
            vmin = np.nanmin(X, axis=0)
            vmax = np.nanmax(X, axis=0)
            for i, c in enumerate(cols):
                params[c] = {"min": float(vmin[i]), "max": float(vmax[i])}

        elif method == "robust":
            q1 = np.nanquantile(X, 0.25, axis=0)
            q3 = np.nanquantile(X, 0.75, axis=0)
            for i, c in enumerate(cols):
                params[c] = {"q1": float(q1[i]), "q3": float(q3[i])}

        elif method in ("boxcox", "yeojohnson"):
            # estimate lambda per column if possible (requires scipy)
            if not _HAVE_SCIPY:
                raise RuntimeError("scipy required to fit Box-Cox / Yeo-Johnson lambdas.")
            lam_map = {}
            for i, c in enumerate(cols):
                col = X[:, i]
                if method == "boxcox":
                    col_pos = np.clip(col, a_min=self.eps, a_max=None)
                    lam = stats.boxcox_normmax(col_pos)
                    lam_map[c] = float(lam)
                else:
                    lam = stats.yeojohnson_normmax(col)
                    lam_map[c] = float(lam)
            for c, lam in lam_map.items():
                params[c] = {"lambda": lam}

        elif method == "winsorize":
            lower_pct = float(fit_kwargs.get("lower_pct", 0.05))
            upper_pct = float(fit_kwargs.get("upper_pct", 0.95))
            for i, c in enumerate(cols):
                col = X[:, i]
                lower = np.nanquantile(col, lower_pct)
                upper = np.nanquantile(col, upper_pct)
                params[c] = {"lower": float(lower), "upper": float(upper), "lower_pct": lower_pct, "upper_pct": upper_pct}

        elif method == "lp-norm":
            p = float(fit_kwargs.get("p", 2.0))
            norms = np.sum(np.abs(X) ** p, axis=0) ** (1.0 / p)
            norms[norms == 0] = self.eps
            for i, c in enumerate(cols):
                params[c] = {"p": p, "norm": float(norms[i])}

        elif method == "whiten":
            # store mean and whitening matrix
            if axis != 0:
                raise ValueError("whiten.fit currently expects axis=0 (features in columns).")
            Xf = X.astype(float)
            mu = np.nanmean(Xf, axis=0, keepdims=True)
            Xc = Xf - mu
            cov = np.dot(Xc.T, Xc) / (Xc.shape[0] - 1 + self.eps)
            U, S, Vt = np.linalg.svd(cov)
            D = np.diag(1.0 / np.sqrt(S + self.eps))
            W = U.dot(D).dot(U.T)
            W_inv = np.linalg.pinv(W)
            # store full objects
            params = {"_whiten": {"mean": mu.squeeze().tolist(), "W": W.tolist(), "W_inv": W_inv.tolist()}}
        else:
            # For other methods: no fit necessary (stateless)
            params = {}

        # store
        self.fitted_params[method] = {"columns": cols, "params": params, "fitted_at": time.time()}
        elapsed = time.time() - start
        if self.verbose:
            print(f"[fit] method={method} cols={cols} fitted_in={elapsed:.4f}s")

        return self

    def transform(self,
                  data: Union[pd.DataFrame, np.ndarray],
                  method: str = None,
                  columns: Optional[Union[str, List[str]]] = None,
                  axis: int = None,
                  inplace: bool = False,
                  **transform_kwargs) -> pd.DataFrame:
        """
        Transform data using method. If required parameters are not fitted, this will call fit(...) automatically (where appropriate).
        Returns transformed DataFrame (copy unless inplace=True).
        """
        
        if axis is None:
            axis=self.axis
        else:
            self.axis=axis

        if method is None:
            method = self.method
        else:
            method=strcmp(method, list(self.methods))[0]
            self.method=method

        if method not in self.methods: 
            method=strcmp(method, list(self.methods))[0]
            print(f"Unknown method, do you mean '{method}', if not, select a method from {list(self.methods)}")

        df = _to_df(data)
        if not inplace:
            df = df.copy()
        cols = self._select_columns(df, columns)
        _func = self.methods[method]

        # if method requires fitted params and not present, call fit automatically when possible
        if method in ("z-transform", "zscore", "minmax", "robust", "boxcox", "yeojohnson", "winsorize", "lp-norm", "whiten"):
            if method not in self.fitted_params:
                # auto-fit (pass transform kwargs into fit if relevant)
                self.fit(df, method=method, columns=cols, axis=axis, **transform_kwargs)

        # prepare array and call func
        arr = df[cols].to_numpy()
        # if method requires per-column lambdas from fitted_params, wrap to use them
        if method in ("boxcox", "yeojohnson"):
            # build lam vector
            lambdas = {c: self.fitted_params[method]["params"][c]["lambda"] for c in self.fitted_params[method]["params"]}
            # apply per column
            out = arr.copy().astype(float)
            for i, c in enumerate(cols):
                lam = lambdas.get(c, transform_kwargs.get("lam", None))
                # call registered function per-column by packaging as a 2D array
                col = arr[:, i].copy()
                if method == "boxcox":
                    col = np.clip(col, a_min=self.eps, a_max=None)
                    if lam is None:
                        # should not happen (we fit above)
                        raise RuntimeError("Box-Cox lambda missing.")
                    if abs(lam) < 1e-8:
                        out[:, i] = np.log(col)
                    else:
                        out[:, i] = (np.power(col, lam) - 1.0) / lam
                else:  # yeojohnson via scipy
                    if not _HAVE_SCIPY:
                        raise RuntimeError("scipy required for yeojohnson transform.")
                    out[:, i] = stats.yeojohnson(col, lmbda=lam)
        elif method == "lp-norm":
            p = transform_kwargs.get("p", self.fitted_params.get("lp-norm", {}).get("params", {}).get(cols[0], {}).get("p", 2.0))
            # use registered func
            out = _func(arr, axis, p=p)
        elif method == "winsorize":
            # use stored cutoffs if fitted
            params_map = self.fitted_params.get("winsorize", {}).get("params", {})
            out = arr.copy()
            for i, c in enumerate(cols):
                if c in params_map:
                    lower = params_map[c]["lower"]
                    upper = params_map[c]["upper"]
                else:
                    # fallback to kwargs
                    lower = transform_kwargs.get("lower", np.nanquantile(arr[:, i], 0.05))
                    upper = transform_kwargs.get("upper", np.nanquantile(arr[:, i], 0.95))
                out[:, i] = np.clip(arr[:, i], lower, upper)
        elif method == "whiten":
            # use fitted W matrix
            if "whiten" not in self.fitted_params:
                self.fit(df, method="whiten", columns=cols, axis=axis)
            whiten_meta = self.fitted_params["whiten"]["params"]["_whiten"]
            mu = np.array(whiten_meta["mean"])
            W = np.array(whiten_meta["W"])
            Xc = arr - mu
            out = Xc.dot(W.T)
        else:
            # generic call
            out = _func(arr, axis, **transform_kwargs)

        # store last_info: before/after stats and meta
        before_df = df[cols].copy()
        df[cols] = out
        after_df = df[cols].copy()

        # compute summaries
        before_stats = before_df.agg(["mean", "std", "skew", pd.Series.kurtosis]).T
        after_stats = after_df.agg(["mean", "std", "skew", pd.Series.kurtosis]).T

        self.last_info = {
            "method": method,
            "columns": cols,
            "axis": axis,
            "before": before_stats,
            "after": after_stats,
            "elapsed": None,
            "transformed_at": time.time()
        }

        if self.verbose:
            print(f"[transform] method={method} columns={cols}")

        return df

    def inverse_transform(self,
                          transformed: Union[pd.DataFrame, np.ndarray],
                          method: str=None,
                          columns: Optional[Union[str, List[str]]] = None,
                          axis: int = None,
                          inplace: bool = False,
                          **kwargs) -> pd.DataFrame:
        """
        Attempt to invert a transform. Only supported where mathematically possible and parameters from fit are available.
        Raises RuntimeError for non-invertible transforms (e.g., winsorize without storing original data, sigmoid, rank, quantile).
        """
        
        if axis is None:
            axis=self.axis
        else:
            self.axis=axis
        if method is None:
            method = self.method
        else:
            method=strcmp(method, list(self.methods))[0]
            self.method=method
        if method not in self.methods: 
            method=strcmp(method, list(self.methods))[0]
            print(f"Unknown method, do you mean '{method}', if not, select a method from {list(self.methods)}") 

        df = _to_df(transformed)
        if not inplace:
            df = df.copy()
        cols = self._select_columns(df, columns)

        if method in ("z-transform", "zscore"):
            if method not in self.fitted_params:
                raise RuntimeError("Method not fitted; cannot inverse_transform without saved parameters.")
            params = self.fitted_params[method]["params"]
            out = df.copy()
            for i, c in enumerate(cols):
                p = params.get(c)
                if p is None:
                    raise RuntimeError(f"No fitted params for column {c}")
                mean = float(p["mean"])
                std = float(p["std"])
                out[c] = df[c] * std + mean
            return out

        elif method == "minmax":
            if method not in self.fitted_params:
                raise RuntimeError("Method not fitted; cannot inverse_transform.")
            params = self.fitted_params[method]["params"]
            out = df.copy()
            for c in cols:
                p = params.get(c)
                if p is None:
                    raise RuntimeError(f"No fitted params for column {c}")
                vmin = float(p["min"])
                vmax = float(p["max"])
                out[c] = df[c] * (vmax - vmin + self.eps) + vmin
            return out

        elif method == "lp-norm":
            # Inverse of lp-norm scaling is not uniquely defined unless original norms stored. If fit was called, we stored norms.
            if "lp-norm" not in self.fitted_params:
                raise RuntimeError("lp-norm not fitted; cannot inverse_transform")
            params = self.fitted_params["lp-norm"]["params"]
            out = df.copy()
            for i, c in enumerate(cols):
                col_params = params.get(c)
                if col_params is None:
                    raise RuntimeError(f"No lp params for {c}")
                p = float(col_params["p"])
                norm = float(col_params["norm"])
                out[c] = df[c] * norm
            return out

        elif method == "whiten":
            if "whiten" not in self.fitted_params:
                raise RuntimeError("whiten not fitted; cannot inverse_transform")
            whiten_meta = self.fitted_params["whiten"]["params"]["_whiten"]
            W_inv = np.array(whiten_meta["W_inv"])
            mu = np.array(whiten_meta["mean"])
            out = df.copy()
            arr = out[cols].to_numpy()
            Xrec = arr.dot(W_inv.T) + mu
            out[cols] = Xrec
            return out

        # Non-invertible / unsupported inverse
        raise RuntimeError(f"inverse_transform is not supported for method='{method}' or parameters are missing")

    # -------------------------
    # Plotting & Reporting
    # -------------------------
    def debug(
            self,
            original: Union[pd.DataFrame, np.ndarray],
            transformed: Union[pd.DataFrame, np.ndarray],
            method: str=None,
            columns: Optional[Union[str, List[str]]] = None,
            bins: int = 40,
            show_kde: bool = True,
            figsize: Tuple[int, int] = (10, 4),
        ):
        """
        Plot distributions before/after for each selected column.

        Supports method-specific overlays:
        ----------------------------------
        z-transform / zscore ‚Üí Gaussian overlay
        whiten                ‚Üí covariance diagonal inset
        minmax                ‚Üí uniform target overlay
        robust                ‚Üí median/IQR annotation
        log / log1p           ‚Üí log-scale axis
        boxcox / yeojohnson   ‚Üí approximate normality overlay
        power                 ‚Üí compression/expansion annotation
        sigmoid / tanh        ‚Üí bounded-range curves
        softmax               ‚Üí sum-to-1 check
        winsorize             ‚Üí clipped boundaries
        lp_norm               ‚Üí vector norm annotation
        """
        df_orig = _to_df(original)
        df_tr = _to_df(transformed)
        
        cols = self._select_columns(df_orig, columns) if columns is None else columns
        cols=[cols] if not isinstance(cols, list) else cols
        if method is None:
            method = self.method 
        method = method.lower()

        for c in cols:
            a = df_orig[c].dropna()
            b = df_tr[c].dropna()

            fig, axes = plt.subplots(1, 2, figsize=figsize)

            # 1) BEFORE
            axes[0].hist(a, bins=bins, alpha=0.7)
            if show_kde:
                try:
                    sns.kdeplot(a, ax=axes[0])
                except Exception:
                    pass
            axes[0].set_title(f"{c} ‚Äî before")

            stA = _describe_series(a)
            axes[0].text(
                0.02, 0.95,
                f"mean={stA['mean']:.3g}\nstd={stA['std']:.3g}\nskew={stA['skew']:.3g}\nkurt={stA['kurt']:.3g}",
                transform=axes[0].transAxes, fontsize=9, va='top'
            )

            # 2) AFTER
            axes[1].hist(b, bins=bins, alpha=0.7)
            if show_kde:
                try:
                    sns.kdeplot(b, ax=axes[1])
                except Exception:
                    pass

            axes[1].set_title(f"{c} ‚Äî after ({method})")

            stB = _describe_series(b)
            axes[1].text(
                0.02, 0.95,
                f"mean={stB['mean']:.3g}\nstd={stB['std']:.3g}\nskew={stB['skew']:.3g}\nkurt={stB['kurt']:.3g}",
                transform=axes[1].transAxes, fontsize=9, va='top'
            )

            # ---------------------------------------------------------
            # Method-specific overlays
            # ---------------------------------------------------------

            # z-transform ‚Üí N(0,1)
            if method in ("z-transform", "zscore"):
                xs = np.linspace(b.min(), b.max(), 300)
                try:
                    y = _st.norm.pdf(xs, 0, 1)
                    axes[1].plot(xs, y * len(b) * (xs[1]-xs[0]), label="N(0,1)")
                    axes[1].legend()
                except Exception:
                    pass

            # whitening ‚Üí show covariance diagonal shrinkage
            elif method == "whiten":
                try:
                    covB = np.cov(df_orig[cols].dropna().to_numpy(), rowvar=False)
                    covA = np.cov(df_tr[cols].dropna().to_numpy(), rowvar=False)
                    diag_b = np.diag(covB)
                    diag_a = np.diag(covA)
                    inset = fig.add_axes([0.72, 0.55, 0.20, 0.30])
                    inset.bar(["before", "after"], [np.mean(diag_b), np.mean(diag_a)])
                    inset.set_title("Cov diag mean")
                except:
                    pass

            # minmax ‚Üí overlay uniform [0,1]
            elif method == "minmax":
                xs = np.linspace(0, 1, 300)
                axes[1].plot(xs, np.ones_like(xs), "--", label="Uniform(0,1)")
                axes[1].legend()

            # robust scaling ‚Üí show median/IQR lines
            elif method == "robust":
                med = b.median()
                q1 = b.quantile(0.25)
                q3 = b.quantile(0.75)
                axes[1].axvline(med, color="black", linestyle="--", label="median")
                axes[1].axvline(q1, color="gray", linestyle=":")
                axes[1].axvline(q3, color="gray", linestyle=":")
                axes[1].legend()

            # log transforms ‚Üí log scale x-axis
            elif method in ("log", "log1p"):
                axes[1].set_xscale("log")

            # power transforms ‚Üí annotate compression/expansion regions
            elif method in ("power", "boxcox", "yeojohnson"):
                axes[1].text(
                    0.5, 0.1,
                    "Power transform ‚Üí reduces skewness",
                    transform=axes[1].transAxes,
                    ha="center",
                    fontsize=9
                )

            # sigmoid / tanh ‚Üí bounded distributions
            elif method in ("sigmoid", "tanh"):
                axes[1].axvline(0, linestyle="--", color="black")
                axes[1].text(0.5, 0.1, "Range bounded", transform=axes[1].transAxes,
                            ha="center", fontsize=9)

            # softmax ‚Üí sum=1 check
            elif method == "softmax":
                s = b.sum()
                axes[1].text(0.5, 0.1, f"sum = {s:.3f}", transform=axes[1].transAxes,
                            ha="center", fontsize=9)

            # winsorize ‚Üí clipped endpoints
            elif method == "winsorize":
                lo = b.min()
                hi = b.max()
                axes[1].axvline(lo, linestyle="--", color="red", label="clipped lo")
                axes[1].axvline(hi, linestyle="--", color="red", label="clipped hi")
                axes[1].legend()

            # lp norm ‚Üí annotate norm used
            elif method == "lp_norm":
                axes[1].text(
                    0.5, 0.1,
                    "Lp norm ‚Üí scaled by vector norm",
                    transform=axes[1].transAxes,
                    ha="center", fontsize=9
                ) 
            elif method in ("clr", "clr-transform","centered_log_ratio"):
                # 1) Check row-wise zero mean (CLR property)
                row_means = df_tr[cols].mean(axis=1)
                mean_abs = row_means.abs().mean()

                axes[1].text(
                    0.5, 0.15,
                    f"CLR check:\nmean(|row_mean|)={mean_abs:.3g}",
                    transform=axes[1].transAxes,
                    ha="center", fontsize=8
                )

                # Inset: distribution of row means (should be ‚âà0)
                try:
                    inset = fig.add_axes([0.72, 0.55, 0.20, 0.25])
                    inset.hist(row_means, bins=30)
                    inset.set_title("row means (should be 0)", fontsize=8)
                except:
                    pass

                # If zeros present before CLR ‚Üí warn
                zero_count = (a == 0).sum()
                if zero_count > 0:
                    axes[0].text(
                        0.5, 0.05,
                        f"WARNING: zeros={zero_count}",
                        color="red",
                        ha="center", transform=axes[0].transAxes
                    )

                # Overlay approx Gaussian curve
                xs = np.linspace(b.min(), b.max(), 300)
                try:
                    y = _st.norm.pdf(xs, b.mean(), b.std())
                    axes[1].plot(xs, y * len(b) * (xs[1]-xs[0]),
                                 linestyle="--", label="Gaussian approx")
                    axes[1].legend()
                except:
                    pass 
            elif method in ("ilr", "ilr-transform","isometric_log_ratio"):
                # 1) ILR values should be approximately Gaussian
                try:
                    inset = fig.add_axes([0.72, 0.55, 0.20, 0.25])
                    _st.probplot(b, dist="norm", plot=inset)
                    inset.set_title("Q-Q plot", fontsize=8)
                except:
                    pass

                # 2) Variance distribution across ILR components
                try:
                    ilr_vars = df_tr[cols].var()
                    inset2 = fig.add_axes([0.72, 0.25, 0.20, 0.20])
                    inset2.bar(range(len(ilr_vars)), ilr_vars)
                    inset2.set_title("ILR variances", fontsize=8)
                except:
                    pass

                # 3) Correlation check (ILR should reduce closure correlations)
                try:
                    corr_val = df_tr[cols].corr().values.mean()
                    axes[1].text(
                        0.5, 0.15,
                        f"mean corr={corr_val:.3f}",
                        ha="center", transform=axes[1].transAxes,
                        fontsize=8
                    )
                except:
                    pass

            try:
                plt.tight_layout()
            except RuntimeError:
                plt.subplots_adjust(top=0.92, right=0.92)
            plt.show()

    def info(
        self,
        data: Union[pd.DataFrame, np.ndarray],
        columns: Optional[Union[str, List[str]]] = None,
        axis: Optional[int] = None,
    ) -> Union[pd.DataFrame, dict]:
        """
        Statistical info respecting axis.
        
        axis:
            0 -> column-wise report (default)
            1 -> row-wise report
            # None -> global summary of entire matrix
        """
        df = _to_df(data)
        df = df.convert_dtypes()
        
        if axis is None:
            axis=self.axis
        else:
            self.axis=axis
        # # If axis is None ‚Üí global stats
        # if axis is None:
        #     arr = df.to_numpy().astype(float).flatten()
        #     arr = arr[~np.isnan(arr)]

        #     return {
        #         "n": arr.size,
        #         "mean": float(np.nanmean(arr)),
        #         "std": float(np.nanstd(arr, ddof=self.ddof)),
        #         "min": float(np.nanmin(arr)),
        #         "25%": float(np.nanpercentile(arr, 25)),
        #         "50%": float(np.nanpercentile(arr, 50)),
        #         "75%": float(np.nanpercentile(arr, 75)),
        #         "max": float(np.nanmax(arr)),
        #         "skew": float(_st.skew(arr, nan_policy="omit")) if _HAVE_SCIPY else float(pd.Series(arr).skew()),
        #         "kurtosis": float(_st.kurtosis(arr, nan_policy="omit")) if _HAVE_SCIPY else float(pd.Series(arr).kurtosis()),
        #         "n_null": int(df.isna().sum().sum()),
        #     }

        # --- axis = 0: compute column-wise ---
        if axis == 0:
            cols = self._select_columns(df, columns)
            rows = []

            for c in cols:
                s = df[c].astype(float)

                skew_val = (
                    float(_st.skew(s, nan_policy="omit"))
                    if _HAVE_SCIPY else float(s.skew())
                )
                kurt_val = (
                    float(_st.kurtosis(s, nan_policy="omit"))
                    if _HAVE_SCIPY else float(s.kurtosis())
                )

                rows.append({
                    "name": c,
                    "n": int(s.count()),
                    "n_null": int(s.isnull().sum()),
                    "mean": float(s.mean()),
                    "std": float(s.std(ddof=self.ddof)),
                    "min": float(s.min()),
                    "25%": float(s.quantile(0.25)),
                    "50%": float(s.quantile(0.50)),
                    "75%": float(s.quantile(0.75)),
                    "max": float(s.max()),
                    "skew": skew_val,
                    "kurtosis": kurt_val,
                })

            return pd.DataFrame(rows).set_index("name")

        # --- axis = 1: compute row-wise ---
        if axis == 1:
            rows = []
            for i, row in df.iterrows():
                r = row.astype(float)

                skew_val = (
                    float(_st.skew(r, nan_policy="omit"))
                    if _HAVE_SCIPY else float(r.skew())
                )
                kurt_val = (
                    float(_st.kurtosis(r, nan_policy="omit"))
                    if _HAVE_SCIPY else float(r.kurtosis())
                )

                rows.append({
                    "name": i,
                    "n": int(r.count()),
                    "n_null": int(r.isnull().sum()),
                    "mean": float(r.mean()),
                    "std": float(r.std(ddof=self.ddof)),
                    "min": float(r.min()),
                    "25%": float(r.quantile(0.25)),
                    "50%": float(r.quantile(0.50)),
                    "75%": float(r.quantile(0.75)),
                    "max": float(r.max()),
                    "skew": skew_val,
                    "kurtosis": kurt_val,
                })

            return pd.DataFrame(rows).set_index("name")

        raise ValueError("axis must be 0, 1, or None.")

    def generate_test_data(self, seed: int = 1) -> pd.DataFrame:
        np.random.seed(seed)
        df = pd.DataFrame({
            "SampleID": [f"S{i:03d}" for i in range(1, 21)],
            "Group": ["A"] * 7 + ["B"] * 7 + ["C"] * 6,
            "Signal1": np.random.normal(50, 10, 20),
            "Signal2": np.random.normal(100, 5, 20),
            "Intensity": np.random.uniform(0, 1, 20),
            "Count": np.random.poisson(30, 20),
            "Noise": np.random.normal(0, 0.1, 20),
            "Outlier": np.concatenate([np.random.normal(10, 1, 19), np.array([200])])
        })
        return df


def df_special_characters_cleaner(
    data: pd.DataFrame, where=["column", "content", "index"]
) -> pd.DataFrame:
    """
    to clean special characters:
    usage:
        df_special_characters_cleaner(data=df, where='column')
    """
    if not isinstance(where, list):
        where = [where]
    where_to_clean = ["column", "content", "index"]
    where_ = [strcmp(i, where_to_clean)[0] for i in where]

    # 1. Clean column names by replacing special characters with underscores
    if "column" in where_:
        try:
            data.columns = data.columns.str.replace(r"[^\w\s]", "_", regex=True)
        except Exception as e:
            print(e)

    # 2. Clean only object-type columns (text columns)
    try:
        if "content" in where_:
            for col in data.select_dtypes(include=["object"]).columns:
                data[col] = data[col].str.replace(r"[^\w\s]", "", regex=True)
        if data.index.dtype == "object" and index in where_:
            data.index = data.index.str.replace(r"[^\w\s]", "_", regex=True)
    except:
        pass
    return data


#! =========HELPER FUN for df_reducer =========
def _consensus_clustering(
    X,
    n_clusters=None,
    n_runs=100,
    random_state=1,
    cluster_range=range(2, 10),
    **kwargs,
):
    """
    Consensus clustering with automatic cluster selection when n_clusters=None.
    Uses PAC (Proportion of Ambiguous Clustering).

    Returns:
        (labels, consensus_matrix, final_k)
    """
    from scipy.spatial.distance import squareform
    from scipy.cluster.hierarchy import linkage, fcluster
    from sklearn.cluster import KMeans
    n_samples = X.shape[0]

    # ------------------------------------------------------
    # CASE A: n_clusters is provided ‚Üí run single consensus
    # ------------------------------------------------------
    if n_clusters is not None:
        consensus_matrix = np.zeros((n_samples, n_samples))
        all_clusterings = []

        for i in range(n_runs):
            kmeans = KMeans(
                n_clusters=n_clusters,
                random_state=random_state + i,
                **kwargs,
            )
            labels = kmeans.fit_predict(X)
            all_clusterings.append(labels)

            for j in range(n_samples):
                for k in range(j, n_samples):
                    if labels[j] == labels[k]:
                        consensus_matrix[j, k] += 1
                        if j != k:
                            consensus_matrix[k, j] += 1

        consensus_matrix /= n_runs

        # final hierarchical clustering
        distance_matrix = 1 - consensus_matrix
        linkage_matrix = linkage(squareform(distance_matrix), method="average")
        cluster_labels = fcluster(linkage_matrix, n_clusters, criterion="maxclust") - 1

        return cluster_labels, consensus_matrix, n_clusters

    # ------------------------------------------------------
    # CASE B: n_clusters=None ‚Üí auto-select via PAC
    # ------------------------------------------------------
    pac_scores = {}
    consensus_mats = {}

    for k in cluster_range:
        cm = np.zeros((n_samples, n_samples))

        for i in range(n_runs):
            kmeans = KMeans(
                n_clusters=k,
                random_state=random_state + i,
                **kwargs,
            )
            labels = kmeans.fit_predict(X)

            for j in range(n_samples):
                for m in range(j, n_samples):
                    if labels[j] == labels[m]:
                        cm[j, m] += 1
                        if j != m:
                            cm[m, j] += 1

        cm /= n_runs
        consensus_mats[k] = cm

        # PAC = ambiguous proportion in (0.1, 0.9)
        lower, upper = 0.1, 0.9
        flat = cm.flatten()
        pac = np.mean((flat > lower) & (flat < upper))
        pac_scores[k] = pac

    # best cluster = lowest PAC
    optimal_k = min(pac_scores, key=pac_scores.get)
    consensus_matrix = consensus_mats[optimal_k]

    # final hierarchical clustering
    distance_matrix = 1 - consensus_matrix
    linkage_matrix = linkage(squareform(distance_matrix), method="average")
    final_labels = fcluster(linkage_matrix, optimal_k, criterion="maxclust") - 1

    return final_labels, consensus_matrix, optimal_k

def _determine_optimal_clusters_consensus(consensus_matrix, max_k=10):
    """Determine optimal n_cluster using PAC score."""
    pac_scores = []

    for k in range(2, max_k + 1):
        # Calculate PAC score (Proportion of Ambiguous Clustering)
        # PAC = proportion of entries in consensus matrix between (0.1, 0.9)
        pac = np.sum((consensus_matrix > 0.1) & (consensus_matrix < 0.9)) / (
            consensus_matrix.size - consensus_matrix.shape[0]
        )
        pac_scores.append(pac)

    # Choose k with minimum PAC score
    optimal_k = np.argmin(pac_scores) + 2
    return optimal_k

def _hdbscan_approximation(X, min_cluster_size=5, **kwargs):
    """
    Approximate HDBSCAN using DBSCAN with varying epsilon values.
    Note: This is an approximation. For real HDBSCAN, install hdbscan package.
    """
    from sklearn.neighbors import NearestNeighbors
    from sklearn.cluster import DBSCAN
    # Compute k-distance graph
    nbrs = NearestNeighbors(n_neighbors=min_cluster_size).fit(X)
    distances, indices = nbrs.kneighbors(X)
    k_distances = np.sort(distances[:, -1])

    # Find elbow point in k-distance graph
    x_range = np.arange(len(k_distances))
    coords = np.vstack([x_range, k_distances]).T
    vec = coords[-1] - coords[0]
    vec_norm = vec / np.linalg.norm(vec)

    distances_from_line = []
    for coord in coords:
        vec_to_point = coord - coords[0]
        distance = np.linalg.norm(
            vec_to_point - np.dot(vec_to_point, vec_norm) * vec_norm
        )
        distances_from_line.append(distance)

    elbow_idx = np.argmax(distances_from_line)
    eps = k_distances[elbow_idx]

    # Run DBSCAN with estimated epsilon
    dbscan = DBSCAN(eps=eps, min_samples=min_cluster_size, **kwargs)
    cluster_labels = dbscan.fit_predict(X)
    n_clusters = len(np.unique(cluster_labels)) - (1 if -1 in cluster_labels else 0)

    return cluster_labels, n_clusters

def _plot_dendrogram(X, cluster_labels, method):
    """Plot dendrogram for hierarchical clustering.""" 
    from scipy.cluster.hierarchy import linkage, dendrogram
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))

    # Plot linkage matrix
    Z = linkage(X, method="ward")
    dendrogram(Z, ax=ax1, truncate_mode="lastp", p=12, show_leaf_counts=True)
    ax1.set_title("Hierarchical Clustering Dendrogram")
    ax1.set_xlabel("Sample index")
    ax1.set_ylabel("Distance")

    # Plot cluster distribution
    unique_labels, counts = np.unique(cluster_labels, return_counts=True)
    bars = ax2.bar(unique_labels.astype(str), counts)
    ax2.set_title(f"Cluster Distribution ({method})")
    ax2.set_xlabel("Cluster")
    ax2.set_ylabel("Number of Points")

    # Add count labels on bars
    for bar, count in zip(bars, counts):
        ax2.text(
            bar.get_x() + bar.get_width() / 2,
            bar.get_height(),
            str(count),
            ha="center",
            va="bottom",
        )
    try:
        plt.tight_layout()
    except RuntimeError:
        plt.subplots_adjust(top=0.92, right=0.92)

def _plot_consensus_matrix(consensus_matrix, cluster_labels, ax=None,cmap="coolwarm",aspect="equal",boundary_color='r',boundary_linewidth=0.5, kws_figsets={}):
    """Plot consensus matrix."""
    if ax is None:
        nexttile=subplot(3,2,figsize=(9,9)) 
    ax1, ax2 = nexttile(2,2), nexttile(1,2)
    # Sort indices by cluster labels
    sorted_idx = np.argsort(cluster_labels)
    sorted_matrix = consensus_matrix[sorted_idx][:, sorted_idx]
    sorted_labels = cluster_labels[sorted_idx]

    # Plot consensus matrix
    im = ax1.imshow(sorted_matrix, cmap=cmap, aspect=aspect)
    ax1.set_title("Consensus Matrix")
    ax1.set_xlabel("Samples")
    ax1.set_ylabel("Samples")
    plt.colorbar(im, ax=ax1)

    # Add cluster boundaries
    unique_labels = np.unique(sorted_labels)
    boundaries = []
    for label in unique_labels:
        boundaries.append(np.where(sorted_labels == label)[0][-1])

    for boundary in boundaries[:-1]:
        ax1.axhline(y=boundary, color=boundary_color, linewidth=boundary_linewidth)
        ax1.axvline(x=boundary, color=boundary_color, linewidth=boundary_linewidth)
    if kws_figsets:
        figsets(ax1,**kws_figsets)
    # Plot consensus distribution
    upper_tri_indices = np.triu_indices_from(consensus_matrix, k=1)
    consensus_values = consensus_matrix[upper_tri_indices]

    ax2.hist(consensus_values, bins=50, edgecolor="black", alpha=0.7)
    ax2.set_title(f"\nDistribution of Consensus Values")
    ax2.set_xlabel("Consensus Value")
    ax2.set_ylabel("Frequency")
    ax2.axvline(x=0.5, color="red", linestyle="--", label="Ambiguity threshold")
    ax2.legend()

    # Calculate and display PAC score
    pac = np.sum((consensus_values > 0.1) & (consensus_values < 0.9)) / len(
        consensus_values
    )
    ax2.text(
        0.1,
        0.95,
        f"PAC: {pac:.3f}",
        transform=ax2.transAxes,
        verticalalignment="top",
        bbox=dict(boxstyle="round", facecolor="wheat", alpha=0.5),
    )
    figsets(ax2,sp=3,**kws_figsets)
    # plt.tight_layout()
    # try:
    #     plt.tight_layout()
    # except RuntimeError:
    #     plt.subplots_adjust(top=0.92, right=0.92)


def _plot_hierarchy(X, n_clusters, kws_figsets={}, **kwargs):
    """Plot hierarchical clustering with multiple linkage methods."""
    from scipy.cluster.hierarchy import linkage, dendrogram
    linkage_methods = ["single", "complete", "average", "ward"]

    fig, axes = plt.subplots(2, 2, figsize=(12, 10))
    axes = axes.flatten()

    for idx, method in enumerate(linkage_methods):
        ax = axes[idx]
        Z = linkage(X, method=method)

        # Plot dendrogram
        dendrogram(
            Z,
            ax=ax,
            truncate_mode="lastp",
            p=min(20, len(X)),
            show_leaf_counts=True,
            color_threshold=Z[-n_clusters + 1, 2] if n_clusters > 1 else None,
        )
        ax.set_title(f"Dendrogram ({method} linkage)")
        ax.set_xlabel("Sample index" if idx in [2, 3] else "")
        ax.set_ylabel("Distance")

        if n_clusters > 1:
            ax.axhline(y=Z[-n_clusters + 1, 2], color="red", linestyle="--")

    plt.suptitle(
        "Hierarchical Clustering with Different Linkage Methods", fontsize=14
    )
    figsets(ax,**kws_figsets)


def _plot_pca_clusters(X, cluster_labels, method):
    """Plot clusters in PCA space."""
    from sklearn.decomposition import PCA

    # Perform PCA
    pca = PCA(n_components=2)
    X_pca = pca.fit_transform(X)

    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))

    # Plot clusters in PCA space
    scatter = ax1.scatter(
        X_pca[:, 0],
        X_pca[:, 1],
        c=cluster_labels,
        cmap="tab10",
        alpha=0.6,
        edgecolors="w",
        linewidth=0.5,
    )
    ax1.set_title(f"Clusters in PCA Space ({method})")
    ax1.set_xlabel(f"PC1 ({pca.explained_variance_ratio_[0]:.1%})")
    ax1.set_ylabel(f"PC2 ({pca.explained_variance_ratio_[1]:.1%})")
    plt.colorbar(scatter, ax=ax1, label="Cluster")

    # Plot explained variance
    pca_full = PCA().fit(X)
    ax2.plot(
        range(1, len(pca_full.explained_variance_ratio_) + 1),
        np.cumsum(pca_full.explained_variance_ratio_),
        "bo-",
    )
    ax2.axhline(y=0.95, color="r", linestyle="--", alpha=0.5, label="95% variance")
    ax2.axhline(
        y=0.90, color="orange", linestyle="--", alpha=0.5, label="90% variance"
    )
    ax2.set_xlabel("Number of Components")
    ax2.set_ylabel("Cumulative Explained Variance")
    ax2.set_title("PCA Explained Variance")
    ax2.grid(True, alpha=0.3)
    ax2.legend() 

# Additional helper functions for the original plots (keep these)
def _plot_silhouette_metrics(
    range_n_clusters,
    silhouette_scores,
    inertia_scores,
    ch_scores,
    db_scores,
    optimal_clusters,
    method,
    ax=None,kws_figsets={}
):
    """Plot silhouette score and other metrics vs n_cluster."""
    if ax is None:
        fig, ax = plt.subplots(figsize=(5, 4))

    colors = get_color(3)
    ax.plot(
        range_n_clusters,
        silhouette_scores,
        "o-",
        color=colors[0],
        label="Silhouette Score",
    )
    ax.set_xlabel("n_cluster")
    ax.set_ylabel("Silhouette Score", color=colors[0])
    ax.tick_params(axis="y", labelcolor=colors[0])
    ax.set_xticks(range_n_clusters)
    ax.grid(True, alpha=0.3)

    # Plot other metrics on secondary axes
    ax2 = ax.twinx()
    ax2.plot(
        range_n_clusters,
        ch_scores,
        "s--",
        color=colors[1],
        label="Calinski-Harabasz",
    )
    ax2.set_ylabel("Calinski-Harabasz", color=colors[1])
    ax2.tick_params(axis="y", labelcolor=colors[1])

    ax3 = ax.twinx()
    ax3.spines["right"].set_position(("outward", 60))
    ax3.plot(
        range_n_clusters, db_scores, "^:", color=colors[2], label="Davies-Bouldin"
    )
    ax3.set_ylabel("Davies-Bouldin", color=colors[2])
    ax3.tick_params(axis="y", labelcolor=colors[2])

    ax.axvline(
        x=optimal_clusters,
        linestyle="--",
        color="black",
        label=f"Optimal (k={optimal_clusters})",
    )

    lines1, labels1 = ax.get_legend_handles_labels()
    lines2, labels2 = ax2.get_legend_handles_labels()
    lines3, labels3 = ax3.get_legend_handles_labels()
    ax.legend(
        lines1 + lines2 + lines3, labels1 + labels2 + labels3, loc="upper right"
    )

    plt.title(f"Clustering Metrics vs n_cluster ({method.upper()})")

    figsets(ax,**kws_figsets)
    figsets(ax2,**kws_figsets)
    figsets(ax3,**kws_figsets)

def _plot_elbow_method(
    range_n_clusters, inertia_scores, optimal_clusters, method, ax=None,kws_figsets={}
):
    """Plot elbow method for determining optimal n_cluster."""

    if ax is None:
        fig, ax = plt.subplots(figsize=(5, 4))
    ax.plot(range_n_clusters, inertia_scores, "o")
    ax.set_xlabel("n_cluster")

    ylabel_map = {
        "kmeans": "Inertia",
        "gmm": "Negative BIC",
        "agglomerative": "Sum of Squared Distances",
        "spectral": "Sum of Squared Distances",
        "birch": "Number of Subclusters",
    }
    ax.set_ylabel(ylabel_map.get(method, "Inertia"))

    ax.set_title(f"Elbow Method for Optimal k ({method.upper()})")
    ax.grid(True, alpha=0.3)
    ax.set_xticks(range_n_clusters)
    ax.axvline(
        x=optimal_clusters,
        linestyle="--",
        color="r",
        label=f"Optimal k={optimal_clusters}",
    )
    ax.legend()
    figsets(ax,**kws_figsets)

def _plot_silhouette_analysis(X, cluster_labels, n_clusters, ax=None,kws_figsets={}):
    """Plot silhouette analysis for each cluster."""
    from sklearn.metrics import silhouette_samples
    silhouette_vals = silhouette_samples(X, cluster_labels)

    if ax is None:
        fig, ax = plt.subplots(figsize=(5, 4))

    y_lower = 10

    color = get_color(n_clusters)
    for i in range(n_clusters):
        ith_cluster_silhouette = silhouette_vals[cluster_labels == i]
        ith_cluster_silhouette.sort()

        size_cluster_i = ith_cluster_silhouette.shape[0]
        y_upper = y_lower + size_cluster_i

        ax.fill_betweenx(
            np.arange(y_lower, y_upper),
            0,
            ith_cluster_silhouette,
            facecolor=color[i],
            edgecolor=color[i],
            alpha=1,
        )

        ax.text(-0.01, y_lower + 0.5 * size_cluster_i, str(i + 1))
        y_lower = y_upper + 10

    ax.axvline(
        x=np.mean(silhouette_vals),
        color="red",
        linestyle="--",
        lw=2,
        label=f"mean: {np.mean(silhouette_vals):.3f}",
    )
    ax.yaxis.set_visible(False)

    
    ax.legend()
    xlabel = kws_figsets.pop("xlabel", "Silhouette Coefficient Values")
    ylabel = kws_figsets.pop("ylabel", "n_clusters")
    title = kws_figsets.pop("title", "Silhouette Plot")
    box = kws_figsets.pop("box", "b")
    figsets(
        ax,
        xlabel=xlabel,
        ylabel=ylabel,
        title=title,
        box=box,
        **kws_figsets
    )

def _plot_pairplot(X, cluster_labels, feature_names, method):
    """Plot pairplot of clusters."""
    n_features = X.shape[1]

    if n_features <= 10:
        if n_features <= 4:
            feature_names_plot = feature_names[:n_features]
        else:
            feature_names_plot = feature_names[:4]

        df_plot = pd.DataFrame(
            X[:, : len(feature_names_plot)], columns=feature_names_plot
        )
        df_plot["Cluster"] = cluster_labels

        # Remove noise points if present
        if -1 in df_plot["Cluster"].unique():
            df_plot = df_plot[df_plot["Cluster"] != -1].copy()

        g = sns.pairplot(
            df_plot,
            hue="Cluster",
            palette="tab10",
            diag_kind="kde",
            plot_kws={"alpha": 0.6},
        )
        g.fig.suptitle(f"Pairplot of Clusters ({method.upper()})", y=1.02)
        
        try:
            plt.tight_layout()
        except RuntimeError:
            plt.subplots_adjust(top=0.92, right=0.92)

def _plot_metrics_comparison(
    range_n_clusters,
    silhouette_scores,
    ch_scores,
    db_scores,
    optimal_clusters,
    method,
):
    """Plot comparison of different clustering metrics."""
    nexttile = subplot(2, 3, figsize=[10, 8])
    metrics = [
        ("Silhouette Score", silhouette_scores, "Higher is better"),
        ("Calinski-Harabasz", ch_scores, "Higher is better"),
        ("Davies-Bouldin", db_scores, "Lower is better"),
    ]
    colors = get_color(3)
    for idx, (name, scores, desc) in enumerate(metrics):
        if len(scores) == 0:
            continue
        ax = nexttile()
        ax.plot(range_n_clusters, scores, "o-", linewidth=2, color=colors[idx])
        ax.axvline(
            x=optimal_clusters, linestyle="--", linewidth=2, color="red", alpha=1
        )
        ax.set_xlabel("n_cluster")
        ax.set_ylabel(name)
        ax.set_title(f"{name} vs n_cluster")
        ax.grid(True, alpha=0.3)
        ax.set_xticks(range_n_clusters)

        opt_idx = list(range_n_clusters).index(optimal_clusters)
        ax.plot(optimal_clusters, scores[opt_idx], "ro", markersize=10)
        ax.text(
            optimal_clusters,
            scores[opt_idx],
            f"  {scores[opt_idx]:.3f}",
            verticalalignment="bottom",
        )

    # Normalized metrics comparison
    ax = nexttile(1, 3)
    if len(silhouette_scores) > 0:
        norm_silhouette = (silhouette_scores - np.min(silhouette_scores)) / (
            np.max(silhouette_scores) - np.min(silhouette_scores)
        )
    else:
        norm_silhouette = np.zeros_like(range_n_clusters)

    if len(ch_scores) > 0:
        norm_ch = (ch_scores - np.min(ch_scores)) / (
            np.max(ch_scores) - np.min(ch_scores)
        )
    else:
        norm_ch = np.zeros_like(range_n_clusters)

    if len(db_scores) > 0:
        norm_db = 1 - (
            (db_scores - np.min(db_scores))
            / (np.max(db_scores) - np.min(db_scores))
        )
    else:
        norm_db = np.zeros_like(range_n_clusters)

    ax.plot(
        range_n_clusters,
        norm_silhouette,
        "o-",
        color=colors[0],
        linewidth=2,
        label="Silhouette (norm)",
    )
    ax.plot(
        range_n_clusters,
        norm_ch,
        "s-",
        color=colors[1],
        linewidth=2,
        label="Calinski-Harabasz (norm)",
    )
    ax.plot(
        range_n_clusters,
        norm_db,
        "^-",
        color=colors[2],
        linewidth=2,
        label="Davies-Bouldin (1-norm)",
    )
    ax.axvline(
        x=optimal_clusters, linestyle="--", linewidth=2, color="red", alpha=1
    )
    ax.set_xlabel("n_cluster")
    ax.set_ylabel("Normalized Score")
    ax.set_title("Normalized Metrics Comparison")
    ax.legend()
    ax.grid(True, alpha=0.3)
    ax.set_xticks(range_n_clusters)

    plt.suptitle(f"Clustering Metrics Comparison ({method.upper()})", fontsize=14)
    
    try:
        plt.tight_layout()
    except RuntimeError:
        plt.subplots_adjust(top=0.92, right=0.92)
    return ax

def compare_clustering_methods(
    data,
    columns=None,
    n_clusters=4,
    scale=True,
    methods=None,
    random_state=1,
    **kwargs,
):
    """Compare different clustering methods on the same data."""

    if methods is None:
        methods = ["kmeans", "agglomerative", "gmm", "spectral", "birch"]

    results = {}
    n_methods = len(methods)
    nexttile = subplot(n_methods, 3, figsize=(15, 5 * n_methods))

    for idx, method in enumerate(methods):
        print(f"\n{'='*60}")
        print(f"Running {method.upper()} clustering")
        print("=" * 60)

        try:
            # Perform clustering
            clustered_data, optimal_k, model, metrics, _ = df_cluster(
                data=data,
                columns=columns,
                n_clusters=n_clusters,
                method=method,
                scale=scale,
                plot=False,
                random_state=random_state,
                **kwargs,
            )

            results[method] = {
                "data": clustered_data,
                "n_clusters": optimal_k,
                "model": model,
                "metrics": metrics,
            }

            # Get data for plotting
            X_data = (
                data[columns].values
                if columns
                else data.select_dtypes(include=[np.number]).values
            )
            cluster_col = f"Cluster_{method}"
            cluster_labels = clustered_data[cluster_col].values

            # First subplot: Scatter plot (first two features)
            ax1 = nexttile()
            scatter = ax1.scatter(
                X_data[:, 0],
                X_data[:, 1],
                c=cluster_labels,
                cmap=get_cmap("coolwarm", n=2),
                alpha=0.6,
            )
            ax1.set_title(f"{method.upper()} (k={optimal_k})")
            ax1.set_xlabel("Feature 1")
            ax1.set_ylabel("Feature 2")
            plt.colorbar(scatter, ax=ax1, label="Cluster")

            # Second subplot: Metrics comparison
            ax2 = nexttile()
            if metrics:
                metric_names = ["Silhouette", "C-H", "1/DB"]
                metric_values = [
                    metrics.get("silhouette", 0),
                    metrics.get("calinski_harabasz", 0),
                    (
                        1 / metrics.get("davies_bouldin", 1)
                        if metrics.get("davies_bouldin", 0) > 0
                        else 0
                    ),
                ]

                bars = ax2.bar(
                    metric_names, metric_values, color=get_color(len(metric_names))
                )
                ax2.set_title(f"{method.upper()} Metrics")
                ax2.set_ylabel("Score")
                ax2.set_ylim(
                    0, max(metric_values) * 1.2 if max(metric_values) > 0 else 1
                )

                for bar, val in zip(bars, metric_values):
                    ax2.text(
                        bar.get_x() + bar.get_width() / 2,
                        bar.get_height(),
                        f"{val:.3f}",
                        ha="center",
                        va="bottom",
                    )
            else:
                ax2.text(
                    0.5,
                    0.5,
                    "No metrics available",
                    ha="center",
                    va="center",
                    transform=ax2.transAxes,
                )
                ax2.set_title(f"{method.upper()} Metrics")

            # Third subplot: Cluster size distribution
            ax3 = nexttile()
            unique_labels, counts = np.unique(cluster_labels, return_counts=True)

            # Handle noise label (-1)
            if -1 in unique_labels:
                noise_idx = np.where(unique_labels == -1)[0][0]
                unique_labels[noise_idx] = "Noise"

            bars = ax3.bar(
                unique_labels.astype(str),
                counts,
                color=get_color(len(unique_labels)),
                edgecolor="black",
            )
            ax3.set_title(f"Cluster Distribution ({method})")
            ax3.set_xlabel("Cluster")
            ax3.set_ylabel("Count")

            for bar, count in zip(bars, counts):
                ax3.text(
                    bar.get_x() + bar.get_width() / 2,
                    bar.get_height(),
                    str(count),
                    ha="center",
                    va="bottom",
                )

        except Exception as e:
            print(e)

    plt.suptitle("Clustering Methods Comparison", fontsize=16, y=1.02)
    plt.subplots_adjust(
        top=0.9, bottom=0.1, left=0.1, right=0.95, hspace=0.4, wspace=0.3
    )

    # Print summary table
    print("\n" + "=" * 80)
    print("CLUSTERING METHODS COMPARISON SUMMARY")
    print("=" * 80)
    print(
        f"{'Method':<15} {'Clusters':<10} {'Silhouette':<12} {'C-H':<12} {'D-B':<12} {'Noise':<10}"
    )
    print("-" * 80)

    for method, result in results.items():
        if "error" not in result:
            metrics = result["metrics"]
            silhouette = metrics.get("silhouette", "N/A")
            ch = metrics.get("calinski_harabasz", "N/A")
            db = metrics.get("davies_bouldin", "N/A")
            noise = metrics.get("n_noise", 0)

            if isinstance(silhouette, (int, float)):
                silhouette = f"{silhouette:.4f}"
            if isinstance(ch, (int, float)):
                ch = f"{ch:.1f}"
            if isinstance(db, (int, float)):
                db = f"{db:.4f}"

            print(
                f"{method:<15} {result['n_clusters']:<10} {silhouette:<12} {ch:<12} {db:<12} {noise:<10}"
            )

    return results

def cluster_stability_analysis(
    data,
    columns=None,
    method="kmeans",
    n_runs=10,
    n_clusters=4,
    scale=True,
    random_state=1,
):
    """
    Analyze clustering stability by running multiple times with different seeds.

    Parameters:
    -----------
    data : pd.DataFrame
        Input data
    columns : list, optional
        Columns to use for clustering
    method : str
        Clustering method
    n_runs : int
        Number of runs with different random seeds
    n_clusters : int
        Number of clusters
    scale : bool
        Whether to scale data
    random_state : int
        Base random seed

    Returns:
    --------
    dict: Stability metrics and results
    """
    from sklearn.metrics import adjusted_rand_score

    if columns is None:
        numeric_cols = data.select_dtypes(include=[np.number]).columns.tolist()
        X = data[numeric_cols].values
    else:
        X = data[columns].values

    if scale:
        from sklearn.preprocessing import StandardScaler
        scaler = StandardScaler()
        X = scaler.fit_transform(X)

    all_labels = []
    metrics_history = []

    for i in range(n_runs):
        seed = random_state + i

        clustered_data, _, _, metrics, _ = df_cluster(
            data=data,
            columns=columns,
            n_clusters=n_clusters,
            method=method,
            scale=False,  # Already scaled
            plot=False,
            random_state=seed,
        )

        cluster_col = f"Cluster_{method}"
        labels = clustered_data[cluster_col].values
        all_labels.append(labels)
        metrics_history.append(metrics)

    # Calculate stability metrics
    stability_metrics = {
        "pairwise_ari": [],  # Adjusted Rand Index between pairs
        "label_variation": [],
    }

    # Calculate pairwise ARI
    for i in range(n_runs):
        for j in range(i + 1, n_runs):
            ari = adjusted_rand_score(all_labels[i], all_labels[j])
            stability_metrics["pairwise_ari"].append(ari)

    # Calculate label variation (how often points change clusters)
    n_samples = len(all_labels[0])
    label_changes = np.zeros(n_samples)

    for i in range(n_samples):
        labels_i = [run_labels[i] for run_labels in all_labels]
        # Count how many unique clusters this point was assigned to
        label_changes[i] = len(np.unique(labels_i))

    stability_metrics["avg_label_changes"] = np.mean(label_changes)
    stability_metrics["std_label_changes"] = np.std(label_changes)
    stability_metrics["max_label_changes"] = np.max(label_changes)

    # Plot stability analysis
    fig, axes = plt.subplots(1, 3, figsize=(15, 5))

    # Plot pairwise ARI distribution
    axes[0].hist(
        stability_metrics["pairwise_ari"], bins=20, edgecolor="black", alpha=0.7
    )
    axes[0].axvline(
        x=np.mean(stability_metrics["pairwise_ari"]),
        color="red",
        linestyle="--",
        label=f'Mean: {np.mean(stability_metrics["pairwise_ari"]):.3f}',
    )
    axes[0].set_xlabel("Adjusted Rand Index")
    axes[0].set_ylabel("Frequency")
    axes[0].set_title("Pairwise Cluster Agreement")
    axes[0].legend()
    axes[0].grid(True, alpha=0.3)

    # Plot label changes per point
    axes[1].hist(
        label_changes,
        bins=np.arange(0.5, n_runs + 1.5, 1),
        edgecolor="black",
        alpha=0.7,
        rwidth=0.8,
    )
    axes[1].set_xlabel("Number of Different Clusters Assigned")
    axes[1].set_ylabel("Number of Points")
    axes[1].set_title("Cluster Assignment Stability")
    axes[1].grid(True, alpha=0.3)

    # Plot metric variability across runs
    if metrics_history and "silhouette" in metrics_history[0]:
        silhouette_scores = [m.get("silhouette", 0) for m in metrics_history]
        axes[2].plot(range(n_runs), silhouette_scores, "bo-")
        axes[2].fill_between(
            range(n_runs),
            np.array(silhouette_scores) - np.std(silhouette_scores),
            np.array(silhouette_scores) + np.std(silhouette_scores),
            alpha=0.2,
        )
        axes[2].set_xlabel("Run")
        axes[2].set_ylabel("Silhouette Score")
        axes[2].set_title("Metric Variability Across Runs")
        axes[2].grid(True, alpha=0.3)

    plt.suptitle(
        f"Clustering Stability Analysis ({method.upper()}, k={n_clusters})",
        fontsize=14,
    )
    # Avoid Matplotlib colorbar layout engine bug
    try:
        plt.tight_layout()
    except RuntimeError:
        plt.subplots_adjust(top=0.92, right=0.92)


    print(f"\nStability Analysis for {method.upper()} (k={n_clusters}):")
    print(
        f"  Average pairwise ARI: {np.mean(stability_metrics['pairwise_ari']):.4f}"
    )
    print(f"  Std pairwise ARI: {np.std(stability_metrics['pairwise_ari']):.4f}")
    print(
        f"  Average label changes per point: {stability_metrics['avg_label_changes']:.2f}"
    )

    return stability_metrics

 
def df_cluster(
    data: pd.DataFrame,
    columns: Optional[List[str]] = None,
    n_clusters: Optional[int] = None,
    range_n_clusters: Union[range, np.ndarray] = range(2, 11),
    method: str = "kmeans",
    scale: bool = True,
    plot: Union[str, List[str]] = "all",
    inplace: bool = False,
    ax=None,
    random_state: int = 1,
    consensus_n_runs: int = 100,
    **kwargs,
):
    """
    Performs clustering analysis on the provided data using various clustering algorithms.

    Parameters:
    -----------
    data : pd.DataFrame
        Input DataFrame containing the data to cluster.

    columns : Optional[List[str]], default=None
        List of column names to use for clustering. If None, uses all numeric columns.

    n_clusters : Optional[int], default=None
        n_cluster to create. If None, determined automatically based on silhouette score.

    range_n_clusters : Union[range, np.ndarray], default=range(2, 11)
        Range of cluster numbers to evaluate when n_clusters is None.

    method : str, default="kmeans"
        Clustering method to use. Options:
        - "kmeans": K-Means clustering
        - "dbscan": DBSCAN density-based clustering
        - "optics": OPTICS density-based clustering
        - "agglomerative": Agglomerative hierarchical clustering
        - "gmm": Gaussian Mixture Models
        - "spectral": Spectral clustering
        - "birch": BIRCH hierarchical clustering
        - "meanshift": Mean Shift clustering
        - "consensus": Consensus clustering (K-Means based)
        - "hdbscan": HDBSCAN (approximated using DBSCAN with varying parameters)

    scale : bool, default=True
        Whether to standardize features before clustering.

    plot : Union[str, List[str]], default="all"
        Which plots to generate. Options: "all", "silhouette", "elbow", "pairplot",
        "metrics", "dendrogram", "consensus_matrix", "hierarchy", "pca".

    inplace : bool, default=False
        Whether to add cluster labels to the original DataFrame.

    ax : matplotlib.axes.Axes, default=None
        Existing matplotlib axes to plot on.

    random_state : int, default=1
        Random seed for reproducibility.

    consensus_n_runs : int, default=100
        Number of runs for consensus clustering.

    **kwargs : dict
        Additional parameters to pass to the clustering algorithm.

    Returns:
    --------
    tuple: (data_with_clusters, n_clusters, model, metrics_dict, ax)
        - data_with_clusters: DataFrame with cluster labels (or None if inplace=True)
        - n_clusters: Number of clusters used/optimal
        - model: Fitted clustering model
        - metrics_dict: Dictionary of clustering metrics
        - ax: Matplotlib axes object


        # ! Interpreting the plots from your K-Means clustering analysis
        # ! 1. Silhouette Score and Inertia vs Number of Clusters
        # Description:
        # This plot has two y-axes: the left y-axis represents the Silhouette Score, and the right y-axis
        # represents Inertia.
        # The x-axis represents the number of clusters (k).

        # Interpretation:

        # Silhouette Score:
        # Ranges from -1 to 1, where a score close to 1 indicates that points are well-clustered, while a
        # score close to -1 indicates that points might be incorrectly clustered.
        # A higher silhouette score generally suggests that the data points are appropriately clustered.
        # Look for the highest value to determine the optimal number of clusters.

        # Inertia:
        # Represents the sum of squared distances from each point to its assigned cluster center.
        # Lower inertia values indicate tighter clusters.
        # As the number of clusters increases, inertia typically decreases, but the rate of decrease
        # may slow down, indicating diminishing returns for additional clusters.

        # Optimal Number of Clusters:
        # You can identify an optimal number of clusters where the silhouette score is maximized and
        # inertia starts to plateau (the "elbow" point).
        # This typically suggests that increasing the number of clusters further yields less meaningful
        # separations.

        # ! 2. Elbow Method Plot
        # Description:
        # This plot shows the Inertia against the number of clusters.

        # Interpretation:
        # The elbow point is where the inertia begins to decrease at a slower rate. This point suggests that
        # adding more clusters beyond this point does not significantly improve the clustering performance.
        # Look for a noticeable bend in the curve to identify the optimal number of clusters, indicated by the
        # vertical dashed line.
        # Inertia plot

        # ! Silhouette Plots
        # 3. Silhouette Plot for Various Clusters
        # Description:
        # This horizontal bar plot shows the silhouette coefficient values for each sample, organized by cluster.

        # Interpretation:
        # Each bar represents the silhouette score of a sample within a specific cluster. Longer bars indicate
        # that the samples are well-clustered.
        # The height of the bars shows how similar points within the same cluster are to one another compared to
        # points in other clusters.
        # The vertical red dashed line indicates the average silhouette score for all samples.
        # You want the majority of silhouette values to be above the average line, indicating that most points
        # are well-clustered.

        # ‰ª•‰∏ã‰ª£Á†Å‰∏çÁî®ÂÜçË∑ë‰∏ÄÊ¨°‰∫Ü
        # n_clusters = (
        #     np.argmax(silhouette_avg_scores) + 2
        # )  # Optimal clusters based on max silhouette score
        # kmeans = KMeans(n_clusters=n_clusters, random_state=1)
        # cluster_labels = kmeans.fit_predict(X)

        # ! pairplot of the clusters
        # Overview of the Pairplot
        # Axes and Grid:
        # The pairplot creates a grid of scatter plots for each pair of features in your dataset.
        # Each point in the scatter plots represents a sample from your dataset, colored according to its cluster assignment.

        # Diagonal Elements:
        # The diagonal plots usually show the distribution of each feature. In this case, since X.shape[1] <= 4,
        # there will be a maximum of four features plotted against each other. The diagonal could display histograms or
        # kernel density estimates (KDE) for each feature.

        # Interpretation of the Pairplot

        # Feature Relationships:
        # Look at each scatter plot in the off-diagonal plots. Each plot shows the relationship between two features. Points that
        # are close together in the scatter plot suggest similar values for those features.
        # Cluster Separation: You want to see clusters of different colors (representing different clusters) that are visually distinct.
        # Good separation indicates that the clustering algorithm effectively identified different groups within your data.
        # Overlapping Points: If points from different clusters overlap significantly in any scatter plot, it indicates that those clusters
        # might not be distinct in terms of the two features being compared.
        # Cluster Characteristics:
        # Shape and Distribution: Observe the shape of the clusters. Are they spherical, elongated, or irregular? This can give insights
        # into how well the K-Means (or other clustering methods) has performed:
        # Spherical Clusters: Indicates that clusters are well defined and separated.
        # Elongated Clusters: May suggest that the algorithm is capturing variations along specific axes but could benefit from adjustments
        # in clustering parameters or methods.
        # Feature Influence: Identify which features contribute most to cluster separation. For instance, if you see that one feature
        # consistently separates two clusters, it may be a key factor for clustering.
        # Diagonal Histograms/KDE:
        # The diagonal plots show the distribution of individual features across all samples. Look for:
        # Distribution Shape: Is the distribution unimodal, bimodal, skewed, or uniform?
        # Concentration: Areas with a high density of points may indicate that certain values are more common among samples.
        # Differences Among Clusters: If you see distinct peaks in the histograms for different clusters, it suggests that those clusters are
        # characterized by specific ranges of feature values.
        # Example Observations
        # Feature 1 vs. Feature 2: If there are clear, well-separated clusters in this scatter plot, it suggests that these two features
        # effectively distinguish between the clusters.
        # Feature 3 vs. Feature 4: If you observe significant overlap between clusters in this plot, it may indicate that these features do not
        # provide a strong basis for clustering.
        # Diagonal Plots: If you notice that one cluster has a higher density of points at lower values for a specific feature, while another
        # cluster is concentrated at higher values, this suggests that this feature is critical for differentiating those clusters.

        # Pairplot of the clusters
        # * ‰∏∫‰ªÄ‰πàË¶ÅÈôêÂà∂Âà∞4‰∏™features?
        # 2 features=1 scatter plot            # 3 features=3 scatter plots
        # 4 features=6 scatter plots           # 5 features=10 scatter plots
        # 6 features=15 scatter plots          # 10 features=45 scatter plots
        # Pairplot works well with low-dimensional data, Â¶ÇÊûúÁª¥Â∫¶ÊØîËæÉÈ´òÁöÑËØù, Â≠êÂõæ‰πüÂæàÂ§ö,Â§±Âéª‰∫ÜÂÆÉÁöÑÊÑè‰πâ
    """
    try:
        # from sklearn.cluster import (
        #     KMeans,
        #     DBSCAN,
        #     AgglomerativeClustering,
        #     OPTICS,
        #     SpectralClustering,
        #     Birch,
        #     MeanShift,
        #     estimate_bandwidth,
        # )
        # from sklearn.mixture import GaussianMixture
        from sklearn.metrics import (
            silhouette_score,
            silhouette_samples,
            calinski_harabasz_score,
            davies_bouldin_score,
            pairwise_distances,
        )
        # from sklearn.preprocessing import StandardScaler
        HAS_SKLEARN = True
    except Exception as e:
        print(e)
        HAS_SKLEARN = False
    try:
        from scipy.cluster.hierarchy import dendrogram, linkage, fcluster
        HAS_SCIPY = True
    except Exception as e:
        print(e)
        HAS_SCIPY = False 
    # Select columns for clustering
    if columns is None:
        numeric_cols = data.select_dtypes(include=[np.number]).columns.tolist()
        X = data[numeric_cols].values
        feature_names = numeric_cols
    else:
        X = data[columns].values
        feature_names = columns

    # Store original data
    X_original = X.copy()

    # Scale features if requested
    scaler = None
    if scale:
        from sklearn.preprocessing import StandardScaler
        scaler = StandardScaler()
        X = scaler.fit_transform(X)

    # Initialize metrics storage
    silhouette_avg_scores = []
    inertia_scores = []
    ch_scores = []
    db_scores = []
    method=strcmp(method, ["kmeans","agglomerative","gmm","spectral","birch","consensus","dbscan","optics","meanshift","hdbscan"])[0]
    # Determine optimal n_cluster if not provided
    if n_clusters is None and method not in [
        "dbscan",
        "optics",
        "meanshift",
        "hdbscan",
    ]:
        # Evaluate different numbers of clusters for methods that require k
        for n_cluster in range_n_clusters:
            if method == "kmeans":
                from sklearn.cluster import KMeans
        
                model = KMeans(
                    n_clusters=n_cluster, random_state=random_state, **kwargs
                )
                cluster_labels = model.fit_predict(X)
                inertia_scores.append(model.inertia_)

            elif method == "agglomerative":
                from sklearn.cluster import AgglomerativeClustering
                model = AgglomerativeClustering(n_clusters=n_cluster, **kwargs)
                cluster_labels = model.fit_predict(X)
                # Calculate inertia for agglomerative clustering
                centroids = np.array(
                    [X[cluster_labels == i].mean(axis=0) for i in range(n_cluster)]
                )
                inertia = sum(
                    np.linalg.norm(X[cluster_labels == i] - centroids[i]) ** 2
                    for i in range(n_cluster)
                )
                inertia_scores.append(inertia)

            elif method == "gmm":
                from sklearn.mixture import GaussianMixture
                model = GaussianMixture(
                    n_components=n_cluster, random_state=random_state, **kwargs
                )
                cluster_labels = model.fit(X).predict(X)
                inertia_scores.append(-model.bic(X))

            elif method == "spectral":
                from sklearn.cluster import SpectralClustering
                model = SpectralClustering(
                    n_clusters=n_cluster, random_state=random_state, **kwargs
                )
                cluster_labels = model.fit_predict(X)
                # Calculate inertia for spectral clustering
                centroids = np.array(
                    [X[cluster_labels == i].mean(axis=0) for i in range(n_cluster)]
                )
                inertia = sum(
                    np.linalg.norm(X[cluster_labels == i] - centroids[i]) ** 2
                    for i in range(n_cluster)
                )
                inertia_scores.append(inertia)

            elif method == "birch":
                from sklearn.cluster import Birch
                model = Birch(n_clusters=n_cluster, **kwargs)
                cluster_labels = model.fit_predict(X)
                inertia_scores.append(model.subcluster_centers_.shape[0])

            elif method == "consensus":
                # For consensus clustering, we'll run it later with optimal k
                continue

            # Calculate metrics
            if len(np.unique(cluster_labels)) > 1:
                silhouette_avg = silhouette_score(X, cluster_labels)
                ch_score = calinski_harabasz_score(X, cluster_labels)
                db_score = davies_bouldin_score(X, cluster_labels)
            else:
                silhouette_avg = 0
                ch_score = 0
                db_score = np.inf

            silhouette_avg_scores.append(silhouette_avg)
            ch_scores.append(ch_score)
            db_scores.append(db_score)

            print(f"For n_clusters = {n_cluster}:")
            print(f"  Silhouette score: {silhouette_avg:.4f}")
            print(f"  Calinski-Harabasz: {ch_score:.4f}")
            print(f"  Davies-Bouldin: {db_score:.4f}")

        # Determine optimal n_cluster
        if method != "consensus":
            if silhouette_avg_scores:
                n_clusters = range_n_clusters[np.argmax(silhouette_avg_scores)]
                print(f"\nOptimal n_clusters based on silhouette score: {n_clusters}")
            else:
                n_clusters = range_n_clusters[0]

    # Perform final clustering with selected method
    if method == "kmeans":
        from sklearn.cluster import KMeans
        model = KMeans(n_clusters=n_clusters, random_state=random_state, **kwargs)
        cluster_labels = model.fit_predict(X)
        final_inertia = model.inertia_

    elif method == "dbscan":
        from sklearn.cluster import Birch
        eps = kwargs.get("eps", 0.5)
        min_samples = kwargs.get("min_samples", 5)
        model = DBSCAN(eps=eps, min_samples=min_samples, **kwargs)
        cluster_labels = model.fit_predict(X)
        n_clusters = len(np.unique(cluster_labels)) - (1 if -1 in cluster_labels else 0)
        final_inertia = None

    elif method == "optics":
        from sklearn.cluster import OPTICS
        model = OPTICS(**kwargs)
        cluster_labels = model.fit_predict(X)
        n_clusters = len(np.unique(cluster_labels)) - (1 if -1 in cluster_labels else 0)
        final_inertia = None

    elif method == "agglomerative":
        from sklearn.cluster import AgglomerativeClustering
        model = AgglomerativeClustering(n_clusters=n_clusters, **kwargs)
        cluster_labels = model.fit_predict(X)
        centroids = np.array(
            [X[cluster_labels == i].mean(axis=0) for i in range(n_clusters)]
        )
        final_inertia = sum(
            np.linalg.norm(X[cluster_labels == i] - centroids[i]) ** 2
            for i in range(n_clusters)
        )

    elif method == "gmm":
        from sklearn.mixture import GaussianMixture
        model = GaussianMixture(
            n_components=n_clusters, random_state=random_state, **kwargs
        )
        cluster_labels = model.fit(X).predict(X)
        final_inertia = -model.bic(X)

    elif method == "spectral":
        from sklearn.cluster import SpectralClustering
        model = SpectralClustering(
            n_clusters=n_clusters, random_state=random_state, **kwargs
        )
        cluster_labels = model.fit_predict(X)
        centroids = np.array(
            [X[cluster_labels == i].mean(axis=0) for i in range(n_clusters)]
        )
        final_inertia = sum(
            np.linalg.norm(X[cluster_labels == i] - centroids[i]) ** 2
            for i in range(n_clusters)
        )

    elif method == "birch":
        from sklearn.cluster import Birch
        model = Birch(n_clusters=n_clusters, **kwargs)
        cluster_labels = model.fit_predict(X)
        final_inertia = model.subcluster_centers_.shape[0]

    elif method == "meanshift":
        from sklearn.cluster import MeanShift,estimate_bandwidth
        bandwidth = kwargs.get("bandwidth", None)
        if bandwidth is None:
            bandwidth = estimate_bandwidth(X, quantile=0.2)
        model = MeanShift(bandwidth=bandwidth, **kwargs)
        cluster_labels = model.fit_predict(X)
        n_clusters = len(np.unique(cluster_labels))
        final_inertia = None

    elif method == "consensus":
        # Consensus clustering (based on K-Means)
        cluster_labels, consensus_matrix, n_clusters = _consensus_clustering(
            X,
            n_clusters=n_clusters,
            n_runs=consensus_n_runs,
            random_state=random_state,
            **kwargs,
        )
        model = None  # Consensus doesn't return a single sklearn model
        final_inertia = None

    elif method == "hdbscan":
        # Approximate HDBSCAN using DBSCAN with varying eps
        cluster_labels, n_clusters = _hdbscan_approximation(X, **kwargs)
        model = None
        final_inertia = None

    else:
        raise ValueError(
            f"Unknown method: {method}. Choose from: kmeans, dbscan, optics, agglomerative, gmm, spectral, birch, meanshift, consensus, hdbscan"
        )

    # Calculate final metrics
    metrics = {}
    if len(np.unique(cluster_labels)) > 1 and (
        -1 not in cluster_labels or np.sum(cluster_labels != -1) > 1
    ):
        # always produce a boolean mask
        if -1 in cluster_labels:
            valid_indices = cluster_labels != -1
        else:
            valid_indices = np.ones_like(cluster_labels, dtype=bool)

        if (
            np.sum(valid_indices) > 1
            and len(np.unique(cluster_labels[valid_indices])) > 1
        ):
            metrics["silhouette"] = silhouette_score(
                X[valid_indices], cluster_labels[valid_indices]
            )
            metrics["calinski_harabasz"] = calinski_harabasz_score(
                X[valid_indices], cluster_labels[valid_indices]
            )
            metrics["davies_bouldin"] = davies_bouldin_score(
                X[valid_indices], cluster_labels[valid_indices]
            )
        metrics["n_clusters"] = n_clusters
        if final_inertia is not None:
            metrics["inertia"] = final_inertia
        metrics["n_noise"] = np.sum(cluster_labels == -1) if -1 in cluster_labels else 0

        print(f"\nFinal clustering metrics ({method}):")
        for key, value in metrics.items():
            if isinstance(value, (int, float, np.floating)):
                print(f"  {key}: {value:.4f}")
            else:
                print(f"  {key}: {value}")

    # Plotting
    if plot:
        if plot == "all":
            if method == "consensus":
                plots_to_generate = [
                    "consensus_matrix",
                    "dendrogram",
                    "pairplot",
                    # "metrics",
                    # "silhouette_analysis",
                ]
            elif method == "agglomerative":
                plots_to_generate = [
                    "dendrogram",
                    "silhouette",
                    "pairplot",
                    "metrics",
                    "silhouette_analysis",
                ]
            else:
                plots_to_generate = [
                    "silhouette",
                    "elbow",
                    "pairplot",
                    "metrics",
                    "silhouette_analysis",
                ]
        elif isinstance(plot, str):
            plots_to_generate = [plot]
        else:
            plots_to_generate = plot

        # # Create figure if no axes provided
        # if ax is None and (
        #     "silhouette" in plots_to_generate or "metrics" in plots_to_generate
        # ):
            # fig, ax = plt.subplots(figsize=(12, 8))

        # Generate plots based on method and requested plots
        if (
            "silhouette" in plots_to_generate
            and method not in ["dbscan", "optics", "meanshift", "hdbscan"]
            and n_clusters is not None
        ):
            _plot_silhouette_metrics(
                range_n_clusters,
                silhouette_avg_scores,
                inertia_scores,
                ch_scores,
                db_scores,
                n_clusters,
                method,
                # ax,
            )

        if (
            "elbow" in plots_to_generate
            and method not in ["dbscan", "optics", "meanshift", "hdbscan"]
            and n_clusters is not None
        ):
            try:
                _plot_elbow_method(range_n_clusters, inertia_scores, n_clusters, method)
            except Exception as e:
                print(e)

        if (
            "silhouette_analysis" in plots_to_generate
            and n_clusters is not None
            and n_clusters > 1
        ):
            _plot_silhouette_analysis(X, cluster_labels, n_clusters)
            compare_clustering_methods(
                data,
                columns=columns,
                n_clusters=n_clusters,
                random_state=random_state,
            )

        if "dendrogram" in plots_to_generate and method in [
            "agglomerative",
            "consensus",
        ]:
            _plot_dendrogram(X, cluster_labels, method)

        if "consensus_matrix" in plots_to_generate and method == "consensus":
            _plot_consensus_matrix(consensus_matrix, cluster_labels)

        if "pairplot" in plots_to_generate and X_original.shape[1] <= 10:
            _plot_pairplot(X_original, cluster_labels, feature_names, method)

        if "hierarchy" in plots_to_generate and method == "agglomerative":
            _plot_hierarchy(X, n_clusters, **kwargs)

        if "pca" in plots_to_generate and X.shape[1] > 2:
            _plot_pca_clusters(X, cluster_labels, method)

        if (
            "metrics" in plots_to_generate
            and method not in ["dbscan", "optics", "meanshift", "hdbscan"]
            and n_clusters is not None
        ):
            _plot_metrics_comparison(
                range_n_clusters,
                silhouette_avg_scores,
                ch_scores,
                db_scores,
                n_clusters,
                method,
            )

    # Add cluster labels to DataFrame
    cluster_col_name = f"Cluster_{method}"
    if inplace:
        data[cluster_col_name] = cluster_labels
        if method == "consensus":
            return data, n_clusters, consensus_matrix, metrics, ax
        else:
            return data, n_clusters, model, metrics, ax
    else:
        data_copy = data.copy()
        data_copy[cluster_col_name] = cluster_labels
        if method == "consensus":
            return data_copy, n_clusters, consensus_matrix, metrics, ax
        else:
            return data_copy, n_clusters, model, metrics, ax


#! ========Main Func: df_cluster (Above)========

#! ========Func: df_reducer (Below)========
"""
# You're on the right track, but let's clarify how PCA and clustering (like KMeans) work, especially 
# in the context of your dataset with 7 columns and 23,121 rows.

# Principal Component Analysis (PCA)
# Purpose of PCA:
# PCA is a dimensionality reduction technique. It transforms your dataset from a high-dimensional space 
# (in your case, 7 dimensions corresponding to your 7 columns) to a lower-dimensional space while 
# retaining as much variance (information) as possible.
# How PCA Works:
# PCA computes new features called "principal components" that are linear combinations of the original 
# features.
# The first principal component captures the most variance, the second captures the next most variance 
# (orthogonal to the first), and so on.
# If you set n_components=2, for example, PCA will reduce your dataset from 7 columns to 2 columns. 
# This helps in visualizing and analyzing the data with fewer dimensions.
# Result of PCA:
# After applying PCA, your original dataset with 7 columns will be transformed into a new dataset with 
# the specified number of components (e.g., 2 or 3).
# The transformed dataset will have fewer columns but should capture most of the important information 
# from the original dataset.

# Clustering (KMeans)
# Purpose of Clustering:
# Clustering is used to group data points based on their similarities. KMeans, specifically, partitions 
# your data into a specified number of clusters (groups).
# How KMeans Works:
# KMeans assigns each data point to one of the k clusters based on the feature space (original or 
# PCA-transformed).
# It aims to minimize the variance within each cluster while maximizing the variance between clusters.
# It does not classify the data in each column independently; instead, it considers the overall similarity 
# between data points based on their features.
# Result of KMeans:
# The output will be cluster labels for each data point (e.g., which cluster a particular observation 
# belongs to).
# You can visualize how many groups were formed and analyze the characteristics of each cluster.

# Summary
# PCA reduces the number of features (columns) in your dataset, transforming it into a lower-dimensional
# space.
# KMeans then classifies data points based on the features of the transformed dataset (or the original 
# if you choose) into different subgroups (clusters).
# By combining these techniques, you can simplify the complexity of your data and uncover patterns that 
# might not be visible in the original high-dimensional space. Let me know if you have further questions!
"""

def df_reducer(
    data: pd.DataFrame,
    columns: Optional[List[str]] = None,
    exclude_cols: Optional[List[str]] = None,
    method: str = "umap",  # 'pca', 'umap'
    n_components: int = 2,  # Default for umap, but 50 for PCA
    umap_neighbors: int = 15,  # UMAP-specific
    umap_min_dist: float = 0.1,  # UMAP-specific
    tsne_perplexity: int = 30,  # t-SNE-specific
    hue: str = None,  # lda-specific
    scale: bool = True,
    fill_missing: bool = False,
    size=None,  # for plot marker size
    markerscale=None,  # for plot, legend marker size scale
    edgecolor="none",  # for plot,
    legend_loc="best",  # for plot,
    bbox_to_anchor=None,
    ncols=1,
    debug: bool = False,
    inplace: bool = False,  # replace the oringinal data
    plot_: bool = False,  # plot scatterplot, but no 'hue',so it is meaningless
    random_state=1,
    ax=None,
    figsize=None,
    verbose=True,
    **kwargs,
) -> pd.DataFrame:
    """
    # example:
    # df_reducer(data=data_log, columns=markers, n_components=2) 
    """
    dict_methods = {
        #!Linear Dimensionality Reduction: For simplifying data with techniques that assume linearity.
        "pca": "pca(Principal Component Analysis): \n\tUseful for reducing dimensionality of continuous data while retaining variance. Advantage: Simplifies data, speeds up computation, reduces noise. Limitation: Assumes linear relationships, may lose interpretability in transformed dimensions.",
        "lda": "lda(Linear Discriminant Analysis):\n\tUseful for supervised dimensionality reduction when class separability is important. Advantage: Enhances separability between classes, can improve classification performance. Limitation: Assumes normal distribution and equal class covariances, linear boundaries only.",
        "factor": "factor(Factor Analysis):\n\tSuitable for datasets with observed and underlying latent variables. Advantage: Reveals hidden structure in correlated data, dimensionality reduction with interpretable factors. Limitation: Assumes factors are linear combinations, less effective for nonlinear data.",
        "svd": "svd(Singular Value Decomposition):\n\tSuitable for matrix decomposition, dimensionality reduction in tasks like topic modeling or image compression. Advantage: Efficient, preserves variance, useful in linear transformations. Limitation: Assumes linear relationships, sensitive to noise, may not capture non-linear structure.",
        #! Non-linear Dimensionality Reduction (Manifold Learning)
        "umap": "umap(Uniform Manifold Approximation and Projection):\n\tBest for high-dimensional data visualization (e.g., embeddings). Advantage: Captures complex structure while preserving both local and global data topology. Limitation: Non-deterministic results can vary, sensitive to parameter tuning.",
        "tsne": "tsne(t-Distributed Stochastic Neighbor Embedding):\n\tt-SNE excels at preserving local structure (i.e., clusters), but it often loses global. relationships, causing clusters to appear in arbitrary proximities to each other.  Ideal for clustering and visualizing high-dimensional data, especially for clear cluster separation. Advantage: Captures local relationships effectively. Limitation: Computationally intensive, does not preserve global structure well, requires parameter tuning.",
        "mds": "mds(Multidimensional Scaling):\n\tAppropriate for visualizing pairwise similarity or distance in data. Advantage: Maintains the perceived similarity or dissimilarity between points. Limitation: Computationally expensive for large datasets, less effective for complex, high-dimensional structures.",
        "lle": "lle(Locally Linear Embedding):\n\tUseful for non-linear dimensionality reduction when local relationships are important (e.g., manifold learning). Advantage: Preserves local data structure, good for manifold-type data. Limitation: Sensitive to noise and number of neighbors, not effective for global structure.",
        "kpca": "kpca(Kernel Principal Component Analysis):\n\tGood for non-linear data with complex structure, enhancing separability. Advantage: Extends PCA to capture non-linear relationships. Limitation: Computationally expensive, sensitive to kernel and parameter choice, less interpretable.",
        "ica": "ica(Independent Component Analysis):\n\tEffective for blind source separation (e.g., EEG, audio signal processing).is generally categorized under Non-linear Dimensionality Reduction, but it also serves a distinct role in Blind Source Separation. While ICA is commonly used for dimensionality reduction, particularly in contexts where data sources need to be disentangled (e.g., separating mixed signals like EEG or audio data), it focuses on finding statistically independent components rather than maximizing variance (like PCA) or preserving distances (like MDS or UMAP). Advantage: Extracts independent signals/components, useful in mixed signal scenarios. Limitation: Assumes statistical independence, sensitive to noise and algorithm choice.",
        #! Anomaly Detection: Specialized for detecting outliers or unusual patterns
        "isolation_forest": "Isolation Forest:\n\tDesigned for anomaly detection, especially in high-dimensional data. Advantage: Effective in detecting outliers, efficient for large datasets. Limitation: Sensitive to contamination ratio parameter, not ideal for highly structured or non-anomalous data.",
        #! more methods
        "truncated_svd": "Truncated Singular Value Decomposition (SVD):\n\tEfficient for large sparse datasets, useful for feature reduction in natural language processing (e.g., Latent Semantic Analysis). Advantage: Efficient in memory usage for large datasets. Limitation: Limited in non-linear transformation.",
        "spectral_embedding": "Spectral Embedding:\n\tBased on graph theory, it can be useful for clustering and visualization, especially for data with connected structures. Advantage: Preserves global structure, good for graph-type data. Limitation: Sensitive to parameter choice, not ideal for arbitrary non-connected data.",
        "autoencoder": "Autoencoder:\n\tA neural network-based approach for complex feature learning and non-linear dimensionality reduction. Advantage: Can capture very complex relationships. Limitation: Computationally expensive, requires neural network expertise for effective tuning.",
        "nmf": "Non-negative Matrix Factorization:\n\tEffective for parts-based decomposition, commonly used for sparse and non-negative data, e.g., text data or images. Advantage: Interpretability with non-negativity, efficient with sparse data. Limitation: Less effective for negative or zero-centered data.",
        "umap_hdbscan": "UMAP + HDBSCAN:\n\tCombination of UMAP for dimensionality reduction and HDBSCAN for density-based clustering, suitable for cluster discovery in high-dimensional data. Advantage: Effective in discovering clusters in embeddings. Limitation: Requires careful tuning of both UMAP and HDBSCAN parameters.",
        "manifold_learning": "Manifold Learning (Isomap, Hessian LLE, etc.):\n\tMethods designed to capture intrinsic geometrical structure. Advantage: Preserves non-linear relationships in low dimensions. Limitation: Computationally expensive and sensitive to noise.",
    }

    from sklearn.preprocessing import StandardScaler
    from sklearn.impute import SimpleImputer

    if plot_:
        import matplotlib.pyplot as plt
        import seaborn as sns
    # Check valid method input
    methods = [
        "pca",
        "umap",
        "umap_hdbscan",
        "tsne",
        "factor",
        "isolation_forest",
        "manifold_learning",
        "lda",
        "kpca",
        "ica",
        "mds",
        "lle",
        "svd",
        "truncated_svd",
        "spectral_embedding",
        # "autoencoder","nmf",
    ]
    method = strcmp(method, methods)[0]
    if run_once_within(reverse=True):
        print(f"support methods:{methods}")

    if verbose:
        df_xample = pd.DataFrame({
            "col1": [1.2, 3.3, 2.1, 0.8],
            "col2": [5.1, 4.3, 5.9, 3.8],
            "col3": [7.3, 8.1, 6.2, 7.9],
            "group": ["cond1", "cond1", "cond2", "cond2"]
        }, index=["idx1","idx2","idx3","idx4"])
        print(f"\n example data: \n{df_xample}")
        print(f"\nprocessing with using {dict_methods[method]}")
    xlabel, ylabel = None, None
    # Auto-select numeric columns by default
    if columns is None:
        columns = data.select_dtypes(include="number").columns.tolist()

    # General-purpose exclusion for any type of column
    exclude_cols = kwargs.get("exclude_cols", exclude_cols)
    if exclude_cols:
        columns = [c for c in columns if c not in exclude_cols]

    if verbose:
        if exclude_cols:
            print(f"Excluded columns: {exclude_cols}")
        print(f"Final columns used for reduction: {columns}" if len(columns)<=10 else f"Final columns used for reduction: {columns[:10]}..." )

    if hue is None:
        hue = data.select_dtypes(exclude="number").columns.tolist()
        print(f"auto select the non-number as 'hue':{hue}")

    if isinstance(hue, list):
        if len(hue) == 0:
            hue = None
        elif len(hue) == 1:
            hue = hue[0]
        else:
            print(f"Warning: hue is a list:{hue}, only the 1st '{hue[0]}' will be used")
            hue = hue[0]

    if hue is None or hue == "" or (isinstance(hue, list) and len(hue) == 0):
        # Select columns if specified, else use all columns
        X = data[columns].values if columns else data.values
    else:
        # Select columns to reduce and hue for LDA
        try:
            X = data[columns].values if columns else data.drop(columns=[hue]).values
            y = data[hue].values
        except:
            pass

    # Handle missing values
    if fill_missing:
        imputer = SimpleImputer(strategy="mean")
        X = imputer.fit_transform(X)

    # Optionally scale the data
    if scale:
        scaler = StandardScaler()
        X = scaler.fit_transform(X)

    # Apply PCA if selected
    if method == "pca":
        from sklearn.decomposition import PCA

        pca = PCA(n_components=n_components)
        X_reduced = pca.fit_transform(X)

        # Additional PCA information
        explained_variance = pca.explained_variance_ratio_
        singular_values = pca.singular_values_
        loadings = pca.components_.T * np.sqrt(pca.explained_variance_)

        if debug:
            print(f"PCA completed: Reduced to {n_components} components.")
            print(f"Explained Variance: {explained_variance}")
            print(f"Singular Values: {singular_values}")

        # Plot explained variance if debug=True
        if debug:
            # Plot explained variance
            cumulative_variance = np.cumsum(explained_variance)
            plt.figure(figsize=(8, 5))
            plt.plot(
                range(1, len(cumulative_variance) + 1), cumulative_variance, marker="o"
            )
            plt.title("Cumulative Explained Variance by Principal Components")
            plt.xlabel("Number of Principal Components")
            plt.ylabel("Cumulative Explained Variance")
            plt.axhline(y=0.95, color="r", linestyle="--", label="Threshold (95%)")
            plt.axvline(
                x=n_components,
                color="g",
                linestyle="--",
                label=f"n_components = {n_components}",
            )
            plt.legend()
            plt.grid()
            plt.show()

        # Prepare reduced DataFrame with additional PCA info
        pca_df = pd.DataFrame(
            X_reduced,
            index=data.index,
            columns=[f"PC_{i+1}" for i in range(n_components)],
        )
        # pca_df["Explained Variance"] = np.tile(explained_variance[:n_components], (pca_df.shape[0], 1))
        # pca_df["Singular Values"] = np.tile(singular_values[:n_components], (pca_df.shape[0], 1))
        # Expand explained variance to multiple columns if needed
        for i in range(n_components):
            pca_df[f"Explained Variance PC_{i+1}"] = np.tile(
                format(explained_variance[i] * 100, ".3f") + "%", (pca_df.shape[0], 1)
            )
        for i in range(n_components):
            pca_df[f"Singular Values PC_{i+1}"] = np.tile(
                singular_values[i], (pca_df.shape[0], 1)
            )
        if hue:
            pca_df[hue] = y
    elif method == "lda":
        from sklearn.discriminant_analysis import LinearDiscriminantAnalysis

        if "hue" not in locals() or hue is None:
            raise ValueError(
                "LDA requires a 'hue' col parameter to specify class labels."
            )

        lda_reducer = LinearDiscriminantAnalysis(n_components=n_components)
        X_reduced = lda_reducer.fit_transform(X, y)

        # Prepare reduced DataFrame with additional LDA info
        lda_df = pd.DataFrame(
            X_reduced,
            index=data.index,
            columns=[f"LDA_{i+1}" for i in range(n_components)],
        )
        if debug:
            print(f"LDA completed: Reduced to {n_components} components.")
            print("Class separability achieved by LDA.")
        if hue:
            lda_df[hue] = y
    # Apply UMAP if selected
    elif method == "umap":
        import umap

        umap_reducer = umap.UMAP(
            n_neighbors=umap_neighbors,
            min_dist=umap_min_dist,
            n_components=n_components,
            random_state=random_state
        )
        X_reduced = umap_reducer.fit_transform(X)

        # Additional UMAP information
        embedding = umap_reducer.embedding_
        trustworthiness = umap_reducer._raw_data[:, :n_components]

        if debug:
            print(f"UMAP completed: Reduced to {n_components} components.")
            print(f"Embedding Shape: {embedding.shape}")
            print(f"Trustworthiness: {trustworthiness}")

        # Prepare reduced DataFrame with additional UMAP info
        umap_df = pd.DataFrame(
            X_reduced,
            index=data.index,
            columns=[f"UMAP_{i+1}" for i in range(n_components)],
        )
        umap_df["Embedding"] = embedding[:, 0]  # Example of embedding data
        umap_df["Trustworthiness"] = trustworthiness[:, 0]  # Trustworthiness metric
        if hue:
            umap_df[hue] = y
    elif method == "tsne":
        from sklearn.manifold import TSNE

        tsne = TSNE(
            n_components=n_components,
            perplexity=tsne_perplexity,
            random_state=random_state,
        )
        X_reduced = tsne.fit_transform(X)
        tsne_df = pd.DataFrame(
            X_reduced,
            index=data.index,
            columns=[f"tSNE_{i+1}" for i in range(n_components)],
        )
        tsne_df["Perplexity"] = np.tile(
            f"Perplexity: {tsne_perplexity}", (tsne_df.shape[0], 1)
        )
        if hue:
            tsne_df[hue] = y
    # Apply Factor Analysis if selected
    elif method == "factor":
        from sklearn.decomposition import FactorAnalysis

        factor = FactorAnalysis(n_components=n_components, random_state=random_state)
        X_reduced = factor.fit_transform(X)
        # Factor Analysis does not directly provide explained variance, but we can approximate it
        fa_variance = factor.noise_variance_
        # Prepare reduced DataFrame with additional Factor Analysis info
        factor_df = pd.DataFrame(
            X_reduced,
            index=data.index,
            columns=[f"Factor_{i+1}" for i in range(n_components)],
        )
        factor_df["Noise Variance"] = np.tile(
            format(np.mean(fa_variance) * 100, ".3f") + "%", (factor_df.shape[0], 1)
        )
        if hue:
            factor_df[hue] = y
    # Apply Isolation Forest for outlier detection if selected
    elif method == "isolation_forest":
        from sklearn.decomposition import PCA
        from sklearn.ensemble import IsolationForest

        # Step 1: Apply PCA for dimensionality reduction to 2 components
        pca = PCA(n_components=n_components)
        X_pca = pca.fit_transform(X)

        explained_variance = pca.explained_variance_ratio_
        singular_values = pca.singular_values_

        # Prepare reduced DataFrame with additional PCA info
        iso_forest_df = pd.DataFrame(
            X_pca, index=data.index, columns=[f"PC_{i+1}" for i in range(n_components)]
        )

        isolation_forest = IsolationForest(
            n_estimators=100, contamination="auto", random_state=1
        )
        isolation_forest.fit(X)
        anomaly_scores = isolation_forest.decision_function(
            X
        )  # Anomaly score: larger is less anomalous
        # Predict labels: 1 (normal), -1 (anomaly)
        anomaly_labels = isolation_forest.fit_predict(X)
        # Add anomaly scores and labels to the DataFrame
        iso_forest_df["Anomaly Score"] = anomaly_scores
        iso_forest_df["Anomaly Label"] = anomaly_labels
        # add info from pca
        for i in range(n_components):
            iso_forest_df[f"Explained Variance PC_{i+1}"] = np.tile(
                format(explained_variance[i] * 100, ".3f") + "%",
                (iso_forest_df.shape[0], 1),
            )
        for i in range(n_components):
            iso_forest_df[f"Singular Values PC_{i+1}"] = np.tile(
                singular_values[i], (iso_forest_df.shape[0], 1)
            )
        if hue:
            iso_forest_df[hue] = y
    # * Apply Kernel PCA if selected
    elif method == "kpca":
        from sklearn.decomposition import KernelPCA

        kpca = KernelPCA(
            n_components=n_components, kernel="rbf", random_state=random_state
        )
        X_reduced = kpca.fit_transform(X)

        # Prepare reduced DataFrame with KPCA info
        kpca_df = pd.DataFrame(
            X_reduced,
            index=data.index,
            columns=[f"KPCA_{i+1}" for i in range(n_components)],
        )
        if debug:
            print("Kernel PCA completed with RBF kernel.")
        if hue:
            kpca_df[hue] = y
    # * Apply ICA if selected
    elif method == "ica":
        from sklearn.decomposition import FastICA

        ica = FastICA(n_components=n_components, random_state=random_state)
        X_reduced = ica.fit_transform(X)

        # Prepare reduced DataFrame with ICA info
        ica_df = pd.DataFrame(
            X_reduced,
            index=data.index,
            columns=[f"ICA_{i+1}" for i in range(n_components)],
        )
        if debug:
            print("Independent Component Analysis (ICA) completed.")
        if hue:
            ica_df[hue] = y
    # * Apply MDS if selected
    elif method == "mds":
        from sklearn.manifold import MDS

        mds = MDS(n_components=n_components, random_state=random_state)
        X_reduced = mds.fit_transform(X)

        # Prepare reduced DataFrame with MDS info
        mds_df = pd.DataFrame(
            X_reduced,
            index=data.index,
            columns=[f"MDS_{i+1}" for i in range(n_components)],
        )
        if debug:
            print("Multidimensional Scaling (MDS) completed.")
        if hue:
            mds_df[hue] = y
    # * Apply Locally Linear Embedding (LLE) if selected
    elif method == "lle":
        from sklearn.manifold import LocallyLinearEmbedding

        lle = LocallyLinearEmbedding(
            n_components=n_components,
            n_neighbors=umap_neighbors,
            random_state=random_state,
        )
        X_reduced = lle.fit_transform(X)

        # Prepare reduced DataFrame with LLE info
        lle_df = pd.DataFrame(
            X_reduced,
            index=data.index,
            columns=[f"LLE_{i+1}" for i in range(n_components)],
        )
        if debug:
            print("Locally Linear Embedding (LLE) completed.")
        if hue:
            lle_df[hue] = y
    # * Apply Singular Value Decomposition (SVD) if selected
    elif method == "svd":
        # Using NumPy's SVD for dimensionality reduction
        U, s, Vt = np.linalg.svd(X, full_matrices=False)
        X_reduced = U[:, :n_components] * s[:n_components]

        # Prepare reduced DataFrame with SVD info
        svd_df = pd.DataFrame(
            X_reduced,
            index=data.index,
            columns=[f"SVD_{i+1}" for i in range(n_components)],
        )
        colname_met = "SVD_"
        if hue:
            svd_df[hue] = y
        if debug:
            print("Singular Value Decomposition (SVD) completed.")
    elif method == "truncated_svd":
        from sklearn.decomposition import TruncatedSVD

        svd = TruncatedSVD(n_components=n_components, random_state=random_state)
        X_reduced = svd.fit_transform(X)
        reduced_df = pd.DataFrame(
            X_reduced,
            columns=[f"SVD Component {i+1}" for i in range(n_components)],
            index=data.index,
        )
        colname_met = "SVD Component "

        if debug:
            print("Truncated SVD completed.")
            print("Explained Variance Ratio:", svd.explained_variance_ratio_)
        if hue:
            reduced_df[hue] = y

    elif method == "spectral_embedding":
        from sklearn.manifold import SpectralEmbedding

        spectral = SpectralEmbedding(
            n_components=n_components, random_state=random_state
        )
        X_reduced = spectral.fit_transform(X)
        reduced_df = pd.DataFrame(
            X_reduced,
            columns=[f"Dimension_{i+1}" for i in range(n_components)],
            index=data.index,
        )
        colname_met = "Dimension_"

        if debug:
            print("Spectral Embedding completed.")
        if hue:
            reduced_df[hue] = y

    elif method == "autoencoder":
        from tensorflow.keras.models import Model
        from tensorflow.keras.layers import Input, Dense

        input_dim = X.shape[1]
        input_layer = Input(shape=(input_dim,))
        encoded = Dense(n_components * 2, activation="relu")(input_layer)
        encoded = Dense(n_components, activation="relu")(encoded)
        autoencoder = Model(input_layer, encoded)
        autoencoder.compile(optimizer="adam", loss="mean_squared_error")
        autoencoder.fit(X, X, epochs=50, batch_size=256, shuffle=True, verbose=0)

        X_reduced = autoencoder.predict(X)
        reduced_df = pd.DataFrame(
            X_reduced,
            columns=[f"Score_{i+1}" for i in range(n_components)],
            index=data.index,
        )
        colname_met = "Score_"

        if debug:
            print("Autoencoder reduction completed.")
        if hue:
            reduced_df[hue] = y

    elif method == "nmf":
        from sklearn.decomposition import NMF

        nmf = NMF(n_components=n_components, random_state=random_state)
        X_reduced = nmf.fit_transform(X)
        reduced_df = pd.DataFrame(
            X_reduced,
            columns=[f"NMF_{i+1}" for i in range(n_components)],
            index=data.index,
        )
        colname_met = "NMF_"

        if debug:
            print("Non-negative Matrix Factorization completed.")
        if hue:
            reduced_df[hue] = y

    elif method == "umap_hdbscan":
        import umap
        import hdbscan

        umap_model = umap.UMAP(
            n_neighbors=umap_neighbors,
            min_dist=umap_min_dist,
            n_components=n_components,
        )
        X_umap = umap_model.fit_transform(X)

        clusterer = hdbscan.HDBSCAN()
        clusters = clusterer.fit_predict(X_umap)

        reduced_df = pd.DataFrame(
            X_umap,
            columns=[f"UMAP_{i+1}" for i in range(n_components)],
            index=data.index,
        )
        reduced_df["Cluster"] = clusters
        colname_met = "UMAP_"
        if debug:
            print("UMAP + HDBSCAN reduction and clustering completed.")
        if hue:
            reduced_df[hue] = y

    elif method == "manifold_learning":
        from sklearn.manifold import Isomap

        isomap = Isomap(n_components=n_components)
        X_reduced = isomap.fit_transform(X)
        reduced_df = pd.DataFrame(
            X_reduced,
            columns=[f"Manifold_{i+1}" for i in range(n_components)],
            index=data.index,
        )
        colname_met = "Manifold_"

        if debug:
            print("Manifold Learning (Isomap) completed.")
        if hue:
            reduced_df[hue] = y

    #! Return reduced data and info as a new DataFrame with the same index
    if method == "pca":
        reduced_df = pca_df
        colname_met = "PC_"
        xlabel = f"PC_1 ({pca_df['Explained Variance PC_1'].tolist()[0]})"
        ylabel = f"PC_2 ({pca_df['Explained Variance PC_2'].tolist()[0]})"
    elif method == "umap":
        reduced_df = umap_df
        colname_met = "UMAP_"
    elif method == "tsne":
        reduced_df = tsne_df
        colname_met = "tSNE_"
    elif method == "factor":
        reduced_df = factor_df
        colname_met = "Factor_"
    elif method == "isolation_forest":
        reduced_df = iso_forest_df  # Already a DataFrame for outliers
        colname_met = "PC_"
        if plot_:
            ax = sns.scatterplot(
                data=iso_forest_df[iso_forest_df["Anomaly Label"] == 1],
                x="PC_1",
                y="PC_2",
                label="normal",
                c="b",
            )
            ax = sns.scatterplot(
                ax=ax,
                data=iso_forest_df[iso_forest_df["Anomaly Label"] == -1],
                x="PC_1",
                y="PC_2",
                c="r",
                label="outlier",
                marker="+",
                s=30,
            )
    elif method == "lda":
        reduced_df = lda_df
        colname_met = "LDA_"
    elif method == "kpca":
        reduced_df = kpca_df
        colname_met = "KPCA_"
    elif method == "ica":
        reduced_df = ica_df
        colname_met = "ICA_"
    elif method == "mds":
        reduced_df = mds_df
        colname_met = "MDS_"
    elif method == "lle":
        reduced_df = lle_df
        colname_met = "LLE_"
    elif method == "svd":
        reduced_df = svd_df
        colname_met = "SVD_"

    if plot_ and (not method in ["isolation_forest"]):
        from .plot import plotxy, figsets, get_color 
        xlabel = f"{colname_met}1" if xlabel is None else xlabel
        ylabel = f"{colname_met}2" if ylabel is None else ylabel
        palette = get_color(len(flatten(data[hue], verbose=0))) if hue is not None else get_color(1)

        reduced_df = reduced_df.sort_values(by=hue) if hue is not None else reduced_df
        print(flatten(reduced_df[hue])) if hue is not None else None

        # default: plotting
        kind_=kwargs.get("kind_",[
                "joint", 
                "ell",
            ]) 
        # auto size
        if size is None:
            if reduced_df.shape[0]<=100:
                size, markerscale = 20,1 if markerscale is None else markerscale
            elif 100<reduced_df.shape[0]<=200:
                size, markerscale  = 15,1.2 if markerscale is None else markerscale
            elif 200<reduced_df.shape[0]<=500:
                size, markerscale  = 10,1.3 if markerscale is None else markerscale
            elif 500<reduced_df.shape[0]<=800:
                size, markerscale  = 5,2.5 if markerscale is None else markerscale
            elif 800<reduced_df.shape[0]<=1600:
                size, markerscale  = 3,4 if markerscale is None else markerscale
            elif 1600<reduced_df.shape[0]<=3000:
                size, markerscale  = 2,5 if markerscale is None else markerscale
            else:
                size, markerscale  = 1,10 if markerscale is None else markerscale
        ax = plotxy(
            data=reduced_df,
            x=colname_met + "1",
            y=colname_met + "2",
            hue=hue,
            palette=palette,
            edgecolor=edgecolor,
            kind_=kind_,
            kws_kde=dict(
                hue=hue,
                levels=2,
                common_norm=False,
                fill=True,
                alpha=0.05,
            ),
            kws_joint=dict(kind="scatter", joint_kws=dict(s=size)),
            kws_ellipse=dict(alpha=0.1, lw=0, label=None),
            verbose=False,
            **kwargs,
        )
        figsets(
            legend=dict(
                loc=legend_loc,
                markerscale=markerscale,
                bbox_to_anchor=bbox_to_anchor,
                ncols=ncols,
                fontsize=10,
            ),
            sp=5,
            xlabel=xlabel if xlabel else None,
            ylabel=ylabel if ylabel else None,
        )

    if inplace:
        # If inplace=True, add components back into the original data
        for col_idx in range(n_components):
            data.loc[:, f"{colname_met}{col_idx+1}"] = reduced_df.iloc[:, col_idx]
        # Add extra info for PCA/UMAP
        if method == "pca":
            for i in range(n_components):
                data.loc[:, f"Explained Variance PC_{i+1}"] = reduced_df.loc[
                    :, f"Explained Variance PC_{i+1}"
                ]
            for i in range(n_components):
                data.loc[:, f"Singular Values PC_{i+1}"] = reduced_df.loc[
                    :, f"Singular Values PC_{i+1}"
                ]
        elif method == "umap":
            for i in range(n_components):
                data.loc[:, f"UMAP_{i+1}"] = reduced_df.loc[:, f"UMAP_{i+1}"]
            data.loc[:, "Embedding"] = reduced_df.loc[:, "Embedding"]
            data.loc[:, "Trustworthiness"] = reduced_df.loc[:, "Trustworthiness"]

        return None  # No return when inplace=True

    return reduced_df

def get_df_format(data, threshold_unique=0.5, verbose=False, sample_size=1000):
    """
    Detect whether a DataFrame is in long or wide format with optimized performance and accuracy.
    
    Parameters:
    - data (pd.DataFrame): DataFrame to analyze
    - threshold_unique (float): Threshold for categorical column detection (0-1)
    - verbose (bool): Whether to print diagnostic messages
    - sample_size (int): Maximum number of rows/columns to sample for large datasets
    
    Returns:
    - "long" if detected as long format
    - "wide" if detected as wide format
    - "uncertain" if format is ambiguous
    """
    
    
    from scipy.stats import entropy
    from sklearn.cluster import AgglomerativeClustering
    from sklearn.preprocessing import StandardScaler
    from sklearn.metrics import pairwise_distances
    from collections import Counter
    
    # ----- Initial Setup and Sampling -----
    n_rows, n_cols = data.shape
    if verbose:
        print(f"Initial shape: {n_rows} rows, {n_cols} columns")
    
    # Sample data if too large
    if n_rows > sample_size:
        data = data.sample(n=sample_size, random_state=1)
        n_rows = sample_size
    if n_cols > sample_size:
        data = data.iloc[:, :sample_size]
        n_cols = sample_size
    
    # Early exit for tiny datasets
    if n_rows < 3 or n_cols < 3:
        return "uncertain"
    
    long_score = 0
    wide_score = 0
    
    # ----- Feature Extraction -----
    # Basic statistics
    row_col_ratio = n_rows / n_cols if n_cols != 0 else float('inf')
    
    # Column types
    numeric_cols = data.select_dtypes(include=np.number).columns
    cat_cols = data.select_dtypes(include=['object', 'category']).columns
    other_cols = [col for col in data.columns if col not in numeric_cols and col not in cat_cols]
    
    # Unique value analysis
    unique_counts = data.nunique(dropna=False)
    duplicate_ratio = 1 - unique_counts / n_rows
    
    # Missing values
    missing_per_row = data.isna().sum(axis=1)
    missing_per_col = data.isna().sum()
    
    # Column name patterns
    col_names = data.columns.astype(str)
    has_suffix = sum(bool(re.search(r'(_\d+|\d+_?$)', col)) for col in col_names)
    has_time = sum(bool(re.search(r'(^time|^date|^year|^month|^day|^t\d+)', col.lower())) for col in col_names)
    
    # ----- Scoring Rules -----
    
    # 1. Row-Column Ratio (weighted)
    if row_col_ratio > 5:
        long_score += 3
        if verbose: print(f"High row/col ratio ({row_col_ratio:.1f}) ‚Üí long +3")
    elif row_col_ratio < 0.2:
        wide_score += 3
        if verbose: print(f"Low row/col ratio ({row_col_ratio:.1f}) ‚Üí wide +3")
    elif row_col_ratio > 2:
        long_score += 1
        if verbose: print(f"Moderate row/col ratio ({row_col_ratio:.1f}) ‚Üí long +1")
    elif row_col_ratio < 0.5:
        wide_score += 1
        if verbose: print(f"Moderate row/col ratio ({row_col_ratio:.1f}) ‚Üí wide +1")
    
    # 2. Duplication Patterns
    high_dupe_cols = sum(duplicate_ratio > 0.3)
    if high_dupe_cols > 0.6 * n_cols:
        wide_score += 2
        if verbose: print(f"Many columns ({high_dupe_cols}/{n_cols}) with duplicates ‚Üí wide +2")
    elif high_dupe_cols < 0.2 * n_cols:
        long_score += 1
        if verbose: print(f"Few columns ({high_dupe_cols}/{n_cols}) with duplicates ‚Üí long +1")
    
    # 3. Categorical Column Analysis
    if len(cat_cols) > 0:
        # Entropy analysis
        cat_entropies = []
        for col in cat_cols:
            counts = data[col].value_counts(normalize=True, dropna=False)
            cat_entropies.append(entropy(counts))
        
        avg_cat_entropy = np.mean(cat_entropies) if cat_entropies else 0
        if avg_cat_entropy < 1.2:
            long_score += 2
            if verbose: print(f"Low categorical entropy ({avg_cat_entropy:.2f}) ‚Üí long +2")
        elif avg_cat_entropy > 2:
            wide_score += 1
            if verbose: print(f"High categorical entropy ({avg_cat_entropy:.2f}) ‚Üí wide +1")
        
        # Entity identifier detection
        if len(cat_cols) >= 2 and n_rows > 10:
            dup_rows = data.duplicated(subset=cat_cols.tolist()[:2], keep=False).sum()
            if dup_rows > 0.3 * n_rows:
                long_score += 2
                if verbose: print(f"Duplicate rows in categorical cols ({dup_rows}/{n_rows}) ‚Üí long +2")
    
    # 4. Column Name Patterns
    if has_suffix > 0.4 * n_cols:
        wide_score += 2
        if verbose: print(f"Many suffix patterns ({has_suffix}/{n_cols}) ‚Üí wide +2")
    if has_time > 0.3 * n_cols:
        wide_score += 1
        if verbose: print(f"Time-like columns ({has_time}/{n_cols}) ‚Üí wide +1")
    
    # 5. Numeric Column Analysis (only if enough numeric columns)
    if len(numeric_cols) > 2:
        # Correlation analysis
        corr_matrix = data[numeric_cols].corr().abs()
        avg_corr = corr_matrix.values[np.triu_indices_from(corr_matrix, k=1)].mean()
        
        if avg_corr > 0.5:
            wide_score += 2
            if verbose: print(f"High numeric correlation ({avg_corr:.2f}) ‚Üí wide +2")
        elif avg_corr < 0.2:
            long_score += 1
            if verbose: print(f"Low numeric correlation ({avg_corr:.2f}) ‚Üí long +1")
        
        # Entropy analysis
        try:
            numeric_data = data[numeric_cols].dropna()
            if len(numeric_data) > 10:
                numeric_entropy = numeric_data.apply(lambda x: entropy(pd.cut(x, bins=min(10, len(x.unique())).value_counts(normalize=True))))
                if numeric_entropy.mean() < 1.5:
                    wide_score += 1
                    if verbose: print(f"Low numeric entropy ({numeric_entropy.mean():.2f}) ‚Üí wide +1")
        except Exception as e:
            if verbose: print(f"Numeric entropy failed: {str(e)}")
    
    # 6. Missing Value Patterns
    missing_row_std = missing_per_row.std()
    if missing_row_std < 1 and missing_per_row.mean() > 0.1 * n_cols:
        wide_score += 1
        if verbose: print(f"Uniform missing pattern (std={missing_row_std:.2f}) ‚Üí wide +1")
    elif missing_per_row.mean() < 0.05 * n_cols:
        long_score += 1
        if verbose: print(f"Few missing values ‚Üí long +1")
    
    # 7. Advanced Clustering (only for medium/large datasets)
    if len(numeric_cols) > 3 and n_rows > 10 and n_cols > 5:
        try:
            # Efficient clustering with sampling
            sample_data = data[numeric_cols].sample(n=min(100, n_rows), random_state=1)
            scaled_data = StandardScaler().fit_transform(sample_data.dropna())
            
            if scaled_data.shape[0] > 5:
                # Column clustering
                col_dist = pairwise_distances(scaled_data.T)
                col_clusters = AgglomerativeClustering(n_clusters=2, 
                                                     affinity='precomputed', 
                                                     linkage='complete').fit(col_dist)
                cluster_counts = Counter(col_clusters.labels_)
                if max(cluster_counts.values()) > 0.7 * len(numeric_cols):
                    wide_score += 2
                    if verbose: print(f"Column clustering shows dominant group ‚Üí wide +2")
                
                # Row clustering
                row_clusters = AgglomerativeClustering(n_clusters=2).fit(scaled_data)
                row_cluster_counts = Counter(row_clusters.labels_)
                if max(row_cluster_counts.values()) > 0.8 * scaled_data.shape[0]:
                    wide_score += 1
                    if verbose: print(f"Row clustering shows homogeneity ‚Üí wide +1")
        except Exception as e:
            if verbose: print(f"Clustering skipped: {str(e)}")
    
    # ----- Decision Logic -----
    score_diff = long_score - wide_score
    abs_diff = abs(score_diff)
    
    if verbose:
        print(f"\nFinal scores - Long: {long_score}, Wide: {wide_score}")
    
    if abs_diff >= 3:
        return "long" if score_diff > 0 else "wide"
    elif abs_diff >= 1:
        # Additional tie-breakers
        if score_diff == 0:
            if row_col_ratio > 1.5:
                return "long"
            elif row_col_ratio < 0.67:
                return "wide"
            elif len(cat_cols) > len(numeric_cols):
                return "long"
            else:
                return "wide"
        return "long" if score_diff > 0 else "wide"
    else:
        return "uncertain"

#! ========== workbook, worksheet, wb,ws =============

import openpyxl
from openpyxl.utils import get_column_letter
from openpyxl.styles import NamedStyle
from openpyxl.worksheet.datavalidation import DataValidation
def ws_get_header(ws, column=None, header=1, return_idx=True):
    header_list = [cell.value for cell in ws[header]]
    if column is None:  # return all values
        return (
            [(i, idx) for i, idx in zip(header_list, range(1, len(header_list) + 1))]
            if return_idx
            else header_list
        )
    if isinstance(column, list):
        return [
            ws_get_header(ws, column=col, header=header, return_idx=return_idx)
            for col in column
        ]
    return (
        strcmp(column, header_list)[0],
        header_list.index((strcmp(column, header_list)[0])) + 1,
    )


def ws_insert_column(ws, column, after=None, before=None, header=None, header_style=None):
    """
    Insert a new column while preserving formatting and data validation

    Args:
        ws: Worksheet object
        column: Name for the new column header
        after: Insert after this column name
        before: Insert before this column name
        header: Header text (defaults to column)
        header_style: Style to apply to header cell
    """
    if after is not None and before is not None:
        raise ValueError("Cannot specify both 'after' and 'before'")

    if after is not None:
        _, col_idx = ws_get_header(ws, after, header=header)
        insert_idx = col_idx + 1
    elif before is not None:
        _, col_idx = ws_get_header(ws, before, header=header)
        insert_idx = col_idx
    else:
        raise ValueError("Must specify either 'after' or 'before'")

    # Insert column
    ws.insert_cols(insert_idx)

    # Set header
    header_text = header if header is not None else column
    header_cell = ws.cell(row=header, column=insert_idx, value=header_text)

    # Apply header style if provided
    if header_style:
        if isinstance(header_style, str):
            header_style = ws.parent.named_styles[header_style]
        header_cell.style = header_style

    return insert_idx


def ws_write_column(ws, column, values, start_row=2, 
                    # preserve_formatting=True, 
                    header=1):
    """
    Write values to a column while optionally preserving formatting

    Args:
        ws: Worksheet object
        column: Name of column to write to
        values: List of values to write
        start_row: Starting row (default 2, assuming row 1 is header)
        preserve_formatting: Whether to keep existing formatting (default True)
    """
    _, col_idx = ws_get_header(ws, column, header=header)

    # Get reference column for formatting (previous column)
    ref_col_idx = col_idx - 1 if col_idx > 1 else col_idx + 1
    ref_col = ws[get_column_letter(ref_col_idx)]

    for row_idx, value in enumerate(values, start_row):
        cell = ws.cell(row=row_idx, column=col_idx, value=value)

def ws_delete_column(ws, column, header=1):
    """
    Delete a column by name while preserving data validation rules

    Args:
        ws: Worksheet object
        column: Name of column to delete
    """
    _, col_idx = ws_get_header(ws, column, header=header)
    ws.delete_cols(col_idx)


def apply_number_format(ws, column, number_format, header=1):
    """
    Apply number formatting to an entire column

    Args:
        ws: Worksheet object
        column: Column name to format
        number_format: Excel number format string (e.g., "0.00%")
    """
    _, col_idx = ws_get_header(ws, column, header=header)
    col_letter = get_column_letter(col_idx)

    for row in ws.iter_rows(min_col=col_idx, max_col=col_idx):
        for cell in row:
            cell.number_format = number_format


def ws_auto_width(ws):
    """Adjust column widths based on content"""
    for column in ws.columns:
        max_length = 0
        column_letter = get_column_letter(column[0].column)

        for cell in column:
            try:
                if len(str(cell.value)) > max_length:
                    max_length = len(str(cell.value))
            except:
                pass

        adjusted_width = (max_length + 2) * 1.2
        ws.column_dimensions[column_letter].width = adjusted_width


def create_style(wb, style_name, **style_attrs):
    """
    Create a named style in the workbook

    Args:
        wb: Workbook object
        style_name: Name for the new style
        **style_attrs: Style attributes (font, fill, border, etc.)
    """
    if style_name in wb.named_styles:
        return wb.named_styles[style_name]

    style = NamedStyle(name=style_name)
    for attr, value in style_attrs.items():
        setattr(style, attr, value)

    wb.add_named_style(style)
    return style


def create_data_validation(validation_type, formula1, formula2=None, **kwargs):
    """
    Create a data validation rule

    Args:
        validation_type: Excel validation type (e.g., "list", "whole", "decimal")
        formula1: First formula (e.g., list range for dropdown)
        formula2: Second formula (for between validations)
        **kwargs: Additional validation options (errorTitle, errorMessage, etc.)
    """
    dv = DataValidation(type=validation_type, formula1=formula1, formula2=formula2)
    for key, value in kwargs.items():
        setattr(dv, key, value)
    return dv

#! ========== workbook, worksheet, wb,ws =============
def plot_cluster(
    data: pd.DataFrame,
    labels: np.ndarray,
    metrics: dict = None,
    cmap="tab20",
    true_labels: Optional[np.ndarray] = None,
) -> None:
    """
    Visualize clustering results with various plots.

    Parameters:
    -----------
    data : pd.DataFrame
        The input data used for clustering.
    labels : np.ndarray
        Cluster labels assigned to each point.
    metrics : dict
        Dictionary containing evaluation metrics from evaluate_cluster function.
    true_labels : Optional[np.ndarray], default=None
        Ground truth labels, if available.
    """
    import seaborn as sns
    from sklearn.metrics import silhouette_samples
    import matplotlib.pyplot as plt

    if metrics is None:
        metrics = evaluate_cluster(data=data, labels=labels, true_labels=true_labels)

    # 1. Scatter Plot of Clusters
    plt.figure(figsize=(15, 6))
    plt.subplot(1, 3, 1)
    plt.scatter(data.iloc[:, 0], data.iloc[:, 1], c=labels, cmap=cmap, s=20)
    plt.title("Cluster Scatter Plot")
    plt.xlabel("Component 1")
    plt.ylabel("Component 2")
    plt.colorbar(label="Cluster Label")
    plt.grid()

    # 2. Silhouette Plot
    if "Silhouette Score" in metrics:
        silhouette_vals = silhouette_samples(data, labels)
        plt.subplot(1, 3, 2)
        y_lower = 10
        for i in range(len(set(labels))):
            # Aggregate the silhouette scores for samples belonging to the current cluster
            cluster_silhouette_vals = silhouette_vals[labels == i]
            cluster_silhouette_vals.sort()
            size_cluster_i = cluster_silhouette_vals.shape[0]
            y_upper = y_lower + size_cluster_i

            plt.fill_betweenx(np.arange(y_lower, y_upper), 0, cluster_silhouette_vals)
            plt.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))
            y_lower = y_upper + 10  # 10 for the 0 samples

        plt.title("Silhouette Plot")
        plt.xlabel("Silhouette Coefficient Values")
        plt.ylabel("Cluster Label")
        plt.axvline(x=metrics["Silhouette Score"], color="red", linestyle="--")
        plt.grid()

    # 3. Metrics Plot
    plt.subplot(1, 3, 3)
    metric_names = ["Davies-Bouldin Index", "Calinski-Harabasz Index"]
    metric_values = [
        metrics["Davies-Bouldin Index"],
        metrics["Calinski-Harabasz Index"],
    ]

    if true_labels is not None:
        metric_names += ["Homogeneity Score", "Completeness Score", "V-Measure"]
        metric_values += [
            metrics["Homogeneity Score"],
            metrics["Completeness Score"],
            metrics["V-Measure"],
        ]

    plt.barh(metric_names, metric_values, color="lightblue")
    plt.title("Clustering Metrics")
    plt.xlabel("Score")
    plt.axvline(x=0, color="gray", linestyle="--")
    plt.grid()
    
    try:
        plt.tight_layout()
    except RuntimeError:
        plt.subplots_adjust(top=0.92, right=0.92)


def evaluate_cluster(
    data: pd.DataFrame, labels: np.ndarray, true_labels: Optional[np.ndarray] = None
) -> dict:
    """
    Evaluate clustering performance using various metrics.

    Parameters:
    -----------
    data : pd.DataFrame
        The input data used for clustering.
    labels : np.ndarray
        Cluster labels assigned to each point.
    true_labels : Optional[np.ndarray], default=None
        Ground truth labels, if available.

    Returns:
    --------
    metrics : dict
        Dictionary containing evaluation metrics.

    1. Silhouette Score:
    The silhouette score measures how similar an object is to its own cluster (cohesion) compared to
    how similar it is to other clusters (separation). The score ranges from -1 to +1:
    +1: Indicates that the data point is very far from the neighboring clusters and well clustered.
    0: Indicates that the data point is on or very close to the decision boundary between two neighboring
    clusters.
    -1: Indicates that the data point might have been assigned to the wrong cluster.

    Interpretation:
    A higher average silhouette score indicates better-defined clusters.
    If the score is consistently high (above 0.5), it suggests that the clusters are well separated.
    A score near 0 may indicate overlapping clusters, while negative scores suggest points may have
    been misclassified.

    2. Davies-Bouldin Index:
    The Davies-Bouldin Index (DBI) measures the average similarity ratio of each cluster with its
    most similar cluster. The index values range from 0 to ‚àû, with lower values indicating better clustering.
    It is defined as the ratio of within-cluster distances to between-cluster distances.

    Interpretation:
    A lower DBI value indicates that the clusters are compact and well-separated.
    Ideally, you want to minimize the Davies-Bouldin Index. If your DBI value is above 1, this indicates
    that your clusters might not be well-separated.

    3. Adjusted Rand Index (ARI):
    The Adjusted Rand Index (ARI) is a measure of the similarity between two data clusterings. The ARI
    score ranges from -1 to +1:
    1: Indicates perfect agreement between the two clusterings.
    0: Indicates that the clusterings are no better than random.
    Negative values: Indicate less agreement than expected by chance.

    Interpretation:
    A higher ARI score indicates better clustering, particularly if it's close to 1.
    An ARI score of 0 or lower suggests that the clustering results do not represent the true labels
    well, indicating a poor clustering performance.

    4. Calinski-Harabasz Index:
    The Calinski-Harabasz Index (also known as the Variance Ratio Criterion) evaluates the ratio of
    the sum of between-cluster dispersion to within-cluster dispersion. Higher values indicate better clustering.

    Interpretation:
    A higher Calinski-Harabasz Index suggests that clusters are dense and well-separated. It is typically
    used to validate the number of clusters, with higher values favoring more distinct clusters.

    5. Homogeneity Score:
    The homogeneity score measures how much a cluster contains only members of a single class (if true labels are provided).
    A score of 1 indicates perfect homogeneity, where all points in a cluster belong to the same class.

    Interpretation:
    A higher homogeneity score indicates that the clustering result is pure, meaning the clusters are composed
    of similar members. Lower values indicate mixed clusters, suggesting poor clustering performance.

    6. Completeness Score:
    The completeness score evaluates how well all members of a given class are assigned to the same cluster.
    A score of 1 indicates perfect completeness, meaning all points in a true class are assigned to a single cluster.

    Interpretation:
    A higher completeness score indicates that the clustering effectively groups all instances of a class together.
    Lower values suggest that some instances of a class are dispersed among multiple clusters.

    7. V-Measure:
    The V-measure is the harmonic mean of homogeneity and completeness, giving a balanced measure of clustering performance.

    Interpretation:
    A higher V-measure score indicates that the clusters are both homogenous (pure) and complete (cover all members of a class).
    Scores closer to 1 indicate better clustering quality.
    """
    from sklearn.metrics import (
        silhouette_score,
        davies_bouldin_score,
        adjusted_rand_score,
        calinski_harabasz_score,
        homogeneity_score,
        completeness_score,
        v_measure_score,
    )

    metrics = {}
    unique_labels = set(labels)
    if len(unique_labels) > 1 and len(unique_labels) < len(data):
        # Calculate Silhouette Score
        try:
            metrics["Silhouette Score"] = silhouette_score(data, labels)
        except Exception as e:
            metrics["Silhouette Score"] = np.nan
            print(f"Silhouette Score calculation failed: {e}")

        # Calculate Davies-Bouldin Index
        try:
            metrics["Davies-Bouldin Index"] = davies_bouldin_score(data, labels)
        except Exception as e:
            metrics["Davies-Bouldin Index"] = np.nan
            print(f"Davies-Bouldin Index calculation failed: {e}")

        # Calculate Calinski-Harabasz Index
        try:
            metrics["Calinski-Harabasz Index"] = calinski_harabasz_score(data, labels)
        except Exception as e:
            metrics["Calinski-Harabasz Index"] = np.nan
            print(f"Calinski-Harabasz Index calculation failed: {e}")

        # Calculate Adjusted Rand Index if true labels are provided
        if true_labels is not None:
            try:
                metrics["Adjusted Rand Index"] = adjusted_rand_score(
                    true_labels, labels
                )
            except Exception as e:
                metrics["Adjusted Rand Index"] = np.nan
                print(f"Adjusted Rand Index calculation failed: {e}")

            # Calculate Homogeneity Score
            try:
                metrics["Homogeneity Score"] = homogeneity_score(true_labels, labels)
            except Exception as e:
                metrics["Homogeneity Score"] = np.nan
                print(f"Homogeneity Score calculation failed: {e}")

            # Calculate Completeness Score
            try:
                metrics["Completeness Score"] = completeness_score(true_labels, labels)
            except Exception as e:
                metrics["Completeness Score"] = np.nan
                print(f"Completeness Score calculation failed: {e}")

            # Calculate V-Measure
            try:
                metrics["V-Measure"] = v_measure_score(true_labels, labels)
            except Exception as e:
                metrics["V-Measure"] = np.nan
                print(f"V-Measure calculation failed: {e}")
    else:
        # Metrics cannot be computed with 1 cluster or all points as noise
        metrics["Silhouette Score"] = np.nan
        metrics["Davies-Bouldin Index"] = np.nan
        metrics["Calinski-Harabasz Index"] = np.nan
        if true_labels is not None:
            metrics["Adjusted Rand Index"] = np.nan
            metrics["Homogeneity Score"] = np.nan
            metrics["Completeness Score"] = np.nan
            metrics["V-Measure"] = np.nan

    return metrics

@decorators.Debug()
def df_qc(
    data: pd.DataFrame,
    columns=None,
    plot_=True,
    max_cols=20,  # only for plots
    hue=None,
    output=False,
    verbose=True,
    dir_save=None,
    #  PyGWalker parameters
    engine="default",  # Options: "default", "skim", "pygwalker"
    pygwalker_params=None,  # Optional: {'dark': True, 'sample': 5000}
    port=8050,
):
    """

    df_qc(df)                         # Basic quality check
    df_qc(df, explore=True)           # + Interactive PyGWalker exploration
    df_qc(df, explore=True, explore_params={'dark': True})  # Dark mode
    
    Parameters:
    -----------
    explore : bool, default=False
        Launch interactive PyGWalker exploration after analysis
        üí° Tip: Great for visual data discovery and pattern recognition!
    
    explore_params : dict, optional
        Customize PyGWalker experience:
        - 'dark': bool ‚Üí Dark/Light mode (default: False)
        - 'sample': int ‚Üí Sample size for large datasets (default: 10000)
        - 'hide_config': bool ‚Üí Hide data source config (default: True)
        - 'spec': str ‚Üí Path to saved PyGWalker config file
        - 'widget': bool ‚Üí Show as Jupyter widget (default: auto-detect)
    
    port : int, default=8050
        Web server port for PyGWalker
    
    Returns:
    --------
    If output=True or plot_=False: dict with QC results
    Otherwise: None (prints results and shows plots)
    
    Examples:
    ---------
    # 1. Basic QC with plots
    df_qc(my_data)
    
    # 2. QC + Interactive exploration (recommended!)
    df_qc(my_data, explore=True)
    
    # 3. QC with custom exploration settings
    df_qc(my_data, explore=True, explore_params={'dark': True, 'sample': 5000})
    
    # 4. Get results dictionary
    results = df_qc(my_data, output=True)
    """
    if engine=="skim":
        try:
            import skimpy

            skimpy.skim(data)
        except:
            numerical_data = data.select_dtypes(include=[np.number])
            skimpy.skim(numerical_data)
    if engine=='default':
        try:
            from statsmodels.stats.outliers_influence import variance_inflation_factor
            from scipy.stats import skew, kurtosis, entropy
            if columns is not None:
                if isinstance(columns, (list, pd.core.indexes.base.Index)):
                    data = data[columns]
            # Fill completely NaN columns with a default value (e.g., 0)
            data = data.copy()
            data.loc[:, data.isna().all()] = 0
            res_qc = {}
            print(f"‚§µ data.shape:{data.shape}\n‚§µ data.sample(10):")
            display(data.sample(min(10, len(data))).style.background_gradient(cmap="coolwarm", axis=1))

            # Missing values
            res_qc["missing_values"] = data.isnull().sum()
            res_qc["missing_percentage"] = round(
                (res_qc["missing_values"] / len(data)) * 100, 2
            )
            res_qc["rows_with_missing"] = data.isnull().any(axis=1).sum()

            # Data types and unique values
            res_qc["data_types"] = data.dtypes
            res_qc["unique_counts"] = (
                data.select_dtypes(exclude=np.number).nunique().sort_values()
            )
            res_qc["unique_values"] = data.select_dtypes(exclude=np.number).apply(
                lambda x: x.unique()
            )
            res_qc["constant_columns"] = [
                col for col in data.columns if data[col].nunique() <= 1
            ]

            # Duplicate rows and columns
            res_qc["duplicate_rows"] = data.duplicated().sum()
            res_qc["duplicate_columns"] = data.columns[data.columns.duplicated()].tolist()

            # Empty columns
            res_qc["empty_columns"] = [col for col in data.columns if data[col].isnull().all()]

            # outliers
            data_outliers = df_outlier(data)
            outlier_num = data_outliers.isna().sum() - data.isnull().sum()
            res_qc["outlier_num"] = outlier_num[outlier_num > 0]
            outlier_percentage = round((outlier_num / len(data_outliers)) * 100, 2)
            res_qc["outlier_percentage"] = outlier_percentage[outlier_percentage > 0]
            try:
                # Correlation and multicollinearity (VIF)
                if any(data.dtypes.apply(pd.api.types.is_numeric_dtype)):
                    numeric_df = data.select_dtypes(include=[np.number]).dropna()
                    corr_matrix = numeric_df.corr()
                    high_corr_pairs = [
                        (col1, col2)
                        for col1 in corr_matrix.columns
                        for col2 in corr_matrix.columns
                        if col1 != col2 and abs(corr_matrix[col1][col2]) > 0.9
                    ]
                    res_qc["high_correlations"] = high_corr_pairs

                    # VIF for multicollinearity check
                    numeric_df = data.select_dtypes(include=[np.number]).dropna()
                    if isinstance(numeric_df.columns, pd.MultiIndex):
                        numeric_df.columns = [
                            "_".join(col).strip() if isinstance(col, tuple) else col
                            for col in numeric_df.columns
                        ]

                    vif_data = pd.DataFrame()
                    res_qc["vif"] = vif_data
                    if numeric_df.shape[1] > 1 and not numeric_df.empty:
                        vif_data["feature"] = numeric_df.columns.tolist()
                        vif_data["VIF"] = [
                            round(variance_inflation_factor(numeric_df.values, i), 2)
                            for i in range(numeric_df.shape[1])
                        ]
                        res_qc["vif"] = vif_data[
                            vif_data["VIF"] > 5
                        ]  # Typically VIF > 5 indicates multicollinearity
            except Exception as e:
                print(e)
            # Skewness and Kurtosis
            skewness = data.skew(numeric_only=True)
            kurtosis_vals = data.kurt(numeric_only=True)
            res_qc["skewness"] = skewness[abs(skewness) > 1]
            res_qc["kurtosis"] = kurtosis_vals[abs(kurtosis_vals) > 3]

            # Entropy for categorical columns (higher entropy suggests more disorder)
            categorical_cols = data.select_dtypes(include=["object", "category"]).columns
            res_qc["entropy_categoricals"] = {
                col: entropy(data[col].value_counts(normalize=True), base=2)
                for col in categorical_cols
            }

            # dtypes counts
            res_qc["dtype_counts"] = data.dtypes.value_counts()

            # Distribution Analysis (mean, median, mode, std dev, IQR for numeric columns)
            
            distribution_stats = data.select_dtypes(include=[np.number]).describe().T
            iqr = data.select_dtypes(include=[np.number]).apply(
                lambda x: x.quantile(0.75) - x.quantile(0.25)
            )
            distribution_stats["IQR"] = iqr
            res_qc["distribution_analysis"] = distribution_stats

            # Variance Check: Identify low-variance columns
            variance_threshold = 0.01
            low_variance_cols = [
                col
                for col in data.select_dtypes(include=[np.number]).columns
                if data[col].var() < variance_threshold
            ]
            res_qc["low_variance_features"] = low_variance_cols

            # Categorical columns and cardinality
            categorical_cols = data.select_dtypes(include=["object", "category"]).columns
            high_cardinality = {
                col: data[col].nunique() for col in categorical_cols if data[col].nunique() > 50
            }
            res_qc["high_cardinality_categoricals"] = high_cardinality

            # Feature-type inconsistency (mixed types in columns)
            inconsistent_types = {}
            for col in data.columns:
                unique_types = set(type(val) for val in data[col].dropna())
                if len(unique_types) > 1:
                    inconsistent_types[col] = unique_types
            res_qc["inconsistent_types"] = inconsistent_types

            # Text length analysis for text fields
            text_lengths = {}
            for col in categorical_cols:
                text_lengths[col] = {
                    "avg_length": data[col].dropna().astype(str).apply(len).mean(),
                    "length_variance": data[col].dropna().astype(str).apply(len).var(),
                }
            res_qc["text_length_analysis"] = text_lengths 

            # Summary statistics
            res_qc["summary_statistics"] = data.describe().T.style.background_gradient(
                cmap="coolwarm", axis=0
            )

            # Automated warnings
            warnings = []
            if res_qc["duplicate_rows"] > 0:
                warnings.append("Warning: Duplicate rows detected.")
            if len(res_qc["empty_columns"]) > 0:
                warnings.append("Warning: Columns with only NaN values detected.")
            if len(res_qc["constant_columns"]) > 0:
                warnings.append("Warning: Columns with a single constant value detected.")
            if len(high_corr_pairs) > 0:
                warnings.append("Warning: Highly correlated columns detected.")
            if len(res_qc["vif"]) > 0:
                warnings.append("Warning: Multicollinearity detected in features.")
            if len(high_cardinality) > 0:
                warnings.append("Warning: High cardinality in categorical columns.")
            if len(inconsistent_types) > 0:
                warnings.append("Warning: Columns with mixed data types detected.")
            res_qc["warnings"] = warnings 
            pd.reset_option("display.max_seq_items")
            if plot_:
                df_qc_plots(
                    data=data, res_qc=res_qc, max_cols=max_cols, hue=hue, dir_save=dir_save
                )
            if output or not plot_:
                return res_qc
            return None
        except Exception as e:
            print(e)

    # ==========================================
    # PYGwalker Integration (after QC analysis)
    # ==========================================
    if engine.lower() == "pygwalker":
        print("\n" + "‚ïê" * 70)
        print("üéÆ PYGwalker")
        print("‚ïê" * 70)
        
        try:
            import pygwalker as pyg
            import warnings
            warnings.filterwarnings('ignore')
            
            # Default PyGWalker parameters
            default_pyg_params = {
                'dark': False,
                'sample': 10000,
                'hide_config': True,
                'spec': None,
                'use_kernel_calc': True,
                'kernel_computation': True,
            }
            
            # Merge user parameters
            if pygwalker_params:
                default_pyg_params.update(pygwalker_params)
            
            # Prepare data for exploration
            explore_data = data.copy()
            
            # Clean data for PyGWalker - ensure no problematic types
            for col in explore_data.columns:
                # Convert problematic types
                if explore_data[col].dtype == 'complex':
                    explore_data[col] = explore_data[col].astype(str)
                elif explore_data[col].dtype == 'object':
                    # Check if it's a string column with mixed types
                    try:
                        explore_data[col] = explore_data[col].astype(str)
                    except:
                        pass
            
            # Sample if dataset is large
            if len(explore_data) > default_pyg_params['sample']:
                sample_size = default_pyg_params['sample']
                explore_data_sample = explore_data.sample(
                    n=min(sample_size, len(explore_data)),
                    random_state=42
                )
                print(f"Dataset: {explore_data.shape[0]} rows √ó {explore_data.shape[1]} columns")
                print(f"Showing: {len(explore_data_sample)} samples (random)")
            else:
                explore_data_sample = explore_data
                print(f"Dataset: {explore_data.shape[0]} rows √ó {explore_data.shape[1]} columns")
            
            print("\n Quick Tips:")
            print("   ‚Ä¢ Drag fields to X/Y axes to create charts")
            print("   ‚Ä¢ Use the toolbar to change chart types")
            print("   ‚Ä¢ Filter data using the filter panel")
            
            # Try different methods to launch PyGWalker
            print("\n Launching interactive explorer...")
            
            try:
                # Method 1: Try with minimal parameters first
                gwalker = pyg.walk(
                    explore_data_sample,
                    hideDataSourceConfig=default_pyg_params['hide_config'],
                    dark='dark' if default_pyg_params['dark'] else 'light'
                )
                
                # Try to display it
                try:
                    display(gwalker)
                    print("‚úÖ Interactive widget loaded above ‚Üë")
                except:
                    print("‚ö†Ô∏è Could not display as widget, but PyGWalker should open in browser")
                    print(f"üìå If browser doesn't open, visit: http://localhost:{port}")
                
            except Exception as e:
                print(f"‚ö†Ô∏è Could not launch PyGWalker with standard method: {e}")
                
                # Method 2: Try simple approach
                try:
                    print("üîÑ Trying alternative method...")
                    import pygwalker as pyg_simple
                    gwalker_simple = pyg_simple.walk(explore_data_sample)
                    print("‚úÖ PyGWalker launched (simple mode)")
                except Exception as e2:
                    print(f"‚ùå Could not launch PyGWalker: {e2}")
                    print("\nüí° Troubleshooting tips:")
                    print("   1. Update PyGWalker: pip install --upgrade pygwalker")
                    print("   2. Try in a clean Python environment")
                    print("   3. Check if you're in a Jupyter notebook")
                    print("   4. Try with a smaller sample: pygwalker_params={'sample': 1000}")
        
        except ImportError:
            print("‚ö†Ô∏è PyGWalker not installed. Interactive exploration skipped.")
            print("üí° Install with: pip install pygwalker[notebook]")
        except Exception as e:
            print(f"‚ö†Ô∏è Unexpected error with PyGWalker: {e}")
            print("üí° The QC analysis above is complete. You can still use the results.")
 

def df_qc_plots(
    data: pd.DataFrame,
    columns=None,
    res_qc: dict = None,
    max_cols=20,
    hue=None,
    dir_save=None,
):
    import matplotlib.pyplot as plt
    import seaborn as sns
    from .plot import subplot, figsets, get_color
    from datetime import datetime

    now_ = datetime.now().strftime("%y%m%d_%H%M%S")

    if columns is not None:
        if isinstance(columns, (list, pd.core.indexes.base.Index)):
            data = data[columns]
    len_total = len(res_qc)
    n_row, n_col = int((len_total + 10)), 3
    nexttile = subplot(n_row, n_col, figsize=[5 * n_col, 5 * n_row], verbose=False)

    missing_data = res_qc["missing_values"][res_qc["missing_values"] > 0].sort_values(
        ascending=False
    )
    if len(missing_data) > max_cols:
        missing_data = missing_data[:max_cols]
    ax_missing_data = sns.barplot(
        y=missing_data.index,
        x=missing_data.values,
        hue=missing_data.index,
        palette=get_color(len(missing_data), cmap="coolwarm")[::-1],
        ax=nexttile(),
    )
    figsets(
        title="Missing (#)",
        xlabel="#",
        ax=ax_missing_data,
        ylabel=None,
        fontsize=8 if len(missing_data) <= 20 else 6,
    )

    outlier_num = res_qc["outlier_num"].sort_values(ascending=False)
    if len(outlier_num) > max_cols:
        outlier_num = outlier_num[:max_cols]
    ax_outlier_num = sns.barplot(
        y=outlier_num.index,
        x=outlier_num.values,
        hue=outlier_num.index,
        palette=get_color(len(outlier_num), cmap="coolwarm")[::-1],
        ax=nexttile(),
    )
    figsets(
        ax=ax_outlier_num,
        title="Outliers (#)",
        xlabel="#",
        ylabel=None,
        fontsize=8 if len(outlier_num) <= 20 else 6,
    )

    #!
    try:
        for col in data.select_dtypes(include="category").columns:
            sns.countplot(
                y=data[col],
                palette=get_color(
                    data.select_dtypes(include="category").shape[1], cmap="coolwarm"
                )[::-1],
                ax=nexttile(),
            )
            figsets(title=f"Count Plot: {col}", xlabel="Count", ylabel=col)
    except Exception as e:
        pass

    # Skewness and Kurtosis Plots
    skewness = res_qc["skewness"].sort_values(ascending=False)
    kurtosis = res_qc["kurtosis"].sort_values(ascending=False)
    if not skewness.empty:
        ax_skewness = sns.barplot(
            y=skewness.index,
            x=skewness.values,
            hue=skewness.index,
            palette=get_color(len(skewness), cmap="coolwarm")[::-1],
            ax=nexttile(),
        )
        figsets(
            title="Highly Skewed Numeric Columns (Skewness > 1)",
            xlabel="Skewness",
            ylabel=None,
            ax=ax_skewness,
            fontsize=8 if len(skewness) <= 20 else 6,
        )
    if not kurtosis.empty:
        ax_kurtosis = sns.barplot(
            y=kurtosis.index,
            x=kurtosis.values,
            hue=kurtosis.index,
            palette=get_color(len(kurtosis), cmap="coolwarm")[::-1],
            ax=nexttile(),
        )
        figsets(
            title="Highly Kurtotic Numeric Columns (Kurtosis > 3)",
            xlabel="Kurtosis",
            ylabel=None,
            ax=ax_kurtosis,
            fontsize=8 if len(kurtosis) <= 20 else 6,
        )

    # Entropy for Categorical Variables
    entropy_data = pd.Series(res_qc["entropy_categoricals"]).sort_values(
        ascending=False
    )
    ax_entropy_data = sns.barplot(
        y=entropy_data.index,
        x=entropy_data.values,
        hue=entropy_data.index,
        palette=get_color(len(entropy_data), cmap="coolwarm")[::-1],
        ax=nexttile(),
    )
    figsets(
        ylabel="Categorical Columns",
        title="Entropy of Categorical Variables",
        xlabel="Entropy (bits)",
        ax=ax_entropy_data,
        fontsize=8 if len(entropy_data) <= 20 else 6,
    )

    # unique counts
    unique_counts = res_qc["unique_counts"].sort_values(ascending=False)
    ax_unique_counts_ = sns.barplot(
        y=unique_counts.index,
        x=unique_counts.values,
        hue=unique_counts.index,
        palette=get_color(len(unique_counts), cmap="coolwarm")[::-1],
        ax=nexttile(),
    )
    figsets(
        title="Unique Counts",
        ylabel=None,
        xlabel="#",
        ax=ax_unique_counts_,
        fontsize=8 if len(unique_counts) <= 20 else 6,
    )
    # Binary Checking
    ax_unique_counts = sns.barplot(
        y=unique_counts[unique_counts < 8].index,
        x=unique_counts[unique_counts < 8].values,
        hue=unique_counts[unique_counts < 8].index,
        palette=get_color(len(unique_counts[unique_counts < 8].index), cmap="coolwarm")[
            ::-1
        ],
        ax=nexttile(),
    )
    plt.axvline(x=2, color="r", linestyle="--", lw=2)
    figsets(
        ylabel=None,
        title="Binary Checking",
        xlabel="#",
        ax=ax_unique_counts,
        fontsize=8 if len(unique_counts[unique_counts < 10].index) <= 20 else 6,
    )

    # dtypes counts
    dtype_counts = res_qc["dtype_counts"]
    txt = []
    for tp in dtype_counts.index:
        txt.append(list(data.select_dtypes(include=tp).columns))

    ax_dtype_counts = sns.barplot(
        x=dtype_counts.index,
        y=dtype_counts.values,
        color="#F3C8B2",
        ax=nexttile(),
    )
    max_columns_per_row = 1  # Maximum number of columns per row
    for i, tp in enumerate(dtype_counts.index):
        if i <= 20:
            column_names = txt[i]
            # Split the column names into multiple lines if too long
            column_name_str = ", ".join(column_names)
            if len(column_name_str) > 40:  # If column names are too long, split them
                column_name_str = "\n".join(
                    [
                        ", ".join(column_names[j : j + max_columns_per_row])
                        for j in range(0, len(column_names), max_columns_per_row)
                    ]
                )
            # Place text annotation with line breaks and rotate the text if needed
            ax_dtype_counts.text(
                i,
                dtype_counts.values[i],
                f"{column_name_str}",
                ha="center",
                va="top",
                c="k",
                fontsize=8 if len(dtype_counts.index) <= 20 else 6,
                rotation=0,
            )

    figsets(
        xlabel=None,
        title="Dtypes",
        ylabel="#",
        ax=ax_dtype_counts,
        fontsize=8 if len(dtype_counts.index) <= 20 else 6,
    )
    # from .plot import pie
    # pie()

    # High cardinality: Show top categorical columns by unique value count
    high_cardinality = res_qc["high_cardinality_categoricals"]
    if high_cardinality and len(high_cardinality) > max_cols:
        high_cardinality = dict(
            sorted(high_cardinality.items(), key=lambda x: x[1], reverse=True)[
                :max_cols
            ]
        )

    if high_cardinality:
        ax_high_cardinality = sns.barplot(
            y=list(high_cardinality.keys()),
            x=list(high_cardinality.values()),
            hue=list(high_cardinality.keys()),
            palette=get_color(len(list(high_cardinality.keys())), cmap="coolwarm")[
                ::-1
            ],
            ax=nexttile(),
        )
        figsets(
            title="High Cardinality Categorical Columns",
            xlabel="Unique Value Count",
            ax=ax_high_cardinality,
            fontsize=8 if len(list(high_cardinality.keys())) <= 20 else 6,
        )
    if res_qc["low_variance_features"]:
        low_variance_data = data[res_qc["low_variance_features"]].copy()
        for col in low_variance_data.columns:
            ax_low_variance_features = sns.histplot(
                low_variance_data[col], bins=20, kde=True, color="coral", ax=nexttile()
            )
            figsets(
                title=f"Low Variance Feature: {col}",
                ax=ax_low_variance_features,
                fontsize=8 if len(low_variance_data[col]) <= 20 else 6,
            )

    # VIF plot for multicollinearity detection
    if "vif" in res_qc and not res_qc["vif"].empty:
        vif_data = res_qc["vif"].sort_values(by="VIF", ascending=False)
        if len(vif_data) > max_cols:
            vif_data = vif_data[:max_cols]
        ax_vif = sns.barplot(
            data=vif_data,
            x="VIF",
            y="feature",
            hue="VIF",
            palette=get_color(len(vif_data), cmap="coolwarm")[::-1],
            ax=nexttile(),
        )
        figsets(
            title="Variance Inflation Factor(VIF)",
            xlabel="VIF",
            ylabel="Features",
            legend=None,
            ax=ax_vif,
            fontsize=8 if len(vif_data) <= 20 else 6,
        )

    # Correlation heatmap for numeric columns with high correlation pairs
    if any(data.dtypes.apply(pd.api.types.is_numeric_dtype)):
        corr = data.select_dtypes(include=[np.number]).corr()
        if corr.shape[1] <= 33:
            mask = np.triu(np.ones_like(corr, dtype=bool))
            num_columns = corr.shape[1]
            fontsize = max(
                6, min(12, 12 - (num_columns - 10) * 0.2)
            )  # Scale between 8 and 12

            ax_heatmap = sns.heatmap(
                corr,
                mask=mask,
                annot=True,
                cmap="coolwarm",
                center=0,
                fmt=".1f",
                linewidths=0.5,
                vmin=-1,
                vmax=1,
                ax=nexttile(2, 2),
                cbar_kws=dict(shrink=0.2, ticks=np.arange(-1, 2, 1)),
                annot_kws={"size": fontsize},
            )

            figsets(xangle=45, title="Correlation Heatmap", ax=ax_heatmap)

    if columns is not None:
        if isinstance(columns, (list, pd.core.indexes.base.Index)):
            data = data[columns]

    #! check distribution
    data_num = data.select_dtypes(include=np.number)
    if len(data_num) > max_cols:
        data_num = data_num.iloc[:, :max_cols]

    data_num = df_scaler(data=data_num, method="standard")

    import scipy.stats as stats

    for column in data_num.columns:
        # * Shapiro-Wilk test for normality
        stat, p_value = stats.shapiro(data_num[column])
        normality = "norm" if p_value > 0.05 else "not_norm"
        # * Plot histogram
        ax_hist = sns.histplot(data_num[column], kde=True, ax=nexttile())
        x_min, x_max = ax_hist.get_xlim()
        y_min, y_max = ax_hist.get_ylim()
        ax_hist.text(
            x_min + (x_max - x_min) * 0.5,
            y_min + (y_max - y_min) * 0.75,
            f"p(Shapiro-Wilk)={p_value:.3f}\n{normality}",
            ha="center",
            va="top",
        )
        figsets(title=column, ax=ax_hist)
        ax_twin = ax_hist.twinx()
        # * Q-Q plot
        stats.probplot(data_num[column], dist="norm", plot=ax_twin)
        figsets(ylabel=f"Q-Q Plot:{column}", title=None)
    # save figure
    if dir_save:
        figsave(dir_save, f"qc_plot_{now_}.pdf")


def df_corr(df: pd.DataFrame, method="pearson"):
    """
    Compute correlation coefficients and p-values for a DataFrame.

    Parameters:
    - df (pd.DataFrame): Input DataFrame with numeric data.
    - method (str): Correlation method ("pearson", "spearman", "kendall").

    Returns:
    - corr_matrix (pd.DataFrame): Correlation coefficient matrix.
    - pval_matrix (pd.DataFrame): P-value matrix.
    """
    from scipy.stats import pearsonr, spearmanr, kendalltau

    methods = ["pearson", "spearman", "kendall"]
    method = strcmp(method, methods)[0]
    methods_dict = {"pearson": pearsonr, "spearman": spearmanr, "kendall": kendalltau}

    cols = df.columns
    corr_matrix = pd.DataFrame(index=cols, columns=cols, dtype=float)
    pval_matrix = pd.DataFrame(index=cols, columns=cols, dtype=float)
    correlation_func = methods_dict[method]

    for col1 in cols:
        for col2 in cols:
            if col1 == col2:
                corr_matrix.loc[col1, col2] = 1.0
                pval_matrix.loc[col1, col2] = 0.0
            else:
                corr, pval = correlation_func(df[col1], df[col2])
                corr_matrix.loc[col1, col2] = corr
                pval_matrix.loc[col1, col2] = pval

    return corr_matrix, pval_matrix


def get_pd_info():
    global _usages_pd
    if _usages_pd is None:
        from pathlib import Path
        current_directory = Path(__file__).resolve().parent
        _usages_pd =  fload(current_directory / "data" / "usages_pd.json")
    return _usages_pd

def use_pd(
    func_name="excel",
    verbose=True,
):
    try:
        if not "_usages_pd" in locals():
            _usages_pd = get_pd_info()
        valid_kinds = list(_usages_pd.keys())
        kind = strcmp(func_name, valid_kinds)[0]
        usage = _usages_pd[kind]
        if verbose:
            for i, i_ in enumerate(ssplit(usage, by=",")):
                i_ = i_.replace("=", "\t= ") + ","
                print(i_) if i == 0 else print("\t", i_)
        else:
            print(usage)
    except Exception as e:
        if verbose:
            print(e)


def get_phone(phone_number: str, region: str = None, verbose=True):
    """
    usage:
        info = get_phone(15237654321, "DE")
        preview(info)

    Extremely advanced phone number analysis function.

    Args:
        phone_number (str): The phone number to analyze.
        region (str): None (Default). Tries to work with international numbers including country codes; otherwise, uses the specified region.

    Returns:
        dict: Comprehensive information about the phone number.
    """
    import phonenumbers
    from phonenumbers import geocoder, carrier, timezone, number_type
    from datetime import datetime
    import pytz
    from tzlocal import get_localzone

    if not isinstance(phone_number, str):
        phone_number = str(phone_number)
    if isinstance(region, str):
        region = region.upper()

    try:
        # Parse the phone number
        parsed_number = phonenumbers.parse(phone_number, region)

        # Validate the phone number
        valid = phonenumbers.is_valid_number(parsed_number)
        possible = phonenumbers.is_possible_number(parsed_number)

        if not valid:
            suggested_fix = phonenumbers.example_number(region) if region else "Unknown"
            return {
                "valid": False,
                "error": "Invalid phone number",
                "suggested_fix": suggested_fix,
            }

        # Basic details
        formatted_international = phonenumbers.format_number(
            parsed_number, phonenumbers.PhoneNumberFormat.INTERNATIONAL
        )
        formatted_national = phonenumbers.format_number(
            parsed_number, phonenumbers.PhoneNumberFormat.NATIONAL
        )
        formatted_e164 = phonenumbers.format_number(
            parsed_number, phonenumbers.PhoneNumberFormat.E164
        )
        country_code = parsed_number.country_code
        region_code = geocoder.region_code_for_number(parsed_number)
        country_name = geocoder.country_name_for_number(parsed_number, "en")

        location = geocoder.description_for_number(parsed_number, "en")
        carrier_name = carrier.name_for_number(parsed_number, "en") or "Unknown Carrier"
        time_zones = timezone.time_zones_for_number(parsed_number)[0]
        current_times = datetime.now(pytz.timezone(time_zones)).strftime(
            "%Y-%m-%d %H:%M:%S %Z"
        )
        number_type_str = {
            phonenumbers.PhoneNumberType.FIXED_LINE: "Fixed Line",
            phonenumbers.PhoneNumberType.MOBILE: "Mobile",
            phonenumbers.PhoneNumberType.FIXED_LINE_OR_MOBILE: "Fixed Line or Mobile",
            phonenumbers.PhoneNumberType.TOLL_FREE: "Toll Free",
            phonenumbers.PhoneNumberType.PREMIUM_RATE: "Premium Rate",
            phonenumbers.PhoneNumberType.SHARED_COST: "Shared Cost",
            phonenumbers.PhoneNumberType.VOIP: "VOIP",
            phonenumbers.PhoneNumberType.PERSONAL_NUMBER: "Personal Number",
            phonenumbers.PhoneNumberType.PAGER: "Pager",
            phonenumbers.PhoneNumberType.UAN: "UAN",
            phonenumbers.PhoneNumberType.UNKNOWN: "Unknown",
        }.get(number_type(parsed_number), "Unknown")

        # Advanced Features
        is_toll_free = (
            number_type(parsed_number) == phonenumbers.PhoneNumberType.TOLL_FREE
        )
        is_premium_rate = (
            number_type(parsed_number) == phonenumbers.PhoneNumberType.PREMIUM_RATE
        )

        # Dialing Information
        dialing_instructions = f"Dial {formatted_national} within {country_name}. Dial {formatted_e164} from abroad."

        # Advanced Timezone Handling
        gmt_offsets = (
            pytz.timezone(time_zones).utcoffset(datetime.now()).total_seconds() / 3600
        )
        # Get the local timezone (current computer's time)
        local_timezone = get_localzone()
        # local_timezone = pytz.timezone(pytz.country_timezones[region_code][0])
        local_offset = local_timezone.utcoffset(datetime.now()).total_seconds() / 3600
        offset_diff = local_offset - gmt_offsets
        head_time = "earlier" if offset_diff < 0 else "later" if offset_diff > 0 else ""
        res = {
            "valid": True,
            "possible": possible,
            "formatted": {
                "international": formatted_international,
                "national": formatted_national,
                "e164": formatted_e164,
            },
            "country_code": country_code,
            "country_name": country_name,
            "region_code": region_code,
            "location": location if location else "Unknown",
            "carrier": carrier_name,
            "time_zone": time_zones,
            "current_times": current_times,
            "local_offset": f"{local_offset} utcoffset",
            "time_zone_diff": f"{head_time} {int(np.abs(offset_diff))} h",
            "number_type": number_type_str,
            "is_toll_free": is_toll_free,
            "is_premium_rate": is_premium_rate,
            "dialing_instructions": dialing_instructions,
            "suggested_fix": None,  # Use phonenumbers.example_number if invalid
            "logs": {
                "number_analysis_completed": datetime.now().strftime(
                    "%Y-%m-%d %H:%M:%S"
                ),
                "raw_input": phone_number,
                "parsed_number": str(parsed_number),
            },
        }

    except phonenumbers.NumberParseException as e:
        res = {"valid": False, "error": str(e)}
    if verbose:
        preview(res)
    return res


def decode_pluscode(
    pluscode: str, reference: tuple = (52.5200, 13.4050), return_bbox: bool = False
):
    """
    Decodes a Plus Code into latitude and longitude (and optionally returns a bounding box).

    Parameters:
        pluscode (str): The Plus Code to decode. Can be full or short.
        reference (tuple, optional): Reference latitude and longitude for decoding short Plus Codes.
                                     Default is None, required if Plus Code is short.
        return_bbox (bool): If True, returns the bounding box coordinates (latitude/longitude bounds).
                            Default is False.

    Returns:
        tuple: (latitude, longitude) if `return_bbox` is False.
               (latitude, longitude, bbox) if `return_bbox` is True.
               bbox = (latitudeLo, latitudeHi, longitudeLo, longitudeHi)
    Raises:
        ValueError: If the Plus Code is invalid or reference is missing for a short code.

    Usage:
    lat, lon = decode_pluscode("7FG6+89")
    print(f"Decoded Short Plus Code: Latitude: {lat}, Longitude: {lon}, Bounding Box: {bbox}")

    lat, lon = decode_pluscode("9F4M7FG6+89")
    print(f"Decoded Full Plus Code: Latitude: {lat}, Longitude: {lon}")
    """
    from openlocationcode import openlocationcode as olc

    # Validate Plus Code
    if not olc.isValid(pluscode):
        raise ValueError(f"Invalid Plus Code: {pluscode}")

    # Handle Short Plus Codes
    if olc.isShort(pluscode):
        if reference is None:
            raise ValueError(
                "Reference location (latitude, longitude) is required for decoding short Plus Codes."
            )
        # Recover the full Plus Code using the reference location
        pluscode = olc.recoverNearest(pluscode, reference[0], reference[1])

    # Decode the Plus Code
    decoded = olc.decode(pluscode)

    # Calculate the center point of the bounding box
    latitude = (decoded.latitudeLo + decoded.latitudeHi) / 2
    longitude = (decoded.longitudeLo + decoded.longitudeHi) / 2

    if return_bbox:
        bbox = (
            decoded.latitudeLo,
            decoded.latitudeHi,
            decoded.longitudeLo,
            decoded.longitudeHi,
        )
        return latitude, longitude, bbox

    return latitude, longitude


def get_loc(input_data, user_agent="0413@mygmail.com)", verbose=True):
    """
        Determine if the input is a city name, lat/lon, or DMS and perform geocoding or reverse geocoding.
    Usage:
        get_loc("Berlin, Germany")  # Example city
        # get_loc((48.8566, 2.3522))  # Example latitude and longitude
        # get_loc("48 51 24.3 N")  # Example DMS input
    """
    from geopy.geocoders import Nominatim
    

    def dms_to_decimal(dms):
        """
        Convert DMS (Degrees, Minutes, Seconds) to Decimal format.
        Input should be in the format of "DD MM SS" or "D M S".
        """
        # Regex pattern for DMS input
        pattern = r"(\d{1,3})[^\d]*?(\d{1,2})[^\d]*?(\d{1,2})"
        match = re.match(pattern, dms)

        if match:
            degrees, minutes, seconds = map(float, match.groups())
            decimal = degrees + (minutes / 60) + (seconds / 3600)
            return decimal
        else:
            raise ValueError("Invalid DMS format")

    geolocator = Nominatim(user_agent="0413@mygmail.com)")
    # Case 1: Input is a city name (string)
    if isinstance(input_data, str) and not re.match(r"^\d+(\.\d+)?$", input_data):
        location = geolocator.geocode(input_data)
        try:
            if verbose:
                print(
                    f"Latitude and Longitude for {input_data}: {location.latitude}, {location.longitude}"
                )
            else:
                print(f"Could not find {input_data}.")
            return location
        except Exception as e:
            print(f"Error: {e}")
            return

    # Case 2: Input is latitude and longitude (float or tuple)
    elif isinstance(input_data, (float, tuple)):
        if isinstance(input_data, tuple) and len(input_data) == 2:
            latitude, longitude = input_data
        elif isinstance(input_data, float):
            latitude = input_data
            longitude = None  # No longitude provided for a single float

        # Reverse geocoding
        location_reversed = geolocator.reverse(
            (latitude, longitude) if longitude else latitude
        )
        if verbose:
            print(
                f"Address from coordinates ({latitude}, {longitude if longitude else ''}): {location_reversed.address}"
            )
        else:
            print("Could not reverse geocode the coordinates.")
        return location_reversed

    # Case 3: Input is a DMS string
    elif isinstance(input_data, str):
        try:
            decimal_lat = dms_to_decimal(input_data)
            print(f"Converted DMS to decimal latitude: {decimal_lat}")

            location_reversed = geolocator.reverse(decimal_lat)
            if verbose:
                print(f"Address from coordinates: {location_reversed.address}")
            else:
                print("Could not reverse geocode the coordinates.")
            return location_reversed
        except ValueError:
            print(
                "Invalid input format. Please provide a city name, latitude/longitude, or DMS string."
            )


def enpass(code: str, method: str = "AES", key: str = None):
    """
    usage: enpass("admin")
    Master encryption function that supports multiple methods: AES, RSA, and SHA256.
    :param code: The input data to encrypt or hash.
    :param method: The encryption or hashing method ('AES', 'RSA', or 'SHA256').
    :param key: The key to use for encryption. For AES and RSA, it can be a password or key in PEM format.
    :return: The encrypted data or hashed value.
    """
    import hashlib

    # AES Encryption (Advanced)
    def aes_encrypt(data: str, key: str):
        """
        Encrypts data using AES algorithm in CBC mode.
        :param data: The data to encrypt.
        :param key: The key to use for AES encryption.
        :return: The encrypted data, base64 encoded.
        """
        from cryptography.hazmat.primitives.ciphers import Cipher, algorithms, modes
        from cryptography.hazmat.backends import default_backend
        from cryptography.hazmat.primitives import padding
        import base64
        

        # Generate a 256-bit key from the provided password
        key = hashlib.sha256(key.encode()).digest()

        # Generate a random initialization vector (IV)
        iv = os.urandom(16)  # 16 bytes for AES block size

        # Pad the data to be a multiple of 16 bytes using PKCS7
        padder = padding.PKCS7(128).padder()
        padded_data = padder.update(data.encode()) + padder.finalize()

        # Create AES cipher object using CBC mode
        cipher = Cipher(algorithms.AES(key), modes.CBC(iv), backend=default_backend())
        encryptor = cipher.encryptor()
        encrypted_data = encryptor.update(padded_data) + encryptor.finalize()

        # Return the base64 encoded result (IV + encrypted data)
        return base64.b64encode(iv + encrypted_data).decode()

    # RSA Encryption (Advanced)
    def rsa_encrypt(data: str, public_key: str):
        """
        Encrypts data using RSA encryption with OAEP padding.
        :param data: The data to encrypt.
        :param public_key: The public key in PEM format.
        :return: The encrypted data, base64 encoded.
        """
        import base64
        from Crypto.PublicKey import RSA
        from Crypto.Cipher import PKCS1_OAEP

        public_key_obj = RSA.import_key(public_key)
        cipher_rsa = PKCS1_OAEP.new(public_key_obj)
        encrypted_data = cipher_rsa.encrypt(data.encode())
        return base64.b64encode(encrypted_data).decode()

    # SHA256 Hashing (Non-reversible)
    def sha256_hash(data: str):
        """
        Generates a SHA256 hash of the data.
        :param data: The data to hash.
        :return: The hashed value (hex string).
        """
        return hashlib.sha256(data.encode()).hexdigest()
    def md5_hash(data:str):
        return hashlib.md5(data.encode()).hexdigest()

    if key is None:
        key = "worldpeace"
    method = strcmp(method, ["AES", "RSA", "SHA256","md5","hash"])[0]
    if method.upper() == "AES":
        return aes_encrypt(code, key)
    elif method.upper() == "RSA":
        return rsa_encrypt(code, key)
    elif method.upper() == "SHA256":
        return sha256_hash(code)
    elif method.lower() == "md5" or method.lower() == "hash":
        return md5_hash(code)
    else:
        raise ValueError("Unsupported encryption method")


# Master Decryption Function (Supports AES, RSA)
def depass(encrypted_code: str, method: str = "AES", key: str = None):
    """
    Master decryption function that supports multiple methods: AES and RSA.
    :param encrypted_code: The encrypted data to decrypt.
    :param method: The encryption method ('AES' or 'RSA').
    :param key: The key to use for decryption. For AES and RSA, it can be a password or key in PEM format.
    :return: The decrypted data.
    """
    import hashlib

    def aes_decrypt(encrypted_data: str, key: str):
        """
        Decrypts data encrypted using AES in CBC mode.
        :param encrypted_data: The encrypted data, base64 encoded.
        :param key: The key to use for AES decryption.
        :return: The decrypted data (string).
        """
        from cryptography.hazmat.primitives.ciphers import Cipher, algorithms, modes
        from cryptography.hazmat.backends import default_backend
        from cryptography.hazmat.primitives import padding
        import base64

        # Generate the same 256-bit key from the password
        key = hashlib.sha256(key.encode()).digest()

        # Decode the encrypted data from base64
        encrypted_data = base64.b64decode(encrypted_data)

        # Extract the IV and the actual encrypted data
        iv = encrypted_data[:16]  # First 16 bytes are the IV
        encrypted_data = encrypted_data[16:]  # Remaining data is the encrypted message

        # Create AES cipher object using CBC mode
        cipher = Cipher(algorithms.AES(key), modes.CBC(iv), backend=default_backend())
        decryptor = cipher.decryptor()
        decrypted_data = decryptor.update(encrypted_data) + decryptor.finalize()

        # Unpad the decrypted data using PKCS7
        unpadder = padding.PKCS7(128).unpadder()
        unpadded_data = unpadder.update(decrypted_data) + unpadder.finalize()

        return unpadded_data.decode()

    def rsa_decrypt(encrypted_data: str, private_key: str):
        """
        Decrypts RSA-encrypted data using the private key.
        :param encrypted_data: The encrypted data, base64 encoded.
        :param private_key: The private key in PEM format.
        :return: The decrypted data (string).
        """
        from Crypto.PublicKey import RSA
        from Crypto.Cipher import PKCS1_OAEP
        import base64

        encrypted_data = base64.b64decode(encrypted_data)
        private_key_obj = RSA.import_key(private_key)
        cipher_rsa = PKCS1_OAEP.new(private_key_obj)
        decrypted_data = cipher_rsa.decrypt(encrypted_data)
        return decrypted_data.decode()

    if key is None:
        key = "worldpeace"
    method = strcmp(method, ["AES", "RSA", "SHA256","md5",'hash'])[0]
    if method == "AES":
        return aes_decrypt(encrypted_code, key)
    elif method == "RSA":
        return rsa_decrypt(encrypted_code, key)
    elif method == "SHA256":
        raise ValueError("SHA256 is a hash function and cannot be decrypted.")
    else:
        raise ValueError("Unsupported decryption method")


def get_clip(dir_save=None):
    """
    Master function to extract content from the clipboard (text, URL, or image).

    Parameters:
        dir_save (str, optional): If an image is found, save it to this path.

    Returns:
        dict: A dictionary with extracted content:
              {
                  "type": "text" | "url" | "image" | "none",
                  "content": <str|Image|None>,
                  "saved_to": <str|None>  # Path if an image is saved
              }
    """
    result = {"type": "none", "content": None, "saved_to": None}

    try:
        import pyperclip
        from PIL import ImageGrab, Image
        import validators

        # 1. Check for text in the clipboard
        clipboard_content = pyperclip.paste()
        if clipboard_content:
            if validators.url(clipboard_content.strip()):
                result["type"] = "url"
                result["content"] = clipboard_content.strip()

            else:
                result["type"] = "text"
                result["content"] = clipboard_content.strip()
            return clipboard_content.strip()

        # 2. Check for image in the clipboard
        image = ImageGrab.grabclipboard()
        if isinstance(image, Image.Image):
            result["type"] = "image"
            result["content"] = image
            if dir_save:
                image.save(dir_save)
                result["saved_to"] = dir_save
                print(f"Image saved to {dir_save}.")
            else:
                print("Image detected in clipboard but not saved.")
            return image
        print("No valid text, URL, or image found in clipboard.")
        return result

    except Exception as e:
        print(f"An error occurred: {e}")
        return result


def keyboard(*args, action="press", n_click=1, interval=0, verbose=False, **kwargs):
    """
    Simulates keyboard input using pyautogui.

    Parameters:
        input_key (str): The key to simulate. Check the list of supported keys with verbose=True.
        action (str): The action to perform. Options are 'press', 'keyDown', or 'keyUp'.
        n_click (int): Number of times to press the key (only for 'press' action).
        interval (float): Time interval between key presses for 'press' action.
        verbose (bool): Print detailed output, including supported keys and debug info.
        kwargs: Additional arguments (reserved for future extensions).

    keyboard("command", "d", action="shorcut")
    """
    import pyautogui

    input_key = args

    actions = ["press", "keyDown", "keyUp", "hold", "release", "hotkey", "shortcut"]
    action = strcmp(action, actions)[0]
    keyboard_keys_ = ["\t","\n","\r"," ","!",'"',"#","$","%","&","'",
        "(",")","*","+",",","-",".","/","0","1","2","3","4","5","6","7",
        "8","9",":",";","<",
        "=",
        ">",
        "?",
        "@",
        "[",
        "\\",
        "]",
        "^",
        "_",
        "`",
        "a",
        "b",
        "c",
        "d",
        "e",
        "f",
        "g",
        "h",
        "i",
        "j",
        "k",
        "l",
        "m",
        "n",
        "o",
        "p",
        "q",
        "r",
        "s",
        "t",
        "u",
        "v",
        "w",
        "x",
        "y",
        "z",
        "{",
        "|",
        "}",
        "~",
        "accept",
        "add",
        "alt",
        "altleft",
        "altright",
        "apps",
        "backspace",
        "browserback",
        "browserfavorites",
        "browserforward",
        "browserhome",
        "browserrefresh",
        "browsersearch",
        "browserstop",
        "capslock",
        "clear",
        "convert",
        "ctrl",
        "ctrlleft",
        "ctrlright",
        "decimal",
        "del",
        "delete",
        "divide",
        "down",
        "end",
        "enter",
        "esc",
        "escape",
        "execute",
        "f1",
        "f10",
        "f11",
        "f12",
        "f13",
        "f14",
        "f15",
        "f16",
        "f17",
        "f18",
        "f19",
        "f2",
        "f20",
        "f21",
        "f22",
        "f23",
        "f24",
        "f3",
        "f4",
        "f5",
        "f6",
        "f7",
        "f8",
        "f9",
        "final",
        "fn",
        "hanguel",
        "hangul",
        "hanja",
        "help",
        "home",
        "insert",
        "junja",
        "kana",
        "kanji",
        "launchapp1",
        "launchapp2",
        "launchmail",
        "launchmediaselect",
        "left",
        "modechange",
        "multiply",
        "nexttrack",
        "nonconvert",
        "num0",
        "num1",
        "num2",
        "num3",
        "num4",
        "num5",
        "num6",
        "num7",
        "num8",
        "num9",
        "numlock",
        "pagedown",
        "pageup",
        "pause",
        "pgdn",
        "pgup",
        "playpause",
        "prevtrack",
        "print",
        "printscreen",
        "prntscrn",
        "prtsc",
        "prtscr",
        "return",
        "right",
        "scrolllock",
        "select",
        "separator",
        "shift",
        "shiftleft",
        "shiftright",
        "sleep",
        "space",
        "stop",
        "subtract",
        "tab",
        "up",
        "volumedown",
        "volumemute",
        "volumeup",
        "win",
        "winleft",
        "winright",
        "yen",
        "command",
        "option",
        "optionleft",
        "optionright",
    ]
    if verbose:
        print(f"supported keys: {keyboard_keys_}")

    if action not in ["hotkey", "shortcut"]:
        if not isinstance(input_key, list):
            input_key = list(input_key)
        input_key = [strcmp(i, keyboard_keys_)[0] for i in input_key]

    # correct action
    cmd_keys = [
        "command",
        "option",
        "optionleft",
        "optionright",
        "win",
        "winleft",
        "winright",
        "ctrl",
        "ctrlleft",
        "ctrlright",
    ]
    try:
        if any([i in cmd_keys for i in input_key]):
            action = "hotkey"
    except:
        pass

    print(f"\n{action}: {input_key}")
    # keyboard
    if action in ["press"]:
        # pyautogui.press(input_key, presses=n_click,interval=interval)
        for _ in range(n_click):
            for key in input_key:
                pyautogui.press(key)
                pyautogui.sleep(interval)
    elif action in ["keyDown", "hold"]:
        # pyautogui.keyDown(input_key)
        for _ in range(n_click):
            for key in input_key:
                pyautogui.keyDown(key)
                pyautogui.sleep(interval)

    elif action in ["keyUp", "release"]:
        # pyautogui.keyUp(input_key)
        for _ in range(n_click):
            for key in input_key:
                pyautogui.keyUp(key)
                pyautogui.sleep(interval)

    elif action in ["hotkey", "shortcut"]:
        pyautogui.hotkey(input_key)


def mouse(
    *args,  # loc
    action: str = "move",
    duration: float = 0.5,
    loc_type: str = "absolute",  # 'absolute', 'relative'
    region: tuple = None,  # (tuple, optional): A region (x, y, width, height) to search for the image.
    image_path: str = None,
    wait: float = 0,
    text: str = None,
    confidence: float = 0.8,
    button: str = "left",
    n_click: int = 1,  # number of clicks
    interval: float = 0.25,  # time between clicks
    scroll_amount: int = -500,
    fail_safe: bool = True,
    grayscale: bool = False,
    n_try: int = 10,
    verbose: bool = True,
    **kwargs,
):
    """
    Master function to handle pyautogui actions.

    Parameters:
        action (str): The action to perform ('click', 'double_click', 'type', 'drag', 'scroll', 'move', 'locate', etc.).
        image_path (str, optional): Path to the image for 'locate' or 'click' actions.
        text (str, optional): Text to type for 'type' action.
        confidence (float, optional): Confidence level for image recognition (default 0.8).
        duration (float, optional): Duration for smooth movements in seconds (default 0.5).
        region (tuple, optional): A region (x, y, width, height) to search for the image.
        button (str, optional): Mouse button to use ('left', 'right', 'middle').
        n_click (int, optional): Number of times to click for 'click' actions.
        interval (float, optional): Interval between clicks for 'click' actions.
        offset (tuple, optional): Horizontal offset from the located image. y_offset (int, optional): Vertical offset from the located image.
        scroll_amount (int, optional): Amount to scroll (positive for up, negative for down).
        fail_safe (bool, optional): Enable/disable pyautogui's fail-safe feature.
        grayscale (bool, optional): Search for the image in grayscale mode.

    Returns:
        tuple or None: Returns coordinates for 'locate' actions, otherwise None.
    """
    import pyautogui
    import time

    # 
    # logging.basicConfig(level=logging.DEBUG, filename="debug.log")

    pyautogui.FAILSAFE = fail_safe  # Enable/disable fail-safe
    loc_type = "absolute" if "abs" in loc_type else "relative"
    if len(args) == 1:
        if isinstance(args[0], str):
            image_path = args[0]
            x_offset, y_offset = None, None
        else:
            x_offset, y_offset = args
    elif len(args) == 2:
        x_offset, y_offset = args
    elif len(args) == 3:
        x_offset, y_offset, action = args
    elif len(args) == 4:
        x_offset, y_offset, action, duration = args
    else:
        x_offset, y_offset = None, None

    what_action = [
        "locate",
        "click",
        "double_click",
        "triple_click",
        "input",
        "write",
        "type",
        "drag",
        "move",
        "scroll",
        "down",
        "up",
        "hold",
        "press",
        "release",
    ]
    action = strcmp(action, what_action)[0]
    # get the locations
    location = None
    if any([x_offset is None, y_offset is None]):
        if region is None:
            w, h = pyautogui.size()
            region = (0, 0, w, h)
        retries = 0
        while location is None and retries <= n_try:
            try:
                confidence_ = round(float(confidence - 0.05 * retries), 2)
                location = pyautogui.locateOnScreen(
                    image_path,
                    confidence=confidence_,
                    region=region,
                    grayscale=grayscale,
                )
            except Exception as e:
                if verbose:
                    print(f"confidence={confidence_},{e}")
                location = None
            retries += 1

    # try:
    if location:
        x, y = pyautogui.center(location)
        x += x_offset if x_offset else 0
        if x_offset is not None:
            x += x_offset
        if y_offset is not None:
            y += y_offset
        x_offset, y_offset = x, y
    print(action) if verbose else None
    if action in ["locate"]:
        x, y = pyautogui.position()
    elif action in ["click", "double_click", "triple_click"]:
        if action == "click":
            pyautogui.moveTo(x_offset, y_offset, duration=duration)
            time.sleep(wait)
            pyautogui.click(
                x=x_offset, y=y_offset, clicks=n_click, interval=interval, button=button
            )
        elif action == "double_click":
            pyautogui.moveTo(x_offset, y_offset, duration=duration)
            time.sleep(wait)
            pyautogui.doubleClick(
                x=x_offset, y=y_offset, interval=interval, button=button
            )
        elif action == "triple_click":
            pyautogui.moveTo(x_offset, y_offset, duration=duration)
            time.sleep(wait)
            pyautogui.tripleClick(
                x=x_offset, y=y_offset, interval=interval, button=button
            )

    elif action in ["type", "write", "input"]:
        pyautogui.moveTo(x_offset, y_offset, duration=duration)
        time.sleep(wait)
        if text is not None:
            pyautogui.typewrite(text, interval=interval)
        else:
            print("Text must be provided for the 'type' action.") if verbose else None

    elif action == "drag":
        if loc_type == "absolute":
            pyautogui.dragTo(x_offset, y_offset, duration=duration, button=button)
        else:
            pyautogui.dragRel(x_offset, y_offset, duration=duration, button=button)

    elif action in ["move"]:
        if loc_type == "absolute":
            pyautogui.moveTo(x_offset, y_offset, duration=duration)
        else:
            pyautogui.moveRel(x_offset, y_offset, duration=duration)

    elif action == "scroll":
        pyautogui.moveTo(x_offset, y_offset, duration=duration)
        time.sleep(wait)
        pyautogui.scroll(scroll_amount)

    elif action in ["down", "hold", "press"]:
        pyautogui.moveTo(x_offset, y_offset, duration=duration)
        time.sleep(wait)
        pyautogui.mouseDown(x_offset, y_offset, button=button, duration=duration)

    elif action in ["up", "release"]:
        pyautogui.moveTo(x_offset, y_offset, duration=duration)
        time.sleep(wait)
        pyautogui.mouseUp(x_offset, y_offset, button=button, duration=duration)

    else:
        raise ValueError(f"Unsupported action: {action}")


def py2installer(
    script_path: str = None,
    flatform: str = "mingw64",
    output_dir: str = "dist",
    icon_path: str = None,
    include_data: list = None,
    include_import: list = None,
    exclude_import: list = None,
    plugins: list = None,
    use_nuitka: bool = True,
    console: bool = True,
    clean_build: bool = False,
    additional_args: list = None,
    verbose: bool = True,
    standalone: bool = True,
    onefile: bool = False,
    use_docker: bool = False,
    docker_image: str = "python:3.12-slim",
):
    """
    to package Python scripts into standalone application.

    script_path (str): Path to the Python script to package.
    output_dir (str): Directory where the executable will be stored.
    icon_path (str): Path to the .ico file for the executable icon.
    include_data (list): List of additional data files or directories in "source:dest" format.
    exclude_import (list): List of hidden imports to include.
    plugins (list): List of plugins imports to include.e.g., 'tk-inter'
    use_nuitka (bool): Whether to use Nuitka instead of PyInstaller.
    console (bool): If False, hides the console window (GUI mode).
    clean_build (bool): If True, cleans previous build and dist directories.
    additional_args (list): Additional arguments for PyInstaller/Nuitka.
    verbose (bool): If True, provides detailed logs.
    use_docker (bool): If True, uses Docker to package the script.
    docker_image (str): Docker image to use for packaging.

    """
    
    

    if run_once_within():
        usage_str = """
            # build locally
            py2installer(
                script_path="update_tab.py",
                output_dir="dist",
                icon_path="icon4app.ico",
                include_data=["dat/*.xlsx:dat"],
                exclude_import=["msoffcrypto", "tkinter", "pandas", "numpy"],
                onefile=True,
                console=False,
                clean_build=True,
                verbose=True,
            )
            # build via docker
            py2installer(
                "my_script.py",
                output_dir="dist",
                onefile=True,
                clean_build=True,
                use_docker=True,
                docker_image="python:3.12-slim"
            )
            # Â∞ΩÈáè‰∏çË¶Å‰ΩøÁî®--include-package,ËøôÂèØËÉΩÂØºËá¥ÂÜ≤Á™Å
            py2installer(
                script_path="update_tab.py",
                # flatform=None,
                output_dir="dist_simp_subprocess",
                icon_path="icon4app.ico",
                standalone=True,
                onefile=False,
                include_data=["dat/*.xlsx=dat"],
                plugins=[
                    "tk-inter",
                ],
                use_nuitka=True,
                console=True,
                clean_build=False,
                verbose=0,
            )
            # ÊúÄÁªàÊñá‰ª∂Â§ßÂ∞èÂØπÊØî
            900 MB: nuitka --mingw64 --standalone --windows-console-mode=attach --show-progress --output-dir=dist --macos-create-app-bundle --macos-app-icon=icon4app.ico --nofollow-import-to=timm,paddle,torch,torchmetrics,torchvision,tensorflow,tensorboard,tensorboardx,tensorboard-data-server,textblob,PIL,sklearn,scienceplots,scikit-image,scikit-learn,scikit-surprise,scipy,spikeinterface,spike-sort-lfpy,stanza,statsmodels,streamlit,streamlit-autorefresh,streamlit-folium,pkg2ls,plotly --include-package=msoffcrypto,tkinter,datetime,pandas,numpy --enable-plugin=tk-inter update_tab.py;
            470 MB: nuitka --mingw64 --standalone --windows-console-mode=attach --show-progress --output-dir=dist_direct_nuitka --macos-create-app-bundle --macos-app-icon=icon4app.ico --enable-plugin=tk-inter update_taby ;
        """
        print(usage_str)
        if verbose:
            return
        else:
            pass
    # Check if the script path exists
    script_path = Path(script_path)
    if not script_path.exists():
        raise FileNotFoundError(f"Script '{script_path}' not found.")

    # Clean build and dist directories if requested
    if clean_build:
        for folder in ["build", "dist"]:
            folder_path = Path(folder)
            if folder_path.exists():
                shutil.rmtree(folder_path, ignore_errors=True)
        # Recreate the folders
        for folder in ["build", "dist"]:
            folder_path = Path(folder)
            folder_path.mkdir(parents=True, exist_ok=True)

    if use_docker:
        # Ensure Docker is installed
        try:
            subprocess.run(
                ["docker", "--version"], check=True, capture_output=True, text=True
            )
        except FileNotFoundError:
            raise EnvironmentError("Docker is not installed or not in the PATH.")

        # Prepare Docker volume mappings
        script_dir = script_path.parent.resolve()
        dist_path = Path(output_dir).resolve()
        volumes = [
            f"{script_dir}:/app:rw",
            f"{dist_path}:/output:rw",
        ]
        docker_cmd = [
            "docker",
            "run",
            "--rm",
            "-v",
            volumes[0],
            "-v",
            volumes[1],
            docker_image,
            "bash",
            "-c",
        ]

        # Build the packaging command inside the container
        cmd = ["nuitka"] if use_nuitka else ["pyinstaller"]
        if onefile:
            cmd.append("--onefile")
        if not console:
            cmd.append("--windowed")
        cmd.extend(["--distpath", "/output"])
        if icon_path:
            cmd.extend(["--icon", f"/app/{Path(icon_path).name}"])
        if include_data:
            for data in include_data:
                cmd.extend(["--add-data", f"/app/{data}"])
        if exclude_import:
            for hidden in exclude_import:
                cmd.extend(["--hidden-import", hidden])
        if additional_args:
            cmd.extend(additional_args)
        cmd.append(f"/app/{script_path.name}")

        # Full command to execute inside the container
        docker_cmd.append(" ".join(cmd))

        if verbose:
            print(f"Running Docker command: {' '.join(docker_cmd)}")

        # Run Docker command
        try:
            subprocess.run(
                docker_cmd,
                capture_output=not verbose,
                text=True,
                check=True,
            )
        except subprocess.CalledProcessError as e:
            print(f"Error during Docker packaging:\n{e.stderr}", file=sys.stderr)
            raise
    else:
        # Handle local packaging (native build)
        cmd = ["nuitka"] if use_nuitka else ["pyinstaller"]
        if "min" in flatform.lower() and use_nuitka:
            cmd.append("--mingw64")
        cmd.append("--standalone") if use_nuitka and standalone else None
        cmd.append("--onefile") if onefile else None
        if not console:
            cmd.append("--windows-console-mode=disable")
        else:
            cmd.append("--windows-console-mode=attach")

        cmd.extend(["--show-progress", f"--output-dir={output_dir}"])
        if icon_path:
            icon_path = Path(icon_path)
            if not icon_path.exists():
                raise FileNotFoundError(f"Icon file '{icon_path}' not found.")
            if sys.platform == "darwin":  # macOS platform
                cmd.extend(
                    ["--macos-create-app-bundle", f"--macos-app-icon={icon_path}"]
                )
            elif sys.platform == "win32":  # Windows platform
                cmd.extend([f"--windows-icon-from-ico={icon_path}"])
            elif sys.platform == "linux":  # Linux platform
                cmd.append("--linux-onefile")

        if include_data:
            for data in include_data:
                if "*" in data:
                    matches = glob.glob(data.split(":")[0])
                    for match in matches:
                        dest = data.split(":")[1]
                        cmd.extend(
                            [
                                "--include-data-file=" if use_nuitka else "--add-data",
                                f"{match}:{dest}",
                            ]
                        )
                else:
                    cmd.extend(
                        ["--include-data-file=" if use_nuitka else "--add-data", data]
                    )
        if exclude_import is not None:
            if any(exclude_import):
                cmd.extend([f"--nofollow-import-to={','.join(exclude_import)}"])
        if include_import is not None:
            if any(
                include_import
            ):  # are included in the final build. Some packages may require manual inclusion.
                cmd.extend([f"--include-package={','.join(include_import)}"])
        if plugins:
            for plugin in plugins:
                # Adds support for tkinter, ensuring it works correctly in the standalone build.
                cmd.extend([f"--enable-plugin={plugin}"])

        if additional_args:
            cmd.extend(additional_args)

        # # clean
        # cmd.extend(
        #     [   "--noinclude-numba-mode=nofollow", #Prevents the inclusion of the numba library and its dependencies, reducing the executable size.
        #         "--noinclude-dask-mode=nofollow",#Excludes the dask library
        #         "--noinclude-IPython-mode=nofollow",#Excludes the IPython library and its dependencies.
        #         "--noinclude-unittest-mode=nofollow",#Excludes the unittest module (used for testing) from the build
        #         "--noinclude-pytest-mode=nofollow",#Excludes the pytest library (used for testing) from the build.
        #         "--noinclude-setuptools-mode=nofollow",#Excludes setuptools, which is not needed for the standalone executable.
        #         "--lto=no",#Disables Link-Time Optimization (LTO), which reduces the compilation time but may slightly increase the size of the output.
        #     ]
        # )

        if clean_build:
            cmd.append(
                "--remove-output"
            )  # Removes intermediate files created during the build process, keeping only the final executable.
        # Add the script path (final positional argument)
        cmd.append(str(script_path))
        # Ensure Windows shell compatibility
        shell_flag = sys.platform.startswith("win")
        print(f"Running command: ‚§µ \n{' '.join(cmd)}\n")
        try:
            result = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                shell=shell_flag,
                check=True,
            )
            if verbose:
                print(result.stdout)
        except subprocess.CalledProcessError as e:
            print(f"Error during packaging:\n{e.stderr}", file=sys.stderr)
            print(" ".join(cmd))
            raise

    print("\nPackaging complete. Check the output directory for the executable.")


def set_theme(
    context="paper",
    style="whitegrid",
    palette="deep",
    font="sans-serif",
    font_scale=1.0,
    color_codes=True,
    grid_alpha=0.5,
    grid_linewidth=0.8,
    grid_linestyle="--",
    tick_direction="out",
    # tick_length=4,
    spine_visibility=False,
    # figsize=(8, 6),
    # linewidth=2,
    dpi=100,
    rc=None,
):
    """
    to configure Seaborn theme with maximum flexibility.

    # Example Usage
    set_sns_theme(font_scale=1.2, grid_alpha=0.8, tick_direction="in", dpi=150)

        Parameters:
        - context: Plotting context ('notebook', 'paper', 'talk', 'poster')
        - style: Style of the plot ('darkgrid', 'whitegrid', 'dark', 'white', 'ticks')
        - palette: Color palette (string or list of colors)
        - font: Font family ('sans-serif', 'serif', etc.)
        - font_scale: Scaling factor for fonts
        - color_codes: Boolean, whether to use seaborn color codes
        - grid_alpha: Opacity of the grid lines
        - grid_linewidth: Thickness of grid lines
        - grid_linestyle: Style of grid lines ('-', '--', '-.', ':')
        - tick_direction: Direction of ticks ('in', 'out', 'inout')
        - tick_length: Length of ticks
        - spine_visibility: Whether to show plot spines (True/False)
        - figsize: Default figure size as tuple (width, height)
        - linewidth: Default line width for plots
        - dpi: Resolution of the figure
        - rc: Dictionary of additional rc settings
    """
    import seaborn as sns
    import matplotlib.pyplot as plt

    # Define additional rc parameters for fine-tuning
    rc_params = {
        # "axes.grid": True,
        "grid.alpha": grid_alpha,
        "grid.linewidth": grid_linewidth,
        "grid.linestyle": grid_linestyle,
        "xtick.direction": tick_direction,
        "ytick.direction": tick_direction,
        # "xtick.major.size": tick_length,
        # "ytick.major.size": tick_length,
        # "axes.linewidth": linewidth,
        # "figure.figsize": figsize,
        "figure.dpi": dpi,
        "axes.spines.top": spine_visibility,
        "axes.spines.right": spine_visibility,
        "axes.spines.bottom": spine_visibility,
        "axes.spines.left": spine_visibility,
    }

    # Merge user-provided rc settings
    if rc:
        rc_params.update(rc)

    # Apply the theme settings
    sns.set_theme(
        context=context,
        style=style,
        palette=palette,
        font=font,
        font_scale=font_scale,
        color_codes=color_codes,
        rc=rc_params,
    )



def df_wide_long(df):
    rows, columns = df.shape
    if columns > rows:
        return "Wide"
    elif rows > columns:
        return "Long"

def df2array(data: pd.DataFrame, x=None, y=None, hue=None, sort=False):
    """
    usage:
    df2array(df_numeric.melt(),x="variable",y="value")
    """
    def sort_rows_move_nan(arr, sort=False):
        # Handle edge cases where all values are NaN
        # if np.all(np.isnan(arr)):
        if np.all(is_nan(arr)):
            return arr  # Return unchanged if the entire array is NaN

        if sort:
            # Replace NaNs with a temporary large value for sorting
            temp_value = (
                np.nanmax(arr[np.isfinite(arr)]) + 1 if np.any(np.isfinite(arr)) else np.inf
            )
            arr_no_nan = np.where(np.isnan(arr), temp_value, arr)

            # Sort each row
            sorted_arr = np.sort(arr_no_nan, axis=1)

            # Move NaNs to the end
            result_arr = np.where(sorted_arr == temp_value, np.nan, sorted_arr)
        else:
            result_rows = []
            for row in arr:
                # Separate non-NaN and NaN values
                non_nan_values = row[~is_nan(row)]
                nan_count = is_nan(row).sum()
                # Create a new row with non-NaN values followed by NaNs
                new_row = np.concatenate([non_nan_values, [np.nan] * nan_count])
                result_rows.append(new_row)
            # Convert the list of rows back into a 2D NumPy array
            result_arr = np.array(result_rows)

        # Remove rows/columns that contain only NaNs
        clean_arr = result_arr[~is_nan(result_arr).all(axis=1)]
        clean_arr_ = clean_arr[:, ~is_nan(clean_arr).all(axis=0)]

        return clean_arr_
    # data = data.copy()
    # data[y] = pd.to_numeric(data[y], errors="coerce")
    # data = data.dropna(subset=[y])
    if hue is None:
        a = []
        if sort:
            cat_x = np.sort(data[x].unique().tolist()).tolist()
        else:
            cat_x = data[x].unique().tolist()
        for i, x_ in enumerate(cat_x):
            new_ = data.loc[data[x] == x_, y].to_list()
            a = padcat(a, new_, axis=0)
        return sort_rows_move_nan(a).T
    else:
        a = []
        if sort:
            cat_x = np.sort(data[x].unique().tolist()).tolist()
            cat_hue = np.sort(data[hue].unique().tolist()).tolist()
        else:
            cat_x = data[x].unique().tolist()
            cat_hue = data[hue].unique().tolist()
        for i, x_ in enumerate(cat_x):
            for j, hue_ in enumerate(cat_hue):
                new_ = data.loc[(data[x] == x_) & (data[hue] == hue_), y].to_list()
                a = padcat(a, new_, axis=0)
        return sort_rows_move_nan(a).T


def array2df(data: np.ndarray):
    df = pd.DataFrame()
    df["group"] = (
        np.tile(
            ["group" + str(i) for i in range(1, data.shape[1] + 1)], [data.shape[0], 1]
        )
        .reshape(-1, 1, order="F")[:, 0]
        .tolist()
    )
    df["value"] = data.reshape(-1, 1, order="F")
    return df


def repmat(
    x,
    reps,
    *,
    like=None,  # "numpy" | "list" | None (auto)
    broadcast=False,  # NumPy-style broadcasting
    axis=None,  # repeat along a single axis
    generator=False,  # memory-efficient generator
    per_element=False   # repeat each element individually if x is iterable
):
    """
            Extended MATLAB-like repmat for Python / NumPy.

            Parameters
            ----------
            x : any
                Scalar, string, list, tuple, dict, or np.ndarray
            reps : int or tuple of int
                Replication shape
            like : {"numpy", "list"}, optional
                Force output type
            broadcast : bool
                Enable NumPy-style broadcasting
            axis : int, optional
                Repeat along a specific axis
            generator : bool
                Return generator instead of materialized list (Python objects only)
            per_element : bool
                If True and x is a list/1D array, repeat each element individually

            Returns
            -------
            list | np.ndarray | generator
    like="numpy" | "list"
        repmat("a", (2, 2), like="list")
        # [['a', 'a'], ['a', 'a']]

        repmat([1, 2], (2, 2), like="numpy")
        # array([[1, 2, 1, 2],
        #        [1, 2, 1, 2]])
    broadcast=True (NumPy-style)
        x = np.array([[1], [2]])
        repmat(x, (1, 3), broadcast=True)
        # array([[1, 1, 1],
        #        [2, 2, 2]])
    axis keyword
        x = np.array([1, 2, 3])
        repmat(x, 3, axis=0)
        # array([1, 1, 1, 2, 2, 2, 3, 3, 3])
        repmat("a", 3, axis=0)
        # ['a', 'a', 'a']
    Memory-efficient generator
        g = repmat({"x": 1}, 10_000_000, generator=True)
        next(g)
        # {'x': 1}
    Drop-in replacement for np.tile
        # These are equivalent
        np.tile(x, (2, 3))
        repmat(x, (2, 3), like="numpy")
    repmat(["a","b"], 3, per_element=True)
    # ['a','a','a','b','b','b']

    repmat("a", (2,2), like="list")
    # [['a','a'], ['a','a']]

    g = repmat(["a","b"], 3, per_element=True, generator=True)
    list(g)
    # ['a','a','a','b','b','b']

    """ 
    if isinstance(reps, int):
        reps = (reps,)
    elif not isinstance(reps, Iterable):
        raise TypeError("reps must be int or tuple of ints")

    if like is None:
        like = "numpy" if isinstance(x, np.ndarray) else "list"
    if like not in {"numpy", "list"}:
        raise ValueError("like must be 'numpy' or 'list'")

    # -----------------------------
    # NumPy mode
    # -----------------------------
    if like == "numpy":
        arr = np.asarray(x)
        if per_element and arr.ndim == 1:
            return np.repeat(arr, reps[0])
        if axis is not None:
            reps_axis = [1] * arr.ndim
            reps_axis[axis] = reps[0] if len(reps) == 1 else reps
            return np.repeat(arr, reps_axis[axis], axis=axis)
        if broadcast:
            return np.broadcast_to(arr, tuple(r * s for r, s in zip(reps, arr.shape)))
        return np.tile(arr, reps)

    # -----------------------------
    # Python object mode
    # -----------------------------
    def _gen_per_element():
        for xi in x:
            yield from repeat(deepcopy(xi), reps[0])

    def _gen_whole():
        for _ in range(reps[0]):
            yield deepcopy(x)

    # 1D list/tuple special handling (flatten if reps is int)
    if isinstance(x, Iterable) and not isinstance(x, (str, bytes)) and len(reps) == 1:
        if per_element:
            return list(_gen_per_element()) if not generator else _gen_per_element()
        else:
            # repeat whole, then flatten
            repeated = list(_gen_whole()) if not generator else _gen_whole()
            if not generator:
                result = []
                for item in repeated:
                    result.extend(item if isinstance(item, Iterable) and not isinstance(item, (str, bytes)) else [item])
                return result
            else:
                # generator flatten
                def gen_flat():
                    for item in repeated:
                        for sub in item if isinstance(item, Iterable) and not isinstance(item, (str, bytes)) else [item]:
                            yield sub
                return gen_flat()

    # Axis handling
    if axis is not None:
        if axis != 0:
            raise NotImplementedError("axis repetition only supported for axis=0 in list mode")
        if generator:
            return repeat(deepcopy(x), reps[0])
        return [deepcopy(x) for _ in range(reps[0])]

    # Multi-dimensional build
    def _build(level):
        if level == len(reps):
            return deepcopy(x)
        return [_build(level + 1) for _ in range(reps[level])]

    if generator:
        def _gen_multi(obj, level=0):
            if level == len(reps):
                yield deepcopy(obj)
            else:
                for _ in range(reps[level]):
                    yield from _gen_multi(obj, level + 1)
        return _gen_multi(x)
    else:
        def to_list(obj):
            if isinstance(obj, Iterable) and not isinstance(obj, (str, bytes, np.ndarray)):
                return [to_list(i) for i in obj]
            return obj
        return to_list(_build(0))



def padcat(*args, fill_value=None, axis=1, align="l"):
    """
    Concatenate vectors with padding.

    Parameters:
    *args : variable number of list or 1D arrays
        Input arrays to concatenate.
    fill_value : scalar, optional
        The value to use for padding the shorter lists (default is np.nan).
    axis : int, optional
        The axis along which to concatenate (0 for rows, 1 for columns, default is 1).
    align : str, default="start"
        Alignment of arrays ("start", "end", "center")

    Returns:
    np.ndarray
        A 2D array with the input arrays concatenated along the specified axis,
        padded with fill_value where necessary.

    # Example usage:
    a = [1, 2]
    b = [1, 2, 3]
    c = [1, 2, 3, 4]
    
    # Different alignments
    result_start = padcat(a, b, c, align="start")
    result_end = padcat(a, b, c, align="end") 
    result_center = padcat(a, b, c, align="center")
    
    print("Start alignment:")
    print(result_start)
    print("End alignment:")
    print(result_end)
    print("Center alignment:")
    print(result_center)


    # ÊµãËØïÊï∞ÊçÆ
    a = [1, 2]
    b = [1, 2, 3]
    c = [1, 2, 3, 4]
    
    print("Original arrays:")
    print("a =", a)
    print("b =", b) 
    print("c =", c)
    print()
    
    # ÊµãËØï‰∏çÂêåÁöÑÂØπÈΩêÊñπÂºè
    print("Start alignment (default):")
    result_start = padcat(a, b, c, align="start")
    print(result_start)
    print()
    
    print("End alignment:")
    result_end = padcat(a, b, c, align="end")
    print(result_end)
    print()
    
    print("Center alignment:")
    result_center = padcat(a, b, c, align="center")
    print(result_center)
    print()
    
    # ÊµãËØïÂ≠óÁ¨¶‰∏≤Êï∞ÁªÑ
    print("String arrays with center alignment:")
    str1 = ["a", "b"]
    str2 = ["c", "d", "e"]
    str3 = ["f", "g", "h", "i"]
    result_str = padcat(str1, str2, str3, align="center", fill_value="")
    print(result_str)

    """

    # --- Validate parameters --- 
    left_str=["left","l","top","t","start","s"]
    right_str=["b","bottom","right","r","end","e"]
    center_str=["center","c","middle","mid"]
    align_str=left_str+center_str+right_str
    # if align not in align_str:
    #     raise ValueError("align must be {align_str}")
    align=strcmp(align,align_str)[0]
    # --- Detect if any input contains strings ---
    contains_str = False
    for arg in args:
        arr = np.asarray(arg)
        if arr.dtype.kind in {"U", "S", "O"}:
            contains_str = True
            break

    # --- Detect if fill_value contains strings ---
    def fill_value_contains_str(fv):
        if isinstance(fv, str):
            return True
        if isinstance(fv, (list, tuple, np.ndarray)):
            return any(isinstance(x, str) for x in fv)
        return False

    if fill_value_contains_str(fill_value):
        contains_str = True

    # --- Default fill_value ---
    if fill_value is None:
        fill_value = np.nan if not contains_str else None

    dtype = object if contains_str else np.float64

    # --- Process input arrays ---
    processed_arrays = []
    for arg in args:
        arr = np.asarray(arg)
        if arr.ndim == 1:
            processed_arrays.append(arr)
        elif arr.ndim == 2:
            if axis == 0:
                processed_arrays.extend(arr)
            elif axis == 1:
                processed_arrays.extend(arr.T)
        else:
            raise ValueError("Input arrays must be 1D or 2D")

    max_len = max(arr.size for arr in processed_arrays)

    # --- If fill_value is a scalar, turn into a repeating list ---
    if not isinstance(fill_value, (list, tuple, np.ndarray)):
        fill_sequence = [fill_value] * max_len
    else:
        fill_sequence = list(fill_value)
        if len(fill_sequence) < max_len:
            repeats = (max_len // len(fill_sequence)) + 1
            fill_sequence = (fill_sequence * repeats)[:max_len]

    # --- Build result ---
    if axis == 0:
        result = np.empty((len(processed_arrays), max_len), dtype=dtype)
        for i, arr in enumerate(processed_arrays):
            n = arr.size
            pad_len = max_len - n
            row = np.empty(max_len, dtype=dtype)

            if align in left_str:
                row[:n] = arr
                if pad_len > 0:
                    row[n:] = fill_sequence[:pad_len]

            elif align in right_str:
                if pad_len > 0:
                    row[:pad_len] = fill_sequence[:pad_len]
                row[pad_len:] = arr

            elif align in center_str:
                pad_before = pad_len // 2
                row[:pad_before] = fill_sequence[:pad_before]
                row[pad_before:pad_before + n] = arr
                row[pad_before + n:] = fill_sequence[pad_before:pad_before + pad_len - pad_before]

            result[i] = row

    elif axis == 1:
        result = np.empty((max_len, len(processed_arrays)), dtype=dtype)
        for i, arr in enumerate(processed_arrays):
            n = arr.size
            pad_len = max_len - n
            col = np.empty(max_len, dtype=dtype)

            if align in left_str:
                col[:n] = arr
                if pad_len > 0:
                    col[n:] = fill_sequence[:pad_len]

            elif align in right_str:
                if pad_len > 0:
                    col[:pad_len] = fill_sequence[:pad_len]
                col[pad_len:] = arr

            elif align in center_str:
                pad_before = pad_len // 2
                col[:pad_before] = fill_sequence[:pad_before]
                col[pad_before:pad_before + n] = arr
                col[pad_before + n:] = fill_sequence[pad_before:pad_before + pad_len - pad_before]

            result[:, i] = col

    else:
        raise ValueError("axis must be 0 or 1")

    return result

# ========== memory cleaner ========== 
import gc 
import psutil 
import weakref
import time
 
import tracemalloc
from collections import defaultdict

class MemoryOptimizer:
    def __init__(self, 
                 verbose: bool = True, 
                 aggressive_mode: bool = True,
                 track_leaks: bool = False,
                 max_history: int = 100):
        self.verbose = verbose
        self.aggressive_mode = aggressive_mode
        self.track_leaks = track_leaks
        self.max_history = max_history
        self.system = platform.system()
        self.process = psutil.Process(os.getpid())
        self.start_time = time.time()
        self.memory_history = []
        self.leak_tracker = None

        if track_leaks:
            self._setup_leak_tracking()
    
    def _setup_leak_tracking(self):
        self.leak_tracker = {
            'snapshots': [],
            'diff_stats': [],
            'object_types': defaultdict(int),
            'suspected_leaks': []
        }
        tracemalloc.start(25)
    
    def log(self, msg: str, level: str = "INFO"):
        if self.verbose:
            rss = self.process.memory_info().rss / (1024 ** 2)
            elapsed = time.time() - self.start_time
            caller = inspect.currentframe().f_back.f_code.co_name
            print(f"[{level}][{elapsed:.2f}s][{rss:.1f}MB][{caller}] {msg}")
    
    def collect_garbage(self, generations: List[int] = None) -> Dict[str, Any]:
        self.log("Starting deep garbage collection...")
        stats = {
            'collected': defaultdict(int),
            'garbage_cleared': 0,
            'freed_mb': 0
        }

        before_mem = self.process.memory_info().rss

        if self.aggressive_mode:
            gc.set_threshold(1, 1, 1)
            gc.set_debug(gc.DEBUG_SAVEALL)

        gens = generations if generations is not None else [2, 1, 0]
        for gen in gens:
            collected = gc.collect(gen)
            stats['collected'][f'gen_{gen}'] = collected
            self.log(f"GC Gen {gen}: Collected {collected} objects")

        stats['garbage_cleared'] = len(gc.garbage)
        gc.garbage.clear()

        self._clear_weakref_caches()

        after_mem = self.process.memory_info().rss
        stats['freed_mb'] = (before_mem - after_mem) / (1024 ** 2)

        return stats

    def _clear_weakref_caches(self):
        self.log("Clearing weak reference caches...")
        try:
            for obj in gc.get_objects():
                if isinstance(obj, weakref.WeakValueDictionary):
                    obj.clear()
        except Exception as e:
            self.log(f"Failed to clear weakref caches: {e}", "WARNING")
    
    def clear_frameworks(self) -> Dict[str, Any]:
        result = {}

        # PyTorch
        try:
            import torch
            if torch.cuda.is_available():
                self.log("Clearing PyTorch CUDA cache...")
                torch.cuda.empty_cache()
                torch.cuda.ipc_collect()
                result['pytorch'] = {
                    'cuda_cache_cleared': True,
                    'allocated_mb': torch.cuda.memory_allocated() / (1024 ** 2),
                    'cached_mb': torch.cuda.memory_reserved() / (1024 ** 2)
                }
        except Exception as e:
            self.log(f"PyTorch skipped: {e}", "WARNING")
            result['pytorch'] = {'error': str(e)}
        
        # TensorFlow
        try:
            import tensorflow as tf
            self.log("Clearing TensorFlow session...")
            tf.keras.backend.clear_session()
            result['tensorflow'] = {'session_cleared': True}
        except Exception as e:
            self.log(f"TensorFlow skipped: {e}", "WARNING")
            result['tensorflow'] = {'error': str(e)}
        
        # OpenCV
        try:
            import cv2
            self.log("Closing OpenCV windows...")
            cv2.destroyAllWindows()
            result['opencv'] = {'windows_closed': True}
        except Exception as e:
            self.log(f"OpenCV skipped: {e}", "WARNING")
            result['opencv'] = {'error': str(e)}

        # Matplotlib
        try:
            import matplotlib.pyplot as plt
            self.log("Closing matplotlib figures...")
            plt.close('all')
            result['matplotlib'] = {'figures_closed': True}
        except Exception as e:
            self.log(f"Matplotlib skipped: {e}", "WARNING")
            result['matplotlib'] = {'error': str(e)}

        # IPython
        try:
            from IPython import get_ipython
            ipython = get_ipython()
            if ipython is not None:
                self.log("Clearing IPython outputs...")
                ipython.run_line_magic('reset', '-f')
                result['ipython'] = {'outputs_cleared': True}
        except Exception as e:
            self.log(f"IPython skipped: {e}", "WARNING")
            result['ipython'] = {'error': str(e)}

        return result
 

    def profile(self, deep: bool = False) -> Dict[str, Any]:
        mem = self.process.memory_info()
        vm = psutil.virtual_memory()
        swap = psutil.swap_memory()

        profile = {
            'timestamp': time.time(),
            'process': {
                'rss_mb': mem.rss / (1024 ** 2),
                'vms_mb': mem.vms / (1024 ** 2),
            },
            'system': {
                'used_gb': vm.used / (1024 ** 3),
                'available_gb': vm.available / (1024 ** 3),
                'percent': vm.percent,
                'swap_used_gb': swap.used / (1024 ** 3),
                'swap_free_gb': swap.free / (1024 ** 3),
            },
            'gc': {
                'objects': len(gc.get_objects()),
                'garbage': len(gc.garbage),
                'thresholds': gc.get_threshold(),
            }
        }

        if deep:
            profile['deep'] = self._deep_memory_analysis()

        self.memory_history.append(profile)
        if len(self.memory_history) > self.max_history:
            self.memory_history.pop(0)

        return profile

    def _deep_memory_analysis(self) -> Dict[str, Any]:
        self.log("Performing deep memory analysis...")
        type_sizes = defaultdict(int)
        for obj in gc.get_objects():
            try:
                obj_type = type(obj).__name__
                type_sizes[obj_type] += sys.getsizeof(obj)
            except Exception:
                continue

        top_types = sorted(type_sizes.items(), key=lambda x: x[1], reverse=True)[:10]
        return {'top_object_types': top_types}
 
    
    def detect_leaks(self, min_growth_mb: float = 5.0) -> Optional[Dict[str, Any]]:
        """
        Detect potential memory leaks by comparing snapshots.
        
        Args:
            min_growth_mb: Minimum growth in MB to consider a leak
            
        Returns:
            Leak detection report or None if no leaks detected
        """
        if not self.track_leaks or len(self.memory_history) < 2:
            return None
        
        current = self.memory_history[-1]
        previous = self.memory_history[-2]
        
        growth_mb = current['process']['rss_mb'] - previous['process']['rss_mb']
        if growth_mb < min_growth_mb:
            return None
        
        leak_report = {
            'growth_mb': growth_mb,
            'time_elapsed': current['timestamp'] - previous['timestamp'],
            'suspected_causes': [],
        }
        
        # Try to identify potential causes
        if 'deep' in current and 'deep' in previous:
            current_counts = current['deep']['object_counts']
            previous_counts = previous['deep']['object_counts']
            
            for obj_type, count in current_counts.items():
                prev_count = previous_counts.get(obj_type, 0)
                if count > prev_count * 1.5 and count - prev_count > 100:
                    leak_report['suspected_causes'].append({
                        'type': obj_type,
                        'growth': count - prev_count,
                        'percent_growth': ((count - prev_count) / prev_count * 100) if prev_count else float('inf')
                    })
        
        if leak_report['suspected_causes']:
            self.leak_tracker['suspected_leaks'].append(leak_report)
            return leak_report
        
        return None
    
    def optimize(self, full: bool = True) -> Dict[str, Any]:
        """
        Perform comprehensive memory optimization.
        
        Args:
            full: Whether to perform all optimization steps
            
        Returns:
            Dictionary with optimization results
        """
        result = {
            'timestamp': time.time(),
            'before': self.profile(deep=self.track_leaks),
            'steps': {}
        }
        
        # Step 1: Garbage collection
        result['steps']['gc'] = self.collect_garbage()
        
        # Step 2: Framework-specific memory clearing
        result['steps']['frameworks'] = self.clear_frameworks()
        
        # # Step 3: System-level cache clearing
        # if full:
        #     result['steps']['system'] = self.clear_system_caches()
        
        # Step 4: Additional aggressive measures
        if self.aggressive_mode and full:
            result['steps']['aggressive'] = self._aggressive_optimizations()
        
        # Final profile and results
        result['after'] = self.profile(deep=self.track_leaks)
        
        # Calculate savings
        saved_mb = result['before']['process']['rss_mb'] - result['after']['process']['rss_mb']
        result['saved_mb'] = saved_mb
        result['saved_percent'] = (saved_mb / result['before']['process']['rss_mb']) * 100
        
        # Check for leaks if tracking enabled
        if self.track_leaks:
            leak_report = self.detect_leaks()
            if leak_report:
                result['leak_detected'] = leak_report
        
        self.log(
            f"Optimization complete: Saved {saved_mb:.2f} MB "
            f"({result['saved_percent']:.1f}% reduction)",
            "SUCCESS"
        )
        
        return result
     
    def _aggressive_optimizations(self):
        self.log("Aggressively clearing known caches...")

        errors = {}
        try:
            gc.collect()
            self.log("Basic garbage collection done.")
        except Exception as e:
            errors['gc_collect'] = str(e)

        try:
            
            _ = np.empty(0)  # trigger allocation to finalize previous arrays
        except Exception as e:
            errors['numpy'] = str(e)

        try:
            
            _ = pd.DataFrame()  # no effect but helps ensure cleanup
        except Exception as e:
            errors['pandas'] = str(e)

        return {'status': 'done', 'errors': errors}

    def memory_report(self, detailed: bool = False) -> str:
        """Generate a comprehensive memory usage report."""
        current = self.profile(deep=detailed)
        report = [
            "="*80,
            f"Memory Report (PID: {os.getpid()})",
            "="*80,
            f"Process RSS: {current['process']['rss_mb']:.1f} MB",
            f"Process VMS: {current['process']['vms_mb']:.1f} MB",
            f"System Memory Used: {current['system']['used_gb']:.1f} GB ({current['system']['percent']}%)",
            f"Available Memory: {current['system']['available_gb']:.1f} GB",
            f"Swap Used: {current['system']['swap_used_gb']:.1f} GB",
            f"GC Objects: {current['gc']['objects']:,}",
            f"GC Garbage: {current['gc']['garbage']:,}",
        ]
        
        if detailed and 'deep' in current:
            report.append("\n[Object Type Breakdown (Top 10)]")
            sorted_types = sorted(
                current['deep']['object_counts'].items(),
                key=lambda x: x[1],
                reverse=True
            )[:10]
            
            for obj_type, count in sorted_types:
                size_mb = current['deep']['estimated_sizes'].get(obj_type, 0)
                report.append(f"{obj_type}: {count:,} objects ({size_mb:.2f} MB)")
        
        if self.track_leaks and self.leak_tracker['suspected_leaks']:
            report.append("\n[Potential Memory Leaks]")
            for i, leak in enumerate(self.leak_tracker['suspected_leaks'], 1):
                report.append(
                    f"Leak {i}: +{leak['growth_mb']:.1f}MB in "
                    f"{leak['time_elapsed']:.1f}s"
                )
                for cause in leak['suspected_causes']:
                    report.append(
                        f"  - {cause['type']}: +{cause['growth']:,} "
                        f"({cause['percent_growth']:.1f}%)"
                    )
        
        return "\n".join(report)


def cleaner(
    verbose: bool = True, 
    aggressive: bool = True,
    track_leaks: bool = False,
    full_clean: bool = True,
    return_output:bool=False
) -> Dict[str, Any]:
    """
    Ultimate memory cleaning function with all optimizations.
    
    Args:
        verbose: Print detailed progress information
        aggressive: Use aggressive memory clearing techniques
        track_leaks: Enable memory leak detection
        full_clean: Perform all cleaning steps (including system-level)
    
    Returns:
        Dictionary with optimization results
    """
    optimizer = MemoryOptimizer(
        verbose=verbose,
        aggressive_mode=aggressive,
        track_leaks=track_leaks
    )
    output=optimizer.optimize(full=full_clean)
    return output if return_output else None

def _has_gpu(verbose=True):
    """check if the system has a GPU"""
    
    
    system = platform.system()
    
    if system == "Darwin":  # MacOS
        try:
            result = subprocess.run(["system_profiler", "SPDisplaysDataType"], 
                                  capture_output=True, text=True)
            if "Apple M1" in result.stdout or "Apple M2" in result.stdout:
                if verbose:
                    print("Apple Silicon GPU detected.")
                return True 
        except:
            pass
    
    elif system == "Linux":
        try:
            result = subprocess.run(["lspci", "-k"], capture_output=True, text=True)
            if "NVIDIA" in result.stdout:
                if verbose:
                    print("NVIDIA GPU detected.")
                return True
            elif "AMD" in result.stdout:
                if verbose:
                    print("AMD GPU detected.")
                return True
        except:
            pass
    
    elif system == "Windows":
        try:
            result = subprocess.run(["wmic", "path", "win32_VideoController", "get", "name"],
                                  capture_output=True, text=True)
            if "NVIDIA" in result.stdout or "AMD" in result.stdout:
                if verbose:
                    print(f"{result.stdout.strip()} GPU detected.")
                return True
        except:
            pass
    if verbose:
        print(f"No GPU detected.")
    return False
def has_gpu(verbose=True):
    """Detect if NVIDIA GPU is available by running 'nvidia-smi'."""
    if verbose:
        gpu_available = _has_gpu(verbose)
    nvidia_smi = shutil.which("nvidia-smi")
    if not nvidia_smi:
        return False
    try:
        output = subprocess.check_output([nvidia_smi], encoding="utf-8")
        return "CUDA Version" in output or "NVIDIA-SMI" in output
    except Exception:
        return False

def set_computing_device(device: str = None, lib: str = None, verbose: bool = True):
    """
    Set computing device for machine learning libraries with automatic detection.
    Usage: set_computing_device("auto", "torch")
    
    Parameters:
    - device: str or None. One of {"auto", "cpu", "gpu", "mps"} 
              "auto" selects best available device (default)
    - lib: str or list or None. One or more libraries or None (apply to all imported)
    - verbose: bool. If True, prints detailed logs
    
    Features:
    - Auto-detects best device (GPU > MPS > CPU) when device="auto" or None
    - Handles 20+ popular ML/DL libraries
    - Sets both environment variables and library-specific device contexts
    - Comprehensive error handling and warnings

    # Case 1: Auto-configure all imported libraries
    set_computing_device("auto")
    
    # Case 2: Force CPU for specific libraries
    set_computing_device("cpu", ["torch", "tensorflow"])
    
    # Case 3: Configure before importing libraries
    set_computing_device("gpu", "torch")
    import torch
    print("PyTorch device:", torch.device("cuda" if torch.cuda.is_available() else "cpu"))
    """ 
    # System-level device detection
    def detect_gpu_available():
        """Check for NVIDIA GPU via nvidia-smi"""
        nvidia_smi = shutil.which("nvidia-smi")
        if not nvidia_smi:
            return False
        try:
            output = subprocess.check_output([nvidia_smi], encoding="utf-8", stderr=subprocess.DEVNULL)
            return "CUDA Version" in output or "NVIDIA-SMI" in output
        except Exception:
            return False

    def detect_mps_available():
        """Check for Apple Silicon GPU capabilities"""
        if platform.system() != "Darwin":
            return False
        try:
            mac_version = tuple(map(int, platform.mac_ver()[0].split('.')))
            return mac_version >= (12, 3)  # MPS requires macOS 12.3+
        except:
            return False

    # Library-specific device selection
    def _auto_select_device(lib_name):
        """Select best device for specific library"""
        # First check for special cases
        if lib_name in {"sklearn", "sklearnex"}:
            return "cpu"  # CPU-only libraries
        
        # Check hardware capabilities
        gpu_available = detect_gpu_available()
        mps_available = detect_mps_available()
        
        # Library-specific logic
        if lib_name in {"torch", "pytorch", "fastai", "transformers", 
                       "detectron2", "stable-baselines3", "diffusers"}:
            try:
                import torch
                if torch.cuda.is_available():
                    return "gpu"
                elif hasattr(torch.backends, "mps") and torch.backends.mps.is_available():
                    return "mps"
            except:
                pass
        
        if lib_name in {"tensorflow", "keras", "jax", "cupy", "cuml"}:
            if gpu_available:
                return "gpu"
        
        # Default fallback
        return "cpu"
    known_libs = [
        "tensorflow",  # via CUDA/cuDNN
        "torch",  # PyTorch, GPU via CUDA
        "jax",  # GPU/TPU via XLA
        "sklearn",  # CPU only by default
        "sklearnex",  # Intel Extension for sklearn, limited GPU via oneAPI
        "cuml",  # RAPIDS: GPU-accelerated sklearn-like API
        "xgboost",  # GPU support via `tree_method='gpu_hist'`
        "lightgbm",  # GPU support via `device='gpu'`
        "catboost",  # GPU support via `task_type='GPU'`
        "cupy",  # NumPy-like GPU array support
        "numba",  # GPU kernel acceleration via @cuda.jit
        "chainer",  # Older deep learning lib with GPU support
        "onnxruntime",  # ONNX inference runtime, supports GPU via CUDA
        "mxnet",  # Deep learning framework, GPU-capable
        "openvino",  # Intel's toolkit (CPU, GPU, VPU)
        "deeplearning4j",  # Java-based but callable from Python, GPU-capable
        "keras",  # Wrapper for TensorFlow (GPU support via TF)
        "mindspore",  # Huawei framework with GPU/Ascend
        "paddlepaddle",  # Baidu's deep learning framework (GPU support)
        "fastai",  # Built on PyTorch, supports GPU
        "transformers",  # HuggingFace models, GPU via PyTorch/TF
        "detectron2",  # Facebook‚Äôs object detection lib (GPU via PyTorch)
        "stable-baselines3",  # RL framework, GPU via PyTorch
        "diffusers",  # HuggingFace diffusion models, GPU via PyTorch
    ]
    imported_libs = [lib for lib in known_libs if lib in sys.modules]

    if lib is not None:
        if not isinstance(lib, list):
            lib = [lib]
        lib = [strcmp(i.lower(), known_libs)[0] for i in lib]
    libs_to_set = lib if lib is not None else imported_libs
    libs_to_set=[l for l in libs_to_set if l in sys.modules]
 
    if not libs_to_set:
        if lib is not None:
            print(f"Nothing changed. Could not find {lib}.")
        else:
            print(f"Nothing changed. Could not find a relevant library in imported: {imported_libs}, it should be one of {known_libs}")
        return

    # Determine device setting
    if device is None or device.lower() == "auto":
        auto_device = True
        device_str = "auto"
    else:
        auto_device = False
        device_str = device.lower()

    # Print header
    if verbose:
        print(f"=>  Setting computing device to '{device_str}' for {len(libs_to_set)} libraries:=>\t|{'|'.join(libs_to_set)}|") 

    # Detect system capabilities
    gpu_available = detect_gpu_available()
    mps_available = detect_mps_available() 
    # Process each library
    for current_lib in libs_to_set:
        # Determine device for this library
        if auto_device:
            target_device = _auto_select_device(current_lib)
        else:
            target_device = device_str 
        try:
            # TensorFlow/Keras
            if current_lib in {"tensorflow", "keras"}:
                pass 
            # PyTorch and family
            elif current_lib in {"torch", "pytorch", "fastai", "transformers", 
                                "detectron2", "stable-baselines3", "diffusers"}:
                try:
                    import torch
                    # Determine device object
                    if target_device == "gpu" and torch.cuda.is_available():
                        device_obj = torch.device("cuda")
                        torch.cuda.set_device(0)
                        if verbose:
                            print(f"  ‚úÖ [{current_lib}] Using CUDA: {torch.cuda.get_device_name(0)}")
                    elif target_device == "mps" and hasattr(torch.backends, "mps") and torch.backends.mps.is_available():
                        device_obj = torch.device("mps")
                        if verbose:
                            print(f"  ‚úÖ [{current_lib}] Using Apple MPS")
                    else:
                        device_obj = torch.device("cpu")
                        if verbose:
                            print(f"  ‚úÖ [{current_lib}] Using CPU")
                    
                    # global device if possible
                    try:
                        torch.set_default_device(device_obj)
                    except:
                        torch.set_default_tensor_type(torch.FloatTensor)
                    
                    # Special handling for transformers
                    if current_lib == "transformers":
                        try:
                            from transformers import set_seed
                            set_seed(1)
                            if verbose:
                                print("  ‚úÖ [{current_lib}] transformers seed to 1")
                        except:
                            pass
                except Exception as e:
                    if verbose:
                        print(f"  ‚ö†Ô∏è [{current_lib}] PyTorch configuration error: {str(e)}")

            # JAX
            elif current_lib == "jax":
                if target_device == "cpu":
                    os.environ["JAX_PLATFORM_NAME"] = "cpu"
                    if verbose:
                        print(f"  ‚úÖ [{current_lib}] to CPU only")
                else:
                    os.environ.pop("JAX_PLATFORM_NAME", None)
                    try:
                        import jax
                        devices = jax.devices()
                        if any("gpu" in str(d).lower() for d in devices):
                            if verbose:
                                print(f"  ‚úÖ [{current_lib}] Using {len(devices)} GPU device(s)")
                        else:
                            if verbose:
                                print(f"  ‚ö†Ô∏è [{current_lib}] No GPU devices found, using CPU")
                    except Exception as e:
                        if verbose:
                            print(f"  ‚ö†Ô∏è [{current_lib}] JAX configuration error: {str(e)}")

            # Tree-based models
            elif current_lib == "xgboost":
                if target_device == "gpu":
                    try:
                        import xgboost
                        xgboost.set_config(device="cuda")
                        if verbose:
                            print(f"  ‚úÖ [{current_lib}] GPU device")
                    except Exception as e:
                        if verbose:
                            print(f"  ‚ö†Ô∏è [{current_lib}] XGBoost GPU configuration error: {str(e)}")
            
            elif current_lib == "lightgbm":
                if target_device == "gpu":
                    try:
                        import lightgbm
                        lightgbm.set_config(device="gpu")
                        if verbose:
                            print(f"  ‚úÖ [{current_lib}] GPU device")
                    except Exception as e:
                        if verbose:
                            print(f"  ‚ö†Ô∏è [{current_lib}] LightGBM GPU configuration error: {str(e)}")
            
            elif current_lib == "catboost":
                if target_device == "gpu":
                    try:
                        import catboost
                        catboost.set_config(device="gpu")
                        if verbose:
                            print(f"  ‚úÖ [{current_lib}] GPU device")
                    except Exception as e:
                        if verbose:
                            print(f"  ‚ö†Ô∏è [{current_lib}] CatBoost GPU configuration error: {str(e)}")
            
            # GPU-accelerated libraries
            elif current_lib == "cuml":
                if target_device == "gpu":
                    try:
                        import cuml
                        cuml.set_global_device_type("gpu")
                        if verbose:
                            print(f"  ‚úÖ [{current_lib}] GPU device")
                    except Exception as e:
                        if verbose:
                            print(f"  ‚ö†Ô∏è [{current_lib}] cuML GPU configuration error: {str(e)}")
            
            elif current_lib == "cupy":
                if target_device != "cpu":
                    try:
                        import cupy
                        if verbose:
                            print(f"  ‚úÖ [{current_lib}] Using GPU device")
                    except Exception as e:
                        if verbose:
                            print(f"  ‚ö†Ô∏è [{current_lib}] CuPy initialization error: {str(e)}")
            
            # CPU-only libraries
            elif current_lib in {"sklearn", "sklearnex"}:
                if verbose:
                    print(f"  ‚ÑπÔ∏è [{current_lib}] CPU-optimized by design")
            
            # Other libraries
            else:
                if verbose:
                    print(f"  ‚ÑπÔ∏è [{current_lib}] No explicit device control available")
        
        except Exception as e:
            if verbose:
                print(f"  ‚ùó Error configuring {current_lib}: {str(e)}") 


#! ======below==== FTP/sFTP: File Transfer Protocol =======
# FTP, or File Transfer Protocol, is a standard network protocol used to transfer files between a client and a server on 
# a computer network. It operates using a client-server model, establishing separate control and data connections between 
# the client and the server. FTP is commonly used for uploading and downloading files, as well as managing files and 
# directories on a remote server.
try:
    from stat import S_ISDIR
    from ftplib import FTP, error_perm # FTP downloader
    import paramiko 
    from pathlib import PurePosixPath # for remote paths
    class FtpDownloader:
        def __init__(self, 
                     host=None, 
                     user=None, 
                     password=None, 
                     port=None, 
                     protocol=None, 
                     n_cpu=None, 
                     verbose=True
                     ):
            if verbose:
                usage_str = """
    downloader = FtpDownloader(host="ftp.example.com", user="username", password="123abc", port=21)  # FTP
    downloader = FtpDownloader(host="sftp.example.com", user="username", password="123abc", port=22) # SFTP
    downloader.download("./", "./")
    """
                print(usage_str)

            if not host or not user or not password or not port:
                raise ValueError("host, user, password, and port must all be provided.")

            self.host = host
            self.user = user
            self.passwd = password
            self.port = port
            self.protocol = protocol or ('sftp' if port == 22 else 'ftp')
            self.lock = threading.Lock()
            self.n_cpu = max(4, cpu_count() - 2)
            self.use_rclone = shutil.which("rclone") is not None

        def connect(self):
            if self.protocol == 'ftp':
                print("protocol=FTP")
                ftp = FTP()
                ftp.connect(self.host, self.port)
                ftp.login(self.user, self.passwd)
                return ftp
            elif self.protocol == 'sftp':
                print("protocol=sFTP")
                transport = paramiko.Transport((self.host, self.port))
                transport.connect(username=self.user, password=self.passwd)
                sftp = paramiko.SFTPClient.from_transport(transport)
                return sftp
            else:
                raise ValueError("Unsupported protocol. Use 'ftp' or 'sftp'.")

        def _download_with_rclone(self, remote_dir, local_dir):
            """‰ΩøÁî® rclone Áõ¥Êé•‰∏ãËΩΩÊï¥‰∏™ÁõÆÂΩï"""
            print(f":rocket: Using rclone for download: {remote_dir} -> {local_dir}")
            try:
                obscured_pass = subprocess.run(
                    ["rclone", "obscure", self.passwd],
                    capture_output=True, text=True, check=True
                ).stdout.strip()
            except Exception as e:
                obscured_pass = self.passwd  # fallback 

            # ÊûÑÈÄ† rclone Ê∫ê URL
            if self.protocol in ["ftp", "sftp"]:
                remote_url = f":{self.protocol},host={self.host},user={self.user},pass={obscured_pass},port={self.port}:{remote_dir.lstrip('/')}"
            else:
                raise ValueError(f"Unsupported protocol for rclone: {self.protocol}")

            cmd = [
                "rclone", "copy",
                remote_url, local_dir,
                "--progress",
                # "--create-empty-src-dirs",  # <-- optional, can be removed if server doesn't support
                "-vv"  # verbose debug output
            ]

            print(f":gear: Running command:\n{' '.join(cmd)}\n")

            # Use subprocess.run with streaming output to see progress in real-time
            with subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True) as proc:
                for line in proc.stdout:
                    print(line, end="")  # stream rclone output to console
                proc.wait()
                if proc.returncode != 0:
                    raise RuntimeError("rclone download failed")
            print(":white_check_mark: Download completed with rclone")

        def list_files_recursive(self, conn, path):
            file_list = []
            if self.protocol == 'ftp':
                try:
                    conn.cwd(path)
                    items = conn.nlst()
                except error_perm:
                    return []
                for item in items:
                    full_path = f"{path}/{item}".replace("//", "/")
                    try:
                        conn.cwd(full_path)
                        conn.cwd("..")
                        file_list += self.list_files_recursive(conn, full_path)
                    except error_perm:
                        file_list.append(full_path)
            else:  # sftp
                try:
                    for item in conn.listdir_attr(path):
                        full_path = f"{path}/{item.filename}".replace("//", "/")
                        if S_ISDIR(item.st_mode):
                            file_list += self.list_files_recursive(conn, full_path)
                        else:
                            file_list.append(full_path)
                except IOError as e:
                    print(f"Error listing {path}: {e}")
            return file_list
        def download_file(self, conn, remote_path, base_remote="/", base_local="/"):
            if not os.path.isabs(base_local):
                base_local = os.path.abspath(base_local)

            rel_path = os.path.relpath(remote_path, base_remote).lstrip("/")
            local_path = os.path.join(base_local, rel_path)
            os.makedirs(os.path.dirname(local_path), exist_ok=True)

            try:
                remote_size = conn.size(remote_path) if self.protocol == 'ftp' else conn.stat(remote_path).st_size
            except Exception as e:
                print(f":warning: Cannot get size for {remote_path}: {e}")
                return

            if os.path.exists(local_path):
                local_size = os.path.getsize(local_path)

                if local_size > remote_size:
                    print(f":warning: Local file is larger than remote. Removing and redownloading: {local_path}")
                    os.remove(local_path)
                    local_size = 0

                elif local_size == remote_size:
                    print(f":white_check_mark: Already exists and matches size: {local_path}")
                    return

                else:
                    print(f":arrow_down: Resuming download: {local_path} ({local_size} / {remote_size} bytes)")
            else:
                local_size = 0

            mode = 'ab' if local_size > 0 else 'wb'

            with open(local_path, mode) as f:
                with tqdm(total=remote_size, unit='B', unit_scale=True,
                        desc=os.path.basename(remote_path), initial=local_size) as pbar:

                    if self.protocol == 'ftp':
                        if local_size > 0:
                            conn.sendcmd(f"REST {local_size}")

                        def callback(data):
                            f.write(data)
                            pbar.update(len(data))

                        conn.retrbinary(f"RETR {remote_path}", callback, rest=local_size)

                    else:  # sftp
                        with conn.open(remote_path, 'rb') as remote_file:
                            if local_size > 0:
                                remote_file.seek(local_size)

                            while True:
                                data = remote_file.read(32768)
                                if not data:
                                    break
                                f.write(data)
                                pbar.update(len(data))

        def download_worker(self, task_queue, base_remote, base_local):
            conn = self.connect()
            while True:
                with self.lock:
                    if not task_queue:
                        break
                    remote_file = task_queue.pop(0)
                try:
                    self.download_file(conn, remote_file, base_remote, base_local)
                except Exception as e:
                    print(f"Error downloading {remote_file}: {e}")
                    # Re-add failed file to queue for retry
                    with self.lock:
                        task_queue.append(remote_file)
            if self.protocol == 'ftp':
                conn.quit()
            else:
                conn.close()

        @decorators.Retry()
        def download(self, remote_dir="./", local_dir="./"):
            if getattr(self, "use_rclone", False):
                try:
                    self._download_with_rclone(remote_dir, local_dir)
                    return
                except Exception as e:
                    print(f"[WARN] rclone failed, fallback to paramiko/ftp: {e}")

            if not remote_dir or not local_dir:
                raise ValueError("Both remote_dir and local_dir must be specified.")

            # Convert local_dir to absolute path if it isn't already
            if not os.path.isabs(local_dir):
                local_dir = os.path.abspath(local_dir)
                print(f":information_source: Local path converted to absolute: {local_dir}")

            print(f":electric_plug: Connecting to {self.protocol.upper()}...")
            conn = self.connect()
            print(f":open_file_folder: Listing files under {remote_dir}...")
            file_list = self.list_files_recursive(conn, remote_dir)
            conn.quit() if self.protocol == 'ftp' else conn.close()
            if not file_list:
                print(":warning: No files found to download.")
                return

            print(f":inbox_tray: Total {len(file_list)} files to download.")
            task_queue = file_list.copy()

            threads = []
            for _ in range(min(self.n_cpu, len(task_queue))):
                t = threading.Thread(target=self.download_worker, args=(task_queue, remote_dir, local_dir))
                t.start()
                threads.append(t)

            for t in threads:
                t.join()

            print(":white_check_mark: All downloads complete.")
                
        def ls(self, remote_dir="./", output="df", verbose=True):
            from datetime import datetime 
            conn = self.connect()
            items = []

            def walk_ftp(path):
                try:
                    conn.cwd(path)
                    names = conn.nlst()
                except Exception:
                    return
                for name in names:
                    full_path = f"{path}/{name}".replace("//", "/")
                    try:
                        conn.cwd(full_path)
                        # It's a directory
                        items.append({
                            "name": name,
                            "path": full_path,
                            "is_dir": True,
                            "size_MB": None,
                            "modified_time": None,
                            "kind": None,
                        })
                        conn.cwd("..")
                        walk_ftp(full_path)
                    except Exception:
                        # It's a file
                        try:
                            size = conn.size(full_path)
                        except Exception:
                            size = -1
                        items.append({
                            "name": name,
                            "path": full_path,
                            "is_dir": False,
                            "size_MB": size / (1024 * 1024) if size > 0 else None,
                            "modified_time": None,
                            "kind": os.path.splitext(name)[1],
                        })

            def walk_sftp(path):
                try:
                    for item in conn.listdir_attr(path):
                        name = item.filename
                        full_path = f"{path}/{name}".replace("//", "/")
                        is_dir = S_ISDIR(item.st_mode)
                        mtime = datetime.fromtimestamp(item.st_mtime)

                        items.append({
                            "name": name,
                            "kind": None if is_dir else os.path.splitext(name)[1],
                            "size_MB": item.st_size / (1024 * 1024) if not is_dir else None,
                            "path": full_path,
                            "modified_time": mtime,
                        })

                        if is_dir:
                            walk_sftp(full_path)
                except Exception as e:
                    print(f"Error listing {path}: {e}")

            try:
                if self.protocol == "ftp":
                    walk_ftp(remote_dir)
                elif self.protocol == "sftp":
                    walk_sftp(remote_dir)
                else:
                    raise ValueError("Unsupported protocol")
            finally:
                if self.protocol == "ftp":
                    conn.quit()
                else:
                    conn.close()

            if output == "df":
                df = pd.DataFrame(items)
                if not df.empty:
                    df = df.sort_values(by="path")
                if verbose:
                    print(df)
                return df
            else:
                return items

except Exception as e:
    print(e)
#! ========above== FTP: File Transfer Protocol =======

import zipfile
import xml.etree.ElementTree as ET

def get_last_modified_by_xlsx(fpath):
    try:
        with zipfile.ZipFile(fpath, 'r') as z:
            with z.open('docProps/core.xml') as core_props:
                tree = ET.parse(core_props)
                root = tree.getroot()

                # Define namespace used in core.xml
                ns = {'cp': 'http://schemas.openxmlformats.org/package/2006/metadata/core-properties',
                      'dc': 'http://purl.org/dc/elements/1.1/',
                      'dcterms': 'http://purl.org/dc/terms/'}

                last_modified_elem = root.find('cp:lastModifiedBy', ns)
                if last_modified_elem is not None:
                    return last_modified_elem.text
    except Exception:
        pass

    return None

# ! ======================== File Watchdog  (below) ========================
try:
    import time
    import csv
    import json
    import tempfile
    import signal
    from datetime import datetime
    from openpyxl import load_workbook
    from concurrent.futures import ThreadPoolExecutor
    import atexit

    log_lock = threading.Lock()

    # global cache
    file_copy_cache = {}
    # ‰∏¥Êó∂Êñá‰ª∂Ê∏ÖÁêÜÈîÅ
    temp_cleanup_lock = threading.Lock()
    temp_files_to_cleanup = set()
    try:
        from openpyxl.utils.exceptions import InvalidFileException
    except ImportError:
        InvalidFileException = None
        
    # *Cloud service detection
    @lru_cache(maxsize=512)
    def detect_cloud_service(file_path):
        try:
            path = os.path.abspath(file_path)
            if "OneDrive" in path:
                return "OneDrive"
            if "GoogleDrive" in path or "Google Drive" in path:
                return "Google Drive"
            if "Dropbox" in path:
                return "Dropbox"
            if "iCloudDrive" in path:
                return "iCloud"
            return None
        except:
            return None

    # @atexit.register
    def cleanup_temp_files():
        """Clean up temporary files that are no longer needed"""
        global temp_files_to_cleanup
        with temp_cleanup_lock:
            files_to_remove = list(temp_files_to_cleanup)
            temp_files_to_cleanup.clear()
        
        for temp_file in files_to_remove:
            try:
                if os.path.exists(temp_file):
                    os.unlink(temp_file)
                    logging.debug(f"Cleaned up temp file: {temp_file}")
            except Exception as e:
                logging.warning(f"Failed to clean up temp file {temp_file}: {e}")

    def register_temp_file(temp_file_path):
        """Register a temporary file for later cleanup"""
        global temp_files_to_cleanup
        with temp_cleanup_lock:
            temp_files_to_cleanup.add(temp_file_path)
        
    # *Process-based user verification (for active editors)
    def get_active_editor(file_path):
        try:
            for proc in psutil.process_iter(['name', 'username', 'open_files']):
                try:
                    if proc.info['open_files']:
                        for f in proc.info['open_files']:
                            if f.path == os.path.abspath(file_path):
                                return proc.info['username']
                except (psutil.NoSuchProcess, psutil.AccessDenied):
                    continue
            return None
        except Exception as e:
            logging.debug(f"Process scan failed: {e}")
            return None

    @lru_cache(maxsize=100)
    def get_user_info_cached(file_path, handler_str, current_mtime):
        """
        Cached version of get_user_info. Use file_path, handler_str and current_mtime as key.
        handler_str is the string representation of the handler class to avoid unhashable type.
        """
        # Ë∞ÉÁî®get_user_infoÈÄªËæëÔºå‰ΩÜ‰∏çÂåÖÊã¨ÁºìÂ≠òÂ§ÑÁêÜ
        # Ê≥®ÊÑèÔºöËøô‰∏™ÂáΩÊï∞ÂÜÖÈÉ®‰∏çÂ∫îËØ•ÂÜç‰ΩøÁî®ÁºìÂ≠òÔºåÂê¶Âàô‰ºöÂØºËá¥ÈÄíÂΩíÁºìÂ≠ò
        # Âõ†Ê≠§ÔºåÊàë‰ª¨Â∞ÜÂéüget_user_infoÂáΩÊï∞ÊîπÂêç‰∏∫_get_user_infoÔºåÂπ∂ËÆ©get_user_infoË∞ÉÁî®Ëøô‰∏™ÁºìÂ≠òÁâàÊú¨
        return _get_user_info(file_path, handler_str, current_mtime)

    def _get_user_info(file_path, handler_str, current_mtime):
        # Âéüget_user_infoÁöÑÈÄªËæëÔºå‰ΩÜ‰∏çÂåÖÊã¨ÁºìÂ≠òÊ£ÄÊü•
        # Ê≥®ÊÑèÔºöhandler_strÊòØÂ≠óÁ¨¶‰∏≤ÔºåÈúÄË¶ÅËΩ¨Êç¢‰∏∫ handler Á±ª
        if handler_str == 'ExcelHandler':
            handler = ExcelHandler
        elif handler_str == 'CSVHandler':
            handler = CSVHandler
        else:
            handler = None

        # ÂàõÂª∫‰∏¥Êó∂Êñá‰ª∂Âπ∂ÊèêÂèñÁî®Êà∑‰ø°ÊÅØ
        tmp = tempfile.NamedTemporaryFile(suffix=os.path.splitext(file_path)[1], delete=False)
        tmp.close()
        shutil.copy2(file_path, tmp.name) 
        user_info = {
            "os": fowner(file_path),
            "cloud": detect_cloud_service(file_path),
            "resolved_user": None
        }

        active_editor = get_active_editor(file_path)
        if active_editor:
            user_info["active"] = active_editor
        ext = file_path.lower().split('.')[-1]
        if handler == ExcelHandler or (handler is None and ext in ['.xlsx', '.xls']):
            try:            
                # Try the fast method for .xlsx files only
                if ext == 'xlsx':
                    last_modified_by = get_last_modified_by_xlsx(tmp.name)
                else:
                    last_modified_by = None  # For .xls fallback to None or use openpyxl fallback below

                # If fast method failed or .xls file, fallback to openpyxl (might be slower)
                if not last_modified_by and ext == 'xls':
                    wb = load_workbook(tmp.name, read_only=True)
                    last_modified_by = wb.properties.last_modified_by
                    wb.close()
                user_info["excel"] = last_modified_by or "Unknown"
                print(f"last_modified_by:{last_modified_by}")
            except Exception as e:
                print("=== Exception caught during Excel metadata extraction ===")
                traceback.print_exc()
                print(f"Error processing {file_path}: {str(e)}")
                user_info["excel"] = "Unknown"

        elif handler == CSVHandler or (handler is None and file_path.lower().endswith('.csv')):
            pass

        if 'active' in user_info:
            user_info["resolved_user"] = user_info['active']
        elif 'excel' in user_info:
            user_info["resolved_user"] = user_info['excel']
        elif user_info.get('cloud'):
            user_info["resolved_user"] = user_info['cloud']
        else:
            user_info["resolved_user"] = user_info['os']

        # Âà†Èô§‰∏¥Êó∂Êñá‰ª∂
        try:
            os.unlink(tmp.name)
        except Exception:
            pass

        return user_info

    def get_user_info(file_path, handler=None):
        current_mtime = os.path.getmtime(file_path)
        handler_str = handler.__name__ if handler else 'None'
        return get_user_info_cached(file_path, handler_str, current_mtime)

    def log_event(filepath, user_info, file, location, change_type, detail, old, new, retries=1, delay=1):
        """Enhanced logging with multi-source user verification"""
        max_log_size = 3 * 1024 * 1024  # 3 MB
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        # Resolve best user identification
        resolved_user = user_info.get('resolved_user', 'Unknown')
        sources = {k: v for k, v in user_info.items() if k != 'resolved_user'}

        # Write to CSV
        def write_log_line():
            # Prepare the new row data first
            new_row = [
                timestamp,
                resolved_user,
                json.dumps(sources),  # Store full source info
                file,
                location,
                change_type,
                detail,
                str(old),
                str(new)
            ]

            # Convert new row to CSV formatted string for comparison
            import io
            import csv as csv_lib
            output = io.StringIO()
            writer = csv_lib.writer(output)
            writer.writerow(new_row)
            new_line_str = output.getvalue()

            # Check last line in the log file
            if os.path.exists(filepath):
                try:
                    with log_lock:
                        with open(filepath, "rb") as f:
                            # Move pointer near the end for efficient last line read
                            f.seek(-1024, os.SEEK_END)  # Read last 1024 bytes or so
                            lines = f.readlines()
                            if lines:
                                last_line_bytes = lines[-1].strip(b'\r\n')
                                last_line_str = last_line_bytes.decode('utf-8', errors='ignore') + "\n"
                                if last_line_str == new_line_str:
                                    # Lines are identical, skip writing
                                    return True
                except OSError:
                    # File is probably smaller than 1024 bytes, fallback to normal read
                    with open(filepath, "r", encoding="utf-8", errors="ignore") as f:
                        lines = f.readlines()
                        if lines and lines[-1].strip() == new_line_str.strip():
                            return True

            # If reached here, lines differ or no file, write new line
            for attempt in range(retries):
                try:
                    with log_lock:  # Lock to prevent concurrent writes
                        with open(filepath, "a", newline="", encoding="utf-8", errors="ignore") as f:
                            writer = csv.writer(f)
                            writer.writerow(new_row)
                    return True
                except (OSError, IOError) as e:
                    logging.info(e)
                    if attempt < retries - 1:
                        logging.info(f"Log write attempt: {attempt+1}/{retries}")
                        time.sleep(delay)
                    else:
                        logging.error(f"Log write failed: {e}")
                        return False

        # Rotate log if needed
        if os.path.exists(filepath) and os.path.getsize(filepath) > max_log_size:
            try:
                with log_lock:  # Ê∑ªÂä†ÈîÅ‰øùÊä§
                    timestamp_rotate = datetime.now().strftime("%Y%m%d_%H%M%S")
                    rotated_name = f"{os.path.splitext(filepath)[0]}_{timestamp_rotate}.csv"
                    os.rename(filepath, rotated_name)
                    logging.info(f"Rotated log: {rotated_name}")
            except Exception as e:
                logging.error(f"Log rotation failed: {e}")
        
        # Initialize log if needed
        if not os.path.exists(filepath):
            with log_lock:  # Ê∑ªÂä†ÈîÅ‰øùÊä§
                init_dir_save(filepath)
        
        # Write log entry
        if write_log_line():
            # Format display filename
            filename = os.path.basename(file)
            if len(filename) > 15:
                filename = f"{filename[:5]}..{filename[-10:]}"
            # Generate human-readable change info
            change_info = ""
            if change_type in ("cell_modified", "cell_added", "cell_deleted"):
                change_info = f"{location}@{detail} | '{old}' ‚Üí '{new}'"
            elif change_type == "summary_change":
                change_info = f"{detail}: {new}"
            else:
                change_info = f"{detail}"
            logger = logging.getLogger("FileWatchdog")
            logger.info(f"{json.dumps(sources)} @ {timestamp} | {change_type.upper()} | {filename} | {change_info}")


    def init_dir_save(logfile):
        """Ensure log file has the correct 9-column header"""
        new_header = [
            "timestamp", 
            "user", 
            "user_sources", 
            "file", 
            "location", 
            "change_type", 
            "details", 
            "old_value", 
            "new_value"
        ]
        
        os.makedirs(os.path.dirname(logfile), exist_ok=True)
        
        # Create new file with correct header if doesn't exist
        if not os.path.exists(logfile):
            with open(logfile, "w", newline="") as f:
                csv.writer(f).writerow(new_header)
            return

        # Read existing content
        with open(logfile, "r", newline="", encoding="utf-8") as f:
            rows = list(csv.reader(f))
            
        # Always rewrite the first row with new header
        if rows:
            rows[0] = new_header  # Replace existing header
        else:
            rows = [new_header]  # Add header to empty file
        
        # Write back with corrected header
        with open(logfile, "w", newline="", encoding="utf-8") as f:
            writer = csv.writer(f)
            writer.writerows(rows)
    @lru_cache(maxsize=512)
    def col_index_to_letter(index):
        """Convert a 0-based column index to Excel column letters (A, B, ..., Z, AA, AB...)."""
        result = ''
        while index >= 0:
            index, remainder = divmod(index, 26)
            result = chr(65 + remainder) + result
            index -= 1
        return result

    class ExcelHandler:
        MONTH_TRANSLATIONS = {
            "January": "Januar", "February": "Februar", "March": "M√§rz", 
            "April": "April", "May": "Mai", "June": "Juni", "July": "Juli",
            "August": "August", "September": "September", "October": "Oktober",
            "November": "November", "December": "Dezember"
        }
        
        @staticmethod
        def detect_structural_changes(prev_data, curr_data):
            """Detect row/column insertions/deletions using prefix/suffix matching."""
            prev_row_count = len(prev_data)
            curr_row_count = len(curr_data)
            prefix = 0
            suffix = 0
            
            # Find common prefix
            min_row = min(prev_row_count, curr_row_count)
            for i in range(min_row):
                if prev_data[i] != curr_data[i]:
                    break
                prefix = i + 1
            
            # Find common suffix
            for i in range(1, min_row - prefix + 1):
                if prev_data[prev_row_count - i] != curr_data[curr_row_count - i]:
                    break
                suffix = i
            
            # Calculate middle differences
            prev_middle = prev_data[prefix:prev_row_count - suffix]
            curr_middle = curr_data[prefix:curr_row_count - suffix]
            row_diff = len(curr_middle) - len(prev_middle)
            
            # Determine row changes
            row_ins_range = None
            row_del_range = None
            if row_diff > 0:
                row_ins_range = f"{prefix+1}-{prefix+row_diff}"
            elif row_diff < 0:
                row_del_range = f"{prefix+1}-{prefix+abs(row_diff)}"
            
            # Detect column changes
            col_ins_idx = None
            col_del_idx = None
            if prev_row_count > 0 and curr_row_count > 0:
                max_cols_prev = max(len(row) for row in prev_data)
                max_cols_curr = max(len(row) for row in curr_data)
                
                # Find first differing column
                for col_idx in range(min(max_cols_prev, max_cols_curr)):
                    col_diff = False
                    for row_idx in range(min(prev_row_count, curr_row_count)):
                        if row_idx < prefix or row_idx >= prev_row_count - suffix:
                            continue
                        prev_val = prev_data[row_idx][col_idx] if col_idx < len(prev_data[row_idx]) else None
                        curr_val = curr_data[row_idx][col_idx] if col_idx < len(curr_data[row_idx]) else None
                        if prev_val != curr_val:
                            col_diff = True
                            break
                    if col_diff:
                        if max_cols_curr > max_cols_prev:
                            col_ins_idx = col_idx
                        elif max_cols_curr < max_cols_prev:
                            col_del_idx = col_idx
                        break
            
            return row_ins_range, row_del_range, col_ins_idx, col_del_idx

        @staticmethod
        def adjust_snapshot(prev_data, row_ins_range=None, row_del_range=None, col_ins_idx=None, col_del_idx=None):
            """Align previous snapshot with current structure."""
            adjusted = [row[:] for row in prev_data]  # Deep copy
            
            # Apply row deletions
            if row_del_range:
                start, end = map(int, row_del_range.split('-'))
                start = max(0, start - 1)
                end = min(len(adjusted), end)

                del adjusted[start-1:end]
            
            # Apply row insertions
            if row_ins_range:
                start, end = map(int, row_ins_range.split('-'))
                num_rows = end - start + 1
                num_cols = max(len(row) for row in adjusted) if adjusted else 0
                for _ in range(num_rows):
                    adjusted.insert(start-1, [None] * num_cols)
            
            # Apply column deletions
            if col_del_idx is not None:
                for row in adjusted:
                    if len(row) > col_del_idx:
                        del row[col_del_idx]
            
            # Apply column insertions
            if col_ins_idx is not None:
                for row in adjusted:
                    if len(row) > col_ins_idx:
                        row.insert(col_ins_idx, None)
            return adjusted

        @staticmethod
        def load_snapshot(file_path, max_retries=2, retry_delay=1):
            global temp_files_to_cleanup
            snapshot, user_sources = {}, {"os": fowner(file_path)}
            cloud_service = detect_cloud_service(file_path)
            if cloud_service:
                user_sources["cloud"] = cloud_service
            retries = 0

            while retries < max_retries:
                if not os.path.exists(file_path):
                    return {}, user_sources
                try:
                    with tempfile.NamedTemporaryFile(suffix=".xlsx", delete=False) as tmp:
                        tmp_path = tmp.name
                        shutil.copy2(file_path, tmp_path)
                    # Ê≥®ÂÜå‰∏¥Êó∂Êñá‰ª∂Áî®‰∫éÊ∏ÖÁêÜ
                    register_temp_file(tmp_path)
                    # ÊîπËøõÁöÑÊï∞ÊçÆÈÄèËßÜË°®Ê£ÄÊµãÊñπÊ≥ï
                    def has_pivot_table(sheet):
                        """Êõ¥ÂèØÈù†ÁöÑÊñπÊ≥ïÊ£ÄÊµãÂ∑•‰ΩúË°®ÊòØÂê¶ÂåÖÂê´Êï∞ÊçÆÈÄèËßÜË°®"""
                        try:
                            # ÊñπÊ≥ï1: Ê£ÄÊü•ÂÖ≥Á≥ª‰∏≠ÁöÑpivotTableÂºïÁî®
                            for rel in sheet.rels:
                                if "pivotTable" in sheet.rels[rel].Target:
                                    return True
                            # ÊñπÊ≥ï2: Ê£ÄÊü•pivotTablesÂ±ûÊÄß
                            if hasattr(sheet, "pivot_tables") and sheet.pivot_tables:
                                return True
                            # ÊñπÊ≥ï3: Ê£ÄÊü•XML‰∏≠ÊòØÂê¶ÂåÖÂê´pivotTableÊ†áÁ≠æ
                            xml = sheet._chartsheet if sheet.sheet_type == "chartSheet" else sheet._worksheet
                            if "<pivotTable" in xml.xml:
                                return True
                        except Exception:
                            pass
                        return False
                    wb = load_workbook(tmp_path, data_only=True, read_only=True)
                    excel_user = wb.properties.last_modified_by or "Unknown"
                    user_sources["excel"] = excel_user
                    
                    # Active editor takes precedence
                    active_user = get_active_editor(file_path)
                    if active_user:
                        user_sources["active"] = active_user
                    
                    # Resolve best user identifier
                    if 'active' in user_sources:
                        user_sources["resolved_user"] = user_sources["active"]
                    elif cloud_service and 'excel' in user_sources:
                        user_sources["resolved_user"] = user_sources["excel"]
                    else:
                        user_sources["resolved_user"] = user_sources["os"]

                    for sheet_name in wb.sheetnames:
                        sheet = wb[sheet_name]
                        data = []
                        for row in sheet.iter_rows(values_only=True):
                            data.append(list(row))
                            
                        has_pivot = has_pivot_table(sheet)
                        snapshot[sheet_name] = {"data": data, "has_pivot": has_pivot}
                    wb.close()
                    return snapshot, user_sources
                except Exception as e:
                    if (InvalidFileException is not None and isinstance(e, InvalidFileException)) or "is not a" in str(e):
                        logging.error(f"Unsupported file format: {e}")
                        with temp_cleanup_lock:
                            temp_files_to_cleanup.add(file_path)
                        return {}, user_sources
                    else:
                        logging.warning(f"Retrying ({retries+1}/{max_retries}): {e}")
                        time.sleep(min(20, retry_delay * (2 ** retries)))
                        retries += 1
                finally:
                    try:
                        cleanup_temp_files()
                    except Exception:
                        pass
            return {}, user_sources

        @staticmethod
        def detect_changes(prev, curr, file, user, logfile, pivot_flags=None, thr_changes=50):
            pivot_flags = pivot_flags or {}
            basename = os.path.basename(file)
            prev_sheets = set(prev.keys())
            curr_sheets = set(curr.keys())

            # Sheet added/removed
            for removed_sheet in prev_sheets - curr_sheets:
                log_event(logfile, user, basename, removed_sheet, "sheet_removed", "", "", "")
            for added_sheet in curr_sheets - prev_sheets:
                log_event(logfile, user, basename, added_sheet, "sheet_added", "", "", "")

            # Sheet content changes
            for sheet in prev_sheets & curr_sheets:
                prev_data = prev[sheet].get("data", [])
                curr_data = curr[sheet].get("data", [])
                pivot_flag = pivot_flags.get(sheet, False)
                # Log pivot update if detected
                if pivot_flag:
                    logging.getLogger("FileWatchdog").info(
                    f"skipping pivot sheet: {sheet} (File {os.path.basename(file)})")
                    continue  # Skip logging pivot updates entirely
                # Detect structural changes
                row_ins_range, row_del_range, col_ins_idx, col_del_idx = ExcelHandler.detect_structural_changes(
                    prev_data, curr_data
                )
                # Adjust previous snapshot to match current structure
                adjusted_prev = ExcelHandler.adjust_snapshot(
                    prev_data, row_ins_range, row_del_range, col_ins_idx, col_del_idx
                )
                # Log structural changes
                if row_ins_range:
                    log_event(logfile, user, basename, sheet, "row_inserted", row_ins_range, "", "")
                if row_del_range:
                    log_event(logfile, user, basename, sheet, "row_deleted", row_del_range, "", "")
                if col_ins_idx is not None:
                    col_letter = col_index_to_letter(col_ins_idx)
                    log_event(logfile, user, basename, sheet, "column_inserted", col_letter, "", "")
                if col_del_idx is not None:
                    col_letter = col_index_to_letter(col_del_idx)
                    log_event(logfile, user, basename, sheet, "column_deleted", col_letter, "", "")
                cell_changes = []
                # Compare cell values
                for row_idx in range(len(curr_data)):
                    curr_row = curr_data[row_idx]
                    prev_row = adjusted_prev[row_idx] if row_idx < len(adjusted_prev) else []
                    
                    for col_idx in range(len(curr_row)):
                        curr_val = curr_row[col_idx]
                        prev_val = prev_row[col_idx] if col_idx < len(prev_row) else None
                        
                        if prev_val == curr_val:
                            continue
                        # Handle month translations
                        if (
                            isinstance(prev_val, str)
                            and isinstance(curr_val, str)
                            and prev_val in ExcelHandler.MONTH_TRANSLATIONS
                            and ExcelHandler.MONTH_TRANSLATIONS[prev_val] == curr_val
                        ):
                            continue
                        cell = f"{col_index_to_letter(col_idx)}{row_idx + 1}"
                        if prev_val is None and curr_val is not None:
                            change_type = "cell_added"
                        elif prev_val is not None and curr_val is None:
                            change_type = "cell_deleted"
                        else:
                            change_type = "cell_modified"
                        cell_changes.append((change_type, cell, prev_val, curr_val))
                def log_summary(logfile, user, file, sheet, changes):
                    # ËÆ°ÁÆóÊ±áÊÄª
                    mod_count = sum(1 for x in changes if x[0] == "cell_modified")
                    add_count = sum(1 for x in changes if x[0] == "cell_added")
                    del_count = sum(1 for x in changes if x[0] == "cell_deleted")
                    summary = f"{mod_count} modified, {add_count} added, {del_count} deleted cells"
                    if mod_count + add_count + del_count > 0:
                        log_event(logfile, user, file, sheet, "summary_change", "diff-summary", "", summary)
                if len(cell_changes) > thr_changes:
                    # ‰ΩøÁî®Á∫øÁ®ãÊ±†ÂºÇÊ≠•Â§ÑÁêÜÂ§ßÊó•Âøó
                    with ThreadPoolExecutor(max_workers=1) as executor:
                        executor.submit(log_summary, logfile, user, basename, sheet, cell_changes)
                else:
                    for change_type, cell, prev_val, curr_val in cell_changes:
                        log_event(logfile, user, basename, sheet, change_type, cell, prev_val, curr_val)

    class CSVHandler:
        @staticmethod
        def detect_structural_changes(prev_data, curr_data):
            """Detect row/column changes in CSV data."""
            prev_row_count = len(prev_data)
            curr_row_count = len(curr_data)
            prefix = 0
            suffix = 0
            
            # Find common prefix
            min_row = min(prev_row_count, curr_row_count)
            for i in range(min_row):
                if prev_data[i] != curr_data[i]:
                    break
                prefix = i + 1
            
            # Find common suffix
            for i in range(1, min_row - prefix + 1):
                if prev_data[prev_row_count - i] != curr_data[curr_row_count - i]:
                    break
                suffix = i
            
            # Calculate middle differences
            prev_middle = prev_data[prefix:prev_row_count - suffix]
            curr_middle = curr_data[prefix:curr_row_count - suffix]
            row_diff = len(curr_middle) - len(prev_middle)
            
            # Determine row changes
            row_ins_range = None
            row_del_range = None
            if row_diff > 0:
                row_ins_range = f"{prefix+1}-{prefix+row_diff}"
            elif row_diff < 0:
                row_del_range = f"{prefix+1}-{prefix+abs(row_diff)}"
            
            # Detect column changes
            col_ins_idx = None
            col_del_idx = None
            if prev_row_count > 0 and curr_row_count > 0:
                max_cols_prev = max(len(row) for row in prev_data)
                max_cols_curr = max(len(row) for row in curr_data)
                
                # Find first differing column
                for col_idx in range(min(max_cols_prev, max_cols_curr)):
                    col_diff = False
                    for row_idx in range(min(prev_row_count, curr_row_count)):
                        if row_idx < prefix or row_idx >= prev_row_count - suffix:
                            continue
                        prev_val = prev_data[row_idx][col_idx] if col_idx < len(prev_data[row_idx]) else None
                        curr_val = curr_data[row_idx][col_idx] if col_idx < len(curr_data[row_idx]) else None
                        if prev_val != curr_val:
                            col_diff = True
                            break
                    if col_diff:
                        if max_cols_curr > max_cols_prev:
                            col_ins_idx = col_idx
                        elif max_cols_curr < max_cols_prev:
                            col_del_idx = col_idx
                        break
            
            return row_ins_range, row_del_range, col_ins_idx, col_del_idx

        @staticmethod
        def adjust_snapshot(prev_data, row_ins_range=None, row_del_range=None, col_ins_idx=None, col_del_idx=None):
            """Align previous CSV snapshot with current structure."""
            adjusted = [row[:] for row in prev_data]  # Deep copy
            # Apply row deletions
            if row_del_range:
                start, end = map(int, row_del_range.split('-'))
                del adjusted[start-1:end]
            # Apply row insertions
            if row_ins_range:
                start, end = map(int, row_ins_range.split('-'))
                num_rows = end - start + 1
                num_cols = max(len(row) for row in adjusted) if adjusted else 0
                for _ in range(num_rows):
                    adjusted.insert(start-1, [None] * num_cols)
            # Apply column deletions
            if col_del_idx is not None:
                for row in adjusted:
                    if len(row) > col_del_idx:
                        del row[col_del_idx]
            # Apply column insertions
            if col_ins_idx is not None:
                for row in adjusted:
                    if len(row) > col_ins_idx:
                        row.insert(col_ins_idx, None)
            return adjusted

        @staticmethod
        def load_snapshot(file_path, max_retries=5, retry_delay=2):
            global file_blacklist_watchdog
            snapshot = {}
            user_sources = {
                "os": fowner(file_path),
                "resolved_user": fowner(file_path)
            }
            
            cloud_service = detect_cloud_service(file_path)
            if cloud_service:
                user_sources["cloud"] = cloud_service

            retries = 0
            while retries < max_retries:
                if not os.path.exists(file_path):
                    return {}, user_sources
                try:
                    with tempfile.NamedTemporaryFile(suffix=".csv", delete=False) as tmp:
                        tmp_path = tmp.name
                        shutil.copy2(file_path, tmp_path)
                    # Ê≥®ÂÜå‰∏¥Êó∂Êñá‰ª∂Áî®‰∫éÊ∏ÖÁêÜ
                    register_temp_file(tmp_path)
                    with open(tmp_path, 'r', newline='', encoding='utf-8') as f:
                        reader = csv.reader(f)
                        snapshot = {'data': list(reader)}
                    os.remove(tmp_path)
                    return snapshot, user_sources
                except Exception as e:
                    logging.warning(f"Retrying ({retries + 1}): Failed to load CSV: {e}")
                    time.sleep(min(10, retry_delay * (2 ** retries)))
                    retries += 1
                finally:
                    try:
                        # Ê∏ÖÁêÜ‰∏¥Êó∂Êñá‰ª∂
                        cleanup_temp_files()
                    except Exception:
                        pass
            return {}, user_sources 
        @staticmethod
        def detect_changes(prev, curr, file, user, logfile, pivot_flags=None, thr_changes=50):
            pivot_flags = pivot_flags or {}
            basename = os.path.basename(file)
            sheet = 'data'
            prev_data = prev.get(sheet, [])
            curr_data = curr.get(sheet, [])
            
            pivot_flag = pivot_flags.get(sheet, False)
            if pivot_flag:
                logging.getLogger("FileWatchdog").info(
                    f"skipping pivot-like sheet: {sheet} (File {os.path.basename(file)})")
                return

            # Detect structural changes
            row_ins_range, row_del_range, col_ins_idx, col_del_idx = CSVHandler.detect_structural_changes(prev_data, curr_data)

            # Adjust previous snapshot to match current structure
            adjusted_prev = CSVHandler.adjust_snapshot(prev_data, row_ins_range, row_del_range, col_ins_idx, col_del_idx)

            # Log structural changes
            if row_ins_range:
                log_event(logfile, user, basename, sheet, "row_inserted", row_ins_range, "", "")
            if row_del_range:
                log_event(logfile, user, basename, sheet, "row_deleted", row_del_range, "", "")
            if col_ins_idx is not None:
                col_letter = col_index_to_letter(col_ins_idx)
                log_event(logfile, user, basename, sheet, "column_inserted", col_letter, "", "")
            if col_del_idx is not None:
                col_letter = col_index_to_letter(col_del_idx)
                log_event(logfile, user, basename, sheet, "column_deleted", col_letter, "", "")

            cell_changes = []
            for row_idx in range(len(curr_data)):# Compare cell values
                curr_row = curr_data[row_idx]
                prev_row = adjusted_prev[row_idx] if row_idx < len(adjusted_prev) else []
                
                for col_idx in range(len(curr_row)):
                    curr_val = curr_row[col_idx]
                    prev_val = prev_row[col_idx] if col_idx < len(prev_row) else None
                    
                    if prev_val == curr_val:
                        continue

                    # Handle month translations
                    if (
                        isinstance(prev_val, str)
                        and isinstance(curr_val, str)
                        and prev_val in ExcelHandler.MONTH_TRANSLATIONS
                        and ExcelHandler.MONTH_TRANSLATIONS[prev_val] == curr_val
                    ):
                        continue

                    cell = f"{col_index_to_letter(col_idx)}{row_idx + 1}"
                    if prev_val is None and curr_val is not None:
                        change_type = "cell_added"
                    elif prev_val is not None and curr_val is None:
                        change_type = "cell_deleted"
                    else:
                        change_type = "cell_modified"
                    cell_changes.append((change_type, cell, prev_val, curr_val))

            if len(cell_changes)>thr_changes:
                mod_count = sum(1 for x in cell_changes if x[0] == "cell_modified")
                add_count = sum(1 for x in cell_changes if x[0] == "cell_added")
                del_count = sum(1 for x in cell_changes if x[0] == "cell_deleted")
                summary = f"{mod_count} modified, {add_count} added, {del_count} deleted cells"
                if mod_count + add_count + del_count > 0:
                    log_event(logfile, user, basename, sheet, "summary_change", "diff-summary", "", summary)
                    notify(f"Multiple changes: {summary}", timeout=1,title="summary_change...")
            else:
                for change_type, cell, prev_val, curr_val in cell_changes:
                    log_event(logfile, user, basename, sheet, change_type, cell, prev_val, curr_val)
                    notify(f"{user}->{curr_val}", timeout=1,title=f"{prev_val}")

    # ======== MAIN FUNCTION ========
    def fwatchdog(
        fpath,
        dir_save="file_monitor_logs",
        exclude=None,
        dir_backup=None,
        check_interval=2,
        backup_interval=3600 * 8,
        backup_keep_days=30,
        max_retries=3,
        retry_delay=2, 
        thr_changes=50,
        notification:bool= True,
        verbose=True):
        """
        Starts monitoring one or more spreadsheet files (Excel or CSV) for structural and content changes.

        The function watches the specified file(s) for changes at a given interval. It logs differences
        such as added/removed sheets (Excel), inserted/deleted rows and columns, and modified cell values.
        Optionally, it creates periodic backups and summarizes large changes.

        Args:
            fpath (str or list): Path to a file or a list of files to monitor (.xlsx, .xls, .csv).
            dir_save (str): Directory to save log files. Defaults to "file_monitor_logs".
            exclude (str | list | dict, optional): Sheets to exclude from monitoring.
                - str: exclude one sheet from all files.
                - list: exclude multiple sheets from all files.
                - dict: map file names to lists of sheets to exclude.
            dir_backup (str, optional): Directory to save file backups. If None, no backups are created.
            check_interval (int): Seconds between file checks. Default is 5 seconds.
            verbose (bool): Whether to print logs to console. Default is True.
            backup_interval (int): Time in seconds between backups. Default is 8 hours.
            backup_keep_days (int): Number of days to retain old backups. Default is 30 days.
            max_retries (int): Max number of retries when loading a file fails. Default is 5.
            retry_delay (int): Delay (in seconds) between retry attempts. Default is 10 seconds.
            thr_changes (int): Collect all the cell-level changes into a list. If the total number of changes (added + deleted + modified) exceeds 50, log a summary event. Otherwise, log each change individually.

        Functionalities:
        Multi-Source User Identification:
            Excel: last_modified_by + OS owner + cloud service + active editor
            CSV: OS owner + cloud service
            Smart resolution: active > excel > os
        Enhanced Logging:
            New CSV column for full user provenance
            Human-readable console output
            Detailed mismatch warnings
        Cross-Platform Support:
            Windows: win32security for file ownership
            Linux/macOS: pwd module
            Cloud path detection
            User Mismatch Detection:
        """
        usage_str="""
        Usages:

        Basic monitoring of a single Excel file:
        ----------------------------------------
        fwatchdog("my_file.xlsx")


        Monitor multiple files:
        ------------------------
        fwatchdog(["report1.xlsx", "data.csv", "inventory.xls"])


        Exclude specific sheet(s) across all files:
        -------------------------------------------
        fwatchdog("my_file.xlsx", exclude="Sheet1")  # Exclude one sheet by name

        fwatchdog("my_file.xlsx", exclude=["Sheet1", "Sheet2"])  # Exclude multiple sheets


        Exclude sheets for specific files only:
        ---------------------------------------
        fwatchdog(
            ["report1.xlsx", "data.xlsx"],
            exclude={
                "report1.xlsx": ["Logs", "Settings"],
                "data.xlsx": ["PivotTable", "Metadata"]
            }
        )


        With backup and logging directory:
        ----------------------------------
        fwatchdog(
            fpath="monthly_financials.xlsx",
            dir_save="logs/",
            dir_backup="backups/",
            backup_interval=3600 * 4,  # backup every 4 hours
            backup_keep_days=7         # keep backups for 7 days
        )


        Change detection interval and verbosity:
        ---------------------------------------
        fwatchdog(
            "project_data.xlsx",
            check_interval=10,   # check every 10 seconds
            verbose=False        # minimal logging
        )


        To stop the monitoring:
        -----------------------
        Create a file named `stop_monitoring.txt` in the same directory.
        The monitoring thread will detect this and exit gracefully.
        """
        print(usage_str) if verbose else None

        # Setup logging and signal handling
        logging.basicConfig(
            level=logging.DEBUG if verbose else logging.WARNING,
            format='[%(asctime)s] %(levelname)s - %(message)s',
            datefmt='%Y-%m-%d %H:%M:%S'
        )
        log = logging.getLogger("->")

        STOP_FLAG = threading.Event()
        def signal_handler(sig, frame):
            log.warning("Received termination signal, stopping monitors...")
            STOP_FLAG.set()
        signal.signal(signal.SIGINT, signal_handler)
        signal.signal(signal.SIGTERM, signal_handler)

        # Ê∑ªÂä†Á®ãÂ∫èÈÄÄÂá∫Êó∂ÁöÑÊ∏ÖÁêÜ
        def cleanup_on_exit():
            log.info("Cleaning up temporary files...")
            cleanup_temp_files()
            log.info("Cleanup completed.")
        atexit.register(cleanup_on_exit)
        if isinstance(fpath, str):
            fpath = [fpath]
        # handle exclude # Normalize exclude to a dict
        exclude_dict = {}
        if isinstance(exclude, str):
            exclude_dict = {f: [exclude] for f in fpath}
        elif isinstance(exclude, list):
            exclude_dict = {f: exclude for f in fpath}
        elif isinstance(exclude, dict):
            exclude_dict = exclude
            
        # Handler registry 
        HANDLERS = {
            '.xlsx': ExcelHandler,
            '.xls': ExcelHandler,
            '.csv': CSVHandler,
        }
        def monitor_file(file, handler):
            """Monitor file with enhanced user tracking and faster response"""
            log.info(f"Starting: {file}")
            logfile = os.path.join(dir_save, f"log_{os.path.basename(file)}.csv")
            init_dir_save(logfile)
            prev_snapshot, prev_user = handler.load_snapshot(file, max_retries, retry_delay)
            last_backup_time = time.time()
            
            # ÂàùÂßãÂåñÊñá‰ª∂‰øÆÊîπÊó∂Èó¥Ë∑üË∏™
            last_mtime = os.path.getmtime(file) if os.path.exists(file) else 0
            last_size = os.path.getsize(file) if os.path.exists(file) else 0
            # ÂìçÂ∫îÂª∂ËøüËÆ°Êï∞Âô®
            last_user_check = 0
            USER_CHECK_INTERVAL = check_interval*5 if check_interval else 10  # seconds between user checks
            while not STOP_FLAG.is_set():
                # Ê£ÄÊü•ÂÅúÊ≠¢‰ø°Âè∑
                if os.path.exists("stop_monitoring.txt"):
                    log.info(f"Stop file detected for {file}")
                    break

                time.sleep(check_interval)
                timer_start = time.time()
                # Âø´ÈÄüÊñá‰ª∂Â≠òÂú®Ê£ÄÊü•
                if not os.path.exists(file):
                    log.warning(f"File not found: {file}. Retrying...") 
                    continue

                try:
                    # Get current modification time
                    current_mtime = os.path.getmtime(file)
                    current_size = os.path.getsize(file)
                    # --- File not modified since last check ---
                    # ‰ΩøÁî®<=ÂíåÊñá‰ª∂Â§ßÂ∞è‰∏ÄËµ∑Âà§Êñ≠
                    if current_mtime <= last_mtime and current_size == last_size:
                        # Only check user changes at intervals
                        current_time = time.time()
                        if (current_time - last_user_check) > USER_CHECK_INTERVAL:
                            timer_start = time.time()
                            current_user_info = get_user_info(file, handler)
                            log.debug(f"current_user_info took {time.time() - timer_start:.2f}s")
                            # Compare resolved users
                            prev_resolved = prev_user.get("resolved_user", "Unknown")
                            curr_resolved = current_user_info.get("resolved_user", "Unknown")
                            # log.info(f"file:{file}\ncurr_resolved:{curr_resolved} \nprev_resolved:{prev_resolved}\n")
                            if curr_resolved != prev_resolved and curr_resolved!="Unknown":
                                log_event(
                                    logfile,
                                    current_user_info,
                                    os.path.basename(file),
                                    "",  # No sheet
                                    "file_opened_no_changes",
                                    "Opened without modification",
                                    prev_resolved,
                                    curr_resolved,
                                )
                                # Update user reference
                                prev_user = current_user_info
                            
                            last_user_check = current_time
                        continue  # Skip processing since no content changed
                    
                    # Êñá‰ª∂Â∑≤‰øÆÊîπÔºåÊõ¥Êñ∞ÊúÄÂêé‰øÆÊîπÊó∂Èó¥ÂíåÂ§ßÂ∞è
                    last_mtime = current_mtime
                    last_size = current_size
                    detection_time = time.time()
                    # Âä†ËΩΩÂΩìÂâçÂø´ÁÖß
                    timer_start = time.time()
                    curr_snapshot, curr_user = handler.load_snapshot(file, max_retries, retry_delay)
                    log.debug(f"load_snapshot took {time.time() - timer_start:.2f}s")
                    if not curr_snapshot:
                        log.warning(f"Failed to load snapshot for {file}. Retrying next time.")
                        continue
                    # Áî®Êà∑ÂèòÊõ¥Ë≠¶Âëä
                    if (prev_user.get('resolved_user') != curr_user.get('resolved_user') and
                        prev_user.get('os') == curr_user.get('os')):
                        log.warning(f"User mismatch: {prev_user} ‚Üí {curr_user}")
                        notify(f'User changed: {prev_user["excel"]} ‚Üí {curr_user["excel"]}", timeout=1,title="watchdog...') if notification else None
                    # Â§ÑÁêÜÊï∞ÊçÆ
                    pivot_flags = {}
                    if handler == ExcelHandler:
                        for sheet_name, sheet_data in curr_snapshot.items():
                            pivot_flags[sheet_name] = sheet_data.get("has_pivot", False)
                    # ËøáÊª§ÊéíÈô§ÁöÑÂ∑•‰ΩúË°®
                    excluded_sheets = exclude_dict.get(file, [])
                    if excluded_sheets:
                        curr_snapshot = {k: v for k, v in curr_snapshot.items() if k not in excluded_sheets}
                        prev_snapshot = {k: v for k, v in prev_snapshot.items() if k not in excluded_sheets}
                    # Ê£ÄÊµãÂèòÊõ¥
                    timer_start = time.time()
                    handler.detect_changes(prev_snapshot, curr_snapshot,file, curr_user, logfile,pivot_flags, thr_changes=thr_changes)
                    log.debug(f"detect_changes took {time.time() - timer_start:.2f}s")
                    # Êõ¥Êñ∞Âø´ÁÖß
                    prev_snapshot, prev_user = curr_snapshot, curr_user
                    # ËÆ∞ÂΩïÂ§ÑÁêÜÊó∂Èó¥
                    process_time = time.time() - detection_time
                    if process_time > 5.0:
                        log.warning(f"Processing took {process_time:.2f}s: {file}")
                    # Â§ÑÁêÜÂ§á‰ªΩ
                    current_time = time.time()
                    if dir_backup and (current_time - last_backup_time >= backup_interval):
                        try:
                            fbackup(file, backup_dir=dir_backup,
                                backup_keep_days=backup_keep_days,
                                verbose=verbose)
                            last_backup_time = current_time
                        except Exception as e:
                            log.warning(f"Backup failed: {e}")
                except Exception as e:
                    print("=== Exception caught in monitor_file ===")
                    traceback.print_exc()
                    log.error(f"Error processing {file}: {str(e)}")
                    time.sleep(5)  # Âá∫ÈîôÂêéÁ®ç‰ΩúÁ≠âÂæÖ
            log.info(f"Stopped monitoring: {file}")
 

        # Main execution
        threads = []
        for file in fpath:
            ext = os.path.splitext(file)[1].lower()
            handler = HANDLERS.get(ext)
            if not handler:
                log.warning(f"Unsupported file type: {ext} for file {file}")
                continue
            thread = threading.Thread(
                target=monitor_file,
                args=(file, handler),
                daemon=True
            )
            thread.start()
            threads.append(thread) 
        for t in threads:
            t.join()
        log.info("All monitoring threads stopped")
except Exception as e:
    logging.error(f"‚ùå Critical error: {e}\n{traceback.format_exc()}")
# ! ======================== File Watchdog  (above) ========================


# !=========== setup terminal ==================
def terminal():
    import os
    import sys
    import json
    import shutil
    import subprocess
    import platform
    import hashlib
    from pathlib import Path
    from datetime import datetime

    MARK_BEGIN = "# >>> SCIENTIFIC_SHELL_AUTOCONFIG_BEGIN >>>"
    MARK_END = "# <<< SCIENTIFIC_SHELL_AUTOCONFIG_END <<<"

    ########################################
    # ULTIMATE BASE ZSH CONFIG
    ########################################

    ZSH_BLOCK = f"""
{MARK_BEGIN}
# === OMZ SAFETY FLAGS ===
export DISABLE_AUTO_TITLE="true"
export DISABLE_UPDATE_PROMPT="true"

# === CORE BASE ENVIRONMENT ===
setopt AUTO_CD
setopt NO_NOMATCH
setopt PROMPT_SUBST
setopt INTERACTIVE_COMMENTS
setopt EXTENDED_GLOB
setopt MULTIOS

# === ENHANCED HISTORY ===
HISTSIZE=1000000
SAVEHIST=1000000
HISTFILE=~/.zsh_history
setopt SHARE_HISTORY
setopt HIST_IGNORE_DUPS
setopt HIST_REDUCE_BLANKS
setopt INC_APPEND_HISTORY
setopt HIST_IGNORE_SPACE
setopt HIST_VERIFY
setopt HIST_SAVE_NO_DUPS
setopt HIST_EXPIRE_DUPS_FIRST
setopt HIST_FIND_NO_DUPS

# === COMPLETION CONFIG===
zstyle ':completion:*' menu select
zstyle ':completion:*' matcher-list 'm:{{a-z}}={{A-Z}}' 'r:|[._-]=* r:|=*' 'l:|=* r:|=*'
zstyle ':completion:*' list-colors "${{(s.:.)LS_COLORS}}"
zstyle ':completion:*' group-name ''
zstyle ':completion:*:descriptions' format '%F{{yellow}}%d%f'
zstyle ':completion:*:*:*:*:processes' command 'ps -u $USER -o pid,user,comm,cmd -w'

# Custom completions for BASE tools
compdef _gnu_generic mlflow dvc prefect kubectl docker
compdef _python python ipython jupyter

# === SMART KEY BINDINGS ===
bindkey -e
bindkey '^[[1;5C' forward-word
bindkey '^[[1;5D' backward-word
bindkey '^U' backward-kill-line
bindkey '^[[A' history-beginning-search-backward
bindkey '^[[B' history-beginning-search-forward
bindkey '^R' history-incremental-search-backward
bindkey '^S' history-incremental-search-forward
bindkey '^T' fzf-file-widget
bindkey '^[c' fzf-cd-widget

# === FZF FOR BASE ===
export FZF_DEFAULT_OPTS="--height 50% --layout=reverse --border --preview-window=right:60%:wrap"
export FZF_DEFAULT_COMMAND="rg --files --hidden --follow --glob '!**/.git/**' --glob '!**/__pycache__/**' --glob '!**/*.ipynb_checkpoints/**' 2>/dev/null || find . -type f -not -path '*/\\.git/*' -not -path '*/__pycache__/*' -not -path '*/*.ipynb_checkpoints/*'"
export FZF_CTRL_T_COMMAND="$FZF_DEFAULT_COMMAND"
export FZF_CTRL_T_OPTS="--preview 'bat --style=numbers --color=always --line-range :500 {{}} 2>/dev/null || cat {{}}'"
export FZF_CTRL_R_OPTS="--preview 'echo {{}}' --preview-window down:3:hidden:wrap --bind '?:toggle-preview'"

# Source fzf safely
[[ -f /usr/share/fzf/key-bindings.zsh ]] && source /usr/share/fzf/key-bindings.zsh
[[ -f /opt/homebrew/opt/fzf/shell/key-bindings.zsh ]] && source /opt/homebrew/opt/fzf/shell/key-bindings.zsh
[[ -f ~/.fzf.zsh ]] && source ~/.fzf.zsh

# === ADVANCED PROMPT ===
conda_prompt() {{
    local env=""
    [[ -n "$CONDA_DEFAULT_ENV" ]] && env="$CONDA_DEFAULT_ENV"
    [[ -z "$env" && -n "$MAMBA_DEFAULT_ENV" ]] && env="$MAMBA_DEFAULT_ENV"
    [[ -n "$env" ]] && echo "%F{{magenta}}ÓòÜ $env%f "
}}

venv_prompt() {{
    [[ -n "$VIRTUAL_ENV" ]] && echo "%F{{blue}}ÓòÜ $(basename $VIRTUAL_ENV)%f "
}}

git_prompt() {{
    local branch="$(git symbolic-ref --short HEAD 2>/dev/null || git describe --tags --always 2>/dev/null)"
    if [[ -n "$branch" ]]; then
        local status="$(git status --porcelain 2>/dev/null | wc -l)"
        local color="%F{{green}}"
        [[ $status -gt 0 ]] && color="%F{{yellow}}"
        echo "$colorÛ∞ä¢ $branch%f "
    fi
}}

# Simple prompt to avoid conflicts
PROMPT='%F{{blue}}%B%n@%m%b%f $(conda_prompt)$(venv_prompt)$(git_prompt)
%F{{yellow}}%~%f %F{{green}}ÓÇ∞%f '

# === BASE ALIASES ===

# Safety aliases
alias rm="rm -i"
alias cp="cp -i"
alias mv="mv -i"
alias ln="ln -i"

# File listing with safety checks
if command -v exa >/dev/null 2>&1; then
    alias ls="exa --group-directories-first --icons"
    alias ll="exa -lh --group-directories-first --icons --git"
    alias la="exa -lha --group-directories-first --icons --git"
    alias lt="exa -lh --tree --icons --group-directories-first --level=2 --git-ignore"
    alias lta="exa -lh --tree --icons --group-directories-first --level=3"
elif [[ $(uname) == "Darwin" ]]; then
    # macOS fallback
    alias ls="ls -G"
    alias ll="ls -lhG"
    alias la="ls -lhaG"
    alias lt="ls -laG | head -20"
else
    # Linux with GNU ls
    alias ls="ls --color=auto --group-directories-first"
    alias ll="ls -lh --color=auto --group-directories-first"
    alias la="ls -lha --color=auto --group-directories-first"
    alias lt="ls -la --color=auto --group-directories-first | head -20"
fi

# Navigation - FIXED: No recursive calls
alias ..="cd .."
alias ...="cd ../.."
alias ....="cd ../../.."
alias cdd="cd ~/data"
alias cdp="cd ~/projects"
alias cdm="cd ~/models"
alias cde="cd ~/experiments"

# Git supercharged
alias g="git"
alias gs="git status --short --branch"
alias ga="git add"
alias gc="git commit"
alias gcm="git commit -m"
alias gca="git commit --amend"
alias gd="git diff"
alias gdc="git diff --cached"
alias gco="git checkout"
alias gcb="git checkout -b"
alias gb="git branch"
alias gl="git log --oneline --graph --decorate -20"
alias gla="git log --oneline --graph --decorate --all -20"
alias gp="git push"
alias gpf="git push --force-with-lease"
alias gpl="git pull --rebase"
alias gst="git stash"
alias gsp="git stash pop"
alias gr="git restore"
alias grs="git restore --staged"

# Python development
alias python="python3"
alias pip="pip3"
alias ipy="ipython"
alias jl="jupyter lab"
alias jn="jupyter notebook"
alias venv="python3 -m venv"
alias activate="source .venv/bin/activate"
alias mkenv="python3 -m venv .venv && source .venv/bin/activate && pip3 install -U pip setuptools wheel"
alias req="pip3 freeze > requirements.txt"
alias install="pip3 install -r requirements.txt"

# Conda/Mamba
alias c="conda"
alias m="mamba"
alias ca="conda activate"
alias cda="conda deactivate"
alias ma="mamba activate"
alias mda="mamba deactivate"
alias cl="conda env list"
alias ml="mamba env list"
alias ce="conda env export --no-builds > environment.yml"
alias me="mamba env export --no-builds > mamba_env.yml"
alias cc="conda create -n"
alias mc="mamba create -n"
alias ci="conda install"
alias mi="mamba install"

# Docker
alias d="docker"
alias dps="docker ps"
alias dpsa="docker ps -a"
alias dim="docker images"
alias drm="docker rm"
alias drmi="docker rmi"
alias dcu="docker-compose up"
alias dcd="docker-compose down"
alias dcb="docker-compose build"

# Data tools
alias csv="column -s, -t"
alias tsv="column -s$'\\t' -t"
alias json="python3 -m json.tool"
alias yaml="python3 -c 'import sys,yaml; yaml.safe_dump(yaml.safe_load(sys.stdin), sys.stdout, default_flow_style=False)'"
alias parquet="python3 -c 'import sys, pandas as pd; print(pd.read_parquet(sys.argv[1]).head())'"
alias feather="python3 -c 'import sys, pandas as pd; print(pd.read_feather(sys.argv[1]).head())'"

# Monitoring with safety checks
gpu() {{
    if command -v nvidia-smi >/dev/null 2>&1; then
        timeout 2 nvidia-smi
    else
        echo 'No NVIDIA GPU found or nvidia-smi not available'
    fi
}}

gpuw() {{
    if command -v nvidia-smi >/dev/null 2>&1 && command -v watch >/dev/null 2>&1; then
        watch -n 1 -t nvidia-smi
    else
        echo 'Required commands not available'
    fi
}}

if [[ $(uname) == "Darwin" ]]; then
    alias cpu="top -o cpu -l 1 -s 0 | head -20"
    alias mem="top -l 1 -s 0 | grep PhysMem"
    alias disk="df -h"
    alias ports="lsof -i -P -n | grep LISTEN | head -20"
else
    alias cpu="top -b -n 1 | head -20"
    alias mem="free -h"
    alias disk="df -h"
    alias ports="ss -tulpn | head -20"
fi
alias net="ifconfig | head -30"

# Productivity
alias t="tmux"
alias ta="tmux attach"
alias tl="tmux list-sessions"
alias v="vim"
alias nv="nvim"
alias p="pwd"
alias h="history | tail -50"
alias hg="history | grep -i"
alias ccat="pygmentize -g"

# === BASE FUNCTIONS ===

# Create project structure - FIXED: No recursive issues
mkds() {{
    local project_name="$1"
    if [[ -z "$project_name" ]]; then
        echo "Usage: mkds <project_name>"
        return 1
    fi
    
    # Prevent creating in sensitive locations
    if [[ "$project_name" == "/" ]] || [[ "$project_name" =~ \\.\\. ]]; then
        echo "Error: Invalid project name"
        return 1
    fi
    
    # Limit depth
    if [[ "$project_name" =~ ^~ ]] && [[ $(echo "$project_name" | tr -cd '/' | wc -c) -gt 5 ]]; then
        echo "Error: Path too deep"
        return 1
    fi
    
    # Create directory structure with limits
    local dirs=(
        "data/raw"
        "data/processed" 
        "data/external"
        "notebooks"
        "src/data"
        "src/models"
        "src/features"
        "src/visualization"
        "reports"
        "models"
        "experiments"
        "tests"
        "docs"
    )
    
    for dir in "${{dirs[@]}}"; do
        mkdir -p "$project_name/$dir"
    done
    
    # Create basic files
    touch "$project_name/README.md"
    touch "$project_name/requirements.txt"
    touch "$project_name/.gitignore"
    touch "$project_name/setup.py"
    touch "$project_name/notebooks/00-exploratory-test.ipynb"
    
    # Write basic .gitignore
    cat > "$project_name/.gitignore" << 'EOF'
# Data
data/raw/
data/processed/
data/external/
models/

# Python
__pycache__/
*.py[cod]
*$py.class
*.so
.Python
.env
env/
venv/
.venv/
ENV/

# Jupyter
.ipynb_checkpoints/
*.ipynb

# IDE
.vscode/
.idea/
*.swp
*.swo

# OS
.DS_Store
Thumbs.db
EOF
    
    echo "Created BASE project: $project_name"
    cd "$project_name" || return 1
}}

# Quick data profiling with safety limits
profile() {{
    local file="$1"
    if [[ -z "$file" ]]; then
        echo "Usage: profile <csv_file>"
        return 1
    fi
    
    # Check file size (limit to 100MB)
    if [[ -f "$file" ]]; then
        local filesize=$(stat -f%z "$file" 2>/dev/null || stat -c%s "$file" 2>/dev/null)
        if [[ $filesize -gt 100000000 ]]; then
            echo "Warning: File is large ($((filesize/1000000))MB). Showing first 1000 rows only."
            python3 -c "
import pandas as pd
import sys
try:
    df = pd.read_csv('$file', nrows=1000)
    print('Shape (first 1000 rows):', df.shape)
    print('\\nColumns:', list(df.columns))
    print('\\nData types:')
    print(df.dtypes)
    print('\\nMissing values:')
    print(df.isnull().sum().head(20))
    print('\\nFirst 5 rows:')
    print(df.head())
except Exception as e:
    print(f'Error: {{e}}')
"
            return
        fi
    fi
    
    python3 -c "
import pandas as pd
import sys
try:
    df = pd.read_csv('$file')
    print('Shape:', df.shape)
    print('\\nColumns:', list(df.columns))
    print('\\nData types:')
    print(df.dtypes)
    print('\\nMissing values:')
    print(df.isnull().sum())
    print('\\nFirst 5 rows:')
    print(df.head())
except Exception as e:
    print(f'Error: {{e}}')
"
}}

# Safe file search - FIXED: Prevents recursion issues
findpy() {{
    local pattern="$1"
    if [[ -z "$pattern" ]]; then
        echo "Usage: findpy <pattern>"
        return 1
    fi
    # Limit depth and exclude problematic directories
    find . -type f -name "*.py" -not -path "./.git/*" -not -path "./venv/*" -not -path "./.venv/*" -not -path "*/__pycache__/*" -exec grep -l "$pattern" {{}} + | head -50
}}

# Safe recursive grep
rgrep() {{
    local pattern="$1"
    if [[ -z "$pattern" ]]; then
        echo "Usage: rgrep <pattern>"
        return 1
    fi
    grep -r --include="*.py" --include="*.md" --include="*.txt" --exclude-dir=.git --exclude-dir=__pycache__ --exclude-dir=.venv --exclude-dir=venv "$pattern" . | head -100
}}

# Safe directory size
dsize() {{
    local dir="${{1:-.}}"
    # Limit to current directory if none specified
    if [[ "$dir" == "." ]] || [[ "$dir" == "$PWD" ]]; then
        du -sh . 2>/dev/null | cut -f1
    else
        # Prevent traversing too deep
        du -sh "$dir" 2>/dev/null | head -1 | cut -f1
    fi
}}

# === ENVIRONMENT VARIABLES ===
export EDITOR="nvim"
export VISUAL="nvim"
export PAGER="less -R"
export LESS="-R -X -F"

# Python
export PYTHONSTARTUP=~/.pythonrc.py
export PYTHONUNBUFFERED=1
export PYTHONIOENCODING=UTF-8
export PIP_REQUIRE_VIRTUALENV=false

# Jupyter
export JUPYTER_CONFIG_DIR=~/.jupyter
export JUPYTER_RUNTIME_DIR=/tmp/jupyter_runtime

# Performance
export OMP_NUM_THREADS=1
export MKL_NUM_THREADS=1
export NUMEXPR_NUM_THREADS=1

# Safety: Limit resources in shell
export SHELL_MAX_DEPTH=10
export SHELL_MAX_FILES=1000

# === SAFETY CHECKS ===
# Prevent fork bombs
ulimit -u 1000 2>/dev/null
ulimit -n 1024 2>/dev/null

# Function to check for recursive patterns
check_recursion() {{
    local depth="${{1:-0}}"
    if [[ $depth -gt 10 ]]; then
        echo "Error: Maximum recursion depth exceeded" >&2
        return 1
    fi
}}

{MARK_END}
"""

    ########################################
    # BASE BASH FALLBACK
    ########################################

    BASH_BLOCK = f"""
{MARK_BEGIN}
# === BASE BASH CONFIG ===
shopt -s autocd
shopt -s globstar
shopt -s dotglob
shopt -s histappend
shopt -s cmdhist
shopt -s checkwinsize

# Enhanced history
HISTSIZE=100000
HISTFILESIZE=200000
HISTCONTROL=ignoreboth:erasedups
HISTTIMEFORMAT="%F %T "
PROMPT_COMMAND="history -a"

# Bash completion
if ! shopt -oq posix; then
    if [ -f /usr/share/bash-completion/bash_completion ]; then
        . /usr/share/bash-completion/bash_completion
    elif [ -f /etc/bash_completion ]; then
        . /etc/bash_completion
    fi
fi

# Better TAB completion
bind 'set show-all-if-ambiguous on'
bind 'set completion-ignore-case on'

# BASE Prompt
conda_prompt() {{
    [[ -n "$CONDA_DEFAULT_ENV" ]] && echo "[conda:$CONDA_DEFAULT_ENV]"
}}

git_prompt_bash() {{
    local branch="$(git symbolic-ref --short HEAD 2>/dev/null || git describe --tags --always 2>/dev/null)"
    [[ -n "$branch" ]] && echo "[git:$branch]"
}}

PS1='\\[\\033[01;36m\\]\\u@\\h\\[\\033[00m\\] $(conda_prompt)$(git_prompt_bash)\\[\\033[01;33m\\]\\w\\[\\033[00m\\]\\n\\[\\033[01;32m\\]\\$\\[\\033[00m\\] '

# BASE Aliases
alias ll='ls -lh --group-directories-first --color=auto'
alias la='ls -lha --group-directories-first --color=auto'
alias grep='grep --color=auto'
alias fgrep='fgrep --color=auto'

# Python
alias python='python3'
alias pip='pip3'
alias ipy='ipython'
alias jl='jupyter lab'

# Git shortcuts
alias gs='git status'
alias gd='git diff'
alias gc='git commit'
alias gp='git push'
alias gl='git log --oneline --graph --decorate -10'

# Safe functions
profile() {{
    local file="$1"
    if [ -z "$file" ]; then
        echo "Usage: profile <csv_file>"
        return 1
    fi
    
    # Check file size
    if [ -f "$file" ]; then
        filesize=$(stat -c%s "$file" 2>/dev/null || stat -f%z "$file" 2>/dev/null)
        if [ $filesize -gt 10000000 ]; then
            echo "File is large. Showing first 1000 rows only."
            python3 -c "
import pandas as pd
import sys
try:
    df = pd.read_csv('$file', nrows=1000)
    print('Shape (first 1000 rows):', df.shape)
    print()
    print(df.head())
except Exception as e:
    print(f'Error: {{e}}')
"
            return
        fi
    fi
    
    python3 -c "
import pandas as pd
import sys
try:
    df = pd.read_csv('$file')
    print('Shape:', df.shape)
    print()
    print(df.head())
except Exception as e:
    print(f'Error: {{e}}')
"
}}

mkds() {{
    local project_name="$1"
    if [ -z "$project_name" ]; then
        echo "Usage: mkds <project_name>"
        return 1
    fi
    
    # Safety check
    if [ "$project_name" = "/" ] || [[ "$project_name" =~ \\.\\. ]]; then
        echo "Error: Invalid project name"
        return 1
    fi
    
    mkdir -p "$project_name"/{{data,notebooks,src,models,experiments}}
    echo "Created project: $project_name"
}}

# Safety limits
ulimit -u 1000 2>/dev/null
ulimit -n 1024 2>/dev/null

{MARK_END}
"""

    ########################################
    # CONFIGURATION FILES
    ########################################

    IPYTHON_CONFIG = """
# === IPYTHON CONFIG ===
c = get_config()

# Autoreload extensions
c.InteractiveShellApp.extensions = ['autoreload']
c.InteractiveShellApp.exec_lines = [
    '%load_ext autoreload',
    '%autoreload 2',
]

# Display settings
c.InteractiveShell.ast_node_interactivity = 'last_expr'
c.InteractiveShell.colors = 'Linux'
c.InteractiveShell.confirm_exit = False

# Magic commands
c.InteractiveShell.magic_commands = True
c.TerminalInteractiveShell.editing_mode = 'emacs'
c.TerminalInteractiveShell.true_color = True

# Autocompletion
c.IPCompleter.use_jedi = False
c.IPCompleter.greedy = True
"""

    JUPYTER_CONFIG = """
# === JUPYTER CONFIG ===
c = get_config()

# Server settings
c.ServerApp.ip = '127.0.0.1'
c.ServerApp.open_browser = False
c.ServerApp.port = 8888
c.ServerApp.token = ''

# Notebook settings
c.NotebookApp.allow_origin = '*'
c.NotebookApp.allow_root = True
c.NotebookApp.quit_button = False

# Kernel settings
c.MappingKernelManager.cull_idle_timeout = 3600
c.MappingKernelManager.cull_interval = 300
"""

    PYTHON_STARTUP = """
# === PYTHON BASE STARTUP ===
import warnings
import pandas as pd
import sys
import numpy as np
import os
from pathlib import Path
from datetime import datetime

# Suppress warnings
warnings.filterwarnings('ignore')

# Better display for DataFrames
pd.set_option('display.max_rows', 50)
pd.set_option('display.max_columns', 20)
pd.set_option('display.width', 120)
pd.set_option('display.float_format', lambda x: '%.3f' % x)

print("BASE environment loaded")
"""

    GIT_CONFIG = """
# === GIT CONFIG FOR BASE ===
[core]
    editor = nvim
[init]
    defaultBranch = main
[color]
    ui = auto
[alias]
    st = status --short --branch
    ci = commit
    co = checkout
    br = branch
    df = diff
    lg = log --color --graph --pretty=format:'%Cred%h%Creset -%C(yellow)%d%Creset %s %Cgreen(%cr) %C(bold blue)<%an>%Creset' --abbrev-commit
[pull]
    rebase = false
[push]
    default = current
"""

    ########################################
    # HELPER FUNCTIONS (FIXED)
    ########################################

    def run_cmd(cmd, check=True, capture=False):
        """Run command safely with timeout"""
        print(f"[RUN] {' '.join(cmd)}")
        try:
            if capture:
                result = subprocess.run(cmd, check=check, capture_output=True, 
                                      text=True, timeout=300)
                return result.stdout.strip()
            else:
                subprocess.run(cmd, check=check, timeout=300)
        except subprocess.TimeoutExpired:
            print(f"[ERROR] Command timed out: {' '.join(cmd)}")
            return None
        except Exception as e:
            print(f"[ERROR] Command failed: {e}")
            if not check:
                return None
            raise

    def command_exists(cmd):
        """Check if command exists"""
        return shutil.which(cmd) is not None

    def safe_write(path, content, backup=True):
        """Safely write content to file"""
        path = Path(path)
        path.parent.mkdir(parents=True, exist_ok=True)
        
        # Don't create files in system directories
        if str(path).startswith(('/', '/etc', '/usr', '/var')):
            print(f"[WARNING] Skipping system file: {path}")
            return
        
        # Check file size if exists
        if path.exists():
            try:
                if path.stat().st_size > 10 * 1024 * 1024:  # 10MB limit
                    print(f"[WARNING] File too large: {path}")
                    return
            except:
                pass
        
        if backup and path.exists():
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            backup_path = path.with_suffix(f"{path.suffix}.{timestamp}.bak")
            try:
                shutil.copy2(path, backup_path)
                print(f"[BACKUP] Created {backup_path}")
            except:
                print(f"[WARNING] Could not backup {path}")
        
        # Update or create file
        try:
            if path.exists():
                existing = path.read_text()
                if MARK_BEGIN in existing:
                    lines = existing.splitlines()
                    start, end = -1, -1
                    for i, line in enumerate(lines):
                        if MARK_BEGIN in line:
                            start = i
                        if MARK_END in line and start != -1:
                            end = i
                            break
                    if start != -1 and end != -1:
                        new_content = '\n'.join(lines[:start] + content.splitlines() + lines[end+1:])
                        path.write_text(new_content)
                        print(f"[UPDATE] Updated {path}")
                        return
            
            # Create new file
            path.write_text(content + '\n')
            print(f"[CREATE] Created {path}")
        except Exception as e:
            print(f"[ERROR] Could not write {path}: {e}")

    def install_data_science_tools():
        """Install tools with safety checks"""
        system = platform.system().lower()
        
        print("\nINSTALLING BASE TOOLS")
        
        # Python packages in small batches
        if command_exists("pip3"):
            print("\nInstalling Python packages...")
            batches = [
                ["py2ls[slim]"],
            ]
            
            for batch in batches:
                try:
                    run_cmd(["pip3", "install", "-U"] + batch, check=False, capture=True)
                    print(f"  Installed: {', '.join(batch)}")
                except:
                    print(f"  Failed to install: {', '.join(batch)}")
        
        # System packages
        print("\nSystem packages:")
        if system == "linux" and command_exists("apt"):
            try:
                run_cmd(["sudo", "apt", "update"], check=False)
                packages = ["zsh", "git", "curl", "tmux", "python3-pip", "python3-venv"]
                run_cmd(["sudo", "apt", "install", "-y"] + packages, check=False)
            except:
                print("  Skipped system packages (no sudo)")
        elif system == "darwin" and command_exists("brew"):
            packages = ["zsh", "git", "python@3.11"]
            run_cmd(["brew", "install"] + packages, check=False)

    def setup_data_science_directories():
        """Create directories safely"""
        home = Path.home()
        
        # Limit directory creation
        directories = {
            "projects": ["active", "archive"],
            "data": ["raw", "processed"],
            "notebooks": [],
        }
        
        print("\nCreating BASE directory structure:")
        for parent, subdirs in directories.items():
            parent_path = home / parent
            try:
                parent_path.mkdir(exist_ok=True)
                for subdir in subdirs:
                    (parent_path / subdir).mkdir(exist_ok=True)
                print(f"  ‚úì {parent}/")
            except Exception as e:
                print(f"  ‚úó Failed to create {parent}: {e}")

    def setup_git_config():
        """Setup git config"""
        gitconfig_path = Path.home() / ".gitconfig"
        if not gitconfig_path.exists():
            safe_write(gitconfig_path, GIT_CONFIG, backup=False)
            print("Created git config for BASE")

    def setup_jupyter():
        """Setup Jupyter safely"""
        jupyter_dir = Path.home() / ".jupyter"
        jupyter_dir.mkdir(exist_ok=True)
        
        config_path = jupyter_dir / "jupyter_server_config.py"
        safe_write(config_path, JUPYTER_CONFIG, backup=False)
        print("üìì Jupyter configured")

    def setup_ipython():
        """Setup IPython safely"""
        ipython_dir = Path.home() / ".ipython" / "profile_default"
        ipython_dir.mkdir(parents=True, exist_ok=True)
        
        config_path = ipython_dir / "ipython_config.py"
        safe_write(config_path, IPYTHON_CONFIG, backup=False)
        
        pythonrc_path = Path.home() / ".pythonrc.py"
        safe_write(pythonrc_path, PYTHON_STARTUP, backup=False)
        print("IPython configured")

    def check_data_science_tools():
        """Check installed tools"""
        print("\nCHECKING BASE TOOLKIT")
        
        tools = [
            ("python3", "Python 3"),
            ("pip3", "Python package manager"),
            ("git", "Version control"),
            ("jupyter", "Jupyter notebooks"),
        ]
        
        for cmd, desc in tools:
            if command_exists(cmd):
                print(f"  ‚úì {desc}")
            else:
                print(f"  ‚úó {desc}")

    def create_useful_scripts():
        """Create safe utility scripts"""
        bin_dir = Path.home() / ".local" / "bin"  # Use .local/bin instead of ~/bin
        bin_dir.mkdir(parents=True, exist_ok=True)
        
        scripts = {
            "ds-clean": """#!/bin/bash
# Clean up Python artifacts safely
find . -maxdepth 3 -type d -name "__pycache__" -exec rm -rf {} + 2>/dev/null || true
find . -maxdepth 3 -type d -name ".ipynb_checkpoints" -exec rm -rf {} + 2>/dev/null || true
find . -maxdepth 3 -name "*.pyc" -delete 2>/dev/null || true
echo "Cleaned up Python artifacts"
""",
            
            "csv-head": """#!/bin/bash
# Show CSV head safely
FILE="$1"
if [ -z "$FILE" ]; then
    echo "Usage: csv-head <file.csv>"
    exit 1
fi
python3 -c "
import pandas as pd
import sys
try:
    df = pd.read_csv('$FILE', nrows=10)
    print('Columns:', list(df.columns))
    print()
    print(df.head())
except Exception as e:
    print(f'Error: {e}')
"
""",
        }
        
        for name, content in scripts.items():
            script_path = bin_dir / name
            try:
                script_path.write_text(content)
                script_path.chmod(0o755)
                print(f"  Created {name}")
            except Exception as e:
                print(f"  Failed to create {name}: {e}")

    ########################################
    # MAIN EXECUTION
    ########################################

    print("\n" + "="*70)
    print("BASE TERMINAL SETUP")
    print("="*70)
    
    print("\nThis will:")
    print("  ‚Ä¢ Install packages (may require sudo)")
    print("  ‚Ä¢ Modify configuration files")
    print("  ‚Ä¢ Create BASE directory structure")
    
    # Ask for confirmation
    try:
        response = 'y' #input("\nContinue? [y/N]: ").strip().lower()
        if response not in ['y', 'yes']:
            print("Setup cancelled.")
            return
    except KeyboardInterrupt:
        print("\nSetup cancelled by user.")
        return
    
    # Get user info
    print("\nPersonalization")
    name = input("Your name (for git config): ").strip()
    email = input("Your email (for git config): ").strip()
    
    # Run setup steps with error handling
    try:
        install_data_science_tools()
    except Exception as e:
        print(f"[WARNING] Tool installation had issues: {e}")
    
    try:
        setup_data_science_directories()
    except Exception as e:
        print(f"[WARNING] Directory setup had issues: {e}")
    
    # Setup shell
    print("\nConfiguring shell...")
    try:
        if command_exists("zsh"):
            safe_write(Path.home() / ".zshrc", ZSH_BLOCK)
            print("zsh configured")
        else:
            safe_write(Path.home() / ".bashrc", BASH_BLOCK)
            print("bash configured")
    except Exception as e:
        print(f"[ERROR] Shell configuration failed: {e}")
        return
    
    # Setup other configs
    try:
        setup_git_config()
        setup_ipython()
        setup_jupyter()
        create_useful_scripts()
        
        # Set git config
        if name and email:
            run_cmd(["git", "config", "--global", "user.name", name], check=False)
            run_cmd(["git", "config", "--global", "user.email", email], check=False)
    except Exception as e:
        print(f"[WARNING] Configuration steps had issues: {e}")
    
    # Final check
    check_data_science_tools()
    
    # Final message
    print("\n" + "="*70)
    print("SETUP COMPLETE!")
    print("="*70)
    
    print("\nNEXT STEPS:")
    print("1. Restart your terminal or run: source ~/.zshrc (or ~/.bashrc)")
    print("2. Explore new commands:")
    print("   ‚Ä¢ mkds <name>     - Create BASE project")
    print("   ‚Ä¢ profile <csv>   - Quick data profiling")
    print("   ‚Ä¢ findpy <text>   - Search Python files")
    print("3. Start a project:")
    print("   mkds my_project")
    print("   cd my_project")
    print("   jupyter lab")
    print("\nFor issues, check backup files with .bak extension")

 

# !=========== make a new project ==================
from pathlib import Path
import subprocess
import textwrap
import sys
import re
import venv
import platform
from typing import Optional, Dict, Any
from datetime import datetime
import os
import requests
import getpass
import json
from cryptography.fernet import Fernet


def run(cmd, cwd=None, capture_output=False, verbose=True):
    """Run a shell command with better error handling."""
    if verbose:
        print(f"[cmd] {' '.join(cmd) if isinstance(cmd, list) else cmd}")

    try:
        if capture_output:
            result = subprocess.run(
                cmd, cwd=cwd, check=True, text=True, capture_output=True
            )
            return result.stdout.strip()
        else:
            subprocess.run(cmd, cwd=cwd, check=True)
    except subprocess.CalledProcessError as e:
        print(f"Command failed with exit code {e.returncode}")
        if hasattr(e, "stderr") and e.stderr:
            print(f"Error: {e.stderr}")
        raise


def make_venv(project_dir: Path, python_version: str = "3") -> Path:
    """
    Create a virtual environment in the project directory.
    """
    env_dir = project_dir / ".venv"

    if env_dir.exists():
        print(f"Virtual environment already exists at {env_dir}")
        return env_dir

    print(f"Creating virtual environment at {env_dir}...")

    python_executable = None
    if python_version:
        python_names = [
            f"python{python_version}",
            f"python{python_version.replace('.', '')}",
            f"python{python_version.split('.')[0]}",
            "python3",
            "python",
        ]

        for name in python_names:
            try:
                subprocess.run([name, "--version"], capture_output=True, check=True)
                python_executable = name
                print(f"Using Python: {python_executable}")
                break
            except (FileNotFoundError, subprocess.CalledProcessError):
                continue

    try:
        if python_executable:
            subprocess.run([python_executable, "-m", "venv", str(env_dir)], check=True)
        else:
            subprocess.run(["python", "-m", "venv", str(env_dir)], check=True)

        print("created virtual environment")
        return env_dir

    except subprocess.CalledProcessError as e:
        print(f"Failed to create virtual environment: {e}")
        raise


def get_activate_command(project_dir: Path) -> str:
    """Get the command to activate the virtual environment."""
    env_dir = project_dir / ".venv"

    if platform.system() == "Windows":
        activate_script = env_dir / "Scripts" / "activate"
        if activate_script.exists():
            return f"{env_dir}\\Scripts\\activate"
        else:
            return f"{env_dir}\\Scripts\\activate.bat"
    else:
        return f"source {env_dir}/bin/activate"


def install_base_requirements(project_dir: Path) -> None:
    """Install base requirements in the virtual environment."""
    env_dir = project_dir / ".venv"

    if not env_dir.exists():
        print("Virtual environment not found. Creating...")
        make_venv(project_dir)

    if platform.system() == "Windows":
        pip_path = env_dir / "Scripts" / "pip.exe"
    else:
        pip_path = env_dir / "bin" / "pip"

    if not pip_path.exists():
        print(f"Pip not found at {pip_path}. Installing pip...")
        get_pip_script = project_dir / "get-pip.py"
        if not get_pip_script.exists():
            import urllib.request
            urllib.request.urlretrieve(
                "https://bootstrap.pypa.io/get-pip.py", get_pip_script
            )

        python_executable = (
            env_dir / "Scripts" / "python.exe"
            if platform.system() == "Windows"
            else env_dir / "bin" / "python"
        )
        subprocess.run([str(python_executable), str(get_pip_script)], check=True)
        if get_pip_script.exists():
            get_pip_script.unlink()

    print("Installing base requirements...")
    subprocess.run([str(pip_path), "install", "--upgrade", "pip"], check=True)
    base_packages = ["py2ls[slim]"]
    
    for package in base_packages:
        subprocess.run([str(pip_path), "install", package], check=True)

    print("‚úì Base requirements installed")


def create_requirements_file(project_dir: Path, config: Dict[str, Any]) -> None:
    """Create basic requirements.txt file."""
    requirements_content = textwrap.dedent(f"""\
        # Project dependencies for {config['project_name']}
        # Generated on {datetime.now().strftime('%Y-%m-%d')}
        
        # Core package
        py2ls[slim]
        
        # Add your project-specific dependencies below
        
    """)
    
    requirements_file = project_dir / "requirements.txt"
    requirements_file.write_text(requirements_content)
    print("‚úì Created requirements.txt")


def create_environment_instructions(project_dir: Path, config: Dict[str, Any]) -> None:
    """Create environment setup instructions."""
    instructions = project_dir / "ENVIRONMENT.md"
    project_name = config["project_name"]

    content = textwrap.dedent(f"""\
        # Environment Setup for {project_name}
        
        This project uses a Python virtual environment located in the `.venv` directory.
        
        ## Quick Setup
        
        ### 1. Create and activate virtual environment
        ```bash
        # Create virtual environment
        python -m venv .venv
        
        # Activate it
        # Linux/Mac:
        source .venv/bin/activate
        
        # Windows:
        .venv\\Scripts\\activate
        ```
        
        ### 2. Install dependencies
        ```bash
        pip install -r requirements.txt
        ```
        
        ### Start working
        ```bash
        cd {project_name}
        source .venv/bin/activate  # or .venv\\Scripts\\activate on Windows
        ```
    """)

    instructions.write_text(content)
    print("‚úì Created ENVIRONMENT.md")


def create_file_structure(project_dir: Path, config: Dict[str, Any]) -> None:
    """Create BASE specific directory structure."""
    data_dirs = ["data/raw","data/processed","data/external","data/interim"]
    result_dirs = ["output"]
    notebook_dirs = ["scripts/EDA"]

    for dir_path in data_dirs + result_dirs + notebook_dirs:
        (project_dir / dir_path).mkdir(parents=True, exist_ok=True)

    data_readme = project_dir / "data" / "README.md"
    data_readme.write_text(textwrap.dedent("""\
        # Data Directory
        
        ## Structure
         
        
        ## Data Processing Pipeline
         
        
        ## Notes
    """))

    for dir_path in data_dirs[1:] + result_dirs + notebook_dirs:
        gitkeep = project_dir / dir_path / ".gitkeep"
        gitkeep.touch()


def create_gitignore(project_dir: Path) -> None:
    """Create comprehensive .gitignore for BASE projects."""
    gitignore_content = textwrap.dedent("""\
        # Python
        __pycache__/
        *.py[cod]
        *$py.class
        *.so
        .Python
        build/
        develop-eggs/
        dist/
        downloads/
        eggs/
        .eggs/
        lib/
        lib64/
        parts/
        sdist/
        var/
        wheels/
        *.egg-info/
        .installed.cfg
        *.egg
        
        # Virtual Environment
        .venv/
        venv/
        env/
        
        # IDE
        .vscode/
        .idea/
        *.swp
        *.swo
        *~
        .project
        .classpath
        
        # Jupyter
        .ipynb_checkpoints
        *.ipynb_checkpoints
        ipython_config.py
        
        # Data and Results
        # Ignore all files inside data/raw/
        data/raw/*
        # But keep .gitkeep
        !data/raw/.gitkeep
        
        data/processed/*
        !data/processed/.gitkeep

        data/external/*
        !data/external/.gitkeep

        data/interim/*
        !data/interim/.gitkeep

        results/*
        !results/.gitkeep
        output/*
        !output/.gitkeep
        
        # Environment variables
        .env
        .env.*
        
        # Logs
        logs/
        *.log
        logs.txt
        
        # OS
        .DS_Store
        .DS_Store?
        ._*
        .Spotlight-V100
        .Trashes
        ehthumbs.db
        Thumbs.db
        
        # Temporary files
        *.tmp
        *.temp
        *~
        
        # Secrets
        *.key
        *.pem
        *.cert
        *.p12
        *.pfx
        secret.txt
        credentials.json
        config.json
        
        # Testing
        .coverage
        .pytest_cache/
        .tox/
        
        # Documentation
        docs/_build/
        docs/api/
        
        # Package manager specific
        Pipfile.lock
        poetry.lock
        pdm.lock
    """)

    gitignore = project_dir / ".gitignore"
    gitignore.write_text(gitignore_content)
    print("‚úì Created .gitignore")


def create_github_repo(
    project_name: str,
    github_token: Optional[str] = None,
    github_username: Optional[str] = None,
    private: bool = True,
    description: str = ""
) -> bool:
    """Create a GitHub repository via API. Returns True if successful."""
    
    if not github_username:
        print("GitHub username not provided. Skipping GitHub repository creation.")
        return False
    
    if not github_token:
        print("GitHub token not provided. Skipping GitHub repository creation.")
        print("You can create repository manually at: https://github.com/new")
        return False
    
    url = "https://api.github.com/user/repos"
    
    headers = {
        "Authorization": f"token {github_token}",
        "Accept": "application/vnd.github.v3+json"
    }
    
    data = {
        "name": project_name,
        "description": description,
        "private": private,
        "auto_init": False,
        "gitignore_template": "Python"
    }
    
    try:
        response = requests.post(url, json=data, headers=headers, timeout=30)
        response.raise_for_status()
        
        repo_data = response.json()
        html_url = repo_data.get("html_url")
        
        print(f"‚úì Created {'private' if private else 'public'} repository on GitHub")
        print(f"  Repository URL: {html_url}")
        print(f"  You can clone it with:")
        print(f"    git clone {repo_data.get('clone_url')}")
        print(f"  Or with SSH:")
        print(f"    git clone {repo_data.get('ssh_url')}")
        
        return True
        
    except requests.exceptions.RequestException as e:
        print(f"‚ö†Ô∏è Failed to create GitHub repository: {e}")
        
        if hasattr(e, 'response') and e.response:
            try:
                error_msg = e.response.json().get('message', 'Unknown error')
                print(f"  Error details: {error_msg}")
                
                if "name already exists" in error_msg.lower():
                    print(f"  Repository '{project_name}' already exists on GitHub.")
                    print(f"  URL: https://github.com/{github_username}/{project_name}")
            except:
                pass
        
        print(f"\nYou can create the repository manually:")
        print(f"  https://github.com/new?name={project_name}")
        if private:
            print("  Don't forget to check 'Private repository'")
        
        return False


def setup_github_credentials(
    github_username: Optional[str] = None,
    github_token: Optional[str] = None,
    interactive: bool = False
) -> tuple[Optional[str], Optional[str]]:
    """
    Setup GitHub credentials interactively if needed.
    Returns (username, token) tuple.
    """
    # Check if we already have credentials
    if github_username and github_token:
        return github_username, github_token
    
    # Try environment variables
    if not github_username:
        github_username = os.environ.get("GITHUB_USERNAME")
    
    if not github_token:
        github_token = os.environ.get("GITHUB_TOKEN")
    
    # If still missing and in interactive mode, prompt user
    if interactive:
        if not github_username:
            print("\n" + "="*60)
            print("GitHub Username Required")
            print("="*60)
            github_username = input("Enter your GitHub username: ").strip()
        
        if not github_token:
            print("\n" + "="*60)
            print("GitHub Personal Access Token Required")
            print("="*60)
            print("To create a token:")
            print("1. Go to: https://github.com/settings/tokens")
            print("2. Click 'Generate new token' ‚Üí 'Generate new token (classic)'")
            print("3. Set expiration (e.g., 90 days)")
            print("4. Check 'repo' scope (required)")
            print("5. Generate and copy the token")
            print("="*60)
            
            github_token = getpass.getpass("Enter GitHub token (hidden): ").strip()
            
            if github_token and not github_token.startswith("ghp_"):
                print("‚ö†Ô∏è Token should start with 'ghp_'. Please check.")
                github_token = None
    
    return github_username, github_token

def nohup(
    command,
    log_dir: str = "logs",
    log_prefix: str = "job"
) -> dict:
    """
    Master function to run commands with nohup.
    
    Supports both styles:
    1. Command as list: nohup_master(["python", "script.py", "--arg", "value"])
    2. Command as string: nohup_master("python script.py --arg value")
    
    Usage:
        # Style 1 (recommended)
        result = nohup_master(
            command=["python", "my_script.py", "arg1", "arg2"],
            log_prefix="analysis"
        )
        
        # Style 2 (string style - what you asked for)
        result = nohup_master(
            command="python my_script.py arg1 arg2",
            log_prefix="analysis"
        )
    """ 
    import shlex  # For parsing command strings
    from datetime import datetime
    # Setup directories
    log_path = Path(log_dir)
    log_path.mkdir(parents=True, exist_ok=True)
    
    # Generate files
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_file = log_path / f"{log_prefix}_{timestamp}.log"
    pid_file = log_path / f"{log_prefix}_{timestamp}.pid"
    
    # Handle both string and list input
    if isinstance(command, str):
        # Parse string into list (respects quotes)
        cmd_parts = shlex.split(command)
        cmd_str = command  # Keep original string
    else:
        # Assume it's already a list
        cmd_parts = command
        cmd_str = " ".join(str(arg) for arg in command)
    
    # Build nohup command
    nohup_cmd = f"nohup {cmd_str} > {log_file} 2>&1 & echo $!"
    
    # Run
    result = subprocess.run(nohup_cmd, shell=True, capture_output=True, text=True)
    pid = int(result.stdout.strip())
    
    # Save PID
    with open(pid_file, 'w') as f:
        f.write(f"{pid}\n")
        f.write(f"# Command: {cmd_str}\n")
        f.write(f"# Started: {datetime.now().isoformat()}\n")
    
    return {
        'pid': pid,
        'log_file': str(log_file),
        'pid_file': str(pid_file),
        'command': cmd_str,
        'command_parts': cmd_parts  # Also return parsed parts
    }

def make_project(
    project_name: str,
    dir_base: str | Path = ".",
    author_name: str = "",
    author_email: str = "",
    description: str = "",
    python_version: str = "3.10",
    github_username: Optional[str] = None,
    github_token: Optional[str] = None,
    create_github_repo_flag: bool = False,
    private_repo: bool = False,
    license_type: str = "MIT",
    create_virtualenv: bool = True,
    install_deps: bool = True,
    interactive: bool = False,
    minimal: bool = True,
) -> Path:
    """
    Initialize a BASE project with .venv virtual environment.
    
    Parameters
    ----------
    create_github_repo_flag : bool
        If True, create a GitHub repository for the project
    private_repo : bool
        If True and creating GitHub repo, make it private
    """
    if not re.match(r"^[a-zA-Z][a-zA-Z0-9_-]*$", project_name):
        raise ValueError(
            "Project name must start with a letter and contain only "
            "letters, numbers, hyphens, and underscores"
        )

    if interactive:
        if not author_name:
            author_name = input("Author name: ").strip() or "Jeff***.Liu"
        if not author_email:
            author_email = input("Author email: ").strip() or "Jeff.Liu***@gmail.com"
        if not description:
            description = input("Project description: ").strip() or "A great project"

    if not author_name:
        author_name = "Jeff***.Liu"
    if not author_email:
        author_email = "Jeff.Liu***@gmail.com"
    if not description:
        description = f"project: {project_name}"

    dir_base = Path(dir_base).expanduser().resolve()
    project_dir = dir_base / project_name

    if project_dir.exists():
        print(f"Project directory already exists: {project_dir}")
        response = input("Continue anyway? (y/n): ").strip().lower()
        if response != "y":
            print("Aborted.")
            sys.exit(1)
    else:
        project_dir.mkdir(parents=True)

    print(f"Creating project: {project_name}")
    print(f"Location: {project_dir}")

    config = {
        "project_name": project_name,
        "author_name": author_name,
        "author_email": author_email,
        "description": description,
        "python_version": python_version,
        "license_type": license_type,
    }

    print("\nCreating directory structure...")
    (project_dir / "tests").mkdir(exist_ok=True)
    (project_dir / "tests" / "__init__.py").touch()
    create_file_structure(project_dir, config)

    print("\nCreating project files...")
    create_gitignore(project_dir)
    create_requirements_file(project_dir, config)

    readme_content = textwrap.dedent(f"""\
        # {project_name}
        
        {description}
        
        ## Project Structure
        
        ```
        {project_name}/
        ‚îú‚îÄ‚îÄ data/           # Data files
        ‚îú‚îÄ‚îÄ scripts/        # Analysis scripts
        ‚îÇ   ‚îî‚îÄ‚îÄ EDA/        # Exploratory Data Analysis
        ‚îú‚îÄ‚îÄ output/         # Results and outputs
        ‚îú‚îÄ‚îÄ tests/          # Test files
        ‚îú‚îÄ‚îÄ requirements.txt # Dependencies
        ‚îú‚îÄ‚îÄ README.md       # This file
        ‚îî‚îÄ‚îÄ ENVIRONMENT.md  # Setup instructions
        ```
        
        ## Author
        
        {author_name} ({author_email})
        
        ## License
        
        {license_type} License
    """)

    (project_dir / "README.md").write_text(readme_content)
    create_environment_instructions(project_dir, config)

    if create_virtualenv:
        print("\nCreating virtual environment...")
        try:
            env_dir = make_venv(project_dir, python_version)
            if install_deps:
                print("\nInstalling dependencies...")
                install_base_requirements(project_dir)
        except Exception as e:
            print(f"‚ö†Ô∏è Could not create virtual environment: {e}")
            print("You can create it manually: python -m venv .venv")

    # Create GitHub repository if requested
    github_repo_created = False
    if create_github_repo_flag:
        print("\n" + "="*60)
        print("GitHub Repository Setup")
        print("="*60)
        
        github_username, github_token = setup_github_credentials(
            github_username=github_username,
            github_token=github_token,
            interactive=interactive
        )
        
        if github_username and github_token:
            github_repo_created = create_github_repo(
                project_name=project_name,
                github_token=github_token,
                github_username=github_username,
                private=private_repo,
                description=description
            )
        else:
            print("‚ö†Ô∏è GitHub credentials not available. Skipping repository creation.")
            print(f"You can create it manually at: https://github.com/new?name={project_name}")

    print("\n" + "=" * 60)
    print("PROJECT CREATED SUCCESSFULLY!")
    print("=" * 60)

    print(f"\nProject location: {project_dir}")

    if create_virtualenv:
        print("\nVirtual environment: .venv/")
        print(f"   Activate with: {get_activate_command(project_dir)}")

    print("\nNext steps:")
    print(f"   1. cd {project_dir}")
    
    if create_virtualenv:
        print(f"   2. {get_activate_command(project_dir)}")
        print("   3. Check requirements.txt for installed packages")
        print("   4. Start coding!")
    else:
        print("   2. python -m venv .venv")
        print(f"   3. {get_activate_command(project_dir)}")
        print("   4. pip install -r requirements.txt")
        print("   5. Start coding!")
    
    # Additional GitHub setup instructions if repo was created
    if github_repo_created:
        print("\nTo initialize git and push to GitHub:")
        print(f"  cd {project_dir}")
        print("  git init")
        print("  git add .")
        print("  git commit -m 'Initial commit'")
        print(f"  git remote add origin https://github.com/{github_username}/{project_name}.git")
        print("  git push -u origin main")
    elif create_github_repo_flag and not github_repo_created:
        print("\nTo setup git later:")
        print(f"  cd {project_dir}")
        print("  git init")
        print("  git add .")
        print("  git commit -m 'Initial commit'")
        print(f"  # Then add remote after creating repository on GitHub")

    return project_dir


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(
        description="Initialize a BASE project with virtual environment",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog=textwrap.dedent("""
            Examples:
              
              # List files
              %(prog)s ls /path/to/dir --kind .py --depth 2
              %(prog)s ls /path/to/dir --filter "*.csv" --contains "2023"
              
              # List with multiple filters
              %(prog)s ls /path/to/dir --kind .py .ipynb --sort-by size --ascending False
        """)
    )    
    # Parser for 'ls' command
    ls_parser = subparsers.add_parser('ls', help='List files with advanced filtering')
    ls_parser.add_argument("rootdir", help="Root directory to scan")
    ls_parser.add_argument("--kind", "-k", nargs="+", help="File extensions to include (e.g., .py .txt)")
    ls_parser.add_argument("--ignore-case", action="store_true", default=True, help="Case-insensitive matching")
    ls_parser.add_argument("--no-ignore-case", action="store_false", dest="ignore_case", help="Case-sensitive matching")
    ls_parser.add_argument("--filter", "-f", nargs="+", help="Wildcard patterns to include (e.g., '*.csv')")
    ls_parser.add_argument("--exclude", "-x", nargs="+", help="Patterns to exclude")
    ls_parser.add_argument("--sort-by", "-s", default="name", help="Sort by column (name, size, mtime, etc.)")
    ls_parser.add_argument("--ascending", action="store_true", default=True, help="Ascending sort")
    ls_parser.add_argument("--descending", action="store_false", dest="ascending", help="Descending sort")
    ls_parser.add_argument("--contains", "-c", help="Regex pattern to match in filenames")
    ls_parser.add_argument("--booster", "-b", choices=['auto', 'true', 'false'], default='auto', help="Enable parallel processing")
    ls_parser.add_argument("--depth", "-d", type=int, default=0, help="Max recursion depth (0=current dir only)")
    ls_parser.add_argument("--hidden", action="store_true", help="Include hidden files")
    ls_parser.add_argument("--orient", "-o", choices=['df', 'list', 'dict', 'records'], default='df', help="Output format")
    ls_parser.add_argument("--verbose", "-v", action="store_true", help="Print progress messages")
    ls_parser.add_argument("--workers", "-w", type=int, help="Number of parallel workers")
    ls_parser.add_argument("--output", choices=['print', 'csv', 'json'], default='print', help="Output destination")
    ls_parser.add_argument("--output-file", help="Output file path (for csv/json)")

    args = parser.parse_args()

    if args.command == "ls":
        try:
            # Convert string booleans
            booster_map = {'auto': 'auto', 'true': True, 'false': False}
            booster = booster_map.get(args.booster, args.booster)
            
            # Run ls function
            result = ls(
                rootdir=args.rootdir,
                kind=args.kind,
                ignore_case=args.ignore_case,
                filter=args.filter,
                exclude=args.exclude,
                sort_by=args.sort_by,
                ascending=args.ascending,
                contains=args.contains,
                booster=booster,
                depth=args.depth,
                hidden=args.hidden,
                orient=args.orient,
                output="df",
                verbose=args.verbose,
                workers=args.workers,
            )
            
            # Handle output
            if args.output == "print":
                if args.orient == "df" and isinstance(result, pd.DataFrame):
                    pd.set_option('display.max_rows', None)
                    pd.set_option('display.max_columns', None)
                    pd.set_option('display.width', None)
                    pd.set_option('display.max_colwidth', 40)
                    
                    print(f"\nFound {len(result)} items:")
                    print("=" * 80)
                    print(result)
                    
                    # Show summary
                    if len(result) > 0:
                        print(f"\nSummary:")
                        print(f"  Total size: {result['size'].sum():.2f} MB")
                        if 'kind' in result.columns:
                            kind_counts = result['kind'].value_counts()
                            print(f"  File types: {len(kind_counts)} unique extensions")
                            for kind, count in kind_counts.head(5).items():
                                print(f"    {kind}: {count} files")
                else:
                    print(result)
            
            elif args.output == "csv":
                if isinstance(result, pd.DataFrame):
                    output_file = args.output_file or f"ls_output_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
                    result.to_csv(output_file, index=False)
                    print(f"‚úì Saved to {output_file}")
                else:
                    print("CSV output only available for DataFrame format")
            
            elif args.output == "json":
                output_file = args.output_file or f"ls_output_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
                if isinstance(result, pd.DataFrame):
                    result.to_json(output_file, orient='records', indent=2)
                else:
                    import json
                    with open(output_file, 'w') as f:
                        json.dump(result, f, indent=2, default=str)
                print(f"‚úì Saved to {output_file}")
        
        except Exception as e:
            print(f"Error listing files: {e}", file=sys.stderr)
            sys.exit(1)
    
    else:
        parser.print_help()
        sys.exit(1)
