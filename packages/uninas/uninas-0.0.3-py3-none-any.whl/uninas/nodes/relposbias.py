import torch.nn as nn
from torch import sqrt, tensor, meshgrid, zeros, arange, stack
from .base import BaseNode
import math
import warnings
from timm.layers.weight_init import trunc_normal_


class RelPosBias(BaseNode):  # simple
    def __init__(self, shape: tuple, root_shape: tuple):
        img_shape = sqrt(tensor(shape[-2:])).tolist()
        assert img_shape[0] % 1 == 0 and img_shape[1] % 1 == 0, 'Implemented only for self-attention-like shapes.'
        img_shape = int(img_shape[0]), int(img_shape[1])
        super().__init__(shape, root_shape, shape[1]*shape[2], (2 * img_shape[0] - 1) * (2 * img_shape[1] - 1))

        H, W = img_shape[0], img_shape[1]
        self.bias_shape = (H * W,) * 2
        self.relative_position_bias_table = nn.Parameter(zeros((2 * H - 1) * (2 * W - 1)))

        def ndgrid(*tensors):
            try:
                return meshgrid(*tensors, indexing='ij')
            except TypeError:
                return meshgrid(*tensors)

        coords = stack(ndgrid(arange(H), arange(W))).flatten(1)  # 2, Wh, Ww
        relative_coords = coords[:, :, None] - coords[:, None, :]  # 2, Wh*Ww, Wh*Ww
        relative_coords = relative_coords.permute(1, 2, 0)  # Qh*Qw, Kh*Kw, 2
        relative_coords[:, :, 0] += H - 1  # shift to start from 0
        relative_coords[:, :, 1] += W - 1
        relative_coords[:, :, 0] *= 2 * W - 1
        relative_position_index = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww

        self.register_buffer("relative_position_index", relative_position_index.contiguous().view(-1),
                             persistent=False)
        trunc_normal_(self.relative_position_bias_table, std=.02)  # FIXME: check if is consistent with _trunc_normal_

    def forward(self, x):
        return x + self.relative_position_bias_table[self.relative_position_index].view(self.bias_shape).unsqueeze(
            0).contiguous()


def _trunc_normal_(tensor, mean, std, a, b):
    # Cut & paste from PyTorch official master until it's in a few official releases - RW
    # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf
    def norm_cdf(x):
        # Computes standard normal cumulative distribution function
        return (1. + math.erf(x / math.sqrt(2.))) / 2.

    if (mean < a - 2 * std) or (mean > b + 2 * std):
        warnings.warn("mean is more than 2 std from [a, b] in nn.init.trunc_normal_. "
                      "The distribution of values may be incorrect.",
                      stacklevel=2)

    # Values are generated by using a truncated uniform distribution and
    # then using the inverse CDF for the normal distribution.
    # Get upper and lower cdf values
    l = norm_cdf((a - mean) / std)
    u = norm_cdf((b - mean) / std)

    # Uniformly fill tensor with values from [l, u], then translate to
    # [2l-1, 2u-1].
    tensor.uniform_(2 * l - 1, 2 * u - 1)

    # Use inverse cdf transform for normal distribution to get truncated
    # standard normal
    tensor.erfinv_()

    # Transform to proper mean, std
    tensor.mul_(std * math.sqrt(2.))
    tensor.add_(mean)

    # Clamp to ensure it's in the proper range
    tensor.clamp_(min=a, max=b)
    return tensor