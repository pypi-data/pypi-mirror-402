# VM Deployment Template with FEAST Support
# ===========================================
# 
# Features:
# - Separate FEAST database and user for isolation
# - Cross-database permissions for MLflow and FEAST users
# - Enhanced health checks and dependencies
# - Comprehensive error handling and validation
# - Proper environment variable management
# 
# Key Variables:
# ==============
# - feast_database_name: Name of FEAST database (default: "feast")
# - feast_database_user: Name of FEAST database user (default: "feast")
# - feast_separate_database: Use separate database for FEAST (default: true)
# - feast_port: FEAST server port (default: 6566)
# - bigquery_dataset: BigQuery dataset for offline store (default: "feast_offline_store")
# - use_postgres: Enable PostgreSQL backend (default: false)
# 
# Database Configuration:
# ======================
# When feast_separate_database = true:
# - MLflow database: {first_tool_name} (e.g., "mlflow")
# - MLflow user: {first_tool_name} (e.g., "mlflow")
# - FEAST database: feast_database_name (default: "feast")
# - FEAST user: feast_database_user (default: "feast")
# 
# Cross-Database Permissions:
# ==========================
# - MLflow user gets full access to FEAST database
# - FEAST user gets read access to MLflow database
# - Both users share the same password for simplicity
# 
# Environment Variables:
# ====================
# - FEAST_REGISTRY_TYPE: "sql" (PostgreSQL) or "file" (SQLite)
# - FEAST_REGISTRY_PATH: PostgreSQL connection string or file path
# - FEAST_ONLINE_STORE_TYPE: "postgres" or "sqlite"
# - FEAST_OFFLINE_STORE_TYPE: "bigquery"
# - GOOGLE_CLOUD_PROJECT: GCP project ID
# 
# Health Checks:
# =============
# - MLflow: HTTP health check on /health endpoint
# - FastAPI: HTTP health check on /health endpoint  
# - FEAST: Port check using nc (netcat)
# 
# Dependencies:
# ============
# - FEAST depends on MLflow being healthy
# - FastAPI depends on MLflow being healthy
# - All services depend on PostgreSQL being ready

# --- UNIFIED BUCKET CREATION ---
# Create buckets based on structured configuration from CLI
{% if bucket_configs %}
# Creating buckets based on unified configuration...
{% for config in bucket_configs %}
# Bucket for {{ config.stage }}/{{ config.tool }}
resource "google_storage_bucket" "{{ config.stage }}_{{ config.tool }}_artifact" {
  name          = "{{ config.bucket_name }}"
  location      = var.region
  force_destroy = true
  count         = {{ config.create | lower }} ? 1 : 0

  labels = {
    component  = "{{ config.tool }}-artifacts"
    managed-by = "terraform"
    stage      = "{{ config.stage }}"
  }
}
{% endfor %}
{% else %}
# No bucket configurations found
{% endif %}

provider "google" {
  project = "{{ project_id }}"
  region  = "{{ region }}"
  zone    = "{{ zone }}"
}

# Detect PostgreSQL and collect service configurations from all stages
{% set flags = namespace(needs_postgres=false, needs_feast=false, needs_grafana=false, needs_airflow=false, first_tool_name="", mlflow_params={}, feast_params={}, grafana_params={}, airflow_params={}) %}

# Set database type flags based on backend_store_uri configuration
{% set flags.use_cloudsql = false %}
{% set flags.use_local_postgres = false %}
{% set flags.use_sqlite = false %}
{% for stage in stack %}
  {% for stage_name, tool in stage.items() %}
    {% if tool.name == "mlflow" %}
      {% for key, value in tool.params.items() %}
        {% set _ = flags.mlflow_params.update({key: value}) %}
      {% endfor %}
      {% if tool.params.get("backend_store_uri", "") == "postgresql" %}
        {% set flags.needs_postgres = true %}
        {% set flags.use_cloudsql = true %}
        {% if not flags.first_tool_name %}
          {% set flags.first_tool_name = tool.name %}
        {% endif %}
      {% elif tool.params.get("backend_store_uri", "") == "cloudsql" %}
        {% set flags.needs_postgres = true %}
        {% set flags.use_cloudsql = true %}
        {% if not flags.first_tool_name %}
          {% set flags.first_tool_name = tool.name %}
        {% endif %}
      {% elif tool.params.get("backend_store_uri", "") == "sqlite" %}
        {% set flags.use_sqlite = true %}
        {% if not flags.first_tool_name %}
          {% set flags.first_tool_name = tool.name %}
        {% endif %}
      {% endif %}
    {% endif %}
    {% if tool.name == "feast" %}
      {% set flags.needs_feast = true %}
      {% for key, value in tool.params.items() %}
        {% set _ = flags.feast_params.update({key: value}) %}
      {% endfor %}
      {% if tool.params.get("backend_store_uri", "") == "postgresql" %}
        {% set flags.needs_postgres = true %}
        {% set flags.use_cloudsql = true %}
        {% if not flags.first_tool_name %}
          {% set flags.first_tool_name = tool.name %}
        {% endif %}
      {% elif tool.params.get("backend_store_uri", "") == "cloudsql" %}
        {% set flags.needs_postgres = true %}
        {% set flags.use_cloudsql = true %}
        {% if not flags.first_tool_name %}
          {% set flags.first_tool_name = tool.name %}
        {% endif %}
      {% elif tool.params.get("backend_store_uri", "") == "sqlite" %}
        {% set flags.use_sqlite = true %}
        {% if not flags.first_tool_name %}
          {% set flags.first_tool_name = tool.name %}
        {% endif %}
      {% endif %}
    {% endif %}
    {% if stage_name == "model_monitoring" and tool.name == "grafana" %}
      {% set flags.needs_grafana = true %}
      {% for key, value in tool.params.items() %}
        {% set _ = flags.grafana_params.update({key: value}) %}
      {% endfor %}
    {% endif %}
    {% if stage_name == "workflow_orchestration" and tool.name == "airflow" %}
      {% set flags.needs_airflow = true %}
      {% for key, value in tool.params.items() %}
        {% set _ = flags.airflow_params.update({key: value}) %}
      {% endfor %}
      {% if tool.params.get("backend_store_uri", "") == "postgresql" %}
        {% set flags.needs_postgres = true %}
        {% set flags.use_cloudsql = true %}
        {% if not flags.first_tool_name %}
          {% set flags.first_tool_name = tool.name %}
        {% endif %}
      {% elif tool.params.get("backend_store_uri", "") == "cloudsql" %}
        {% set flags.needs_postgres = true %}
        {% set flags.use_cloudsql = true %}
        {% if not flags.first_tool_name %}
          {% set flags.first_tool_name = tool.name %}
        {% endif %}
      {% elif tool.params.get("backend_store_uri", "") == "sqlite" %}
        {% set flags.use_sqlite = true %}
        {% if not flags.first_tool_name %}
          {% set flags.first_tool_name = tool.name %}
        {% endif %}
      {% endif %}
    {% endif %}
  {% endfor %}
{% endfor %}

# Debug: Print flags for troubleshooting
# {% if flags.needs_postgres %}
#   <!-- PostgreSQL is enabled -->
# {% else %}
#   <!-- PostgreSQL is NOT enabled -->
# {% endif %}
# {% if flags.needs_airflow %}
#   <!-- Airflow is enabled -->
# {% else %}
#   <!-- Airflow is NOT enabled -->
# {% endif %}

# Enable required Google Cloud APIs for fresh project
resource "google_project_service" "required_apis" {
  for_each = toset([
    "compute.googleapis.com",                   # Compute Engine (VMs, disks, networks)
    "storage.googleapis.com",                   # Cloud Storage (artifact buckets)
    "iam.googleapis.com",                       # Identity and Access Management
    "iamcredentials.googleapis.com",            # IAM Service Account Credentials
    "logging.googleapis.com",                   # Cloud Logging
    "monitoring.googleapis.com",                # Cloud Monitoring
    "serviceusage.googleapis.com",              # Service Usage (for enabling APIs)
    "cloudresourcemanager.googleapis.com",     # Cloud Resource Manager (project operations)
    {% if flags.use_cloudsql %}
    "sqladmin.googleapis.com",                  # Cloud SQL Admin API (for PostgreSQL)
    "sql-component.googleapis.com",             # Cloud SQL component API
    "servicenetworking.googleapis.com",         # For private service connections
    "cloudkms.googleapis.com",                  # For encryption keys (if using CMEK)
    {% endif %}
    {% if flags.needs_feast or flags.needs_airflow %}
    "bigquery.googleapis.com",                  # BigQuery API (for Feast offline store and Airflow)
    "datastore.googleapis.com",                 # Datastore API (for Feast online store option)
    "bigtable.googleapis.com",                  # Bigtable API (for Feast online store option)
    {% endif %}
  ])

  project = "{{ project_id }}"
  service = each.value

  # Keep APIs enabled when destroying resources
  disable_on_destroy = false
}

# Wait for API propagation (critical for fresh projects)
resource "time_sleep" "wait_for_api_propagation" {
  depends_on = [
    google_project_service.required_apis
  ]

  create_duration = "120s"  # 2 minutes for API propagation (sufficient for most cases)
}

# Simple API readiness marker (no external dependencies)
resource "null_resource" "api_readiness_check" {
  depends_on = [time_sleep.wait_for_api_propagation]
  
  # Simple trigger to ensure proper sequencing
  triggers = {
    api_wait_complete = timestamp()
  }
}

# Create Cloud SQL PostgreSQL instance if needed
{% if flags.needs_postgres %}
# Random password for the database
resource "random_password" "db_password" {
  length  = 16
  special = true
  override_special = "!#$*+-.=_"
}

{% if flags.needs_airflow %}
# Random passwords for Airflow
resource "random_password" "airflow_secret_key" {
  length  = 32
  special = false
}

resource "random_password" "airflow_fernet_key" {
  length  = 32
  special = false
}
{% endif %}

{% if flags.use_cloudsql %}
# Cloud SQL PostgreSQL instance
resource "google_sql_database_instance" "postgres" {
  name             = "{{ flags.first_tool_name }}-postgres-{{ project_id }}"
  database_version = "POSTGRES_14"
  region           = var.region
  project          = var.project_id
  depends_on       = [null_resource.api_readiness_check]

  settings {
    tier = "db-f1-micro"
    ip_configuration {
      authorized_networks {
        value = "0.0.0.0/0"
      }
      ipv4_enabled = true
    }
  }

  deletion_protection = false
}
{% endif %}

{% if flags.use_cloudsql %}
# Database within the instance for MLflow
resource "google_sql_database" "db" {
  name     = "{{ flags.first_tool_name }}"
  instance = google_sql_database_instance.postgres.name
  project  = var.project_id
  depends_on = [google_sql_database_instance.postgres]
}
{% endif %}

{% if flags.use_cloudsql and flags.needs_feast %}
# Separate database for FEAST
resource "google_sql_database" "feast_db" {
  name     = var.feast_database_name
  instance = google_sql_database_instance.postgres.name
  project  = var.project_id
  depends_on = [google_sql_database_instance.postgres]
}
{% endif %}

{% if flags.use_cloudsql and flags.needs_airflow %}
# Separate database for Airflow
resource "google_sql_database" "airflow_db" {
  name     = var.airflow_database_name
  instance = google_sql_database_instance.postgres.name
  project  = var.project_id
  depends_on = [google_sql_database_instance.postgres]
}
{% endif %}

{% if flags.use_cloudsql %}
# Database user for MLflow
resource "google_sql_user" "users" {
  name     = "{{ flags.first_tool_name }}"
  instance = google_sql_database_instance.postgres.name
  password = random_password.db_password.result
  project  = var.project_id
  depends_on = [google_sql_database_instance.postgres]
}
{% endif %}

{% if flags.use_cloudsql and flags.needs_feast %}
# Separate database user for FEAST
resource "google_sql_user" "feast_user" {
  name     = var.feast_database_user
  instance = google_sql_database_instance.postgres.name
  password = random_password.db_password.result
  project  = var.project_id
  depends_on = [google_sql_database_instance.postgres]
}
{% endif %}

{% if flags.use_cloudsql and flags.needs_airflow %}
# Separate database user for Airflow
resource "google_sql_user" "airflow_user" {
  name     = var.airflow_database_user
  instance = google_sql_database_instance.postgres.name
  password = random_password.db_password.result
  project  = var.project_id
  depends_on = [google_sql_database_instance.postgres]
}
{% endif %}

# Note: If terraform destroy fails with "role cann       ot be dropped because some objects depend on it",
# this is because MLflow created tables/objects owned by the user. To fix:
# 1. Connect to the database: gcloud sql connect {{ flags.first_tool_name }}-postgres-{{ project_id }} --user={{ flags.first_tool_name }}
# 2. Run: DROP OWNED BY {{ flags.first_tool_name }} CASCADE;
# 3. Then retry terraform destroy
{% endif %}

# Main artifact bucket is now handled by stage-specific buckets above
# This ensures consistent bucket naming and avoids conflicts

# Create a service account for the VM
resource "google_service_account" "vm_service_account" {
  account_id   = "mlflow-vm-sa"
  display_name = "Service Account for MLflow VM"
  project      = var.project_id
  
  # Explicit dependency to ensure APIs are ready FIRST
  depends_on = [null_resource.api_readiness_check]
}

# Project-level IAM bindings for the service account (these will show up in the IAM UI)
{% if bucket_configs %}
# Grant storage permissions if any buckets are configured
resource "google_project_iam_member" "vm_service_account_storage_admin" {
  project    = var.project_id
  role       = "roles/storage.objectAdmin"
  member     = "serviceAccount:${google_service_account.vm_service_account.email}"
  depends_on = [google_service_account.vm_service_account, null_resource.api_readiness_check]
}

resource "google_project_iam_member" "vm_service_account_storage_viewer" {
  project    = var.project_id
  role       = "roles/storage.objectViewer"
  member     = "serviceAccount:${google_service_account.vm_service_account.email}"
  depends_on = [google_service_account.vm_service_account, null_resource.api_readiness_check]
}
{% endif %}

# Additional useful permissions for the VM service account
resource "google_project_iam_member" "vm_service_account_logging" {
  project    = var.project_id
  role       = "roles/logging.logWriter"
  member     = "serviceAccount:${google_service_account.vm_service_account.email}"
  depends_on = [google_service_account.vm_service_account, null_resource.api_readiness_check]
}

resource "google_project_iam_member" "vm_service_account_monitoring" {
  project    = var.project_id
  role       = "roles/monitoring.metricWriter"
  member     = "serviceAccount:${google_service_account.vm_service_account.email}"
  depends_on = [google_service_account.vm_service_account, null_resource.api_readiness_check]
}

{% if flags.needs_feast %}
# BigQuery dataset for Feast offline store
resource "google_bigquery_dataset" "feast_offline_store" {
  count       = var.create_bigquery_dataset ? 1 : 0
  dataset_id  = var.bigquery_dataset
  project     = var.project_id
  location    = var.region
  
  description = "Feast offline store dataset"
  
  # Ensure Terraform can destroy the dataset even if tables exist
  delete_contents_on_destroy = true
  
  labels = {
    component  = "feast-offline-store"
    managed-by = "terraform"
  }
  
  depends_on = [null_resource.api_readiness_check]
}

# BigQuery permissions for Feast
resource "google_project_iam_member" "vm_service_account_bigquery_user" {
  project    = var.project_id
  role       = "roles/bigquery.user"
  member     = "serviceAccount:${google_service_account.vm_service_account.email}"
  depends_on = [google_service_account.vm_service_account, null_resource.api_readiness_check]
}

resource "google_project_iam_member" "vm_service_account_bigquery_data_editor" {
  project    = var.project_id
  role       = "roles/bigquery.dataEditor"
  member     = "serviceAccount:${google_service_account.vm_service_account.email}"
  depends_on = [google_service_account.vm_service_account, null_resource.api_readiness_check]
}

resource "google_project_iam_member" "vm_service_account_bigquery_job_user" {
  project    = var.project_id
  role       = "roles/bigquery.jobUser"
  member     = "serviceAccount:${google_service_account.vm_service_account.email}"
  depends_on = [google_service_account.vm_service_account, null_resource.api_readiness_check]
}
{% endif %}

# Define a Google Compute Engine instance
resource "google_compute_instance" "mlflow_vm" {
  name         = "{{ flags.mlflow_params.vm_name }}"
  machine_type = "{{ flags.mlflow_params.machine_type }}"
  zone         = "{{ zone }}"

  # Explicit dependency to ensure APIs are ready FIRST
  depends_on = [null_resource.api_readiness_check{% if flags.needs_postgres %}, google_sql_database_instance.postgres, google_sql_database.db, google_sql_user.users{% endif %}]

  boot_disk {
    initialize_params {
      image = "debian-cloud/debian-12"
      size  = {{ flags.mlflow_params.disk_size_gb }}
      type  = "pd-standard"
    }
  }

  network_interface {
    network = "default"
    access_config {
      // Ephemeral public IP
    }
  }

  # Service account
  service_account {
    email  = google_service_account.vm_service_account.email
    scopes = ["cloud-platform"] # Grant access to Google Cloud APIs
  }

  # Startup script to install Docker and deploy MLflow
  metadata = {
    startup-script = local.default_startup_script
  }

  tags = ["mlflow-server"]

  # Allow stopping for update
  allow_stopping_for_update = true
}

# Local variables for startup script
locals {
  {% if flags.needs_postgres %}
  # Use PostgreSQL connection string from the direct resources
      postgres_connection_string = "postgresql+psycopg2://${google_sql_user.users.name}:${urlencode(random_password.db_password.result)}@${google_sql_database_instance.postgres.public_ip_address}:5432/${google_sql_database.db.name}"
    backend_store_uri = "postgresql+psycopg2://${google_sql_user.users.name}:${urlencode(random_password.db_password.result)}@${google_sql_database_instance.postgres.public_ip_address}:5432/${google_sql_database.db.name}"
  {% if flags.needs_feast %}
  # FEAST PostgreSQL configuration - use separate database and user
      feast_registry_uri = "postgresql+psycopg2://${google_sql_user.feast_user.name}:${urlencode(random_password.db_password.result)}@${google_sql_database_instance.postgres.public_ip_address}:5432/${google_sql_database.feast_db.name}"
    feast_postgres_host = google_sql_database_instance.postgres.public_ip_address
  feast_postgres_port = "5432"
  feast_postgres_database = google_sql_database.feast_db.name
  feast_postgres_user = google_sql_user.feast_user.name
  feast_postgres_password = random_password.db_password.result
  {% else %}
  # FEAST PostgreSQL configuration - use same database as MLflow
  feast_registry_uri = "postgresql+psycopg2://${google_sql_user.users.name}:${urlencode(random_password.db_password.result)}@${google_sql_database_instance.postgres.public_ip_address}:5432/${google_sql_database.db.name}"
  feast_postgres_host = google_sql_database_instance.postgres.public_ip_address
  feast_postgres_port = "5432"
  feast_postgres_database = google_sql_database.db.name
  feast_postgres_user = google_sql_user.users.name
  feast_postgres_password = random_password.db_password.result
  {% endif %}

  {% if flags.needs_airflow %}
  # Airflow PostgreSQL configuration - use separate database and user
      airflow_sql_alchemy_conn = "postgresql+psycopg2://${google_sql_user.airflow_user.name}:${urlencode(random_password.db_password.result)}@${google_sql_database_instance.postgres.public_ip_address}:5432/${google_sql_database.airflow_db.name}"
    airflow_postgres_host = google_sql_database_instance.postgres.public_ip_address
  airflow_postgres_port = "5432"
  airflow_postgres_database = google_sql_database.airflow_db.name
  airflow_postgres_user = google_sql_user.airflow_user.name
  airflow_postgres_password = random_password.db_password.result
  {% else %}
  # Airflow SQLite configuration (fallback)
  airflow_sql_alchemy_conn = "sqlite:///airflow.db"
  {% endif %}

  # Generate Airflow keys if not provided
  airflow_secret_key = var.airflow_secret_key != "" ? var.airflow_secret_key : random_password.airflow_secret_key.result
  airflow_fernet_key = var.airflow_fernet_key != "" ? var.airflow_fernet_key : random_password.airflow_fernet_key.result
  postgres_host = google_sql_database_instance.postgres.public_ip_address
  postgres_port = "5432"
  postgres_database = google_sql_database.db.name
  postgres_user = google_sql_user.users.name
  postgres_password = random_password.db_password.result
  {% else %}
  # Use SQLite for default backend
  backend_store_uri = "sqlite:///mlflow.db"
  {% endif %}



  default_startup_script = <<-EOF
    #!/bin/bash
    set -e
    
    echo "Starting MLflow VM setup{{ ' with PostgreSQL backend' if flags.needs_postgres else '' }}..."
    
    # Log all output to a file for debugging
    exec > >(tee /var/log/mlflow-startup.log) 2>&1
    
    echo "$(date): Starting MLflow VM setup..."
    
    # Detect target deploy user (metadata ssh-keys -> /home -> fallback)
    TARGET_USER=""
    META_SSH=$(curl -sf -H "Metadata-Flavor: Google" \
      http://metadata.google.internal/computeMetadata/v1/instance/attributes/ssh-keys || true)
    if [ -n "$META_SSH" ]; then
      TARGET_USER=$(echo "$META_SSH" | head -n1 | cut -d: -f1)
      echo "Detected user from metadata ssh-keys: $TARGET_USER"
    fi
    if [ -z "$TARGET_USER" ]; then
      CANDIDATE=$(ls -1 /home 2>/dev/null | grep -v '^root$' | head -n1 || true)
      if [ -n "$CANDIDATE" ] && getent passwd "$CANDIDATE" >/dev/null 2>&1; then
        TARGET_USER="$CANDIDATE"
        echo "Detected user from /home: $TARGET_USER"
      fi
    fi
    if [ -z "$TARGET_USER" ]; then
      TARGET_USER="deployml"
      echo "No user detected, defaulting to: $TARGET_USER"
    fi
    echo "Target VM user: $TARGET_USER"

    # Ensure the user exists and has sudo and docker access
    if ! id -u "$TARGET_USER" >/dev/null 2>&1; then
      useradd -m -s /bin/bash "$TARGET_USER"
    fi
    apt-get update -y
    apt-get install -y sudo >/dev/null 2>&1 || true
    usermod -aG sudo "$TARGET_USER" || true
    echo "$TARGET_USER ALL=(ALL) NOPASSWD:ALL" > /etc/sudoers.d/90-$TARGET_USER
    chmod 440 /etc/sudoers.d/90-$TARGET_USER
    
    # Update system packages
    echo "Updating system packages..."
    sudo apt-get update -y
    
    # Install necessary packages for Docker and Python
    echo "Installing dependencies..."
    sudo apt-get install -y \
      apt-transport-https \
      ca-certificates \
      curl \
      gnupg \
      lsb-release \
      software-properties-common \
      python3 \
      python3-pip \
      python3-venv \
      python3-dev \
      build-essential \
      git \
      wget \
      unzip{% if flags.needs_postgres %} \
      postgresql-client{% endif %}
    
    # Verify Python and pip are available
    echo "Verifying Python installation..."
    python3 --version
    pip3 --version
    
    # Add Docker's official GPG key
    echo "Adding Docker GPG key..."
    curl -fsSL https://download.docker.com/linux/debian/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg
    
    # Set up Docker repository
    echo "Setting up Docker repository..."
    echo "deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/debian $(lsb_release -cs) stable" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null
    
    # Update packages and install Docker
    echo "Installing Docker..."
    sudo apt-get update -y
    sudo apt-get install -y docker-ce docker-ce-cli containerd.io docker-compose-plugin
    
    # Start and enable Docker
    echo "Starting Docker service..."
    sudo systemctl enable docker
    sudo systemctl start docker
    
    # Add current user to docker group
    echo "Configuring Docker permissions for user: $TARGET_USER"
    sudo usermod -aG docker $TARGET_USER
    
    # Wait for Docker to be ready
    echo "Waiting for Docker to be ready..."
    sleep 10
    
    # Test Docker installation
    echo "Testing Docker installation..."
    sudo docker run --rm hello-world
    
    # Set up containerized MLflow environment
    echo "Setting up containerized MLflow environment..."
    
    # Create deployment directory structure
    echo "Creating deployment directory structure..."
    mkdir -p /home/$TARGET_USER/deployml/docker
    mkdir -p /home/$TARGET_USER/deployml/docker/mlflow
    mkdir -p /home/$TARGET_USER/deployml/docker/fastapi
    {% if flags.needs_feast %}
    mkdir -p /home/$TARGET_USER/deployml/docker/feast
    {% endif %}
    {% if flags.needs_airflow %}
    mkdir -p /home/$TARGET_USER/deployml/docker/airflow
    {% endif %}
    
    echo "âœ… Directory structure created:"
    echo "   - /home/$TARGET_USER/deployml/docker"
    echo "   - /home/$TARGET_USER/deployml/docker/mlflow"
    echo "   - /home/$TARGET_USER/deployml/docker/fastapi"
    {% if flags.needs_feast %}
    echo "   - /home/$TARGET_USER/deployml/docker/feast"
    {% endif %}
    {% if flags.needs_airflow %}
    echo "   - /home/$TARGET_USER/deployml/docker/airflow"
    {% endif %}

    {% if flags.use_local_postgres %}
    # Install and configure local PostgreSQL
    echo "Setting up local PostgreSQL database..."
    sudo apt-get install -y postgresql postgresql-contrib
    
    # Start PostgreSQL service
    sudo systemctl enable postgresql
    sudo systemctl start postgresql
    
    # Create databases and users
    echo "Creating local PostgreSQL databases and users..."
    sudo -u postgres psql -c "CREATE USER mlflow WITH PASSWORD '{{ '${random_password.db_password.result}' }}';"
    sudo -u postgres psql -c "CREATE DATABASE mlflow OWNER mlflow;"
    {% if flags.needs_feast %}
    sudo -u postgres psql -c "CREATE USER feast WITH PASSWORD '{{ '${random_password.db_password.result}' }}';"
    sudo -u postgres psql -c "CREATE DATABASE feast OWNER feast;"
    {% endif %}
    {% if flags.needs_airflow %}
    sudo -u postgres psql -c "CREATE USER airflow WITH PASSWORD '{{ '${random_password.db_password.result}' }}';"
    sudo -u postgres psql -c "CREATE DATABASE airflow OWNER airflow;"
    {% endif %}
    
    echo "âœ… Local PostgreSQL setup completed"
    {% endif %}

    {% if flags.needs_feast and flags.use_cloudsql %}
    # Grant MLflow user access to FEAST database for cross-database operations (Cloud SQL)
    echo "Setting up Cloud SQL database permissions for MLflow and FEAST..."
    
    # Wait for PostgreSQL to be ready
    echo "Waiting for Cloud SQL PostgreSQL to be ready for permission setup..."
    until pg_isready -h {{ '${google_sql_database_instance.postgres.public_ip_address}' }} -p 5432 -U {{ flags.first_tool_name }}; do
        echo "Cloud SQL PostgreSQL not ready yet, waiting..."
        sleep 5
    done
    
    # Grant MLflow user access to FEAST database
    echo "Granting MLflow user access to FEAST database..."
    PGPASSWORD={{ '${random_password.db_password.result}' }} psql -h {{ '${google_sql_database_instance.postgres.public_ip_address}' }} -p 5432 -U {{ flags.first_tool_name }} -d {{ '${var.feast_database_name}' }} -c "
    GRANT CONNECT ON DATABASE {{ '${var.feast_database_name}' }} TO {{ flags.first_tool_name }};
    GRANT USAGE ON SCHEMA public TO {{ flags.first_tool_name }};
    GRANT CREATE ON SCHEMA public TO {{ flags.first_tool_name }};
    GRANT ALL PRIVILEGES ON ALL TABLES IN SCHEMA public TO {{ flags.first_tool_name }};
    GRANT ALL PRIVILEGES ON ALL SEQUENCES IN SCHEMA public TO {{ flags.first_tool_name }};
    ALTER DEFAULT PRIVILEGES IN SCHEMA public GRANT ALL ON TABLES TO {{ flags.first_tool_name }};
    ALTER DEFAULT PRIVILEGES IN SCHEMA public GRANT ALL ON SEQUENCES TO {{ flags.first_tool_name }};
    " || echo "Warning: Could not grant permissions (this may be normal if FEAST database doesn't exist yet)"
    
    # Grant FEAST user access to MLflow database (for potential cross-database operations)
    echo "Granting FEAST user access to MLflow database..."
    PGPASSWORD={{ '${random_password.db_password.result}' }} psql -h {{ '${google_sql_database_instance.postgres.public_ip_address}' }} -p 5432 -U {{ '${var.feast_database_user}' }} -d {{ flags.first_tool_name }} -c "
    GRANT CONNECT ON DATABASE {{ flags.first_tool_name }} TO {{ '${var.feast_database_user}' }};
    GRANT USAGE ON SCHEMA public TO {{ '${var.feast_database_user}' }};
    GRANT SELECT ON ALL TABLES IN SCHEMA public TO {{ '${var.feast_database_user}' }};
    " || echo "Warning: Could not grant permissions (this may be normal if MLflow database doesn't exist yet)"
    
    echo "Cloud SQL database permissions setup completed"
    {% endif %}

    {% if flags.needs_airflow and flags.use_cloudsql %}
    # Grant MLflow user access to Airflow database for cross-database operations (Cloud SQL)
    echo "Setting up Cloud SQL database permissions for MLflow and Airflow..."
    
    # Grant MLflow user access to Airflow database
    echo "Granting MLflow user access to Airflow database..."
    PGPASSWORD={{ '${random_password.db_password.result}' }} psql -h {{ '${google_sql_database_instance.postgres.public_ip_address}' }} -p 5432 -U {{ flags.first_tool_name }} -d {{ '${var.airflow_database_name}' }} -c "
    GRANT CONNECT ON DATABASE {{ '${var.airflow_database_name}' }} TO {{ flags.first_tool_name }};
    GRANT USAGE ON SCHEMA public TO {{ flags.first_tool_name }};
    GRANT SELECT ON ALL TABLES IN SCHEMA public TO {{ flags.first_tool_name }};
    " || echo "Warning: Could not grant permissions (this may be normal if Airflow database doesn't exist yet)"
    
    # Grant Airflow user access to MLflow database (for potential cross-database operations)
    echo "Granting Airflow user access to MLflow database..."
    PGPASSWORD={{ '${random_password.db_password.result}' }} psql -h {{ '${google_sql_database_instance.postgres.public_ip_address}' }} -p 5432 -U {{ '${var.airflow_database_user}' }} -d {{ flags.first_tool_name }} -c "
    GRANT CONNECT ON DATABASE {{ flags.first_tool_name }} TO {{ '${var.airflow_database_user}' }};
    GRANT USAGE ON SCHEMA public TO {{ '${var.airflow_database_user}' }};
    GRANT SELECT ON ALL TABLES IN SCHEMA public TO {{ flags.first_tool_name }};
    " || echo "Warning: Could not grant permissions (this may be normal if MLflow database doesn't exist yet)"
    
    echo "Cloud SQL database permissions setup completed"
    {% endif %}

{% if flags.needs_feast %}
    # Create Feast environment configuration with actual PostgreSQL credentials
    echo "Setting up Feast environment configuration..."
    cat > /home/$TARGET_USER/deployml/docker/feast_environment.env << 'FEAST_ENV_EOF'
FEAST_REGISTRY_TYPE={{ 'sql' if flags.use_cloudsql or flags.use_local_postgres else 'file' }}
FEAST_REGISTRY_PATH={% if flags.use_cloudsql %}postgresql+psycopg://{{ '${var.feast_database_user}' }}:{{ '${urlencode(random_password.db_password.result)}' }}@{{ '${google_sql_database_instance.postgres.public_ip_address}' }}:5432/{{ '${var.feast_database_name}' }}{% elif flags.use_local_postgres %}postgresql+psycopg://{{ '${var.feast_database_user}' }}:{{ '${urlencode(random_password.db_password.result)}' }}@localhost:5432/{{ '${var.feast_database_name}' }}{% else %}data/registry.db{% endif %}
FEAST_ONLINE_STORE_TYPE={{ 'postgres' if flags.use_cloudsql or flags.use_local_postgres else 'sqlite' }}
{% if flags.use_cloudsql %}
FEAST_ONLINE_STORE_HOST={{ '${google_sql_database_instance.postgres.public_ip_address}' }}
FEAST_ONLINE_STORE_PORT=5432
FEAST_ONLINE_STORE_DATABASE={{ '${var.feast_database_name}' }}
FEAST_ONLINE_STORE_USER={{ '${var.feast_database_user}' }}
FEAST_ONLINE_STORE_PASSWORD={{ '${random_password.db_password.result}' }}
{% elif flags.use_local_postgres %}
FEAST_ONLINE_STORE_HOST=localhost
FEAST_ONLINE_STORE_PORT=5432
FEAST_ONLINE_STORE_DATABASE={{ '${var.feast_database_name}' }}
FEAST_ONLINE_STORE_USER={{ '${var.feast_database_user}' }}
FEAST_ONLINE_STORE_PASSWORD={{ '${random_password.db_password.result}' }}
{% endif %}
FEAST_OFFLINE_STORE_TYPE=bigquery
FEAST_OFFLINE_STORE_PROJECT={{ project_id }}
FEAST_OFFLINE_STORE_DATASET={{ flags.feast_params.get('bigquery_dataset', 'feast_offline_store_' ~ stack_name ~ '_' ~ project_id ~ '_' ~ name_hash) }}
FEAST_ARTIFACT_BUCKET={% if bucket_configs %}{% for config in bucket_configs %}{% if config.create %}{{ config.bucket_name }}{% endif %}{% endfor %}{% else %}{% endif %}
GOOGLE_CLOUD_PROJECT={{ project_id }}
USE_POSTGRES={{ 'true' if flags.use_cloudsql or flags.use_local_postgres else 'false' }}
FEAST_PORT={{ flags.feast_params.get('feast_port', 6566) }}
FEAST_PROJECT={{ flags.feast_params.get('project', (stack_name ~ '_' ~ project_id ~ '_' ~ name_hash) | replace('-', '_')) }}
FEAST_ENV_EOF
    chown $TARGET_USER:$TARGET_USER /home/$TARGET_USER/deployml/docker/feast_environment.env
{% endif %}

{% if flags.needs_airflow %}
    # Create Airflow environment configuration
    echo "Setting up Airflow environment configuration..."
    
    # Force PostgreSQL usage when Airflow is enabled and PostgreSQL is configured
    {% if flags.use_cloudsql or flags.use_local_postgres %}
    echo "ðŸ”§ Airflow will use PostgreSQL database"
    {% else %}
    echo "âš ï¸  WARNING: Airflow is enabled but PostgreSQL is not configured!"
    echo "   This will cause Airflow to use SQLite, which may cause issues."
    {% endif %}
    
    cat > /home/$TARGET_USER/deployml/docker/airflow_environment.env << 'AIRFLOW_ENV_EOF'
AIRFLOW__CORE__EXECUTOR={{ flags.airflow_params.get('airflow_executor', 'LocalExecutor') }}
{% if flags.use_cloudsql %}
AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://{{ '${google_sql_user.airflow_user.name}' }}:{{ '${urlencode(random_password.db_password.result)}' }}@{{ '${google_sql_database_instance.postgres.public_ip_address}' }}:5432/{{ '${google_sql_database.airflow_db.name}' }}
{% elif flags.use_local_postgres %}
AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:{{ '${random_password.db_password.result}' }}@localhost:5432/airflow
{% else %}
AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=sqlite:////opt/airflow/airflow.db
{% endif %}
AIRFLOW__CORE__FERNET_KEY={{ '${random_password.airflow_fernet_key.result}' }}
AIRFLOW__CORE__SECRET_KEY={{ '${random_password.airflow_secret_key.result}' }}
AIRFLOW__CORE__LOAD_EXAMPLES=False
AIRFLOW__API__SECRET_KEY={{ '${random_password.airflow_secret_key.result}' }}
# RBAC is handled by Simple Auth Manager in Airflow 3.0+
AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION=True
AIRFLOW__CORE__PARALLELISM={{ flags.airflow_params.get('airflow_parallelism', 4) }}
AIRFLOW__CORE__DAG_CONCURRENCY={{ flags.airflow_params.get('airflow_dag_concurrency', 2) }}
AIRFLOW__CORE__MAX_ACTIVE_RUNS_PER_DAG={{ flags.airflow_params.get('airflow_max_active_runs_per_dag', 2) }}
AIRFLOW__API__WORKERS={{ flags.airflow_params.get('airflow_webserver_workers', 2) }}
AIRFLOW__API__AUTHENTICATE=True
AIRFLOW__API__AUTH_BACKEND=airflow.auth.backend.basic_auth
AIRFLOW__API__AUTH_MANAGER=airflow.auth.manager.basic_auth.BasicAuthManager
AIRFLOW__API__SESSION_TIMEOUT=3600
AIRFLOW_ENV_EOF
    chown $TARGET_USER:$TARGET_USER /home/$TARGET_USER/deployml/docker/airflow_environment.env
    
    echo "âœ… Airflow environment file created at /home/$TARGET_USER/deployml/docker/airflow_environment.env"
    echo "ðŸ“ File size: $(wc -c < /home/$TARGET_USER/deployml/docker/airflow_environment.env) bytes"
    echo "ðŸ”§ Airflow will use: {% if flags.use_cloudsql or flags.use_local_postgres %}PostgreSQL{% else %}SQLite{% endif %} database"
    
    # Show the actual database connection string that will be used
    echo "ðŸ” Database connection details:"
    {% if flags.use_cloudsql %}
    echo "   Type: PostgreSQL (Cloud SQL)"
    echo "   Host: {{ '${google_sql_database_instance.postgres.public_ip_address}' }}"
    echo "   Port: 5432"
    echo "   Database: {{ '${google_sql_database.airflow_db.name}' }}"
    echo "   User: {{ '${google_sql_user.airflow_user.name}' }}"
    {% elif flags.use_local_postgres %}
    echo "   Type: PostgreSQL (Local)"
    echo "   Host: localhost"
    echo "   Port: 5432"
    echo "   Database: airflow"
    echo "   User: airflow"
    {% else %}
    echo "   Type: SQLite"
    echo "   Path: /opt/airflow/airflow.db"
    {% endif %}
{% endif %}

{% if flags.needs_airflow %}
    # Set up Airflow directory and permissions
    echo "Setting up Airflow directory and permissions..."
    sudo mkdir -p /opt/airflow
    sudo chown -R 50000:50000 /opt/airflow
    sudo chmod -R 755 /opt/airflow
    echo "âœ… Airflow directory setup completed"
    
    {% if flags.needs_postgres %}
    # Wait for PostgreSQL to be ready for Airflow
    echo "Waiting for PostgreSQL to be ready for Airflow..."
    echo "ðŸ” Checking PostgreSQL connection for Airflow..."
    echo "   Host: {{ '${google_sql_database_instance.postgres.public_ip_address}' }}"
    echo "   Port: 5432"
    echo "   User: {{ '${google_sql_user.airflow_user.name}' }}"
    echo "   Database: {{ '${google_sql_database.airflow_db.name}' }}"
    
    until pg_isready -h {{ '${google_sql_database_instance.postgres.public_ip_address}' }} -p 5432 -U {{ '${google_sql_user.airflow_user.name}' }}; do
        echo "PostgreSQL not ready yet for Airflow, waiting..."
        sleep 5
    done
    echo "âœ… PostgreSQL is ready for Airflow!"
    {% else %}
    echo "âš ï¸  WARNING: PostgreSQL not configured for Airflow - will use SQLite!"
    echo "   This may cause issues with multi-container deployments."
    {% endif %}
{% endif %}
    
    # Create Docker Compose file
    echo "Creating Docker Compose configuration..."
    cat > /home/$TARGET_USER/deployml/docker/docker-compose.yml << 'DOCKER_COMPOSE_EOF'
version: '3.8'

services:
  mlflow:
    build: 
      context: ./mlflow
      dockerfile: Dockerfile
    container_name: mlflow-server
    ports:
      - "{{ flags.mlflow_params.mlflow_port }}:5000"
    environment:
      - MLFLOW_BACKEND_STORE_URI={% if flags.needs_postgres %}postgresql+psycopg2://{{ flags.first_tool_name }}:{{ '${urlencode(random_password.db_password.result)}' }}@{{ '${google_sql_database_instance.postgres.public_ip_address}' }}:5432/{{ '${google_sql_database.db.name}' }}{% else %}sqlite:///mlflow.db{% endif %}
      - MLFLOW_DEFAULT_ARTIFACT_ROOT={% if bucket_configs %}{% for config in bucket_configs %}{% if config.create %}gs://{{ config.bucket_name }}{% endif %}{% endfor %}{% else %}./mlflow-artifacts{% endif %}
      - MLFLOW_SERVER_HOST=0.0.0.0
      - MLFLOW_SERVER_PORT=5000
    volumes:
      - mlflow-data:/app/mlflow-data
      - mlflow-config:/app/mlflow-config
    networks:
      - mlflow-network
    restart: unless-stopped
    # Wait for PostgreSQL to be ready before starting MLflow (when using PostgreSQL)
    entrypoint: ["/bin/bash"]
    command: ["-c", "{% if flags.needs_postgres %}echo 'Waiting for PostgreSQL to be ready...' && until pg_isready -h {{ '${google_sql_database_instance.postgres.public_ip_address}' }} -p 5432 -U {{ flags.first_tool_name }}; do echo 'PostgreSQL not ready yet, waiting...' && sleep 5; done && echo 'PostgreSQL is ready! Starting MLflow...' && {% endif %}mlflow server --host 0.0.0.0 --port 5000 --backend-store-uri {% if flags.needs_postgres %}postgresql+psycopg2://{{ flags.first_tool_name }}:{{ '${urlencode(random_password.db_password.result)}' }}@{{ '${google_sql_database_instance.postgres.public_ip_address}' }}:5432/{{ '${google_sql_database.db.name}' }}{% else %}sqlite:///mlflow.db{% endif %} --default-artifact-root {% if bucket_configs %}{% for config in bucket_configs %}{% if config.create %}gs://{{ config.bucket_name }}{% endif %}{% endfor %}{% else %}./mlflow-artifacts{% endif %}"]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    
  fastapi:
    build:
      context: ./fastapi
      dockerfile: Dockerfile
    container_name: fastapi-proxy
    ports:
      - "{{ flags.mlflow_params.get('fastapi_port', 8000) }}:8000"
    environment:
      - MLFLOW_BASE_URL=http://mlflow:5000
      - MLFLOW_EXTERNAL_URL=$${EXTERNAL_MLFLOW_URL}
      - FASTAPI_PORT=8000
    depends_on:
      - mlflow
    networks:
      - mlflow-network
    restart: unless-stopped
{% if flags.needs_grafana %}

  grafana:
    image: grafana/grafana:latest
    container_name: grafana-server
    ports:
      - "{{ flags.grafana_params.get('grafana_port', 3000) }}:3000"
    environment:
      - GF_SECURITY_ADMIN_USER={{ flags.grafana_params.get('grafana_admin_user', 'admin') }}
      - GF_SECURITY_ADMIN_PASSWORD={{ flags.grafana_params.get('grafana_admin_password', 'admin') }}
      - GF_USERS_ALLOW_SIGN_UP=false
      - GF_SERVER_ROOT_URL=http://localhost:3000
      - GF_INSTALL_PLUGINS=grafana-clock-panel,grafana-simple-json-datasource
    volumes:
      - grafana-data:/var/lib/grafana
      - grafana-config:/etc/grafana
      - grafana-logs:/var/log/grafana
    networks:
      - mlflow-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3000/api/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
{% endif %}
{% if flags.needs_feast %}

  feast:
    build:
      context: ./feast
      dockerfile: Dockerfile
    container_name: feast-server
    ports:
      - "{{ flags.feast_params.get('feast_port', 6566) }}:6566"
    env_file:
      - ./feast_environment.env
    volumes:
      - feast-data:/app/feast-data
      - feast-config:/app/feast-config
    networks:
      - mlflow-network
    restart: unless-stopped
    entrypoint: ["/entrypoint.sh"]
    depends_on:
      mlflow:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "nc", "-z", "localhost", "6566"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
{% endif %}

{% if flags.needs_feast %}

  feast-fastapi:
    build:
      context: ./feast-fastapi
      dockerfile: Dockerfile
    container_name: feast-fastapi-server
    ports:
      - "9000:8000"
    env_file:
      - ./feast_fastapi_environment.env
    volumes:
      - feast-fastapi-data:/app/feast-fastapi-data
    networks:
      - mlflow-network
    restart: unless-stopped
    depends_on:
      feast:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
{% endif %}

{% if flags.needs_airflow %}

  airflow-webserver:
    image: apache/airflow:3.0.4
    container_name: airflow-webserver
    ports:
      - "{{ flags.airflow_params.get('airflow_port', 8080) }}:8080"
    environment:
      - AIRFLOW__CORE__EXECUTOR={{ flags.airflow_params.get('airflow_executor', 'LocalExecutor') }}
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN={% if flags.use_cloudsql %}postgresql+psycopg2://{{ '${google_sql_user.airflow_user.name}' }}:{{ '${urlencode(random_password.db_password.result)}' }}@{{ '${google_sql_database_instance.postgres.public_ip_address}' }}:5432/{{ '${google_sql_database.airflow_db.name}' }}{% elif flags.use_local_postgres %}postgresql+psycopg2://airflow:{{ '${urlencode(random_password.db_password.result)}' }}@localhost:5432/airflow{% else %}sqlite:////opt/airflow/airflow.db{% endif %}
      - AIRFLOW__CORE__FERNET_KEY={{ '${random_password.airflow_fernet_key.result}' }}
      - AIRFLOW__CORE__SECRET_KEY={{ '${random_password.airflow_secret_key.result}' }}
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      - AIRFLOW__API__SECRET_KEY={{ '${random_password.airflow_secret_key.result}' }}
      # RBAC is handled by Simple Auth Manager in Airflow 3.0+
      - AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION=True
      - AIRFLOW__CORE__PARALLELISM={{ flags.airflow_params.get('airflow_parallelism', 4) }}
      - AIRFLOW__CORE__DAG_CONCURRENCY={{ flags.airflow_params.get('airflow_dag_concurrency', 2) }}
      - AIRFLOW__CORE__MAX_ACTIVE_RUNS_PER_DAG={{ flags.airflow_params.get('airflow_max_active_runs_per_dag', 2) }}
      - AIRFLOW__API__WORKERS={{ flags.airflow_params.get('airflow_webserver_workers', 2) }}
      # Airflow 3.x uses Simple Auth Manager by default
      # User creation is handled automatically through simple_auth_manager_users
      - AIRFLOW__CORE__SIMPLE_AUTH_MANAGER_USERS=admin:admin
    volumes:
      - /opt/airflow:/opt/airflow
    networks:
      - mlflow-network
    restart: unless-stopped
    depends_on:
      - airflow-init
    command: ["airflow", "api-server"]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/"]  # Root endpoint for health check
      interval: 120s
      timeout: 60s
      retries: 10
      start_period: 300s

  airflow-scheduler:
    image: apache/airflow:3.0.4
    container_name: airflow-scheduler
    environment:
      - AIRFLOW__CORE__EXECUTOR={{ flags.airflow_params.get('airflow_executor', 'LocalExecutor') }}
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN={% if flags.use_cloudsql %}postgresql+psycopg2://{{ '${google_sql_user.airflow_user.name}' }}:{{ '${urlencode(random_password.db_password.result)}' }}@{{ '${google_sql_database_instance.postgres.public_ip_address}' }}:5432/{{ '${google_sql_database.airflow_db.name}' }}{% elif flags.use_local_postgres %}postgresql+psycopg2://airflow:{{ '${urlencode(random_password.db_password.result)}' }}@localhost:5432/airflow{% else %}sqlite:////opt/airflow/airflow.db{% endif %}
      - AIRFLOW__CORE__FERNET_KEY={{ '${random_password.airflow_fernet_key.result}' }}
      - AIRFLOW__CORE__SECRET_KEY={{ '${random_password.airflow_secret_key.result}' }}
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      - AIRFLOW__API__SECRET_KEY={{ '${random_password.airflow_secret_key.result}' }}
      # RBAC is handled by Simple Auth Manager in Airflow 3.0+
      - AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION=True
      - AIRFLOW__CORE__PARALLELISM={{ flags.airflow_params.get('airflow_parallelism', 4) }}
      - AIRFLOW__CORE__DAG_CONCURRENCY={{ flags.airflow_params.get('airflow_dag_concurrency', 2) }}
      - AIRFLOW__CORE__MAX_ACTIVE_RUNS_PER_DAG={{ flags.airflow_params.get('airflow_max_active_runs_per_dag', 2) }}
      - AIRFLOW__API__WORKERS={{ flags.airflow_params.get('airflow_webserver_workers', 2) }}
      # Airflow 3.x uses Simple Auth Manager by default
      # User creation is handled automatically through simple_auth_manager_users
      - AIRFLOW__CORE__SIMPLE_AUTH_MANAGER_USERS=admin:admin
    volumes:
      - /opt/airflow:/opt/airflow
    networks:
      - mlflow-network
    restart: unless-stopped
    depends_on:
      - airflow-init
    command: ["airflow", "scheduler"]
    healthcheck:
      test: ["CMD", "airflow", "version"]
      interval: 120s
      timeout: 60s
      retries: 10
      start_period: 300s

  airflow-init:
    image: apache/airflow:3.0.4
    container_name: airflow-init
    environment:
      - AIRFLOW__CORE__EXECUTOR={{ flags.airflow_params.get('airflow_executor', 'LocalExecutor') }}
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN={% if flags.use_cloudsql %}postgresql+psycopg2://{{ '${google_sql_user.airflow_user.name}' }}:{{ '${urlencode(random_password.db_password.result)}' }}@{{ '${google_sql_database_instance.postgres.public_ip_address}' }}:5432/{{ '${google_sql_database.airflow_db.name}' }}{% elif flags.use_local_postgres %}postgresql+psycopg2://airflow:{{ '${urlencode(random_password.db_password.result)}' }}@localhost:5432/airflow{% else %}sqlite:////opt/airflow/airflow.db{% endif %}
      - AIRFLOW__CORE__FERNET_KEY={{ '${random_password.airflow_fernet_key.result}' }}
      - AIRFLOW__CORE__SECRET_KEY={{ '${random_password.airflow_secret_key.result}' }}
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      - AIRFLOW__API__SECRET_KEY={{ '${random_password.airflow_secret_key.result}' }}
      # RBAC is handled by Simple Auth Manager in Airflow 3.0+
      - AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION=True
      - AIRFLOW__CORE__PARALLELISM={{ flags.airflow_params.get('airflow_parallelism', 4) }}
      - AIRFLOW__CORE__DAG_CONCURRENCY={{ flags.airflow_params.get('airflow_dag_concurrency', 2) }}
      - AIRFLOW__CORE__MAX_ACTIVE_RUNS_PER_DAG={{ flags.airflow_params.get('airflow_max_active_runs_per_dag', 2) }}
      - AIRFLOW__API__WORKERS={{ flags.airflow_params.get('airflow_webserver_workers', 2) }}
      # Airflow 3.x uses Simple Auth Manager by default
      # User creation is handled automatically through simple_auth_manager_users
      - AIRFLOW__CORE__SIMPLE_AUTH_MANAGER_USERS=admin:admin
      - _AIRFLOW_DB_MIGRATE=true
    volumes:
      - /opt/airflow:/opt/airflow
    networks:
      - mlflow-network
    restart: "no"
    depends_on: []
    entrypoint: "/bin/bash"
    command: ["-c", "airflow db migrate"]
{% endif %}

volumes:
  mlflow-data:
  mlflow-config:{% if flags.needs_grafana %}
  grafana-data:
  grafana-config:
  grafana-logs:{% endif %}{% if flags.needs_feast %}
  feast-data:
  feast-config:{% endif %}{% if flags.needs_feast %}
  feast-fastapi-data:{% endif %}

networks:
  mlflow-network:
    driver: bridge
DOCKER_COMPOSE_EOF

    echo "âœ… Docker Compose file created at /home/$TARGET_USER/deployml/docker/docker-compose.yml"
    echo "ðŸ“ File size: $(wc -c < /home/$TARGET_USER/deployml/docker/docker-compose.yml) bytes"

    # Create MLflow Dockerfile
    echo "Creating MLflow Dockerfile..."
    cat > /home/$TARGET_USER/deployml/docker/mlflow/Dockerfile << 'MLFLOW_DOCKERFILE_EOF'
FROM python:3.9-slim-bullseye

# Set working directory
WORKDIR /app

# Install system dependencies
RUN apt-get update && apt-get install -y \
    curl \
    {% if flags.needs_postgres %}postgresql-client \{% endif %}
    && rm -rf /var/lib/apt/lists/*

# Install Python dependencies
RUN pip install --upgrade pip setuptools wheel

# Install MLflow and dependencies
RUN pip install \
    mlflow[extras] \
    sqlalchemy \
    {% if flags.needs_postgres %}psycopg2-binary \{% endif %}
    google-cloud-storage \
    boto3

# Create mlflow user
RUN useradd -m -s /bin/bash mlflow

# Create directories
RUN mkdir -p /app/mlflow-data /app/mlflow-config
RUN chown -R mlflow:mlflow /app

# Switch to mlflow user
USER mlflow

# Expose MLflow port
EXPOSE 5000

# Health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=60s --retries=3 \
    CMD curl -f http://localhost:5000/health || exit 1

# Default command
CMD ["mlflow", "server", "--host", "0.0.0.0", "--port", "5000"]
MLFLOW_DOCKERFILE_EOF

    # Create FastAPI Dockerfile
    echo "Creating FastAPI Dockerfile..."
    cat > /home/$TARGET_USER/deployml/docker/fastapi/Dockerfile << 'FASTAPI_DOCKERFILE_EOF'
FROM python:3.9-slim-bullseye

# Set working directory
WORKDIR /app

# Install system dependencies
RUN apt-get update && apt-get install -y \
    curl \
    && rm -rf /var/lib/apt/lists/*

# Install Python dependencies
RUN pip install --upgrade pip setuptools wheel

# Install FastAPI and dependencies
RUN pip install \
    fastapi \
    uvicorn \
    httpx \
    mlflow \
    pandas \
    joblib \
    scikit-learn \
    numpy \
    google-cloud-storage \
    google-cloud-core

# Create fastapi user
RUN useradd -m -s /bin/bash fastapi

# Create app directory
RUN mkdir -p /app/fastapi-app
RUN chown -R fastapi:fastapi /app

# Copy FastAPI application
COPY main.py /app/fastapi-app/main.py

# Switch to fastapi user
USER fastapi

# Expose FastAPI port
EXPOSE 8000

# Health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=30s --retries=3 \
    CMD curl -f http://localhost:8000/health || exit 1

# Default command
CMD ["uvicorn", "fastapi-app.main:app", "--host", "0.0.0.0", "--port", "8000"]
FASTAPI_DOCKERFILE_EOF

{% if flags.needs_feast %}
    # Create Feast Dockerfile
    echo "Creating Feast Dockerfile..."
    mkdir -p /home/$TARGET_USER/deployml/docker/feast
    cat > /home/$TARGET_USER/deployml/docker/feast/Dockerfile << 'FEAST_DOCKERFILE_EOF'
FROM python:3.9-slim-bullseye

# Set working directory
WORKDIR /app

# Install system dependencies
RUN apt-get update && apt-get install -y \
    curl \
    git \
    gcc \
    netcat-openbsd \
    gettext \
    gnupg \
    postgresql-client \
    && rm -rf /var/lib/apt/lists/*

# Install Google Cloud SDK
RUN echo "deb [signed-by=/usr/share/keyrings/cloud.google.gpg] https://packages.cloud.google.com/apt cloud-sdk main" | tee -a /etc/apt/sources.list.d/google-cloud-sdk.list && \
    curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key --keyring /usr/share/keyrings/cloud.google.gpg add - && \
    apt-get update && apt-get install -y google-cloud-cli && \
    rm -rf /var/lib/apt/lists/*

# Install Python dependencies
RUN pip install --upgrade pip setuptools wheel

# Install Feast and dependencies
RUN pip install \
    feast[gcp] \
    pandas \
    numpy \
    pyarrow \
    google-cloud-bigquery \
    google-cloud-storage \
    psycopg[binary] \
    psycopg-pool

# Create directories
RUN mkdir -p /app/feast-data /app/feast-config /app/feature_repo

# Copy feature repository
COPY feature_repo/ /app/feature_repo/

# Set environment variables
ENV PYTHONPATH=/app
ENV FEAST_REPO_PATH=/app/feature_repo

# Copy entrypoint script
COPY entrypoint.sh /entrypoint.sh
RUN chmod +x /entrypoint.sh

# Expose port
EXPOSE 6566

# Health check (Feast doesn't have /health endpoint, check if service is responding)
HEALTHCHECK --interval=30s --timeout=10s --start-period=60s --retries=3 \
    CMD nc -z localhost 6566 || exit 1

# Use entrypoint script as default
ENTRYPOINT ["/entrypoint.sh"]
FEAST_DOCKERFILE_EOF

    # Create Feast entrypoint script
    cat > /home/$TARGET_USER/deployml/docker/feast/entrypoint.sh << 'FEAST_ENTRYPOINT_EOF'
#!/bin/bash
set -e

echo "ðŸ½ï¸ Starting Feast container..."

# Use VM's default service account for GCP authentication
# The VM already has the necessary BigQuery and Storage permissions
echo "ðŸ”‘ Using VM service account for GCP authentication"
# Ensure we're using the default compute service account
gcloud auth list --filter=status:ACTIVE --format="value(account)" | head -n 1 > /tmp/active_account.txt
echo "Active account: $(cat /tmp/active_account.txt)"
# Set project explicitly
gcloud config set project {{ project_id }}
# Unset any explicit credentials to force use of default compute service account
unset GOOGLE_APPLICATION_CREDENTIALS
# The VM's default compute service account is automatically available
# No need for explicit authentication in containerized environment

# Wait for PostgreSQL to be available if using PostgreSQL
if [ "$USE_POSTGRES" = "true" ]; then
    echo "ðŸ” Waiting for PostgreSQL to be available..."
    until pg_isready -h "$FEAST_ONLINE_STORE_HOST" -p "$FEAST_ONLINE_STORE_PORT" -U "$FEAST_ONLINE_STORE_USER"; do
        echo "â³ PostgreSQL not ready yet, waiting..."
        sleep 5
    done
    echo "âœ… PostgreSQL is ready!"
fi

# Navigate to feature repo
cd /app/feature_repo

# Copy feature store configuration if template exists and substitute environment variables
if [ -f feature_store.yaml.template ]; then
    echo "ðŸ“ Configuring feature store with environment variables..."
    if command -v envsubst >/dev/null 2>&1; then
        envsubst < feature_store.yaml.template > feature_store.yaml
        echo "âœ… Feature store configuration updated with envsubst"
    else
        # Fallback: manual substitution using sed
        echo "âš ï¸ envsubst not available, using manual substitution..."
        cp feature_store.yaml.template feature_store.yaml
        sed -i "s|\$FEAST_PROJECT|$${FEAST_PROJECT:-deployml}|g" feature_store.yaml
        sed -i "s|\$FEAST_REGISTRY_PATH|$${FEAST_REGISTRY_PATH:-data/registry.db}|g" feature_store.yaml
        sed -i "s|\$FEAST_ONLINE_STORE_HOST|$${FEAST_ONLINE_STORE_HOST:-localhost}|g" feature_store.yaml
        sed -i "s|\$FEAST_ONLINE_STORE_PORT|$${FEAST_ONLINE_STORE_PORT:-5432}|g" feature_store.yaml
        sed -i "s|\$FEAST_ONLINE_STORE_DATABASE|$${FEAST_ONLINE_STORE_DATABASE:-feast}|g" feature_store.yaml
        sed -i "s|\$FEAST_ONLINE_STORE_USER|$${FEAST_ONLINE_STORE_USER:-feast}|g" feature_store.yaml
        sed -i "s|\$FEAST_ONLINE_STORE_PASSWORD|$${FEAST_ONLINE_STORE_PASSWORD:-}|g" feature_store.yaml
        sed -i "s|\$FEAST_OFFLINE_STORE_PROJECT|$${FEAST_OFFLINE_STORE_PROJECT:-}|g" feature_store.yaml
        sed -i "s|\$FEAST_OFFLINE_STORE_DATASET|$${FEAST_OFFLINE_STORE_DATASET:-feast_offline_store_{{ stack_name }}_{{ project_id }}_{{ name_hash }} }|g" feature_store.yaml
        echo "âœ… Feature store configuration updated with manual substitution"
    fi
fi

# Display current configuration (without sensitive data)
echo "ðŸ“‹ Current feature store configuration:"
grep -v "password\|PASSWORD" feature_store.yaml || true

# Apply feature definitions to registry with retry logic
echo "ðŸš€ Applying feature definitions..."
max_apply_retries=3
apply_retry_count=0

while [ $apply_retry_count -lt $max_apply_retries ]; do
    if feast apply --skip-source-validation; then
        echo "âœ… Feature definitions applied successfully"
        break
    else
        apply_retry_count=$((apply_retry_count + 1))
        echo "âš ï¸ Failed to apply feature definitions (attempt $apply_retry_count/$max_apply_retries)"
        if [ $apply_retry_count -lt $max_apply_retries ]; then
            echo "â³ Retrying in 10 seconds..."
            sleep 10
        else
            echo "âŒ Failed to apply feature definitions after $max_apply_retries attempts"
            echo "âš ï¸ Continuing with FEAST server startup anyway..."
            echo "ðŸ’¡ You can manually apply feature definitions later using: feast apply"
            break
        fi
    fi
done

# Start Feast server
echo "ðŸŒ Starting Feast server on 0.0.0.0:6566..."
exec feast serve --host 0.0.0.0 --port 6566
FEAST_ENTRYPOINT_EOF

    # Create Feast feature repository
    echo "Creating Feast feature repository..."
    mkdir -p /home/$TARGET_USER/deployml/docker/feast/feature_repo
    # Ensure package scaffolding for relative imports
    touch /home/$TARGET_USER/deployml/docker/feast/feature_repo/__init__.py
    mkdir -p /home/$TARGET_USER/deployml/docker/feast/feature_repo/features
    touch /home/$TARGET_USER/deployml/docker/feast/feature_repo/features/__init__.py
    
    # Create feature_store.yaml template that uses environment variables
    cat > /home/$TARGET_USER/deployml/docker/feast/feature_repo/feature_store.yaml.template << 'FEAST_CONFIG_EOF'
project: $FEAST_PROJECT
project_id: $FEAST_OFFLINE_STORE_PROJECT
# Registry configuration - uses environment variables
{% if flags.needs_postgres %}
registry:
  registry_type: sql
  path: $FEAST_REGISTRY_PATH
{% else %}
registry: data/registry.db
{% endif %}
provider: gcp
online_store:
{% if flags.needs_postgres %}
  type: postgres
  host: $FEAST_ONLINE_STORE_HOST
  port: $FEAST_ONLINE_STORE_PORT
  database: $FEAST_ONLINE_STORE_DATABASE
  db_schema: public
  user: $FEAST_ONLINE_STORE_USER
  password: "$FEAST_ONLINE_STORE_PASSWORD"
{% else %}
  type: sqlite
  path: data/online_store.db
{% endif %}
offline_store:
  type: bigquery
  project_id: $FEAST_OFFLINE_STORE_PROJECT
  dataset: $FEAST_OFFLINE_STORE_DATASET
entity_key_serialization_version: 3
FEAST_CONFIG_EOF

    # Create example feature definitions (only when sample_data=true)
    {% if flags.feast_params.get('sample_data', false) %}
    cat > /home/$TARGET_USER/deployml/docker/feast/feature_repo/example_features.py << 'FEAST_FEATURES_EOF'
from datetime import timedelta
import pandas as pd
from feast import (
    BigQuerySource,
    Entity,
    FeatureService,
    FeatureView,
    Field,
    RequestSource,
)
from feast.on_demand_feature_view import on_demand_feature_view
from feast.types import Float64, Int64

# Define an entity for the customer
customer = Entity(name="customer", join_keys=["customer_id"])

{% if flags.feast_params.get('sample_data', false) %}
# Define a data source from BigQuery for sample sales data
sample_sales_source = BigQuerySource(
    name="sample_sales_source",
    table="{{ project_id }}.{{ flags.feast_params.get('bigquery_dataset', 'feast_offline_store_' ~ stack_name ~ '_' ~ project_id ~ '_' ~ name_hash) }}.sample_sales_data",
    timestamp_field="sale_date",
    created_timestamp_column="created_timestamp",
)

# Feature views group features based on how they're stored
sample_sales_fv = FeatureView(
    name="sample_sales_features",
    entities=[customer],
    ttl=timedelta(weeks=52),
    schema=[
        Field(name="sale_amount", dtype=Float64),
        Field(name="quantity", dtype=Int64),
        Field(name="region", dtype=Float64),
        Field(name="customer_segment", dtype=Float64),
    ],
    source=sample_sales_source,
    tags={"team": "sales_analytics"},
)

# Define a request data source for real-time features
input_request = RequestSource(
    name="vals_to_add",
    schema=[
        Field(name="val_to_add", dtype=Int64),
        Field(name="val_to_add_2", dtype=Int64),
    ],
)

# On-demand feature view for real-time transformations
@on_demand_feature_view(
    sources=[sample_sales_fv, input_request],
    schema=[
        Field(name="sale_amount_plus_val1", dtype=Float64),
        Field(name="sale_amount_plus_val2", dtype=Float64),
    ],
)
def transformed_sale_amount(inputs: pd.DataFrame) -> pd.DataFrame:
    df = pd.DataFrame()
    df["sale_amount_plus_val1"] = inputs["sale_amount"] + inputs["val_to_add"]
    df["sale_amount_plus_val2"] = inputs["sale_amount"] + inputs["val_to_add_2"]
    return df

# Feature service for model serving
sales_activity_v1 = FeatureService(
    name="sales_activity_v1",
    features=[
        sample_sales_fv[["sale_amount", "quantity"]],
        transformed_sale_amount,
    ],
    )
    {% else %}
# Define a data source from BigQuery for driver data (fallback)
driver_stats_source = BigQuerySource(
    name="driver_hourly_stats_source",
    table="{{ project_id }}.{{ flags.feast_params.get('bigquery_dataset', 'feast_offline_store_' ~ stack_name ~ '_' ~ project_id ~ '_' ~ name_hash) }}.driver_hourly_stats",
    timestamp_field="event_timestamp",
    created_timestamp_column="created_timestamp",
)

# Feature views group features based on how they're stored
driver_stats_fv = FeatureView(
    name="driver_hourly_stats",
    entities=[customer],  # Using customer entity as fallback
    ttl=timedelta(weeks=52),
    schema=[
        Field(name="conv_rate", dtype=Float64),
        Field(name="acc_rate", dtype=Float64),
        Field(name="avg_daily_trips", dtype=Int64),
    ],
    source=driver_stats_source,
    tags={"team": "driver_performance"},
)

# Define a request data source for real-time features
input_request = RequestSource(
    name="vals_to_add",
    schema=[
        Field(name="val_to_add", dtype=Int64),
        Field(name="val_to_add_2", dtype=Int64),
    ],
)

# On-demand feature view for real-time transformations
@on_demand_feature_view(
    sources=[driver_stats_fv, input_request],
    schema=[
        Field(name="conv_rate_plus_val1", dtype=Float64),
        Field(name="conv_rate_plus_val2", dtype=Float64),
    ],
)
def transformed_conv_rate(inputs: pd.DataFrame) -> pd.DataFrame:
    df = pd.DataFrame()
    df["conv_rate_plus_val1"] = inputs["conv_rate"] + inputs["val_to_add"]
    df["conv_rate_plus_val2"] = inputs["conv_rate"] + inputs["val_to_add_2"]
    return df

# Feature service for model serving
driver_activity_v1 = FeatureService(
    name="driver_activity_v1",
    features=[
        driver_stats_fv[["conv_rate"]],
        transformed_conv_rate,
    ],
    )
    {% endif %}
    FEAST_FEATURES_EOF
    {% endif %}

    # Create FEAST FastAPI service
    echo "Creating FEAST FastAPI service..."
    mkdir -p /home/$TARGET_USER/deployml/docker/feast-fastapi
    
    # Create FEAST FastAPI Dockerfile
    cat > /home/$TARGET_USER/deployml/docker/feast-fastapi/Dockerfile << 'FEAST_FASTAPI_DOCKERFILE_EOF'
FROM python:3.10-slim

# Set work directory
WORKDIR /app

# Install system dependencies
RUN apt-get update && apt-get install -y \
    gcc \
    g++ \
    curl \
    && rm -rf /var/lib/apt/lists/*

# Install Python dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copy application code
COPY . .

# Set environment variables
ENV PYTHONPATH=/app
ENV FEAST_FASTAPI_PORT=8000

# Expose port
EXPOSE 8000

# Health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=60s --retries=3 \
    CMD curl -f http://localhost:8000/health || exit 1

# Use entrypoint script
COPY entrypoint.sh /entrypoint.sh
RUN chmod +x /entrypoint.sh

ENTRYPOINT ["/entrypoint.sh"]
FEAST_FASTAPI_DOCKERFILE_EOF

    # Create FEAST FastAPI requirements.txt
    cat > /home/$TARGET_USER/deployml/docker/feast-fastapi/requirements.txt << 'FEAST_FASTAPI_REQUIREMENTS_EOF'
fastapi==0.110.0
uvicorn==0.34.0
feast[gcp]==0.50.0
pandas==2.3.1
numpy==2.2.2
requests==2.32.3
psycopg2-binary==2.9.10
sqlalchemy==2.0.30
google-cloud-storage==2.17.0
google-cloud-bigquery==3.15.0
pydantic==2.10.6
FEAST_FASTAPI_REQUIREMENTS_EOF

    # Create FEAST FastAPI main application
    cat > /home/$TARGET_USER/deployml/docker/feast-fastapi/main.py << 'FEAST_FASTAPI_MAIN_EOF'
import os
from urllib.parse import quote_plus
import json
import logging
from datetime import datetime
from typing import Dict, List, Optional, Any
from contextlib import asynccontextmanager

from fastapi import FastAPI, HTTPException, Depends
from pydantic import BaseModel
import requests
import pandas as pd
import numpy as np

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Configuration
FEAST_SERVER_URL = os.getenv("FEAST_SERVER_URL", "http://feast-server:6566")
FEAST_FASTAPI_PORT = int(os.getenv("FEAST_FASTAPI_PORT", "8000"))
PROJECT_ID = "{{ project_id }}"

@asynccontextmanager
async def lifespan(app: FastAPI):
    # Startup
    logger.info("ðŸš€ Starting FEAST FastAPI service...")
    
    # Check FEAST server connection
    try:
        response = requests.get(f"{FEAST_SERVER_URL}/health", timeout=5)
        feast_available = response.status_code == 200
        logger.info(f"FEAST Server: {'âœ… Available' if feast_available else 'âŒ Unavailable'}")
    except Exception as e:
        logger.error(f"FEAST Server: âŒ Connection failed: {e}")
        feast_available = False
    
    if not feast_available:
        logger.warning("âš ï¸ FEAST server not available - some features may not work")
    
    logger.info("ðŸŽ¯ FEAST FastAPI Ready!")
    
    yield
    
    # Shutdown
    logger.info("ðŸ›‘ Shutting down FEAST FastAPI...")
    logger.info("âœ… Shutdown complete")

app = FastAPI(
    title="FEAST FastAPI Service",
    description="HTTP API wrapper for FEAST feature store",
    version="1.0.0",
    lifespan=lifespan
)

# =============================================================================
# PYDANTIC MODELS
# =============================================================================

class FeatureRequest(BaseModel):
    project: str
    entities: Dict[str, List[Any]]
    features: List[str]

class FeatureResponse(BaseModel):
    project: str
    status: str
    features: Dict[str, List[Any]]
    metadata: Dict[str, Any]

class ProjectInfo(BaseModel):
    name: str
    valid: bool
    feature_views: List[str]
    entities: List[str]

class HealthResponse(BaseModel):
    status: str
    feast_server_available: bool
    feast_server_url: str
    timestamp: str

# =============================================================================
# FEAST INTEGRATION
# =============================================================================

def call_feast_server(endpoint: str, data: Dict[str, Any]) -> Dict[str, Any]:
    """Make HTTP call to FEAST server"""
    try:
        # Pass project via query string to ensure Feast server uses it
        project_qs = ""
        try:
            project_value = data.get("project")
            if project_value:
                project_qs = f"?project={quote_plus(str(project_value))}"
        except Exception:
            project_qs = ""

        # Include header used by Feast server for project selection
        headers = {"Content-Type": "application/json"}
        if project_qs:
            try:
                headers["feast-project"] = str(data.get("project"))
            except Exception:
                pass

        response = requests.post(
            f"{FEAST_SERVER_URL}/{endpoint}{project_qs}",
            json=data,
            headers=headers,
            timeout=30
        )
        
        if response.status_code != 200:
            raise HTTPException(
                status_code=502,
                detail=f"FEAST server error: {response.status_code} - {response.text}"
            )
        
        return response.json()
        
    except requests.exceptions.RequestException as e:
        logger.error(f"FEAST server connection error: {e}")
        raise HTTPException(
            status_code=502,
            detail=f"Cannot connect to FEAST server: {str(e)}"
        )

# =============================================================================
# API ENDPOINTS
# =============================================================================

@app.get("/")
def root():
    """API information"""
    return {
        "service": "FEAST FastAPI Service",
        "version": "1.0.0",
        "feast_server_url": FEAST_SERVER_URL,
        "endpoints": {
            "health": "/health",
            "projects": "/projects",
            "get_features": "/get-online-features",
            "write_data": "/write-to-online-store",
            "materialize": "/materialize",
            "docs": "/docs"
        }
    }

@app.get("/health", response_model=HealthResponse)
def health_check():
    """Health check for the FEAST FastAPI service"""
    
    # Check FEAST server status
    feast_status = False
    try:
        response = requests.get(f"{FEAST_SERVER_URL}/health", timeout=5)
        feast_status = response.status_code == 200
    except:
        pass
    
    overall_status = feast_status
    
    return HealthResponse(
        status="healthy" if overall_status else "unhealthy",
        feast_server_available=feast_status,
        feast_server_url=FEAST_SERVER_URL,
        timestamp=datetime.now().isoformat()
    )

@app.get("/projects")
def list_projects():
    """List all available FEAST projects"""
    try:
        # Call FEAST server to get projects
        response = requests.get(f"{FEAST_SERVER_URL}/projects", timeout=10)
        
        if response.status_code == 200:
            data = response.json()
            return {
                "projects": data.get("projects", []),
                "total_projects": data.get("total_projects", 0),
                "timestamp": datetime.now().isoformat()
            }
        else:
            raise HTTPException(
                status_code=502,
                detail=f"FEAST server error: {response.status_code}"
            )
            
    except requests.exceptions.RequestException as e:
        logger.error(f"Error listing projects: {e}")
        raise HTTPException(
            status_code=502,
            detail=f"Cannot connect to FEAST server: {str(e)}"
        )

@app.get("/projects/{project_name}")
def get_project_info(project_name: str):
    """Get detailed information about a specific project"""
    try:
        response = requests.get(f"{FEAST_SERVER_URL}/projects/{project_name}", timeout=10)
        
        if response.status_code == 200:
            return response.json()
        elif response.status_code == 404:
            raise HTTPException(status_code=404, detail=f"Project '{project_name}' not found")
        else:
            raise HTTPException(
                status_code=502,
                detail=f"FEAST server error: {response.status_code}"
            )
            
    except requests.exceptions.RequestException as e:
        logger.error(f"Error getting project info: {e}")
        raise HTTPException(
            status_code=502,
            detail=f"Cannot connect to FEAST server: {str(e)}"
        )

@app.post("/get-online-features")
def get_online_features(request: FeatureRequest):
    """Get online features from FEAST and return Feast JSON unmodified"""
    project = request.project or os.getenv("FEAST_PROJECT")
    if not project:
        raise HTTPException(status_code=400, detail="Feast project not specified. Provide 'project' in request or set FEAST_PROJECT.")
    payload = {**request.dict(), "project": project}
    logger.info(f"Getting features for project: {project}")
    try:
        feast_response = call_feast_server("get-online-features", payload)
        return feast_response
    except Exception as e:
        logger.error(f"Error getting features: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to get features: {str(e)}")

@app.post("/write-to-online-store")
def write_to_online_store(
    project: str,
    feature_view_name: str,
    df: Dict[str, Any],
    allow_registry_cache: bool = True,
    transform_on_write: bool = True
):
    """Write data to FEAST online store"""
    
    logger.info(f"Writing data to project: {project}, feature_view: {feature_view_name}")
    
    try:
        data = {
            "project": project,
            "feature_view_name": feature_view_name,
            "df": df,
            "allow_registry_cache": allow_registry_cache,
            "transform_on_write": transform_on_write
        }
        
        feast_response = call_feast_server("write-to-online-store", data)
        
        return {
            "status": "success",
            "message": f"Data written to {project}:{feature_view_name}",
            "timestamp": datetime.now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"Error writing data: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"Failed to write data: {str(e)}"
        )

@app.post("/materialize")
def materialize_features(
    project: str,
    start_ts: str,
    end_ts: str,
    feature_views: Optional[List[str]] = None
):
    """Materialize features for a project"""
    
    logger.info(f"Materializing features for project: {project}")
    
    try:
        data = {
            "project": project,
            "start_ts": start_ts,
            "end_ts": end_ts,
            "feature_views": feature_views or []
        }
        
        feast_response = call_feast_server("materialize", data)
        
        return {
            "status": "success",
            "message": f"Features materialized for {project}",
            "timestamp": datetime.now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"Error materializing features: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"Failed to materialize features: {str(e)}"
        )

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=FEAST_FASTAPI_PORT)
FEAST_FASTAPI_MAIN_EOF

    # Create FEAST FastAPI entrypoint script
    cat > /home/$TARGET_USER/deployml/docker/feast-fastapi/entrypoint.sh << 'FEAST_FASTAPI_ENTRYPOINT_EOF'
#!/bin/bash
set -e

echo "ðŸ½ï¸ Starting FEAST FastAPI service..."

# Wait for FEAST server to be ready
echo "â³ Waiting for FEAST server to be ready..."
until curl -f $FEAST_SERVER_URL/health 2>/dev/null; do
    echo "FEAST server not ready yet, waiting..."
    sleep 5
done

echo "âœ… FEAST server is ready!"

# Start FEAST FastAPI service
echo "ðŸš€ Starting FEAST FastAPI on port 8000..."
exec uvicorn main:app --host 0.0.0.0 --port 8000
FEAST_FASTAPI_ENTRYPOINT_EOF

    # Create FEAST FastAPI environment configuration
    cat > /home/$TARGET_USER/deployml/docker/feast_fastapi_environment.env << 'FEAST_FASTAPI_ENV_EOF'
# FEAST FastAPI Environment Configuration
FEAST_SERVER_URL=http://feast:6566
FEAST_FASTAPI_PORT=8000
FEAST_PROJECT={{ flags.feast_params.get('project', (stack_name ~ '_' ~ project_id ~ '_' ~ name_hash) | replace('-', '_')) }}

## Minimal required vars only; avoid non-interpolated Terraform expressions
## FEAST FastAPI only needs to reach internal FEAST via Docker DNS
FEAST_FASTAPI_ENV_EOF

    # Set proper permissions
    chmod +x /home/$TARGET_USER/deployml/docker/feast-fastapi/entrypoint.sh


{% endif %}

    # Setup FastAPI application
    echo "Setting up FastAPI application..."
    FASTAPI_SOURCE="{{ flags.mlflow_params.get('fastapi_app_source', 'template') }}"
    
    if [ "$FASTAPI_SOURCE" = "template" ]; then
        echo "Using default containerized FastAPI template..."
        # Create a containerized FastAPI application with MLflow proxy and model integration
        cat > /home/$TARGET_USER/deployml/docker/fastapi/main.py << 'FASTAPI_TEMPLATE_EOF'
from fastapi import FastAPI, HTTPException, Request
from fastapi.responses import RedirectResponse, HTMLResponse
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import httpx
import os
from contextlib import asynccontextmanager
import logging
import asyncio
import mlflow
import pandas as pd
from datetime import datetime
from typing import Optional

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# MLflow configuration - use container name for inter-container communication
MLFLOW_BASE_URL = os.getenv("MLFLOW_BASE_URL", "http://mlflow:5000")
MLFLOW_EXTERNAL_URL = os.getenv("MLFLOW_EXTERNAL_URL", MLFLOW_BASE_URL)  # External URL for UI links
FASTAPI_PORT = int(os.getenv("FASTAPI_PORT", "8000"))

# Global variables for model
model = None
feature_names = None
model_info = {
    "name": None,
    "version": None,
    "loaded_at": None,
    "last_checked": None,
    "status": "initializing"
}

# Configuration for model refresh
MODEL_CHECK_INTERVAL = int(os.getenv("MODEL_CHECK_INTERVAL", "300"))  # 5 minutes default
AUTO_REFRESH_ENABLED = os.getenv("AUTO_REFRESH_ENABLED", "true").lower() == "true"

# Pydantic models for generic prediction
from typing import Any, Dict, List, Union

class GenericPredictionRequest(BaseModel):
    """Generic prediction request supporting either a single record (dict)
    or a list of records (list of dicts). Use the 'inputs' field."""
    inputs: Union[Dict[str, Any], List[Dict[str, Any]]]

async def load_mlflow_model(model_name: str = None) -> bool:
    """Load or reload the MLflow model. Returns True if successful."""
    global model, feature_names, model_info
    
    # If no model name provided, don't attempt to load
    if not model_name:
        logger.info("No model name provided, skipping model load")
        model_info.update({
            "last_checked": datetime.now().isoformat(),
            "status": "no_model_specified"
        })
        return False
    
    try:
        logger.info(f"Loading/refreshing MLflow model: {model_name}")
        mlflow_tracking_uri = os.getenv("MLFLOW_TRACKING_URI", MLFLOW_BASE_URL)
        experiment_name = os.getenv("EXPERIMENT_NAME", os.getenv("MODEL_EXPERIMENT", "default_experiment"))
        
        mlflow.set_tracking_uri(mlflow_tracking_uri)
        mlflow.set_experiment(experiment_name)
        
        # Get model info first
        client = mlflow.tracking.MlflowClient()
        try:
            latest_version = client.get_latest_versions(model_name, stages=["None", "Staging", "Production"])
            if latest_version:
                # Get the latest version (highest version number)
                latest_model = max(latest_version, key=lambda x: int(x.version))
                model_version = latest_model.version
                
                # Check if this is a new version
                if model_info["version"] == model_version and model is not None and model_info["name"] == model_name:
                    logger.info(f"Model {model_name} version {model_version} already loaded, skipping refresh")
                    model_info["last_checked"] = datetime.now().isoformat()
                    return True
                
                logger.info(f"Loading model version: {model_version}")
            else:
                logger.warning("No model versions found, trying latest anyway")
                model_version = "latest"
        except Exception as e:
            logger.warning(f"Could not get model version info: {e}, using 'latest'")
            model_version = "latest"
        
        model_uri = f"models:/{model_name}/latest"
        new_model = mlflow.pyfunc.load_model(model_uri)
        
        # Attempt to derive expected feature names from model signature; fallback to None
        try:
            from mlflow.models import get_model_info
            info = get_model_info(model_uri)
            sig = info.signature
            if sig and sig.inputs and hasattr(sig.inputs, 'inputs'):
                feature_names = [inp.name for inp in sig.inputs.inputs if hasattr(inp, 'name')]
            elif sig and sig.inputs:
                feature_names = [getattr(inp, 'name', None) for inp in sig.inputs if getattr(inp, 'name', None)]
            else:
                feature_names = None
        except Exception:
            feature_names = None
        
        # Update model and info atomically
        model = new_model
        model_info.update({
            "name": model_name,
            "version": model_version,
            "loaded_at": datetime.now().isoformat(),
            "last_checked": datetime.now().isoformat(),
            "status": "loaded"
        })
        
        logger.info(f"âœ… Successfully loaded model: {model_name} (version: {model_version})")
        return True
        
    except Exception as e:
        logger.error(f"âŒ Failed to load MLflow model '{model_name}': {e}")
        model_info.update({
            "name": model_name,
            "last_checked": datetime.now().isoformat(),
            "status": "error",
            "error": str(e)
        })
        return False

async def check_for_model_updates():
    """Background task to periodically check for model updates."""
    while True:
        try:
            if AUTO_REFRESH_ENABLED and model_info["status"] == "loaded" and model_info.get("name"):
                logger.info(f"Checking for updates to model: {model_info['name']}")
                await load_mlflow_model(model_info["name"])
            await asyncio.sleep(MODEL_CHECK_INTERVAL)
        except Exception as e:
            logger.error(f"Error in background model check: {e}")
            await asyncio.sleep(MODEL_CHECK_INTERVAL)

async def save_model_to_registry(model_name: str, model_obj, model_version: str = None) -> bool:
    """Helper function to save a model to the MLflow registry with fallback methods."""
    try:
        # Try to save as pyfunc model first
        mlflow.pyfunc.log_model(
            artifact_path="model",
            python_model=model_obj,
            registered_model_name=model_name
        )
        logger.info(f"âœ… Saved model '{model_name}' to MLflow registry as pyfunc")
        return True
    except Exception as pyfunc_error:
        logger.warning(f"Could not save as pyfunc model: {pyfunc_error}")
        
        # Try alternative saving methods
        try:
            # For scikit-learn models, try sklearn flavor
            if hasattr(model_obj, 'predict_proba'):
                mlflow.sklearn.log_model(
                    model_obj,
                    artifact_path="model",
                    registered_model_name=model_name
                )
                logger.info(f"âœ… Saved model '{model_name}' to MLflow registry as sklearn")
                return True
            else:
                logger.warning("Model type not recognized for MLflow saving")
                return False
        except Exception as alt_error:
            logger.error(f"Failed to save model with alternative method: {alt_error}")
            return False

@asynccontextmanager
async def lifespan(app: FastAPI):
    """Application lifespan events"""
    logger.info("FastAPI MLflow Proxy starting...")
    logger.info(f"Proxying requests to MLflow at: {MLFLOW_BASE_URL}")
    logger.info(f"Auto-refresh enabled: {AUTO_REFRESH_ENABLED}, Check interval: {MODEL_CHECK_INTERVAL}s")
    
    # Wait for MLflow to be ready
    logger.info("Waiting for MLflow to be ready...")
    max_retries = 30
    for i in range(max_retries):
        try:
            async with httpx.AsyncClient() as client:
                response = await client.get(f"{MLFLOW_BASE_URL}/health", timeout=5.0)
                if response.status_code == 200:
                    logger.info("âœ… MLflow is ready!")
                    break
        except Exception as e:
            logger.info(f"Waiting for MLflow... (attempt {i+1}/{max_retries})")
            if i == max_retries - 1:
                logger.error(f"âŒ MLflow not ready after {max_retries} attempts: {e}")
            await asyncio.sleep(2)
    
    # Initialize model_info without loading a model
    model_info.update({
        "name": None,
        "version": None,
        "loaded_at": None,
        "last_checked": datetime.now().isoformat(),
        "status": "ready"
    })
    logger.info("âœ… FastAPI ready - awaiting model selection")
    
    # Start background task for model checking
    if AUTO_REFRESH_ENABLED:
        asyncio.create_task(check_for_model_updates())
    
    yield
    logger.info("FastAPI MLflow Proxy shutting down...")

# Create FastAPI application
app = FastAPI(
    title="MLflow Model API",
    description="Containerized FastAPI server with MLflow model integration for predictions and MLflow proxy",
    version="1.0.0",
    lifespan=lifespan
)

# Add CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

@app.get("/", response_class=HTMLResponse)
async def root():
    """Root endpoint with links to available services"""
    # Determine status display
    if model is not None:
        model_status = "âœ… Model Loaded"
    elif model_info["status"] == "ready":
        model_status = "â³ Ready - No Model Selected"
    elif model_info["status"] == "error":
        model_status = "âŒ Error Loading Model"
    else:
        model_status = "â³ Initializing"
    
    model_version = model_info.get("version", "N/A")
    loaded_at = model_info.get("loaded_at", "N/A")
    last_checked = model_info.get("last_checked", "Never")
    
    html_content = """
    <!DOCTYPE html>
    <html>
    <head>
        <title>MLflow Model API</title>
        <style>
            body { font-family: Arial, sans-serif; margin: 40px; }
            h1 { color: #333; }
            .links { margin: 20px 0; }
            .link { display: block; padding: 10px; margin: 5px 0; background: #f0f0f0; text-decoration: none; border-radius: 5px; }
            .link:hover { background: #e0e0e0; }
            .container-info { background: #e7f3ff; padding: 15px; border-radius: 5px; margin: 20px 0; }
            .model-status { background: #f0f8e7; padding: 15px; border-radius: 5px; margin: 20px 0; }
            .refresh-section { background: #fff3cd; padding: 15px; border-radius: 5px; margin: 20px 0; }
            .refresh-button { background: #28a745; color: white; padding: 8px 16px; border: none; border-radius: 4px; cursor: pointer; text-decoration: none; display: inline-block; }
            .refresh-button:hover { background: #218838; }
            .config-info { background: #e2e3e5; padding: 15px; border-radius: 5px; margin: 20px 0; font-size: 0.9em; }
        </style>
        <script>
            async function refreshModel() {
                const button = document.getElementById('refresh-btn');
                const modelNameInput = document.getElementById('model-name-input');
                const modelName = modelNameInput.value.trim();
                
                if (!modelName) {
                    alert('Please enter a model name');
                    return;
                }
                
                button.disabled = true;
                button.textContent = 'Loading...';
                
                try {
                    const response = await fetch('/refresh-model', {
                        method: 'POST',
                        headers: {
                            'Content-Type': 'application/json'
                        },
                        body: JSON.stringify({ model_name: modelName })
                    });
                    const result = await response.json();
                    
                    if (response.ok) {
                        alert('Model loaded successfully!');
                        location.reload();
                    } else {
                        alert('Failed to load model: ' + result.detail);
                    }
                } catch (error) {
                    alert('Error loading model: ' + error.message);
                } finally {
                    button.disabled = false;
                    button.textContent = 'ðŸ”„ Load Model';
                }
            }
        </script>
    </head>
    <body>
        <h1>ðŸš€ MLflow Model API</h1>
        <div class="container-info">
            <h3>ðŸ³ Containerized Deployment</h3>
            <p>This FastAPI server is running in a Docker container with MLflow model integration.</p>
            <p><strong>MLflow URL:</strong> """ + MLFLOW_EXTERNAL_URL + """</p>
        </div>
        <div class="model-status">
            <h3>ðŸ¤– Model Status</h3>
            <p><strong>Status:</strong> """ + model_status + """</p>
            <p><strong>Model:</strong> """ + str(model_info.get('name', 'None')) + """</p>
            <p><strong>Version:</strong> """ + str(model_version) + """</p>
            <p><strong>Loaded At:</strong> """ + str(loaded_at) + """</p>
            <p><strong>Last Checked:</strong> """ + str(last_checked) + """</p>
        </div>
        <div class="refresh-section">
            <h3>ðŸ”„ Model Management</h3>
            <p>Enter the MLflow model name to load:</p>
            <input type="text" id="model-name-input" placeholder="Enter model name (e.g., iris_model)" style="padding: 8px; margin: 10px 0; width: 300px; border: 1px solid #ccc; border-radius: 4px;">
            <br>
            <button id="refresh-btn" class="refresh-button" onclick="refreshModel()">ðŸ”„ Load Model</button>
        </div>
        <div class="config-info">
            <h3>âš™ï¸ Configuration</h3>
            <p><strong>Auto-refresh:</strong> """ + ('Enabled' if AUTO_REFRESH_ENABLED else 'Disabled') + """</p>
            <p><strong>Check Interval:</strong> """ + str(MODEL_CHECK_INTERVAL) + """ seconds</p>
            <p><strong>MLflow URL:</strong> """ + MLFLOW_EXTERNAL_URL + """</p>
            <p><strong>Deployment:</strong> Containerized</p>
        </div>
        <div class="links">
            <a class="link" href="/docs">ðŸ”® Model Prediction (Interactive API)</a>
            <a class="link" href="/model-info">ðŸ“‹ Model Information</a>
            <a class="link" href="/mlflow">ðŸ“Š MLflow UI</a>
            <a class="link" href="/health">ðŸ¥ Health Check</a>
            <a class="link" href="/container-info">ðŸ³ Container Info</a>
            <a class="link" href="/docs#/default/save_model_save_model_post">ðŸ’¾ Save Model to Registry</a>
            <a class="link" href="/docs#/default/test_data_processing_test_data_processing_post">ðŸ§ª Test Data Processing</a>
        </div>
    </body>
    </html>
    """
    return HTMLResponse(content=html_content)

@app.post("/predict")
async def predict(data: GenericPredictionRequest):
    """Predict using the loaded MLflow model with generic inputs.
    Body:
    { "inputs": {..} } or { "inputs": [ {..}, ... ] }
    """
    if model is None:
        raise HTTPException(
            status_code=503, 
            detail="Model not loaded. Please check MLflow configuration and ensure model exists."
        )
    
    try:
        # Normalize inputs to DataFrame
        records = data.inputs if isinstance(data.inputs, list) else [data.inputs]
        input_data = pd.DataFrame(records)
        
        # If we know expected feature order, align and validate
        if feature_names:
            missing = [f for f in feature_names if f not in input_data.columns]
            if missing:
                raise HTTPException(status_code=400, detail=f"Missing required features: {missing}")
            input_data = input_data[feature_names]
        
        # Make prediction
        predictions = model.predict(input_data)
        
        # Create MLflow run to log prediction data
        mlflow_tracking_uri = os.getenv("MLFLOW_TRACKING_URI", MLFLOW_BASE_URL)
        experiment_name = os.getenv("EXPERIMENT_NAME", os.getenv("MODEL_EXPERIMENT", "default_experiment"))
        
        mlflow.set_tracking_uri(mlflow_tracking_uri)
        mlflow.set_experiment(experiment_name)
        
        with mlflow.start_run(run_name=f"prediction_{datetime.now().strftime('%Y%m%d_%H%M%S')}"):
            # Log minimal metadata
            mlflow.log_params({
                "num_records": len(input_data),
                "num_features": len(input_data.columns),
                "model_name": model_info.get("name", "unknown"),
                "model_version": model_info.get("version", "unknown"),
                "deployment": "containerized"
            })
            
            metrics_payload = {"prediction_count": float(len(predictions))}
            # Log a numeric first prediction as metric if possible; otherwise, log as param
            if len(predictions) > 0:
                try:
                    metrics_payload["prediction_value_0"] = float(predictions[0])
                except Exception:
                    mlflow.log_params({"prediction_value_0": str(predictions[0])})
            mlflow.log_metrics(metrics_payload)
            
            # Save the model if it hasn't been saved before or if it's a new version
            if model_info.get("name") and model_info.get("status") == "loaded":
                try:
                    # Check if model already exists in MLflow
                    client = mlflow.tracking.MlflowClient()
                    existing_models = client.search_model_versions(f"name='{model_info['name']}'")
                    
                    if not existing_models:
                        # Model doesn't exist, save it
                        await save_model_to_registry(model_info["name"], model)
                    else:
                        # Check if current version exists
                        current_version = model_info.get("version", "latest")
                        version_exists = any(v.version == str(current_version) for v in existing_models)
                        
                        if not version_exists:
                            # Save new version
                            await save_model_to_registry(model_info["name"], model, current_version)
                            
                except Exception as save_error:
                    logger.warning(f"Could not save model to registry: {save_error}")
                    # Continue with prediction even if saving fails
        
        return {
            "predictions": predictions.tolist(),
            "model": {
                "name": model_info.get("name"),
                "version": model_info.get("version"),
            },
            "feature_order": feature_names,
            "records": len(input_data),
            "deployment": "containerized",
            "mlflow_run_id": mlflow.active_run().info.run_id if mlflow.active_run() else None,
            "timestamp": datetime.now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"Prediction error: {e}")
        raise HTTPException(
            status_code=500, 
            detail=f"Prediction failed: {str(e)}"
        )

class ModelRefreshRequest(BaseModel):
    model_name: str

@app.post("/refresh-model")
async def refresh_model(request: ModelRefreshRequest):
    """Manually load or refresh the MLflow model"""
    logger.info(f"Manual model load requested: {request.model_name}")
    success = await load_mlflow_model(request.model_name)
    
    if success:
        return {
            "status": "success",
            "message": f"Model '{request.model_name}' loaded successfully",
            "model_info": model_info
        }
    else:
        raise HTTPException(
            status_code=500,
            detail=f"Failed to load model '{request.model_name}': {model_info.get('error', 'Unknown error')}"
        )

@app.post("/save-model")
async def save_model():
    """Manually save the currently loaded model to MLflow registry"""
    if model is None:
        raise HTTPException(
            status_code=503,
            detail="No model loaded. Please load a model first."
        )
    
    if not model_info.get("name"):
        raise HTTPException(
            status_code=400,
            detail="Model name not available. Please refresh the model first."
        )
    
    try:
        success = await save_model_to_registry(model_info["name"], model, model_info.get("version"))
        if success:
            return {
                "status": "success",
                "message": f"Model '{model_info['name']}' saved successfully to MLflow registry",
                "model_info": model_info
            }
        else:
            raise HTTPException(
                status_code=500,
                detail=f"Failed to save model '{model_info['name']}' to registry"
            )
    except Exception as e:
        logger.error(f"Error saving model: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"Error saving model: {str(e)}"
        )

@app.post("/test-data-processing")
async def test_data_processing(data: GenericPredictionRequest):
    """Test endpoint to debug request-to-DataFrame processing for generic inputs"""
    try:
        records = data.inputs if isinstance(data.inputs, list) else [data.inputs]
        input_data = pd.DataFrame(records)
        
        # Align to expected feature order if known
        aligned = None
        missing = []
        if feature_names:
            missing = [f for f in feature_names if f not in input_data.columns]
            if not missing:
                aligned = input_data[feature_names]
        
        return {
            "columns": input_data.columns.tolist(),
            "shape": list(input_data.shape),
            "sample": input_data.head(5).to_dict(orient="records"),
            "feature_names_expected": feature_names,
            "missing_features": missing,
            "aligned_preview": (aligned.head(5).to_dict(orient="records") if aligned is not None else None),
            "model_loaded": model is not None,
            "model_type": str(type(model)) if model is not None else None
        }
    except Exception as e:
        logger.error(f"Data processing test failed: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"Data processing test failed: {str(e)}"
        )

@app.get("/model-info")
async def get_model_info():
    """Get current model information"""
    return {
        "model_loaded": model is not None,
        "model_info": model_info,
        "model_type": str(type(model)) if model is not None else None,
        "model_has_predict": hasattr(model, 'predict') if model is not None else False,
        "feature_names": feature_names if feature_names is not None else None,
        "config": {
            "auto_refresh_enabled": AUTO_REFRESH_ENABLED,
            "check_interval_seconds": MODEL_CHECK_INTERVAL,
            "mlflow_tracking_uri": os.getenv("MLFLOW_TRACKING_URI", "sqlite:///mlflow.db"),
            "experiment_name": os.getenv("EXPERIMENT_NAME", "iris_experiment")
        },
        "deployment": "containerized"
    }

@app.get("/health")
async def health_check():
    """Health check endpoint"""
    try:
        async with httpx.AsyncClient() as client:
            response = await client.get(f"{MLFLOW_BASE_URL}/health", timeout=5.0)
            if response.status_code == 200:
                return {
                    "status": "healthy",
                    "mlflow": "connected",
                    "mlflow_url": MLFLOW_BASE_URL,
                    "proxy_port": FASTAPI_PORT,
                    "deployment": "containerized",
                    "model_loaded": model is not None
                }
            else:
                return {
                    "status": "unhealthy",
                    "mlflow": "disconnected",
                    "mlflow_status_code": response.status_code,
                    "deployment": "containerized",
                    "model_loaded": model is not None
                }
    except Exception as e:
        logger.error(f"Health check failed: {e}")
        return {
            "status": "unhealthy",
            "error": str(e),
            "mlflow_url": MLFLOW_BASE_URL,
            "deployment": "containerized",
            "model_loaded": model is not None
        }

@app.get("/container-info")
async def container_info():
    """Container information endpoint"""
    return {
        "container_name": "fastapi-proxy",
        "mlflow_container": "mlflow-server",
        "mlflow_url": MLFLOW_BASE_URL,
        "network": "mlflow-network",
        "ports": {
            "fastapi": FASTAPI_PORT,
            "mlflow": 5000
        },
        "model_loaded": model is not None
    }

@app.get("/mlflow", include_in_schema=False)
async def mlflow_ui_redirect():
    """Redirect to MLflow UI"""
    return RedirectResponse(url=f"{MLFLOW_EXTERNAL_URL}/")

@app.get("/mlflow/{path:path}", include_in_schema=False)
async def proxy_mlflow_ui(path: str, request: Request):
    """Proxy MLflow UI requests"""
    try:
        query_params = str(request.url.query)
        url = f"{MLFLOW_BASE_URL}/{path}"
        if query_params:
            url += f"?{query_params}"
        
        async with httpx.AsyncClient() as client:
            response = await client.get(url, headers=dict(request.headers))
            return response.content
    except Exception as e:
        logger.error(f"MLflow UI proxy error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.api_route("/api/2.0/{path:path}", methods=["GET", "POST", "PUT", "DELETE", "PATCH"], include_in_schema=False)
async def proxy_mlflow_api(path: str, request: Request):
    """Proxy MLflow API requests"""
    try:
        query_params = str(request.url.query)
        url = f"{MLFLOW_BASE_URL}/api/2.0/{path}"
        if query_params:
            url += f"?{query_params}"
        
        async with httpx.AsyncClient() as client:
            response = await client.request(
                method=request.method,
                url=url,
                headers=dict(request.headers),
                content=await request.body()
            )
            return response.content
    except Exception as e:
        logger.error(f"MLflow API proxy error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=FASTAPI_PORT)
FASTAPI_TEMPLATE_EOF
        echo "âœ… Default FastAPI template created successfully!"
    elif [[ "$FASTAPI_SOURCE" == gs://* ]]; then
        echo "Downloading FastAPI application from GCS: $FASTAPI_SOURCE"
        if gsutil cp "$FASTAPI_SOURCE" /home/$TARGET_USER/deployml/docker/fastapi/main.py; then
            echo "âœ… FastAPI application downloaded successfully from GCS!"
        else
            echo "âŒ ERROR: Failed to download FastAPI application from GCS: $FASTAPI_SOURCE"
            echo "Please ensure the file exists and you have proper permissions."
            exit 1
        fi
    elif [[ "$FASTAPI_SOURCE" == /* ]]; then
        echo "Copying FastAPI application from local path: $FASTAPI_SOURCE"
        if [ -f "$FASTAPI_SOURCE" ]; then
            cp "$FASTAPI_SOURCE" /home/$TARGET_USER/deployml/docker/fastapi/main.py
            echo "âœ… FastAPI application copied successfully!"
        else
            echo "âŒ ERROR: FastAPI application not found at: $FASTAPI_SOURCE"
            echo "Please provide a valid file path or use 'template' for the default application."
            exit 1
        fi
    else
        echo "âŒ ERROR: Invalid FastAPI source: $FASTAPI_SOURCE"
        echo "Supported sources:"
        echo "  - 'template' for default FastAPI application"
        echo "  - 'gs://bucket/path/main.py' for GCS file"
        echo "  - '/absolute/path/main.py' for local file"
        exit 1
    fi

    # Set proper permissions
    echo "Setting proper permissions for user: $TARGET_USER"
    sudo chown -R $TARGET_USER:$TARGET_USER /home/$TARGET_USER/deployml
    
    # Create systemd service for Docker Compose
    echo "Creating Docker Compose systemd service..."
    sudo tee /etc/systemd/system/mlflow-docker.service > /dev/null <<DOCKER_SERVICE_EOF
[Unit]
Description=MLflow Docker Compose Stack
After=network.target docker.service
Requires=docker.service

[Service]
Type=oneshot
RemainAfterExit=true
User=$TARGET_USER
Group=$TARGET_USER
WorkingDirectory=/home/$TARGET_USER/deployml/docker
Environment=MLFLOW_BACKEND_STORE_URI={% if flags.needs_postgres %}postgresql+psycopg2://{{ flags.first_tool_name }}:{{ '${urlencode(random_password.db_password.result)}' }}@{{ '${google_sql_database_instance.postgres.public_ip_address}' }}:5432/{{ '${google_sql_database.db.name}' }}{% else %}sqlite:///mlflow.db{% endif %}
Environment=MLFLOW_DEFAULT_ARTIFACT_ROOT={% if bucket_configs %}{% for config in bucket_configs %}{% if config.create %}gs://{{ config.bucket_name }}{% endif %}{% endfor %}{% else %}./mlflow-artifacts{% endif %}
{% if flags.needs_feast and flags.needs_postgres %}
# FEAST PostgreSQL environment variables for Docker Compose
Environment=FEAST_REGISTRY_TYPE=sql
Environment=FEAST_REGISTRY_PATH={% if flags.needs_postgres %}postgresql+psycopg2://{{ '${var.feast_database_user}' }}:{{ '${urlencode(random_password.db_password.result)}' }}@{{ '${google_sql_database_instance.postgres.public_ip_address}' }}:5432/{{ '${var.feast_database_name}' }}{% else %}data/registry.db{% endif %}
Environment=FEAST_ONLINE_STORE_TYPE={% if flags.needs_postgres %}postgres{% else %}sqlite{% endif %}
Environment=FEAST_ONLINE_STORE_HOST={% if flags.needs_postgres %}{{ '${google_sql_database_instance.postgres.public_ip_address}' }}{% else %}localhost{% endif %}
Environment=FEAST_ONLINE_STORE_PORT={% if flags.needs_postgres %}5432{% else %}3306{% endif %}
Environment=FEAST_ONLINE_STORE_DATABASE={% if flags.needs_postgres %}{{ '${var.feast_database_name}' }}{% else %}feast_offline{% endif %}
Environment=FEAST_ONLINE_STORE_USER={% if flags.needs_postgres %}{{ '${var.feast_database_user}' }}{% else %}feast{% endif %}
Environment=FEAST_ONLINE_STORE_PASSWORD={% if flags.needs_postgres %}{{ '${random_password.db_password.result}' }}{% else %}feast{% endif %}
Environment=FEAST_OFFLINE_STORE_TYPE=bigquery
Environment=FEAST_PROJECT={{ flags.feast_params.get('project', (stack_name ~ '_' ~ project_id ~ '_' ~ name_hash) | replace('-', '_')) }}
Environment=FEAST_OFFLINE_STORE_PROJECT={{ project_id }}
Environment=FEAST_OFFLINE_STORE_DATASET={{ flags.feast_params.get('bigquery_dataset', 'feast_offline_store_' ~ stack_name ~ '_' ~ project_id ~ '_' ~ name_hash) }}
Environment=FEAST_ARTIFACT_BUCKET={% if bucket_configs %}{% for config in bucket_configs %}{% if config.create %}{{ config.bucket_name }}{% endif %}{% endfor %}{% else %}{% endif %}
Environment=GOOGLE_CLOUD_PROJECT={{ project_id }}
Environment=USE_POSTGRES={% if flags.needs_postgres %}true{% else %}false{% endif %}
Environment=FEAST_PORT={{ flags.feast_params.get('feast_port', 6566) }}
{% endif %}

ExecStart=/usr/bin/docker compose up -d
ExecStop=/usr/bin/docker compose down
ExecReload=/usr/bin/docker compose restart
Restart=no
StandardOutput=journal
StandardError=journal

[Install]
WantedBy=multi-user.target
DOCKER_SERVICE_EOF

    # Get external IP and set environment variable for MLflow external URL
    echo "Getting external IP for MLflow URL..."
    EXTERNAL_IP=$(curl -s http://metadata.google.internal/computeMetadata/v1/instance/network-interfaces/0/access-configs/0/external-ip -H "Metadata-Flavor: Google")
    echo "External IP: $EXTERNAL_IP"
    
    # Create .env file for Docker Compose
    echo "Creating environment file for Docker Compose..."
    cat > /home/$TARGET_USER/deployml/docker/.env << ENV_EOF
EXTERNAL_MLFLOW_URL=http://$EXTERNAL_IP:{{ flags.mlflow_params.mlflow_port }}
ENV_EOF
    
    # Build Docker containers first
    echo "Building Docker containers..."
    cd /home/$TARGET_USER/deployml/docker
    if ! sudo -u $TARGET_USER docker compose build; then
        echo "âŒ Docker build failed. Checking logs..."
        sudo -u $TARGET_USER docker compose logs
        echo "âŒ Exiting due to Docker build failure"
        exit 1
    fi
    echo "âœ… Docker containers built successfully"
    
    # Reload systemd and enable Docker Compose service
    echo "Enabling Docker Compose service..."
    sudo systemctl daemon-reload
    sudo systemctl enable mlflow-docker.service
    
    echo "Starting Docker containers via systemd..."
    sudo systemctl start mlflow-docker.service
    
    # Wait for containers to start
    echo "Waiting for containers to start..."
    sleep 30
    
    # Check Docker Compose service status
    echo "Checking Docker Compose service status..."
    sudo systemctl status mlflow-docker --no-pager
    
    # Check container status
    echo "Checking container status..."
    sudo -u $TARGET_USER docker ps
    
    # Test MLflow container
    echo "Testing MLflow container..."
    for i in {1..10}; do
      if curl -s http://localhost:{{ flags.mlflow_params.mlflow_port }}/health > /dev/null; then
        echo "âœ… MLflow container is running successfully!"
        break
      else
        echo "Attempt $i: MLflow container not responding yet..."
        if [ $i -eq 10 ]; then
          echo "âš ï¸  MLflow container may still be starting up..."
          echo "Checking MLflow container logs..."
          sudo -u $TARGET_USER docker logs mlflow-server
        fi
        sleep 15
      fi
    done
    
    # Test FastAPI container
    echo "Testing FastAPI container..."
    for i in {1..10}; do
      if curl -s http://localhost:{{ flags.mlflow_params.get('fastapi_port', 8000) }}/health > /dev/null; then
        echo "âœ… FastAPI container is running successfully!"
        break
      else
        echo "Attempt $i: FastAPI container not responding yet..."
        if [ $i -eq 10 ]; then
          echo "âš ï¸  FastAPI container may still be starting up..."
          echo "Checking FastAPI container logs..."
          sudo -u $TARGET_USER docker logs fastapi-proxy
        fi
        sleep 15
      fi
    done
    
    {% if flags.needs_feast %}
    # Test FEAST container
    echo "Testing FEAST container..."
    for i in {1..20}; do
      if nc -z localhost {{ flags.feast_params.get('feast_port', 6566) }} 2>/dev/null; then
        echo "âœ… FEAST container is running successfully!"
        break
      else
        echo "Attempt $i: FEAST container not responding yet..."
        if [ $i -eq 20 ]; then
          echo "âš ï¸  FEAST container may still be starting up..."
          echo "Checking FEAST container logs..."
          sudo -u $TARGET_USER docker logs feast-server
        fi
        sleep 15
      fi
    done
    
    # Run FEAST validation script
    echo "Running FEAST deployment validation..."
    if sudo -u $TARGET_USER /home/$TARGET_USER/deployml/docker/validate_feast.sh; then
        echo "âœ… FEAST deployment validation passed"
    else
        echo "âš ï¸ FEAST deployment validation had issues - check logs for details"
    fi

    # Test FEAST FastAPI container
    echo "Testing FEAST FastAPI container..."
    for i in {1..20}; do
      if curl -f http://localhost:9000/health 2>/dev/null; then
        echo "âœ… FEAST FastAPI container is running successfully!"
        break
      else
        echo "Attempt $i: FEAST FastAPI container not responding yet..."
        if [ $i -eq 20 ]; then
          echo "âš ï¸  FEAST FastAPI container may still be starting up..."
          echo "Checking FEAST FastAPI container logs..."
          sudo -u $TARGET_USER docker logs feast-fastapi-server
        fi
        sleep 15
      fi
    done
    {% endif %}
    
    {% if flags.needs_airflow %}
    # Test Airflow containers
    echo "Testing Airflow containers..."
    
    # Wait for airflow-init to complete
    echo "Waiting for Airflow initialization to complete..."
    for i in {1..30}; do
      if sudo -u $TARGET_USER docker logs airflow-init 2>/dev/null | grep -q "3.0.4"; then
        echo "âœ… Airflow initialization completed successfully!"
        break
      else
        echo "Attempt $i: Airflow initialization not complete yet..."
        if [ $i -eq 30 ]; then
          echo "âš ï¸  Airflow initialization may have failed..."
          echo "Checking Airflow init container logs..."
          sudo -u $TARGET_USER docker logs airflow-init
        fi
        sleep 10
      fi
    done
    
    # Test Airflow webserver
    echo "Testing Airflow webserver..."
    for i in {1..20}; do
      if curl -s http://localhost:{{ flags.airflow_params.get('airflow_port', 8080) }}/health > /dev/null; then
        echo "âœ… Airflow webserver is running successfully!"
        break
      else
        echo "Attempt $i: Airflow webserver not responding yet..."
        if [ $i -eq 20 ]; then
          echo "âš ï¸  Airflow webserver may still be starting up..."
          echo "Checking Airflow webserver logs..."
          sudo -u $TARGET_USER docker logs airflow-webserver
        fi
        sleep 15
      fi
    done
    
    # Test Airflow scheduler
    echo "Testing Airflow scheduler..."
    for i in {1..20}; do
      if sudo -u $TARGET_USER docker logs airflow-scheduler 2>/dev/null | grep -q "Application startup complete"; then
        echo "âœ… Airflow scheduler is running successfully!"
        break
      else
        echo "Attempt $i: Airflow scheduler not responding yet..."
        if [ $i -eq 20 ]; then
          echo "âš ï¸  Airflow scheduler may still be starting up..."
          echo "Checking Airflow scheduler logs..."
          sudo -u $TARGET_USER docker logs airflow-scheduler
        fi
        sleep 15
      fi
    done
    {% endif %}
    
    # Get external IP for display
    EXTERNAL_IP=$(curl -s http://metadata.google.internal/computeMetadata/v1/instance/network-interfaces/0/access-configs/0/external-ip -H "Metadata-Flavor: Google")
    echo "ðŸ³ Containerized deployment{{ ' with PostgreSQL backend' if flags.needs_postgres else '' }} completed successfully!"
    echo "ðŸŒ MLflow UI will be available at: http://$EXTERNAL_IP:{{ flags.mlflow_params.mlflow_port }}"
    echo "ðŸš€ FastAPI Proxy will be available at: http://$EXTERNAL_IP:{{ flags.mlflow_params.get('fastapi_port', 8000) }}"
    echo "ðŸ“Š Container Info: http://$EXTERNAL_IP:{{ flags.mlflow_params.get('fastapi_port', 8000) }}/container-info"
    {% if flags.needs_feast %}
    echo "ðŸ½ï¸ FEAST Feature Server (gRPC/REST API) will be available at: http://$EXTERNAL_IP:{{ flags.feast_params.get('feast_port', 6566) }}"
    echo "ðŸ” FEAST Health Check: http://$EXTERNAL_IP:{{ flags.feast_params.get('feast_port', 6566) }}/health"
    echo "ðŸ“¡ FEAST gRPC Endpoint: $EXTERNAL_IP:{{ flags.feast_params.get('feast_port', 6566) }}"
    echo "ðŸŒ FEAST FastAPI Service will be available at: http://$EXTERNAL_IP:9000"
    echo "ðŸ” FEAST FastAPI Health Check: http://$EXTERNAL_IP:9000/health"
    echo "ðŸ“š FEAST FastAPI Docs: http://$EXTERNAL_IP:9000/docs"
    echo "ðŸ’¡ Note: FEAST is a feature store API server, not a web UI. Use gRPC or REST clients to interact with it."
    echo "ðŸ’¡ FEAST FastAPI provides HTTP API wrapper for easier integration with external services."
    {% endif %}
    {% if flags.needs_airflow %}
    echo "ðŸª‚ Airflow Web UI will be available at: http://$EXTERNAL_IP:{{ flags.airflow_params.get('airflow_port', 8080) }}"
    echo "ðŸ”‘ Airflow Login: Username: {{ flags.airflow_params.get('airflow_admin_user', 'admin') }}, Password: Generated by Simple Auth Manager"
    echo "ðŸ’¡ To find the password, run: docker logs airflow-webserver | grep 'Password for user'"
    echo "ðŸ“Š Airflow Health Check: http://$EXTERNAL_IP:{{ flags.airflow_params.get('airflow_port', 8080) }}/health"
    {% endif %}
    echo "ðŸ”§ SSH into the VM with: gcloud compute ssh {{ flags.mlflow_params.vm_name }} --zone={{ zone }}"
    echo "ðŸ³ Manage containers: docker ps, docker logs mlflow-server, docker logs fastapi-proxy{% if flags.needs_feast %}, docker logs feast-server{% endif %}{% if flags.needs_airflow %}, docker logs airflow-webserver, docker logs airflow-scheduler{% endif %}"
    echo "ðŸ”§ Docker Compose: docker compose up -d, docker compose down, docker compose restart"
    echo "Backend store: {{ 'PostgreSQL' if flags.needs_postgres else 'SQLite' }}"
    {% if bucket_configs %}
    {% for config in bucket_configs %}
    {% if config.create %}
    echo "Artifact store: gs://{{ config.bucket_name }}"
    {% endif %}
    {% endfor %}
    {% endif %}
    
    # Final verification of created files
    echo "ðŸ” Final verification of created files..."
    echo "ðŸ“ Checking /home/$TARGET_USER/deployml/docker/ directory:"
    ls -la /home/$TARGET_USER/deployml/docker/ || echo "âŒ Directory listing failed"
    
    if [ -f "/home/$TARGET_USER/deployml/docker/docker-compose.yml" ]; then
        echo "âœ… docker-compose.yml exists"
    else
        echo "âŒ docker-compose.yml missing!"
    fi
    
    if [ -f "/home/$TARGET_USER/deployml/docker/airflow_environment.env" ]; then
        echo "âœ… airflow_environment.env exists"
    else
        echo "âŒ airflow_environment.env missing!"
    fi
    
    echo "$(date): VM setup completed successfully!"
    echo "Startup script completed successfully" | sudo tee /var/log/mlflow-startup-complete.log
    
EOF
}

# Firewall rules for MLflow access
resource "google_compute_firewall" "allow_mlflow" {
  name    = "{{ ('dl-' ~ name_hash ~ '-allow-mlflow') | lower }}"
  network = "default"
  
  # Explicit dependency to ensure APIs are ready FIRST
  depends_on = [null_resource.api_readiness_check]
  
  allow {
    protocol = "tcp"
    ports    = ["{{ flags.mlflow_params.mlflow_port }}"]
  }
  source_ranges = ["0.0.0.0/0"]
  target_tags   = ["mlflow-server"]
}

# Firewall rule to allow FastAPI traffic
resource "google_compute_firewall" "allow_fastapi" {
  name    = "{{ ('dl-' ~ name_hash ~ '-allow-fastapi') | lower }}"
  network = "default"
  
  # Explicit dependency to ensure APIs are ready FIRST
  depends_on = [null_resource.api_readiness_check]
  
  allow {
    protocol = "tcp"
    ports    = ["{{ flags.mlflow_params.get('fastapi_port', 8000) }}"]
  }
  source_ranges = ["0.0.0.0/0"]
  target_tags   = ["mlflow-server"]
}

{% if flags.needs_feast %}
# Firewall rule to allow Feast server traffic
resource "google_compute_firewall" "allow_feast" {
  name    = "{{ ('dl-' ~ name_hash ~ '-allow-feast') | lower }}"
  network = "default"
  
  # Explicit dependency to ensure APIs are ready FIRST
  depends_on = [null_resource.api_readiness_check]
  
  allow {
    protocol = "tcp"
    ports    = ["{{ flags.feast_params.get('feast_port', 6566) }}"]
  }
  source_ranges = ["0.0.0.0/0"]
  target_tags   = ["mlflow-server"]
}
{% endif %}

{% if flags.needs_feast %}
# Firewall rule to allow Feast FastAPI traffic (port 9000)
resource "google_compute_firewall" "allow_feast_fastapi" {
  name    = "{{ ('dl-' ~ name_hash ~ '-allow-feast-fastapi-9000') | lower }}"
  network = "default"

  depends_on = [null_resource.api_readiness_check]

  allow {
    protocol = "tcp"
    ports    = ["9000"]
  }
  source_ranges = ["0.0.0.0/0"]
  target_tags   = ["mlflow-server"]
}
{% endif %}

{% if flags.needs_grafana %}
# Firewall rule to allow Grafana traffic
resource "google_compute_firewall" "allow_grafana" {
  name    = "{{ ('dl-' ~ name_hash ~ '-allow-grafana') | lower }}"
  network = "default"
  
  # Explicit dependency to ensure APIs are ready FIRST
  depends_on = [null_resource.api_readiness_check]
  
  allow {
    protocol = "tcp"
    ports    = ["{{ flags.grafana_params.get('grafana_port', 3000) }}"]
  }
  source_ranges = ["0.0.0.0/0"]
  target_tags   = ["mlflow-server"]
}
{% endif %}

{% if flags.needs_airflow %}
# Firewall rule to allow Airflow traffic
resource "google_compute_firewall" "allow_airflow" {
  name    = "{{ ('dl-' ~ name_hash ~ '-allow-airflow') | lower }}"
  network = "default"
  
  # Explicit dependency to ensure APIs are ready FIRST
  depends_on = [null_resource.api_readiness_check]
  
  allow {
    protocol = "tcp"
    ports    = [
      "{{ flags.airflow_params.get('airflow_port', 8080) }}",      # Airflow web UI
      "{{ flags.airflow_params.get('airflow_worker_port', 8793) }}", # Airflow worker logs
      "{{ flags.airflow_params.get('airflow_flower_port', 5555) }}"  # Airflow Flower monitoring
    ]
  }
  source_ranges = ["0.0.0.0/0"]
  target_tags   = ["mlflow-server"]
}
{% endif %}

# Firewall rules for HTTP/HTTPS (if needed)
{% if flags.mlflow_params.get("allow_public_access", false) %}
resource "google_compute_firewall" "allow_http_https" {
  name    = "allow-http-https"
  network = "default"
  
  # Explicit dependency to ensure APIs are ready FIRST
  depends_on = [null_resource.api_readiness_check]
  
  allow {
    protocol = "tcp"
    ports    = ["80", "443"]
  }
  source_ranges = ["0.0.0.0/0"]
  target_tags   = ["http-server", "https-server"]
}

resource "google_compute_firewall" "allow_lb_health_checks" {
  name    = "allow-lb-health-checks"
  network = "default"
  
  # Explicit dependency to ensure APIs are ready FIRST
  depends_on = [null_resource.api_readiness_check]
  
  allow {
    protocol = "tcp"
    ports    = ["80", "443"]
  }
  source_ranges = ["130.211.0.0/22", "35.191.0.0/16"]
  target_tags   = ["http-server", "https-server"]
}
{% endif %}

# Outputs
output "vm_external_ip" {
  value = google_compute_instance.mlflow_vm.network_interface[0].access_config[0].nat_ip
}

output "mlflow_url" {
  value = "http://${google_compute_instance.mlflow_vm.network_interface[0].access_config[0].nat_ip}:{{ flags.mlflow_params.mlflow_port }}"
}

output "service_url" {
  value = "http://${google_compute_instance.mlflow_vm.network_interface[0].access_config[0].nat_ip}:{{ flags.mlflow_params.mlflow_port }}"
}

output "fastapi_url" {
  value = "http://${google_compute_instance.mlflow_vm.network_interface[0].access_config[0].nat_ip}:{{ flags.mlflow_params.get('fastapi_port', 8000) }}"
}

output "fastapi_health_url" {
  value = "http://${google_compute_instance.mlflow_vm.network_interface[0].access_config[0].nat_ip}:{{ flags.mlflow_params.get('fastapi_port', 8000) }}/health"
}

output "container_info_url" {
  value = "http://${google_compute_instance.mlflow_vm.network_interface[0].access_config[0].nat_ip}:{{ flags.mlflow_params.get('fastapi_port', 8000) }}/container-info"
}

{% if flags.needs_feast %}
output "feast_url" {
  value = "http://${google_compute_instance.mlflow_vm.network_interface[0].access_config[0].nat_ip}:{{ flags.feast_params.get('feast_port', 6566) }}"
  description = "FEAST server base URL (gRPC/REST API server)"
}

output "feast_health_url" {
  value = "http://${google_compute_instance.mlflow_vm.network_interface[0].access_config[0].nat_ip}:{{ flags.feast_params.get('feast_port', 6566) }}/health"
  description = "FEAST health check endpoint"
}

output "feast_grpc_url" {
  value = "${google_compute_instance.mlflow_vm.network_interface[0].access_config[0].nat_ip}:{{ flags.feast_params.get('feast_port', 6566) }}"
  description = "FEAST gRPC endpoint (for gRPC clients)"
}

output "feast_ui_url" {
  value = "http://${google_compute_instance.mlflow_vm.network_interface[0].access_config[0].nat_ip}:{{ flags.feast_params.get('feast_port', 6566) }}/health"
  description = "FEAST health check endpoint (FEAST doesn't have a web UI, it's a gRPC/REST API server)"
}

output "feast_fastapi_url" {
  value = "http://${google_compute_instance.mlflow_vm.network_interface[0].access_config[0].nat_ip}:9000"
  description = "FEAST FastAPI service base URL"
}

output "feast_fastapi_health" {
  value = "http://${google_compute_instance.mlflow_vm.network_interface[0].access_config[0].nat_ip}:9000/health"
  description = "FEAST FastAPI health endpoint"
}

output "feast_fastapi_docs" {
  value = "http://${google_compute_instance.mlflow_vm.network_interface[0].access_config[0].nat_ip}:9000/docs"
  description = "FEAST FastAPI Swagger docs"
}

output "feast_deployment_info" {
  value = {
    container_name = "feast-server"
    port = {{ flags.feast_params.get('feast_port', 6566) }}
    database = {% if flags.needs_postgres %}"${local.feast_postgres_database}"{% else %}"sqlite"{% endif %}
    database_user = {% if flags.needs_postgres %}"${local.feast_postgres_user}"{% else %}"sqlite"{% endif %}
    database_name_var = var.feast_database_name
    database_user_var = var.feast_database_user
    separate_database = var.feast_separate_database
    registry_type = {% if flags.needs_postgres %}"sql"{% else %}"file"{% endif %}
    offline_store = "bigquery"
    offline_dataset = "{{ flags.feast_params.get('bigquery_dataset', 'feast_offline_store_' ~ stack_name ~ '_' ~ project_id ~ '_' ~ name_hash) }}"
    validation_script = "/home/deployml/docker/validate_feast.sh"
    deployment_type = "vm"
  }
  description = "FEAST deployment configuration and information"
}
{% endif %}

{% if flags.needs_grafana %}
output "grafana_url" {
  value = "http://${google_compute_instance.mlflow_vm.network_interface[0].access_config[0].nat_ip}:{{ flags.grafana_params.get('grafana_port', 3000) }}"
  description = "Grafana monitoring dashboard URL"
}

output "grafana_enabled" {
  value = true
  description = "Whether Grafana is enabled"
}
{% endif %}

{% if flags.needs_airflow %}
output "airflow_url" {
  value = "http://${google_compute_instance.mlflow_vm.network_interface[0].access_config[0].nat_ip}:{{ flags.airflow_params.get('airflow_port', 8080) }}"
  description = "Airflow web UI URL"
}

output "airflow_worker_logs_url" {
  value = "http://${google_compute_instance.mlflow_vm.network_interface[0].access_config[0].nat_ip}:{{ flags.airflow_params.get('airflow_worker_port', 8793) }}"
  description = "Airflow worker logs URL"
}

output "airflow_flower_url" {
  value = "http://${google_compute_instance.mlflow_vm.network_interface[0].access_config[0].nat_ip}:{{ flags.airflow_params.get('airflow_flower_port', 5555) }}"
  description = "Airflow Flower monitoring URL"
}

output "airflow_enabled" {
  value = true
  description = "Whether Airflow is enabled"
}

output "airflow_deployment_info" {
  value = {
    container_names = ["airflow-webserver", "airflow-scheduler", "airflow-init"]
    web_port = {{ flags.airflow_params.get('airflow_port', 8080) }}
    worker_port = {{ flags.airflow_params.get('airflow_worker_port', 8793) }}
    flower_port = {{ flags.airflow_params.get('airflow_flower_port', 5555) }}
    database = {% if flags.needs_postgres %}"${local.airflow_postgres_database}"{% else %}"sqlite"{% endif %}
    database_user = {% if flags.needs_postgres %}"${local.airflow_postgres_user}"{% else %}"sqlite"{% endif %}
    database_name_var = var.airflow_database_name
    database_user_var = var.airflow_database_user
    separate_database = var.airflow_separate_database
    executor = "{{ flags.airflow_params.get('airflow_executor', 'LocalExecutor') }}"
    admin_user = "{{ flags.airflow_params.get('airflow_admin_user', 'admin') }}"
    admin_password = "Generated by Simple Auth Manager - check 'docker logs airflow-webserver'"
    parallelism = {{ flags.airflow_params.get('airflow_parallelism', 4) }}
    dag_concurrency = {{ flags.airflow_params.get('airflow_dag_concurrency', 2) }}
    max_active_runs_per_dag = {{ flags.airflow_params.get('airflow_max_active_runs_per_dag', 2) }}
    deployment_type = "vm"
  }
  description = "Airflow deployment configuration and information"
}
{% endif %}

output "docker_commands" {
  value = {
    check_containers = "docker ps"
    mlflow_logs      = "docker logs mlflow-server"
    fastapi_logs     = "docker logs fastapi-proxy"{% if flags.needs_feast %}
    feast_logs       = "docker logs feast-server"
    feast_fastapi_logs = "docker logs feast-fastapi-server"{% endif %}{% if flags.needs_grafana %}
    grafana_logs     = "docker logs grafana-server"{% endif %}{% if flags.needs_airflow %}
    airflow_webserver_logs = "docker logs airflow-webserver"
    airflow_scheduler_logs = "docker logs airflow-scheduler"
    airflow_init_logs = "docker logs airflow-init"{% endif %}
    restart_services = "docker compose restart"
    stop_services    = "docker compose down"
    start_services   = "docker compose up -d"

  }
}

output "vm_name" {
  value = "{{ flags.mlflow_params.vm_name }}"
}

output "ssh_command" {
  value = "gcloud compute ssh --zone {{ zone }} {{ flags.mlflow_params.vm_name }}"
}

# Artifact bucket output  
{% if bucket_configs %}
{% for config in bucket_configs %}
output "bucket_name_{{ config.stage }}" {
  value = "{{ config.bucket_name }}"
  description = "Name of the {{ config.stage }} artifact bucket"
}

output "bucket_created_{{ config.stage }}" {
  value = {{ config.create | lower }}
  description = "Whether the {{ config.stage }} artifact bucket was created by Terraform"
}

output "bucket_resource_{{ config.stage }}" {
  value = {{ config.create | lower }} ? google_storage_bucket.{{ config.stage }}_{{ config.tool }}_artifact[0].name : "No bucket created"
  description = "Name of the created {{ config.stage }} bucket resource (if any)"
}
{% endfor %}
{% else %}
# No bucket configurations found
{% endif %}

# Zone output
output "deployment_zone" {
  value = "{{ zone }}"
}

# Debug outputs for IAM troubleshooting
output "service_account_email" {
  description = "Email of the created service account"
  value       = google_service_account.vm_service_account.email
}

output "iam_debug_info" {
  description = "Debug information for IAM configuration"
  value = {
    service_account_id = google_service_account.vm_service_account.account_id
    project_id         = var.project_id
    {% if bucket_configs %}
    {% for config in bucket_configs %}
    artifact_bucket_{{ config.stage }} = "{{ config.bucket_name }}"
    iam_bindings_created_{{ config.stage }} = "yes - storage permissions applied"
    {% endfor %}
    {% else %}
    artifact_buckets = "none configured"
    iam_bindings_created = "no storage permissions needed"
    {% endif %}
  }
}

# FEAST feature store module for VM deployment
{% for stage in stack %}
  {% for stage_name, tool in stage.items() %}
    {% if stage_name == "feature_store" and tool.name == "feast" %}
module "{{ stage_name }}_{{ tool.name }}" {
  source              = "./modules/{{ tool.name }}/cloud/gcp/cloud_vm"
  project_id          = var.project_id
  region              = var.region
  bigquery_dataset    = var.bigquery_dataset
  create_bigquery_dataset = var.create_bigquery_dataset
  artifact_bucket     = {% if bucket_configs %}{% for config in bucket_configs %}{% if config.create %}google_storage_bucket.{{ config.stage }}_{{ config.tool }}_artifact[0].name{% endif %}{% endfor %}{% else %}""{% endif %}
  feast_port         = var.feast_port
  feast_database_name = var.feast_database_name
  feast_database_user = var.feast_database_user
  feast_separate_database = var.feast_separate_database
  {% if flags.needs_postgres %}
  backend_store_uri   = "postgresql+psycopg2://${google_sql_user.feast_user.name}:${urlencode(random_password.db_password.result)}@${google_sql_database_instance.postgres.public_ip_address}:5432/${google_sql_database.feast_db.name}"
  use_postgres        = true
  postgres_host       = google_sql_database_instance.postgres.public_ip_address
  postgres_port       = "5432"
  postgres_database   = google_sql_database.feast_db.name
  postgres_user       = google_sql_user.feast_user.name
  postgres_password   = random_password.db_password.result
  {% else %}
  backend_store_uri   = "{{ tool.params.get('backend_store_uri', 'sqlite:///feast.db') }}"
  use_postgres        = false
  postgres_host       = ""
  postgres_port       = "5432"
  postgres_database   = ""
  postgres_user       = ""
  postgres_password   = ""
  {% endif %}
  
  depends_on = [
    google_compute_instance.mlflow_vm,
    {% if flags.needs_postgres %}
    google_sql_database_instance.postgres,
    google_sql_database.db,
    google_sql_user.users,
    {% if flags.needs_feast %}
    google_sql_database.feast_db,
    google_sql_user.feast_user,
    {% endif %}
    {% if flags.needs_airflow %}
    google_sql_database.airflow_db,
    google_sql_user.airflow_user,
    {% endif %}
    {% endif %}
    null_resource.api_readiness_check
  ]
}

{% if flags.feast_params.get('sample_data', false) %}
# Create sample sales data table for FEAST
resource "google_bigquery_table" "sample_sales_data" {
  dataset_id = google_bigquery_dataset.feast_offline_store[0].dataset_id
  table_id   = "sample_sales_data"

  schema = <<EOF
[
  {
    "name": "customer_id",
    "type": "STRING",
    "mode": "REQUIRED"
  },
  {
    "name": "product_id", 
    "type": "STRING",
    "mode": "REQUIRED"
  },
  {
    "name": "sale_amount",
    "type": "FLOAT64",
    "mode": "REQUIRED"
  },
  {
    "name": "quantity",
    "type": "INT64",
    "mode": "REQUIRED"
  },
  {
    "name": "sale_date",
    "type": "TIMESTAMP",
    "mode": "REQUIRED"
  },
  {
    "name": "created_timestamp",
    "type": "TIMESTAMP",
    "mode": "REQUIRED"
  },
  {
    "name": "region",
    "type": "STRING",
    "mode": "REQUIRED"
  },
  {
    "name": "customer_segment",
    "type": "STRING",
    "mode": "REQUIRED"
  }
]
EOF

  deletion_protection = false
}

# Insert sample sales data
resource "google_bigquery_job" "insert_sample_data" {
  job_id = "insert_sample_sales_data_${random_id.job_suffix.hex}"

  query {
    query = <<EOF
INSERT INTO `${var.project_id}.${var.bigquery_dataset}.sample_sales_data` 
(customer_id, product_id, sale_amount, quantity, sale_date, created_timestamp, region, customer_segment)
VALUES
('CUST001', 'PROD001', 299.99, 1, TIMESTAMP('2024-01-15 10:30:00'), TIMESTAMP('2024-01-15 10:30:00'), 'West', 'Premium'),
('CUST002', 'PROD002', 149.50, 2, TIMESTAMP('2024-01-15 11:15:00'), TIMESTAMP('2024-01-15 11:15:00'), 'East', 'Standard'),
('CUST003', 'PROD001', 599.98, 2, TIMESTAMP('2024-01-15 14:20:00'), TIMESTAMP('2024-01-15 14:20:00'), 'North', 'Premium'),
('CUST004', 'PROD003', 89.99, 1, TIMESTAMP('2024-01-15 16:45:00'), TIMESTAMP('2024-01-15 16:45:00'), 'South', 'Standard'),
('CUST005', 'PROD002', 448.50, 3, TIMESTAMP('2024-01-16 09:30:00'), TIMESTAMP('2024-01-16 09:30:00'), 'West', 'Premium'),
('CUST006', 'PROD001', 299.99, 1, TIMESTAMP('2024-01-16 12:00:00'), TIMESTAMP('2024-01-16 12:00:00'), 'East', 'Standard'),
('CUST007', 'PROD003', 179.98, 2, TIMESTAMP('2024-01-16 15:30:00'), TIMESTAMP('2024-01-16 15:30:00'), 'North', 'Premium'),
('CUST008', 'PROD002', 149.50, 1, TIMESTAMP('2024-01-16 17:15:00'), TIMESTAMP('2024-01-16 17:15:00'), 'South', 'Standard'),
('CUST009', 'PROD001', 899.97, 3, TIMESTAMP('2024-01-17 08:45:00'), TIMESTAMP('2024-01-17 08:45:00'), 'West', 'Premium'),
('CUST010', 'PROD003', 269.97, 3, TIMESTAMP('2024-01-17 11:30:00'), TIMESTAMP('2024-01-17 11:30:00'), 'East', 'Standard')
EOF

    destination_table {
      project_id = var.project_id
      dataset_id = var.bigquery_dataset
      table_id   = "sample_sales_data"
    }
  }

  depends_on = [google_bigquery_table.sample_sales_data]
}

resource "random_id" "job_suffix" {
  byte_length = 4
}
{% endif %}
    {% endif %}
  {% endfor %}
{% endfor %}

{% if flags.needs_postgres %}
output "instance_connection_name" {
  value = google_sql_database_instance.postgres.connection_name
  description = "Cloud SQL instance connection name"
}

output "postgresql_credentials" {
  value = {
    db_user                  = google_sql_user.users.name
    db_password              = random_password.db_password.result
    db_name                  = google_sql_database.db.name
    db_public_ip             = google_sql_database_instance.postgres.public_ip_address
    instance_connection_name = google_sql_database_instance.postgres.connection_name
  connection_string        = "postgresql+psycopg2://${google_sql_user.users.name}:${urlencode(random_password.db_password.result)}@${google_sql_database_instance.postgres.public_ip_address}:5432/${google_sql_database.db.name}"
  }
  sensitive = true
  description = "PostgreSQL database credentials"
}

output "db_connection_string" {
  value = "postgresql+psycopg2://${google_sql_user.users.name}:${random_password.db_password.result}@${google_sql_database_instance.postgres.public_ip_address}:5432/${google_sql_database.db.name}"
  sensitive = true
  description = "PostgreSQL connection string"
}
{% endif %}
