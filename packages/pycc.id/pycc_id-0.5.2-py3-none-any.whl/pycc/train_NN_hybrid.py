# my_library/train_models.py
import time
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import os
import copy
import time
import re # for processing equation string
import matplotlib.pyplot as plt
import sympy as sp

# Check for GPU availability
#if torch.cuda.is_available():
#    print("GPU is available, using GPU")
#else:
#    print("GPU is not available, using CPU.")
#device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
# Free GPU memory if using CUDA
#if torch.cuda.is_available():
#    torch.cuda.empty_cache()
#    torch.cuda.ipc_collect()
    
if torch.cuda.is_available():
    print("GPU cuda is available")   
    # Free GPU memory in case of being used
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.ipc_collect()
elif hasattr(torch, "xpu") and torch.xpu.is_available():
    print("GPU intel xpu available")        
else:
    print("GPU not available")
    
# --- 1) Define NN model class ---
class NNModel(nn.Module):
    def __init__(self, neurons=100, layers=3, activation='relu'):
        super().__init__()
        self.input_layer = nn.Linear(1, neurons)
        self.hidden_layers = nn.ModuleList([nn.Linear(neurons, neurons) for _ in range(layers)])
        self.output_layer = nn.Linear(neurons, 1) 
        self.activation_name = activation.lower()
        
        # Verify the function exists in torch.nn.functional
        if not hasattr(torch.nn.functional, self.activation_name):
            raise ValueError(f"Activation function '{activation}' not found in torch.nn.functional")

    def apply_act(self, x):
        # specific handling for rrelu which needs the training flag
        if self.activation_name == 'rrelu':
            return torch.nn.functional.rrelu(x, training=self.training)
        
        # Generic handling for relu, tanh, sigmoid, etc.
        func = getattr(torch.nn.functional, self.activation_name)
        return func(x)

    def forward(self, x):
        x = self.apply_act(self.input_layer(x))
        for layer in self.hidden_layers:
            x = self.apply_act(layer(x))
        return self.output_layer(x)
                
# --- 2) Parse equation, detect f_i ---
def parse_functions(equation_str):
    # regex to find all f\d+(\w+)
    #pattern = r'(f\d+)\(([a-zA-Z_]+)\)' #without_numbers
    pattern = r'(f\d+)\((\w+)\)' # with numbers, i.e. f(x1)
    funcs = re.findall(pattern, equation_str)
    # funcs: list of tuples [('f1', 'x'), ('f2', 'x_dot'), ...]
    unique_funcs = list(dict.fromkeys(funcs)) # unique preserving order
    return unique_funcs
def extract_parameters(equation_str):
    return sorted(set(re.findall(r'\ba\d+\b', equation_str)))
# --- 3) Convert string equation to sympy expression ---
def sympy_expression(equation_str):
    # Replace ^ with **
    equation_str = equation_str.replace('^', '**')
    # Parse sympy Eq object
    lhs_str, rhs_str = equation_str.split('=')
    lhs = sp.sympify(lhs_str)
    rhs = sp.sympify(rhs_str)
    eq = sp.Eq(lhs, rhs)
    return eq
# --- 4) Extract symbolic parameters like a1, a2 ---
def extract_parameters(equation_str):
    return sorted(set(re.findall(r'\ba\d+\b', equation_str)))
# --- 5) Prepare PyTorch tensors from DataFrame columns ---
def prepare_tensors(df, variables):
    tensors = {}
    for v in variables:
        tensors[v] = torch.tensor(df[v].values, dtype=torch.float32).unsqueeze(1)
    return tensors
# --- 6) Evaluate sympy expression with models ---
# Create a dictionary to map sympy functions to torch functions
torch_math = {
    'exp': torch.exp,
    'cos': torch.cos,
    'sin': torch.sin,
    'log': torch.log,
    'log10': torch.log10,
    'sqrt': torch.sqrt,
    'abs': torch.abs,
    'tanh': torch.tanh,
    'tan': torch.tan,
    'cosh': torch.cosh,
    'sinh': torch.sinh,
    'acos': torch.acos,
    'asin': torch.asin,
    'atan': torch.atan,
    'atanh': torch.atanh,
    'acosh': torch.acosh,
    'asinh': torch.asinh,
    'sigmoid': torch.sigmoid,
    'erf': torch.erf,
    'erfc': torch.erfc,
    'erfinv': torch.erfinv,
    'expm1': torch.expm1,
    'log1p': torch.log1p,
    'ceil': torch.ceil,
    'floor': torch.floor,
    'trunc': torch.trunc,
    'round': torch.round,
    'sign': torch.sign,
    'sech': lambda x: 1 / torch.cosh(x),
    'csch': lambda x: 1 / torch.sinh(x),
    'coth': lambda x: torch.cosh(x) / torch.sinh(x),
}
def evaluate_expr(expr, tensors, models, scalar_params):
    if expr.is_Number:
        return torch.tensor(float(expr), dtype=torch.float32)
    elif expr.is_Symbol:
        var_name = str(expr)
        if var_name in tensors:
            return tensors[var_name]
        elif var_name in scalar_params:
            return scalar_params[var_name]
        else:
            raise ValueError(f"Unknown symbol {var_name}")
    elif expr.is_Function:
        func_name = expr.func.__name__
        arg = expr.args[0]
        arg_tensor = evaluate_expr(arg, tensors, models, scalar_params)
        
        # Check if it's a learned function (f1, f2, etc.)
        if func_name in models:
            return models[func_name](arg_tensor)
        # Check if it's a standard mathematical function
        elif func_name in torch_math:
            return torch_math[func_name](arg_tensor)
        else:
            raise ValueError(f"Model or known function '{func_name}' not found")
    elif expr.is_Add or expr.is_Mul or expr.is_Pow:
        args = [evaluate_expr(arg, tensors, models, scalar_params) for arg in expr.args]
        if expr.is_Add:
            return sum(args)
        elif expr.is_Mul:
            result = args[0]
            for a in args[1:]:
                result = result * a
            return result
        elif expr.is_Pow:
            base, exp = args
            return base ** exp
    else:
        raise NotImplementedError(f"Expr type {expr} not implemented")
# --- 7) Optional constraints ---
def compute_constraint_loss(models, constraints, func_list, tensors, device):
    """
    models: dict of function name -> nn.Module
    constraints: list of dicts like {'constraint': str, 'penalty': float}
    func_list: list of (f_name, var_name) tuples, e.g. [('f1', 'x_dot'), ('f2', 'x')]
    tensors: dict of variable name -> torch tensor
    
    Uses data points from tensors[var_name] for oddness check.
    """
    total_loss = 0
    for c in constraints:
        cons_str = c['constraint'].strip()
        penalty = c.get('penalty', 1.0) # weight=1 as default
        mode = c.get('eval','array') # eval can be 'data' or 'array'
        Nval_array = c.get('Nval_array',100)
        
        # --- Match value-at-point: e.g. f1(3) = 2.5 ---
        m_val_at = re.match(r'(f\d+)\(([-+]?\d*\.?\d+)\)\s*=\s*([-+]?\d*\.?\d+)', cons_str)
        if m_val_at:
            f_name = m_val_at.group(1)
            x_val = float(m_val_at.group(2))
            y_target_val = float(m_val_at.group(3))
            #y_target = torch.tensor(y_target_val, dtype=torch.float32, device=device)
            if f_name in models:
                model = models[f_name]
                try:
                    model_dev = next(model.parameters()).device
                except StopIteration:
                    # model has no parameters? fallback to provided device
                    model_dev = device if isinstance(device, torch.device) else torch.device(str(device))
                x_tensor = torch.tensor([[x_val]], dtype=torch.float32, device=model_dev)
                y_target = torch.tensor([[y_target_val]], dtype=torch.float32, device=model_dev)
                #y_pred = model(x_tensor)
                with torch.no_grad():
                    y_pred = model(x_tensor)                
                loss = ((y_pred - y_target) ** 2).mean()
                total_loss += penalty * loss
            continue
        
        # --- Match odd function constraint: e.g. "f2 odd" ---
        m_odd = re.match(r'(f\d+)\s+odd', cons_str)
        if m_odd:
            f_name = m_odd.group(1)
            # Find var_name from func_list
            var_name = None
            for fn, vn in func_list:
                if fn == f_name:
                    var_name = vn
                    break
            if f_name in models and var_name is not None:
                model = models[f_name]
                x_data = tensors[var_name].flatten()
                if mode=='data':
                    #print(f"Constraint data for {f_name}")
                    # simple implementation where we evaluate the data points
                    x_pos = x_data[x_data >= 0].unsqueeze(1)
                elif mode=='array':
                    # create a linspace to evaluate an equispaced interval
                    #print(f"Constraint array for {f_name}")
                    max_abs_val = x_data.abs().max()
                    x_pos = torch.linspace(0.0, max_abs_val, steps=Nval_array, device=x_data.device).unsqueeze(1)
                else:
                    print(f"Warning: eval mode of constraint '{m_odd}' not recognized.")
                x_neg = -x_pos
                y_pos = model(x_pos)
                y_neg = model(x_neg)
                loss = ((y_pos + y_neg) ** 2).mean()
                total_loss += penalty * loss
            continue
        # --- Match even function constraint: e.g. "f2 even" ---
        m_even = re.match(r'(f\d+)\s+even', cons_str)
        if m_even:
            f_name = m_even.group(1)
            # Find var_name from func_list
            var_name = None
            for fn, vn in func_list:
                if fn == f_name:
                    var_name = vn
                    break
            if f_name in models and var_name is not None:
                model = models[f_name]
                x_data = tensors[var_name].flatten()
                if mode=='data':
                    # simple implementation where we evaluate the data points
                    x_pos = x_data[x_data >= 0].unsqueeze(1)
                elif mode=='array':
                    # create a linspace to evaluate an equispaced interval
                    max_abs_val = x_data.abs().max()
                    x_pos = torch.linspace(0.0, max_abs_val, steps=Nval_array, device=x_data.device).unsqueeze(1)
                else:
                    print(f"Warning: eval mode of constraint '{m_even}' not recognized.")
                x_neg = -x_pos
                y_pos = model(x_pos)
                y_neg = model(x_neg)
                loss = ((y_pos - y_neg) ** 2).mean()
                total_loss += penalty * loss
            continue
        
        
        # --- You can add more constraint types here ---
        
        else:
            # Unknown constraint string
            print(f"Warning: constraint '{cons_str}' not recognized.")
    
    return total_loss
# --- 8) Training function ---
def train_NN_hybrid(df, equation_str, params=None):
    """
    Backwards-compatible trainer that accepts one equation (str) or multiple (list/tuple).
    Added: per-equation weights via params['eq_weights'] (default 1.0 for each equation).
    """
    if params is None:
        params = {}
    device_option = params.get('device', 'automatic').lower()    



    if device_option == 'automatic' or device_option in ['gpu', 'cuda']:
        if torch.cuda.is_available():
            device = torch.device("cuda")
        elif hasattr(torch, "xpu") and torch.xpu.is_available():
            device = torch.device("xpu")
        else:
            device = torch.device("cpu")
    elif device_option == 'xpu':
        if hasattr(torch, "xpu") and torch.xpu.is_available():
            device = torch.device("xpu")
        else:
            print("‚ö†Ô∏è Intel XPU requested but not available. Running on CPU.")
            device = torch.device("cpu")
    elif device_option == 'cpu':
        device = torch.device("cpu")
    else:
        raise ValueError(f"Invalid device option '{device_option}'. Use 'automatic', 'cpu', 'gpu'/'cuda', or 'xpu'.")

    print(f"üíª Using device: {device}")


    
#    if device_option == 'automatic':
#        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
#    elif device_option in ['gpu', 'cuda']:
#        if torch.cuda.is_available():
#            device = torch.device("cuda")
#        else:
#            print("‚ö†Ô∏è GPU requested but not available. Running on CPU.")
#            device = torch.device("cpu")
#    elif device_option == 'cpu':
#        device = torch.device("cpu")
#    else:
#        raise ValueError(f"Invalid device option '{device_option}'. Use 'automatic', 'cpu', or 'gpu'/'cuda'.")
        
    #if torch.cuda.is_available():
    #    print("GPU is available, using GPU") 
    #else:
    #    print("GPU is not available, using CPU.")
    #device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    

        
    # Extract hyperparameters with defaults
    neurons = params.get('neurons', 100)
    layers = params.get('layers', 3)
    activation_name = params.get('activation', 'relu') # <--- ADD THIS LINE
    lr = params.get('lr', 1e-3)
    scalar_lr = params.get('scalar_lr', lr)
    epochs = params.get('epochs', 1000)
    error_threshold = params.get('error_threshold', 1e-6)
    extrapolation = params.get('extrapolation', None)
    weight_loss_param = params.get('weight_loss_param', 1e-3)
    constraints = params.get('constraints', [])
    n_eval = int(params.get('n_eval', 200))
    initial_params = params.get('initial_params', {})
    # Accept single equation or list of equations (backward compatible)
    if isinstance(equation_str, (list, tuple)):
        equations = list(equation_str)
    else:
        equations = [equation_str]
    # Handle eq_weights from params
    eq_weights = params.get('eq_weights', None)
    if eq_weights is None:
        weights = [1.0] * len(equations)
    elif np.isscalar(eq_weights):
        weights = [float(eq_weights)] * len(equations)
    else:
        # assume list/tuple-like
        if len(eq_weights) != len(equations):
            raise ValueError(f"eq_weights length {len(eq_weights)} does not match number of equations {len(equations)}")
        weights = [float(w) for w in eq_weights]
    # Parse functions and parameters from all equations, preserving order and ensuring consistency
    func_map = {} # f_name -> var_name (ensure consistent)
    func_order = [] # list of (f_name, var_name) in appearance order
    for eq in equations:
        flist = parse_functions(eq)
        for f_name, var_name in flist:
            if f_name in func_map:
                if func_map[f_name] != var_name:
                    raise ValueError(f"Function {f_name} used with different arguments: "
                                     f"{func_map[f_name]} vs {var_name}")
            else:
                func_map[f_name] = var_name
                func_order.append((f_name, var_name))
    # collect unique scalar parameter names across all equations
    param_names_set = set()
    for eq in equations:
        for p in extract_parameters(eq):
            param_names_set.add(p)
    param_names = sorted(param_names_set)
    # create models and scalar params
    #models = {f_name: NNModel(neurons,layers) for f_name, _ in func_order}
    models = {f_name: NNModel(neurons, layers, activation=activation_name) for f_name, _ in func_order} 
    
#    #working original    
#    scalar_params = {name: nn.Parameter(torch.tensor(1.0, dtype=torch.float32)) for name in param_names}
    
#    #modified cuda    
#    scalar_params = {name: nn.Parameter(torch.tensor(1.0, dtype=torch.float32)) for name in param_names}
    scalar_params = {
        name: nn.Parameter(torch.tensor(1.0, dtype=torch.float32, device=device)) 
        for name in param_names
    }
    
    # Prepare tensors from df (same as before)
    variables = list(df.columns)
    tensors = prepare_tensors(df, variables)
     # --- Move all models, params, and tensors to GPU ---
    if device.type=='cuda' or device=='xpu':
        print(f"Moving models and data to {device}...")
        #if torch.cuda.is_available():
        #    print("Moving models and data to GPU...")
        for model in models.values():
            model.to(device)
        for k in tensors:
            tensors[k] = tensors[k].to(device)
        #for k in scalar_params:
        #    scalar_params[k] = scalar_params[k].to(device)
    
    
    # Parse every equation into sympy Eq objects
    eq_objs = [sympy_expression(eq) for eq in equations]
    lhs_exprs = [eqo.lhs for eqo in eq_objs]
    rhs_exprs = [eqo.rhs for eqo in eq_objs]
    criterion = nn.MSELoss() 
    # optimizer over all parameters (scalar params + all NN params)
    full_params = list(scalar_params.values()) + [p for model in models.values() for p in model.parameters()]
    optimizer = optim.Adam(full_params, lr=lr)


    # --- Scheduler Logic To Accelerate Calculations ---
    scheduler_type = params.get('scheduler', 'none') # Options: 'one_cycle', 'plateau', 'none'
    scheduler = None
    if scheduler_type == 'one_cycle':
        # One Cycle Policy: Ramps up to max_lr then decays
        # This assumes your 'epochs' is the total number of steps since you do full-batch training
        scheduler = torch.optim.lr_scheduler.OneCycleLR(
            optimizer,
            max_lr=params.get('max_lr', 1e-2), # Try 1e-2 first, 1e-1 might explode
            total_steps=epochs,
            pct_start=params.get('pct_start', 0.3), # 30% of time warming up
            anneal_strategy='cos',
            div_factor=25.0,        # Initial LR = max_lr / 25
            final_div_factor=1000.0 # Final LR = Initial LR / 1000
        )
        print(f"Using OneCycleLR scheduler (Max LR: {params.get('max_lr', 1e-2)})")
    elif scheduler_type == 'plateau':
        # Reduce LR when loss stops improving (Safer, standard approach)
        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
            optimizer, 
            mode='min', 
            factor=0.5, 
            patience=50, 
            verbose=True
        )
        print(" Using ReduceLROnPlateau scheduler")

###### THIS MODULE CAN BE COMMENTED AND WORK except only constants with gpu
#    # new implementation with a module container
#    # Build a module container so everything is registered and moveable as one unit
    model_container = nn.Module()
    model_container.models = nn.ModuleDict(models)
#    # Register scalar parameters in a ParameterDict so they're visible to module.parameters()
    model_container.scalars = nn.ParameterDict({
        #name: nn.Parameter(torch.tensor(1.0, dtype=torch.float32))
        name: nn.Parameter(torch.tensor(
            # Get the initial value from the dict, or default to 1.0
            initial_params.get(name, 1.0), 
            dtype=torch.float32
        ))
        for name in param_names
        })
#    # Move the whole container to device BEFORE creating optimizer
    model_container.to(device)
#    # Expose convenient references for evaluate & training loops
    models = model_container.models
    scalar_params = model_container.scalars    
#    models = {k: model_container.models[k] for k in model_container.models.keys()}
#    scalar_params = {k: model_container.scalars[k] for k in model_container.scalars.keys()}
#    # Prepare tensors (move to device)
    variables = list(df.columns)
    tensors = prepare_tensors(df, variables)
    for k in tensors:
        tensors[k] = tensors[k].to(device)
#    # Optional: set different hyperparams per parameter group
#    # e.g. smaller LR for scalar params, or no weight decay for them:
#    scalar_lr = params.get('scalar_lr', lr)
#    scalar_wd = params.get('scalar_weight_decay', 0.0)
#    nn_wd = params.get('nn_weight_decay', 0.0)
#    param_groups = [
#        {'params': [p for p in model_container.models.parameters()], 'lr': lr, 'weight_decay': nn_wd},
#        {'params': [p for _, p in model_container.scalars.items()], 'lr': scalar_lr, 'weight_decay': scalar_wd}
#    ]
    param_groups = [
        {'params': model_container.models.parameters(), 'lr': lr},        
        {'params': model_container.scalars.parameters(), 'lr': scalar_lr}
    ]
    optimizer = optim.Adam(param_groups)
### END BLOCK 
 
    #print("device=",device, ";  device.type=",device.type)
    # --- Intel XPU optimization with IPEX ---
    if device.type == 'xpu':
        try:
            import intel_extension_for_pytorch as ipex
            print(f"IPEX version: {ipex.__version__}")
            print(f"XPU available: {torch.xpu.is_available()}")
            print(f"XPU device count: {torch.xpu.device_count()}")
            print("Applying Intel IPEX optimization for XPU device...")            
          
            #try1
            #for f_name, model in models.items():
            #    model, optimizer = ipex.optimize(model, optimizer=optimizer)
            # 1. Optimize each model with a dummy optimizer or just the model
            
            
            #try2
            #for f_name, model in models.items():
            #    optimized_model, _ = ipex.optimize(model, optimizer=None)
            #    models[f_name] = optimized_model # Update the models dictionary
            #full_params = list(scalar_params.values()) + [p for model in models.values() for p in model.parameters()]
            #optimizer = optim.Adam(full_params, lr=lr)
            
            # try3
            ### We use model=None to only optimize/move the optimizer state.
            #dummy_module = torch.nn.Module().to(device)
            #_, optimizer = ipex.optimize(dummy_module, optimizer=optimizer)
            #model_container = nn.ModuleDict(models)
            ## Optimize the container ONCE. The optimizer already contains all 
            ## parameters (NNs + scalars) and IPEX will handle them correctly.
            #model_container, optimizer = ipex.optimize(model_container, optimizer=optimizer)
            #for f_name in models.keys():
            #    models[f_name] = model_container[f_name] 
                
            #try4     
            model_container = nn.Module()
            model_container.models = nn.ModuleDict(models)   # models is dict created earlier 
            # use ParameterDict so scalars are registered and visible to module.parameters()
            model_container.scalars = nn.ParameterDict({
                name: nn.Parameter(torch.tensor(1.0, dtype=torch.float32))
                for name in param_names
            })
            model_container.to(device)
            optimizer = optim.Adam(model_container.parameters(), lr=lr)
            model_container, optimizer = ipex.optimize(model_container, optimizer=optimizer)
            models = {k: model_container.models[k] for k in model_container.models.keys()}
            scalar_params = {k: model_container.scalars[k] for k in model_container.scalars.keys()}

            #try5
            #model_container, optimizer = ipex.optimize(model_container, optimizer=optimizer)
            ## rebind references in case identities changed
            #models = {k: model_container.models[k] for k in model_container.models.keys()}
            #scalar_params = {k: model_container.scalars[k] for k in model_container.scalars.keys()}
 
        except ImportError:
            print("‚ö†Ô∏è IPEX not installed. Continuing without XPU optimizations.")
            
    print("Note: if Constraints are present, then:  Total_loss=Loss+Constraints")
    for epoch in range(epochs):
        optimizer.zero_grad()
        # Compute total loss as weighted sum over all equations' MSE(lhs, rhs)
        total_data_loss = 0.0
        for lhs_expr, rhs_expr, w in zip(lhs_exprs, rhs_exprs, weights):
            lhs_val = evaluate_expr(lhs_expr, tensors, models, scalar_params)
            rhs_val = evaluate_expr(rhs_expr, tensors, models, scalar_params)
            # compute MSE and multiply by eq weight
            total_data_loss = total_data_loss + w * criterion(lhs_val, rhs_val)
        # constraint loss (same helper, uses func_order and tensors)
        constraint_loss = compute_constraint_loss(models, constraints, func_order, tensors, device)
        # L2 penalty for scalar params
        param_penalty = 0.0
        if weight_loss_param > 0:
            for p in scalar_params.values():
                param_penalty += (p ** 2).mean()
            param_penalty *= weight_loss_param
        total_loss = total_data_loss + constraint_loss + param_penalty
        total_loss.backward()
        optimizer.step()
        
        # --- Update Scheduler ---
        if scheduler_type == 'one_cycle':
            scheduler.step()
        elif scheduler_type == 'plateau':
            # Note: Plateau scheduler needs the metric (loss) to decide
            scheduler.step(total_loss)        
        
        
        # printing ‚Äî same behavior as before but using total_data_loss
        if epoch % 100 == 0 or total_data_loss.item() < error_threshold:
            if scalar_params and constraints:
                print(f"Epoch {epoch}, Loss: {total_data_loss.item():.2e}, Constraints: {constraint_loss:.2e}, Params: ", end="")
                for k in scalar_params:
                    print(f"{k}: {scalar_params[k].item():.2e}", end="  ")
                print()
            elif scalar_params:
                print(f"Epoch {epoch}, Loss: {total_data_loss.item():.2e}, Params: ", end="")
                for k in scalar_params:
                    print(f"{k}: {scalar_params[k].item():.2e}", end="  ")
                print()
            elif constraints:
                print(f"Epoch {epoch}, Loss: {total_data_loss.item():.2e}, Constraints: {constraint_loss:.2e}")
            else:
                print(f"Epoch {epoch}, Loss: {total_data_loss.item():.2e}")
        if total_data_loss.item() < error_threshold:
            print(f"Early stopping at epoch {epoch}")
            break
    # --- Move everything back to CPU for evaluation/plotting ---
    if device.type == 'xpu' or device.type=='cuda':
        print("Moving models and data back to CPU...")
        for model in models.values():
            model.to('cpu')
        for k in tensors:
            tensors[k] = tensors[k].to('cpu')
        for k in scalar_params:
            scalar_params[k] = scalar_params[k].to('cpu')        
            
    # Evaluate learned functions on their variable ranges for plotting (same as before)
    results = []
    for f_name, var in func_order:
        model = models[f_name]
        model.eval()
        x_vals = tensors[var].detach().numpy().flatten()
        x_plot = np.linspace(np.min(x_vals), np.max(x_vals), n_eval).reshape(-1, 1).astype(np.float32)
        x_plot_tensor = torch.tensor(x_plot)
        with torch.no_grad():
            y_plot = model(x_plot_tensor).numpy().flatten()
        results.extend([x_plot.flatten(), y_plot])
    return models, results, scalar_params
