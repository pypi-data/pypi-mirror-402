"""Generated model names for groq provider.

Auto-generated by scripts/generate_model_names.py - do not edit manually.
Total models: 20
"""

from __future__ import annotations

from typing import Literal


# Fully-qualified model IDs (provider:model format)
GroqModelId = Literal[
    "groq:allam-2-7b",
    "groq:canopylabs/orpheus-arabic-saudi",
    "groq:canopylabs/orpheus-v1-english",
    "groq:groq/compound",
    "groq:groq/compound-mini",
    "groq:llama-3.1-8b-instant",
    "groq:llama-3.3-70b-versatile",
    "groq:meta-llama/llama-4-maverick-17b-128e-instruct",
    "groq:meta-llama/llama-4-scout-17b-16e-instruct",
    "groq:meta-llama/llama-guard-4-12b",
    "groq:meta-llama/llama-prompt-guard-2-22m",
    "groq:meta-llama/llama-prompt-guard-2-86m",
    "groq:moonshotai/kimi-k2-instruct",
    "groq:moonshotai/kimi-k2-instruct-0905",
    "groq:openai/gpt-oss-120b",
    "groq:openai/gpt-oss-20b",
    "groq:openai/gpt-oss-safeguard-20b",
    "groq:qwen/qwen3-32b",
    "groq:whisper-large-v3",
    "groq:whisper-large-v3-turbo",
]

# Model names without provider prefix
GroqModelName = Literal[
    "allam-2-7b",
    "canopylabs/orpheus-arabic-saudi",
    "canopylabs/orpheus-v1-english",
    "groq/compound",
    "groq/compound-mini",
    "llama-3.1-8b-instant",
    "llama-3.3-70b-versatile",
    "meta-llama/llama-4-maverick-17b-128e-instruct",
    "meta-llama/llama-4-scout-17b-16e-instruct",
    "meta-llama/llama-guard-4-12b",
    "meta-llama/llama-prompt-guard-2-22m",
    "meta-llama/llama-prompt-guard-2-86m",
    "moonshotai/kimi-k2-instruct",
    "moonshotai/kimi-k2-instruct-0905",
    "openai/gpt-oss-120b",
    "openai/gpt-oss-20b",
    "openai/gpt-oss-safeguard-20b",
    "qwen/qwen3-32b",
    "whisper-large-v3",
    "whisper-large-v3-turbo",
]

__all__ = ["GroqModelId", "GroqModelName"]
