#!/usr/bin/env python3
"""
NLP æ—¥å¿—åˆ†æå·¥å…·

åˆ†æ NLP æ‰§è¡Œæ—¥å¿—ï¼Œç”Ÿæˆç»Ÿè®¡æŠ¥å‘Šå’Œä¼˜åŒ–å»ºè®®
"""

import json
import sys
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any
from collections import Counter, defaultdict

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

from core.nlp_logger import NLPExecutionLogger


class NLPLogAnalyzer:
    """NLP æ—¥å¿—åˆ†æå™¨"""

    def __init__(self, log_dir: str = None):
        """
        åˆå§‹åŒ–åˆ†æå™¨

        Args:
            log_dir: æ—¥å¿—ç›®å½•
        """
        self.logger = NLPExecutionLogger(log_dir)
        self.log_dir = self.logger.log_dir

    def analyze_failures(self, limit: int = 100) -> Dict[str, Any]:
        """
        åˆ†æå¤±è´¥è®°å½•

        Args:
            limit: åˆ†æçš„è®°å½•æ•°é‡

        Returns:
            åˆ†æç»“æœå­—å…¸
        """
        failures = self.logger.get_recent_failures(limit)

        if not failures:
            return {"total": 0, "message": "æ²¡æœ‰å¤±è´¥è®°å½•"}

        # ç»Ÿè®¡å¤±è´¥åŸå› 
        failure_reasons = Counter([f['failure_reason'] for f in failures])

        # ç»Ÿè®¡å¤±è´¥çš„æè¿°æ¨¡å¼
        descriptions = [f['user_description'] for f in failures]

        # æŒ‰ä½ç½®ç»Ÿè®¡
        positions = Counter([
            f['parsed_result'].get('position', 'unknown')
            for f in failures
        ])

        # æŒ‰ç±»å‹ç»Ÿè®¡
        types = Counter([
            f['parsed_result'].get('type', 'unknown')
            for f in failures
        ])

        # æŒ‰ç­–ç•¥ç»Ÿè®¡
        strategies = Counter()
        for f in failures:
            for strategy in f['strategies_tried']:
                strategies[strategy] += 1

        # åˆ†æå¤±è´¥çš„æè¿°æ¨¡å¼
        description_patterns = self._analyze_description_patterns(descriptions)

        return {
            "total": len(failures),
            "failure_reasons": dict(failure_reasons.most_common(10)),
            "positions": dict(positions.most_common(10)),
            "types": dict(types.most_common(10)),
            "strategies": dict(strategies.most_common(10)),
            "description_patterns": description_patterns,
            "recent_failures": failures[:10]
        }

    def _analyze_description_patterns(self, descriptions: List[str]) -> Dict[str, Any]:
        """
        åˆ†ææè¿°æ¨¡å¼

        Args:
            descriptions: æè¿°åˆ—è¡¨

        Returns:
            æ¨¡å¼åˆ†æç»“æœ
        """
        patterns = {
            "length_distribution": {"short": 0, "medium": 0, "long": 0},
            "has_position": 0,
            "has_type": 0,
            "has_quotes": 0,
            "complexity": {"simple": 0, "medium": 0, "complex": 0}
        }

        for desc in descriptions:
            # é•¿åº¦åˆ†å¸ƒ
            length = len(desc)
            if length < 10:
                patterns["length_distribution"]["short"] += 1
            elif length < 20:
                patterns["length_distribution"]["medium"] += 1
            else:
                patterns["length_distribution"]["long"] += 1

            # å…³é”®è¯æ£€æŸ¥
            for pos in ['å·¦ä¸Š', 'å³ä¸Š', 'å·¦ä¸‹', 'å³ä¸‹', 'é¡¶éƒ¨', 'åº•éƒ¨', 'å·¦ä¾§', 'å³ä¾§', 'ä¸­é—´', 'ä¸­å¤®']:
                if pos in desc:
                    patterns["has_position"] += 1
                    break

            for type_kw in ['å›¾æ ‡', 'æŒ‰é’®', 'æ–‡å­—', 'è¾“å…¥æ¡†']:
                if type_kw in desc:
                    patterns["has_type"] += 1
                    break

            if '"' in desc or "'" in desc:
                patterns["has_quotes"] += 1

            # å¤æ‚åº¦
            complexity_score = 0
            if patterns["has_position"]:
                complexity_score += 1
            if patterns["has_type"]:
                complexity_score += 1
            if patterns["has_quotes"]:
                complexity_score += 1

            if complexity_score == 0:
                patterns["complexity"]["simple"] += 1
            elif complexity_score <= 1:
                patterns["complexity"]["medium"] += 1
            else:
                patterns["complexity"]["complex"] += 1

        # è®¡ç®—ç™¾åˆ†æ¯”
        total = len(descriptions) if descriptions else 1
        patterns["has_position"] = f"{patterns['has_position']/total*100:.1f}%"
        patterns["has_type"] = f"{patterns['has_type']/total*100:.1f}%"
        patterns["has_quotes"] = f"{patterns['has_quotes']/total*100:.1f}%"

        return patterns

    def generate_optimization_suggestions(self) -> List[str]:
        """
        ç”Ÿæˆä¼˜åŒ–å»ºè®®

        Returns:
            å»ºè®®åˆ—è¡¨
        """
        analysis = self.analyze_failures()
        suggestions = []

        # æ£€æŸ¥å¤±è´¥åŸå› 
        if "æœªæ‰¾åˆ°åŒ¹é…çš„å›¾æ ‡" in analysis.get("failure_reasons", {}):
            suggestions.append(
                "ğŸ’¡ å»ºè®®: æ·»åŠ æ›´å¤šæè¿°å…³é”®è¯åˆ° desc_keywords åˆ—è¡¨"
            )
            suggestions.append(
                "ğŸ’¡ å»ºè®®: è€ƒè™‘ä½¿ç”¨ AI è§†è§‰è¯†åˆ«ä½œä¸ºå¤‡é€‰ç­–ç•¥"
            )

        # æ£€æŸ¥ä½ç½®å…³é”®è¯ä½¿ç”¨
        positions = analysis.get("positions", {})
        if positions.get("unknown", 0) > 10:
            suggestions.append(
                "ğŸ’¡ å»ºè®®: å¾ˆå¤šå¤±è´¥æ²¡æœ‰ä½ç½®ä¿¡æ¯ï¼Œé¼“åŠ±ç”¨æˆ·ä½¿ç”¨ä½ç½®å…³é”®è¯"
            )

        # æ£€æŸ¥ç±»å‹å…³é”®è¯ä½¿ç”¨
        types = analysis.get("types", {})
        if types.get("unknown", 0) > 10:
            suggestions.append(
                "ğŸ’¡ å»ºè®®: å¾ˆå¤šå¤±è´¥æ²¡æœ‰ç±»å‹ä¿¡æ¯ï¼Œé¼“åŠ±ç”¨æˆ·æ˜ç¡®å…ƒç´ ç±»å‹"
            )

        # æ£€æŸ¥æè¿°é•¿åº¦
        patterns = analysis.get("description_patterns", {})
        length_dist = patterns.get("length_distribution", {})
        if length_dist.get("short", 0) > length_dist.get("medium", 0):
            suggestions.append(
                "ğŸ’¡ å»ºè®®: å¾ˆå¤šå¤±è´¥æè¿°è¿‡çŸ­ï¼Œå»ºè®®ç”¨æˆ·æä¾›æ›´è¯¦ç»†çš„æè¿°"
            )

        # æ£€æŸ¥ç­–ç•¥æ•ˆæœ
        strategies = analysis.get("strategies", {})
        if strategies.get("position+type", 0) > strategies.get("text", 0):
            suggestions.append(
                "ğŸ’¡ å»ºè®®: position+type ç­–ç•¥å¤±è´¥è¾ƒå¤šï¼Œä¼˜åŒ–ä½ç½®èŒƒå›´æˆ–ç±»å‹åŒ¹é…"
            )

        return suggestions

    def generate_daily_report(self, date: str = None) -> str:
        """
        ç”Ÿæˆæ¯æ—¥æŠ¥å‘Š

        Args:
            date: æ—¥æœŸï¼ˆYYYY-MM-DDï¼‰ï¼Œé»˜è®¤ä¸ºä»Šå¤©

        Returns:
            Markdown æ ¼å¼çš„æŠ¥å‘Š
        """
        stats = self.logger.get_stats(date)

        if not stats:
            return f"# NLP æ‰§è¡ŒæŠ¥å‘Š\n\næ—¥æœŸ: {date or datetime.now().strftime('%Y-%m-%d')}\n\næ²¡æœ‰æ•°æ®"

        report = f"""# NLP æ‰§è¡ŒæŠ¥å‘Š

**æ—¥æœŸ**: {stats['date']}
**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

## ğŸ“Š æ€»ä½“ç»Ÿè®¡

- **æ€»æ‰§è¡Œæ¬¡æ•°**: {stats['total']}
- **æˆåŠŸæ¬¡æ•°**: {stats['success']} ({stats['success']/stats['total']*100:.1f}%)
- **å¤±è´¥æ¬¡æ•°**: {stats['failure']} ({stats['failure']/stats['total']*100:.1f}%)
- **æˆåŠŸç‡**: {stats['success']/stats['total']*100:.1f}%

## ğŸ“ˆ æŒ‰ç­–ç•¥ç»Ÿè®¡

| ç­–ç•¥ | ä½¿ç”¨æ¬¡æ•° | å æ¯” |
|------|---------|------|
"""

        for strategy, count in stats['by_strategy'].most_common():
            percentage = count / stats['total'] * 100
            report += f"| {strategy} | {count} | {percentage:.1f}% |\n"

        report += f"""
## ğŸ“ æŒ‰ä½ç½®ç»Ÿè®¡

| ä½ç½® | ä½¿ç”¨æ¬¡æ•° | å æ¯” |
|------|---------|------|
"""

        for position, count in stats['by_position'].most_common():
            percentage = count / stats['total'] * 100
            report += f"| {position} | {count} | {percentage:.1f}% |\n"

        report += f"""
## ğŸ¯ æŒ‰ç±»å‹ç»Ÿè®¡

| ç±»å‹ | ä½¿ç”¨æ¬¡æ•° | å æ¯” |
|------|---------|------|
"""

        for elem_type, count in stats['by_type'].most_common():
            percentage = count / stats['total'] * 100
            report += f"| {elem_type} | {count} | {percentage:.1f}% |\n"

        report += f"""
## âŒ æœ€è¿‘å¤±è´¥è®°å½•

| æ—¶é—´ | æè¿° | åŸå›  |
|------|------|------|
"""

        for failure in stats['failures'][-10:]:
            report += f"| {failure['time'].split('T')[1][:8]} | {failure['description']} | {failure['reason']} |\n"

        return report

    def export_to_csv(self, output_file: str = None) -> str:
        """
        å¯¼å‡ºæ—¥å¿—åˆ° CSV

        Args:
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„

        Returns:
            è¾“å‡ºæ–‡ä»¶è·¯å¾„
        """
        import csv

        if output_file is None:
            output_file = self.log_dir / f"export_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"

        output_file = Path(output_file)

        # è¯»å–æ‰€æœ‰æ‰§è¡Œæ—¥å¿—
        records = []
        with open(self.logger.execution_log, 'r', encoding='utf-8') as f:
            for line in f:
                if line.strip():
                    records.append(json.loads(line))

        if not records:
            return "æ²¡æœ‰æ•°æ®å¯å¯¼å‡º"

        # å†™å…¥ CSV
        with open(output_file, 'w', newline='', encoding='utf-8') as f:
            writer = csv.DictWriter(f, fieldnames=records[0].keys())
            writer.writeheader()
            writer.writerows(records)

        return str(output_file)


def main():
    """ä¸»å‡½æ•° - å‘½ä»¤è¡Œæ¥å£"""
    import argparse

    parser = argparse.ArgumentParser(description='NLP æ—¥å¿—åˆ†æå·¥å…·')
    parser.add_argument('--log-dir', help='æ—¥å¿—ç›®å½•')
    parser.add_argument('--date', help='åˆ†ææ—¥æœŸ (YYYY-MM-DD)')
    parser.add_argument('--export', action='store_true', help='å¯¼å‡ºä¸º CSV')
    parser.add_argument('--report', action='store_true', help='ç”Ÿæˆæ¯æ—¥æŠ¥å‘Š')
    parser.add_argument('--failures', type=int, default=50, help='åˆ†æçš„å¤±è´¥è®°å½•æ•°é‡')

    args = parser.parse_args()

    analyzer = NLPLogAnalyzer(args.log_dir)

    if args.report:
        print("\n" + "="*60)
        print("ç”Ÿæˆæ¯æ—¥æŠ¥å‘Š")
        print("="*60 + "\n")
        report = analyzer.generate_daily_report(args.date)
        print(report)

        # ä¿å­˜æŠ¥å‘Š
        report_file = analyzer.log_dir / f"report_{args.date or datetime.now().strftime('%Y-%m-%d')}.md"
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report)
        print(f"\næŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")

    if args.export:
        print("\n" + "="*60)
        print("å¯¼å‡ºæ—¥å¿—åˆ° CSV")
        print("="*60 + "\n")
        csv_file = analyzer.export_to_csv()
        print(f"å·²å¯¼å‡ºåˆ°: {csv_file}")

    # é»˜è®¤åˆ†æå¤±è´¥è®°å½•
    if not args.report and not args.export:
        print("\n" + "="*60)
        print("åˆ†æå¤±è´¥è®°å½•")
        print("="*60 + "\n")

        analysis = analyzer.analyze_failures(args.failures)

        print(f"ğŸ“Š åˆ†ææœ€è¿‘ {analysis['total']} æ¡å¤±è´¥è®°å½•\n")

        print("âŒ å¤±è´¥åŸå› åˆ†å¸ƒ:")
        for reason, count in analysis['failure_reasons'].items():
            print(f"  â€¢ {reason}: {count} æ¬¡")

        print("\nğŸ“ å¤±è´¥çš„ä½ç½®åˆ†å¸ƒ:")
        for position, count in analysis['positions'].items():
            print(f"  â€¢ {position}: {count} æ¬¡")

        print("\nğŸ¯ å¤±è´¥çš„ç±»å‹åˆ†å¸ƒ:")
        for elem_type, count in analysis['types'].items():
            print(f"  â€¢ {elem_type}: {count} æ¬¡")

        print("\nğŸ”§ ä¼˜åŒ–å»ºè®®:")
        suggestions = analyzer.generate_optimization_suggestions()
        if suggestions:
            for suggestion in suggestions:
                print(f"  {suggestion}")
        else:
            print("  âœ… æš‚æ— ä¼˜åŒ–å»ºè®®")


if __name__ == '__main__':
    main()
