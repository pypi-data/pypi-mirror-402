{
"cells": [
{
"cell_type": "markdown",
"metadata": {},
"source": [
"# {{ project_name }} - Base Notebook\n",
"\n",
"**Author:** {{ author_name }}\n",
"**Description:** {{ description }}"
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {},
"outputs": [],
"source": [
"# Universal Setup (Local / Colab / Kaggle)\n",
"import sys\n",
"import os\n",
"import yaml\n",
"# Check if running on Colab\n",
"if 'google.colab' in sys.modules:\n",
" print('Detected Google Colab environment')\n",
" # Clone/Install the package if needed\n",
" # !pip install git+https://github.com/{{ author_name | lower | replace(' ', '') }}/{{ project_name }}.git\n",
"\n",
"from {{ package_name }} import get_dataset_path, SETTINGS\n",
"\n",
"print(f\"Project: {SETTINGS.get('project_name')}\")"
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {},
"outputs": [],
"source": [
"# Standard Imports\n",
"import numpy as np\n",
"import pandas as pd\n",
"import matplotlib.pyplot as plt\n",
"import seaborn as sns\n",
"import kagglehub as kh\n",
"from kagglehub import KaggleDatasetAdapter\n",
"\n",
"{% if project_type == 'dl' %}\n",
"{% if framework == 'pytorch' %}\n",
"import torch\n",
"import torch.nn as nn\n",
"import torch.optim as optim\n",
"print(f\"PyTorch Version: {torch.__version__}\")\n",
"print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
"{% elif framework == 'tensorflow' %}\n",
"import tensorflow as tf\n",
"print(f\"TensorFlow Version: {tf.__version__}\")\n",
"print(f\"GPU Available: {len(tf.config.list_physical_devices('GPU')) > 0}\")\n",
"{% endif %}\n",
"{% elif project_type == 'ml' %}\n",
"from sklearn.model_selection import train_test_split\n",
"from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
"from sklearn.compose import ColumnTransformer, make_column_selector\n",
"from sklearn.pipeline import Pipeline\n",
"{% endif %}\n",
"\n",
"%matplotlib inline"
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {},
"outputs": [],
"source": [
"# Load Dataset using Config + KaggleHub\n",
"# Example: Loading Titanic dataset defined in config.yaml\n",
"\n",
"try:\n",
" # Load dataset path from config (or use direct name for kagglehub)\n",
" # The config.yaml defines 'titanic': 'heptapod/titanic'\n",
" dataset_name = SETTINGS.get('datasets', {}).get('titanic', 'heptapod/titanic')\n",
" print(f\"Loading dataset: {dataset_name}...\")\n",
" \n",
" path = kh.dataset_download(dataset_name)\n",
" print(f\"Path: {path}\")\n",
" \n",
" # Example loading csv found in the path\n",
" # Adjust 'train.csv' based on actual dataset content\n",
" df = pd.read_csv(f\"{path}/train.csv\")\n",
" display(df.head())\n",
" df.info()\n",
"except Exception as e:\n",
" print(f\"Failed to load dataset: {e}\")"
]
}
],
"metadata": {
"kernelspec": {
"display_name": "Python 3",
"language": "python",
"name": "python3"
},
"language_info": {
"codemirror_mode": {
"name": "ipython",
"version": 3
},
"file_extension": ".py",
"mimetype": "text/x-python",
"name": "python",
"nbconvert_exporter": "python",
"pygments_lexer": "ipython3",
"version": "3.11.0"
}
},
"nbformat": 4,
"nbformat_minor": 5
}