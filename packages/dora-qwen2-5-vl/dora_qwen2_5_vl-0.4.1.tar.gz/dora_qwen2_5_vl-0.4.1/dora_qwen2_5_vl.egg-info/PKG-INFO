Metadata-Version: 2.4
Name: dora-qwen2-5-vl
Version: 0.4.1
Summary: Dora Node for VLM
Author-email: Haixuan Xavier Tao <tao.xavier@outlook.com>, Enzo Le Van <dev@enzo-le-van.fr>
License: MIT
Requires-Python: >=3.10
Description-Content-Type: text/markdown
Requires-Dist: dora-rs>=0.3.9
Requires-Dist: numpy<2.0.0
Requires-Dist: torch>=2.7.0
Requires-Dist: torchvision>=0.22
Requires-Dist: torchaudio>=2.7.0
Requires-Dist: qwen-vl-utils>=0.0.5
Requires-Dist: opencv-python>=4.1.1
Requires-Dist: modelscope>=1.18.1
Requires-Dist: peft==0.13.2
Requires-Dist: accelerate>=1.3.0
Requires-Dist: transformers
Requires-Dist: setuptools>=65.0.0

# Dora QwenVL2.5 node

Experimental node for using a VLM within dora.

## YAML Specification

This node is supposed to be used as follows:

```yaml
- id: dora-qwenvl
  build: pip install dora-qwen2-5-vl
  path: dora-qwen2-5-vl
  inputs:
    image:
      source: camera/image
      queue_size: 1
    text: dora-distil-whisper/text
  outputs:
    - text
  env:
    DEFAULT_QUESTION: Describe the image in a very short sentence.
```

## Additional documentation

- Qwenvl: https://github.com/QwenLM/Qwen-VL

## Examples

- Vision Language Model
  - Github: https://github.com/dora-rs/dora-hub/blob/main/examples/vlm
