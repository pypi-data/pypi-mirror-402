from __future__ import annotations

import warnings
from dataclasses import dataclass
from pathlib import Path
from time import time
from typing import Any, Optional, Tuple, List, Callable, Dict

import numpy as np

from flowreg3d._runtime import RuntimeContext
from flowreg3d.core.optical_flow_3d import get_displacement, imregister_wrapper
import flowreg3d.util.image_processing_3D as im3d
from flowreg3d.motion_correction.OF_options_3D import OutputFormat

# Import to trigger executor registration (side effect)


@dataclass
class RegistrationConfig:
    """Simplified configuration."""

    n_jobs: int = -1  # -1 = all cores
    batch_size: int = 10  # Smaller for 3D volumes
    verbose: bool = False
    parallelization: Optional[str] = (
        None  # None = auto-select, or 'sequential', 'threading', 'multiprocessing'
    )


class BatchMotionCorrector:
    """
    Main registration pipeline.
    """

    def __init__(self, options: Any, config: Optional[RegistrationConfig] = None):
        self.options = options
        self.config = config or RegistrationConfig()

        # Statistics
        self.mean_disp: List[float] = []
        self.max_disp: List[float] = []
        self.mean_div: List[float] = []
        self.mean_translation: List[float] = []

        # State
        self.reference_raw: Optional[np.ndarray] = None
        self.reference_proc: Optional[np.ndarray] = None
        self.weight: Optional[np.ndarray] = None
        self.w_init: Optional[np.ndarray] = None

        # I/O
        self.video_reader = None
        self.video_writer = None
        self.w_writer = None

        # Progress callbacks
        self.progress_callbacks: List[Callable[[int, int], None]] = []
        # Task-based progress tracking: {task_id: (frames_processed, total_frames)}
        self._progress_trackers: Dict[str, Tuple[int, Optional[int]]] = {}

        # Get number of workers
        if self.config.n_jobs == -1:
            import os

            self.n_workers = os.cpu_count() or 4
        else:
            self.n_workers = self.config.n_jobs

        # Initialize executor from RuntimeContext
        self._setup_executor()

    def _setup_executor(self):
        """Setup the parallelization executor based on configuration."""
        # Get executor class from RuntimeContext
        executor_name = self.config.parallelization

        if executor_name is None:
            # Auto-select based on available parallelization (3D versions)
            available = RuntimeContext.get("available_parallelization", set())
            if "multiprocessing3d" in available:
                executor_name = "multiprocessing3d"
            elif "threading3d" in available:
                executor_name = "threading3d"
            else:
                executor_name = "sequential3d"
        else:
            # Add 3d suffix if not already present
            if not executor_name.endswith("3d"):
                executor_name = executor_name + "3d"

        # Get executor class
        executor_class = RuntimeContext.get_parallelization_executor(executor_name)
        if executor_class is None:
            # Fallback to sequential3d if requested executor not available
            if not self.config.verbose:
                print(
                    f"Warning: {executor_name} executor not available, falling back to sequential3d"
                )
            executor_class = RuntimeContext.get_parallelization_executor("sequential3d")

            # If sequential3d is also not available, import and register it
            if executor_class is None:
                from flowreg3d.motion_correction.parallelization.sequential_3d import (
                    SequentialExecutor3D,
                )

                SequentialExecutor3D.register()
                executor_class = RuntimeContext.get_parallelization_executor(
                    "sequential3d"
                )

                # Final safety check
                if executor_class is None:
                    raise RuntimeError(
                        "Could not load any executor, including sequential3d fallback"
                    )

        # Create executor instance
        self.executor = executor_class(n_workers=self.n_workers)

        if not self.config.verbose:
            print(f"Using {executor_name} executor with {self.n_workers} workers")

    def register_progress_callback(self, callback: Callable[[int, int], None]) -> None:
        """
        Register a progress callback function.

        Args:
            callback: Function that receives (current_frame, total_frames) as arguments.
                      For multiprocessing, updates are batch-wise rather than frame-wise.
        """
        if callback not in self.progress_callbacks:
            self.progress_callbacks.append(callback)

    def _notify_progress(self, frames_completed: int, task_id: str = "main") -> None:
        """
        Notify all registered progress callbacks for a specific task.

        Args:
            frames_completed: Number of frames just completed (to add to total)
            task_id: Identifier for the task being tracked (default: "main")
        """
        # Initialize task tracker if needed
        if task_id not in self._progress_trackers:
            # For main task, use the total frames from video reader
            total = self._total_frames if task_id == "main" else None
            self._progress_trackers[task_id] = (0, total)

        # Update progress for this task
        current, total = self._progress_trackers[task_id]
        current += frames_completed
        self._progress_trackers[task_id] = (current, total)

        # Only notify callbacks for the main task
        if task_id == "main" and total and self.progress_callbacks:
            for callback in self.progress_callbacks:
                try:
                    callback(current, total)
                except Exception as e:
                    warnings.warn(f"Progress callback error: {e}")

    def _setup_io(self):
        """Setup I/O handlers."""
        output_path = Path(self.options.output_path)
        output_path.mkdir(parents=True, exist_ok=True)

        self.video_reader = self.options.get_video_reader()
        self.video_writer = self.options.get_video_writer()

        # Create displacement writer if requested
        if getattr(self.options, "save_w", False):
            try:
                from flowreg3d.util.io.factory import get_video_file_writer

                # Use ArrayWriter for displacements when main output is ARRAY
                if self.options.output_format == OutputFormat.ARRAY:
                    self.w_writer = get_video_file_writer(
                        None,  # Path ignored for ARRAY format
                        "ARRAY",
                    )
                else:
                    # Use HDF5 for file-based output (preserves double precision)
                    w_path = output_path / "w.h5"
                    self.w_writer = get_video_file_writer(
                        str(w_path),
                        "HDF5",
                        dataset_names=["u", "v", "w"],  # 3D flow components
                    )
            except Exception as e:
                warnings.warn(
                    f"Failed to create displacement writer: {e}. Displacements will not be saved."
                )
                self.w_writer = None
                self.options.save_w = False  # Disable to avoid trying to write later

    def _setup_reference(self, reference_frame: Optional[np.ndarray] = None):
        """Setup reference frame and weights."""
        if reference_frame is None:
            self.reference_raw = self.options.get_reference_frame(
                self.video_reader
            ).astype(np.float64)
        else:
            self.reference_raw = reference_frame.astype(np.float64)

        # For 3D: shape is (Z, Y, X, C) or (Z, Y, X)
        Z, Y, X = self.reference_raw.shape[:3]
        n_channels = self.reference_raw.shape[3] if self.reference_raw.ndim == 4 else 1

        # Setup weights for 3D
        self.weight = np.ones((Z, Y, X, n_channels), dtype=np.float64)
        if hasattr(self.options, "get_weight_at"):
            for c in range(n_channels):
                self.weight[:, :, :, c] = self.options.get_weight_at(c, n_channels)
        else:
            weight_1d = np.asarray(getattr(self.options, "weight", [1.0] * n_channels))
            weight_sum = weight_1d.sum()
            if weight_sum > 0:
                weight_1d = weight_1d / weight_sum
            for c in range(n_channels):
                self.weight[:, :, :, c] = (
                    weight_1d[c] if c < len(weight_1d) else 1.0 / n_channels
                )

        # Preprocess reference (MATLAB order: normalize then filter)
        self.reference_proc = self._preprocess_frames(self.reference_raw)

    def _preprocess_frames(
        self, frames: np.ndarray, normalization_ref: Optional[np.ndarray] = None
    ) -> np.ndarray:
        """Preprocess frames: normalize -> filter (MATLAB order).

        Args:
            frames: Frames to preprocess
            normalization_ref: Reference for normalization. If None, uses frames' own min/max.
                              For consistent normalization, pass reference_raw.
        """
        # First normalize
        normalized = im3d.normalize(
            frames,
            ref=normalization_ref,
            channel_normalization=getattr(
                self.options, "channel_normalization", "together"
            ),
        )
        # Then filter
        filtered = im3d.apply_gaussian_filter(
            normalized,
            sigma=np.asarray(self.options.sigma),
            mode="reflect",
            truncate=4.0,
        )
        return filtered.astype(np.float64)

    def _compute_flow_single(
        self,
        frame_proc: np.ndarray,
        ref_proc: np.ndarray,
        w_init: Optional[np.ndarray] = None,
    ) -> np.ndarray:
        """Compute flow for a single frame."""
        flow_params = {
            "alpha": self.options.alpha,
            "weight": self.weight,
            "levels": self.options.levels,
            "min_level": getattr(
                self.options,
                "effective_min_level",
                getattr(self.options, "min_level", 0),
            ),
            "eta": self.options.eta,
            "update_lag": self.options.update_lag,
            "iterations": self.options.iterations,
            "a_smooth": self.options.a_smooth,
            "a_data": self.options.a_data,
        }

        if w_init is not None:
            flow_params["uvw"] = w_init

        # Note: get_displacement expects (reference, moving)
        return get_displacement(ref_proc, frame_proc, **flow_params)

    def _process_batch_parallel(
        self,
        batch: np.ndarray,
        batch_proc: np.ndarray,
        w_init: np.ndarray,
        task_id: str = "main",
    ) -> Tuple[np.ndarray, np.ndarray]:
        """Process batch using the configured executor.

        Args:
            batch: Raw batch of frames
            batch_proc: Preprocessed batch
            w_init: Initial displacement field
            task_id: Task identifier for progress tracking (default: "main")
        """
        # Build flow parameters dictionary
        flow_params = {
            "alpha": self.options.alpha,
            "weight": self.weight,
            "levels": self.options.levels,
            "min_level": getattr(
                self.options,
                "effective_min_level",
                getattr(self.options, "min_level", 0),
            ),
            "eta": self.options.eta,
            "update_lag": self.options.update_lag,
            "iterations": self.options.iterations,
            "a_smooth": self.options.a_smooth,
            "a_data": self.options.a_data,
        }

        # Get interpolation method
        interp_method = getattr(self.options, "interpolation_method", "cubic")

        # Create progress callback wrapper for executor if we have callbacks registered
        # Only track progress for "main" task
        executor_progress_callback = None
        if self.progress_callbacks and task_id == "main":

            def executor_progress_callback(frames_completed: int):
                self._notify_progress(frames_completed, task_id)

        # Use executor to process batch
        return self.executor.process_batch(
            batch=batch,
            batch_proc=batch_proc,
            reference_raw=self.reference_raw,
            reference_proc=self.reference_proc,
            w_init=w_init,
            get_displacement_func=get_displacement,
            imregister_func=imregister_wrapper,
            interpolation_method=interp_method,
            progress_callback=executor_progress_callback,
            flow_params=flow_params,
        )

    def _compute_initial_w(
        self, first_batch: np.ndarray, first_batch_proc: np.ndarray
    ) -> np.ndarray:
        """Compute initial displacement field from first frames."""
        # Check if CC initialization is enabled
        use_cc = getattr(self.options, "cc_initialization", False)

        if use_cc:
            # For CC initialization, start with zero flow
            # The CC will be computed in the workers
            Z, Y, X = self.reference_proc.shape[:3]
            w_init = np.zeros((Z, Y, X, 3), dtype=np.float32)

            if not self.config.verbose:
                print("Using cross-correlation initialization (w_init = 0)")
        else:
            # Original auto-initialization from first frames
            n_init = min(22, first_batch.shape[0])  # T is first dimension

            if not self.config.verbose:
                print("Computing initial displacement field...")

            # Process first n_init frames - use "initial_w" task to avoid counting toward main progress
            if n_init > 4:  # Use parallel for multiple frames
                _, w = self._process_batch_parallel(
                    first_batch[:n_init],
                    first_batch_proc[:n_init],
                    np.zeros(
                        (
                            self.reference_proc.shape[0],
                            self.reference_proc.shape[1],
                            self.reference_proc.shape[2],
                            3,
                        )
                    ),
                    task_id="initial_w",  # Don't count toward main progress
                )
            else:  # Serial for very few frames
                Z, Y, X = self.reference_proc.shape[:3]
                w = np.zeros((n_init, Z, Y, X, 3), dtype=np.float32)
                for t in range(n_init):
                    w[t] = self._compute_flow_single(
                        first_batch_proc[t], self.reference_proc
                    )

            # Average flows
            w_init = np.mean(w, axis=0)

            if not self.config.verbose:
                print("Done pre-registration to get w_init.")

        return w_init

    def _update_reference(self, batch_proc: np.ndarray, w: np.ndarray):
        """Update reference using compensated frames (MATLAB-compatible)."""
        n_ref_frames = min(100, batch_proc.shape[0])  # T is first dimension
        if n_ref_frames < 1:
            return

        # Use last n_ref_frames
        start_idx = batch_proc.shape[0] - n_ref_frames

        # Compensate and average per channel
        Z, Y, X, C = self.reference_proc.shape
        new_ref = np.zeros_like(self.reference_proc)

        for c in range(C):
            compensated = np.zeros((n_ref_frames, Z, Y, X), dtype=np.float64)
            for t in range(n_ref_frames):
                volume_c = (
                    batch_proc[start_idx + t, :, :, :, c]
                    if C > 1
                    else batch_proc[start_idx + t, :, :, :, 0]
                )
                interp_method = getattr(self.options, "interpolation_method", "cubic")
                compensated[t] = imregister_wrapper(
                    volume_c,
                    w[start_idx + t, :, :, :, 0],  # u displacement
                    w[start_idx + t, :, :, :, 1],  # v displacement
                    w[start_idx + t, :, :, :, 2],  # w displacement
                    self.reference_proc[:, :, :, c]
                    if C > 1
                    else self.reference_proc[:, :, :, 0],
                    interpolation_method=interp_method,
                )
            new_ref[:, :, :, c] = np.mean(compensated, axis=0)

        self.reference_proc = new_ref

    def run(self, reference_frame: Optional[np.ndarray] = None) -> np.ndarray:
        """Run complete registration pipeline."""

        # Setup
        self._setup_io()
        self._setup_reference(reference_frame)

        # Initialize total frames for progress tracking
        self._total_frames = len(self.video_reader) if self.video_reader else None

        if not self.config.verbose:
            quality = getattr(self.options, "quality_setting", "balanced")
            print(f"\nStarting compensation with quality={quality}")
            print(f"Batch size: {self.config.batch_size}, Workers: {self.n_workers}")

        # Process batches
        batch_idx = 0
        total_frames = 0
        start_time = time()

        try:
            while self.video_reader.has_batch():
                batch_idx += 1
                batch_start = time()

                # Read batch
                batch = self.video_reader.read_batch()  # (T,Z,Y,X,C)

                # Preprocess entire batch (normalize -> filter)
                # Use reference_raw for normalization to match how reference_proc was created
                batch_proc = self._preprocess_frames(
                    batch, normalization_ref=self.reference_raw
                )

                # First batch: compute w_init
                if batch_idx == 1:
                    self.w_init = self._compute_initial_w(batch, batch_proc)

                # Decide whether to use w_init
                if not getattr(self.options, "update_initialization_w", True):
                    current_w_init = np.zeros_like(self.w_init)
                else:
                    current_w_init = self.w_init

                # Process batch in parallel (progress callbacks handled internally)
                registered, w = self._process_batch_parallel(
                    batch, batch_proc, current_w_init
                )

                # Update w_init for next batch
                if getattr(self.options, "update_initialization_w", True):
                    if w.shape[0] > 20:
                        self.w_init = np.mean(w[-20:], axis=0)
                    else:
                        self.w_init = np.mean(w, axis=0)

                # Compute statistics for 3D
                disp_magnitude = np.sqrt(
                    w[:, :, :, :, 0] ** 2
                    + w[:, :, :, :, 1] ** 2
                    + w[:, :, :, :, 2] ** 2
                )
                self.mean_disp.extend(np.mean(disp_magnitude, axis=(1, 2, 3)).tolist())
                self.max_disp.extend(np.max(disp_magnitude, axis=(1, 2, 3)).tolist())

                # Divergence and translation for 3D
                for t in range(w.shape[0]):
                    du_dx = np.gradient(w[t, :, :, :, 0], axis=2)  # x-axis is dim 2
                    dv_dy = np.gradient(w[t, :, :, :, 1], axis=1)  # y-axis is dim 1
                    dw_dz = np.gradient(w[t, :, :, :, 2], axis=0)  # z-axis is dim 0
                    self.mean_div.append(float(np.mean(du_dx + dv_dy + dw_dz)))

                    u_mean = float(np.mean(w[t, :, :, :, 0]))
                    v_mean = float(np.mean(w[t, :, :, :, 1]))
                    w_mean = float(np.mean(w[t, :, :, :, 2]))
                    self.mean_translation.append(
                        float(np.sqrt(u_mean**2 + v_mean**2 + w_mean**2))
                    )

                # Write results
                self.video_writer.write_frames(registered)

                # Save flows if requested
                if getattr(self.options, "save_w", False):
                    if self.w_writer is not None:
                        # w has shape (T, Z, Y, X, 3) where last dimension is [u, v, w]
                        # Writer with dataset_names=['u', 'v', 'w'] will split into separate datasets
                        self.w_writer.write_frames(w)
                    else:
                        warnings.warn(
                            "Displacement saving was requested but writer could not be initialized. Skipping displacement save."
                        )

                # Update reference if requested
                if getattr(self.options, "update_reference", False):
                    self._update_reference(batch_proc, w)

                # Progress
                total_frames += registered.shape[0]
                batch_time = time() - batch_start

                if not self.config.verbose:
                    fps = registered.shape[0] / batch_time
                    print(
                        f"Batch {batch_idx}: {registered.shape[0]} frames in {batch_time:.2f}s ({fps:.1f} fps)"
                    )

        finally:
            # Cleanup executor
            if self.executor is not None:
                self.executor.cleanup()

        # Final stats
        total_time = time() - start_time
        if not self.config.verbose:
            avg_fps = total_frames / max(1e-6, total_time)
            print(
                f"\nProcessed {total_frames} frames in {total_time:.2f}s (avg {avg_fps:.1f} fps)"
            )

        # Save metadata
        self._save_metadata()

        # Cleanup
        self._cleanup()

        return self.reference_raw

    def _save_metadata(self):
        """Save statistics and reference frame."""
        if not getattr(self.options, "save_meta_info", False):
            return

        output_path = Path(self.options.output_path)

        # Save statistics
        stats_path = output_path / "statistics.npz"
        np.savez(
            str(stats_path),
            mean_disp=np.array(self.mean_disp),
            max_disp=np.array(self.max_disp),
            mean_div=np.array(self.mean_div),
            mean_translation=np.array(self.mean_translation),
        )

        # Save reference
        if self.reference_raw is not None:
            ref_path = output_path / "reference_frame.npy"
            np.save(str(ref_path), self.reference_raw)

        print(f"Saved metadata to {output_path}")

    def _cleanup(self):
        """Close file handlers."""
        if self.video_writer is not None:
            self.video_writer.close()
        if hasattr(self, "w_writer") and self.w_writer is not None:
            self.w_writer.close()


def compensate_recording(
    options: Any,
    reference_frame: Optional[np.ndarray] = None,
    config: Optional[RegistrationConfig] = None,
) -> np.ndarray:
    """
    Main entry point matching MATLAB API.

    Args:
        options: OF_options object with parameters
        reference_frame: Optional pre-computed reference
        config: Optional registration configuration

    Returns:
        The reference frame used
    """
    pipeline = BatchMotionCorrector(options, config)
    return pipeline.run(reference_frame)


if __name__ == "__main__":
    try:
        from OF_options import OFOptions

        options = OFOptions(
            input_file="test_video.h5",
            output_path="results",
            quality_setting="balanced",
            save_w=True,
            sigma=[[1.0, 1.0, 0.5], [1.0, 1.0, 0.5]],  # [sx, sy, st] per channel
            update_reference=False,
            update_initialization_w=True,
        )

        config = RegistrationConfig(
            n_jobs=-1,  # Use all cores
            batch_size=100,
        )

        ref = compensate_recording(options, config=config)

    except ImportError:
        print("OF_options not available")
