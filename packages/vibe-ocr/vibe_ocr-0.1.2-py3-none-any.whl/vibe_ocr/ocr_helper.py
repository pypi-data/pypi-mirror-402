"""
OCR Helper Class - åŸºäºPaddleOCRçš„æ–‡å­—è¯†åˆ«å’Œå®šä½å·¥å…·ç±»
æä¾›ç»Ÿä¸€çš„æ¥å£ä¾›å¤–éƒ¨è°ƒç”¨ï¼Œè¾“å…¥å›¾åƒæ–‡ä»¶å’Œè¦æŸ¥æ‰¾çš„æ–‡å­—ï¼Œè¿”å›æ–‡å­—æ‰€åœ¨çš„å›¾ç‰‡åŒºåŸŸ
æ”¯æŒåŒºåŸŸåˆ†å‰²åŠŸèƒ½ï¼Œå¯ä»¥æŒ‡å®šåªè¯†åˆ«å±å¹•çš„ç‰¹å®šåŒºåŸŸï¼Œå¤§å¤§æå‡è¯†åˆ«é€Ÿåº¦
"""

import base64
import json
import logging
import os
import sqlite3
import time
import uuid
from datetime import datetime
from typing import Any, Callable, Dict, List, Optional, Tuple, Union

import cv2
import imagehash
import requests
from dotenv import load_dotenv
from PIL import Image

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()


class OCRHelper:
    """OCRè¾…åŠ©å·¥å…·ç±»ï¼Œå°è£…PaddleOCRåŠŸèƒ½"""

    def __init__(
        self,
        output_dir="output",
        resize_image=True,
        max_width=960,
        delete_temp_screenshots=True,
        max_cache_size=200,
        hash_type="dhash",  # å¯é€‰: "phash", "dhash", "ahash", "whash"
        hash_threshold=10,  # hash æ±‰æ˜è·ç¦»é˜ˆå€¼
        correction_map: Optional[Dict[str, str]] = None,
        snapshot_func: Optional[Callable[..., Any]] = None,
    ):
        """
        åˆå§‹åŒ–OCR Helper

        Args:
            output_dir (str): è¾“å‡ºç›®å½•è·¯å¾„
            resize_image (bool): æ˜¯å¦è‡ªåŠ¨ç¼©å°å›¾ç‰‡ä»¥æå‡é€Ÿåº¦
            max_width (int): å›¾ç‰‡æœ€å¤§å®½åº¦ï¼Œé»˜è®¤960ï¼ˆå»ºè®®åœ¨640-960ä¹‹é—´ï¼‰
            delete_temp_screenshots (bool): æ˜¯å¦åˆ é™¤ä¸´æ—¶æˆªå›¾æ–‡ä»¶ï¼Œé»˜è®¤ä¸ºTrue
            max_cache_size (int): æœ€å¤§ç¼“å­˜æ¡ç›®æ•°ï¼Œé»˜è®¤200
            hash_type (str): å“ˆå¸Œç®—æ³•ç±»å‹ï¼Œé»˜è®¤"dhash"ï¼ˆå·®åˆ†å“ˆå¸Œï¼Œæœ€å¿«ï¼‰
            hash_threshold (int): å“ˆå¸Œæ±‰æ˜è·ç¦»é˜ˆå€¼ï¼Œé»˜è®¤10
            correction_map (dict): OCR çº æ­£æ˜ å°„ï¼Œä¾‹å¦‚ {"è£…å„": "è£…å¤‡"}
            snapshot_func (callable): è‡ªå®šä¹‰æˆªå›¾å‡½æ•°ï¼Œæ¥å— filename å‚æ•°
        """
        self.output_dir = output_dir
        self.resize_image = resize_image
        self.max_width = max_width
        self.delete_temp_screenshots = delete_temp_screenshots
        self.max_cache_size = max_cache_size
        self.hash_type = hash_type
        self.hash_threshold = hash_threshold
        self.correction_map = correction_map or {}
        self.snapshot_func = snapshot_func

        self.ocr_url = os.getenv("OCR_SERVER_URL", "http://localhost:8080/ocr")

        # åˆ›å»ºè¾“å‡ºç›®å½•
        if not os.path.exists(self.output_dir):
            os.makedirs(self.output_dir)

        # åˆ›å»ºç¼“å­˜ç›®å½•å’Œä¸´æ—¶ç›®å½•
        self.cache_dir = os.path.join(self.output_dir, "cache")
        self.temp_dir = os.path.join(self.output_dir, "temp")
        os.makedirs(self.cache_dir, exist_ok=True)
        os.makedirs(self.temp_dir, exist_ok=True)

        # é…ç½®æ—¥å¿—
        self.logger = logging.getLogger(__name__)

        # åˆå§‹åŒ–ç¼“å­˜ï¼ˆä»…ä¿ç•™å…¼å®¹å±æ€§ï¼Œä¸å†ä¿å­˜å›¾ç‰‡è·¯å¾„ï¼‰
        # æ—§ç‰ˆæ ¼å¼: [(image_path, json_file_path), ...]
        self.ocr_cache = []

        # ç¼“å­˜ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆ95%ä»¥ä¸Šè®¤ä¸ºæ˜¯åŒä¸€å¼ å›¾ï¼‰
        self.cache_similarity_threshold = 0.95

        # åˆå§‹åŒ– SQLite ç¼“å­˜æ•°æ®åº“
        self.cache_db_path = os.path.join(self.cache_dir, "cache.db")
        self._init_cache_db()

        # ä»…ä½¿ç”¨ SQLite å­˜å‚¨ç¼“å­˜ï¼Œé¿å…è½ç›˜å›¾ç‰‡æ–‡ä»¶

    def _init_cache_db(self):
        """
        åˆå§‹åŒ–ç¼“å­˜æ•°æ®åº“ï¼Œåˆ›å»ºå¿…è¦çš„è¡¨
        """
        try:
            with sqlite3.connect(self.cache_db_path) as conn:
                cursor = conn.cursor()
                # åˆ é™¤æ—§ç¼“å­˜è¡¨ï¼Œé¿å…ç»§ç»­è½ç›˜å›¾ç‰‡ç¼“å­˜
                cursor.execute("DROP TABLE IF EXISTS cache_entries")
                # åˆ›å»ºç¼“å­˜è¡¨ï¼ˆä»…ä¿å­˜å“ˆå¸Œä¸ JSON æ•°æ®ï¼Œä¸è½ç›˜å›¾ç‰‡ï¼‰
                cursor.execute("""
                    CREATE TABLE IF NOT EXISTS ocr_cache (
                        image_hash TEXT PRIMARY KEY,
                        phash TEXT,
                        dhash TEXT,
                        ahash TEXT,
                        whash TEXT,
                        regions TEXT,  -- JSON å­˜å‚¨åŒºåŸŸä¿¡æ¯
                        hit_count INTEGER DEFAULT 0,
                        last_access_time REAL,
                        created_time REAL,
                        image_size INTEGER,  -- å›¾ç‰‡å­—èŠ‚å¤§å°
                        json_data TEXT NOT NULL  -- OCR ç»“æœ JSON
                    )
                """)
                # åˆ›å»ºç´¢å¼•ä»¥åŠ é€ŸæŸ¥è¯¢
                cursor.execute("CREATE INDEX IF NOT EXISTS idx_ocr_cache_phash ON ocr_cache(phash)")
                cursor.execute("CREATE INDEX IF NOT EXISTS idx_ocr_cache_dhash ON ocr_cache(dhash)")
                cursor.execute(
                    "CREATE INDEX IF NOT EXISTS idx_ocr_cache_last_access ON ocr_cache(last_access_time)"
                )
                cursor.execute(
                    "CREATE INDEX IF NOT EXISTS idx_ocr_cache_image_hash ON ocr_cache(image_hash)"
                )
                conn.commit()
            self.logger.debug(f"âœ… ç¼“å­˜æ•°æ®åº“åˆå§‹åŒ–æˆåŠŸ: {self.cache_db_path}")
        except Exception as e:
            self.logger.error(f"âŒ åˆå§‹åŒ–ç¼“å­˜æ•°æ®åº“å¤±è´¥: {e}")
            raise

    def _compute_image_hash(
        self,
        image_path: Optional[str] = None,
        image: Optional[Any] = None,
        hash_type: Optional[str] = None,
    ) -> Optional[str]:
        """
        è®¡ç®—å›¾åƒçš„æ„ŸçŸ¥å“ˆå¸Œå€¼

        Args:
            image_path: å›¾åƒè·¯å¾„
            image: OpenCV å›¾åƒå¯¹è±¡
            hash_type: å“ˆå¸Œç±»å‹ ("phash", "dhash", "ahash", "whash")

        Returns:
            å“ˆå¸Œå€¼çš„åå…­è¿›åˆ¶å­—ç¬¦ä¸²ï¼Œå¤±è´¥è¿”å›None
        """
        if hash_type is None:
            hash_type = self.hash_type

        try:
            pil_img = self._to_pil_image(image_path=image_path, image=image)
            if pil_img is None:
                return None
            if hash_type == "phash":
                hash_obj = imagehash.phash(pil_img)
            elif hash_type == "dhash":
                hash_obj = imagehash.dhash(pil_img)
            elif hash_type == "ahash":
                hash_obj = imagehash.average_hash(pil_img)
            elif hash_type == "whash":
                hash_obj = imagehash.whash(pil_img)
            else:
                self.logger.warning(f"æœªçŸ¥çš„å“ˆå¸Œç±»å‹: {hash_type}ï¼Œä½¿ç”¨é»˜è®¤ dhash")
                hash_obj = imagehash.dhash(pil_img)
            return str(hash_obj)
        except Exception as e:
            self.logger.debug(f"è®¡ç®—å›¾åƒå“ˆå¸Œå¤±è´¥: {e}")
            return None

    def _compute_all_hashes(
        self, image_path: Optional[str] = None, image: Optional[Any] = None
    ) -> Dict[str, str]:
        """
        è®¡ç®—å›¾åƒçš„æ‰€æœ‰å“ˆå¸Œå€¼

        Args:
            image_path: å›¾åƒè·¯å¾„
            image: OpenCV å›¾åƒå¯¹è±¡

        Returns:
            åŒ…å«æ‰€æœ‰å“ˆå¸Œå€¼çš„å­—å…¸
        """
        hashes = {}
        try:
            pil_img = self._to_pil_image(image_path=image_path, image=image)
            if pil_img is None:
                return hashes
            hashes["phash"] = str(imagehash.phash(pil_img))
            hashes["dhash"] = str(imagehash.dhash(pil_img))
            hashes["ahash"] = str(imagehash.average_hash(pil_img))
            hashes["whash"] = str(imagehash.whash(pil_img))
        except Exception as e:
            self.logger.debug(f"è®¡ç®—å›¾åƒå“ˆå¸Œå¤±è´¥: {e}")
        return hashes

    def _to_pil_image(
        self, image_path: Optional[str] = None, image: Optional[Any] = None
    ) -> Optional[Image.Image]:
        """
        å°†è¾“å…¥è½¬æ¢ä¸º PIL Image
        """
        try:
            if image is not None:
                if len(image.shape) == 2:
                    return Image.fromarray(image)
                rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
                return Image.fromarray(rgb)
            if image_path:
                with Image.open(image_path) as img:
                    return img.copy()
        except Exception as e:
            self.logger.debug(f"è½¬æ¢ PIL å›¾ç‰‡å¤±è´¥: {e}")
        return None

    def _get_image_bytes(
        self, image_path: Optional[str] = None, image: Optional[Any] = None
    ) -> Optional[bytes]:
        """
        è·å–å›¾åƒå­—èŠ‚æ•°æ®ï¼Œç”¨äºè®¡ç®—ç²¾ç¡®å“ˆå¸Œ
        """
        try:
            if image is not None:
                success, buffer = cv2.imencode(".png", image)
                if not success:
                    return None
                return buffer.tobytes()
            if image_path:
                with open(image_path, "rb") as f:
                    return f.read()
        except Exception as e:
            self.logger.debug(f"è¯»å–å›¾åƒå­—èŠ‚å¤±è´¥: {e}")
        return None

    def _compute_image_md5(
        self, image_path: Optional[str] = None, image: Optional[Any] = None
    ) -> Optional[str]:
        """
        è®¡ç®—å›¾åƒå­—èŠ‚çš„ MD5 å“ˆå¸Œ
        """
        try:
            image_bytes = self._get_image_bytes(image_path=image_path, image=image)
            if image_bytes is None:
                return None
            import hashlib

            return hashlib.md5(image_bytes).hexdigest()
        except Exception as e:
            self.logger.debug(f"è®¡ç®— MD5 å¤±è´¥: {e}")
            return None

    def _calculate_hamming_distance(self, hash1: str, hash2: str) -> int:
        """
        è®¡ç®—ä¸¤ä¸ªå“ˆå¸Œå€¼ä¹‹é—´çš„æ±‰æ˜è·ç¦»

        Args:
            hash1: ç¬¬ä¸€ä¸ªå“ˆå¸Œå€¼
            hash2: ç¬¬äºŒä¸ªå“ˆå¸Œå€¼

        Returns:
            æ±‰æ˜è·ç¦»ï¼ˆä¸åŒä½çš„æ•°é‡ï¼‰
        """
        try:
            # å°†åå…­è¿›åˆ¶è½¬æ¢ä¸ºäºŒè¿›åˆ¶å­—ç¬¦ä¸²
            h1 = int(hash1, 16)
            h2 = int(hash2, 16)
            # å¼‚æˆ–åè®¡ç®—1çš„ä¸ªæ•°
            return bin(h1 ^ h2).count("1")
        except Exception:
            return 999  # è¿”å›ä¸€ä¸ªå¤§å€¼è¡¨ç¤ºæ— æ³•æ¯”è¾ƒ

    def _get_cache_key(self, image_path: str, regions: Optional[List[int]] = None) -> str:
        """
        ç”Ÿæˆç¼“å­˜é”®ï¼ŒåŒ…å«å›¾åƒè·¯å¾„å’ŒåŒºåŸŸä¿¡æ¯

        Args:
            image_path: å›¾åƒè·¯å¾„
            regions: åŒºåŸŸåˆ—è¡¨

        Returns:
            å”¯ä¸€çš„ç¼“å­˜é”®
        """
        if regions:
            regions_str = "_".join(map(str, sorted(regions)))
            return f"{image_path}_{regions_str}"
        return image_path

    def _find_similar_in_cache(
        self,
        image_path: Optional[str] = None,
        image: Optional[Any] = None,
        regions: Optional[List[int]] = None,
    ) -> Optional[Dict[str, Any]]:
        """
        åœ¨ç¼“å­˜ä¸­æŸ¥æ‰¾ç›¸ä¼¼çš„å›¾åƒ

        Args:
            image_path: å›¾åƒè·¯å¾„
            image: OpenCV å›¾åƒå¯¹è±¡
            regions: åŒºåŸŸåˆ—è¡¨ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰

        Returns:
            OCR ç»“æœå­—å…¸ï¼Œæ²¡æ‰¾åˆ°è¿”å›None
        """
        try:
            image_bytes = self._get_image_bytes(image_path=image_path, image=image)
            if image_bytes is None:
                return None

            # è®¡ç®—å½“å‰å›¾åƒçš„å“ˆå¸Œå€¼
            current_hashes = self._compute_all_hashes(image_path=image_path, image=image)
            if not current_hashes.get(self.hash_type):
                return None

            image_hash = self._compute_image_md5(image_path=image_path, image=image)
            if not image_hash:
                return None

            regions_json = json.dumps(sorted(regions)) if regions else None

            # è¿æ¥æ•°æ®åº“
            with sqlite3.connect(self.cache_db_path) as conn:
                cursor = conn.cursor()

                # é¦–å…ˆæ£€æŸ¥æ˜¯å¦æœ‰å®Œå…¨ç›¸åŒçš„å›¾åƒï¼ˆé€šè¿‡æ–‡ä»¶å“ˆå¸Œï¼‰
                if regions_json is None:
                    cursor.execute(
                        "SELECT json_data FROM ocr_cache WHERE image_hash = ?",
                        (image_hash,),
                    )
                else:
                    cursor.execute(
                        "SELECT json_data FROM ocr_cache WHERE image_hash = ? AND regions = ?",
                        (image_hash, regions_json),
                    )
                result = cursor.fetchone()
                if result and result[0]:
                    self.logger.debug("?? ç¼“å­˜å‘½ä¸­ï¼ˆå®Œå…¨ç›¸åŒï¼‰")
                    # æ›´æ–°è®¿é—®ä¿¡æ¯
                    cursor.execute(
                        "UPDATE ocr_cache SET hit_count = hit_count + 1, last_access_time = ? WHERE image_hash = ?",
                        (time.time(), image_hash),
                    )
                    conn.commit()
                    return json.loads(result[0])

                # æŸ¥æ‰¾ç›¸ä¼¼çš„å›¾åƒï¼ˆåŸºäºæ„ŸçŸ¥å“ˆå¸Œï¼‰
                primary_hash = current_hashes[self.hash_type]

                if regions_json is None:
                    cursor.execute(
                        f"""
                        SELECT image_hash, json_data, {self.hash_type}
                        FROM ocr_cache
                        WHERE {self.hash_type} IS NOT NULL
                        ORDER BY last_access_time DESC
                        LIMIT 100
                    """
                    )
                else:
                    cursor.execute(
                        f"""
                        SELECT image_hash, json_data, {self.hash_type}
                        FROM ocr_cache
                        WHERE {self.hash_type} IS NOT NULL AND regions = ?
                        ORDER BY last_access_time DESC
                        LIMIT 100
                    """,
                        (regions_json,),
                    )

                candidates = cursor.fetchall()
                best_match = None
                best_distance = 999

                for cached_image_hash, json_data, cached_hash in candidates:
                    if not cached_hash:
                        continue

                    distance = self._calculate_hamming_distance(primary_hash, cached_hash)
                    if distance < best_distance and distance <= self.hash_threshold:
                        best_distance = distance
                        best_match = (cached_image_hash, json_data, distance)

                if best_match:
                    cached_image_hash, json_data, distance = best_match
                    self.logger.debug(f"?? ç¼“å­˜å‘½ä¸­ï¼ˆå“ˆå¸Œç›¸ä¼¼ï¼Œè·ç¦»={distance}ï¼‰")

                    # æ›´æ–°è®¿é—®ä¿¡æ¯
                    cursor.execute(
                        "UPDATE ocr_cache SET hit_count = hit_count + 1, last_access_time = ? WHERE image_hash = ?",
                        (time.time(), cached_image_hash),
                    )
                    conn.commit()

                    if json_data:
                        return json.loads(json_data)

            return None
        except Exception as e:
            self.logger.error(f"æŸ¥æ‰¾ç¼“å­˜å¤±è´¥: {e}")
            return None

    def _evict_cache(self):
        """
        æ·˜æ±°æœ€ä¹…æœªè®¿é—®çš„ç¼“å­˜æ¡ç›®ï¼Œä¿æŒç¼“å­˜å¤§å°åœ¨é™åˆ¶å†…
        """
        try:
            with sqlite3.connect(self.cache_db_path) as conn:
                cursor = conn.cursor()

                # è·å–å½“å‰ç¼“å­˜æ¡ç›®æ•°
                cursor.execute("SELECT COUNT(*) FROM ocr_cache")
                count = cursor.fetchone()[0]

                if count > self.max_cache_size:
                    # è®¡ç®—éœ€è¦åˆ é™¤çš„æ¡ç›®æ•°
                    to_delete = count - self.max_cache_size + 10  # å¤šåˆ é™¤ä¸€äº›ï¼Œé¿å…é¢‘ç¹æ“ä½œ

                    # åˆ é™¤æœ€ä¹…æœªè®¿é—®çš„æ¡ç›®
                    cursor.execute(
                        """
                        DELETE FROM ocr_cache
                        WHERE image_hash IN (
                            SELECT image_hash FROM ocr_cache
                            ORDER BY last_access_time ASC
                            LIMIT ?
                        )
                    """,
                        (to_delete,),
                    )

                    conn.commit()
                    self.logger.debug(f"?? æ·˜æ±°äº† {to_delete} ä¸ªç¼“å­˜æ¡ç›®")
        except Exception as e:
            self.logger.error(f"æ·˜æ±°ç¼“å­˜å¤±è´¥: {e}")

    def _save_to_cache_db(
        self,
        image_path: Optional[str] = None,
        ocr_result: Optional[Dict[str, Any]] = None,
        regions: Optional[List[int]] = None,
        image: Optional[Any] = None,
    ):
        """
        ä¿å­˜ç¼“å­˜æ¡ç›®åˆ°æ•°æ®åº“

        Args:
            image_path: å›¾åƒè·¯å¾„
            ocr_result: OCR ç»“æœå­—å…¸
            regions: åŒºåŸŸåˆ—è¡¨
            image: OpenCV å›¾åƒå¯¹è±¡
        """
        try:
            if ocr_result is None:
                return

            image_bytes = self._get_image_bytes(image_path=image_path, image=image)
            if image_bytes is None:
                return

            # è®¡ç®—æ‰€æœ‰å“ˆå¸Œå€¼
            hashes = self._compute_all_hashes(image_path=image_path, image=image)

            # è®¡ç®—æ–‡ä»¶å“ˆå¸Œ
            image_hash = self._compute_image_md5(image_path=image_path, image=image)
            if not image_hash:
                return

            # è·å–æ–‡ä»¶å¤§å°
            image_size = len(image_bytes)

            # å‡†å¤‡åŒºåŸŸä¿¡æ¯
            regions_json = json.dumps(sorted(regions)) if regions else None

            json_data = json.dumps(ocr_result, ensure_ascii=False)

            with sqlite3.connect(self.cache_db_path) as conn:
                cursor = conn.cursor()

                # æ’å…¥æˆ–æ›´æ–°è®°å½•
                cursor.execute(
                    """
                    INSERT OR REPLACE INTO ocr_cache
                    (image_hash, phash, dhash, ahash, whash, regions,
                     hit_count, last_access_time, created_time, image_size, json_data)
                    VALUES (?, ?, ?, ?, ?, ?, 1, ?, ?, ?, ?)
                """,
                    (
                        image_hash,
                        hashes.get("phash"),
                        hashes.get("dhash"),
                        hashes.get("ahash"),
                        hashes.get("whash"),
                        regions_json,
                        time.time(),
                        time.time(),
                        image_size,
                        json_data,
                    ),
                )

                conn.commit()

            # æ£€æŸ¥æ˜¯å¦éœ€è¦æ·˜æ±°
            self._evict_cache()

        except Exception as e:
            self.logger.error(f"ä¿å­˜ç¼“å­˜åˆ°æ•°æ®åº“å¤±è´¥: {e}")

    def _merge_regions(self, regions: List[int]) -> Tuple[int, int, int, int]:
        """
        åˆå¹¶å¤šä¸ªåŒºåŸŸä¸ºä¸€ä¸ªè¿ç»­çš„çŸ©å½¢åŒºåŸŸ

        Args:
            regions: è¦åˆå¹¶çš„åŒºåŸŸåˆ—è¡¨ï¼ˆ1-9ï¼‰
                    1 2 3
                    4 5 6
                    7 8 9

        Returns:
            åˆå¹¶åçš„è¾¹ç•Œ (min_row, max_row, min_col, max_col)ï¼Œéƒ½æ˜¯0-basedç´¢å¼•
        """
        if not regions:
            return (0, 2, 0, 2)  # æ•´ä¸ªå›¾åƒ

        # å°†åŒºåŸŸIDè½¬æ¢ä¸ºè¡Œåˆ—ç´¢å¼•
        rows = []
        cols = []
        for region_id in regions:
            if not 1 <= region_id <= 9:
                self.logger.warning(f"æ— æ•ˆçš„åŒºåŸŸID: {region_id}ï¼Œè·³è¿‡")
                continue
            row = (region_id - 1) // 3
            col = (region_id - 1) % 3
            rows.append(row)
            cols.append(col)

        if not rows:
            return (0, 2, 0, 2)

        # è®¡ç®—åŒ…å«æ‰€æœ‰åŒºåŸŸçš„æœ€å°çŸ©å½¢
        min_row = min(rows)
        max_row = max(rows)
        min_col = min(cols)
        max_col = max(cols)

        return (min_row, max_row, min_col, max_col)

    def _get_region_bounds(
        self, image_shape: Tuple[int, int], regions: Optional[List[int]] = None
    ) -> Tuple[int, int, int, int]:
        """
        å°†å›¾åƒåˆ†æˆ3x3ç½‘æ ¼ï¼Œè¿”å›åˆå¹¶åçš„åŒºåŸŸè¾¹ç•Œ

        Args:
            image_shape: å›¾åƒå½¢çŠ¶ (height, width)
            regions: è¦æå–çš„åŒºåŸŸåˆ—è¡¨ï¼Œä½¿ç”¨æ•°å­—1-9è¡¨ç¤ºï¼ˆä»å·¦åˆ°å³ï¼Œä»ä¸Šåˆ°ä¸‹ï¼‰
                    1 2 3
                    4 5 6
                    7 8 9
                    å¦‚æœä¸ºNoneï¼Œè¿”å›æ•´ä¸ªå›¾åƒ
                    å¤šä¸ªåŒºåŸŸä¼šè¢«åˆå¹¶æˆä¸€ä¸ªè¿ç»­çš„çŸ©å½¢

        Returns:
            åŒºåŸŸè¾¹ç•Œ (x, y, w, h)
        """
        height, width = image_shape

        if regions is None:
            # è¿”å›æ•´ä¸ªå›¾åƒ
            return (0, 0, width, height)

        # åˆå¹¶åŒºåŸŸ
        min_row, max_row, min_col, max_col = self._merge_regions(regions)

        # è®¡ç®—æ¯ä¸ªæ ¼å­çš„å¤§å°
        cell_height = height // 3
        cell_width = width // 3

        # è®¡ç®—åˆå¹¶åçš„è¾¹ç•Œ
        x = min_col * cell_width
        y = min_row * cell_height
        w = (max_col - min_col + 1) * cell_width
        h = (max_row - min_row + 1) * cell_height

        # å¤„ç†è¾¹ç•Œæƒ…å†µï¼Œç¡®ä¿è¦†ç›–åˆ°å›¾åƒè¾¹ç¼˜
        if max_col == 2:  # åŒ…å«æœ€å³åˆ—
            w = width - x
        if max_row == 2:  # åŒ…å«æœ€ä¸‹è¡Œ
            h = height - y

        return (x, y, w, h)

    def _extract_region(
        self,
        image: Any,
        regions: Optional[List[int]] = None,
        debug_save_path: Optional[str] = None,
    ) -> Tuple[Any, Tuple[int, int]]:
        """
        ä»å›¾åƒä¸­æå–æŒ‡å®šçš„åŒºåŸŸï¼ˆåˆå¹¶åçš„å•ä¸ªåŒºåŸŸï¼‰

        Args:
            image: OpenCVå›¾åƒå¯¹è±¡
            regions: è¦æå–çš„åŒºåŸŸåˆ—è¡¨ï¼ˆ1-9ï¼‰ï¼Œä¼šè¢«åˆå¹¶æˆä¸€ä¸ªè¿ç»­çš„çŸ©å½¢
            debug_save_path: è°ƒè¯•ç”¨ï¼Œä¿å­˜åŒºåŸŸæˆªå›¾çš„è·¯å¾„

        Returns:
            (region_image, (offset_x, offset_y))
        """
        if image is None:
            return None, (0, 0)

        height, width = image.shape[:2]
        x, y, w, h = self._get_region_bounds((height, width), regions)

        region_img = image[y : y + h, x : x + w]

        # è°ƒè¯•ï¼šä¿å­˜åŒºåŸŸæˆªå›¾
        if debug_save_path:
            cv2.imwrite(debug_save_path, region_img)
            self.logger.debug(f"ğŸ” è°ƒè¯•ï¼šåŒºåŸŸæˆªå›¾å·²ä¿å­˜åˆ° {debug_save_path}")
            self.logger.debug(f"   åŒºåŸŸèŒƒå›´: x={x}, y={y}, w={w}, h={h}")
            self.logger.debug(f"   åŸå›¾å°ºå¯¸: {width}x{height}")

        return region_img, (x, y)

    def _get_region_description(self, regions: Optional[List[int]]) -> str:
        """
        è·å–åŒºåŸŸçš„æè¿°æ–‡å­—

        Args:
            regions: åŒºåŸŸåˆ—è¡¨

        Returns:
            åŒºåŸŸæè¿°ï¼Œå¦‚ "åŒºåŸŸ[1,2,3]ï¼ˆä¸Šéƒ¨ï¼‰"
        """
        if not regions:
            return "å…¨å±"

        # åˆå¹¶åŒºåŸŸ
        min_row, max_row, min_col, max_col = self._merge_regions(regions)

        # ç”Ÿæˆæè¿°
        parts = []

        # è¡Œæè¿°
        if min_row == max_row:
            row_names = ["ä¸Šéƒ¨", "ä¸­éƒ¨", "ä¸‹éƒ¨"]
            parts.append(row_names[min_row])
        elif min_row == 0 and max_row == 2:
            parts.append("å…¨é«˜")
        else:
            parts.append(f"ç¬¬{min_row + 1}-{max_row + 1}è¡Œ")

        # åˆ—æè¿°
        if min_col == max_col:
            col_names = ["å·¦ä¾§", "ä¸­é—´", "å³ä¾§"]
            parts.append(col_names[min_col])
        elif min_col == 0 and max_col == 2:
            parts.append("å…¨å®½")
        else:
            parts.append(f"ç¬¬{min_col + 1}-{max_col + 1}åˆ—")

        region_str = ",".join(map(str, sorted(regions)))
        return f"åŒºåŸŸ[{region_str}]ï¼ˆ{' '.join(parts)}ï¼‰"

    def _empty_result(self) -> Dict[str, Any]:
        """è¿”å›ç©ºçš„æŸ¥æ‰¾ç»“æœ"""
        return {
            "found": False,
            "center": None,
            "text": None,
            "confidence": None,
            "bbox": None,
            "total_matches": 0,
            "selected_index": 0,
        }

    def _adjust_coordinates_to_full_image(
        self, bbox: List[List[int]], offset: Tuple[int, int]
    ) -> List[List[int]]:
        """
        å°†åŒºåŸŸå†…çš„åæ ‡è°ƒæ•´ä¸ºåŸå›¾ä¸­çš„åæ ‡

        Args:
            bbox: åŒºåŸŸå†…çš„è¾¹ç•Œæ¡†åæ ‡ [[x1,y1], [x2,y2], [x3,y3], [x4,y4]]
            offset: åŒºåŸŸåœ¨åŸå›¾ä¸­çš„åç§»é‡ (offset_x, offset_y)

        Returns:
            è°ƒæ•´åçš„è¾¹ç•Œæ¡†åæ ‡
        """
        offset_x, offset_y = offset
        adjusted_bbox = []
        for point in bbox:
            adjusted_bbox.append([point[0] + offset_x, point[1] + offset_y])
        return adjusted_bbox

    def _load_existing_cache(self):
        """
        åŠ è½½ç¼“å­˜ç›®å½•ä¸­å·²æœ‰çš„ç¼“å­˜æ–‡ä»¶
        """
        try:
            if not os.path.exists(self.cache_dir):
                return

            # æŸ¥æ‰¾æ‰€æœ‰ç¼“å­˜æ–‡ä»¶å¯¹
            cache_files = os.listdir(self.cache_dir)
            cache_pairs = {}

            # å°†å›¾ç‰‡å’Œ JSON æ–‡ä»¶é…å¯¹
            for filename in cache_files:
                if filename.startswith("cache_") and filename.endswith(".png"):
                    # æå–ç¼“å­˜ ID
                    cache_id = filename.replace("cache_", "").replace(".png", "")
                    json_filename = f"cache_{cache_id}_res.json"

                    image_path = os.path.join(self.cache_dir, filename)
                    json_path = os.path.join(self.cache_dir, json_filename)

                    # æ£€æŸ¥å¯¹åº”çš„ JSON æ–‡ä»¶æ˜¯å¦å­˜åœ¨
                    if os.path.exists(json_path):
                        cache_pairs[cache_id] = (image_path, json_path)

            # æŒ‰ ID æ’åºå¹¶åŠ è½½åˆ°ç¼“å­˜åˆ—è¡¨
            for cache_id in sorted(cache_pairs.keys(), key=lambda x: int(x) if x.isdigit() else 0):
                self.ocr_cache.append(cache_pairs[cache_id])

            if self.ocr_cache:
                self.logger.debug(f"ğŸ’¾ åŠ è½½äº† {len(self.ocr_cache)} ä¸ªç¼“å­˜æ–‡ä»¶")
        except Exception as e:
            self.logger.error(f"åŠ è½½ç¼“å­˜å¤±è´¥: {e}")

    def _find_similar_cached_image(self, current_image_path, regions: Optional[List[int]] = None):
        """
        æŸ¥æ‰¾ç¼“å­˜ä¸­æ˜¯å¦æœ‰ç›¸ä¼¼çš„å›¾ç‰‡ï¼ˆä½¿ç”¨æ–°çš„å“ˆå¸Œç´¢å¼•ç³»ç»Ÿï¼‰

        Args:
            current_image_path (str): å½“å‰å›¾ç‰‡è·¯å¾„
            regions (List[int], optional): åŒºåŸŸåˆ—è¡¨

        Returns:
            dict: ç¼“å­˜çš„ OCR ç»“æœï¼Œå¦‚æœæ²¡æœ‰æ‰¾åˆ°åˆ™è¿”å› None
        """
        try:
            return self._find_similar_in_cache(image_path=current_image_path, regions=regions)
        except Exception as e:
            self.logger.error(f"æŸ¥æ‰¾ç›¸ä¼¼ç¼“å­˜å›¾ç‰‡å¤±è´¥: {e}")
            return None

    def _save_to_cache(
        self,
        image_path: str,
        ocr_result: Dict[str, Any],
        regions: Optional[List[int]] = None,
    ):
        """
        ä¿å­˜ OCR ç»“æœåˆ°ç¼“å­˜ï¼ˆä»…å†™å…¥ SQLiteï¼‰

        Args:
            image_path (str): å›¾ç‰‡è·¯å¾„
            ocr_result (dict): OCR ç»“æœ
            regions (List[int], optional): åŒºåŸŸåˆ—è¡¨
        """
        try:
            if not ocr_result:
                return
            self._save_to_cache_db(image_path=image_path, ocr_result=ocr_result, regions=regions)
        except Exception as e:
            self.logger.error(f"ä¿å­˜ç¼“å­˜å¤±è´¥: {e}")

    def _resize_image_for_ocr(self, image_path):
        """

        è°ƒæ•´å›¾ç‰‡å¤§å°ä»¥åŠ é€Ÿ OCR è¯†åˆ«



        Args:

            image_path (str): åŸå§‹å›¾ç‰‡è·¯å¾„



        Returns:

            Tuple[str, float]: (è°ƒæ•´åçš„å›¾ç‰‡è·¯å¾„, ç¼©æ”¾æ¯”ä¾‹)

        """

        if not self.resize_image:
            return image_path, 1.0

        try:
            img = cv2.imread(image_path)

            if img is None:
                return image_path, 1.0

            height, width = img.shape[:2]

            # å¦‚æœå›¾ç‰‡å®½åº¦å¤§äºæœ€å¤§å®½åº¦ï¼Œè¿›è¡Œç¼©æ”¾

            if width > self.max_width:
                scale = self.max_width / width

                new_width = self.max_width

                new_height = int(height * scale)

                # ç¼©å°å›¾ç‰‡

                resized_img = cv2.resize(img, (new_width, new_height), interpolation=cv2.INTER_AREA)

                # ä¿å­˜åˆ°ä¸´æ—¶æ–‡ä»¶

                temp_path = image_path.replace(".png", "_resized.png")

                cv2.imwrite(temp_path, resized_img)

                self.logger.debug(
                    f"ğŸ”§ å›¾ç‰‡å·²ç¼©å°: {width}x{height} -> {new_width}x{new_height} (scale={scale:.2f})"
                )

                return temp_path, scale

            return image_path, 1.0

        except Exception as e:
            self.logger.warning(f"å›¾ç‰‡ç¼©æ”¾å¤±è´¥: {e}ï¼Œä½¿ç”¨åŸå›¾")

            return image_path, 1.0

    def _predict_with_timing(self, image_path):
        """
        æ‰§è¡Œ OCR è¯†åˆ«å¹¶è®°å½•è€—æ—¶ (Remote PaddleX 3.0)

        Args:
            image_path (str): å›¾åƒæ–‡ä»¶è·¯å¾„

        Returns:
            OCR è¯†åˆ«ç»“æœ (dict: {rec_texts: [], rec_scores: [], dt_polys: []})
        """
        # é¢„å¤„ç†ï¼šç¼©å°å›¾ç‰‡
        processed_image_path, scale = self._resize_image_for_ocr(image_path)

        start_time = time.time()
        result = None

        try:
            # 1. è½¬æ¢ä¸º Base64
            with open(processed_image_path, "rb") as f:
                image_data = base64.b64encode(f.read()).decode("utf-8")

            # 2. æ„å»ºç¬¦åˆ PaddleX 3.0 è§„èŒƒçš„ Payload
            payload = {
                "file": image_data,
                "fileType": 1,
                "useDocOrientationClassify": False,
                "useDocUnwarping": False,
                "useTextlineOrientation": False,
            }

            # 3. å‘é€è¯·æ±‚ (é»˜è®¤ç«¯å£ 8080)
            response = requests.post(self.ocr_url, json=payload, timeout=60)
            response.raise_for_status()
            json_resp = response.json()

            if json_resp.get("errorCode") == 0:
                # PaddleX 3.0 çš„ç»“æœåµŒå¥—åœ¨ result.ocrResults[0].prunedResult ä¸­
                ocr_results = json_resp.get("result", {}).get("ocrResults", [])
                if ocr_results:
                    pruned = ocr_results[0].get("prunedResult", {})

                    dt_polys = pruned.get("dt_polys", [])

                    # å¦‚æœè¿›è¡Œäº†ç¼©æ”¾ï¼Œéœ€è¦è¿˜åŸåæ ‡
                    if scale != 1.0 and dt_polys:
                        restored_polys = []
                        for poly in dt_polys:
                            # poly æ˜¯ [[x1, y1], [x2, y2], ...]
                            restored_poly = []
                            for point in poly:
                                restored_poly.append([int(point[0] / scale), int(point[1] / scale)])
                            restored_polys.append(restored_poly)
                        dt_polys = restored_polys

                    rec_texts = pruned.get("rec_texts", [])
                    if self.correction_map:
                        corrected_texts = []
                        for t in rec_texts:
                            corrected_texts.append(self.correction_map.get(t, t))
                        rec_texts = corrected_texts

                    # è½¬æ¢æ ¼å¼ä¸º OCRHelper æ‰€éœ€çš„æ ¼å¼
                    result = {
                        "rec_texts": rec_texts,
                        "rec_scores": pruned.get("rec_scores", []),
                        "dt_polys": dt_polys,
                    }

                else:
                    self.logger.warning("OCR Server returned empty ocrResults")
            else:
                self.logger.error(f"OCR Server Error: {json_resp.get('errorMsg')}")

        except Exception as e:
            self.logger.error(f"OCR Request Failed: {e}")

        elapsed_time = time.time() - start_time

        filename = os.path.basename(image_path)
        self.logger.debug(f"â±ï¸ OCRè¯†åˆ«è€—æ—¶: {elapsed_time:.3f}ç§’ (æ–‡ä»¶: {filename})")

        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        if processed_image_path != image_path and os.path.exists(processed_image_path):
            try:
                os.remove(processed_image_path)
            except Exception:
                pass

        return result

    def _get_or_create_ocr_result(
        self, image_path, use_cache=True, regions: Optional[List[int]] = None
    ):
        """
        è·å–æˆ–åˆ›å»º OCR è¯†åˆ«ç»“æœï¼ˆå¸¦ç¼“å­˜ï¼‰

        Args:
            image_path (str): å›¾åƒæ–‡ä»¶è·¯å¾„
            use_cache (bool): æ˜¯å¦ä½¿ç”¨ç¼“å­˜ï¼Œé»˜è®¤ä¸º True
            regions (List[int], optional): åŒºåŸŸåˆ—è¡¨

        Returns:
            dict: OCR ç»“æœ
        """
        # å¦‚æœå¯ç”¨ç¼“å­˜ï¼Œæ£€æŸ¥ç¼“å­˜ä¸­æ˜¯å¦æœ‰ç›¸ä¼¼å›¾ç‰‡
        if use_cache:
            cached_result = self._find_similar_cached_image(image_path, regions)
            if cached_result:
                return cached_result

        # ç¼“å­˜æœªå‘½ä¸­æˆ–ç¦ç”¨ç¼“å­˜ï¼Œæ‰§è¡Œ OCR è¯†åˆ«
        result = self._predict_with_timing(image_path)

        if result:
            # å¦‚æœå¯ç”¨ç¼“å­˜ï¼ŒåŒæ—¶ä¿å­˜åˆ°ç¼“å­˜
            if use_cache:
                self._save_to_cache(image_path, result, regions)

            return result

        return None

    def find_text_in_image(
        self,
        image_path,
        target_text,
        confidence_threshold=0.5,
        occurrence=1,
        use_cache=True,
        regions: Optional[List[int]] = None,
        debug_save_path: Optional[str] = None,
        return_all=False,
    ):
        """
        åœ¨æŒ‡å®šå›¾åƒä¸­æŸ¥æ‰¾ç›®æ ‡æ–‡å­—çš„ä½ç½®

        Args:
            image_path (str): å›¾åƒæ–‡ä»¶è·¯å¾„
            target_text (str): è¦æŸ¥æ‰¾çš„ç›®æ ‡æ–‡å­—
            confidence_threshold (float): ç½®ä¿¡åº¦é˜ˆå€¼ (0-1)
            occurrence (int): æŒ‡å®šç‚¹å‡»ç¬¬å‡ ä¸ªå‡ºç°çš„æ–‡å­— (1-based)ï¼Œé»˜è®¤ä¸º1
            use_cache (bool): æ˜¯å¦ä½¿ç”¨ç¼“å­˜ï¼Œé»˜è®¤ä¸º True
            regions (List[int], optional): è¦æœç´¢çš„åŒºåŸŸåˆ—è¡¨ï¼ˆ1-9ï¼‰
            debug_save_path (str, optional): è°ƒè¯•ç”¨
            return_all (bool): æ˜¯å¦è¿”å›æ‰€æœ‰åŒ¹é…é¡¹çš„åˆ—è¡¨

        Returns:
            dict | list: å¦‚æœ return_all=False, è¿”å›æŸ¥æ‰¾ç»“æœå­—å…¸;
                         å¦‚æœ return_all=True, è¿”å›åŒ…å«æ‰€æœ‰åŒ¹é…å­—å…¸çš„åˆ—è¡¨
        """
        try:
            # å¦‚æœæŒ‡å®šäº†åŒºåŸŸï¼Œä½¿ç”¨åŒºåŸŸæœç´¢
            if regions is not None:
                return self._find_text_in_regions(
                    image_path,
                    target_text,
                    confidence_threshold,
                    occurrence,
                    regions,
                    debug_save_path,
                    use_cache,
                    return_all=return_all,
                )

            # è·å–æˆ–åˆ›å»º OCR ç»“æœ
            ocr_data = self._get_or_create_ocr_result(
                image_path, use_cache=use_cache, regions=regions
            )

            if not ocr_data:
                return [] if return_all else self._empty_result()

            # ä» OCR ç»“æœä¸­æŸ¥æ‰¾ç›®æ ‡æ–‡å­—
            return self._find_text_in_json(
                ocr_data, target_text, confidence_threshold, occurrence, return_all=return_all
            )

        except Exception as e:
            self.logger.error(f"å›¾åƒOCRè¯†åˆ«å‡ºé”™: {e}")
            return [] if return_all else self._empty_result()

    def capture_and_find_all_texts(
        self,
        target_text,
        confidence_threshold=0.5,
        use_cache=True,
        regions: Optional[List[int]] = None,
    ):
        """
        æˆªå›¾å¹¶æŸ¥æ‰¾æ‰€æœ‰åŒ¹é…çš„ç›®æ ‡æ–‡å­—

        Args:
            target_text (str): è¦æŸ¥æ‰¾çš„ç›®æ ‡æ–‡å­—
            confidence_threshold (float): ç½®ä¿¡åº¦é˜ˆå€¼ (0-1)
            use_cache (bool): æ˜¯å¦ä½¿ç”¨ç¼“å­˜ï¼Œé»˜è®¤ä¸º True
            regions (List[int], optional): è¦æœç´¢çš„åŒºåŸŸåˆ—è¡¨

        Returns:
            list: åŒ…å«æ‰€æœ‰åŒ¹é…é¡¹ä¿¡æ¯çš„åˆ—è¡¨
        """
        if not self.snapshot_func:
            self.logger.error("snapshot_func not set")
            return []

        # å†…éƒ¨å¤ç”¨ capture_and_find_text çš„éƒ¨åˆ†é€»è¾‘ï¼ˆæˆªå›¾ä¸é‡è¯•ï¼‰
        # ä½†ä¼ é€’ return_all=True

        # ä¸ºäº†ç®€æ´ï¼Œè¿™é‡Œç›´æ¥è°ƒç”¨ find_text_in_image é€»è¾‘
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        unique_id = str(uuid.uuid4())[:8]
        screenshot_path = os.path.join(self.temp_dir, f"all_texts_{timestamp}_{unique_id}.png")

        try:
            self.snapshot_func(filename=screenshot_path)
            results = self.find_text_in_image(
                screenshot_path,
                target_text,
                confidence_threshold,
                use_cache=use_cache,
                regions=regions,
                return_all=True,
            )
            return results
        finally:
            if self.delete_temp_screenshots and os.path.exists(screenshot_path):
                try:
                    os.remove(screenshot_path)
                except Exception:
                    pass

    def _find_text_in_regions(
        self,
        image_path: str,
        target_text: str,
        confidence_threshold: float,
        occurrence: int,
        regions: List[int],
        debug_save_path: Optional[str] = None,
        use_cache: bool = True,
        return_all: bool = False,
    ) -> Union[Dict[str, Any], List[Dict[str, Any]]]:
        """
        åœ¨æŒ‡å®šåŒºåŸŸä¸­æŸ¥æ‰¾æ–‡å­—ï¼ˆå†…éƒ¨æ–¹æ³•ï¼‰
        """
        try:
            # è¯»å–å›¾åƒ
            image = cv2.imread(image_path)
            if image is None:
                self.logger.error(f"æ— æ³•è¯»å–å›¾åƒ: {image_path}")
                return [] if return_all else self._empty_result()

            # æå–åˆå¹¶åçš„åŒºåŸŸ
            region_img, offset = self._extract_region(image, regions, debug_save_path)
            if region_img is None:
                self.logger.warning("æœªèƒ½æå–åŒºåŸŸ")
                return [] if return_all else self._empty_result()

            # æ˜¾ç¤ºåŒºåŸŸä¿¡æ¯
            region_desc = self._get_region_description(regions)
            self.logger.debug(f"ğŸ” åœ¨{region_desc}æœç´¢æ–‡å­—: '{target_text}'")

            # åˆå§‹åŒ–ç»“æœ
            result = None
            cache_used = False
            elapsed_time = 0

            # åªæœ‰åœ¨ä½¿ç”¨ç¼“å­˜æ—¶æ‰å°è¯•ä»ç¼“å­˜è¯»å–
            if use_cache:
                cached_result = self._find_similar_in_cache(image=region_img, regions=regions)
                if cached_result:
                    self.logger.debug(f"?? åŒºåŸŸç¼“å­˜å‘½ä¸­: {region_desc}")
                    result = [cached_result]
                    cache_used = True

            # å¦‚æœæ²¡æœ‰å‘½ä¸­ç¼“å­˜æˆ–ä¸ä½¿ç”¨ç¼“å­˜ï¼Œè¿›è¡ŒOCRè¯†åˆ«
            if result is None:
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                unique_id = str(uuid.uuid4())[:8]
                temp_region_path = os.path.join(
                    self.temp_dir, f"region_{timestamp}_{unique_id}.png"
                )
                cv2.imwrite(temp_region_path, region_img)

                # å¯¹åŒºåŸŸè¿›è¡ŒOCRè¯†åˆ« (Remote)
                ocr_dict = self._predict_with_timing(temp_region_path)

                if temp_region_path and os.path.exists(temp_region_path):
                    try:
                        os.remove(temp_region_path)
                    except Exception:
                        pass

                if ocr_dict:
                    result = [ocr_dict]  # åŒ…è£…æˆåˆ—è¡¨ä»¥å…¼å®¹åç»­é€»è¾‘

                    # ä¿å­˜OCRç»“æœåˆ°ç¼“å­˜ï¼ˆä»…åœ¨ä½¿ç”¨ç¼“å­˜æ—¶ï¼‰
                    if use_cache:
                        self._save_to_cache_db(
                            image=region_img, ocr_result=ocr_dict, regions=regions
                        )
                else:
                    result = []

            if not result or len(result) == 0:
                return [] if return_all else self._empty_result()

            # æ”¶é›†æ‰€æœ‰åŒ¹é…ç»“æœ
            all_matches = []
            for res in result:
                res_cache_used = cache_used
                res_elapsed_time = elapsed_time

                # å®‰å…¨åœ°è·å–å­—æ®µï¼Œå…¼å®¹ dict å’Œ object
                if isinstance(res, dict):
                    rec_texts = res.get("rec_texts", [])
                    rec_scores = res.get("rec_scores", [])
                    dt_polys = res.get("dt_polys", [])
                else:
                    rec_texts = getattr(res, "rec_texts", [])
                    rec_scores = getattr(res, "rec_scores", [])
                    dt_polys = getattr(res, "dt_polys", [])

                # æŸ¥æ‰¾åŒ¹é…çš„æ–‡å­—
                for i, (text, score) in enumerate(zip(rec_texts, rec_scores)):
                    if score >= confidence_threshold and target_text in text:
                        if i < len(dt_polys):
                            poly = dt_polys[i]

                            # è°ƒæ•´åæ ‡åˆ°åŸå›¾
                            adjusted_poly = self._adjust_coordinates_to_full_image(poly, offset)

                            # è®¡ç®—ä¸­å¿ƒç‚¹
                            x_coords = [point[0] for point in adjusted_poly]
                            y_coords = [point[1] for point in adjusted_poly]
                            center_x = int(sum(x_coords) / len(x_coords))
                            center_y = int(sum(y_coords) / len(y_coords))

                            all_matches.append(
                                {
                                    "center": (center_x, center_y),
                                    "text": text,
                                    "confidence": score,
                                    "bbox": adjusted_poly,
                                    "index": len(all_matches) + 1,
                                    "cache_used": res_cache_used,
                                    "elapsed_time": res_elapsed_time,
                                }
                            )

            if return_all:
                return all_matches

            # å¤„ç†åŒ¹é…ç»“æœ
            total_matches = len(all_matches)
            if total_matches == 0:
                return self._empty_result()

            # é€‰æ‹©æŒ‡å®šçš„åŒ¹é…é¡¹
            if occurrence > total_matches:
                selected_match = all_matches[-1]
                selected_index = total_matches
            else:
                selected_match = all_matches[occurrence - 1]
                selected_index = occurrence

            return {
                "found": True,
                "center": selected_match["center"],
                "text": selected_match["text"],
                "confidence": selected_match["confidence"],
                "bbox": selected_match["bbox"],
                "total_matches": total_matches,
                "selected_index": selected_index,
            }

        except Exception as e:
            self.logger.error(f"åŒºåŸŸæœç´¢å‡ºé”™: {e}")
            return [] if return_all else self._empty_result()

    def _find_text_in_json(
        self, json_file_path, target_text, confidence_threshold=0.5, occurrence=1, return_all=False
    ):
        """
        ä»OCRç»“æœä¸­æŸ¥æ‰¾ç›®æ ‡æ–‡å­—
        """
        try:
            # è¯»å–JSONæ–‡ä»¶æˆ–ç›´æ¥ä½¿ç”¨ç»“æœå­—å…¸
            if isinstance(json_file_path, dict):
                data = json_file_path
            else:
                with open(json_file_path, "r", encoding="utf-8") as f:
                    data = json.load(f)

            # è·å–è¯†åˆ«çš„æ–‡å­—åˆ—è¡¨å’Œå¯¹åº”çš„åæ ‡æ¡†
            rec_texts = data.get("rec_texts", [])
            rec_scores = data.get("rec_scores", [])
            dt_polys = data.get("dt_polys", [])  # æ£€æµ‹æ¡†åæ ‡

            # æ”¶é›†æ‰€æœ‰åŒ¹é…çš„æ–‡å­—
            matches = []
            for i, (text, score) in enumerate(zip(rec_texts, rec_scores)):
                # æ£€æŸ¥ç½®ä¿¡åº¦å’Œæ–‡å­—åŒ¹é…ï¼ˆè¯†åˆ«å‡ºçš„æ–‡å­—åŒ…å«ç›®æ ‡æ–‡å­—ï¼‰
                if score >= confidence_threshold and target_text in text:
                    # è·å–å¯¹åº”çš„åæ ‡æ¡†
                    if i < len(dt_polys):
                        poly = dt_polys[i]

                        # è®¡ç®—ä¸­å¿ƒç‚¹åæ ‡
                        x_coords = [point[0] for point in poly]
                        y_coords = [point[1] for point in poly]
                        center_x = int(sum(x_coords) / len(x_coords))
                        center_y = int(sum(y_coords) / len(y_coords))

                        matches.append(
                            {
                                "center": (center_x, center_y),
                                "text": text,
                                "confidence": score,
                                "bbox": poly,
                                "index": len(matches) + 1,
                            }
                        )

            if return_all:
                return matches

            total_matches = len(matches)
            if total_matches == 0:
                return self._empty_result()

            # é€‰æ‹©æŒ‡å®šçš„åŒ¹é…é¡¹
            if occurrence > total_matches:
                selected_match = matches[-1]
                selected_index = total_matches
            else:
                selected_match = matches[occurrence - 1]
                selected_index = occurrence

            return {
                "found": True,
                "center": selected_match["center"],
                "text": selected_match["text"],
                "confidence": selected_match["confidence"],
                "bbox": selected_match["bbox"],
                "total_matches": total_matches,
                "selected_index": selected_index,
            }

        except Exception as e:
            self.logger.error(f"å¤„ç†OCRæ•°æ®æ—¶å‡ºé”™: {e}")
            return [] if return_all else self._empty_result()

    def capture_and_get_all_texts(
        self,
        use_cache=True,
        regions: Optional[List[int]] = None,
    ):
        """
        æˆªå›¾å¹¶è·å–æ‰€æœ‰è¯†åˆ«åˆ°çš„æ–‡å­—ä¿¡æ¯

        Args:
            use_cache (bool): æ˜¯å¦ä½¿ç”¨ç¼“å­˜
            regions (List[int], optional): è¦è·å–çš„åŒºåŸŸåˆ—è¡¨

        Returns:
            list: åŒ…å«æ‰€æœ‰æ–‡å­—ä¿¡æ¯çš„åˆ—è¡¨
        """
        if not self.snapshot_func:
            self.logger.error("snapshot_func not set")
            return []

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        unique_id = str(uuid.uuid4())[:8]
        screenshot_path = os.path.join(self.temp_dir, f"get_all_{timestamp}_{unique_id}.png")

        try:
            self.snapshot_func(filename=screenshot_path)
            # ä½¿ç”¨ç°æœ‰é€»è¾‘è·å–æ‰€æœ‰æ–‡æœ¬
            if regions:
                # å€Ÿç”¨ _find_text_in_regions é€»è¾‘ï¼Œä¼ é€’ç©ºå­—ç¬¦ä¸²åŒ¹é…æ‰€æœ‰
                return self._find_text_in_regions(
                    screenshot_path,
                    target_text="",
                    confidence_threshold=0.0,
                    occurrence=1,
                    regions=regions,
                    use_cache=use_cache,
                    return_all=True,
                )
            else:
                # è·å–å…¨å± OCR ç»“æœ
                ocr_data = self._get_or_create_ocr_result(
                    screenshot_path, use_cache=use_cache, regions=None
                )
                if not ocr_data:
                    return []
                # ä»æ•°æ®ä¸­æå–æ‰€æœ‰é¡¹
                return self._find_text_in_json(
                    ocr_data, target_text="", confidence_threshold=0.0, return_all=True
                )
        finally:
            if self.delete_temp_screenshots and os.path.exists(screenshot_path):
                try:
                    os.remove(screenshot_path)
                except Exception:
                    pass

    def find_all_matching_texts(self, image_path, target_text, confidence_threshold=0.5):
        """
        æŸ¥æ‰¾å›¾åƒä¸­æ‰€æœ‰åŒ¹é…çš„æ–‡å­—

        Args:
            image_path (str): å›¾åƒæ–‡ä»¶è·¯å¾„
            target_text (str): è¦æŸ¥æ‰¾çš„ç›®æ ‡æ–‡å­—
            confidence_threshold (float): ç½®ä¿¡åº¦é˜ˆå€¼ (0-1)

        Returns:
            list: åŒ…å«æ‰€æœ‰åŒ¹é…æ–‡å­—ä¿¡æ¯çš„åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å«center, text, confidence, bbox
        """
        try:
            # OCR è¯†åˆ«
            result = self._predict_with_timing(image_path)

            if not result:
                self.logger.warning(f"OCRè¯†åˆ«ç»“æœä¸ºç©º: {image_path}")
                return []

            # ä¿å­˜è¯†åˆ«ç»“æœåˆ°JSON (å¯é€‰ï¼Œä¿æŒå…¼å®¹æ€§)
            json_filename = os.path.basename(image_path).replace(".png", "_res.json")
            json_file = os.path.join(self.output_dir, json_filename)

            with open(json_file, "w", encoding="utf-8") as f:
                json.dump(result, f, ensure_ascii=False, indent=2)

            # ç›´æ¥ä»å†…å­˜ç»“æœå¤„ç†ï¼Œæˆ–è€…è¯»å–åˆšæ‰ä¿å­˜çš„æ–‡ä»¶
            # ä¸ºäº†å¤ç”¨é€»è¾‘ï¼Œè¿™é‡Œå¤ç”¨ _find_all_matching_texts_in_json
            return self._find_all_matching_texts_in_json(
                json_file, target_text, confidence_threshold
            )

        except Exception as e:
            self.logger.error(f"æŸ¥æ‰¾æ‰€æœ‰åŒ¹é…æ–‡å­—æ—¶å‡ºé”™: {e}")
            return []

    def _find_all_matching_texts_in_json(
        self, json_file_path, target_text, confidence_threshold=0.5
    ):
        """
        ä»JSONæ–‡ä»¶ä¸­æŸ¥æ‰¾æ‰€æœ‰åŒ¹é…çš„æ–‡å­—

        Args:
            json_file_path (str): JSONæ–‡ä»¶è·¯å¾„
            target_text (str): è¦æŸ¥æ‰¾çš„ç›®æ ‡æ–‡å­—
            confidence_threshold (float): ç½®ä¿¡åº¦é˜ˆå€¼ (0-1)

        Returns:
            list: æ‰€æœ‰åŒ¹é…çš„æ–‡å­—ä¿¡æ¯åˆ—è¡¨
        """
        try:
            with open(json_file_path, "r", encoding="utf-8") as f:
                data = json.load(f)

            rec_texts = data.get("rec_texts", [])
            rec_scores = data.get("rec_scores", [])
            dt_polys = data.get("dt_polys", [])

            matches = []
            for i, (text, score) in enumerate(zip(rec_texts, rec_scores)):
                # æ£€æŸ¥ç½®ä¿¡åº¦å’Œæ–‡å­—åŒ¹é…ï¼ˆè¯†åˆ«å‡ºçš„æ–‡å­—åŒ…å«ç›®æ ‡æ–‡å­—ï¼‰
                if score >= confidence_threshold and target_text in text:
                    if i < len(dt_polys):
                        poly = dt_polys[i]
                        x_coords = [point[0] for point in poly]
                        y_coords = [point[1] for point in poly]
                        center_x = int(sum(x_coords) / len(x_coords))
                        center_y = int(sum(y_coords) / len(y_coords))

                        matches.append(
                            {
                                "center": (center_x, center_y),
                                "text": text,
                                "confidence": score,
                                "bbox": poly,
                                "index": len(matches) + 1,
                            }
                        )

            return matches

        except Exception as e:
            self.logger.error(f"å¤„ç†JSONæ–‡ä»¶æ—¶å‡ºé”™: {e}")
            return []

    def get_all_texts_from_image(self, image_path):
        """
        è·å–å›¾åƒä¸­æ‰€æœ‰è¯†åˆ«åˆ°çš„æ–‡å­—ä¿¡æ¯

        Args:
            image_path (str): å›¾åƒæ–‡ä»¶è·¯å¾„

        Returns:
            list: åŒ…å«æ‰€æœ‰æ–‡å­—ä¿¡æ¯çš„åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ ä¸ºå­—å…¸åŒ…å«text, confidence, center, bbox
        """
        try:
            # OCR è¯†åˆ«
            result = self._predict_with_timing(image_path)

            if not result:
                self.logger.warning(f"OCRè¯†åˆ«ç»“æœä¸ºç©º: {image_path}")
                return []

            # ä¿å­˜è¯†åˆ«ç»“æœåˆ°JSON
            json_filename = os.path.basename(image_path).replace(".png", "_res.json")
            json_file = os.path.join(self.output_dir, json_filename)

            with open(json_file, "w", encoding="utf-8") as f:
                json.dump(result, f, ensure_ascii=False, indent=2)

            # ä»JSONæ–‡ä»¶è¯»å–æ‰€æœ‰æ–‡å­—
            return self._get_all_texts_from_json(json_file)

        except Exception as e:
            self.logger.error(f"è·å–å›¾åƒæ–‡å­—ä¿¡æ¯å‡ºé”™: {e}")
            return []

    def _get_all_texts_from_json(self, json_file_path):
        """
        ä»JSONæ–‡ä»¶ä¸­è·å–æ‰€æœ‰æ–‡å­—ä¿¡æ¯

        Args:
            json_file_path (str): JSONæ–‡ä»¶è·¯å¾„

        Returns:
            list: æ‰€æœ‰æ–‡å­—ä¿¡æ¯åˆ—è¡¨
        """
        try:
            with open(json_file_path, "r", encoding="utf-8") as f:
                data = json.load(f)

            rec_texts = data.get("rec_texts", [])
            rec_scores = data.get("rec_scores", [])
            dt_polys = data.get("dt_polys", [])

            texts_info = []

            for i, (text, score) in enumerate(zip(rec_texts, rec_scores)):
                if i < len(dt_polys):
                    poly = dt_polys[i]
                    x_coords = [point[0] for point in poly]
                    y_coords = [point[1] for point in poly]
                    center_x = int(sum(x_coords) / len(x_coords))
                    center_y = int(sum(y_coords) / len(y_coords))

                    texts_info.append(
                        {
                            "text": text,
                            "confidence": score,
                            "center": (center_x, center_y),
                            "bbox": poly,
                        }
                    )

            return texts_info

        except Exception as e:
            self.logger.error(f"è¯»å–JSONæ–‡ä»¶å‡ºé”™: {e}")
            return []