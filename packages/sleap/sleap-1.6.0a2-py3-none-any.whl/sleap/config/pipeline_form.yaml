training:

- name: _pipeline
  label: Training/Inference Pipeline Type
  type: stacked
  default: "multi-animal bottom-up "
  options: "multi-animal bottom-up,multi-animal top-down,multi-animal bottom-up-id,multi-animal top-down-id,single animal"

  multi-animal bottom-up:
  - type: text
    text: '<b>Multi-Animal Bottom-Up Pipeline</b>:<br />
      This pipeline uses single model with two output heads:
      a "<u>confidence map</u>" head to predicts the
      nodes for an entire image and a "<u>part affinity field</u>" head to group
      the nodes into distinct animal instances.'
  - label: Max Instances
    name: _max_instances
    type: optional_int
    help: Maximum number of instances per frame.
    none_label: No max
    default_disabled: true
    range: 1,100
    default: 1

  - name: model_config.head_configs.bottomup.confmaps.sigma
    label: Sigma for Nodes
    type: double
    default: 5.0
    help: Spread of the Gaussian distribution of the confidence maps as a scalar float.
      Smaller values are more precise but may be difficult to learn as they have a
      lower density within the image space. Larger values are easier to learn but
      are less precise with respect to the peak coordinate. This spread is in units
      of pixels of the model input image, i.e., the image resolution after any input
      scaling is applied.

  - name: model_config.head_configs.bottomup.pafs.sigma
    label: Sigma for Edges
    type: double
    default: 15.0
    help: Spread of the Gaussian distribution that weigh the part affinity fields
      as a function of their distance from the edge they represent. Smaller values
      are more precise but may be difficult to learn as they have a lower density
      within the image space. Larger values are easier to learn but are less precise
      with respect to the edge distance, so can be less useful in disambiguating between
      edges that are nearby and parallel in direction. This spread is in units of
      pixels of the model input image, i.e., the image resolution after any input
      scaling is applied.

  multi-animal top-down:
  - type: text
    text: '<b>Multi-Animal Top-Down Pipeline</b>:<br />
    This pipeline uses two models: a "<u>centroid</u>" model to
    locate and crop around each animal in the frame, and a
    "<u>centered-instance confidence map</u>" model for predicted node locations
    for each individual animal predicted by the centroid model.'
  - label: Max Instances
    name: _max_instances
    type: optional_int
    help: Maximum number of instances per frame.
    none_label: No max
    default_disabled: true
    range: 1,100
    default: 1

  - default: 5.0
    help: Spread of the Gaussian distribution of the confidence maps as a scalar float.
      Smaller values are more precise but may be difficult to learn as they have a
      lower density within the image space. Larger values are easier to learn but
      are less precise with respect to the peak coordinate. This spread is in units
      of pixels of the model input image, i.e., the image resolution after any input
      scaling is applied.
    label: Sigma for Centroids
    name: model_config.head_configs.centroid.confmaps.sigma
    type: double

  - default: null
    help: Text name of a body part (node) to use as the anchor point. If None, the
      midpoint of the bounding box of all visible instance points will be used as
      the anchor. The bounding box midpoint will also be used if the anchor part is
      specified but not visible in the instance. Setting a reliable anchor point can
      significantly improve topdown model accuracy as they benefit from a consistent
      geometry of the body parts relative to the center of the image.
    label: Anchor Part
    name: model_config.head_configs.centered_instance.confmaps.anchor_part
    type: optional_list

  - default: 5.0
    help: Spread of the Gaussian distribution of the confidence maps as a scalar float.
      Smaller values are more precise but may be difficult to learn as they have a
      lower density within the image space. Larger values are easier to learn but
      are less precise with respect to the peak coordinate. This spread is in units
      of pixels of the model input image, i.e., the image resolution after any input
      scaling is applied.
    label: Sigma for Nodes
    name: model_config.head_configs.centered_instance.confmaps.sigma
    type: double

  multi-animal bottom-up-id:
  - type: text
    text: '<b>Multi-Animal Bottom-Up-Id Pipeline</b>:<br />
      This pipeline uses single model with two output heads: a "<u>confidence
      map</u>" head to predicts the nodes for an entire image and a "<u>part
      affinity field</u>" head to group the nodes into distinct animal
      instances. It also handles classification and tracking.'

  - name: model_config.head_configs.multi_class_bottomup.confmaps.sigma
    label: Sigma for Nodes
    type: double
    default: 5.0
    help: Spread of the Gaussian distribution of the confidence maps as a scalar float.
      Smaller values are more precise but may be difficult to learn as they have a
      lower density within the image space. Larger values are easier to learn but
      are less precise with respect to the peak coordinate. This spread is in units
      of pixels of the model input image, i.e., the image resolution after any input
      scaling is applied.

  - name: model_config.head_configs.multi_class_bottomup.class_maps.sigma
    label: Sigma for Edges
    type: double
    default: 15.0
    help: Spread of the Gaussian distribution that weigh the part affinity fields
      as a function of their distance from the edge they represent. Smaller values
      are more precise but may be difficult to learn as they have a lower density
      within the image space. Larger values are easier to learn but are less precise
      with respect to the edge distance, so can be less useful in disambiguating between
      edges that are nearby and parallel in direction. This spread is in units of
      pixels of the model input image, i.e., the image resolution after any input
      scaling is applied.

  multi-animal top-down-id:
  - type: text
    text: '<b>Multi-Animal Top-Down-Id Pipeline</b>:<br />
    This pipeline uses two models: a "<u>centroid</u>" model to locate and crop
    around each animal in the frame, and a "<u>centered-instance confidence
    map</u>" model for predicted node locations for each individual animal
    predicted by the centroid model. It also handles classification and
    tracking.'

  - default: 5.0
    help: Spread of the Gaussian distribution of the confidence maps as a scalar float.
      Smaller values are more precise but may be difficult to learn as they have a
      lower density within the image space. Larger values are easier to learn but
      are less precise with respect to the peak coordinate. This spread is in units
      of pixels of the model input image, i.e., the image resolution after any input
      scaling is applied.
    label: Sigma for Centroids
    name: model_config.head_configs.centroid.confmaps.sigma
    type: double

  - default: null
    help: Text name of a body part (node) to use as the anchor point. If None, the
      midpoint of the bounding box of all visible instance points will be used as
      the anchor. The bounding box midpoint will also be used if the anchor part is
      specified but not visible in the instance. Setting a reliable anchor point can
      significantly improve topdown model accuracy as they benefit from a consistent
      geometry of the body parts relative to the center of the image.
    label: Anchor Part
    name: model_config.head_configs.multi_class_topdown.confmaps.anchor_part
    type: optional_list

  - default: 5.0
    help: Spread of the Gaussian distribution of the confidence maps as a scalar float.
      Smaller values are more precise but may be difficult to learn as they have a
      lower density within the image space. Larger values are easier to learn but
      are less precise with respect to the peak coordinate. This spread is in units
      of pixels of the model input image, i.e., the image resolution after any input
      scaling is applied.
    label: Sigma for Nodes
    name: model_config.head_configs.multi_class_topdown.confmaps.sigma
    type: double

  single animal:
  - type: text
    text: '<b>Single Animal Pipeline</b>:<br />
      This pipeline uses a single "<u>confidence map</u>"
      model to predicts the nodes for an entire image and then groups all of
      these nodes into a single animal instance.<br /><br />
      For predicting on videos with more than one animal per frame, use a
      multi-animal pipeline (even if your training data has one instance per frame).'

  - name: model_config.head_configs.single_instance.confmaps.sigma
    label: Sigma for Nodes
    type: double
    default: 5.0
    help: Spread of the Gaussian distribution of the confidence maps as a scalar float.
      Smaller values are more precise but may be difficult to learn as they have a
      lower density within the image space. Larger values are easier to learn but
      are less precise with respect to the peak coordinate. This spread is in units
      of pixels of the model input image, i.e., the image resolution after any input
      scaling is applied.

#general:
#- default: 8
#  help: Number of examples per minibatch, i.e., a single step of training. Higher
#    numbers can increase generalization performance by averaging model gradient updates
#    over a larger number of examples at the cost of considerably more GPU memory,
#    especially for larger sized images. Lower numbers may lead to overfitting, but
#    may be beneficial to the optimization process when few but varied examples are
#    available.
#  label: Batch Size
#  name: optimization.batch_size
#  type: int

- type: text
  text: '<b>Input Data Options</b>'

- default: ''
  help: If set, converts the image to RGB/grayscale if not already.
  label: Convert Image To
  name: _ensure_channels
  options: ',RGB,grayscale'
  type: list

- type: text
  text: '<b>Data Pipeline and Hardware Options</b>'

- default: 'Cache in Memory'
  help: 'Data loading strategy. Stream: Reads frames directly from video as needed (set workers=0). Cache in Memory: Pre-loads all frames into RAM for faster training (recommended). Cache to Disk: Saves frames to disk for reuse across training runs.'
  label: Data Pipeline Framework
  name: _data_pipeline_fw
  options: 'Stream (no caching),Cache in Memory,Cache to Disk'
  type: list

- name: trainer_config.train_data_loader.num_workers
  label: Data Loader Workers
  type: int
  default: 0
  range: 0,16
  help: Number of subprocesses for data loading. Use 0 for "Stream" mode to avoid race conditions. Use 2-4 with caching modes for faster loading.

- name: trainer_config.trainer_devices
  label: Number of GPU Devices
  type: optional_int
  default: null
  none_label: Auto
  help: Number of GPU devices to train on. "Auto" detects available GPUs.

- name: trainer_config.trainer_accelerator
  label: Device Accelerator
  type: list
  default: "auto"
  options: auto,cuda,cpu,mps
  help: Hardware accelerator for training. "auto" uses CUDA if available, otherwise CPU. Use "mps" for Apple Silicon Macs.

- type: text
  text: '<b>WandB options</b>'

- default: false
  help: If True, the model training will be logged to WandB.
  label: Enable WandB for logging
  name: trainer_config.use_wandb
  type: bool

- name: trainer_config.wandb.entity
  label:  Entity Name
  type: optional_string
  default: null
  help: Account name (username or organization).

- name: trainer_config.wandb.project
  label:  Project Name
  type: optional_string
  default: null
  help: Name for the group of runs.

- name: trainer_config.wandb.api_key
  label: WandB API Key
  type: optional_string
  default: null
  help: WandB API Key. From https://wandb.ai/authorize. You could also set it in your terminal by exporting the WANDB_API_KEY environment variable or `wandb.login()` in your Python shell.
  hidden: true

- name: trainer_config.wandb.prv_runid
  label: Previous Run ID
  type: optional_string
  default: null
  help: Previous run ID if this is a continuation of a previous run that was stopped.

- name: trainer_config.wandb.group
  label: Group Name
  type: optional_string
  default: null
  help: Grouping within the project.

- name: trainer_config.wandb.save_viz_imgs_wandb
  label: Upload Viz to WandB
  type: bool
  default: false
  help: If True, sample predictions saved during training will also be uploaded to WandB. Requires both "Enable WandB for logging" and "Visualize Predictions During Training" to be enabled.

- type: text
  text: '<b>Output Options</b>'

- default: null
  help: String to set as the run name. ckpts are saved to {Runs folder}/{Run name}. If None, it will be auto-generated {trainer_config.ckpt_dir}/{time_stamp}_{head_name}_n={num_samples}.
  label: Run Name
  name: trainer_config.run_name
  type: optional_string
- default: models
  help: 'Path to the folder that run data should be stored in. All the data for a
    single run are stored in the path: "{Runs folder}/{Run name}". Non-existing folders will be created
    if they do not already exist. Defaults to the "models" subdirectory of the current
    working directory.'
  label: Runs Folder
  name: trainer_config.ckpt_dir
  type: string
- default: true
  help: 'If True, the model will be saved at the end of an epoch if the validation
    loss has improved. If enabled, the model will be serialized to: "{run_folder}/best_model.h5"'
  label: Best Model
  name: trainer_config.save_ckpt
  type: bool
- default: false
  help: 'If True, the model will be saved at the end of every epoch, regardless of
    whether there was an improvement detected, but will overwrite the previous latest
    model. If enabled, the model will be serialized to: "{run_folder}/latest_model.h5"'
  label: Latest Model
  name: trainer_config.model_ckpt.save_last
  type: bool

- name: trainer_config.visualize_preds_during_training
  label: Visualize Predictions
  type: bool
  default: true
  help: If True, sample predictions will be saved during training for visual inspection.

- name: trainer_config.keep_viz
  label: Keep Viz Images
  type: bool
  default: false
  help: If True, keep prediction visualization images after training completes. Otherwise they are deleted.

- name: _predict_frames
  label: Predict On
  type: list
  options: current frame,random frames
  default: current frame
  help: Which frames to run prediction on for visualization during training. "Current frame" uses the frame currently displayed in the GUI. "Random frames" selects random frames from the labeled data.

inference:

- name: _pipeline
  label: Training/Inference Pipeline Type
  type: stacked
  default: "multi-animal bottom-up "
  options: "multi-animal bottom-up,multi-animal top-down,multi-animal bottom-up-id,multi-animal top-down-id,single animal,movenet-lightning,movenet-thunder,tracking-only"

  multi-animal bottom-up:
  - type: text
    text: '<b>Multi-Animal Bottom-Up Pipeline</b>:<br />
      This pipeline uses single model with two output heads:
      a "<u>confidence map</u>" head to predicts the
      nodes for an entire image and a "<u>part affinity field</u>" head to group
      the nodes into distinct animal instances.'
  - label: Max Instances
    name: _max_instances
    type: optional_int
    help: Maximum number of instances per frame.
    none_label: No max
    default_disabled: true
    range: 1,100
    default: 1

  multi-animal top-down:
  - type: text
    text: '<b>Multi-Animal Top-Down Pipeline</b>:<br />
    This pipeline uses two models: a "<u>centroid</u>" model to
    locate and crop around each animal in the frame, and a
    "<u>centered-instance confidence map</u>" model for predicted node locations
    for each individual animal predicted by the centroid model.'
  - label: Max Instances
    name: _max_instances
    type: optional_int
    help: Maximum number of instances per frame.
    none_label: No max
    default_disabled: true
    range: 1,100
    default: 1

  multi-animal bottom-up-id:
  - type: text
    text: '<b>Multi-Animal Bottom-Up-Id Pipeline</b>:<br />
      This pipeline uses single model with two output heads: a "<u>confidence
      map</u>" head to predicts the nodes for an entire image and a "<u>part
      affinity field</u>" head to group the nodes into distinct animal
      instances. It also handles classification and tracking.'

  multi-animal top-down-id:
  - type: text
    text: '<b>Multi-Animal Top-Down-Id Pipeline</b>:<br />
    This pipeline uses two models: a "<u>centroid</u>" model to locate and crop
    around each animal in the frame, and a "<u>centered-instance confidence
    map</u>" model for predicted node locations for each individual animal
    predicted by the centroid model. It also handles classification and
    tracking.'

  single animal:
  - type: text
    text: '<b>Single Animal Pipeline</b>:<br />
      This pipeline uses a single "<u>confidence map</u>"
      model to predicts the nodes for an entire image and then groups all of
      these nodes into a single animal instance.<br /><br />
      For predicting on videos with more than one animal per frame, use a
      multi-animal pipeline (even if your training data has one instance per frame).'

  movenet-lightning:
  - type: text
    text: '<b>MoveNet Lightning Pipeline</b>:<br />
      This pipeline uses a pretrained MoveNet Lightning model to predict the
      nodes for an entire image and then groups all of these nodes into a single
      instance. Lightning is intended for latency-critical applications. Note
      that this model is intended for human pose estimation. There is no support
      for videos containing more than one instance'

  movenet-thunder:
  - type: text
    text: '<b>MoveNet Thunder Pipeline</b>:<br />
      This pipeline uses a pretrained MoveNet Thunder model to predict the nodes
      for an entire image and then groups all of these nodes into a single
      instance. Thunder is intended for applications that require high accuracy.
      Note that this model is intended for human pose estimation. There is no
      support for videos containing more than one instance'

  tracking-only:

- name: _batch_size
  label: Batch Size
  type: int
  default: 4
  range: 1,512
  help: Number of frames to process in parallel during inference. Higher values can speed up inference by better utilizing GPU parallelism, but require more GPU memory. Reduce if you encounter out-of-memory errors.

- name: tracking.tracker
  label: Tracker (cross-frame identity) Method
  type: stacked
  default: none
  options: none,flow,simple

  none:

  flow:
    # - type: text
    #   text: '<b>Pre-tracker data cleaning</b>:'
    # - name: tracking.target_instance_count
    #   label: Target Number of Instances Per Frame
    #   type: optional_int
    #   none_label: No target
    #   default_disabled: true
    #   range: 1,100
    #   default: 1
    # - name: tracking.pre_cull_to_target
    #   label: Cull to Target Instance Count
    #   type: bool
    #   default: false
    # - name: tracking.pre_cull_iou_threshold
    #   label: Cull using IoU Threshold
    #   type: double
    #   default: 0.8
    - type: text
      text: '<b>Tracking with optical flow</b>:<br />
      This tracker "shifts" instances from previous frames using optical flow
      before matching instances in each frame to the <i>shifted</i> instances from
      prior frames.'
    - name: tracking.max_tracks
      label: Max number of tracks
      type: optional_int
      none_label: No limit
      default_disabled: true
      range: 1,100
      default: 1
      help: Maximum number of unique track identities to maintain. Set to limit memory usage and computation for videos with many animals. "No limit" allows unlimited tracks.
    - name: tracking.similarity
      label: Similarity Method
      type: list
      default: oks
      options: "oks,iou,centroids"
      help: Method to compute similarity between instances across frames. OKS (Object Keypoint Similarity) uses all keypoint positions. IoU uses intersection over union of bounding boxes. Centroids uses only the center point distance.
    - name: tracking.match
      label: Matching Method
      type: list
      default: greedy
      options: greedy,hungarian
      help: Algorithm for assigning track identities across frames. Greedy assigns best matches first (faster). Hungarian finds the optimal global assignment (more accurate but slower).
    - name: tracking.track_window
      label: Elapsed Frame Window
      type: int
      default: 5
      help: Number of previous frames to consider when matching instances to existing tracks. Larger values help recover from brief occlusions or detection failures but use more memory.
    - name: tracking.robust
      label: 'Robust quantile of similarity scores'
      help: 'For a value between 0 and 1 (excluded), use a robust quantile
      of the similarity scores to assign a track to an instance.<br />If equal to 1,
      use the max similarity score (non-robust).'
      type: optional_double
      default_disabled: true
      none_label: Use max (non-robust)
      range: 0,1
      default: 0.95
    - type: text
      text: '<b>Post-tracker data cleaning</b>:'
    - name: tracking.post_connect_single_breaks
      label: Connect Single Track Breaks
      type: bool
      default: false
      help: If enabled, automatically connect track fragments that have single-frame gaps. This can help recover from brief detection failures.

  simple:
    # - type: text
    #   text: '<b>Pre-tracker data cleaning</b>:'
    # - name: tracking.target_instance_count
    #   label: Target Number of Instances Per Frame
    #   type: optional_int
    #   none_label: No target
    #   default_disabled: true
    #   range: 1,100
    #   default: 1
    # - name: tracking.pre_cull_to_target
    #   label: Cull to Target Instance Count
    #   type: bool
    #   default: false
    # - name: tracking.pre_cull_iou_threshold
    #   label: Cull using IoU Threshold
    #   type: double
    #   default: 0.8
    - type: text
      text: '<b>Tracking</b>:<br />
        This tracker assigns track identities by matching instances from prior
        frames to instances on subsequent frames.'
    # - name: tracking.max_tracking
    #   label: Limit max number of tracks
    #   type: bool
    #   default: false
    - name: tracking.max_tracks
      label: Max number of tracks
      type: optional_int
      none_label: No limit
      default_disabled: true
      range: 1,100
      default: 1
      help: Maximum number of unique track identities to maintain. Set to limit memory usage and computation for videos with many animals. "No limit" allows unlimited tracks.
    - name: tracking.similarity
      label: Similarity Method
      type: list
      default: instance
      options: "centroid,iou,object keypoint"
      help: Method to compute similarity between instances across frames. Centroid uses only the center point distance (fastest). IoU uses intersection over union of bounding boxes. Object keypoint uses all keypoint positions (most accurate).
    - name: tracking.match
      label: Matching Method
      type: list
      default: hungarian
      options: greedy,hungarian
      help: Algorithm for assigning track identities across frames. Greedy assigns best matches first (faster). Hungarian finds the optimal global assignment (more accurate but slower).
    - name: tracking.track_window
      label: Elapsed Frame Window
      type: int
      default: 5
      help: Number of previous frames to consider when matching instances to existing tracks. Larger values help recover from brief occlusions or detection failures but use more memory.
    - name: tracking.robust
      label: 'Robust quantile of similarity scores'
      help: 'For a value between 0 and 1 (excluded), use a robust quantile
      of the similarity scores to assign a track to an instance.<br />If equal to 1,
      use the max similarity score (non-robust).'
      type: optional_double
      default_disabled: true
      none_label: Use max (non-robust)
      range: 0,1
      default: 0.95
    - type: text
      text: '<b>Post-tracker data cleaning</b>:'
    - name: tracking.post_connect_single_breaks
      label: Connect Single Track Breaks
      type: bool
      default: false
      help: If enabled, automatically connect track fragments that have single-frame gaps. This can help recover from brief detection failures.




- name: _predict_frames
  label: Predict On
  type: list
  options: current frame,random frames
  default: current frame
  help: Which frames to run prediction on. "Current frame" uses the frame currently displayed in the GUI. "Random frames" selects random frames from the video.
