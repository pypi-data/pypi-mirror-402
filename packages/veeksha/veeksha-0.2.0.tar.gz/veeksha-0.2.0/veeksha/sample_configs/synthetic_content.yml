seed: 42

traffic_scheduler:
  type: rate
  interval_generator:
    type: poisson
    arrival_rate: 10
  cancel_session_on_failure: False

session_generator:
  type: synthetic
  session_graph:
    type: linear
    inherit_history: True
    num_request_generator:
      type: uniform
      min: 2
      max: 4
    request_wait_generator:
      type: poisson
      arrival_rate: 5
  channels:
    - type: text
      shared_prefix_ratio: 0.2
      shared_prefix_probability: 0.5 # 50% of root requests have 20% shared prefix
      body_length_generator:
        type: uniform
        min: 20
        max: 30
  output_spec:
    text:
      output_length_generator:
        type: uniform
        min: 100
        max: 500

client:
  type: openai_chat_completions
  model: meta-llama/Llama-3.2-1B-Instruct
  request_timeout: 60
  api_base: http://localhost:30002/v1
  max_tokens_param: max_completion_tokens
  min_tokens_param: min_tokens
  use_min_tokens_prompt_fallback: False

runtime:
  max_sessions: 10
  benchmark_timeout: 60

trace_recorder:
  enabled: True
  include_content: True

evaluators:
  - type: performance
    target_channels: ["text"]
    stream_metrics: true
    stream_metrics_interval: 5.0
    slos:
      - name: "P99 TTFC under 500ms"
        metric: ttfc          # Metric to check
        percentile: 0.99      # Percentile level
        value: 0.5            # Threshold in seconds
        type: constant        # SLO type
