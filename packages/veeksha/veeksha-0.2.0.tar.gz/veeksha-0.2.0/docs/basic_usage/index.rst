Basic usage
===========

This section covers the fundamentals of using Veeksha for LLM benchmarking.
You'll learn how to create configurations, run benchmarks, and interpret results.

Whether you're evaluating a new inference engine or monitoring production
performance, these guides will help you get started quickly.


Prerequisites
-------------

Before running benchmarks, ensure you have:

1. **Veeksha installed** (see :doc:`/installation`)
2. **A running LLM inference endpoint**
   - or use a :doc:`/advanced_usage/server_management` to have Veeksha launch one automatically
3. **Python 3.12+** (3.14t highly recommended for best performance)


In this section
---------------

.. toctree::
   :maxdepth: 2

   quick_start
   configuration
   output_files
   wandb_integration