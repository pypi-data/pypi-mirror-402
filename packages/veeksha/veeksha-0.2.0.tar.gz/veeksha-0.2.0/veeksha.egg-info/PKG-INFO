Metadata-Version: 2.4
Name: veeksha
Version: 0.2.0
Summary: A framework for holistic evaluation of LLM Inference Systems
Author: Vajra Team
License: Apache-2.0
Project-URL: Homepage, https://github.com/project-vajra/veeksha
Project-URL: Documentation, https://project-vajra.github.io/veeksha
Classifier: Programming Language :: Python :: 3.12
Classifier: Programming Language :: Python :: 3.13
Classifier: Programming Language :: Python :: 3.14
Classifier: License :: OSI Approved :: Apache Software License
Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
Requires-Python: >=3.12
Description-Content-Type: text/markdown
License-File: LICENSE
License-File: NOTICE.txt
Requires-Dist: pydantic
Requires-Dist: transformers
Requires-Dist: tqdm
Requires-Dist: ddsketch
Requires-Dist: wandb
Requires-Dist: pandas
Requires-Dist: numpy
Requires-Dist: jinja2
Requires-Dist: datasets
Requires-Dist: lm-eval
Requires-Dist: evaluate
Requires-Dist: pytablewriter
Requires-Dist: Pillow
Requires-Dist: rekha
Requires-Dist: textual
Requires-Dist: textual-plotext
Requires-Dist: requests
Requires-Dist: uvloop
Requires-Dist: httpx
Requires-Dist: nvidia-ml-py
Provides-Extra: dev
Requires-Dist: autoflake; extra == "dev"
Requires-Dist: black==25.9.0; extra == "dev"
Requires-Dist: codespell; extra == "dev"
Requires-Dist: isort==7.0.0; extra == "dev"
Requires-Dist: pyright; extra == "dev"
Requires-Dist: pytest>=8.0; extra == "dev"
Requires-Dist: pytest-cov; extra == "dev"
Requires-Dist: requests; extra == "dev"
Provides-Extra: test
Requires-Dist: pytest>=8.0; extra == "test"
Requires-Dist: pytest-cov; extra == "test"
Requires-Dist: requests; extra == "test"
Requires-Dist: vllm; extra == "test"
Requires-Dist: rich; extra == "test"
Requires-Dist: torch; extra == "test"
Requires-Dist: jinja2; extra == "test"
Dynamic: license-file

# Veeksha

[![Publish Release to PyPI](https://github.com/project-vajra/veeksha/actions/workflows/publish_release.yml/badge.svg)](https://github.com/project-vajra/veeksha/actions/workflows/publish_release.yml) [![Deploy Documentation](https://github.com/project-vajra/veeksha/actions/workflows/deploy_docs.yml/badge.svg)](https://github.com/project-vajra/veeksha/actions/workflows/deploy_docs.yml) [![Test Suite](https://github.com/project-vajra/veeksha/actions/workflows/test_veeksha.yml/badge.svg)](https://github.com/project-vajra/veeksha/actions/workflows/test_veeksha.yml) [![Run Linters](https://github.com/project-vajra/veeksha/actions/workflows/lint.yml/badge.svg)](https://github.com/project-vajra/veeksha/actions/workflows/lint.yml)

**Veeksha** is a high-fidelity benchmarking framework for LLM inference systems.
Whether you're optimizing a production deployment, comparing serving backends, or
running capacity planning experiments, Veeksha lets you measure what matters to you:
realistic multi-turn conversations, agentic workflows, high-frequency stress tests, or targeted
microbenchmarks. One tool, any workload.

**From isolated requests to complex agentic sessions, Veeksha captures the full complexity of modern LLM workloads.**

ðŸ‘‰ **[Why Veeksha?](https://project-vajra.github.io/veeksha/why_veeksha.html)** â€” Learn what sets Veeksha apart  
ðŸ“š **[Documentation](https://project-vajra.github.io/veeksha)** â€” Full guides and API reference

## Quick start

In a fresh environment (Python 3.14t recommended for true parallelism):

Install from PyPI:

```bash
pip install veeksha
```

Run a benchmark against an OpenAI-compatible endpoint:

```bash
python -Xgil=0 -m veeksha.benchmark \
    --client-type openai_chat_completions \
    --openai-chat-completions-client-api-base http://localhost:8000/v1 \
    --openai-chat-completions-client-model meta-llama/Llama-3.2-1B-Instruct \
    --traffic-scheduler-type rate \
    --rate-traffic-scheduler-interval-generator-type poisson \
    --rate-traffic-scheduler-poisson-interval-generator-arrival-rate 5.0 \
    --runtime-benchmark-timeout 60
```

Or use a YAML configuration file:

```bash
python -Xgil=0 -m veeksha.benchmark --benchmark-config-from-file my_benchmark.veeksha.yml
```

## Installation from source

```bash
git clone https://github.com/project-vajra/veeksha.git
cd veeksha

# Install uv if needed
curl -LsSf https://astral.sh/uv/install.sh | sh

# Create environment (Python 3.14t recommended for true parallelism)
uv venv --python 3.14t
source .venv/bin/activate
uv pip install -e .
```
