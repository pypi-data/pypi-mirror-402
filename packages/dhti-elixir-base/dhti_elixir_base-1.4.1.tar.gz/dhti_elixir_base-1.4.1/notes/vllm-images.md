## sending images to a vllm model

const base64ImageMessage = new HumanMessage({
    content: [{
            type: 'text',
            text: `${input}`,
        },{
            type: 'image_url',
            image_url: fileBase64,
        },
    ],
});

## Using in a chain

# Got inspiration from here: https://github.com/langchain-ai/langchain/discussions/20820

        human = HumanMessagePromptTemplate.from_template(
            template=[
                {"type": "text", "text": "{input}"},
                {
                     "type": "image_url",
                     "image_url": "{encoded_image_url}",
                },
            ]
        )

        self.image_prompt = ChatPromptTemplate.from_messages(
            [
                (
                    "system", "{system_prompt}",
                ),
                human,
            ]
        )

        self.lang_agent = (
           {
               "system_prompt": lambda x: x["system_prompt"],
               "input": lambda x: x["input"],
               "encoded_image_url": lambda x: x["encoded_image_url"],
           }
            | self.image_prompt
            | self.llm
            | OpenAIToolsAgentOutputParser()
        )


** Then when you run the agent you have to supply "encoded_image_url" (URL or b64 data).
**  Seems like you have to provide the image *every time* you want to ask a question.

You can generally create ToolMessages containing an image content block, e.g.,

ToolMessage(
    content=[
        {
            "type": "image",
            "source_type": "base64",
            "data": image_data,
            "mime_type": "image/jpeg",
        },
    ],
    tool_call_id="...",
)
The above format is LangChainâ€™s cross-provider standard. Providers also support OpenAI format:

ToolMessage(
    content=[
        {
            "type": "image_url",
            "image_url": {"url": f"data:image/jpeg;base64,{image_data}"},
        },
    ],
    tool_call_id="...",
)

from pydantic import BaseModel, Field
from langchain_openai import ChatOpenAI
from langchain_core.messages import SystemMessage, HumanMessage
from langchain_core.tools import tool

class ImageInfo(BaseModel):
    description: str = Field(description="A description of the image")
    table: bool = Field(description="Whether there is a table in the image")
    table_content: str = Field(description="The content of the table if there is one")

llm = ChatOpenAI(model="gpt-4o-mini")
structured_llm = llm.with_structured_output(ImageInfo)

@tool
def extract_image_info(image_url: str) -> dict:
    """
    Extracts structured information from an image given the following output schema:
    - description: A description of the image
    - table: Whether there is a table in the image
    - table_content: The content of the table if there is one

    Args:
        image_url: The URL of the image to analyze

    Returns:
        A dictionary containing the description, table, and table content
    """


    response = structured_llm.invoke([
        SystemMessage(content="You are a helpful assistant that extracts structured information from an image."),
        HumanMessage(
            content=[
                {"type": "text", "text": "Analyze the image"},
                {
                    "type": "image",
                    "source_type": "url",
                    "url": image_url,
                },
            ],
        )
    ])
    return response.model_dump()


# Image Path

from langchain.tools import tool
from PIL import Image

@tool
def analyze_image_file(image_path: str) -> str:
    """Useful for analyzing the contents of an image file at a specific path."""
    try:
        # Open and process the image locally
        with Image.open(image_path) as img:
            width, height = img.size
            # Placeholder for actual analysis (e.g., calling an OCR or CLIP model)
            return f"The image at {image_path} is {width}x{height} pixels."
    except Exception as e:
        return f"Error processing image: {str(e)}"

# Returning image

python
import base64
from langchain_core.tools import tool

def encode_image(image_path: str) -> str:
    with open(image_path, "rb") as image_file:
        return base64.b64encode(image_file.read()).decode('utf-8')

@tool
def get_screenshot_tool() -> list:
    """Captures a screenshot and returns it as a multimodal data block."""
    # Assuming path is generated by a screenshot function
    path = "screenshot.png"
    base64_image = encode_image(path)

    # Return formatted content block for multimodal models
    return [
        {
            "type": "image_url",
            "image_url": {"url": f"data:image/png;base64,{base64_image}"}
        }
    ]


## Input schema
from pydantic import BaseModel, Field
from langchain.tools import tool

class ImageInput(BaseModel):
    image_data: str = Field(description="Base64 encoded image string or image URL")
    operation: str = Field(description="The action to perform: 'resize', 'grayscale', etc.")

@tool(args_schema=ImageInput)
def image_processor(image_data: str, operation: str) -> str:
    """Processes image data based on the specified operation."""
    # Implementation logic here
    return f"Successfully performed {operation} on the provided image data."