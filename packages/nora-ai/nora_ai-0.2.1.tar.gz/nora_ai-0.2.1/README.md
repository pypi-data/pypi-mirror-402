# Nora Observability SDK

OpenAI, Anthropic, Google Gemini ë“± ì£¼ìš” AI ë¼ì´ë¸ŒëŸ¬ë¦¬ í˜¸ì¶œì„ ìë™ìœ¼ë¡œ ì¶”ì í•˜ê³  ë¶„ì„í•˜ëŠ” Python SDKì…ë‹ˆë‹¤.

## âœ¨ ì£¼ìš” ê¸°ëŠ¥

- ğŸš€ **2ì¤„ë¡œ ì‹œì‘**: `import nora` + `nora.init()`ë§Œìœ¼ë¡œ ìë™ trace í™œì„±í™”
- ğŸ” **ìë™ ê°ì§€**: OpenAI, Anthropic, Gemini API ìë™ íŒ¨ì¹˜
- ğŸ› ï¸ **Tool ì‹¤í–‰ ì¶”ì **: AIê°€ í˜¸ì¶œí•œ function tool ì‹¤í–‰ë„ ìë™ìœ¼ë¡œ ì¶”ì 
- ğŸ“Š **ìƒì„¸í•œ ë©”íƒ€ë°ì´í„°**: í”„ë¡¬í”„íŠ¸, ì‘ë‹µ, í† í° ì‚¬ìš©ëŸ‰, ì‹¤í–‰ ì‹œê°„, ë¹„ìš© ëª¨ë‘ ê¸°ë¡
- ğŸ‘¥ **TraceGroup**: ì—¬ëŸ¬ API í˜¸ì¶œì„ ë…¼ë¦¬ì ìœ¼ë¡œ ê·¸ë£¹í™”í•˜ì—¬ ë©€í‹° ì—ì´ì „íŠ¸ ì›Œí¬í”Œë¡œìš° ì¶”ì 
- âš¡ **ë¹„ë™ê¸° ì§€ì›**: ë™ê¸°/ë¹„ë™ê¸° ëª¨ë‘ ì™„ë²½ ì§€ì› (async/await, generators)
- ğŸ¯ **ë°ì½”ë ˆì´í„° ì§€ì›**: `@nora.trace_group` ë°ì½”ë ˆì´í„°ë¡œ í•¨ìˆ˜ ë‹¨ìœ„ ì¶”ì 
- ğŸ›¡ï¸ **ì•ˆì „í•œ ë™ì‘**: ì—ëŸ¬ ë°œìƒ ì‹œì—ë„ ì‚¬ìš©ì ì½”ë“œì— ì˜í–¥ ì—†ìŒ

## ëª©ì°¨

- [ì„¤ì¹˜](#ì„¤ì¹˜)
- [ë¹ ë¥¸ ì‹œì‘](#ë¹ ë¥¸-ì‹œì‘)
- [nora.init() ì‚¬ìš©ë²•](#norainit-ì‚¬ìš©ë²•)
- [@nora.trace_group ë°ì½”ë ˆì´í„°](#noratrace_group-ë°ì½”ë ˆì´í„°)
- [with/async with context manager](#withasync-with-context-manager)
- [ê³ ê¸‰ ì‚¬ìš©ë²•](#ê³ ê¸‰-ì‚¬ìš©ë²•)
- [API ì‘ë‹µ í˜•ì‹](#api-ì‘ë‹µ-í˜•ì‹)
- [ì§€ì›í•˜ëŠ” AI ë¼ì´ë¸ŒëŸ¬ë¦¬](#ì§€ì›í•˜ëŠ”-ai-ë¼ì´ë¸ŒëŸ¬ë¦¬)

## ì„¤ì¹˜

```bash
pip install nora-observability
```

## ë¹ ë¥¸ ì‹œì‘

### ê¸°ë³¸ ì‚¬ìš© (OpenAI)

```python
import nora
from openai import OpenAI

# 1. Nora ì´ˆê¸°í™” (ë‹¨ í•œ ì¤„!)
nora.init(api_key="YOUR_NORA_API_KEY")

# 2. OpenAI í´ë¼ì´ì–¸íŠ¸ ì‚¬ìš© - ì´ì œ ëª¨ë“  í˜¸ì¶œì´ ìë™ìœ¼ë¡œ ì¶”ì ë©ë‹ˆë‹¤!
client = OpenAI(api_key="YOUR_OPENAI_API_KEY")
response = client.chat.completions.create(
    model="gpt-4o-mini",
    messages=[{"role": "user", "content": "Hello!"}]
)

print(response.choices[0].message.content)
# ì¶œë ¥: "Hello! How can I assist you today?"
```

**ë¬´ìŠ¨ ì¼ì´ ì¼ì–´ë‚¬ë‚˜ìš”?**
- Noraê°€ OpenAI API í˜¸ì¶œì„ ìë™ìœ¼ë¡œ ê°ì§€
- ìš”ì²­ íŒŒë¼ë¯¸í„°(model, messages)ì™€ ì‘ë‹µ(content, tokens) ìˆ˜ì§‘
- ë°±ì—”ë“œ ì„œë²„ë¡œ trace ë°ì´í„° ì „ì†¡
- ëŒ€ì‹œë³´ë“œì—ì„œ ì‹¤ì‹œê°„ í™•ì¸ ê°€ëŠ¥

### Anthropic (Claude) ì‚¬ìš©

```python
import nora
from anthropic import Anthropic

nora.init(api_key="YOUR_NORA_API_KEY")

client = Anthropic(api_key="YOUR_ANTHROPIC_API_KEY")
response = client.messages.create(
    model="claude-opus-4-5-20251101",
    max_tokens=1024,
    messages=[{"role": "user", "content": "Explain quantum computing"}]
)

print(response.content[0].text)
```

### Google Gemini ì‚¬ìš©

```python
import nora
from google import genai

nora.init(api_key="YOUR_NORA_API_KEY")

client = genai.Client(api_key="YOUR_GOOGLE_API_KEY")
response = client.models.generate_content(
    model="gemini-2.0-flash-exp",
    contents="Write a haiku about AI"
)

print(response.text)
```

---

## nora.init() ì‚¬ìš©ë²•

`nora.init()`ì€ Nora Observabilityë¥¼ ì´ˆê¸°í™”í•˜ê³  AI ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ìë™ íŒ¨ì¹˜í•©ë‹ˆë‹¤.

### ê¸°ë³¸ ë¬¸ë²•

```python
nora.init(
    api_key: str,                    # í•„ìˆ˜
    api_url: str = "...",            # ì„ íƒ
    auto_patch: bool = True,         # ì„ íƒ
    traced_functions: List[str] = None,  # ì„ íƒ
    service_url: str = None,         # ì„ íƒ
    environment: str = "default"     # ì„ íƒ
)
```

### íŒŒë¼ë¯¸í„° ìƒì„¸ ì„¤ëª…

#### `api_key` (í•„ìˆ˜)
- **íƒ€ì…**: `str`
- **ì„¤ëª…**: Nora ë°±ì—”ë“œ ì¸ì¦ìš© API í‚¤
- **ì˜ˆì‹œ**:
  ```python
  nora.init(api_key="mOSAWtuWhb58tQXMxkyAU6rfUhHUZll465cCqIpaymQ")
  ```

#### `api_url` (ì„ íƒ)
- **íƒ€ì…**: `str`
- **ê¸°ë³¸ê°’**: `"https://noraobservabilitybackend-staging.up.railway.app/v1"`
- **ì„¤ëª…**: Trace ë°ì´í„°ë¥¼ ì „ì†¡í•  ë°±ì—”ë“œ ì„œë²„ URL
- **ì˜ˆì‹œ**:
  ```python
  # í”„ë¡œë•ì…˜ ì„œë²„ ì‚¬ìš©
  nora.init(
      api_key="your-key",
      api_url="https://api.nora-production.com/v1"
  )
  
  # ë¡œì»¬ ê°œë°œ ì„œë²„ ì‚¬ìš©
  nora.init(
      api_key="your-key",
      api_url="http://localhost:8000/v1"
  )
  ```

#### `auto_patch` (ì„ íƒ)
- **íƒ€ì…**: `bool`
- **ê¸°ë³¸ê°’**: `True`
- **ì„¤ëª…**: AI ë¼ì´ë¸ŒëŸ¬ë¦¬ ìë™ íŒ¨ì¹˜ í™œì„±í™” ì—¬ë¶€
- **ì˜ˆì‹œ**:
  ```python
  # ìë™ íŒ¨ì¹˜ ë¹„í™œì„±í™” (ìˆ˜ë™ìœ¼ë¡œ trace ê¸°ë¡í•  ë•Œ)
  nora.init(api_key="your-key", auto_patch=False)
  ```

#### `traced_functions` (ì„ íƒ)
- **íƒ€ì…**: `List[str]`
- **ê¸°ë³¸ê°’**: `None`
- **ì„¤ëª…**: ìë™ìœ¼ë¡œ `trace_group`ìœ¼ë¡œ ê°ìŒ€ í•¨ìˆ˜ëª… ë¦¬ìŠ¤íŠ¸
- **ì˜ˆì‹œ**:
  ```python
  # multi_agent_workflowì™€ process_data í•¨ìˆ˜ ìë™ ì¶”ì 
  nora.init(
      api_key="your-key",
      traced_functions=["multi_agent_workflow", "process_data"]
  )
  
  # ì´ì œ ì´ í•¨ìˆ˜ë“¤ì´ í˜¸ì¶œë˜ë©´ ìë™ìœ¼ë¡œ trace_groupì´ ìƒì„±ë¨
  def multi_agent_workflow():
      # ì—¬ê¸°ì„œ ë°œìƒí•˜ëŠ” ëª¨ë“  AI í˜¸ì¶œì´ ê·¸ë£¹ìœ¼ë¡œ ë¬¶ì„
      agent1_response = openai_client.chat.completions.create(...)
      agent2_response = openai_client.chat.completions.create(...)
  ```

#### `service_url` (ì„ íƒ)
- **íƒ€ì…**: `str`
- **ê¸°ë³¸ê°’**: `None`
- **ì„¤ëª…**: ì™¸ë¶€ í”¼ë“œë°± ì„œë¹„ìŠ¤ URL (ë‚˜ì¤‘ì— ì‚¬ìš©ì í”¼ë“œë°± ìˆ˜ì§‘ìš©)
- **ì˜ˆì‹œ**:
  ```python
  nora.init(
      api_key="your-key",
      service_url="https://my-app.com/api/feedback"
  )
  ```

#### `environment` (ì„ íƒ)
- **íƒ€ì…**: `str`
- **ê¸°ë³¸ê°’**: `"default"`
- **ì„¤ëª…**: ì‹¤í–‰ í™˜ê²½ íƒœê·¸ (ê°œë°œ/ìŠ¤í…Œì´ì§•/í”„ë¡œë•ì…˜ êµ¬ë¶„)
- **ì˜ˆì‹œ**:
  ```python
  import os
  
  # í™˜ê²½ë³„ë¡œ ë‹¤ë¥¸ íƒœê·¸ ì„¤ì •
  env = os.getenv("APP_ENV", "development")
  nora.init(
      api_key="your-key",
      environment=env  # "development", "staging", "production"
  )
  
  # ëŒ€ì‹œë³´ë“œì—ì„œ í™˜ê²½ë³„ë¡œ í•„í„°ë§ ê°€ëŠ¥
  ```

### ì‹¤ì „ ì˜ˆì‹œ

#### ì˜ˆì‹œ 1: ìµœì†Œ ì„¤ì • (í”„ë¡œë•ì…˜)
```python
import nora

nora.init(api_key="mOSAWtuWhb58tQXMxkyAU6rfUhHUZll465cCqIpaymQ")
```

#### ì˜ˆì‹œ 2: í™˜ê²½ ë³€ìˆ˜ ì‚¬ìš© (ê¶Œì¥)
```python
import os
from dotenv import load_dotenv
import nora

load_dotenv()

nora.init(
    api_key=os.getenv("NORA_API_KEY"),
    environment=os.getenv("APP_ENV", "production")
)
```

#### ì˜ˆì‹œ 3: ë©€í‹° ì—ì´ì „íŠ¸ ìë™ ì¶”ì 
```python
import nora

nora.init(
    api_key="your-key",
    traced_functions=["research_agent", "writer_agent", "reviewer_agent"],
    environment="production"
)

# ì´ì œ ì´ í•¨ìˆ˜ë“¤ì´ í˜¸ì¶œë˜ë©´ ìë™ìœ¼ë¡œ ê·¸ë£¹ìœ¼ë¡œ ì¶”ì ë¨
def research_agent(query):
    return openai_client.chat.completions.create(...)

def writer_agent(research_result):
    return openai_client.chat.completions.create(...)

def reviewer_agent(draft):
    return openai_client.chat.completions.create(...)
```

#### ì˜ˆì‹œ 4: ê°œë°œ í™˜ê²½ ì „ì²´ ì„¤ì •
```python
import nora

nora.init(
    api_key="dev-api-key",
    api_url="http://localhost:8000/v1",
    traced_functions=["main_workflow"],
    environment="development"
)

# í”„ë¡œê·¸ë¨ ì¢…ë£Œ ì „ ë‚¨ì€ trace ë°ì´í„° ì¦‰ì‹œ ì „ì†¡
import atexit
atexit.register(lambda: nora.flush(sync=True))
```

---

## @nora.trace_group ë°ì½”ë ˆì´í„°

í•¨ìˆ˜ë¥¼ ë°ì½”ë ˆì´í„°ë¡œ ê°ì‹¸ì„œ í•´ë‹¹ í•¨ìˆ˜ ë‚´ë¶€ì˜ ëª¨ë“  AI í˜¸ì¶œì„ í•˜ë‚˜ì˜ ê·¸ë£¹ìœ¼ë¡œ ì¶”ì í•©ë‹ˆë‹¤.

### ê¸°ë³¸ ë¬¸ë²•

```python
@nora.trace_group(name="ê·¸ë£¹ì´ë¦„", metadata={"key": "value"})
def my_function():
    # ì´ í•¨ìˆ˜ ë‚´ ëª¨ë“  AI í˜¸ì¶œì´ ê·¸ë£¹ìœ¼ë¡œ ë¬¶ì„
    pass
```

### íŒŒë¼ë¯¸í„°

- **`name`** (ì„ íƒ): ê·¸ë£¹ ì´ë¦„ (ìƒëµ ì‹œ í•¨ìˆ˜ ì´ë¦„ ì‚¬ìš©)
- **`metadata`** (ì„ íƒ): ì¶”ê°€ ë©”íƒ€ë°ì´í„° ë”•ì…”ë„ˆë¦¬

### ì˜ˆì‹œ 1: ê¸°ë³¸ ì‚¬ìš© (ë™ê¸° í•¨ìˆ˜)

```python
import nora
from openai import OpenAI

nora.init(api_key="your-key")
client = OpenAI()

@nora.trace_group(name="summarize_article")
def summarize_article(article_text):
    """ê¸°ì‚¬ë¥¼ ìš”ì•½í•˜ëŠ” í•¨ìˆ˜"""
    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {"role": "system", "content": "You are a summarization expert."},
            {"role": "user", "content": f"Summarize this: {article_text}"}
        ]
    )
    return response.choices[0].message.content

# í•¨ìˆ˜ í˜¸ì¶œ
summary = summarize_article("Long article text here...")
print(summary)
```

**ëŒ€ì‹œë³´ë“œì—ì„œ ë³´ì´ëŠ” ë‚´ìš©:**
- ê·¸ë£¹ ì´ë¦„: `summarize_article`
- í¬í•¨ëœ í˜¸ì¶œ: 1ê°œ (GPT-4o-mini)
- í† í° ì‚¬ìš©ëŸ‰: ì´í•© í‘œì‹œ
- ì‹¤í–‰ ì‹œê°„: í•¨ìˆ˜ ì‹œì‘~ì¢…ë£Œ

### ì˜ˆì‹œ 2: ì´ë¦„ ìƒëµ (í•¨ìˆ˜ ì´ë¦„ ìë™ ì‚¬ìš©)

```python
@nora.trace_group  # ê´„í˜¸ ì—†ì´ ì§ì ‘ ì ìš©
def translate_text(text, target_lang):
    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": f"Translate to {target_lang}: {text}"}]
    )
    return response.choices[0].message.content

# ê·¸ë£¹ ì´ë¦„ì´ ìë™ìœ¼ë¡œ "translate_text"ê°€ ë©ë‹ˆë‹¤
```

### ì˜ˆì‹œ 3: ë©”íƒ€ë°ì´í„° ì¶”ê°€

```python
@nora.trace_group(
    name="user_query_handler",
    metadata={"user_id": "user_123", "session_id": "sess_456"}
)
def handle_user_query(user_id, query):
    """ì‚¬ìš©ì ì¿¼ë¦¬ ì²˜ë¦¬ - ë©”íƒ€ë°ì´í„°ë¡œ ì‚¬ìš©ì ì¶”ì """
    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": query}]
    )
    return response.choices[0].message.content

# ëŒ€ì‹œë³´ë“œì—ì„œ user_id, session_idë¡œ í•„í„°ë§ ê°€ëŠ¥
```

### ì˜ˆì‹œ 4: ë©€í‹° ì—ì´ì „íŠ¸ ì›Œí¬í”Œë¡œìš°

```python
@nora.trace_group(name="multi_agent_research")
def multi_agent_research(topic):
    """3ê°œì˜ AI ì—ì´ì „íŠ¸ê°€ í˜‘ì—…í•˜ëŠ” ë¦¬ì„œì¹˜ ì›Œí¬í”Œë¡œìš°"""
    
    # Agent 1: ì£¼ì œ ë¶„ì„
    analysis = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": f"Analyze topic: {topic}"}]
    ).choices[0].message.content
    
    # Agent 2: ì •ë³´ ìˆ˜ì§‘
    research = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": f"Research based on: {analysis}"}]
    ).choices[0].message.content
    
    # Agent 3: ìµœì¢… ìš”ì•½
    summary = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": f"Summarize: {research}"}]
    ).choices[0].message.content
    
    return summary

result = multi_agent_research("Quantum Computing")
```

**ëŒ€ì‹œë³´ë“œ í‘œì‹œ:**
```
ê·¸ë£¹: multi_agent_research
â”œâ”€ Execution 1: gpt-4o-mini (ë¶„ì„) - 120 tokens
â”œâ”€ Execution 2: gpt-4o-mini (ë¦¬ì„œì¹˜) - 450 tokens
â””â”€ Execution 3: gpt-4o-mini (ìš”ì•½) - 200 tokens
ì´ í† í°: 770 tokens
ì´ ì‹œê°„: 3.2ì´ˆ
```

### ì˜ˆì‹œ 5: ë¹„ë™ê¸° í•¨ìˆ˜ (Async)

```python
@nora.trace_group(name="async_translate")
async def async_translate(text):
    """ë¹„ë™ê¸° í•¨ìˆ˜ë„ ìë™ìœ¼ë¡œ ì²˜ë¦¬ë¨"""
    from openai import AsyncOpenAI
    
    async_client = AsyncOpenAI()
    response = await async_client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": f"Translate: {text}"}]
    )
    return response.choices[0].message.content

# ì‚¬ìš©
import asyncio
result = asyncio.run(async_translate("Hello world"))
```

### ì˜ˆì‹œ 6: ì œë„ˆë ˆì´í„° í•¨ìˆ˜

```python
@nora.trace_group(name="streaming_chat")
def streaming_chat(messages):
    """ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µë„ ì¶”ì  ê°€ëŠ¥"""
    stream = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=messages,
        stream=True
    )
    
    for chunk in stream:
        if chunk.choices[0].delta.content:
            yield chunk.choices[0].delta.content

# ì‚¬ìš©
for text_chunk in streaming_chat([{"role": "user", "content": "Tell me a story"}]):
    print(text_chunk, end="")
```

### ì˜ˆì‹œ 7: Tool Callingê³¼ í•¨ê»˜

```python
@nora.trace_group(name="weather_assistant")
def weather_assistant(city):
    """Tool callingì„ í¬í•¨í•œ ë³µì¡í•œ ì›Œí¬í”Œë¡œìš°"""
    
    tools = [{
        "type": "function",
        "function": {
            "name": "get_weather",
            "description": "Get weather information",
            "parameters": {
                "type": "object",
                "properties": {"location": {"type": "string"}}
            }
        }
    }]
    
    # 1ë‹¨ê³„: AIê°€ tool í˜¸ì¶œ ìš”ì²­
    response1 = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": f"What's the weather in {city}?"}],
        tools=tools
    )
    
    tool_call = response1.choices[0].message.tool_calls[0]
    
    # 2ë‹¨ê³„: Tool ì‹¤í–‰ (ì‹¤ì œ ë‚ ì”¨ API í˜¸ì¶œ)
    weather_data = get_weather(location=city)
    
    # 3ë‹¨ê³„: Tool ê²°ê³¼ë¡œ ìµœì¢… ë‹µë³€ ìƒì„±
    response2 = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {"role": "user", "content": f"What's the weather in {city}?"},
            response1.choices[0].message,
            {"role": "tool", "tool_call_id": tool_call.id, "content": weather_data}
        ]
    )
    
    return response2.choices[0].message.content

# ì „ì²´ í”Œë¡œìš°ê°€ í•˜ë‚˜ì˜ ê·¸ë£¹ìœ¼ë¡œ ì¶”ì ë¨
answer = weather_assistant("Seoul")
```

---

## with/async with context manager

`with` ë¬¸ì´ë‚˜ `async with` ë¬¸ì„ ì‚¬ìš©í•˜ì—¬ ì½”ë“œ ë¸”ë¡ ë‹¨ìœ„ë¡œ AI í˜¸ì¶œì„ ê·¸ë£¹í™”í•©ë‹ˆë‹¤.

### ê¸°ë³¸ ë¬¸ë²• (ë™ê¸°)

```python
with nora.trace_group(name="ê·¸ë£¹ì´ë¦„", metadata={"key": "value"}):
    # ì´ ë¸”ë¡ ë‚´ ëª¨ë“  AI í˜¸ì¶œì´ ê·¸ë£¹ìœ¼ë¡œ ë¬¶ì„
    response1 = client.chat.completions.create(...)
    response2 = client.chat.completions.create(...)
```

### ë¹„ë™ê¸° ë¬¸ë²•

```python
async with nora.trace_group(name="ê·¸ë£¹ì´ë¦„"):
    # ë¹„ë™ê¸° í˜¸ì¶œë„ ì§€ì›
    response1 = await async_client.chat.completions.create(...)
    response2 = await async_client.chat.completions.create(...)
```

### ì˜ˆì‹œ 1: ê¸°ë³¸ with ì‚¬ìš©

```python
import nora
from openai import OpenAI

nora.init(api_key="your-key")
client = OpenAI()

# ì‚¬ìš©ì ìš”ì²­ ì²˜ë¦¬ë¥¼ í•˜ë‚˜ì˜ ê·¸ë£¹ìœ¼ë¡œ ë¬¶ê¸°
with nora.trace_group(name="user_request_handler"):
    # ì˜ë„ ë¶„ì„
    intent = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": "I want to book a flight to Paris"}]
    ).choices[0].message.content
    
    # ì„¸ë¶€ ì •ë³´ ì¶”ì¶œ
    details = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": f"Extract flight details from: {intent}"}]
    ).choices[0].message.content
    
    print(f"Intent: {intent}")
    print(f"Details: {details}")
```

**ëŒ€ì‹œë³´ë“œ í‘œì‹œ:**
```
ê·¸ë£¹: user_request_handler
â”œâ”€ gpt-4o-mini (ì˜ë„ ë¶„ì„) - 0.8ì´ˆ, 95 tokens
â””â”€ gpt-4o-mini (ì„¸ë¶€ ì •ë³´) - 1.1ì´ˆ, 120 tokens
ì´ ì‹¤í–‰ ì‹œê°„: 1.9ì´ˆ
ì´ í† í°: 215 tokens
ìƒíƒœ: success âœ“
```

### ì˜ˆì‹œ 2: ë©”íƒ€ë°ì´í„°ì™€ í•¨ê»˜

```python
user_id = "user_12345"
session_id = "sess_abc"

with nora.trace_group(
    name="chat_conversation",
    metadata={
        "user_id": user_id,
        "session_id": session_id,
        "input": "How do I reset my password?"
    }
):
    # ëŒ€í™” ì´ë ¥ í¬í•¨í•œ ì‘ë‹µ ìƒì„±
    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {"role": "system", "content": "You are a helpful support agent."},
            {"role": "user", "content": "How do I reset my password?"}
        ]
    )
    
    answer = response.choices[0].message.content
    print(answer)

# metadataë¡œ ëŒ€ì‹œë³´ë“œì—ì„œ ê²€ìƒ‰/í•„í„° ê°€ëŠ¥
```

### ì˜ˆì‹œ 3: ì—ëŸ¬ ì²˜ë¦¬

```python
try:
    with nora.trace_group(name="risky_operation"):
        # ì—ëŸ¬ê°€ ë°œìƒí•  ìˆ˜ ìˆëŠ” AI í˜¸ì¶œ
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": "Some query"}]
        )
except Exception as e:
    print(f"Error occurred: {e}")
    # traceëŠ” ìë™ìœ¼ë¡œ status="error"ë¡œ ê¸°ë¡ë¨
```

**ëŒ€ì‹œë³´ë“œ í‘œì‹œ (ì—ëŸ¬ ì‹œ):**
```
ê·¸ë£¹: risky_operation
ìƒíƒœ: error âœ—
ì—ëŸ¬: RateLimitError: Too many requests
ì‹¤í–‰ ì‹œê°„: 0.5ì´ˆ
```

### ì˜ˆì‹œ 4: ì¤‘ì²© ê·¸ë£¹ (Nested Groups)

```python
# ì™¸ë¶€ ê·¸ë£¹: ì „ì²´ ì›Œí¬í”Œë¡œìš°
with nora.trace_group(name="document_processing"):
    
    # ë‚´ë¶€ ê·¸ë£¹ 1: ë¬¸ì„œ ë¶„ì„
    with nora.trace_group(name="analyze_document"):
        analysis = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": "Analyze this document..."}]
        ).choices[0].message.content
    
    # ë‚´ë¶€ ê·¸ë£¹ 2: ìš”ì•½ ìƒì„±
    with nora.trace_group(name="generate_summary"):
        summary = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": f"Summarize: {analysis}"}]
        ).choices[0].message.content
    
    print(summary)
```

**ëŒ€ì‹œë³´ë“œ ê³„ì¸µ êµ¬ì¡°:**
```
document_processing (ë¶€ëª¨ ê·¸ë£¹)
â”œâ”€ analyze_document (ìì‹ ê·¸ë£¹)
â”‚  â””â”€ gpt-4o-mini - 1.2ì´ˆ, 200 tokens
â””â”€ generate_summary (ìì‹ ê·¸ë£¹)
   â””â”€ gpt-4o-mini - 0.9ì´ˆ, 150 tokens
```

### ì˜ˆì‹œ 5: async with (ë¹„ë™ê¸°)

```python
from openai import AsyncOpenAI
import asyncio

async_client = AsyncOpenAI()

async def async_workflow():
    async with nora.trace_group(name="async_multi_query"):
        # ì—¬ëŸ¬ ë¹„ë™ê¸° í˜¸ì¶œì„ ë³‘ë ¬ë¡œ ì‹¤í–‰
        task1 = async_client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": "Query 1"}]
        )
        
        task2 = async_client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": "Query 2"}]
        )
        
        # ë™ì‹œ ì‹¤í–‰
        responses = await asyncio.gather(task1, task2)
        
        return [r.choices[0].message.content for r in responses]

# ì‹¤í–‰
results = asyncio.run(async_workflow())
print(results)
```

### ì˜ˆì‹œ 6: ë°˜ë³µë¬¸ì—ì„œ ì‚¬ìš©

```python
queries = ["What is AI?", "Explain ML", "Define DL"]

with nora.trace_group(name="batch_queries", metadata={"count": len(queries)}):
    results = []
    
    for i, query in enumerate(queries):
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": query}]
        )
        results.append(response.choices[0].message.content)
        print(f"Query {i+1} done")
    
    # 3ê°œì˜ AI í˜¸ì¶œì´ ëª¨ë‘ í•˜ë‚˜ì˜ ê·¸ë£¹ìœ¼ë¡œ ì¶”ì ë¨
```

**ëŒ€ì‹œë³´ë“œ í‘œì‹œ:**
```
ê·¸ë£¹: batch_queries
ë©”íƒ€ë°ì´í„°: count=3
â”œâ”€ gpt-4o-mini (Query 1) - 0.7ì´ˆ, 80 tokens
â”œâ”€ gpt-4o-mini (Query 2) - 0.8ì´ˆ, 95 tokens
â””â”€ gpt-4o-mini (Query 3) - 0.6ì´ˆ, 70 tokens
ì´ í† í°: 245 tokens
ì´ ì‹œê°„: 2.1ì´ˆ
```

### ì˜ˆì‹œ 7: Tool Calling ì „ì²´ í”Œë¡œìš°

```python
tools = [{
    "type": "function",
    "function": {
        "name": "get_weather",
        "description": "Get current weather",
        "parameters": {
            "type": "object",
            "properties": {"location": {"type": "string"}}
        }
    }
}]

def get_weather(location):
    # ì‹¤ì œ API í˜¸ì¶œ
    return f"Weather in {location}: Sunny, 22Â°C"

with nora.trace_group(name="weather_query", metadata={"input": "Seoul weather"}):
    # 1. AIê°€ tool í˜¸ì¶œ ê²°ì •
    response1 = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": "What's the weather in Seoul?"}],
        tools=tools,
        tool_choice="auto"
    )
    
    tool_calls = response1.choices[0].message.tool_calls
    
    if tool_calls:
        # 2. Tool ì‹¤í–‰
        tool_results = []
        for tool_call in tool_calls:
            func_name = tool_call.function.name
            func_args = json.loads(tool_call.function.arguments)
            result = get_weather(**func_args)
            tool_results.append({
                "role": "tool",
                "tool_call_id": tool_call.id,
                "name": func_name,
                "content": result
            })
        
        # 3. Tool ê²°ê³¼ë¡œ ìµœì¢… ë‹µë³€
        messages = [
            {"role": "user", "content": "What's the weather in Seoul?"},
            response1.choices[0].message.model_dump(),
            *tool_results
        ]
        
        response2 = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=messages
        )
        
        final_answer = response2.choices[0].message.content
        print(final_answer)
```

**ëŒ€ì‹œë³´ë“œ í‘œì‹œ:**
```
ê·¸ë£¹: weather_query
ì…ë ¥: "Seoul weather"
â”œâ”€ gpt-4o-mini (Tool ìš”ì²­) - 0.5ì´ˆ, 50 tokens
â”‚  â””â”€ Tool: get_weather(location="Seoul")
â””â”€ gpt-4o-mini (ìµœì¢… ë‹µë³€) - 0.7ì´ˆ, 45 tokens
ì´ ì‹¤í–‰ ì‹œê°„: 1.2ì´ˆ
ì´ í† í°: 95 tokens
ìƒíƒœ: success âœ“
ì¶œë ¥: "The weather in Seoul is sunny with a temperature of 22Â°C."
```

---

## ê³ ê¸‰ ì‚¬ìš©ë²•

### í”„ë¡œê·¸ë¨ ì¢…ë£Œ ì‹œ ë°ì´í„° í”ŒëŸ¬ì‹œ

```python
import nora
import atexit

nora.init(api_key="your-key")

# í”„ë¡œê·¸ë¨ ì¢…ë£Œ ì‹œ ìë™ìœ¼ë¡œ ë‚¨ì€ trace ì „ì†¡
atexit.register(lambda: nora.flush(sync=True))
```

### Trace ì¼ì‹œ ì¤‘ì§€/ì¬ê°œ

```python
nora.init(api_key="your-key")

# ì¼ì‹œì ìœ¼ë¡œ trace ë¹„í™œì„±í™”
nora.disable()

# ì´ í˜¸ì¶œì€ ì¶”ì ë˜ì§€ ì•ŠìŒ
response = client.chat.completions.create(...)

# ë‹¤ì‹œ í™œì„±í™”
nora.enable()

# ì´ì œ ë‹¤ì‹œ ì¶”ì ë¨
response = client.chat.completions.create(...)
```

### Trace ë°ì´í„° ì¡°íšŒ

```python
# ëª¨ë“  trace group ì¡°íšŒ
groups = nora.get_trace_groups()
for group in groups:
    print(f"Group: {group['name']}")
    print(f"  Traces: {group['trace_count']}")
    print(f"  Tokens: {group['total_tokens']}")
    print(f"  Duration: {group['total_duration']}s")

# íŠ¹ì • ê·¸ë£¹ì˜ trace ì°¾ê¸°
traces = nora.find_traces_by_group("multi_agent_workflow")
for trace in traces:
    print(f"Model: {trace['model']}, Tokens: {trace['tokens_used']}")

# ê·¸ë£¹ IDë¡œ ê²€ìƒ‰
traces = nora.find_traces_by_group_id("group-uuid-here")
```

---

## API ì‘ë‹µ í˜•ì‹

### Trace ìƒì„± ì‘ë‹µ

**POST** `/v1/traces/`

```json
{
  "id": "44bcdebf-4418-44c2-ad78-9dd19b990e2f",
  "trace_name": "multi_agent_workflow",
  "input": "ì‚¬ìš©ì ìš”ì²­ í…ìŠ¤íŠ¸",
  "output": null,
  "latency": null,
  "tokens": null,
  "cost": null,
  "status": "pending",
  "environment": "production",
  "created_at": "2025-12-16T04:15:30.123Z",
  "updated_at": "2025-12-16T04:15:30.123Z"
}
```

### Execution Span ìƒì„± ì‘ë‹µ

**POST** `/v1/executions/`

```json
{
  "id": "exec-uuid-123",
  "trace_id": "44bcdebf-4418-44c2-ad78-9dd19b990e2f",
  "span_name": "openai_gpt-4o-mini",
  "span_data": {
    "id": "chatcmpl-xyz",
    "timestamp": "2025-12-16T04:15:31.456Z",
    "provider": "openai",
    "model": "gpt-4o-mini",
    "prompt": "User: Hello, how are you?",
    "response": "Assistant: I'm doing well, thank you!",
    "metadata": {
      "trace_group": {
        "id": "group-uuid-456",
        "name": "multi_agent_workflow"
      }
    },
    "start_time": 1734324931.123,
    "end_time": 1734324932.456,
    "duration": 1.333,
    "tokens_used": 25,
    "finish_reason": "stop",
    "response_id": "chatcmpl-xyz",
    "system_fingerprint": "fp_abc123",
    "tool_calls": null,
    "environment": "production"
  },
  "created_at": "2025-12-16T04:15:32.789Z"
}
```

### Trace ì—…ë°ì´íŠ¸ ì‘ë‹µ

**PATCH** `/v1/traces/{trace_id}`

```json
{
  "id": "44bcdebf-4418-44c2-ad78-9dd19b990e2f",
  "trace_name": "multi_agent_workflow",
  "input": "ì‚¬ìš©ì ìš”ì²­ í…ìŠ¤íŠ¸",
  "output": "ìµœì¢… AI ì‘ë‹µ ê²°ê³¼",
  "latency": 3.245,
  "tokens": {
    "total_tokens": 150,
    "prompt_tokens": 80,
    "completion_tokens": 70
  },
  "cost": 0.00045,
  "status": "success",
  "environment": "production",
  "created_at": "2025-12-16T04:15:30.123Z",
  "updated_at": "2025-12-16T04:15:33.456Z"
}
```

### OpenAI ì‘ë‹µ ì˜ˆì‹œ (ìë™ ì¶”ì )

```json
{
  "id": "chatcmpl-CnGgzl3lTNCnFJdhxUUdKcU9k3u1N",
  "choices": [
    {
      "finish_reason": "tool_calls",
      "index": 0,
      "message": {
        "content": null,
        "role": "assistant",
        "tool_calls": [
          {
            "id": "call_Gu6m4p8lTxey883A6lGaXeWP",
            "function": {
              "arguments": "{\"location\": \"Seoul, South Korea\"}",
              "name": "get_weather"
            },
            "type": "function"
          },
          {
            "id": "call_N0P1bFGIdpaEylgS35rSkb37",
            "function": {
              "arguments": "{\"location\": \"Tokyo, Japan\"}",
              "name": "get_weather"
            },
            "type": "function"
          }
        ]
      }
    }
  ],
  "created": 1765858273,
  "model": "gpt-4o-mini-2024-07-18",
  "object": "chat.completion",
  "service_tier": "default",
  "system_fingerprint": "fp_11f3029f6b",
  "usage": {
    "completion_tokens": 50,
    "prompt_tokens": 80,
    "total_tokens": 130,
    "completion_tokens_details": {
      "accepted_prediction_tokens": 0,
      "audio_tokens": 0,
      "reasoning_tokens": 0,
      "rejected_prediction_tokens": 0
    },
    "prompt_tokens_details": {
      "audio_tokens": 0,
      "cached_tokens": 0
    }
  }
}
```

**ìœ„ ì‘ë‹µì´ Noraë¡œ ì „ì†¡ë˜ëŠ” í˜•ì‹:**

```json
{
  "trace_id": "parent-trace-uuid",
  "span_name": "openai_gpt-4o-mini",
  "span_data": {
    "id": "chatcmpl-CnGgzl3lTNCnFJdhxUUdKcU9k3u1N",
    "provider": "openai",
    "model": "gpt-4o-mini-2024-07-18",
    "prompt": "User: What's the weather in Seoul and Tokyo?",
    "response": null,
    "tokens_used": 130,
    "finish_reason": "tool_calls",
    "tool_calls": [
      {
        "id": "call_Gu6m4p8lTxey883A6lGaXeWP",
        "function": {
          "name": "get_weather",
          "arguments": "{\"location\": \"Seoul, South Korea\"}"
        }
      },
      {
        "id": "call_N0P1bFGIdpaEylgS35rSkb37",
        "function": {
          "name": "get_weather",
          "arguments": "{\"location\": \"Tokyo, Japan\"}"
        }
      }
    ],
    "metadata": {
      "usage": {
        "total_tokens": 130,
        "prompt_tokens": 80,
        "completion_tokens": 50
      }
    },
    "duration": 1.234,
    "environment": "production"
  }
}
```

### ì—ëŸ¬ ì‘ë‹µ ì˜ˆì‹œ

```json
{
  "detail": "Unauthorized"
}
```

**HTTP ìƒíƒœ ì½”ë“œ:**
- `200`: ì„±ê³µ
- `201`: ìƒì„± ì„±ê³µ
- `401`: ì¸ì¦ ì‹¤íŒ¨ (ì˜ëª»ëœ API í‚¤)
- `400`: ì˜ëª»ëœ ìš”ì²­
- `500`: ì„œë²„ ì—ëŸ¬

---

## ì§€ì›í•˜ëŠ” AI ë¼ì´ë¸ŒëŸ¬ë¦¬

### OpenAI
- âœ… Chat Completions API (`gpt-4`, `gpt-4o`, `gpt-4o-mini`, `gpt-3.5-turbo` ë“±)
- âœ… Tool/Function Calling
- âœ… Streaming ì‘ë‹µ
- âœ… ë™ê¸°/ë¹„ë™ê¸° (`AsyncOpenAI`)
- âœ… í† í° ì‚¬ìš©ëŸ‰ ì¶”ì  (reasoning tokens, cached tokens í¬í•¨)
- âœ… ë¹„ìš© ê³„ì‚°

**ì§€ì› ë©”ì„œë“œ:**
```python
client.chat.completions.create(...)  # ë™ê¸°
await async_client.chat.completions.create(...)  # ë¹„ë™ê¸°
```

### Anthropic (Claude)
- âœ… Messages API (`claude-3-5-sonnet`, `claude-opus-4-5`, ë“±)
- âœ… Tool Calling
- âœ… Streaming ì‘ë‹µ
- âœ… ë™ê¸°/ë¹„ë™ê¸°
- âœ… í† í° ì‚¬ìš©ëŸ‰ ì¶”ì 

**ì§€ì› ë©”ì„œë“œ:**
```python
client.messages.create(...)  # ë™ê¸°
await async_client.messages.create(...)  # ë¹„ë™ê¸°
```

### Google Gemini
- âœ… Generate Content API (`gemini-2.0-flash`, `gemini-pro` ë“±)
- âœ… Tool Calling
- âœ… ë™ê¸°/ë¹„ë™ê¸°
- âœ… í† í° ì‚¬ìš©ëŸ‰ ì¶”ì 

**ì§€ì› ë©”ì„œë“œ:**
```python
client.models.generate_content(...)  # ë™ê¸°
await client.aio.models.generate_content(...)  # ë¹„ë™ê¸°
```

---

## ì‹¤ì „ ì˜ˆì‹œ ëª¨ìŒ

### ì˜ˆì‹œ 1: RAG (Retrieval-Augmented Generation)

```python
import nora
from openai import OpenAI

nora.init(api_key="your-key")
client = OpenAI()

@nora.trace_group(name="rag_query")
def rag_query(user_question, context_docs):
    """RAG íŒŒì´í”„ë¼ì¸ ì „ì²´ë¥¼ í•˜ë‚˜ì˜ ê·¸ë£¹ìœ¼ë¡œ ì¶”ì """
    
    # 1. ê´€ë ¨ ë¬¸ì„œ ì„ë² ë”© ë° ê²€ìƒ‰ (ì™¸ë¶€ ë¡œì§)
    relevant_context = "\n".join(context_docs)
    
    # 2. ì»¨í…ìŠ¤íŠ¸ í¬í•¨ í”„ë¡¬í”„íŠ¸ ìƒì„±
    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {"role": "system", "content": "Answer based on the context."},
            {"role": "user", "content": f"Context:\n{relevant_context}\n\nQuestion: {user_question}"}
        ]
    )
    
    return response.choices[0].message.content

answer = rag_query(
    "What is machine learning?",
    ["Doc1: ML is...", "Doc2: AI involves..."]
)
```

### ì˜ˆì‹œ 2: ë©€í‹° ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œ

```python
@nora.trace_group(name="multi_agent_system")
async def multi_agent_workflow(task):
    """3ê°œ ì—ì´ì „íŠ¸ê°€ ìˆœì°¨ì ìœ¼ë¡œ ì‘ì—…"""
    from openai import AsyncOpenAI
    
    async_client = AsyncOpenAI()
    
    # Agent 1: Planner
    plan = await async_client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": f"Create a plan for: {task}"}]
    )
    plan_text = plan.choices[0].message.content
    
    # Agent 2: Executor
    execution = await async_client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": f"Execute plan: {plan_text}"}]
    )
    execution_result = execution.choices[0].message.content
    
    # Agent 3: Reviewer
    review = await async_client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": f"Review result: {execution_result}"}]
    )
    
    return review.choices[0].message.content

import asyncio
result = asyncio.run(multi_agent_workflow("Build a website"))
```

### ì˜ˆì‹œ 3: ë°°ì¹˜ ì²˜ë¦¬

```python
documents = ["Doc 1 text...", "Doc 2 text...", "Doc 3 text..."]

with nora.trace_group(name="batch_summarization", metadata={"count": len(documents)}):
    summaries = []
    
    for i, doc in enumerate(documents):
        summary = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": f"Summarize: {doc}"}]
        ).choices[0].message.content
        
        summaries.append(summary)
        print(f"Processed {i+1}/{len(documents)}")
    
    # ëª¨ë“  ìš”ì•½ì„ í•˜ë‚˜ì˜ ê·¸ë£¹ìœ¼ë¡œ ì¶”ì 
```

### ì˜ˆì‹œ 4: A/B í…ŒìŠ¤íŠ¸

```python
import random

user_query = "Explain photosynthesis"

# ëœë¤ìœ¼ë¡œ ëª¨ë¸ ì„ íƒ
model = random.choice(["gpt-4o-mini", "gpt-4o"])

with nora.trace_group(
    name="ab_test",
    metadata={"model": model, "variant": "A" if model == "gpt-4o-mini" else "B"}
):
    response = client.chat.completions.create(
        model=model,
        messages=[{"role": "user", "content": user_query}]
    )
    
    print(f"Model: {model}")
    print(response.choices[0].message.content)

# ëŒ€ì‹œë³´ë“œì—ì„œ variantë³„ ì„±ëŠ¥ ë¹„êµ ê°€ëŠ¥
```

### ì˜ˆì‹œ 5: ì—ëŸ¬ ì¬ì‹œë„ ë¡œì§

```python
from tenacity import retry, stop_after_attempt, wait_exponential

@retry(stop=stop_after_attempt(3), wait=wait_exponential(min=1, max=10))
@nora.trace_group(name="robust_query")
def robust_ai_call(prompt):
    """ì¬ì‹œë„ ë¡œì§ í¬í•¨ - ê° ì‹œë„ê°€ ëª¨ë‘ ì¶”ì ë¨"""
    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": prompt}]
    )
    return response.choices[0].message.content

try:
    result = robust_ai_call("Tell me about quantum physics")
except Exception as e:
    print(f"Failed after retries: {e}")
```

---

## íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### ë¬¸ì œ: Traceê°€ ëŒ€ì‹œë³´ë“œì— í‘œì‹œë˜ì§€ ì•ŠìŒ

**í•´ê²° ë°©ë²•:**
1. API í‚¤ í™•ì¸
   ```python
   nora.init(api_key="ì˜¬ë°”ë¥¸_í‚¤")
   ```

2. ë„¤íŠ¸ì›Œí¬ ì—°ê²° í™•ì¸
   ```python
   import requests
   response = requests.get("https://noraobservabilitybackend-staging.up.railway.app/v1/health")
   print(response.status_code)  # 200ì´ì–´ì•¼ í•¨
   ```

3. ìˆ˜ë™ìœ¼ë¡œ flush
   ```python
   nora.flush(sync=True)
   ```

### ë¬¸ì œ: 401 Unauthorized ì—ëŸ¬

**ì›ì¸:** ì˜ëª»ëœ API í‚¤

**í•´ê²°:**
```python
# í™˜ê²½ ë³€ìˆ˜ì—ì„œ ë¡œë“œ
import os
nora.init(api_key=os.getenv("NORA_API_KEY"))
```

### ë¬¸ì œ: Tool callsê°€ ì¶”ì ë˜ì§€ ì•ŠìŒ

**í™•ì¸ ì‚¬í•­:**
1. `trace_group` ì•ˆì—ì„œ í˜¸ì¶œí–ˆëŠ”ì§€ í™•ì¸
2. Tool ì‘ë‹µì„ ì˜¬ë°”ë¥¸ í˜•ì‹ìœ¼ë¡œ ì „ë‹¬í–ˆëŠ”ì§€ í™•ì¸

```python
with nora.trace_group(name="tool_workflow"):
    # 1. Tool call ìš”ì²­
    response1 = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[...],
        tools=[...]
    )
    
    # 2. Tool ì‹¤í–‰
    tool_result = execute_tool(...)
    
    # 3. ê²°ê³¼ ì „ë‹¬ (ì˜¬ë°”ë¥¸ í˜•ì‹)
    response2 = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            ...,
            response1.choices[0].message.model_dump(),  # ì¤‘ìš”!
            {"role": "tool", "tool_call_id": "...", "content": tool_result}
        ]
    )
```

### ë¬¸ì œ: ë¹„ë™ê¸° í•¨ìˆ˜ì—ì„œ ì‘ë™í•˜ì§€ ì•ŠìŒ

**í•´ê²°:**
```python
# async with ì‚¬ìš©
async with nora.trace_group(name="async_workflow"):
    response = await async_client.chat.completions.create(...)

# ë˜ëŠ” ë°ì½”ë ˆì´í„°
@nora.trace_group(name="async_func")
async def my_async_function():
    ...
```

---

## ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤

### 1. í™˜ê²½ ë³€ìˆ˜ë¡œ API í‚¤ ê´€ë¦¬

```python
# .env íŒŒì¼
NORA_API_KEY=your-api-key-here
OPENAI_API_KEY=your-openai-key
ANTHROPIC_API_KEY=your-anthropic-key

# ì½”ë“œ
from dotenv import load_dotenv
import os

load_dotenv()
nora.init(api_key=os.getenv("NORA_API_KEY"))
```

### 2. ì˜ë¯¸ ìˆëŠ” ê·¸ë£¹ ì´ë¦„ ì‚¬ìš©

```python
# ë‚˜ìœ ì˜ˆ
with nora.trace_group(name="func1"):
    ...

# ì¢‹ì€ ì˜ˆ
with nora.trace_group(name="user_onboarding_email_generation"):
    ...
```

### 3. ë©”íƒ€ë°ì´í„° í™œìš©

```python
with nora.trace_group(
    name="customer_support",
    metadata={
        "user_id": user.id,
        "ticket_id": ticket.id,
        "priority": ticket.priority,
        "category": ticket.category
    }
):
    response = generate_support_response(ticket)
```

### 4. í”„ë¡œë•ì…˜ í™˜ê²½ êµ¬ë¶„

```python
import os

env = os.getenv("ENVIRONMENT", "development")
nora.init(
    api_key=os.getenv("NORA_API_KEY"),
    environment=env
)
```

### 5. ì—ëŸ¬ í•¸ë“¤ë§

```python
try:
    with nora.trace_group(name="critical_operation"):
        result = client.chat.completions.create(...)
except Exception as e:
    # TraceëŠ” ìë™ìœ¼ë¡œ error ìƒíƒœë¡œ ê¸°ë¡ë¨
    logger.error(f"AI call failed: {e}")
    # Fallback ë¡œì§
```

---

## í”„ë¡œì íŠ¸ êµ¬ì¡°

```
nora/
â”œâ”€â”€ __init__.py              # ë©”ì¸ API (init, trace_group, tool ë“±)
â”œâ”€â”€ client.py                # NoraClient ë° TraceGroup êµ¬í˜„
â”œâ”€â”€ utils.py                 # ê³µí†µ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
â”œâ”€â”€ openai/                  # OpenAI ì „ìš© ëª¨ë“ˆ
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ types.py             # íƒ€ì… ì •ì˜
â”‚   â”œâ”€â”€ utils.py             # ì‘ë‹µ íŒŒì‹± ìœ í‹¸ë¦¬í‹°
â”‚   â”œâ”€â”€ metadata_builder.py  # Trace ë©”íƒ€ë°ì´í„° êµ¬ì„±
â”‚   â”œâ”€â”€ tool_tracer.py       # Tool ì‹¤í–‰ ìë™ ê°ì§€
â”‚   â””â”€â”€ streaming.py         # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì²˜ë¦¬
â”œâ”€â”€ anthropic/               # Anthropic ì „ìš© ëª¨ë“ˆ
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ types.py
â”‚   â”œâ”€â”€ utils.py
â”‚   â”œâ”€â”€ metadata_builder.py
â”‚   â””â”€â”€ streaming.py
â”œâ”€â”€ gemini/                  # Gemini ì „ìš© ëª¨ë“ˆ
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ types.py
â”‚   â”œâ”€â”€ utils.py
â”‚   â”œâ”€â”€ metadata_builder.py
â”‚   â””â”€â”€ streaming.py
â””â”€â”€ patches/                 # AI ë¼ì´ë¸ŒëŸ¬ë¦¬ íŒ¨ì¹˜
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ openai_patch.py      # OpenAI API íŒ¨ì¹˜
    â”œâ”€â”€ anthropic_patch.py   # Anthropic API íŒ¨ì¹˜
    â””â”€â”€ gemini_patch.py      # Gemini API íŒ¨ì¹˜
```

## ê°œë°œ

### ì˜ì¡´ì„± ì„¤ì¹˜

```bash
# ê¸°ë³¸ ì„¤ì¹˜
pip install nora-observability

# ê°œë°œ í™˜ê²½ ì„¤ì¹˜
pip install -e ".[dev]"
```

### í…ŒìŠ¤íŠ¸ ì‹¤í–‰

```bash
# ì „ì²´ í…ŒìŠ¤íŠ¸
pytest tests/ -v

# íŠ¹ì • í…ŒìŠ¤íŠ¸ë§Œ
pytest tests/test_decorator_and_with.py -v

# ì»¤ë²„ë¦¬ì§€ í¬í•¨
pytest tests/ --cov=nora --cov-report=html
```

### ì½”ë“œ í’ˆì§ˆ

```bash
# í¬ë§·íŒ…
black nora/

# Linting
ruff check nora/

# íƒ€ì… ì²´í¬
mypy nora/
```

## ì•„í‚¤í…ì²˜

### ìë™ íŒ¨ì¹˜ ë©”ì»¤ë‹ˆì¦˜

Nora SDKëŠ” monkey-patchingì„ ì‚¬ìš©í•˜ì—¬ AI ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ íˆ¬ëª…í•˜ê²Œ ê°€ë¡œì±•ë‹ˆë‹¤:

1. **ì´ˆê¸°í™”**: `nora.init()` í˜¸ì¶œ ì‹œ `patches/` ëª¨ë“ˆì˜ íŒ¨ì¹˜ í•¨ìˆ˜ë“¤ì´ ì‹¤í–‰ë¨
2. **ë©”ì„œë“œ ë˜í•‘**: ì›ë³¸ API ë©”ì„œë“œ(`chat.completions.create` ë“±)ë¥¼ ë˜í¼ í•¨ìˆ˜ë¡œ êµì²´
3. **ìš”ì²­ ì¸í„°ì…‰íŠ¸**: API í˜¸ì¶œ ì‹œì ì— ìš”ì²­ íŒŒë¼ë¯¸í„° ìˆ˜ì§‘ (model, messages, tools ë“±)
4. **ì‘ë‹µ ì²˜ë¦¬**: ì‘ë‹µì—ì„œ í…ìŠ¤íŠ¸, tool calls, í† í° ì‚¬ìš©ëŸ‰ ì¶”ì¶œ
5. **Trace ìƒì„±**: `NoraClient.trace()` ë©”ì„œë“œë¡œ trace ë°ì´í„° ìƒì„±
6. **ë°°ì¹˜ ì „ì†¡**: ë°±ê·¸ë¼ìš´ë“œ ìŠ¤ë ˆë“œì—ì„œ ë°°ì¹˜ë¡œ ì„œë²„ì— ì „ì†¡

**íŒ¨ì¹˜ ì˜ˆì‹œ (ê°„ì†Œí™”):**
```python
# patches/openai_patch.py
original_create = openai.chat.completions.create

def patched_create(*args, **kwargs):
    start_time = time.time()
    response = original_create(*args, **kwargs)  # ì›ë³¸ í˜¸ì¶œ
    end_time = time.time()
    
    # Trace ë°ì´í„° ìˆ˜ì§‘
    client = get_nora_client()
    client.trace(
        provider="openai",
        model=kwargs.get("model"),
        prompt=format_messages(kwargs.get("messages")),
        response=response.choices[0].message.content,
        tokens_used=response.usage.total_tokens,
        start_time=start_time,
        end_time=end_time
    )
    
    return response

# íŒ¨ì¹˜ ì ìš©
openai.chat.completions.create = patched_create
```

### TraceGroup ë™ì‘ ì›ë¦¬

`TraceGroup`ì€ ì»¨í…ìŠ¤íŠ¸ ê´€ë¦¬ìë¡œ, ë¸”ë¡ ë‚´ì˜ ëª¨ë“  traceë¥¼ í•˜ë‚˜ë¡œ ë¬¶ìŠµë‹ˆë‹¤:

1. **ì§„ì…** (`__enter__`):
   - ê³ ìœ  `group_id` ìƒì„±
   - ë°±ì—”ë“œì— pending trace ìƒì„± â†’ `trace_id` ë°›ìŒ
   - í˜„ì¬ ê·¸ë£¹ì„ `ContextVar`ì— ì €ì¥
   - ìë™ flush ë¹„í™œì„±í™”

2. **ì‹¤í–‰ ì¤‘**:
   - AI í˜¸ì¶œë§ˆë‹¤ `client.trace()` ì‹¤í–‰
   - í˜„ì¬ ê·¸ë£¹ í™•ì¸ â†’ trace ë°ì´í„°ì— `trace_group` ë©”íƒ€ë°ì´í„° ì¶”ê°€
   - `_send_execution_span()`ìœ¼ë¡œ ì¦‰ì‹œ ë°±ì—”ë“œì— ì „ì†¡

3. **ì¢…ë£Œ** (`__exit__`):
   - ê·¸ë£¹ ë‚´ ëª¨ë“  trace ì§‘ê³„ (í† í°, ë¹„ìš©, ì¶œë ¥)
   - ë°±ì—”ë“œ traceë¥¼ `success` ë˜ëŠ” `error` ìƒíƒœë¡œ ì—…ë°ì´íŠ¸
   - ìë™ flush ì¬ê°œ

**í”Œë¡œìš° ë‹¤ì´ì–´ê·¸ë¨:**
```
nora.init()
    â†“
[User Code]
    â†“
with trace_group("my_group"):  â† __enter__: POST /traces/ (pending)
    â†“
    AI Call 1  â†’ client.trace() â†’ POST /executions/ (span 1)
    â†“
    AI Call 2  â†’ client.trace() â†’ POST /executions/ (span 2)
    â†“
                                â† __exit__: PATCH /traces/{id} (success)
```

### ë¹„ë™ê¸° ì²˜ë¦¬

- **ìŠ¤ë ˆë“œ ì‚¬ìš©**: Trace ì „ì†¡ì€ ë³„ë„ daemon ìŠ¤ë ˆë“œì—ì„œ ë¹„ë™ê¸° ì‹¤í–‰
- **ë¸”ë¡œí‚¹ ë°©ì§€**: ë©”ì¸ ìŠ¤ë ˆë“œëŠ” AI ì‘ë‹µ ëŒ€ê¸°ë§Œ í•˜ê³ , trace ì „ì†¡ì€ ë°±ê·¸ë¼ìš´ë“œ
- **ì—ëŸ¬ ê²©ë¦¬**: Trace ì „ì†¡ ì‹¤íŒ¨ ì‹œ ë©”ì¸ ì½”ë“œì— ì˜í–¥ ì—†ìŒ

```python
def _send_execution_span(trace_id, span_data):
    def _send():
        try:
            requests.post(url, json=payload)
        except Exception as e:
            print(f"Warning: {e}")
    
    thread = threading.Thread(target=_send, daemon=True)
    thread.start()  # ë¹„ë™ê¸° ì „ì†¡
```

---

## ë¼ì´ì„ ìŠ¤

MIT License

---

## ê¸°ì—¬í•˜ê¸°

ì´ìŠˆ ë° PRì€ GitHubì—ì„œ í™˜ì˜í•©ë‹ˆë‹¤:
- Repository: `Kr-TeamWise/observability_sdk`
- Issues: [GitHub Issues](https://github.com/Kr-TeamWise/observability_sdk/issues)

---

## ë³€ê²½ ë¡œê·¸

### v1.0.19 (2025-12-16)
- âœ¨ Execution span ì¦‰ì‹œ ì „ì†¡ ê¸°ëŠ¥ ì¶”ê°€
- ğŸ› Token aggregation ë²„ê·¸ ìˆ˜ì • (None ì²˜ë¦¬)
- ğŸ§ª í…ŒìŠ¤íŠ¸ ìŠ¤ìœ„íŠ¸ ì „ì²´ ì¬ì‘ì„±
- ğŸ“ README ëŒ€í­ ì—…ë°ì´íŠ¸

### v1.0.17 (2024-12-14)
- âœ¨ Environment íŒŒë¼ë¯¸í„° ì¶”ê°€
- ğŸ› Trace group ë©”íƒ€ë°ì´í„° ê°œì„ 

### v1.0.16 (2024-12-13)
- âœ¨ Service URL ë“±ë¡ ê¸°ëŠ¥
- ğŸ› ë²„ê·¸ ìˆ˜ì •

---

## ì§€ì›

- ğŸ“§ ì´ë©”ì¼: support@nora.ai
- ğŸ’¬ Discord: [Nora Community](https://discord.gg/nora)
- ğŸ“š ë¬¸ì„œ: [docs.nora.ai](https://docs.nora.ai)

