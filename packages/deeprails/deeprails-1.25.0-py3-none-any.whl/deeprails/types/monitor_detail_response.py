# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.

from typing import Dict, List, Optional
from datetime import datetime
from typing_extensions import Literal

from pydantic import Field as FieldInfo

from .._models import BaseModel

__all__ = [
    "MonitorDetailResponse",
    "Capability",
    "Evaluation",
    "EvaluationModelInput",
    "EvaluationModelInputContext",
    "File",
    "Stats",
]


class Capability(BaseModel):
    capability: Optional[str] = None
    """The type of capability."""


class EvaluationModelInputContext(BaseModel):
    content: Optional[str] = None
    """The content of the message."""

    role: Optional[str] = None
    """The role of the speaker."""


class EvaluationModelInput(BaseModel):
    """A dictionary of inputs sent to the LLM to generate output.

    The dictionary must contain a `user_prompt` field. For ground_truth_adherence  guardrail metric, `ground_truth` should be provided. When `context_awareness` is enabled, `context` should be provided.
    """

    user_prompt: str
    """The user prompt used to generate the output."""

    context: Optional[List[EvaluationModelInputContext]] = None
    """
    Any structured information that directly relates to the model’s input and
    expected output—e.g., the recent turn-by-turn history between an AI tutor and a
    student, facts or state passed through an agentic workflow, or other
    domain-specific signals your system already knows and wants the model to
    condition on.
    """

    ground_truth: Optional[str] = None
    """The ground truth for evaluating Ground Truth Adherence guardrail."""

    system_prompt: Optional[str] = None
    """The system prompt used to generate the output."""


class Evaluation(BaseModel):
    evaluation_status: Literal["in_progress", "completed", "canceled", "queued", "failed"]
    """Status of the evaluation."""

    api_model_input: EvaluationModelInput = FieldInfo(alias="model_input")
    """A dictionary of inputs sent to the LLM to generate output.

    The dictionary must contain a `user_prompt` field. For ground_truth_adherence
    guardrail metric, `ground_truth` should be provided. When `context_awareness` is
    enabled, `context` should be provided.
    """

    api_model_output: str = FieldInfo(alias="model_output")
    """Output generated by the LLM to be evaluated."""

    run_mode: Literal["precision_plus", "precision", "smart", "economy"]
    """Run mode for the evaluation.

    The run mode allows the user to optimize for speed, accuracy, and cost by
    determining which models are used to evaluate the event.
    """

    created_at: Optional[datetime] = None
    """The time the evaluation was created in UTC."""

    error_message: Optional[str] = None
    """Error message if the evaluation failed."""

    evaluation_result: Optional[Dict[str, object]] = None
    """
    Evaluation result consisting of average scores and rationales for each of the
    evaluated guardrail metrics.
    """

    evaluation_total_cost: Optional[float] = None
    """Total cost of the evaluation."""

    guardrail_metrics: Optional[
        List[
            Literal[
                "correctness",
                "completeness",
                "instruction_adherence",
                "context_adherence",
                "ground_truth_adherence",
                "comprehensive_safety",
            ]
        ]
    ] = None
    """
    An array of guardrail metrics that the input and output pair will be evaluated
    on.
    """

    nametag: Optional[str] = None
    """An optional, user-defined tag for the evaluation."""

    progress: Optional[int] = None
    """Evaluation progress.

    Values range between 0 and 100; 100 corresponds to a completed
    `evaluation_status`.
    """


class File(BaseModel):
    file_id: Optional[str] = None
    """The ID of the file."""

    file_name: Optional[str] = None
    """The name of the file."""

    file_size: Optional[int] = None
    """The size of the file in bytes."""


class Stats(BaseModel):
    """
    Contains five fields used for stats of this monitor: total evaluations, completed evaluations, failed evaluations, queued evaluations, and in progress evaluations.
    """

    completed_evaluations: Optional[int] = None
    """Number of evaluations that completed successfully."""

    failed_evaluations: Optional[int] = None
    """Number of evaluations that failed."""

    in_progress_evaluations: Optional[int] = None
    """Number of evaluations currently in progress."""

    queued_evaluations: Optional[int] = None
    """Number of evaluations currently queued."""

    total_evaluations: Optional[int] = None
    """Total number of evaluations performed by this monitor."""


class MonitorDetailResponse(BaseModel):
    capabilities: List[Capability]
    """An array of extended AI capabilities associated with this monitor.

    Can be `web_search`, `file_search`, and/or `context_awareness`.
    """

    created_at: datetime
    """The time the monitor was created in UTC."""

    evaluations: List[Evaluation]
    """An array of all evaluations performed by this monitor.

    Each one corresponds to a separate monitor event.
    """

    files: List[File]
    """An array of files associated with this monitor."""

    monitor_id: str
    """A unique monitor ID."""

    name: str
    """Name of this monitor."""

    stats: Stats
    """
    Contains five fields used for stats of this monitor: total evaluations,
    completed evaluations, failed evaluations, queued evaluations, and in progress
    evaluations.
    """

    status: Literal["active", "inactive"]
    """Status of the monitor.

    Can be `active` or `inactive`. Inactive monitors no longer record and evaluate
    events.
    """

    updated_at: datetime
    """The most recent time the monitor was modified in UTC."""

    description: Optional[str] = None
    """Description of this monitor."""
