{
  "$schema": "https://static.modelcontextprotocol.io/schemas/2025-12-11/server.schema.json",
  "name": "io.github.egoughnour/massive-context-mcp",
  "description": "Handles 10M+ token contexts with chunking, sub-queries, and local Ollama inference.",
  "repository": {
    "url": "https://github.com/egoughnour/massive-context-mcp",
    "source": "github"
  },
  "version": "2.0.8",
  "packages": [
    {
      "registryType": "pypi",
      "identifier": "massive-context-mcp",
      "version": "2.0.8",
      "transport": {
        "type": "stdio"
      },
      "environmentVariables": [
        {
          "name": "RLM_DATA_DIR",
          "description": "Directory for storing context data",
          "isRequired": false,
          "format": "string",
          "isSecret": false
        },
        {
          "name": "OLLAMA_URL",
          "description": "URL for Ollama server (default: http://localhost:11434)",
          "isRequired": false,
          "format": "string",
          "isSecret": false
        }
      ]
    },
    {
      "registryType": "mcpb",
      "identifier": "https://github.com/egoughnour/massive-context-mcp/releases/latest/download/massive-context-mcp.mcpb",
      "transport": {
        "type": "stdio"
      },
      "environmentVariables": [
        {
          "name": "RLM_DATA_DIR",
          "description": "Directory for storing context data",
          "isRequired": false,
          "format": "string",
          "isSecret": false
        },
        {
          "name": "OLLAMA_URL",
          "description": "URL for Ollama server (default: http://localhost:11434)",
          "isRequired": false,
          "format": "string",
          "isSecret": false
        }
      ]
    }
  ]
}
