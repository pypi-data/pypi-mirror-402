{
  "$schema": "https://static.modelcontextprotocol.io/schemas/2025-09-29/server.schema.json",
  "name": "io.github.egoughnour/massive-context-mcp",
  "displayName": "Massive Context MCP",
  "description": "Handles 10M+ token contexts with chunking, sub-queries, and local Ollama inference.",
  "version": "2.0.1",
  "homepage": "https://github.com/egoughnour/massive-context-mcp",
  "repository": {
    "url": "https://github.com/egoughnour/massive-context-mcp",
    "source": "github"
  },
  "license": "MIT",
  "authors": [
    {
      "name": "Erik Goughnour",
      "email": "e.goughnour@gmail.com"
    }
  ],
  "tags": [
    "massive-context",
    "chunking",
    "ollama",
    "local-inference",
    "sub-query",
    "rlm",
    "context-management"
  ],
  "server": {
    "mcpConfig": {
      "command": "uvx",
      "args": [
        "massive-context-mcp"
      ],
      "env": {
        "RLM_DATA_DIR": "${HOME}/.rlm-data",
        "OLLAMA_URL": "http://localhost:11434"
      }
    }
  },
  "packages": [
    {
      "registryBaseUrl": "https://pypi.org",
      "registryType": "pypi",
      "identifier": "massive-context-mcp",
      "version": "2.0.1",
      "transport": {
        "type": "stdio"
      }
    }
  ],
  "tools": [
    {
      "name": "rlm_system_check",
      "description": "Check system requirements for Ollama"
    },
    {
      "name": "rlm_setup_ollama",
      "description": "Install Ollama via Homebrew"
    },
    {
      "name": "rlm_setup_ollama_direct",
      "description": "Install Ollama via direct download (no sudo)"
    },
    {
      "name": "rlm_ollama_status",
      "description": "Check Ollama availability"
    },
    {
      "name": "rlm_auto_analyze",
      "description": "One-step analysis with auto-detection"
    },
    {
      "name": "rlm_load_context",
      "description": "Load context as external variable"
    },
    {
      "name": "rlm_inspect_context",
      "description": "Get structure info"
    },
    {
      "name": "rlm_chunk_context",
      "description": "Chunk by lines/chars/paragraphs"
    },
    {
      "name": "rlm_get_chunk",
      "description": "Retrieve specific chunk"
    },
    {
      "name": "rlm_filter_context",
      "description": "Filter with regex"
    },
    {
      "name": "rlm_exec",
      "description": "Execute Python code (sandboxed)"
    },
    {
      "name": "rlm_sub_query",
      "description": "Make sub-LLM call on chunk"
    },
    {
      "name": "rlm_sub_query_batch",
      "description": "Process chunks in parallel"
    },
    {
      "name": "rlm_store_result",
      "description": "Store sub-call result"
    },
    {
      "name": "rlm_get_results",
      "description": "Retrieve stored results"
    },
    {
      "name": "rlm_list_contexts",
      "description": "List all loaded contexts"
    }
  ]
}
