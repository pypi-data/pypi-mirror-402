"""AsyncGLMAgent - å¼‚æ­¥ GLM Agent å®ç°ï¼Œæ”¯æŒåŸç”Ÿæµå¼è¾“å‡ºå’Œç«‹å³å–æ¶ˆã€‚"""

import asyncio
import json
import traceback
from typing import Any, AsyncIterator, Callable

from openai import AsyncOpenAI

from AutoGLM_GUI.actions import ActionHandler, ActionResult
from AutoGLM_GUI.agents.protocols import AsyncAgent
from AutoGLM_GUI.config import AgentConfig, ModelConfig
from AutoGLM_GUI.device_protocol import DeviceProtocol
from AutoGLM_GUI.logger import logger
from AutoGLM_GUI.prompt_config import get_messages, get_system_prompt

from .message_builder import MessageBuilder
from .parser import GLMParser


class AsyncGLMAgent(AsyncAgent):
    """å¼‚æ­¥ GLM Agent å®ç°ã€‚

    æ ¸å¿ƒç‰¹æ€§:
    - ä½¿ç”¨ AsyncOpenAI è¿›è¡Œå¼‚æ­¥ LLM è°ƒç”¨
    - åŸç”Ÿæ”¯æŒæµå¼è¾“å‡º (async for)
    - æ”¯æŒç«‹å³å–æ¶ˆ (asyncio.CancelledError)
    - ä½¿ç”¨ asyncio.to_thread åŒ…è£…åŒæ­¥çš„è®¾å¤‡æ“ä½œ

    ä¸ GLMAgent çš„åŒºåˆ«:
    - stream() æ–¹æ³•è¿”å› AsyncIteratorï¼Œä¸éœ€è¦ worker çº¿ç¨‹
    - cancel() å¯ä»¥ç«‹å³ä¸­æ–­ HTTP è¯·æ±‚
    - ä¸éœ€è¦ monkey-patch thinking_callback
    """

    def __init__(
        self,
        model_config: ModelConfig,
        agent_config: AgentConfig,
        device: DeviceProtocol,
        confirmation_callback: Callable[[str], bool] | None = None,
        takeover_callback: Callable[[str], None] | None = None,
    ):
        self.model_config = model_config
        self.agent_config = agent_config

        # ä½¿ç”¨ AsyncOpenAI
        self.openai_client = AsyncOpenAI(
            base_url=model_config.base_url,
            api_key=model_config.api_key,
            timeout=120,
        )
        self.parser = GLMParser()

        self.device = device
        self.action_handler = ActionHandler(
            device=self.device,
            confirmation_callback=confirmation_callback,
            takeover_callback=takeover_callback,
        )

        # å–æ¶ˆæœºåˆ¶
        self._cancel_event = asyncio.Event()

        # åˆå§‹åŒ– system promptï¼ˆAgent ä¸€åˆ›å»ºå°±æœ‰"äººæ ¼"ï¼‰
        system_prompt = self.agent_config.system_prompt
        if system_prompt is None:
            system_prompt = get_system_prompt(self.agent_config.lang)

        # ä¿å­˜åˆå§‹ system message ç”¨äº reset()
        self._initial_system_message = MessageBuilder.create_system_message(
            system_prompt
        )

        # çŠ¶æ€
        self._context: list[dict[str, Any]] = [self._initial_system_message]
        self._step_count = 0
        self._is_running = False

    async def stream(self, task: str) -> AsyncIterator[dict[str, Any]]:
        """æµå¼æ‰§è¡Œä»»åŠ¡ï¼Œæ”¯æŒå–æ¶ˆã€‚

        Args:
            task: ä»»åŠ¡æè¿°

        Yields:
            dict[str, Any]: äº‹ä»¶å­—å…¸ï¼Œæ ¼å¼ä¸º {"type": str, "data": dict}

        äº‹ä»¶ç±»å‹:
        - "thinking": {"chunk": str}
        - "step": {"step": int, "thinking": str, "action": dict, ...}
        - "done": {"message": str, "steps": int, "success": bool}
        - "cancelled": {"message": str}
        - "error": {"message": str}
        """
        self._is_running = True
        self._cancel_event.clear()

        try:
            # ===== åˆå§‹åŒ–é˜¶æ®µï¼šæ·»åŠ é¦–æ¬¡ç”¨æˆ·è¾“å…¥ =====
            try:
                screenshot = await asyncio.to_thread(self.device.get_screenshot)
                current_app = await asyncio.to_thread(self.device.get_current_app)
            except Exception as e:
                logger.error(f"Failed to get device info during initialization: {e}")
                yield {
                    "type": "error",
                    "data": {"message": f"Device error: {e}"},
                }
                yield {
                    "type": "done",
                    "data": {
                        "message": f"Device error: {e}",
                        "steps": 0,
                        "success": False,
                    },
                }
                return

            screen_info = MessageBuilder.build_screen_info(current_app)
            initial_message = f"{task}\n\n** Screen Info **\n\n{screen_info}"

            self._context.append(
                MessageBuilder.create_user_message(
                    text=initial_message, image_base64=screenshot.base64_data
                )
            )

            # ===== æ‰§è¡Œé˜¶æ®µï¼šå¾ªç¯æ‰§è¡Œæ­¥éª¤ =====
            while self._step_count < self.agent_config.max_steps and self._is_running:
                # æ£€æŸ¥å–æ¶ˆ
                if self._cancel_event.is_set():
                    raise asyncio.CancelledError()

                async for event in self._execute_step_async():
                    yield event

                    # æ£€æŸ¥æ˜¯å¦å®Œæˆ
                    if event["type"] == "step" and event["data"].get("finished"):
                        yield {
                            "type": "done",
                            "data": {
                                "message": event["data"].get(
                                    "message", "Task completed"
                                ),
                                "steps": self._step_count,
                                "success": event["data"].get("success", True),
                            },
                        }
                        return

            # è¾¾åˆ°æœ€å¤§æ­¥æ•°
            yield {
                "type": "done",
                "data": {
                    "message": "Max steps reached",
                    "steps": self._step_count,
                    "success": False,
                },
            }

        except asyncio.CancelledError:
            yield {
                "type": "cancelled",
                "data": {"message": "Task cancelled by user"},
            }
            raise

        finally:
            self._is_running = False

    async def _execute_step_async(self) -> AsyncIterator[dict[str, Any]]:
        """æ‰§è¡Œå•æ­¥ï¼Œæ”¯æŒæµå¼è¾“å‡ºå’Œå–æ¶ˆã€‚

        æ³¨æ„ï¼šä¸å†éœ€è¦ user_prompt å‚æ•°ï¼Œå› ä¸ºï¼š
        - é¦–æ¬¡ç”¨æˆ·è¾“å…¥å·²åœ¨ stream() çš„åˆå§‹åŒ–é˜¶æ®µæ·»åŠ 
        - æ­¤æ–¹æ³•åªè´Ÿè´£æ‰§è¡Œæ­¥éª¤ï¼šè·å–å±å¹• â†’ è°ƒç”¨ LLM â†’ æ‰§è¡ŒåŠ¨ä½œ

        Yields:
            dict[str, Any]: äº‹ä»¶å­—å…¸
        """
        self._step_count += 1

        # 1. è·å–å½“å‰å±å¹•çŠ¶æ€ï¼ˆä½¿ç”¨çº¿ç¨‹æ± ï¼‰
        try:
            screenshot = await asyncio.to_thread(self.device.get_screenshot)
            current_app = await asyncio.to_thread(self.device.get_current_app)
        except Exception as e:
            logger.error(f"Failed to get device info: {e}")
            yield {
                "type": "error",
                "data": {"message": f"Device error: {e}"},
            }
            yield {
                "type": "step",
                "data": {
                    "step": self._step_count,
                    "thinking": "",
                    "action": None,
                    "success": False,
                    "finished": True,
                    "message": f"Device error: {e}",
                },
            }
            return

        # 2. æ„å»ºæ¶ˆæ¯ï¼ˆç»Ÿä¸€æ ¼å¼ï¼šåªæœ‰å±å¹•ä¿¡æ¯ï¼‰
        screen_info = MessageBuilder.build_screen_info(current_app)
        text_content = f"** Screen Info **\n\n{screen_info}"

        self._context.append(
            MessageBuilder.create_user_message(
                text=text_content, image_base64=screenshot.base64_data
            )
        )

        # 3. æµå¼è°ƒç”¨ OpenAIï¼ˆçœŸæ­£çš„å¼‚æ­¥ï¼Œå¯å–æ¶ˆï¼‰
        try:
            if self.agent_config.verbose:
                msgs = get_messages(self.agent_config.lang)
                logger.debug(f"ğŸ’­ {msgs['thinking']}:")

            thinking_parts = []
            raw_content = ""

            async for chunk_data in self._stream_openai(self._context):
                # æ£€æŸ¥å–æ¶ˆ
                if self._cancel_event.is_set():
                    raise asyncio.CancelledError()

                if chunk_data["type"] == "thinking":
                    thinking_parts.append(chunk_data["content"])

                    # Yield thinking event
                    yield {
                        "type": "thinking",
                        "data": {"chunk": chunk_data["content"]},
                    }

                    # Verbose output
                    if self.agent_config.verbose:
                        logger.debug(chunk_data["content"])

                elif chunk_data["type"] == "raw":
                    raw_content += chunk_data["content"]

            thinking = "".join(thinking_parts)

        except asyncio.CancelledError:
            logger.info(f"Step {self._step_count} cancelled during LLM call")
            raise

        except Exception as e:
            logger.error(f"LLM error: {e}")
            if self.agent_config.verbose:
                logger.debug(traceback.format_exc())

            yield {
                "type": "error",
                "data": {"message": f"Model error: {e}"},
            }
            yield {
                "type": "step",
                "data": {
                    "step": self._step_count,
                    "thinking": "",
                    "action": None,
                    "success": False,
                    "finished": True,
                    "message": f"Model error: {e}",
                },
            }
            return

        # 4. è§£æ action
        _, action_str = self._parse_raw_response(raw_content)

        try:
            action = self.parser.parse(action_str)
        except ValueError as e:
            if self.agent_config.verbose:
                logger.warning(f"Failed to parse action: {e}, treating as finish")
            action = {"_metadata": "finish", "message": action_str}

        if self.agent_config.verbose:
            msgs = get_messages(self.agent_config.lang)
            logger.debug(f"ğŸ¯ {msgs['action']}:")
            logger.debug(json.dumps(action, ensure_ascii=False, indent=2))

        # 5. æ‰§è¡Œ actionï¼ˆä½¿ç”¨çº¿ç¨‹æ± ï¼‰
        try:
            result = await asyncio.to_thread(
                self.action_handler.execute, action, screenshot.width, screenshot.height
            )
        except Exception as e:
            logger.error(f"Action execution error: {e}")
            if self.agent_config.verbose:
                logger.debug(traceback.format_exc())
            result = ActionResult(success=False, should_finish=True, message=str(e))

        # 6. æ›´æ–°ä¸Šä¸‹æ–‡
        self._context[-1] = MessageBuilder.remove_images_from_message(self._context[-1])

        self._context.append(
            MessageBuilder.create_assistant_message(
                f"<think>{thinking}</think><answer>{action_str}</answer>"
            )
        )

        # 7. æ£€æŸ¥æ˜¯å¦å®Œæˆ
        finished = action.get("_metadata") == "finish" or result.should_finish

        if finished and self.agent_config.verbose:
            msgs = get_messages(self.agent_config.lang)
            logger.debug(
                f"âœ… {msgs['task_completed']}: {result.message or action.get('message', msgs['done'])}"
            )

        # 8. è¿”å›æ­¥éª¤ç»“æœ
        yield {
            "type": "step",
            "data": {
                "step": self._step_count,
                "thinking": thinking,
                "action": action,
                "success": result.success,
                "finished": finished,
                "message": result.message or action.get("message"),
            },
        }

    async def _stream_openai(
        self, messages: list[dict[str, Any]]
    ) -> AsyncIterator[dict[str, str]]:
        """æµå¼è°ƒç”¨ OpenAIï¼Œyield thinking chunksã€‚

        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨

        Yields:
            dict[str, str]: {"type": "thinking" | "raw", "content": str}

        Raises:
            asyncio.CancelledError: ä»»åŠ¡è¢«å–æ¶ˆ
        """
        stream = await self.openai_client.chat.completions.create(
            messages=messages,  # type: ignore[arg-type]
            model=self.model_config.model_name,
            max_tokens=self.model_config.max_tokens,
            temperature=self.model_config.temperature,
            top_p=self.model_config.top_p,
            frequency_penalty=self.model_config.frequency_penalty,
            extra_body=self.model_config.extra_body,
            stream=True,
        )

        buffer = ""
        action_markers = ["finish(message=", "do(action="]
        in_action_phase = False

        try:
            async for chunk in stream:
                # æ£€æŸ¥å–æ¶ˆ
                if self._cancel_event.is_set():
                    await stream.close()  # å…³é”®ï¼šå…³é—­ HTTP è¿æ¥
                    raise asyncio.CancelledError()

                if len(chunk.choices) == 0:
                    continue

                if chunk.choices[0].delta.content is not None:
                    content = chunk.choices[0].delta.content
                    yield {"type": "raw", "content": content}

                    if in_action_phase:
                        continue

                    buffer += content

                    # æ£€æŸ¥æ˜¯å¦åˆ°è¾¾ action æ ‡è®°
                    marker_found = False
                    for marker in action_markers:
                        if marker in buffer:
                            thinking_part = buffer.split(marker, 1)[0]
                            yield {"type": "thinking", "content": thinking_part}
                            in_action_phase = True
                            marker_found = True
                            break

                    if marker_found:
                        continue

                    # æ£€æŸ¥æ˜¯å¦æ˜¯æ½œåœ¨çš„ marker å‰ç¼€
                    is_potential_marker = False
                    for marker in action_markers:
                        for i in range(1, len(marker)):
                            if buffer.endswith(marker[:i]):
                                is_potential_marker = True
                                break
                        if is_potential_marker:
                            break

                    if not is_potential_marker and len(buffer) > 0:
                        yield {"type": "thinking", "content": buffer}
                        buffer = ""

        finally:
            await stream.close()  # ç¡®ä¿èµ„æºé‡Šæ”¾

    def _parse_raw_response(self, content: str) -> tuple[str, str]:
        """è§£æåŸå§‹å“åº”ï¼Œæå– thinking å’Œ actionã€‚

        Args:
            content: åŸå§‹å“åº”å†…å®¹

        Returns:
            tuple[str, str]: (thinking, action)
        """
        if "finish(message=" in content:
            parts = content.split("finish(message=", 1)
            thinking = parts[0].strip()
            action = "finish(message=" + parts[1]
            return thinking, action

        if "do(action=" in content:
            parts = content.split("do(action=", 1)
            thinking = parts[0].strip()
            action = "do(action=" + parts[1]
            return thinking, action

        if "<answer>" in content:
            parts = content.split("<answer>", 1)
            thinking = parts[0].replace("<think>", "").replace("</think>", "").strip()
            action = parts[1].replace("</answer>", "").strip()
            return thinking, action

        return "", content

    async def cancel(self) -> None:
        """å–æ¶ˆå½“å‰æ‰§è¡Œã€‚

        è®¾ç½®å–æ¶ˆæ ‡å¿—ï¼Œä¸­æ–­æ­£åœ¨è¿›è¡Œçš„ HTTP è¯·æ±‚ã€‚
        """
        self._cancel_event.set()
        self._is_running = False
        logger.info("AsyncGLMAgent cancelled by user")

    def reset(self) -> None:
        """é‡ç½®çŠ¶æ€ï¼ˆæ¢å¤åˆ°åˆå§‹çŠ¶æ€ï¼Œä¿ç•™ system messageï¼‰ã€‚"""
        self._context = [self._initial_system_message]
        self._step_count = 0
        self._is_running = False
        self._cancel_event.clear()

    async def run(self, task: str) -> str:
        """è¿è¡Œå®Œæ•´ä»»åŠ¡ï¼ˆå…¼å®¹æ¥å£ï¼‰ã€‚

        Args:
            task: ä»»åŠ¡æè¿°

        Returns:
            str: æœ€ç»ˆç»“æœæ¶ˆæ¯
        """
        final_message = ""
        async for event in self.stream(task):
            if event["type"] == "done":
                final_message = event["data"].get("message", "")
        return final_message

    @property
    def step_count(self) -> int:
        return self._step_count

    @property
    def context(self) -> list[dict[str, Any]]:
        return self._context.copy()

    @property
    def is_running(self) -> bool:
        return self._is_running
