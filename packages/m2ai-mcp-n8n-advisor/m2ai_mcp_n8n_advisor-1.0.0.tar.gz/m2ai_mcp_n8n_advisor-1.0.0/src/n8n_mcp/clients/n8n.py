"""n8n API client for workflow management.

Generated by GRIMLOCK MCP Factory
"""

import logging
import os
from typing import Any

import httpx

logger = logging.getLogger("n8n_mcp")


class N8nClient:
    """Client for interacting with the n8n API."""

    def __init__(
        self,
        base_url: str | None = None,
        api_key: str | None = None,
    ):
        """Initialize the n8n client.

        Args:
            base_url: n8n instance URL. Defaults to N8N_BASE_URL env var.
            api_key: n8n API key. Defaults to N8N_API_KEY env var.
        """
        self.base_url = (base_url or os.environ.get("N8N_BASE_URL", "")).rstrip("/")
        self.api_key = api_key or os.environ.get("N8N_API_KEY", "")

    def _get_headers(self) -> dict[str, str]:
        """Get request headers with authentication.

        Returns:
            Dictionary of HTTP headers
        """
        return {
            "X-N8N-API-KEY": self.api_key,
            "Content-Type": "application/json",
            "Accept": "application/json",
        }

    async def list_workflows(self) -> list[dict[str, Any]]:
        """List all workflows in the n8n instance.

        Returns:
            List of workflow objects with id, name, active status, etc.
        """
        async with httpx.AsyncClient(timeout=30.0) as client:
            response = await client.get(
                f"{self.base_url}/api/v1/workflows",
                headers=self._get_headers(),
            )
            response.raise_for_status()
            data = response.json()

        # n8n API returns {"data": [...]} for workflows
        workflows = data.get("data", data) if isinstance(data, dict) else data
        return workflows

    async def get_workflow(self, workflow_id: str) -> dict[str, Any]:
        """Get a specific workflow by ID.

        Args:
            workflow_id: The workflow ID

        Returns:
            Workflow object with full details
        """
        async with httpx.AsyncClient(timeout=30.0) as client:
            response = await client.get(
                f"{self.base_url}/api/v1/workflows/{workflow_id}",
                headers=self._get_headers(),
            )
            response.raise_for_status()
            return response.json()

    async def list_executions(
        self,
        workflow_id: str | None = None,
        status: str | None = None,
        limit: int = 20,
    ) -> list[dict[str, Any]]:
        """List workflow executions.

        Args:
            workflow_id: Optional filter by workflow ID
            status: Optional filter by status ('success', 'error', 'waiting')
            limit: Maximum number of executions to return

        Returns:
            List of execution objects
        """
        params: dict[str, Any] = {"limit": limit}
        if workflow_id:
            params["workflowId"] = workflow_id
        if status:
            params["status"] = status

        async with httpx.AsyncClient(timeout=30.0) as client:
            response = await client.get(
                f"{self.base_url}/api/v1/executions",
                headers=self._get_headers(),
                params=params,
            )
            response.raise_for_status()
            data = response.json()

        executions = data.get("data", data) if isinstance(data, dict) else data
        return executions

    async def check_workflows(self) -> dict[str, Any]:
        """Review all workflows and report their status.

        Returns:
            Summary of all workflows with status information
        """
        workflows = await self.list_workflows()

        # Get recent executions to understand workflow health
        executions = await self.list_executions(limit=100)

        # Build execution stats per workflow
        workflow_execution_stats: dict[str, dict[str, int]] = {}
        for execution in executions:
            wf_id = execution.get("workflowId", "")
            if wf_id:
                if wf_id not in workflow_execution_stats:
                    workflow_execution_stats[wf_id] = {
                        "success": 0,
                        "error": 0,
                        "waiting": 0,
                        "running": 0,
                    }
                status = execution.get("status", "").lower()
                if status in workflow_execution_stats[wf_id]:
                    workflow_execution_stats[wf_id][status] += 1

        # Build workflow summary
        workflow_summaries = []
        active_count = 0
        inactive_count = 0
        error_workflows = []

        for workflow in workflows:
            wf_id = str(workflow.get("id", ""))
            name = workflow.get("name", "Unnamed")
            active = workflow.get("active", False)

            if active:
                active_count += 1
            else:
                inactive_count += 1

            stats = workflow_execution_stats.get(wf_id, {})
            error_count = stats.get("error", 0)

            summary = {
                "id": wf_id,
                "name": name,
                "active": active,
                "recent_executions": {
                    "success": stats.get("success", 0),
                    "error": error_count,
                    "waiting": stats.get("waiting", 0),
                    "running": stats.get("running", 0),
                },
            }

            if error_count > 0:
                error_workflows.append({"id": wf_id, "name": name, "errors": error_count})

            workflow_summaries.append(summary)

        return {
            "total_workflows": len(workflows),
            "active_workflows": active_count,
            "inactive_workflows": inactive_count,
            "workflows_with_errors": len(error_workflows),
            "error_workflows": error_workflows,
            "workflows": workflow_summaries,
        }
