"""Git Diff ç»„ç»‡å’Œæ™ºèƒ½å‹ç¼©å·¥å…·.

æä¾›ä¸‰çº§å‹ç¼©ç­–ç•¥ï¼š
1. åŸå§‹ diffï¼ˆå¦‚æœåœ¨ token é™åˆ¶å†…ï¼‰
2. ç»“æ„åŒ–å‹ç¼©ï¼ˆåŸºäºæ–‡ä»¶é‡è¦æ€§ä¼˜å…ˆçº§ï¼‰
3. ç®€å•è¡Œå‹ç¼©ï¼ˆfallbackï¼‰
"""

import re
from dataclasses import dataclass
from typing import Final

from unidiff import PatchSet

from commity.utils.token_counter import count_tokens

# ============================================================================
# å¸¸é‡é…ç½®
# ============================================================================

MAX_DIFF_LENGTH: Final[int] = 15000  # å•ä¸ª diff çš„æœ€å¤§å­—ç¬¦æ•°
MAX_FILES_IN_SUMMARY: Final[int] = 30  # æ‘˜è¦ä¸­æœ€å¤šæ˜¾ç¤ºçš„æ–‡ä»¶æ•°
MAX_COMPRESSED_LINES: Final[int] = 1000  # å‹ç¼©æ¨¡å¼ä¸‹çš„æœ€å¤§è¡Œæ•°
MAX_HUNKS_PER_FILE: Final[int] = 5  # æ¯ä¸ªæ–‡ä»¶æœ€å¤šæ˜¾ç¤ºçš„ hunk æ•°
MAX_LINES_PER_HUNK: Final[int] = 10  # æ¯ä¸ª hunk æœ€å¤šæ˜¾ç¤ºçš„è¡Œæ•°


# ============================================================================
# æ•°æ®ç»“æ„
# ============================================================================


@dataclass
class FileImportance:
    """æ–‡ä»¶å˜æ›´é‡è¦æ€§è¯„åˆ†."""

    path: str  # æ–‡ä»¶è·¯å¾„
    score: int  # é‡è¦æ€§åˆ†æ•°ï¼ˆè¶Šé«˜è¶Šé‡è¦ï¼‰
    added: int  # æ–°å¢è¡Œæ•°
    removed: int  # åˆ é™¤è¡Œæ•°


# ============================================================================
# ç¬¬ä¸€éƒ¨åˆ†ï¼šæ–‡ä»¶é‡è¦æ€§è¯„ä¼°
# ============================================================================


def calculate_file_importance(file_path: str, added: int, removed: int) -> int:
    """è®¡ç®—æ–‡ä»¶çš„é‡è¦æ€§åˆ†æ•°.

    è¯„åˆ†è§„åˆ™ï¼š
    - æ ¸å¿ƒæºä»£ç æ–‡ä»¶ï¼ˆ.py, .js, .ts ç­‰ï¼‰: åŸºç¡€åˆ† 10
    - é…ç½®æ–‡ä»¶ï¼ˆ.json, .yaml ç­‰ï¼‰: åŸºç¡€åˆ† 5
    - æµ‹è¯•å’Œæ–‡æ¡£: åŸºç¡€åˆ† 2
    - Lock æ–‡ä»¶: åŸºç¡€åˆ† 1
    - å˜æ›´è¡Œæ•°è´¡çŒ®ï¼ˆä¸Šé™ 50ï¼‰

    Args:
    ----
        file_path: æ–‡ä»¶è·¯å¾„
        added: æ–°å¢è¡Œæ•°
        removed: åˆ é™¤è¡Œæ•°

    Returns:
    -------
        é‡è¦æ€§åˆ†æ•°ï¼ˆè¶Šé«˜è¶Šé‡è¦ï¼‰

    """
    score = 0

    # 1. æ ¹æ®æ–‡ä»¶ç±»å‹è¯„åˆ†
    if file_path.endswith((".py", ".js", ".ts", ".go", ".rs", ".java", ".cpp", ".c")):
        score += 10  # æ ¸å¿ƒæºä»£ç 
    elif file_path.endswith((".json", ".yaml", ".yml", ".toml", ".ini", ".conf")):
        score += 5  # é…ç½®æ–‡ä»¶
    elif "test" in file_path.lower() or file_path.endswith((".md", ".txt", ".rst")):
        score += 2  # æµ‹è¯•å’Œæ–‡æ¡£
    elif file_path in ("package-lock.json", "yarn.lock", "Cargo.lock", "poetry.lock"):
        score += 1  # Lock æ–‡ä»¶ï¼ˆæœ€ä½ä¼˜å…ˆçº§ï¼‰

    # 2. ç‰¹æ®Šæ–‡ä»¶åŠ åˆ†
    if file_path in ("README.md", "pyproject.toml", "package.json", "Cargo.toml"):
        score += 8

    # 3. å˜æ›´è§„æ¨¡è´¡çŒ®ï¼ˆä¸Šé™ 50ï¼‰
    change_size = min(added + removed, 50)
    score += change_size

    return score


def rank_files_by_importance(patch: PatchSet) -> list[FileImportance]:
    """å¯¹æ‰€æœ‰å˜æ›´æ–‡ä»¶æŒ‰é‡è¦æ€§æ’åº.

    Args:
    ----
        patch: unidiff è§£æçš„ PatchSet å¯¹è±¡

    Returns:
    -------
        æŒ‰é‡è¦æ€§é™åºæ’åˆ—çš„æ–‡ä»¶åˆ—è¡¨

    """
    file_list = []

    for patched_file in patch:
        score = calculate_file_importance(
            patched_file.path, patched_file.added, patched_file.removed
        )
        file_list.append(
            FileImportance(
                path=patched_file.path,
                score=score,
                added=patched_file.added,
                removed=patched_file.removed,
            )
        )

    # æŒ‰åˆ†æ•°é™åºæ’åº
    file_list.sort(key=lambda x: x.score, reverse=True)

    return file_list


# ============================================================================
# ç¬¬äºŒéƒ¨åˆ†ï¼šDiff å†…å®¹æå–å’Œæ ¼å¼åŒ–
# ============================================================================


def extract_hunk_context(hunk) -> str:
    """ä» hunk ä¸­æå–å‡½æ•°/ç±»åä¸Šä¸‹æ–‡.

    Args:
    ----
        hunk: unidiff çš„ Hunk å¯¹è±¡

    Returns:
    -------
        æ ¼å¼åŒ–çš„ä¸Šä¸‹æ–‡å­—ç¬¦ä¸²ï¼Œå¦‚ "  â†³ def function_name(...)"

    """
    if not hunk.section_header:
        return ""

    # æ¸…ç†å‡½æ•°ç­¾å
    header = hunk.section_header.strip()
    # ç§»é™¤å¸¸è§çš„å‡½æ•°å®šä¹‰å…³é”®å­—
    header = re.sub(r"^(def|class|function|func|fn|public|private|protected)\s+", "", header)

    if header:
        return f"  â†³ {header}"

    return ""


def extract_hunk_changes(hunk, max_lines: int = MAX_LINES_PER_HUNK) -> tuple[list[str], list[str]]:
    """æå– hunk ä¸­çš„å…³é”®å˜æ›´.

    Args:
    ----
        hunk: unidiff çš„ Hunk å¯¹è±¡
        max_lines: æ¯ä¸ª hunk æœ€å¤šæå–çš„è¡Œæ•°

    Returns:
    -------
        (removed_lines, added_lines) å…ƒç»„

    """
    added_lines = []
    removed_lines = []
    line_count = 0

    for line in hunk:
        if line_count >= max_lines:
            break

        # è·³è¿‡ import è¯­å¥ï¼ˆé€šå¸¸ä¸æ˜¯å…³é”®å˜æ›´ï¼‰
        if line.value.strip().startswith(("import ", "from ")):
            continue

        if line.is_added:
            added_lines.append(f"    + {line.value.rstrip()}")
            line_count += 1
        elif line.is_removed:
            removed_lines.append(f"    - {line.value.rstrip()}")
            line_count += 1

    return removed_lines, added_lines


def format_file_summary(patched_file, max_hunks: int = MAX_HUNKS_PER_FILE) -> str:
    """æ ¼å¼åŒ–å•ä¸ªæ–‡ä»¶çš„å˜æ›´æ‘˜è¦.

    è¾“å‡ºæ ¼å¼ï¼š
    ğŸ“„ **æ–‡ä»¶è·¯å¾„**
       +X -Y lines
      â†³ å‡½æ•°/ç±»ä¸Šä¸‹æ–‡
        - åˆ é™¤çš„è¡Œ
        + æ–°å¢çš„è¡Œ

    Args:
    ----
        patched_file: unidiff çš„ PatchedFile å¯¹è±¡
        max_hunks: æœ€å¤šæ˜¾ç¤ºçš„ hunk æ•°é‡

    Returns:
    -------
        æ ¼å¼åŒ–çš„æ–‡ä»¶æ‘˜è¦å­—ç¬¦ä¸²

    """
    lines = [
        f"ğŸ“„ **{patched_file.path}**",
        f"   +{patched_file.added} -{patched_file.removed} lines",
    ]

    for idx, hunk in enumerate(patched_file):
        if idx >= max_hunks:
            remaining = len(patched_file) - max_hunks
            lines.append(f"   ... +{remaining} more hunks")
            break

        # æ·»åŠ ä¸Šä¸‹æ–‡
        context = extract_hunk_context(hunk)
        if context:
            lines.append(context)

        # æå–å˜æ›´
        removed_lines, added_lines = extract_hunk_changes(hunk)

        # å…ˆæ˜¾ç¤ºåˆ é™¤ï¼Œå†æ˜¾ç¤ºæ–°å¢ï¼ˆæ›´ç¬¦åˆ diff ä¹ æƒ¯ï¼‰
        lines.extend(removed_lines[:5])
        lines.extend(added_lines[:5])

    return "\n".join(lines)


# ============================================================================
# ç¬¬ä¸‰éƒ¨åˆ†ï¼šå‹ç¼©ç­–ç•¥å®ç°
# ============================================================================


def compress_with_structure(diff_text: str, max_tokens: int, model_name: str, provider: str) -> str:
    """ç­–ç•¥2ï¼šç»“æ„åŒ–å‹ç¼©ï¼ˆåŸºäºä¼˜å…ˆçº§ï¼‰.

    å·¥ä½œæµç¨‹ï¼š
    1. ä½¿ç”¨ unidiff è§£æ diff ä¸ºç»“æ„åŒ–æ•°æ®
    2. è®¡ç®—æ¯ä¸ªæ–‡ä»¶çš„é‡è¦æ€§åˆ†æ•°
    3. æŒ‰ä¼˜å…ˆçº§æ’åº
    4. é€ä¸ªæ·»åŠ æ–‡ä»¶ï¼Œç›´åˆ°è¾¾åˆ° token é™åˆ¶

    Args:
    ----
        diff_text: åŸå§‹ git diff æ–‡æœ¬
        max_tokens: token é™åˆ¶
        model_name: æ¨¡å‹åç§°
        provider: LLM provider

    Returns:
    -------
        ç»“æ„åŒ–å‹ç¼©åçš„ diff æ–‡æœ¬

    """
    try:
        patch = PatchSet(diff_text)
    except Exception:
        # è§£æå¤±è´¥ï¼Œé™çº§åˆ°ç®€å•å‹ç¼©
        return compress_with_lines(diff_text, MAX_COMPRESSED_LINES)

    # 1. è¯„ä¼°å¹¶æ’åºæ–‡ä»¶
    ranked_files = rank_files_by_importance(patch)

    if not ranked_files:
        return "No changes detected."

    # 2. æ„å»ºæ–‡ä»¶æ˜ å°„ï¼ˆæ–¹ä¾¿æŸ¥æ‰¾ï¼‰
    files_map = {pf.path: pf for pf in patch}

    # 3. é€ä¸ªæ·»åŠ æ–‡ä»¶ï¼ˆä¼˜å…ˆçº§é«˜çš„å…ˆåŠ ï¼‰
    result_parts: list[str] = []
    total_files = len(ranked_files)

    for file_info in ranked_files:
        patched_file = files_map.get(file_info.path)
        if not patched_file:
            continue

        # ç”Ÿæˆè¿™ä¸ªæ–‡ä»¶çš„æ‘˜è¦
        file_summary = format_file_summary(patched_file)

        # å°è¯•æ·»åŠ ï¼Œæ£€æŸ¥æ˜¯å¦è¶…å‡ºé™åˆ¶
        test_content = "\n\n".join([*result_parts, file_summary])
        if count_tokens(test_content, model_name, provider) > max_tokens:
            # å¦‚æœä¸€ä¸ªæ–‡ä»¶éƒ½æ²¡æ·»åŠ ï¼Œè‡³å°‘æ·»åŠ ä¸€ä¸ªç®€åŒ–ç‰ˆæœ¬
            if not result_parts:
                minimal = (
                    f"ğŸ“„ **{file_info.path}**\n   +{file_info.added} -{file_info.removed} lines"
                )
                result_parts.append(minimal)
            break

        result_parts.append(file_summary)

    # 4. ç”Ÿæˆå¤´éƒ¨ä¿¡æ¯
    shown_files = len(result_parts)
    header_lines = [f"ğŸ“ Changes in {shown_files}/{total_files} files (sorted by importance):"]

    if shown_files < total_files:
        omitted = total_files - shown_files
        header_lines.append(f"âš ï¸ {omitted} files omitted due to space constraints")

    header = "\n".join(header_lines)

    return header + "\n\n" + "\n\n".join(result_parts)


def compress_with_lines(diff_text: str, max_lines: int = MAX_COMPRESSED_LINES) -> str:
    """ç­–ç•¥3ï¼šç®€å•è¡Œå‹ç¼©ï¼ˆfallbackï¼‰.

    å°† diff å‹ç¼©ä¸ºç®€å•çš„æ–‡ä»¶ + å˜æ›´åˆ—è¡¨æ ¼å¼ã€‚
    å½“ç»“æ„åŒ–è§£æå¤±è´¥æˆ–éœ€è¦æ›´æ¿€è¿›çš„å‹ç¼©æ—¶ä½¿ç”¨ã€‚

    Args:
    ----
        diff_text: åŸå§‹ git diff æ–‡æœ¬
        max_lines: æœ€å¤šä¿ç•™çš„è¡Œæ•°

    Returns:
    -------
        è¡Œå‹ç¼©åçš„ diff æ–‡æœ¬

    """
    lines = diff_text.splitlines()
    compressed = []
    current_file = None

    for line in lines:
        # è¯†åˆ«æ–‡ä»¶å¤´
        if line.startswith("diff --git"):
            match = re.search(r"diff --git a/(.+?) b/", line)
            if match:
                current_file = match.group(1)
                compressed.append(f"\nğŸ“„ {current_file}")

        # æå–å˜æ›´è¡Œ
        elif line.startswith("+") and not line.startswith("+++"):
            compressed.append(f"  + {line[1:].strip()}")
        elif line.startswith("-") and not line.startswith("---"):
            compressed.append(f"  - {line[1:].strip()}")

        # è¾¾åˆ°è¡Œæ•°é™åˆ¶
        if len(compressed) >= max_lines:
            compressed.append("\n...<truncated>")
            break

    return "\n".join(compressed)


# ============================================================================
# ç¬¬å››éƒ¨åˆ†ï¼šä¸»å…¥å£å‡½æ•°
# ============================================================================


def summary_and_tokens_checker(
    diff_text: str, max_output_tokens: int, model_name: str, provider: str = "openai"
) -> str:
    """æ™ºèƒ½ Diff å‹ç¼©çš„ä¸»å…¥å£å‡½æ•°.

    ä¸‰çº§å‹ç¼©ç­–ç•¥ï¼š
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ ç­–ç•¥1: åŸå§‹ diff                â”‚
    â”‚ æ¡ä»¶: token_count â‰¤ limit      â”‚
    â”‚ ä¼˜ç‚¹: ä¿ç•™å®Œæ•´ä¿¡æ¯              â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“ (è¶…å‡ºé™åˆ¶)
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ ç­–ç•¥2: ç»“æ„åŒ–å‹ç¼©               â”‚
    â”‚ æ–¹æ³•: ä¼˜å…ˆçº§æ’åº + é€‰æ‹©æ€§ä¿ç•™  â”‚
    â”‚ ä¼˜ç‚¹: ä¿ç•™é‡è¦æ–‡ä»¶å’Œä¸Šä¸‹æ–‡      â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“ (ä»è¶…å‡ºé™åˆ¶)
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ ç­–ç•¥3: ç®€å•è¡Œå‹ç¼©               â”‚
    â”‚ æ–¹æ³•: æå– +/- è¡Œï¼Œé™åˆ¶è¡Œæ•°     â”‚
    â”‚ ä¼˜ç‚¹: æé™å‹ç¼©ï¼Œå¿…å®šæˆåŠŸ        â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    Args:
    ----
        diff_text: Git diff æ–‡æœ¬
        max_output_tokens: token ä¸Šé™
        model_name: æ¨¡å‹åç§°ï¼ˆç”¨äº token è®¡æ•°ï¼‰
        provider: LLM providerï¼ˆopenai/gemini/ollama/openrouterï¼‰

    Returns:
    -------
        å¤„ç†åçš„ diff æ–‡æœ¬ï¼ˆç¡®ä¿åœ¨ token é™åˆ¶å†…ï¼‰

    """
    # ç­–ç•¥1ï¼šæ£€æŸ¥åŸå§‹ diff æ˜¯å¦æ»¡è¶³é™åˆ¶
    original_tokens = count_tokens(diff_text, model_name, provider)
    if original_tokens <= max_output_tokens:
        return diff_text

    # ç­–ç•¥2ï¼šç»“æ„åŒ–å‹ç¼©
    compressed = compress_with_structure(diff_text, max_output_tokens, model_name, provider)
    compressed_tokens = count_tokens(compressed, model_name, provider)

    if compressed_tokens <= max_output_tokens:
        return compressed

    # ç­–ç•¥3ï¼šç®€å•è¡Œå‹ç¼©ï¼ˆfallbackï¼‰
    # ä¼°ç®—å¯ä»¥ä¿ç•™çš„è¡Œæ•°
    avg_tokens_per_line = compressed_tokens / max(len(compressed.splitlines()), 1)
    safe_lines = int(max_output_tokens / avg_tokens_per_line * 0.8)  # 80% å®‰å…¨è¾¹ç•Œ
    safe_lines = max(safe_lines, 50)  # è‡³å°‘ä¿ç•™ 50 è¡Œ

    fallback = compress_with_lines(diff_text, max_lines=safe_lines)

    # æ·»åŠ è­¦å‘Šä¿¡æ¯
    if len(diff_text) > MAX_DIFF_LENGTH:
        warning = (
            f"âš ï¸ Diff too long ({len(diff_text)} characters), "
            "it is recommended to submit in batches or simplify changesã€‚\n\n"
        )
        return warning + fallback

    return fallback
