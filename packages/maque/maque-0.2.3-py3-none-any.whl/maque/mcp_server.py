"""
maque MCP Server - API æ–‡æ¡£æŸ¥è¯¢ + LLM è°ƒç”¨æœåŠ¡

æä¾›ä¸¤å¤§åŠŸèƒ½ï¼š
1. API æ–‡æ¡£æŸ¥è¯¢ï¼šè®© AI Agent æŸ¥è¯¢ maque çš„å¯ç”¨åŠŸèƒ½
2. LLM è°ƒç”¨ï¼šç›´æ¥è°ƒç”¨é…ç½®å¥½çš„ LLM è¿›è¡Œæ¨ç†

å¯åŠ¨æ–¹å¼:
    python -m maque.mcp_server

é…ç½® Claude Code (~/.claude.json):
    {
        "mcpServers": {
            "maque": {
                "command": "python",
                "args": ["-m", "maque.mcp_server"]
            }
        }
    }
"""

import ast
import asyncio
import inspect
import pkgutil
import importlib
from pathlib import Path
from typing import Optional, List, Dict, Any
from dataclasses import dataclass

from mcp.server import Server
from mcp.server.stdio import stdio_server
from mcp.types import TextContent, Tool

# æ ¸å¿ƒæ¨¡å—åˆ—è¡¨ï¼ˆæŒ‰é‡è¦æ€§æ’åºï¼‰- åªä¿ç•™æ¨¡å—åï¼Œå…¶ä»–ä¿¡æ¯è‡ªåŠ¨æå–
CORE_MODULES = [
    "mllm",       # LLM/MLLM å®¢æˆ·ç«¯
    "embedding",  # æ–‡æœ¬/å¤šæ¨¡æ€ Embedding
    "async_api",  # å¼‚æ­¥å¹¶å‘æ‰§è¡Œ
    "retriever",  # RAG æ£€ç´¢
    "clustering", # èšç±»åˆ†æ
    "io",         # æ–‡ä»¶ IO
    "performance",# æ€§èƒ½ç›‘æ§
    "llm",        # LLM æ¨ç†æœåŠ¡
]


def get_module_exports(module_name: str) -> list[str]:
    """ä»æ¨¡å—çš„ __init__.py è‡ªåŠ¨è·å– __all__ å¯¼å‡ºåˆ—è¡¨"""
    root = get_maque_root()
    init_file = root / module_name / "__init__.py"

    if not init_file.exists():
        return []

    try:
        source = init_file.read_text(encoding='utf-8')
        tree = ast.parse(source)

        for node in ast.walk(tree):
            if isinstance(node, ast.Assign):
                for target in node.targets:
                    if isinstance(target, ast.Name) and target.id == "__all__":
                        if isinstance(node.value, ast.List):
                            return [
                                elt.value for elt in node.value.elts
                                if isinstance(elt, ast.Constant) and isinstance(elt.value, str)
                            ]
        return []
    except Exception:
        return []


def get_module_docstring(module_name: str) -> tuple[str, str]:
    """
    ä»æ¨¡å—çš„ __init__.py è‡ªåŠ¨è·å– docstring

    Returns:
        (description, example): æè¿°å’Œç¤ºä¾‹ä»£ç 
    """
    root = get_maque_root()
    init_file = root / module_name / "__init__.py"

    if not init_file.exists():
        return ("", "")

    try:
        source = init_file.read_text(encoding='utf-8')
        tree = ast.parse(source)
        docstring = ast.get_docstring(tree) or ""

        # åˆ†ç¦»æè¿°å’Œç¤ºä¾‹
        if "Example:" in docstring:
            parts = docstring.split("Example:", 1)
            description = parts[0].strip()
            example = parts[1].strip() if len(parts) > 1 else ""
        else:
            description = docstring.split("\n\n")[0].strip()  # å–ç¬¬ä¸€æ®µä½œä¸ºæè¿°
            example = ""

        return (description, example)
    except Exception:
        return ("", "")


@dataclass
class APIInfo:
    """API ä¿¡æ¯"""
    name: str
    module: str
    type: str  # 'class' | 'function' | 'module'
    signature: str
    docstring: str
    example: str = ""
    methods: list = None  # ç±»çš„ä¸»è¦æ–¹æ³•åˆ—è¡¨


def get_maque_root() -> Path:
    """è·å– maque åŒ…çš„æ ¹ç›®å½•"""
    import maque
    return Path(maque.__file__).parent


def extract_docstring_from_source(file_path: Path, target_name: str) -> Optional[str]:
    """ä»æºç ä¸­æå–æŒ‡å®šç±»æˆ–å‡½æ•°çš„ docstringï¼ˆä¸å¯¼å…¥æ¨¡å—ï¼‰"""
    try:
        source = file_path.read_text(encoding='utf-8')
        tree = ast.parse(source)

        for node in ast.walk(tree):
            if isinstance(node, (ast.ClassDef, ast.FunctionDef, ast.AsyncFunctionDef)):
                if node.name == target_name:
                    return ast.get_docstring(node)
        return None
    except Exception:
        return None


def extract_class_info_from_source(file_path: Path, class_name: str) -> Optional[APIInfo]:
    """ä»æºç æå–ç±»ä¿¡æ¯ï¼ˆåŒ…æ‹¬æ–¹æ³•åˆ—è¡¨ï¼‰"""
    try:
        source = file_path.read_text(encoding='utf-8')
        tree = ast.parse(source)

        for node in ast.walk(tree):
            if isinstance(node, ast.ClassDef) and node.name == class_name:
                docstring = ast.get_docstring(node) or "æ— æ–‡æ¡£"

                # æå– __init__ ç­¾å
                init_sig = ""
                methods = []
                for item in node.body:
                    if isinstance(item, ast.FunctionDef) and item.name == "__init__":
                        args = []
                        for arg in item.args.args[1:]:  # è·³è¿‡ self
                            arg_str = arg.arg
                            if arg.annotation:
                                arg_str += f": {ast.unparse(arg.annotation)}"
                            args.append(arg_str)

                        # å¤„ç†é»˜è®¤å€¼
                        defaults = item.args.defaults
                        num_defaults = len(defaults)
                        num_args = len(args)
                        for i, default in enumerate(defaults):
                            arg_idx = num_args - num_defaults + i
                            try:
                                default_val = ast.unparse(default)
                                args[arg_idx] += f" = {default_val}"
                            except Exception:
                                pass

                        init_sig = f"({', '.join(args)})"

                    # æå–å…¬å¼€æ–¹æ³•ï¼ˆä¸ä»¥ _ å¼€å¤´ï¼Œæ’é™¤ __init__ ç­‰ï¼‰
                    elif isinstance(item, (ast.FunctionDef, ast.AsyncFunctionDef)):
                        if not item.name.startswith('_'):
                            method_doc = ast.get_docstring(item) or ""
                            method_desc = method_doc.split('\n')[0] if method_doc else ""
                            # æ„å»ºæ–¹æ³•ç­¾å
                            method_args = []
                            for arg in item.args.args:
                                if arg.arg in ('self', 'cls'):
                                    continue
                                method_args.append(arg.arg)
                            prefix = "async " if isinstance(item, ast.AsyncFunctionDef) else ""
                            methods.append({
                                "name": item.name,
                                "signature": f"{prefix}{item.name}({', '.join(method_args)})",
                                "description": method_desc[:100] if len(method_desc) > 100 else method_desc,
                            })

                return APIInfo(
                    name=class_name,
                    module=str(file_path.relative_to(get_maque_root().parent)).replace('/', '.').replace('.py', ''),
                    type='class',
                    signature=f"class {class_name}{init_sig}",
                    docstring=docstring[:1000] if len(docstring) > 1000 else docstring,
                    methods=methods[:10] if methods else None,  # æœ€å¤š 10 ä¸ªæ–¹æ³•
                )
        return None
    except Exception as e:
        return None


def search_in_module(module_path: Path, keyword: str) -> list[APIInfo]:
    """åœ¨æ¨¡å—ä¸­æœç´¢å…³é”®è¯"""
    results = []
    keyword_lower = keyword.lower()

    try:
        source = module_path.read_text(encoding='utf-8')
        tree = ast.parse(source)

        for node in ast.walk(tree):
            if isinstance(node, ast.ClassDef):
                docstring = ast.get_docstring(node) or ""
                if keyword_lower in node.name.lower() or keyword_lower in docstring.lower():
                    info = extract_class_info_from_source(module_path, node.name)
                    if info:
                        results.append(info)

            elif isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef)):
                # è·³è¿‡ç§æœ‰å‡½æ•°
                if node.name.startswith('_'):
                    continue
                docstring = ast.get_docstring(node) or ""
                if keyword_lower in node.name.lower() or keyword_lower in docstring.lower():
                    # æ„å»ºç­¾å
                    args = []
                    for arg in node.args.args:
                        arg_str = arg.arg
                        if arg.annotation:
                            try:
                                arg_str += f": {ast.unparse(arg.annotation)}"
                            except Exception:
                                pass
                        args.append(arg_str)

                    prefix = "async def" if isinstance(node, ast.AsyncFunctionDef) else "def"
                    sig = f"{prefix} {node.name}({', '.join(args)})"

                    results.append(APIInfo(
                        name=node.name,
                        module=str(module_path.relative_to(get_maque_root().parent)).replace('/', '.').replace('.py', ''),
                        type='function',
                        signature=sig,
                        docstring=docstring[:300] if len(docstring) > 300 else docstring,
                    ))
    except Exception:
        pass

    return results


def search_maque(keyword: str) -> list[APIInfo]:
    """æœç´¢ maque ä¸­çš„ API"""
    results = []
    root = get_maque_root()

    for py_file in root.rglob("*.py"):
        # è·³è¿‡æµ‹è¯•å’Œç§æœ‰æ¨¡å—
        if '__pycache__' in str(py_file) or py_file.name.startswith('_'):
            continue
        results.extend(search_in_module(py_file, keyword))

    return results[:20]  # é™åˆ¶ç»“æœæ•°é‡


def get_module_info(module_name: str) -> str:
    """è·å–æ¨¡å—çš„è¯¦ç»†ä½¿ç”¨è¯´æ˜ï¼ˆè‡ªåŠ¨ä» docstring æå–ï¼‰"""
    description, example = get_module_docstring(module_name)

    if example:
        return example

    if description:
        return description

    return f"æ¨¡å— {module_name} æš‚æ— è¯¦ç»†ä½¿ç”¨ç¤ºä¾‹ã€‚è¯·ä½¿ç”¨ search_maque_api æœç´¢å…·ä½“åŠŸèƒ½ã€‚"


def list_all_modules() -> str:
    """åˆ—å‡ºæ‰€æœ‰æ ¸å¿ƒæ¨¡å—ï¼ˆè‡ªåŠ¨ä»ä»£ç æå–ï¼‰"""
    lines = ["# maque æ ¸å¿ƒæ¨¡å—\n"]

    for module_name in CORE_MODULES:
        description, _ = get_module_docstring(module_name)
        exports = get_module_exports(module_name)

        # å–æè¿°çš„ç¬¬ä¸€è¡Œ
        desc_line = description.split('\n')[0] if description else "æ— æè¿°"

        lines.append(f"## {module_name}")
        lines.append(f"  {desc_line}")
        if exports:
            # åªæ˜¾ç¤ºå‰ 5 ä¸ªå¯¼å‡º
            display_exports = exports[:5]
            suffix = f" ... (+{len(exports)-5})" if len(exports) > 5 else ""
            lines.append(f"  ä¸»è¦å¯¼å‡º: {', '.join(display_exports)}{suffix}")
        lines.append("")

    lines.append("\nä½¿ç”¨ `get_module_usage(module_name)` è·å–è¯¦ç»†ç”¨æ³•")
    return "\n".join(lines)


# CLI å‘½ä»¤ç»„æ˜ å°„
CLI_GROUPS = {
    "config": "é…ç½®ç®¡ç†",
    "mllm": "å¤šæ¨¡æ€ LLM æ“ä½œ",
    "llm": "LLM æ¨ç†æœåŠ¡",
    "data": "æ•°æ®å¤„ç†å·¥å…·",
    "embedding": "Embedding æœåŠ¡",
    "system": "ç³»ç»Ÿå·¥å…·",
    "git": "Git è¾…åŠ©å‘½ä»¤",
    "service": "æœåŠ¡ç®¡ç†",
    "doctor": "è¯Šæ–­å·¥å…·",
    "mcp": "MCP æœåŠ¡",
}


@dataclass
class CLICommand:
    """CLI å‘½ä»¤ä¿¡æ¯"""
    name: str
    group: str  # ç©ºå­—ç¬¦ä¸²è¡¨ç¤ºé¡¶çº§å‘½ä»¤
    description: str
    signature: str


def extract_cli_commands() -> list[CLICommand]:
    """æå–æ‰€æœ‰ CLI å‘½ä»¤"""
    commands = []
    root = get_maque_root()

    # 1. æå–é¡¶çº§å‘½ä»¤ï¼ˆä» __main__.py çš„ NewCli ç±»ï¼‰
    main_file = root / "__main__.py"
    if main_file.exists():
        commands.extend(_extract_commands_from_class(main_file, "NewCli", ""))

    # 2. æå–åˆ†ç»„å‘½ä»¤ï¼ˆä» cli/groups/*.pyï¼‰
    groups_dir = root / "cli" / "groups"
    if groups_dir.exists():
        for py_file in groups_dir.glob("*.py"):
            if py_file.name.startswith("_"):
                continue
            # ä»æ–‡ä»¶åæ¨æ–­ group åç§°
            group_name = py_file.stem
            if group_name == "mllm_simple":
                continue  # è·³è¿‡ç®€åŒ–ç‰ˆ
            commands.extend(_extract_commands_from_file(py_file, group_name))

    return commands


def _extract_commands_from_class(file_path: Path, class_name: str, group: str) -> list[CLICommand]:
    """ä»æŒ‡å®šç±»ä¸­æå–å‘½ä»¤"""
    commands = []
    try:
        source = file_path.read_text(encoding='utf-8')
        tree = ast.parse(source)

        for node in ast.walk(tree):
            if isinstance(node, ast.ClassDef) and node.name == class_name:
                for item in node.body:
                    if isinstance(item, (ast.FunctionDef, ast.AsyncFunctionDef)):
                        # è·³è¿‡ç§æœ‰æ–¹æ³•å’Œç‰¹æ®Šæ–¹æ³•
                        if item.name.startswith('_'):
                            continue
                        # è·³è¿‡å±æ€§æ–¹æ³•ï¼ˆæ²¡æœ‰å®é™…åŠŸèƒ½ï¼‰
                        if any(isinstance(d, ast.Name) and d.id == 'property' for d in item.decorator_list):
                            continue

                        docstring = ast.get_docstring(item) or "æ— æè¿°"
                        # åªå– docstring ç¬¬ä¸€è¡Œ
                        desc = docstring.split('\n')[0].strip()

                        # æ„å»ºç­¾å
                        args = []
                        for arg in item.args.args:
                            if arg.arg in ('self', 'cls'):
                                continue
                            args.append(arg.arg)
                        sig = f"({', '.join(args)})" if args else "()"

                        commands.append(CLICommand(
                            name=item.name,
                            group=group,
                            description=desc,
                            signature=sig,
                        ))
    except Exception:
        pass
    return commands


def _extract_commands_from_file(file_path: Path, group_name: str) -> list[CLICommand]:
    """ä»æ–‡ä»¶ä¸­æå– Group ç±»çš„å‘½ä»¤"""
    commands = []
    try:
        source = file_path.read_text(encoding='utf-8')
        tree = ast.parse(source)

        for node in ast.walk(tree):
            if isinstance(node, ast.ClassDef) and node.name.endswith('Group'):
                commands.extend(_extract_commands_from_class(file_path, node.name, group_name))
                break  # æ¯ä¸ªæ–‡ä»¶åªå¤„ç†ä¸€ä¸ª Group ç±»
    except Exception:
        pass
    return commands


def list_cli_commands() -> str:
    """åˆ—å‡ºæ‰€æœ‰ CLI å‘½ä»¤"""
    commands = extract_cli_commands()

    # åˆ†ç»„æ˜¾ç¤º
    top_level = [c for c in commands if not c.group]
    grouped = {}
    for c in commands:
        if c.group:
            if c.group not in grouped:
                grouped[c.group] = []
            grouped[c.group].append(c)

    lines = ["# maque CLI å‘½ä»¤\n"]

    # é¡¶çº§å‘½ä»¤
    if top_level:
        lines.append("## é¡¶çº§å‘½ä»¤")
        lines.append("ç”¨æ³•: `maque <command> [args]`\n")
        for cmd in sorted(top_level, key=lambda x: x.name):
            lines.append(f"- **{cmd.name}**{cmd.signature}: {cmd.description}")
        lines.append("")

    # åˆ†ç»„å‘½ä»¤
    lines.append("## åˆ†ç»„å‘½ä»¤")
    lines.append("ç”¨æ³•: `maque <group> <command> [args]`\n")

    for group_name in sorted(grouped.keys()):
        group_desc = CLI_GROUPS.get(group_name, "")
        lines.append(f"### {group_name}" + (f" - {group_desc}" if group_desc else ""))
        for cmd in sorted(grouped[group_name], key=lambda x: x.name):
            lines.append(f"- **{cmd.name}**{cmd.signature}: {cmd.description}")
        lines.append("")

    return "\n".join(lines)


# =============================================================================
# LLM-Friendly APIï¼ˆä¸“ä¸º AI Agent ä¼˜åŒ–çš„ç®€åŒ–æ¥å£ï¼‰
# =============================================================================

async def ask(question: str, context: str = None) -> str:
    """
    æœ€ç®€å•çš„é—®ç­”æ¥å£ - ä¸“ä¸º LLM Agent è®¾è®¡

    Args:
        question: é—®é¢˜ï¼ˆçº¯æ–‡æœ¬å³å¯ï¼‰
        context: å¯é€‰çš„ä¸Šä¸‹æ–‡ä¿¡æ¯

    Returns:
        å›ç­”æ–‡æœ¬
    """
    client = _get_llm_client()

    if context:
        content = f"ä¸Šä¸‹æ–‡:\n{context}\n\né—®é¢˜: {question}"
    else:
        content = question

    return await client.chat_completions(
        messages=[{"role": "user", "content": content}]
    )


async def ask_batch(questions: List[str]) -> List[str]:
    """
    æ‰¹é‡é—®ç­” - æ¥å—ç®€å•çš„é—®é¢˜åˆ—è¡¨ï¼Œè€Œä¸æ˜¯åµŒå¥—çš„ messages ç»“æ„

    Args:
        questions: é—®é¢˜åˆ—è¡¨ï¼ˆç®€å•å­—ç¬¦ä¸²åˆ—è¡¨ï¼‰

    Returns:
        å›ç­”åˆ—è¡¨ï¼ˆä¸é—®é¢˜ä¸€ä¸€å¯¹åº”ï¼‰
    """
    client = _get_llm_client()
    messages_list = [[{"role": "user", "content": q}] for q in questions]
    return await client.chat_completions_batch(messages_list, show_progress=False)


async def extract_json(text: str, schema_desc: str = None) -> Dict[str, Any]:
    """
    ä»æ–‡æœ¬ä¸­æå– JSON ç»“æ„

    Args:
        text: è¦å¤„ç†çš„æ–‡æœ¬
        schema_desc: æœŸæœ›çš„ JSON ç»“æ„æè¿°ï¼ˆå¦‚ "name, age, email"ï¼‰

    Returns:
        è§£æåçš„ dictï¼ˆå¦‚æœè§£æå¤±è´¥åˆ™è¿”å› {"error": "...", "raw": "..."}ï¼‰
    """
    import json
    import re

    client = _get_llm_client()

    if schema_desc:
        prompt = f"ä»ä»¥ä¸‹æ–‡æœ¬ä¸­æå–ä¿¡æ¯ï¼Œä»¥ JSON æ ¼å¼è¾“å‡ºï¼ŒåŒ…å«å­—æ®µ: {schema_desc}\nåªè¾“å‡º JSONï¼Œä¸è¦å…¶ä»–å†…å®¹:\n\n{text}"
    else:
        prompt = f"ä»ä»¥ä¸‹æ–‡æœ¬ä¸­æå–ç»“æ„åŒ–ä¿¡æ¯ï¼Œä»¥ JSON æ ¼å¼è¾“å‡ºï¼Œä¸è¦å…¶ä»–å†…å®¹:\n\n{text}"

    result = await client.chat_completions([{"role": "user", "content": prompt}])

    # å°è¯•è§£æ JSON
    try:
        # å°è¯•æ‰¾åˆ° JSON å—
        json_match = re.search(r'```(?:json)?\s*([\s\S]*?)\s*```', result)
        if json_match:
            return json.loads(json_match.group(1))
        return json.loads(result)
    except json.JSONDecodeError:
        return {"error": "JSON è§£æå¤±è´¥", "raw": result}


def get_capabilities() -> Dict[str, Any]:
    """
    è·å–å½“å‰ LLM çš„èƒ½åŠ›ä¿¡æ¯ - å¸®åŠ© Agent å†³ç­–

    Returns:
        æ¨¡å‹èƒ½åŠ›å­—å…¸ï¼ŒåŒ…å«æ¨¡å‹åã€æ˜¯å¦æ”¯æŒ JSON æ¨¡å¼ç­‰
    """
    config = _load_llm_config()
    model = config.get("model", "unknown")

    # æ ¹æ®æ¨¡å‹åæ¨æ–­èƒ½åŠ›
    model_lower = model.lower()

    capabilities = {
        "model": model,
        "base_url": config.get("base_url", ""),
        "supports_json_mode": any(x in model_lower for x in ["gpt-4", "gpt-3.5", "gemini", "qwen"]),
        "supports_vision": any(x in model_lower for x in ["vision", "vl", "gpt-4o", "gemini"]),
        "supports_thinking": any(x in model_lower for x in ["o1", "o3", "deepseek-r1", "gemini-2"]),
        "max_context_estimate": 128000 if "gpt-4" in model_lower or "gemini" in model_lower else 32000,
    }

    return capabilities


# =============================================================================
# LLM å®¢æˆ·ç«¯åŠŸèƒ½ï¼ˆåŸæœ‰æ¥å£ä¿ç•™ï¼‰
# =============================================================================

def _load_llm_config() -> Dict[str, Any]:
    """åŠ è½½ LLM é…ç½®ï¼ˆä» maque é…ç½®æ–‡ä»¶ï¼‰"""
    from maque import yaml_load

    # é…ç½®æœç´¢è·¯å¾„
    search_paths = [
        Path.cwd() / "maque_config.yaml",
        Path.home() / ".maque" / "config.yaml",
    ]

    # æ£€æŸ¥é¡¹ç›®æ ¹ç›®å½•
    current = Path.cwd()
    while current != current.parent:
        if (current / ".git").exists() or (current / "pyproject.toml").exists():
            project_config = current / "maque_config.yaml"
            if project_config not in search_paths:
                search_paths.insert(1, project_config)
            break
        current = current.parent

    # é»˜è®¤é…ç½®
    default_config = {
        "base_url": "http://localhost:11434/v1",
        "api_key": "EMPTY",
        "model": "gemma3:4b",
    }

    for path in search_paths:
        if path.exists():
            try:
                config = yaml_load(str(path))
                if config and "mllm" in config:
                    mllm_config = config["mllm"]
                    return {
                        "base_url": mllm_config.get("base_url", default_config["base_url"]),
                        "api_key": mllm_config.get("api_key", default_config["api_key"]),
                        "model": mllm_config.get("model", default_config["model"]),
                    }
            except Exception:
                continue

    return default_config


def _get_llm_client():
    """è·å– LLMClient å®ä¾‹"""
    from flexllm import LLMClient
    from flexllm.response_cache import ResponseCacheConfig

    config = _load_llm_config()
    return LLMClient(
        base_url=config["base_url"],
        api_key=config["api_key"],
        model=config["model"],
        cache=ResponseCacheConfig(enabled=False),  # MCP æœåŠ¡ä¸éœ€è¦å“åº”ç¼“å­˜
    )


async def llm_chat(
    messages: List[Dict[str, str]],
    model: str = None,
    max_tokens: int = None,
    temperature: float = None,
) -> str:
    """
    è°ƒç”¨ LLM è¿›è¡Œå•æ¡èŠå¤©

    Args:
        messages: æ¶ˆæ¯åˆ—è¡¨ï¼Œæ ¼å¼ä¸º [{"role": "user", "content": "..."}]
        model: æ¨¡å‹åç§°ï¼ˆå¯é€‰ï¼Œä½¿ç”¨é…ç½®é»˜è®¤å€¼ï¼‰
        max_tokens: æœ€å¤§ç”Ÿæˆ token æ•°
        temperature: æ¸©åº¦å‚æ•°

    Returns:
        LLM ç”Ÿæˆçš„å›å¤
    """
    client = _get_llm_client()

    kwargs = {}
    if max_tokens:
        kwargs["max_tokens"] = max_tokens
    if temperature is not None:
        kwargs["temperature"] = temperature

    result = await client.chat_completions(
        messages=messages,
        model=model,
        **kwargs,
    )
    return result


async def llm_chat_batch(
    messages_list: List[List[Dict[str, str]]],
    model: str = None,
    max_tokens: int = None,
    temperature: float = None,
) -> List[str]:
    """
    æ‰¹é‡è°ƒç”¨ LLM

    Args:
        messages_list: æ¶ˆæ¯åˆ—è¡¨çš„åˆ—è¡¨
        model: æ¨¡å‹åç§°
        max_tokens: æœ€å¤§ç”Ÿæˆ token æ•°
        temperature: æ¸©åº¦å‚æ•°

    Returns:
        LLM ç”Ÿæˆçš„å›å¤åˆ—è¡¨
    """
    client = _get_llm_client()

    kwargs = {}
    if max_tokens:
        kwargs["max_tokens"] = max_tokens
    if temperature is not None:
        kwargs["temperature"] = temperature

    results = await client.chat_completions_batch(
        messages_list=messages_list,
        model=model,
        show_progress=False,
        **kwargs,
    )
    return results


def llm_models() -> List[str]:
    """è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨"""
    client = _get_llm_client()
    return client.model_list()


def llm_config() -> Dict[str, Any]:
    """è·å–å½“å‰ LLM é…ç½®"""
    config = _load_llm_config()
    # éšè— API key çš„éƒ¨åˆ†å†…å®¹
    api_key = config.get("api_key", "")
    if api_key and len(api_key) > 8:
        config["api_key"] = api_key[:4] + "****" + api_key[-4:]
    return config


# åˆ›å»º MCP Server
server = Server("maque-docs")


@server.list_tools()
async def list_tools() -> list[Tool]:
    """åˆ—å‡ºå¯ç”¨å·¥å…·"""
    return [
        # ===== API æ–‡æ¡£æŸ¥è¯¢å·¥å…· =====
        Tool(
            name="search_maque_api",
            description="æœç´¢ maque åº“ä¸­çš„ APIï¼ˆç±»ã€å‡½æ•°ã€æ¨¡å—ï¼‰ã€‚ç”¨äºæŸ¥æ‰¾å¯å¤ç”¨çš„åŠŸèƒ½ï¼Œé¿å…é‡å¤é€ è½®å­ã€‚",
            inputSchema={
                "type": "object",
                "properties": {
                    "keyword": {
                        "type": "string",
                        "description": "æœç´¢å…³é”®è¯ï¼Œå¦‚ 'embedding', 'llm', 'async', 'retry' ç­‰"
                    }
                },
                "required": ["keyword"]
            }
        ),
        Tool(
            name="get_module_usage",
            description="è·å– maque æŒ‡å®šæ¨¡å—çš„è¯¦ç»†ä½¿ç”¨ç¤ºä¾‹ã€‚",
            inputSchema={
                "type": "object",
                "properties": {
                    "module": {
                        "type": "string",
                        "description": "æ¨¡å—åç§°ï¼Œå¦‚ 'mllm', 'embedding', 'async_api', 'io', 'retriever', 'clustering'"
                    }
                },
                "required": ["module"]
            }
        ),
        Tool(
            name="list_maque_modules",
            description="åˆ—å‡º maque æ‰€æœ‰æ ¸å¿ƒæ¨¡å—åŠå…¶åŠŸèƒ½æ¦‚è¿°ã€‚",
            inputSchema={
                "type": "object",
                "properties": {}
            }
        ),
        Tool(
            name="list_cli_commands",
            description="åˆ—å‡º maque æ‰€æœ‰å¯ç”¨çš„ CLI å‘½ä»¤ï¼ŒåŒ…æ‹¬é¡¶çº§å‘½ä»¤å’Œåˆ†ç»„å‘½ä»¤ã€‚",
            inputSchema={
                "type": "object",
                "properties": {}
            }
        ),
        # ===== LLM è°ƒç”¨å·¥å…· =====
        Tool(
            name="llm_chat",
            description="è°ƒç”¨ LLM è¿›è¡Œå•æ¡èŠå¤©ã€‚ä½¿ç”¨ maque é…ç½®æ–‡ä»¶ä¸­çš„ LLM è®¾ç½®ã€‚",
            inputSchema={
                "type": "object",
                "properties": {
                    "messages": {
                        "type": "array",
                        "description": "æ¶ˆæ¯åˆ—è¡¨ï¼Œæ ¼å¼ä¸º [{\"role\": \"user\", \"content\": \"...\"}]",
                        "items": {
                            "type": "object",
                            "properties": {
                                "role": {"type": "string", "enum": ["system", "user", "assistant"]},
                                "content": {"type": "string"}
                            },
                            "required": ["role", "content"]
                        }
                    },
                    "model": {
                        "type": "string",
                        "description": "æ¨¡å‹åç§°ï¼ˆå¯é€‰ï¼Œä½¿ç”¨é…ç½®é»˜è®¤å€¼ï¼‰"
                    },
                    "max_tokens": {
                        "type": "integer",
                        "description": "æœ€å¤§ç”Ÿæˆ token æ•°"
                    },
                    "temperature": {
                        "type": "number",
                        "description": "æ¸©åº¦å‚æ•° (0-2)"
                    }
                },
                "required": ["messages"]
            }
        ),
        Tool(
            name="llm_chat_batch",
            description="æ‰¹é‡è°ƒç”¨ LLMï¼Œé€‚åˆå¤„ç†å¤šä¸ªç‹¬ç«‹è¯·æ±‚ã€‚",
            inputSchema={
                "type": "object",
                "properties": {
                    "messages_list": {
                        "type": "array",
                        "description": "æ¶ˆæ¯åˆ—è¡¨çš„åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ æ˜¯ä¸€ä¸ªå®Œæ•´çš„å¯¹è¯",
                        "items": {
                            "type": "array",
                            "items": {
                                "type": "object",
                                "properties": {
                                    "role": {"type": "string"},
                                    "content": {"type": "string"}
                                }
                            }
                        }
                    },
                    "model": {
                        "type": "string",
                        "description": "æ¨¡å‹åç§°"
                    },
                    "max_tokens": {
                        "type": "integer",
                        "description": "æœ€å¤§ç”Ÿæˆ token æ•°"
                    },
                    "temperature": {
                        "type": "number",
                        "description": "æ¸©åº¦å‚æ•°"
                    }
                },
                "required": ["messages_list"]
            }
        ),
        Tool(
            name="llm_models",
            description="è·å–å¯ç”¨çš„ LLM æ¨¡å‹åˆ—è¡¨ã€‚",
            inputSchema={
                "type": "object",
                "properties": {}
            }
        ),
        Tool(
            name="llm_config",
            description="è·å–å½“å‰ LLM é…ç½®ä¿¡æ¯ï¼ˆbase_url, model ç­‰ï¼‰ã€‚",
            inputSchema={
                "type": "object",
                "properties": {}
            }
        ),
        # ===== LLM-Friendly APIï¼ˆä¸“ä¸º AI Agent ä¼˜åŒ–ï¼‰=====
        Tool(
            name="ask",
            description="æœ€ç®€å•çš„é—®ç­”æ¥å£ã€‚ç›´æ¥ä¼ å…¥é—®é¢˜å­—ç¬¦ä¸²ï¼Œæ— éœ€æ„é€  messages æ•°ç»„ã€‚",
            inputSchema={
                "type": "object",
                "properties": {
                    "question": {
                        "type": "string",
                        "description": "é—®é¢˜ï¼ˆçº¯æ–‡æœ¬ï¼‰"
                    },
                    "context": {
                        "type": "string",
                        "description": "å¯é€‰çš„ä¸Šä¸‹æ–‡ä¿¡æ¯"
                    }
                },
                "required": ["question"]
            }
        ),
        Tool(
            name="ask_batch",
            description="æ‰¹é‡é—®ç­”ã€‚æ¥å—ç®€å•çš„é—®é¢˜åˆ—è¡¨ï¼Œæ— éœ€åµŒå¥— messages ç»“æ„ã€‚",
            inputSchema={
                "type": "object",
                "properties": {
                    "questions": {
                        "type": "array",
                        "items": {"type": "string"},
                        "description": "é—®é¢˜åˆ—è¡¨ï¼ˆç®€å•å­—ç¬¦ä¸²åˆ—è¡¨ï¼‰"
                    }
                },
                "required": ["questions"]
            }
        ),
        Tool(
            name="extract_json",
            description="ä»æ–‡æœ¬ä¸­æå–ç»“æ„åŒ– JSON æ•°æ®ã€‚è‡ªåŠ¨è§£æï¼Œè¿”å› dict è€Œéå­—ç¬¦ä¸²ã€‚",
            inputSchema={
                "type": "object",
                "properties": {
                    "text": {"type": "string", "description": "è¦å¤„ç†çš„æ–‡æœ¬"},
                    "schema_desc": {"type": "string", "description": "æœŸæœ›çš„å­—æ®µï¼Œå¦‚ 'name, age, email'"}
                },
                "required": ["text"]
            }
        ),
        Tool(
            name="get_capabilities",
            description="è·å–å½“å‰ LLM çš„èƒ½åŠ›ä¿¡æ¯ã€‚å¸®åŠ© Agent äº†è§£æ¨¡å‹æ”¯æŒä»€ä¹ˆåŠŸèƒ½ï¼ˆvision, thinking ç­‰ï¼‰ã€‚",
            inputSchema={
                "type": "object",
                "properties": {}
            }
        ),
    ]


@server.call_tool()
async def call_tool(name: str, arguments: dict) -> list[TextContent]:
    """å¤„ç†å·¥å…·è°ƒç”¨"""

    # ===== API æ–‡æ¡£æŸ¥è¯¢å·¥å…· =====
    if name == "search_maque_api":
        keyword = arguments.get("keyword", "")
        results = search_maque(keyword)

        if not results:
            return [TextContent(type="text", text=f"æœªæ‰¾åˆ°ä¸ '{keyword}' ç›¸å…³çš„ API")]

        lines = [f"# æœç´¢ç»“æœ: '{keyword}'\n"]
        for api in results:
            lines.append(f"## {api.name}")
            lines.append(f"  æ¨¡å—: `{api.module}`")
            lines.append(f"  ç±»å‹: {api.type}")
            lines.append(f"  ç­¾å: `{api.signature}`")
            if api.docstring:
                lines.append(f"  è¯´æ˜: {api.docstring}")
            # å±•ç¤ºç±»çš„ä¸»è¦æ–¹æ³•
            if api.methods:
                lines.append(f"  ä¸»è¦æ–¹æ³•:")
                for method in api.methods:
                    desc = f" - {method['description']}" if method['description'] else ""
                    lines.append(f"    - `{method['signature']}`{desc}")
            lines.append("")

        return [TextContent(type="text", text="\n".join(lines))]

    elif name == "get_module_usage":
        module = arguments.get("module", "")
        usage = get_module_info(module)
        return [TextContent(type="text", text=f"# {module} æ¨¡å—ä½¿ç”¨ç¤ºä¾‹\n\n```python{usage}\n```")]

    elif name == "list_maque_modules":
        return [TextContent(type="text", text=list_all_modules())]

    elif name == "list_cli_commands":
        return [TextContent(type="text", text=list_cli_commands())]

    # ===== LLM è°ƒç”¨å·¥å…· =====
    elif name == "llm_chat":
        try:
            messages = arguments.get("messages", [])
            model = arguments.get("model")
            max_tokens = arguments.get("max_tokens")
            temperature = arguments.get("temperature")

            result = await llm_chat(
                messages=messages,
                model=model,
                max_tokens=max_tokens,
                temperature=temperature,
            )
            return [TextContent(type="text", text=result)]
        except Exception as e:
            return [TextContent(type="text", text=f"LLM è°ƒç”¨å¤±è´¥: {str(e)}")]

    elif name == "llm_chat_batch":
        try:
            messages_list = arguments.get("messages_list", [])
            model = arguments.get("model")
            max_tokens = arguments.get("max_tokens")
            temperature = arguments.get("temperature")

            results = await llm_chat_batch(
                messages_list=messages_list,
                model=model,
                max_tokens=max_tokens,
                temperature=temperature,
            )
            # æ ¼å¼åŒ–è¾“å‡º
            output_lines = ["# æ‰¹é‡è°ƒç”¨ç»“æœ\n"]
            for i, result in enumerate(results, 1):
                output_lines.append(f"## ç»“æœ {i}")
                output_lines.append(result)
                output_lines.append("")
            return [TextContent(type="text", text="\n".join(output_lines))]
        except Exception as e:
            return [TextContent(type="text", text=f"æ‰¹é‡ LLM è°ƒç”¨å¤±è´¥: {str(e)}")]

    elif name == "llm_models":
        try:
            models = llm_models()
            if models:
                lines = ["# å¯ç”¨æ¨¡å‹åˆ—è¡¨\n"]
                for model in models:
                    lines.append(f"- {model}")
                return [TextContent(type="text", text="\n".join(lines))]
            else:
                return [TextContent(type="text", text="æœªè·å–åˆ°æ¨¡å‹åˆ—è¡¨ï¼Œè¯·æ£€æŸ¥ LLM æœåŠ¡æ˜¯å¦æ­£å¸¸è¿è¡Œ")]
        except Exception as e:
            return [TextContent(type="text", text=f"è·å–æ¨¡å‹åˆ—è¡¨å¤±è´¥: {str(e)}")]

    elif name == "llm_config":
        try:
            config = llm_config()
            lines = ["# å½“å‰ LLM é…ç½®\n"]
            for key, value in config.items():
                lines.append(f"- **{key}**: {value}")
            return [TextContent(type="text", text="\n".join(lines))]
        except Exception as e:
            return [TextContent(type="text", text=f"è·å–é…ç½®å¤±è´¥: {str(e)}")]

    # ===== LLM-Friendly API =====
    elif name == "ask":
        try:
            question = arguments.get("question", "")
            context = arguments.get("context")
            result = await ask(question, context)
            return [TextContent(type="text", text=result)]
        except Exception as e:
            return [TextContent(type="text", text=f"è°ƒç”¨å¤±è´¥: {str(e)}")]

    elif name == "ask_batch":
        try:
            questions = arguments.get("questions", [])
            results = await ask_batch(questions)
            output = "\n\n---\n\n".join([f"**Q{i+1}**: {q}\n**A{i+1}**: {a}" for i, (q, a) in enumerate(zip(questions, results))])
            return [TextContent(type="text", text=output)]
        except Exception as e:
            return [TextContent(type="text", text=f"æ‰¹é‡è°ƒç”¨å¤±è´¥: {str(e)}")]

    elif name == "extract_json":
        try:
            import json
            text = arguments.get("text", "")
            schema_desc = arguments.get("schema_desc")
            result = await extract_json(text, schema_desc)
            return [TextContent(type="text", text=json.dumps(result, ensure_ascii=False, indent=2))]
        except Exception as e:
            return [TextContent(type="text", text=f"æå–å¤±è´¥: {str(e)}")]

    elif name == "get_capabilities":
        try:
            import json
            caps = get_capabilities()
            return [TextContent(type="text", text=json.dumps(caps, ensure_ascii=False, indent=2))]
        except Exception as e:
            return [TextContent(type="text", text=f"è·å–èƒ½åŠ›å¤±è´¥: {str(e)}")]

    return [TextContent(type="text", text=f"æœªçŸ¥å·¥å…·: {name}")]


async def main_stdio():
    """ä»¥ stdio æ¨¡å¼å¯åŠ¨ MCP Serverï¼ˆClaude Code è‡ªåŠ¨ç®¡ç†ï¼‰"""
    async with stdio_server() as (read_stream, write_stream):
        await server.run(read_stream, write_stream, server.create_initialization_options())


def main_sse(host: str = "0.0.0.0", port: int = 8765):
    """
    ä»¥ SSE æ¨¡å¼å¯åŠ¨ MCP Serverï¼ˆç‹¬ç«‹ HTTP æœåŠ¡ï¼‰

    å¯åŠ¨: python -m maque.mcp_server --sse --port 8765
    é…ç½®: claude mcp add maque-remote --transport sse --url http://localhost:8765/sse
    """
    from mcp.server.sse import SseServerTransport
    from starlette.applications import Starlette
    from starlette.routing import Route
    import uvicorn

    sse = SseServerTransport("/messages")

    async def handle_sse(request):
        async with sse.connect_sse(
            request.scope, request.receive, request._send
        ) as streams:
            await server.run(
                streams[0], streams[1], server.create_initialization_options()
            )

    async def handle_messages(request):
        await sse.handle_post_message(request.scope, request.receive, request._send)

    app = Starlette(
        routes=[
            Route("/sse", endpoint=handle_sse),
            Route("/messages", endpoint=handle_messages, methods=["POST"]),
        ]
    )

    print(f"ğŸš€ MCP Server (SSE) running at http://{host}:{port}")
    print(f"   é…ç½®å‘½ä»¤: claude mcp add maque --transport sse --url http://localhost:{port}/sse")
    uvicorn.run(app, host=host, port=port)


if __name__ == "__main__":
    import sys
    import asyncio

    if "--sse" in sys.argv:
        # SSE æ¨¡å¼ï¼šç‹¬ç«‹ HTTP æœåŠ¡
        port = 8765
        for i, arg in enumerate(sys.argv):
            if arg == "--port" and i + 1 < len(sys.argv):
                port = int(sys.argv[i + 1])
        main_sse(port=port)
    else:
        # stdio æ¨¡å¼ï¼šClaude Code è‡ªåŠ¨ç®¡ç†
        asyncio.run(main_stdio())
