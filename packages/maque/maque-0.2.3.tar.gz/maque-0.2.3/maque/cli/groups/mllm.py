"""MLLM (å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹) å‘½ä»¤ç»„"""

import os
import sys

# å¼ºåˆ¶å¯ç”¨é¢œè‰²æ”¯æŒ
os.environ["FORCE_COLOR"] = "1"
if not os.environ.get("TERM"):
    os.environ["TERM"] = "xterm-256color"

from rich.console import Console
from rich import print
from rich.markdown import Markdown

console = Console(
    force_terminal=True,
    width=100,
    color_system="windows",
    legacy_windows=True,
    safe_box=True
)


def safe_print(*args, **kwargs):
    """å®‰å…¨çš„æ‰“å°å‡½æ•°ï¼Œç¡®ä¿åœ¨æ‰€æœ‰ç»ˆç«¯ä¸­æ­£ç¡®æ˜¾ç¤ºé¢œè‰²"""
    try:
        console.print(*args, **kwargs)
    except Exception:
        # é™çº§åˆ°æ™®é€šprintï¼Œå¤„ç†ç¼–ç é—®é¢˜
        import re
        import sys
        import builtins

        clean_args = []
        for arg in args:
            if isinstance(arg, str):
                # å»é™¤rich markup
                clean_arg = re.sub(r"\[/?[^\]]*\]", "", str(arg))
                # å¤„ç†emojiå’Œç‰¹æ®Šå­—ç¬¦
                try:
                    # å°è¯•ç¼–ç ä¸ºgbk (Windowsé»˜è®¤ç¼–ç )
                    clean_arg.encode('gbk')
                    clean_args.append(clean_arg)
                except UnicodeEncodeError:
                    # å¦‚æœåŒ…å«æ— æ³•ç¼–ç çš„å­—ç¬¦ï¼Œæ›¿æ¢emojiä¸ºæ–‡æœ¬æè¿°
                    clean_arg = re.sub(r'âŒ', '[é”™è¯¯]', clean_arg)
                    clean_arg = re.sub(r'âœ…', '[æˆåŠŸ]', clean_arg)
                    clean_arg = re.sub(r'ğŸ’¡', '[æç¤º]', clean_arg)
                    clean_arg = re.sub(r'ğŸš€', '[å¯åŠ¨]', clean_arg)
                    clean_arg = re.sub(r'ğŸ“¦', '[æ¨¡å‹]', clean_arg)
                    clean_arg = re.sub(r'ğŸŒ', '[æœåŠ¡å™¨]', clean_arg)
                    clean_arg = re.sub(r'ğŸ‘‹', '[å†è§]', clean_arg)
                    clean_arg = re.sub(r'ğŸ“', '[è®°å½•]', clean_arg)
                    clean_arg = re.sub(r'âš ï¸', '[è­¦å‘Š]', clean_arg)
                    clean_arg = re.sub(r'ğŸ”', '[æœç´¢]', clean_arg)
                    clean_arg = re.sub(r'ğŸ¤–', '[æœºå™¨äºº]', clean_arg)
                    clean_arg = re.sub(r'ğŸ“¡', '[ç½‘ç»œ]', clean_arg)
                    clean_arg = re.sub(r'ğŸ”Œ', '[è¿æ¥]', clean_arg)
                    clean_arg = re.sub(r'ğŸ“‹', '[é…ç½®]', clean_arg)
                    clean_arg = re.sub(r'ğŸ“', '[æ–‡ä»¶]', clean_arg)
                    clean_arg = re.sub(r'ğŸ”§', '[è®¾ç½®]', clean_arg)
                    clean_arg = re.sub(r'ğŸ¯', '[ç›®æ ‡]', clean_arg)
                    clean_arg = re.sub(r'ğŸ“Š', '[ç»Ÿè®¡]', clean_arg)
                    clean_arg = re.sub(r'ğŸ§ ', '[æ€è€ƒ]', clean_arg)
                    clean_arg = re.sub(r'ğŸ’­', '[æ¨ç†]', clean_arg)
                    clean_arg = re.sub(r'ğŸ”—', '[é€»è¾‘]', clean_arg)
                    # ç§»é™¤å…¶ä»–æ— æ³•æ˜¾ç¤ºçš„emoji
                    clean_arg = re.sub(r'[\U0001F600-\U0001F64F\U0001F300-\U0001F5FF\U0001F680-\U0001F6FF\U0001F1E0-\U0001F1FF\U00002600-\U000027BF\U0001F900-\U0001F9FF]', '', clean_arg)
                    clean_args.append(clean_arg)
            else:
                clean_args.append(str(arg))

        # ä½¿ç”¨å†…ç½®print
        try:
            builtins.print(*clean_args, **kwargs)
        except UnicodeEncodeError:
            # æœ€åçš„é™çº§ï¼šä½¿ç”¨é”™è¯¯æ›¿æ¢
            safe_args = [arg.encode('gbk', errors='replace').decode('gbk') if isinstance(arg, str) else arg for arg in clean_args]
            builtins.print(*safe_args, **kwargs)


def safe_print_stream(text, **kwargs):
    """å®‰å…¨çš„æµå¼æ‰“å°å‡½æ•°ï¼Œç”¨äºæµå¼è¾“å‡º

    é»˜è®¤ä½¿ç”¨åŸç”Ÿ print å®ç°çœŸæ­£çš„æµå¼è¾“å‡ºï¼Œé¿å… Rich console çš„æ ¼å¼åŒ–å¹²æ‰°ã€‚
    """
    import builtins

    flush = kwargs.pop('flush', True)  # æµå¼è¾“å‡ºé»˜è®¤ flush
    end = kwargs.pop('end', '')  # æµå¼è¾“å‡ºé»˜è®¤ä¸æ¢è¡Œ

    try:
        builtins.print(text, end=end, flush=flush, **kwargs)
    except UnicodeEncodeError:
        # ç¼–ç å¤±è´¥æ—¶ï¼Œå°è¯•ä½¿ç”¨ stdout buffer
        if hasattr(sys.stdout, 'buffer'):
            sys.stdout.buffer.write(text.encode('utf-8', errors='replace'))
            if flush:
                sys.stdout.buffer.flush()
        else:
            # æœ€åçš„é™çº§æ–¹æ¡ˆï¼šæ›¿æ¢æ— æ³•ç¼–ç çš„å­—ç¬¦
            safe_text = text.encode('gbk', errors='replace').decode('gbk')
            builtins.print(safe_text, end=end, flush=flush, **kwargs)


def safe_print_markdown(content, **kwargs):
    """å®‰å…¨çš„Markdownæ¸²æŸ“å‡½æ•°"""
    try:
        # ä½¿ç”¨Richçš„Markdownæ¸²æŸ“
        markdown = Markdown(content)
        console.print(markdown, **kwargs)
    except Exception:
        # é™çº§åˆ°æ™®é€šæ‰“å°
        safe_print(content, **kwargs)


class StreamingMarkdownRenderer:
    """æµå¼Markdownæ¸²æŸ“å™¨ - å®æ—¶è§£æå¹¶æ¸²æŸ“Markdown"""

    def __init__(self):
        self.buffer = ""
        self.last_rendered_length = 0
        self.in_code_block = False
        self.code_block_lang = ""

    def add_token(self, token):
        """æ·»åŠ æ–°tokenå¹¶å°è¯•æ¸²æŸ“"""
        self.buffer += token
        self._try_render_incremental()

    def _try_render_incremental(self):
        """å°è¯•å¢é‡æ¸²æŸ“Markdown"""
        # æ£€æµ‹ä»£ç å—
        if "```" in self.buffer[self.last_rendered_length:]:
            code_block_matches = self.buffer.count("```")
            self.in_code_block = (code_block_matches % 2) == 1

        # å¦‚æœåœ¨ä»£ç å—ä¸­ï¼Œç›´æ¥è¾“å‡ºåŸå§‹æ–‡æœ¬
        if self.in_code_block:
            new_content = self.buffer[self.last_rendered_length:]
            if new_content:
                safe_print_stream(new_content, end="", flush=True)
                self.last_rendered_length = len(self.buffer)
            return

        # å°è¯•æ‰¾åˆ°å¯ä»¥å®‰å…¨æ¸²æŸ“çš„è¾¹ç•Œï¼ˆå¥å­ã€æ®µè½ç­‰ï¼‰
        render_boundary = self._find_render_boundary()
        if render_boundary > self.last_rendered_length:
            content_to_render = self.buffer[self.last_rendered_length:render_boundary]
            self._render_content(content_to_render)
            self.last_rendered_length = render_boundary

    def _find_render_boundary(self):
        """æ‰¾åˆ°é€‚åˆæ¸²æŸ“çš„è¾¹ç•Œä½ç½®"""
        content = self.buffer

        # å¯»æ‰¾å¥å­ç»“æŸæ ‡è®°
        for i in range(len(content) - 1, self.last_rendered_length - 1, -1):
            char = content[i]
            # å¥å­ç»“æŸ
            if char in '.!?ã€‚ï¼ï¼Ÿ':
                # ç¡®ä¿åé¢æœ‰ç©ºæ ¼æˆ–æ¢è¡Œï¼Œé¿å…è¯¯åˆ¤å°æ•°ç‚¹ç­‰
                if i + 1 < len(content) and content[i + 1] in ' \n\t':
                    return i + 1
            # æ®µè½ç»“æŸ
            elif char == '\n' and (i + 1 >= len(content) or content[i + 1] == '\n'):
                return i + 1

        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°åˆé€‚çš„è¾¹ç•Œï¼Œè¿”å›å½“å‰é•¿åº¦ï¼ˆä¸æ¸²æŸ“ï¼‰
        return self.last_rendered_length

    def _render_content(self, content):
        """æ¸²æŸ“å†…å®¹ç‰‡æ®µ"""
        if not content.strip():
            safe_print_stream(content, end="", flush=True)
            return

        # ç®€å•çš„è¡Œå†…Markdownæ¸²æŸ“
        try:
            # æ£€æŸ¥æ˜¯å¦åŒ…å«Markdownå…ƒç´ 
            if any(marker in content for marker in ['**', '*', '`', '#', '-', '1.']):
                # ç®€å•çš„å®æ—¶æ¸²æŸ“ï¼Œåªå¤„ç†åŸºæœ¬å…ƒç´ 
                rendered = self._simple_markdown_render(content)
                safe_print_stream(rendered, end="", flush=True)
            else:
                # çº¯æ–‡æœ¬ç›´æ¥è¾“å‡º
                safe_print_stream(content, end="", flush=True)
        except Exception:
            # å‡ºé”™æ—¶é™çº§åˆ°åŸå§‹æ–‡æœ¬
            safe_print_stream(content, end="", flush=True)

    def _simple_markdown_render(self, content):
        """ç®€å•çš„Markdownæ¸²æŸ“ - åªå¤„ç†åŸºæœ¬æ ¼å¼"""
        import re

        # ç²—ä½“ **text**
        content = re.sub(r'\*\*([^\*]+)\*\*', r'[bold]\1[/bold]', content)
        # æ–œä½“ *text*
        content = re.sub(r'\*([^\*]+)\*', r'[italic]\1[/italic]', content)
        # è¡Œå†…ä»£ç  `code`
        content = re.sub(r'`([^`]+)`', r'[code]\1[/code]', content)

        return content

    def finalize(self):
        """å®Œæˆæ¸²æŸ“ï¼Œå¤„ç†å‰©ä½™å†…å®¹"""
        if self.last_rendered_length < len(self.buffer):
            remaining = self.buffer[self.last_rendered_length:]
            self._render_content(remaining)

        safe_print_stream("", end="\n")  # æ¢è¡Œ


def safe_print_stream_markdown(content, is_complete=False, **kwargs):
    """æµå¼Markdownæ¸²æŸ“å‡½æ•°ï¼Œç´¯ç§¯å†…å®¹åæ¸²æŸ“"""
    if is_complete:
        # å®Œæ•´å†…å®¹ï¼Œè¿›è¡ŒMarkdownæ¸²æŸ“
        try:
            markdown = Markdown(content)
            console.print(markdown, **kwargs)
        except Exception:
            safe_print_stream(content, **kwargs)
    else:
        # æµå¼è¾“å‡ºï¼Œç›´æ¥æ‰“å°åŸå§‹æ–‡æœ¬
        safe_print_stream(content, **kwargs)


def get_user_input(prompt_text="You"):
    """è·å–ç”¨æˆ·è¾“å…¥ï¼Œæ”¯æŒRichæ ¼å¼çš„æç¤º"""
    try:
        # ä½¿ç”¨console.inputæ¥æ”¯æŒRichæ ¼å¼
        return console.input(f"[bold yellow]{prompt_text}:[/bold yellow] ")
    except Exception:
        # é™çº§åˆ°æ™®é€šinput
        return input(f"{prompt_text}: ")


class AdvancedInput:
    """é«˜çº§è¾“å…¥å¤„ç†å™¨ï¼Œæ”¯æŒå¤šè¡Œè¾“å…¥ï¼ˆAlt+Enter æ¢è¡Œï¼‰"""

    def __init__(self):
        self._use_prompt_toolkit = False
        self._bindings = None
        self._init_prompt_toolkit()

    def _init_prompt_toolkit(self):
        """åˆå§‹åŒ– prompt_toolkit çš„é”®ç»‘å®š"""
        try:
            from prompt_toolkit.key_binding import KeyBindings
            from prompt_toolkit.keys import Keys

            # åˆ›å»ºå¿«æ·é”®ç»‘å®š
            self._bindings = KeyBindings()

            @self._bindings.add(Keys.Enter)
            def _(event):
                """Enter æäº¤è¾“å…¥"""
                event.current_buffer.validate_and_handle()

            # Alt+Enter (Escape + Enter) æ¢è¡Œ - æœ€å¯é çš„æ–¹å¼
            @self._bindings.add('escape', 'enter')
            def _(event):
                """Alt+Enter æ¢è¡Œ"""
                event.current_buffer.insert_text('\n')

            self._use_prompt_toolkit = True
        except ImportError:
            self._use_prompt_toolkit = False

    def _sync_prompt(self, prompt_text: str) -> str:
        """åŒæ­¥è°ƒç”¨ prompt_toolkitï¼ˆåœ¨å•ç‹¬çº¿ç¨‹ä¸­è¿è¡Œï¼‰"""
        from prompt_toolkit import prompt as pt_prompt
        return pt_prompt(
            f"{prompt_text}: ",
            key_bindings=self._bindings,
            multiline=False,
        )

    def get_input(self, prompt_text="You") -> str:
        """è·å–ç”¨æˆ·è¾“å…¥ï¼Œæ”¯æŒå¤šè¡Œï¼ˆåŒæ­¥ç‰ˆæœ¬ï¼‰"""
        if self._use_prompt_toolkit:
            try:
                return self._sync_prompt(prompt_text)
            except (KeyboardInterrupt, EOFError):
                raise
            except Exception:
                # å‡ºé”™æ—¶é™çº§åˆ°åŸºæœ¬è¾“å…¥
                self._use_prompt_toolkit = False

        # Fallback åˆ°åŸºæœ¬è¾“å…¥
        return get_user_input(prompt_text)

    async def get_input_async(self, prompt_text="You") -> str:
        """è·å–ç”¨æˆ·è¾“å…¥ï¼Œæ”¯æŒå¤šè¡Œï¼ˆå¼‚æ­¥ç‰ˆæœ¬ï¼Œåœ¨å•ç‹¬çº¿ç¨‹ä¸­è¿è¡Œï¼‰"""
        if self._use_prompt_toolkit:
            try:
                import asyncio
                # åœ¨å•ç‹¬çº¿ç¨‹ä¸­è¿è¡Œ prompt_toolkitï¼Œé¿å…ä¸ asyncio å†²çª
                return await asyncio.to_thread(self._sync_prompt, prompt_text)
            except (KeyboardInterrupt, EOFError):
                raise
            except Exception:
                # å‡ºé”™æ—¶é™çº§åˆ°åŸºæœ¬è¾“å…¥
                self._use_prompt_toolkit = False

        # Fallback åˆ°åŸºæœ¬è¾“å…¥
        return get_user_input(prompt_text)


class ChatCommands:
    """èŠå¤©å¿«æ·å‘½ä»¤å¤„ç†å™¨"""

    COMMANDS = {
        '/clear': 'æ¸…ç©ºå¯¹è¯å†å²',
        '/retry': 'é‡æ–°ç”Ÿæˆä¸Šä¸€æ¡å›å¤',
        '/save': 'ä¿å­˜å¯¹è¯åˆ°æ–‡ä»¶ (ç”¨æ³•: /save [æ–‡ä»¶å])',
        '/model': 'åˆ‡æ¢æ¨¡å‹ (ç”¨æ³•: /model [æ¨¡å‹å])',
        '/help': 'æ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯',
    }

    @classmethod
    def is_command(cls, text: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦æ˜¯å‘½ä»¤"""
        return text.strip().startswith('/')

    @classmethod
    def parse(cls, text: str) -> tuple:
        """è§£æå‘½ä»¤ï¼Œè¿”å› (å‘½ä»¤å, å‚æ•°åˆ—è¡¨)"""
        parts = text.strip().split(maxsplit=1)
        cmd = parts[0].lower()
        args = parts[1] if len(parts) > 1 else ""
        return cmd, args

    @classmethod
    def show_help(cls):
        """æ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯"""
        safe_print("\n[bold cyan]ğŸ“‹ å¯ç”¨å‘½ä»¤:[/bold cyan]")
        for cmd, desc in cls.COMMANDS.items():
            safe_print(f"  [green]{cmd:12}[/green] - {desc}")
        safe_print("")

    @classmethod
    def handle_clear(cls, messages: list, system_prompt: str = None) -> list:
        """æ¸…ç©ºå¯¹è¯å†å²"""
        new_messages = []
        if system_prompt:
            new_messages.append({"role": "system", "content": system_prompt})
        safe_print("[dim]ğŸ—‘ï¸  å¯¹è¯å†å²å·²æ¸…ç©º[/dim]\n")
        return new_messages

    @classmethod
    def handle_save(cls, messages: list, filename: str = None):
        """ä¿å­˜å¯¹è¯åˆ°æ–‡ä»¶"""
        import json
        from datetime import datetime

        if not filename:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"chat_{timestamp}.json"

        if not filename.endswith('.json'):
            filename += '.json'

        # è¿‡æ»¤æ‰ç³»ç»Ÿæ¶ˆæ¯ï¼Œåªä¿å­˜ç”¨æˆ·å’ŒåŠ©æ‰‹çš„å¯¹è¯
        chat_history = [
            msg for msg in messages
            if msg.get('role') in ['user', 'assistant']
        ]

        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump({
                    'saved_at': datetime.now().isoformat(),
                    'messages': chat_history
                }, f, ensure_ascii=False, indent=2)
            safe_print(f"[green]ğŸ’¾ å¯¹è¯å·²ä¿å­˜åˆ°: {filename}[/green]\n")
        except Exception as e:
            safe_print(f"[red]âŒ ä¿å­˜å¤±è´¥: {e}[/red]\n")

    @classmethod
    def handle_retry(cls, messages: list) -> tuple:
        """å‡†å¤‡é‡è¯•ï¼šç§»é™¤æœ€åä¸€æ¡åŠ©æ‰‹å›å¤ï¼Œè¿”å›æ˜¯å¦éœ€è¦é‡è¯•"""
        if len(messages) < 2:
            safe_print("[yellow]âš ï¸  æ²¡æœ‰å¯ä»¥é‡è¯•çš„å›å¤[/yellow]\n")
            return messages, False

        # æ‰¾åˆ°æœ€åä¸€æ¡åŠ©æ‰‹æ¶ˆæ¯å¹¶ç§»é™¤
        if messages[-1].get('role') == 'assistant':
            messages.pop()
            safe_print("[dim]ğŸ”„ æ­£åœ¨é‡æ–°ç”Ÿæˆ...[/dim]")
            return messages, True
        else:
            safe_print("[yellow]âš ï¸  æœ€åä¸€æ¡ä¸æ˜¯åŠ©æ‰‹å›å¤ï¼Œæ— æ³•é‡è¯•[/yellow]\n")
            return messages, False


class MllmGroup:
    """MLLMå‘½ä»¤ç»„ - ç»Ÿä¸€ç®¡ç†å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ç›¸å…³åŠŸèƒ½"""

    def __init__(self, cli_instance):
        self.cli = cli_instance

    def call_table(
        self,
        table_path: str,
        model: str = None,
        base_url: str = None,
        api_key: str = None,
        image_col: str = "image",
        system_prompt: str = "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å›¾åƒè¯†åˆ«ä¸“å®¶ã€‚",
        text_prompt: str = "è¯·æè¿°è¿™å¼ å›¾åƒã€‚",
        system_prompt_file: str = None,
        text_prompt_file: str = None,
        sheet_name: str = 0,
        max_num=None,
        output_file: str = "table_results.csv",
        temperature: float = 0.1,
        max_tokens: int = 2000,
        concurrency_limit: int = 10,
        max_qps: int = 50,
        retry_times: int = 3,
        skip_existing: bool = False,
        **kwargs,
    ):
        """å¯¹è¡¨æ ¼ä¸­çš„å›¾åƒåˆ—è¿›è¡Œæ‰¹é‡å¤§æ¨¡å‹è¯†åˆ«å’Œåˆ†æ

        Args:
            table_path: è¡¨æ ¼æ–‡ä»¶è·¯å¾„ (xlsx/csv)
            model: æ¨¡å‹åç§°
            base_url: APIæœåŠ¡åœ°å€
            api_key: APIå¯†é’¥
            image_col: å›¾ç‰‡åˆ—å
            system_prompt: ç³»ç»Ÿæç¤ºè¯
            text_prompt: æ–‡æœ¬æç¤ºè¯
            system_prompt_file: ç³»ç»Ÿæç¤ºè¯æ–‡ä»¶è·¯å¾„ï¼ˆä¼˜å…ˆäº system_promptï¼‰
            text_prompt_file: æ–‡æœ¬æç¤ºè¯æ–‡ä»¶è·¯å¾„ï¼ˆä¼˜å…ˆäº text_promptï¼‰
            sheet_name: sheetåç§°
            max_num: æœ€å¤§å¤„ç†æ•°é‡
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§tokenæ•°
            concurrency_limit: å¹¶å‘é™åˆ¶
            max_qps: æœ€å¤§QPS
            retry_times: é‡è¯•æ¬¡æ•°
            skip_existing: æ˜¯å¦è·³è¿‡å·²æœ‰ç»“æœçš„è¡Œï¼ˆæ–­ç‚¹ç»­ä¼ ï¼‰
        """
        import asyncio
        import pandas as pd
        import os
        from flexllm.mllm_client import MllmClient

        # ä»é…ç½®æ–‡ä»¶è·å–é»˜è®¤å€¼
        mllm_config = self.cli.maque_config.get("mllm", {})
        model = model or mllm_config.get("model", "gemma3:latest")
        base_url = base_url or mllm_config.get("base_url", "http://localhost:11434/v1")
        api_key = api_key or mllm_config.get("api_key", "EMPTY")

        # ä»æ–‡ä»¶è¯»å– promptï¼ˆå¦‚æœæŒ‡å®šï¼‰
        if system_prompt_file and os.path.exists(system_prompt_file):
            with open(system_prompt_file, 'r', encoding='utf-8') as f:
                system_prompt = f.read().strip()
            safe_print(f"[dim]ğŸ“„ ä»æ–‡ä»¶åŠ è½½ system_prompt: {system_prompt_file}[/dim]")

        if text_prompt_file and os.path.exists(text_prompt_file):
            with open(text_prompt_file, 'r', encoding='utf-8') as f:
                text_prompt = f.read().strip()
            safe_print(f"[dim]ğŸ“„ ä»æ–‡ä»¶åŠ è½½ text_prompt: {text_prompt_file}[/dim]")

        async def run_call_table():
            try:
                safe_print(f"\n[bold green]ğŸ“Š å¼€å§‹æ‰¹é‡å¤„ç†è¡¨æ ¼[/bold green]")
                safe_print(f"[cyan]ğŸ“ æ–‡ä»¶: {table_path}[/cyan]")
                safe_print(f"[dim]ğŸ”§ æ¨¡å‹: {model} | å¹¶å‘: {concurrency_limit} | QPS: {max_qps}[/dim]")

                # åˆå§‹åŒ–å®¢æˆ·ç«¯
                client = MllmClient(
                    model=model,
                    base_url=base_url,
                    api_key=api_key,
                    concurrency_limit=concurrency_limit,
                    max_qps=max_qps,
                    retry_times=retry_times,
                    **kwargs,
                )

                # åŠ è½½æ•°æ®
                if table_path.endswith(".xlsx"):
                    df = pd.read_excel(table_path, sheet_name=sheet_name)
                else:
                    df = pd.read_csv(table_path)

                total_rows = len(df)
                if max_num:
                    df = df.head(max_num)

                safe_print(f"[dim]ğŸ“ æ€»è¡Œæ•°: {total_rows}, å¤„ç†è¡Œæ•°: {len(df)}[/dim]")

                # æ£€æŸ¥å¹¶åˆ›å»ºç»“æœåˆ—
                result_col = "mllm_result"
                if result_col not in df.columns:
                    df[result_col] = None

                # æ–­ç‚¹ç»­ä¼ ï¼šè¿‡æ»¤å·²æœ‰ç»“æœçš„è¡Œ
                if skip_existing and os.path.exists(output_file):
                    existing_df = pd.read_csv(output_file) if output_file.endswith('.csv') else pd.read_excel(output_file)
                    if result_col in existing_df.columns:
                        # åˆå¹¶å·²æœ‰ç»“æœ
                        df[result_col] = existing_df[result_col] if len(existing_df) == len(df) else df[result_col]
                        safe_print(f"[yellow]â­ï¸  æ–­ç‚¹ç»­ä¼ : æ£€æµ‹åˆ°å·²æœ‰ç»“æœæ–‡ä»¶[/yellow]")

                # æ‰¾å‡ºéœ€è¦å¤„ç†çš„è¡Œ
                if skip_existing:
                    pending_mask = df[result_col].isna() | (df[result_col] == '') | (df[result_col] == 'None')
                    pending_indices = df[pending_mask].index.tolist()
                else:
                    pending_indices = df.index.tolist()

                if not pending_indices:
                    safe_print(f"[green]âœ… æ‰€æœ‰è¡Œå·²å¤„ç†å®Œæˆï¼Œæ— éœ€é‡æ–°å¤„ç†[/green]")
                    return

                safe_print(f"[cyan]ğŸ”„ å¾…å¤„ç†: {len(pending_indices)} è¡Œ[/cyan]")

                # æ„å»ºå¾…å¤„ç†çš„ messages
                messages_list = []
                for idx in pending_indices:
                    row = df.loc[idx]
                    messages = []
                    if system_prompt:
                        messages.append({"role": "system", "content": system_prompt})
                    messages.append({
                        "role": "user",
                        "content": [
                            {"type": "text", "text": text_prompt},
                            {"type": "image_url", "image_url": {"url": str(row[image_col])}},
                        ],
                    })
                    messages_list.append(messages)

                # è°ƒç”¨ MLLM
                results = await client.call_llm(
                    messages_list,
                    temperature=temperature,
                    max_tokens=max_tokens,
                )

                # å¡«å……ç»“æœ
                for i, idx in enumerate(pending_indices):
                    df.at[idx, result_col] = results[i] if i < len(results) else None

                # ä¿å­˜ç»“æœ
                if output_file.endswith('.csv'):
                    df.to_csv(output_file, index=False, encoding='utf-8-sig')
                else:
                    df.to_excel(output_file, index=False)

                safe_print(f"\n[bold green]âœ… å¤„ç†å®Œæˆï¼ç»“æœå·²ä¿å­˜åˆ°: {output_file}[/bold green]")

                # ç»Ÿè®¡
                success_count = df[result_col].notna().sum()
                safe_print(f"[dim]ğŸ“Š æˆåŠŸ: {success_count}/{len(df)}[/dim]")

            except Exception as e:
                safe_print(f"[red]âŒ å¤„ç†å¤±è´¥: {e}[/red]")
                import traceback
                traceback.print_exc()

        return asyncio.run(run_call_table())

    def call_images(
        self,
        folder_path: str,
        model: str = None,
        base_url: str = None,
        api_key: str = None,
        system_prompt: str = "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å›¾åƒè¯†åˆ«ä¸“å®¶ã€‚",
        text_prompt: str = "è¯·æè¿°è¿™å¼ å›¾åƒã€‚",
        system_prompt_file: str = None,
        text_prompt_file: str = None,
        recursive: bool = True,
        max_num: int = None,
        extensions: str = None,
        output_file: str = "results.csv",
        temperature: float = 0.1,
        max_tokens: int = 2000,
        concurrency_limit: int = 10,
        max_qps: int = 50,
        retry_times: int = 3,
        skip_existing: bool = False,
        **kwargs,
    ):
        """å¯¹æ–‡ä»¶å¤¹ä¸­çš„å›¾åƒè¿›è¡Œæ‰¹é‡å¤§æ¨¡å‹è¯†åˆ«å’Œåˆ†æ

        Args:
            folder_path: æ–‡ä»¶å¤¹è·¯å¾„
            model: æ¨¡å‹åç§°
            base_url: APIæœåŠ¡åœ°å€
            api_key: APIå¯†é’¥
            system_prompt: ç³»ç»Ÿæç¤ºè¯
            text_prompt: æ–‡æœ¬æç¤ºè¯
            system_prompt_file: ç³»ç»Ÿæç¤ºè¯æ–‡ä»¶è·¯å¾„ï¼ˆä¼˜å…ˆäº system_promptï¼‰
            text_prompt_file: æ–‡æœ¬æç¤ºè¯æ–‡ä»¶è·¯å¾„ï¼ˆä¼˜å…ˆäº text_promptï¼‰
            recursive: æ˜¯å¦é€’å½’æ‰«æå­æ–‡ä»¶å¤¹
            max_num: æœ€å¤§å¤„ç†æ•°é‡
            extensions: æ”¯æŒçš„æ–‡ä»¶æ‰©å±•åï¼ˆé€—å·åˆ†éš”ï¼Œå¦‚ "jpg,png,webp"ï¼‰
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§tokenæ•°
            concurrency_limit: å¹¶å‘é™åˆ¶
            max_qps: æœ€å¤§QPS
            retry_times: é‡è¯•æ¬¡æ•°
            skip_existing: æ˜¯å¦è·³è¿‡å·²å¤„ç†çš„å›¾ç‰‡ï¼ˆæ–­ç‚¹ç»­ä¼ ï¼‰
        """
        import asyncio
        import pandas as pd
        import os
        from pathlib import Path
        from flexllm.mllm_client import MllmClient

        # ä»é…ç½®æ–‡ä»¶è·å–é»˜è®¤å€¼
        mllm_config = self.cli.maque_config.get("mllm", {})
        model = model or mllm_config.get("model", "gemma3:latest")
        base_url = base_url or mllm_config.get("base_url", "http://localhost:11434/v1")
        api_key = api_key or mllm_config.get("api_key", "EMPTY")

        # ä»æ–‡ä»¶è¯»å– promptï¼ˆå¦‚æœæŒ‡å®šï¼‰
        if system_prompt_file and os.path.exists(system_prompt_file):
            with open(system_prompt_file, 'r', encoding='utf-8') as f:
                system_prompt = f.read().strip()
            safe_print(f"[dim]ğŸ“„ ä»æ–‡ä»¶åŠ è½½ system_prompt: {system_prompt_file}[/dim]")

        if text_prompt_file and os.path.exists(text_prompt_file):
            with open(text_prompt_file, 'r', encoding='utf-8') as f:
                text_prompt = f.read().strip()
            safe_print(f"[dim]ğŸ“„ ä»æ–‡ä»¶åŠ è½½ text_prompt: {text_prompt_file}[/dim]")

        # è§£ææ‰©å±•å
        ext_set = None
        if extensions:
            ext_set = {f".{ext.strip().lower().lstrip('.')}" for ext in extensions.split(',')}

        async def run_call_images():
            try:
                safe_print(f"\n[bold green]ğŸ“ å¼€å§‹æ‰¹é‡å¤„ç†æ–‡ä»¶å¤¹å›¾ç‰‡[/bold green]")
                safe_print(f"[cyan]ğŸ“‚ è·¯å¾„: {folder_path}[/cyan]")
                safe_print(f"[dim]ğŸ”§ æ¨¡å‹: {model} | å¹¶å‘: {concurrency_limit} | QPS: {max_qps}[/dim]")

                # åˆå§‹åŒ–å®¢æˆ·ç«¯
                client = MllmClient(
                    model=model,
                    base_url=base_url,
                    api_key=api_key,
                    concurrency_limit=concurrency_limit,
                    max_qps=max_qps,
                    retry_times=retry_times,
                    **kwargs,
                )

                # æ‰«æå›¾ç‰‡æ–‡ä»¶
                image_files = client.folder.scan_folder_images(
                    folder_path=folder_path,
                    recursive=recursive,
                    max_num=max_num,
                    extensions=ext_set,
                )

                if not image_files:
                    safe_print(f"[yellow]âš ï¸  æœªæ‰¾åˆ°å›¾ç‰‡æ–‡ä»¶[/yellow]")
                    return

                # åˆ›å»ºç»“æœ DataFrame
                df = pd.DataFrame({'image_path': image_files})
                result_col = "mllm_result"
                df[result_col] = None

                # æ–­ç‚¹ç»­ä¼ ï¼šåŠ è½½å·²æœ‰ç»“æœ
                processed_paths = set()
                if skip_existing and os.path.exists(output_file):
                    try:
                        existing_df = pd.read_csv(output_file) if output_file.endswith('.csv') else pd.read_excel(output_file)
                        if 'image_path' in existing_df.columns and result_col in existing_df.columns:
                            # åˆ›å»ºè·¯å¾„åˆ°ç»“æœçš„æ˜ å°„
                            for _, row in existing_df.iterrows():
                                path = row['image_path']
                                result = row[result_col]
                                if pd.notna(result) and result != '' and result != 'None':
                                    processed_paths.add(path)
                                    # æ›´æ–° df ä¸­å¯¹åº”è¡Œçš„ç»“æœ
                                    mask = df['image_path'] == path
                                    if mask.any():
                                        df.loc[mask, result_col] = result
                            safe_print(f"[yellow]â­ï¸  æ–­ç‚¹ç»­ä¼ : å·²å¤„ç† {len(processed_paths)} ä¸ªæ–‡ä»¶[/yellow]")
                    except Exception as e:
                        safe_print(f"[yellow]âš ï¸  è¯»å–å·²æœ‰ç»“æœå¤±è´¥: {e}[/yellow]")

                # æ‰¾å‡ºéœ€è¦å¤„ç†çš„æ–‡ä»¶
                pending_indices = []
                for idx, row in df.iterrows():
                    if row['image_path'] not in processed_paths:
                        pending_indices.append(idx)

                if not pending_indices:
                    safe_print(f"[green]âœ… æ‰€æœ‰å›¾ç‰‡å·²å¤„ç†å®Œæˆï¼Œæ— éœ€é‡æ–°å¤„ç†[/green]")
                    return

                safe_print(f"[cyan]ğŸ”„ å¾…å¤„ç†: {len(pending_indices)} ä¸ªå›¾ç‰‡[/cyan]")

                # æ„å»º messages
                messages_list = []
                pending_files = []
                for idx in pending_indices:
                    image_path = df.loc[idx, 'image_path']
                    pending_files.append(image_path)
                    messages = []
                    if system_prompt:
                        messages.append({"role": "system", "content": system_prompt})
                    messages.append({
                        "role": "user",
                        "content": [
                            {"type": "text", "text": text_prompt},
                            {"type": "image_url", "image_url": {"url": f"file://{image_path}"}},
                        ],
                    })
                    messages_list.append(messages)

                # è°ƒç”¨ MLLM
                results = await client.call_llm(
                    messages_list,
                    temperature=temperature,
                    max_tokens=max_tokens,
                )

                # å¡«å……ç»“æœ
                for i, idx in enumerate(pending_indices):
                    df.at[idx, result_col] = results[i] if i < len(results) else None

                # ä¿å­˜ç»“æœ
                if output_file.endswith('.csv'):
                    df.to_csv(output_file, index=False, encoding='utf-8-sig')
                else:
                    df.to_excel(output_file, index=False)

                safe_print(f"\n[bold green]âœ… å¤„ç†å®Œæˆï¼ç»“æœå·²ä¿å­˜åˆ°: {output_file}[/bold green]")

                # ç»Ÿè®¡
                success_count = df[result_col].notna().sum()
                safe_print(f"[dim]ğŸ“Š æˆåŠŸ: {success_count}/{len(df)}[/dim]")

            except Exception as e:
                safe_print(f"[red]âŒ å¤„ç†å¤±è´¥: {e}[/red]")
                import traceback
                traceback.print_exc()

        return asyncio.run(run_call_images())

    def chat(
        self,
        message: str = None,
        image: str = None,
        model: str = None,
        base_url: str = None,
        api_key: str = None,
        system_prompt: str = None,
        temperature: float = 0.1,
        max_tokens: int = 2000,
        stream: bool = True,
        **kwargs,
    ):
        """äº¤äº’å¼å¤šæ¨¡æ€å¯¹è¯"""
        # åŒæ­¥ç‰ˆæœ¬ï¼Œç®€åŒ–å¤„ç†
        import asyncio
        from flexllm.mllm_client import MllmClient

        # ä»é…ç½®æ–‡ä»¶è·å–é»˜è®¤å€¼
        mllm_config = self.cli.maque_config.get("mllm", {})

        if model is None:
            model_name = mllm_config.get("model", "gemma3:latest")
        else:
            model_name = model

        if base_url is None:
            base_url_val = mllm_config.get("base_url", "http://localhost:11434/v1")
        else:
            base_url_val = base_url

        if api_key is None:
            api_key_val = mllm_config.get("api_key", "EMPTY")
        else:
            api_key_val = api_key

        if message:
            # å•æ¬¡å¯¹è¯æ¨¡å¼
            def run_single_chat():
                async def _single_chat():
                    try:
                        # åˆå§‹åŒ–å®¢æˆ·ç«¯
                        client = MllmClient(
                            model=model_name,
                            base_url=base_url_val,
                            api_key=api_key_val,
                            **kwargs,
                        )

                        messages = [
                            {
                                "role": "user",
                                "content": message
                                if not image
                                else [
                                    {"type": "text", "text": message},
                                    {"type": "image_url", "image_url": {"url": image}},
                                ],
                            }
                        ]

                        if system_prompt:
                            messages.insert(
                                0, {"role": "system", "content": system_prompt}
                            )

                        if stream:
                            # æµå¼è¾“å‡º - ä½¿ç”¨ä¼˜é›…çš„Markdownæ¸²æŸ“å™¨
                            safe_print(f"[bold blue]Assistant:[/bold blue] ")

                            renderer = StreamingMarkdownRenderer()
                            try:
                                async for token in client.call_llm_stream(
                                    messages=messages,
                                    temperature=temperature,
                                    max_tokens=max_tokens,
                                    **kwargs,
                                ):
                                    renderer.add_token(token)
                                # å®Œæˆæµå¼è¾“å‡º
                                renderer.finalize()
                            except KeyboardInterrupt:
                                safe_print_stream("\n")
                                safe_print("[dim]â¸ï¸  è¾“å‡ºå·²ä¸­æ–­[/dim]")
                            return renderer.buffer
                        else:
                            # éæµå¼è¾“å‡ºï¼Œä½¿ç”¨Markdownæ¸²æŸ“
                            results = await client.call_llm(
                                messages_list=[messages], show_progress=False
                            )
                            response = (
                                results[0] if results and results[0] else "æ— å“åº”"
                            )
                            safe_print(f"[bold blue]Assistant:[/bold blue]")
                            safe_print_markdown(response)
                            return response
                    except KeyboardInterrupt:
                        safe_print("\n[dim]ğŸ‘‹ å†è§ï¼[/dim]")
                        return None
                    except Exception as e:
                        safe_print(f"[red]âŒ æ‰§è¡Œé”™è¯¯: {e}[/red]")
                        safe_print("[yellow]ğŸ’¡ è¯·æ£€æŸ¥æ¨¡å‹é…ç½®å’Œç½‘ç»œè¿æ¥[/yellow]")
                        return None

                try:
                    return asyncio.run(_single_chat())
                except KeyboardInterrupt:
                    safe_print("\n[dim]ğŸ‘‹ å†è§ï¼[/dim]")
                    return None

            return run_single_chat()
        else:
            # å¤šè½®äº¤äº’æ¨¡å¼
            def run_interactive_chat():
                async def _interactive_chat():
                    try:
                        # åˆå§‹åŒ–å®¢æˆ·ç«¯
                        client = MllmClient(
                            model=model_name,
                            base_url=base_url_val,
                            api_key=api_key_val,
                            **kwargs,
                        )

                        # åˆå§‹åŒ–å¯¹è¯å†å²
                        messages = []
                        if system_prompt:
                            messages.append(
                                {"role": "system", "content": system_prompt}
                            )

                        # åˆå§‹åŒ–é«˜çº§è¾“å…¥å¤„ç†å™¨
                        advanced_input = AdvancedInput()
                        current_model = model_name  # ç”¨äºæ”¯æŒ /model åˆ‡æ¢

                        safe_print("\n[bold green]ğŸš€ å¤šè½®å¯¹è¯æ¨¡å¼å¯åŠ¨[/bold green]")
                        safe_print(f"[cyan]ğŸ“¦ æ¨¡å‹: [/cyan][bold]{current_model}[/bold]")
                        safe_print(f"[cyan]ğŸŒ æœåŠ¡å™¨: [/cyan][bold]{base_url_val}[/bold]")
                        safe_print(f"[dim]ğŸ’¡ è¾“å…¥ [bold]/help[/bold] æŸ¥çœ‹å‘½ä»¤ | [bold]Ctrl+C[/bold] é€€å‡º | [bold]Alt+Enter[/bold] æ¢è¡Œ[/dim]")
                        safe_print(f"[dim]{'â”€' * 60}[/dim]\n")

                        while True:
                            try:
                                # è·å–ç”¨æˆ·è¾“å…¥ï¼ˆæ”¯æŒå¤šè¡Œï¼Œå¼‚æ­¥ç‰ˆæœ¬é¿å…ä¸ asyncio å†²çªï¼‰
                                user_input = (await advanced_input.get_input_async("You")).strip()

                                # æ£€æŸ¥é€€å‡ºå‘½ä»¤
                                if user_input.lower() in ["quit", "exit", "q", "é€€å‡º"]:
                                    safe_print("[dim]ğŸ‘‹ å†è§ï¼[/dim]")
                                    break

                                if not user_input:
                                    continue

                                # å¤„ç†å¿«æ·å‘½ä»¤
                                if ChatCommands.is_command(user_input):
                                    cmd, args = ChatCommands.parse(user_input)

                                    if cmd == '/help':
                                        ChatCommands.show_help()
                                        continue

                                    elif cmd == '/clear':
                                        messages = ChatCommands.handle_clear(messages, system_prompt)
                                        continue

                                    elif cmd == '/save':
                                        ChatCommands.handle_save(messages, args if args else None)
                                        continue

                                    elif cmd == '/model':
                                        if args:
                                            current_model = args.strip()
                                            # é‡æ–°åˆ›å»ºå®¢æˆ·ç«¯
                                            client = MllmClient(
                                                model=current_model,
                                                base_url=base_url_val,
                                                api_key=api_key_val,
                                                **kwargs,
                                            )
                                            safe_print(f"[green]âœ… æ¨¡å‹å·²åˆ‡æ¢ä¸º: {current_model}[/green]\n")
                                        else:
                                            safe_print(f"[cyan]å½“å‰æ¨¡å‹: {current_model}[/cyan]")
                                            safe_print(f"[dim]ç”¨æ³•: /model <æ¨¡å‹å>[/dim]\n")
                                        continue

                                    elif cmd == '/retry':
                                        messages, should_retry = ChatCommands.handle_retry(messages)
                                        if not should_retry:
                                            continue
                                        # ç»§ç»­æ‰§è¡Œä¸‹é¢çš„ç”Ÿæˆé€»è¾‘
                                    else:
                                        safe_print(f"[yellow]âš ï¸  æœªçŸ¥å‘½ä»¤: {cmd}[/yellow]")
                                        safe_print(f"[dim]è¾“å…¥ /help æŸ¥çœ‹å¯ç”¨å‘½ä»¤[/dim]\n")
                                        continue

                                # æ£€æµ‹æ˜¯å¦åŒ…å«å›¾ç‰‡è·¯å¾„æˆ–URL
                                import os
                                import re

                                # /retry æ—¶ä¸éœ€è¦æ·»åŠ æ–°æ¶ˆæ¯ï¼Œç›´æ¥é‡æ–°ç”Ÿæˆ
                                is_retry = ChatCommands.is_command(user_input) and ChatCommands.parse(user_input)[0] == '/retry'
                                image_path = None
                                text_content = user_input

                                if not is_retry:
                                    # æ£€æŸ¥æ˜¯å¦æ˜¯URL
                                    url_pattern = r'(https?://[^\s]+\.(?:jpg|jpeg|png|gif|bmp|webp)(?:\?[^\s]*)?)'
                                    url_match = re.search(url_pattern, user_input, re.IGNORECASE)

                                    if url_match:
                                        image_path = url_match.group(1)
                                        text_content = user_input.replace(image_path, "").strip()
                                        if not text_content:
                                            text_content = "è¯·æè¿°è¿™å¼ å›¾ç‰‡"
                                    else:
                                        # æ£€æŸ¥æ˜¯å¦åŒ…å«æœ¬åœ°æ–‡ä»¶è·¯å¾„
                                        # æ”¯æŒå¤šç§æ ¼å¼ï¼šç»å¯¹è·¯å¾„ã€ç›¸å¯¹è·¯å¾„ã€å¸¦å¼•å·çš„è·¯å¾„
                                        path_patterns = [
                                            r'"([^"]+\.(?:jpg|jpeg|png|gif|bmp|webp))"',  # åŒå¼•å·è·¯å¾„
                                            r"'([^']+\.(?:jpg|jpeg|png|gif|bmp|webp))'",  # å•å¼•å·è·¯å¾„
                                            r'([^\s]+\.(?:jpg|jpeg|png|gif|bmp|webp))(?:\s|$)',  # æ— å¼•å·è·¯å¾„
                                        ]

                                        for pattern in path_patterns:
                                            match = re.search(pattern, user_input, re.IGNORECASE)
                                            if match:
                                                potential_path = match.group(1)
                                                # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
                                                if os.path.exists(potential_path):
                                                    image_path = os.path.abspath(potential_path)
                                                    text_content = user_input.replace(match.group(0), "").strip()
                                                    if not text_content:
                                                        text_content = "è¯·æè¿°è¿™å¼ å›¾ç‰‡"
                                                    break
                                                # å°è¯•ç›¸å¯¹è·¯å¾„
                                                elif os.path.exists(os.path.join(os.getcwd(), potential_path)):
                                                    image_path = os.path.abspath(os.path.join(os.getcwd(), potential_path))
                                                    text_content = user_input.replace(match.group(0), "").strip()
                                                    if not text_content:
                                                        text_content = "è¯·æè¿°è¿™å¼ å›¾ç‰‡"
                                                    break

                                    # æ„å»ºæ¶ˆæ¯å†…å®¹
                                    if image_path:
                                        # å¦‚æœæ˜¯æœ¬åœ°æ–‡ä»¶ï¼Œè½¬æ¢ä¸ºfile://æ ¼å¼
                                        if not image_path.startswith('http'):
                                            image_url = f"file://{image_path.replace(os.sep, '/')}"
                                        else:
                                            image_url = image_path

                                        safe_print(f"[dim]ğŸ“· å‘é€å›¾ç‰‡: {image_path}[/dim]")
                                        message_content = [
                                            {"type": "text", "text": text_content},
                                            {"type": "image_url", "image_url": {"url": image_url}}
                                        ]
                                    else:
                                        message_content = user_input

                                    # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯åˆ°å†å²
                                    messages.append({"role": "user", "content": message_content})

                                if stream:
                                    # æµå¼è¾“å‡º - ä½¿ç”¨ä¼˜é›…çš„Markdownæ¸²æŸ“å™¨
                                    safe_print(f"[bold blue]Assistant:[/bold blue] ")

                                    renderer = StreamingMarkdownRenderer()
                                    stream_interrupted = False
                                    try:
                                        async for token in client.call_llm_stream(
                                            messages=messages,
                                            temperature=temperature,
                                            max_tokens=max_tokens,
                                            **kwargs,
                                        ):
                                            renderer.add_token(token)
                                    except KeyboardInterrupt:
                                        stream_interrupted = True
                                        safe_print_stream("\n")
                                        safe_print("[dim]â¸ï¸  è¾“å‡ºå·²ä¸­æ–­[/dim]")

                                    # å®Œæˆæµå¼è¾“å‡º
                                    if not stream_interrupted:
                                        renderer.finalize()
                                    full_response = renderer.buffer

                                    # æ·»åŠ åŠ©æ‰‹å“åº”åˆ°å†å²ï¼ˆå³ä½¿ä¸­æ–­ä¹Ÿä¿å­˜å·²è·å–çš„å†…å®¹ï¼‰
                                    if full_response:
                                        messages.append(
                                            {
                                                "role": "assistant",
                                                "content": full_response,
                                            }
                                        )
                                else:
                                    # éæµå¼è¾“å‡ºï¼Œä½¿ç”¨Markdownæ¸²æŸ“
                                    results = await client.call_llm(
                                        messages_list=[messages],
                                        show_progress=False,
                                        temperature=temperature,
                                        max_tokens=max_tokens,
                                        **kwargs,
                                    )
                                    response = (
                                        results[0]
                                        if results and results[0]
                                        else "æ— å“åº”"
                                    )
                                    safe_print(f"[bold blue]Assistant:[/bold blue]")
                                    safe_print_markdown(response)

                                    # æ·»åŠ åŠ©æ‰‹å“åº”åˆ°å†å²
                                    if response and response != "æ— å“åº”":
                                        messages.append(
                                            {"role": "assistant", "content": response}
                                        )

                            except KeyboardInterrupt:
                                safe_print("\n[dim]ğŸ‘‹ å†è§ï¼[/dim]")
                                break
                            except EOFError:
                                safe_print("\n[dim]ğŸ‘‹ å†è§ï¼[/dim]")
                                break
                            except Exception as e:
                                safe_print(f"[red]âŒ å¤„ç†é”™è¯¯: {e}[/red]")
                                safe_print("[yellow]ğŸ’¡ è¯·é‡è¯•æˆ–è¾“å…¥ 'quit' é€€å‡º[/yellow]")
                                continue

                    except Exception as e:
                        safe_print(f"[red]âŒ åˆå§‹åŒ–é”™è¯¯: {e}[/red]")
                        safe_print("[yellow]ğŸ’¡ è¯·æ£€æŸ¥MLLMå®¢æˆ·ç«¯é…ç½®æˆ–æœåŠ¡å™¨è¿æ¥[/yellow]")
                        return None

                try:
                    return asyncio.run(_interactive_chat())
                except KeyboardInterrupt:
                    safe_print("\n[dim]ğŸ‘‹ å†è§ï¼[/dim]")
                    return None

            # æ£€æŸ¥æ˜¯å¦åœ¨äº¤äº’ç¯å¢ƒä¸­
            import sys

            if not sys.stdin.isatty():
                safe_print("[red]âŒ é”™è¯¯: äº¤äº’æ¨¡å¼éœ€è¦åœ¨ç»ˆç«¯ä¸­è¿è¡Œ[/red]")
                safe_print(
                    "[yellow]ğŸ’¡ è¯·åœ¨äº¤äº’å¼ç»ˆç«¯ä¸­è¿è¡Œæ­¤å‘½ä»¤ï¼Œæˆ–æä¾›å…·ä½“çš„æ¶ˆæ¯å‚æ•°[/yellow]"
                )
                safe_print('[dim]ğŸ“ ç¤ºä¾‹: [bold]maque mllm chat "ä½ å¥½"[/bold][/dim]')
                return

            try:
                return run_interactive_chat()
            except KeyboardInterrupt:
                safe_print("\n[dim]ğŸ‘‹ å†è§ï¼[/dim]")
                return None

    def models(self, base_url: str = None, api_key: str = None):
        """åˆ—å‡ºå¯ç”¨æ¨¡å‹"""
        import requests

        # ä»é…ç½®è·å–é»˜è®¤å€¼
        mllm_config = self.cli.maque_config.get("mllm", {})
        base_url = base_url or mllm_config.get("base_url", "http://localhost:11434/v1")
        api_key = api_key or mllm_config.get("api_key", "EMPTY")

        try:
            headers = {"Authorization": f"Bearer {api_key}"}
            response = requests.get(
                f"{base_url.rstrip('/')}/models", headers=headers, timeout=10
            )

            if response.status_code == 200:
                models_data = response.json()

                safe_print(f"\n[bold blue]ğŸ¤– å¯ç”¨æ¨¡å‹åˆ—è¡¨[/bold blue]")
                safe_print(f"[dim]ğŸ“¡ æœåŠ¡å™¨: {base_url}[/dim]")
                safe_print(f"[dim]{'â”€' * 50}[/dim]")

                if isinstance(models_data, dict) and "data" in models_data:
                    models = models_data["data"]
                elif isinstance(models_data, list):
                    models = models_data
                else:
                    models = []

                if models:
                    for i, model in enumerate(models, 1):
                        if isinstance(model, dict):
                            model_id = model.get("id", model.get("name", "unknown"))
                            safe_print(f"[green]{i:2d}. [/green][cyan]{model_id}[/cyan]")
                        else:
                            safe_print(f"[green]{i:2d}. [/green][cyan]{model}[/cyan]")
                    safe_print(f"\n[dim]âœ… å…±æ‰¾åˆ° {len(models)} ä¸ªå¯ç”¨æ¨¡å‹[/dim]")
                else:
                    safe_print("[yellow]âš ï¸  æœªæ‰¾åˆ°å¯ç”¨æ¨¡å‹[/yellow]")
                    safe_print("[dim]ğŸ’¡ è¯·æ£€æŸ¥æœåŠ¡å™¨é…ç½®æˆ–ç½‘ç»œè¿æ¥[/dim]")

            else:
                safe_print(f"[red]âŒ è·å–æ¨¡å‹åˆ—è¡¨å¤±è´¥: HTTP {response.status_code}[/red]")
                safe_print(f"[yellow]ğŸ’¡ è¯·æ£€æŸ¥æœåŠ¡å™¨çŠ¶æ€æˆ–APIæƒé™[/yellow]")

        except requests.exceptions.RequestException as e:
            safe_print(f"[red]ğŸ”Œ è¿æ¥å¤±è´¥: {e}[/red]")
            safe_print(f"[yellow]ğŸ’¡ è¯·æ£€æŸ¥æœåŠ¡åœ°å€: [bold]{base_url}[/bold][/yellow]")
            safe_print(f"[dim]æç¤º: ç¡®ä¿æœåŠ¡å™¨æ­£åœ¨è¿è¡Œå¹¶ä¸”åœ°å€æ­£ç¡®[/dim]")
        except Exception as e:
            safe_print(f"[red]âŒ æœªçŸ¥é”™è¯¯: {e}[/red]")

    def test(
        self,
        model: str = None,
        base_url: str = None,
        api_key: str = None,
        message: str = "Hello, please respond with 'OK' if you can see this message.",
        timeout: int = 30,
    ):
        """æµ‹è¯•MLLMæœåŠ¡è¿æ¥å’Œé…ç½®

        Args:
            model: æ¨¡å‹åç§°ï¼ˆå¯é€‰ï¼Œä¸æŒ‡å®šåˆ™åªæµ‹è¯•è¿æ¥ï¼‰
            base_url: APIæœåŠ¡åœ°å€
            api_key: APIå¯†é’¥
            message: æµ‹è¯•æ¶ˆæ¯
            timeout: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
        """
        import requests
        import time

        # ä»é…ç½®è·å–é»˜è®¤å€¼
        mllm_config = self.cli.maque_config.get("mllm", {})
        base_url = base_url or mllm_config.get("base_url", "http://localhost:11434/v1")
        api_key = api_key or mllm_config.get("api_key", "EMPTY")
        model = model or mllm_config.get("model")

        safe_print(f"\n[bold blue]ğŸ” MLLM æœåŠ¡è¿æ¥æµ‹è¯•[/bold blue]")
        safe_print(f"[dim]{'â”€' * 50}[/dim]")

        results = {
            "connection": False,
            "models_api": False,
            "chat_api": False,
        }

        # 1. æµ‹è¯•åŸºæœ¬è¿æ¥
        safe_print(f"\n[cyan]1. æµ‹è¯•æœåŠ¡å™¨è¿æ¥...[/cyan]")
        safe_print(f"   [dim]åœ°å€: {base_url}[/dim]")
        try:
            start_time = time.time()
            response = requests.get(
                f"{base_url.rstrip('/')}/models",
                headers={"Authorization": f"Bearer {api_key}"},
                timeout=timeout
            )
            elapsed = time.time() - start_time

            if response.status_code == 200:
                safe_print(f"   [green]âœ… è¿æ¥æˆåŠŸ[/green] [dim]({elapsed:.2f}s)[/dim]")
                results["connection"] = True
                results["models_api"] = True

                # è§£ææ¨¡å‹åˆ—è¡¨
                models_data = response.json()
                if isinstance(models_data, dict) and "data" in models_data:
                    models = models_data["data"]
                elif isinstance(models_data, list):
                    models = models_data
                else:
                    models = []

                model_count = len(models)
                safe_print(f"   [dim]å¯ç”¨æ¨¡å‹æ•°: {model_count}[/dim]")

            elif response.status_code == 401:
                safe_print(f"   [yellow]âš ï¸  è®¤è¯å¤±è´¥ (401)[/yellow]")
                safe_print(f"   [dim]è¯·æ£€æŸ¥ API Key æ˜¯å¦æ­£ç¡®[/dim]")
                results["connection"] = True
            elif response.status_code == 404:
                safe_print(f"   [yellow]âš ï¸  /models ç«¯ç‚¹ä¸å­˜åœ¨ (404)[/yellow]")
                safe_print(f"   [dim]æœåŠ¡å™¨å¯èƒ½ä¸æ”¯æŒ OpenAI å…¼å®¹ API[/dim]")
                results["connection"] = True
            else:
                safe_print(f"   [yellow]âš ï¸  HTTP {response.status_code}[/yellow]")
                results["connection"] = True

        except requests.exceptions.ConnectionError:
            safe_print(f"   [red]âŒ è¿æ¥å¤±è´¥: æ— æ³•è¿æ¥åˆ°æœåŠ¡å™¨[/red]")
            safe_print(f"   [dim]è¯·æ£€æŸ¥æœåŠ¡å™¨æ˜¯å¦è¿è¡Œåœ¨ {base_url}[/dim]")
        except requests.exceptions.Timeout:
            safe_print(f"   [red]âŒ è¿æ¥è¶…æ—¶ ({timeout}s)[/red]")
        except Exception as e:
            safe_print(f"   [red]âŒ è¿æ¥é”™è¯¯: {e}[/red]")

        # 2. æµ‹è¯• Chat APIï¼ˆå¦‚æœæŒ‡å®šäº†æ¨¡å‹ï¼‰
        if model and results["connection"]:
            safe_print(f"\n[cyan]2. æµ‹è¯• Chat API...[/cyan]")
            safe_print(f"   [dim]æ¨¡å‹: {model}[/dim]")

            try:
                start_time = time.time()
                response = requests.post(
                    f"{base_url.rstrip('/')}/chat/completions",
                    headers={
                        "Authorization": f"Bearer {api_key}",
                        "Content-Type": "application/json"
                    },
                    json={
                        "model": model,
                        "messages": [{"role": "user", "content": message}],
                        "max_tokens": 50,
                        "temperature": 0.1
                    },
                    timeout=timeout
                )
                elapsed = time.time() - start_time

                if response.status_code == 200:
                    data = response.json()
                    content = ""
                    if "choices" in data and data["choices"]:
                        content = data["choices"][0].get("message", {}).get("content", "")

                    safe_print(f"   [green]âœ… Chat API æ­£å¸¸[/green] [dim]({elapsed:.2f}s)[/dim]")
                    if content:
                        # æˆªæ–­è¿‡é•¿çš„å“åº”
                        display_content = content[:100] + "..." if len(content) > 100 else content
                        safe_print(f"   [dim]å“åº”: {display_content}[/dim]")
                    results["chat_api"] = True

                    # æ˜¾ç¤º token ä½¿ç”¨æƒ…å†µ
                    usage = data.get("usage", {})
                    if usage:
                        safe_print(f"   [dim]Token ä½¿ç”¨: prompt={usage.get('prompt_tokens', 'N/A')}, "
                                  f"completion={usage.get('completion_tokens', 'N/A')}[/dim]")

                elif response.status_code == 404:
                    safe_print(f"   [yellow]âš ï¸  æ¨¡å‹ä¸å­˜åœ¨æˆ– API ç«¯ç‚¹ä¸å¯ç”¨[/yellow]")
                    safe_print(f"   [dim]è¯·æ£€æŸ¥æ¨¡å‹åç§°: {model}[/dim]")
                elif response.status_code == 401:
                    safe_print(f"   [yellow]âš ï¸  è®¤è¯å¤±è´¥[/yellow]")
                else:
                    safe_print(f"   [yellow]âš ï¸  HTTP {response.status_code}[/yellow]")
                    try:
                        error_detail = response.json()
                        safe_print(f"   [dim]{error_detail}[/dim]")
                    except:
                        pass

            except requests.exceptions.Timeout:
                safe_print(f"   [yellow]âš ï¸  è¯·æ±‚è¶…æ—¶ ({timeout}s)[/yellow]")
                safe_print(f"   [dim]æ¨¡å‹å¯èƒ½æ­£åœ¨åŠ è½½æˆ–æœåŠ¡å™¨ç¹å¿™[/dim]")
            except Exception as e:
                safe_print(f"   [red]âŒ è¯·æ±‚å¤±è´¥: {e}[/red]")

        # 3. æ€»ç»“
        safe_print(f"\n[dim]{'â”€' * 50}[/dim]")
        safe_print(f"[bold]æµ‹è¯•ç»“æœæ±‡æ€»:[/bold]")

        status_icons = {True: "[green]âœ…[/green]", False: "[red]âŒ[/red]"}

        safe_print(f"  {status_icons[results['connection']]} æœåŠ¡å™¨è¿æ¥")
        safe_print(f"  {status_icons[results['models_api']]} Models API")
        if model:
            safe_print(f"  {status_icons[results['chat_api']]} Chat API ({model})")

        # ç»™å‡ºå»ºè®®
        if all(results.values()) or (results["connection"] and results["models_api"] and not model):
            safe_print(f"\n[green]ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼MLLM æœåŠ¡é…ç½®æ­£ç¡®ã€‚[/green]")
        else:
            safe_print(f"\n[yellow]ğŸ’¡ å»ºè®®:[/yellow]")
            if not results["connection"]:
                safe_print(f"   - æ£€æŸ¥æœåŠ¡å™¨æ˜¯å¦å¯åŠ¨")
                safe_print(f"   - æ£€æŸ¥ base_url é…ç½®æ˜¯å¦æ­£ç¡®")
            if results["connection"] and not results["models_api"]:
                safe_print(f"   - æ£€æŸ¥ API Key æ˜¯å¦æ­£ç¡®")
                safe_print(f"   - ç¡®è®¤æœåŠ¡å™¨æ”¯æŒ OpenAI å…¼å®¹ API")
            if model and not results["chat_api"]:
                safe_print(f"   - æ£€æŸ¥æ¨¡å‹åç§°æ˜¯å¦æ­£ç¡®")
                safe_print(f"   - ä½¿ç”¨ 'mq mllm models' æŸ¥çœ‹å¯ç”¨æ¨¡å‹")

        return results

    def chain_analysis(
        self,
        query: str,
        steps: int = 3,
        model: str = None,
        base_url: str = None,
        api_key: str = None,
        temperature: float = 0.1,
        max_tokens: int = 2000,
        show_details: bool = False,
        **kwargs,
    ):
        """ä½¿ç”¨Chain of Thoughtè¿›è¡Œåˆ†ææ¨ç†
        
        Args:
            query: è¦åˆ†æçš„é—®é¢˜æˆ–å†…å®¹
            steps: åˆ†ææ­¥éª¤æ•°ï¼Œé»˜è®¤3æ­¥
            model: ä½¿ç”¨çš„æ¨¡å‹
            base_url: APIæœåŠ¡åœ°å€
            api_key: APIå¯†é’¥
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§tokenæ•°
            show_details: æ˜¯å¦æ˜¾ç¤ºæ¯ä¸ªæ­¥éª¤çš„è¯¦ç»†ä¿¡æ¯
        """
        import asyncio
        from flexllm.chain_of_thought_client import ChainOfThoughtClient, LinearStep, ExecutionConfig
        from flexllm.openaiclient import OpenAIClient

        # ä»é…ç½®è·å–é»˜è®¤å€¼
        mllm_config = self.cli.maque_config.get("mllm", {})
        model = model or mllm_config.get("model", "gemma3:latest")
        base_url = base_url or mllm_config.get("base_url", "http://localhost:11434/v1")
        api_key = api_key or mllm_config.get("api_key", "EMPTY")

        async def run_chain_analysis():
            try:
                safe_print(f"[bold green]ğŸ” å¼€å§‹Chain of Thoughtåˆ†ææ¨ç†[/bold green]")
                safe_print(f"[cyan]ğŸ“ é—®é¢˜: {query}[/cyan]")
                safe_print(f"[dim]ğŸ”§ æ¨¡å‹: {model}, æ­¥éª¤æ•°: {steps}[/dim]\n")

                # åˆå§‹åŒ–å®¢æˆ·ç«¯
                openai_client = OpenAIClient(model=model, base_url=base_url, api_key=api_key)
                
                # é…ç½®æ‰§è¡Œå‚æ•°
                config = ExecutionConfig(
                    enable_monitoring=True,
                    enable_progress=show_details,
                    log_level="INFO" if show_details else "WARNING"
                )
                
                chain_client = ChainOfThoughtClient(openai_client, config)

                # å®šä¹‰åˆ†ææ­¥éª¤
                def create_analysis_step(step_num: int, step_name: str, prompt_template: str):
                    def prepare_messages(context):
                        previous_analysis = ""
                        if context.history:
                            previous_analysis = "\n\n".join([
                                f"æ­¥éª¤{i+1}: {step.response}" 
                                for i, step in enumerate(context.history)
                            ])
                        
                        system_prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åˆ†æå¸ˆï¼Œæ­£åœ¨è¿›è¡Œç¬¬{step_num}æ­¥åˆ†æã€‚
è¯·æ ¹æ®é—®é¢˜å’Œä¹‹å‰çš„åˆ†æç»“æœï¼Œ{step_name}ã€‚
ä¿æŒé€»è¾‘æ¸…æ™°ï¼Œåˆ†ææ·±å…¥ã€‚"""

                        user_prompt = prompt_template.format(
                            query=context.query,
                            previous_analysis=previous_analysis
                        )

                        return [
                            {"role": "system", "content": system_prompt},
                            {"role": "user", "content": user_prompt}
                        ]
                    
                    return LinearStep(
                        name=f"analysis_step_{step_num}",
                        prepare_messages_fn=prepare_messages,
                        model_params={
                            "temperature": temperature,
                            "max_tokens": max_tokens,
                            **kwargs
                        }
                    )

                # åˆ›å»ºåˆ†æé“¾æ¡
                analysis_steps = []
                
                if steps >= 1:
                    analysis_steps.append(create_analysis_step(
                        1, "ç†è§£å’Œåˆ†è§£é—®é¢˜",
                        "è¯·ä»”ç»†åˆ†æè¿™ä¸ªé—®é¢˜ï¼š\n{query}\n\nè¯·åˆ†è§£è¿™ä¸ªé—®é¢˜çš„å…³é”®è¦ç´ ï¼Œæ˜ç¡®åˆ†æçš„æ–¹å‘å’Œé‡ç‚¹ã€‚"
                    ))
                
                if steps >= 2:
                    analysis_steps.append(create_analysis_step(
                        2, "æ·±å…¥åˆ†æå„ä¸ªæ–¹é¢",
                        "åŸºäºç¬¬ä¸€æ­¥çš„åˆ†æï¼š\n{previous_analysis}\n\nè¯·ä»å¤šä¸ªè§’åº¦æ·±å…¥åˆ†æé—®é¢˜ï¼Œæ¢è®¨å¯èƒ½çš„è§£å†³æ–¹æ¡ˆæˆ–ç­”æ¡ˆã€‚"
                    ))
                
                if steps >= 3:
                    analysis_steps.append(create_analysis_step(
                        3, "ç»¼åˆç»“è®ºå’Œå»ºè®®",
                        "åŸºäºå‰é¢çš„åˆ†æï¼š\n{previous_analysis}\n\nè¯·æ€»ç»“åˆ†æç»“æœï¼Œç»™å‡ºæ˜ç¡®çš„ç»“è®ºå’Œå®ç”¨çš„å»ºè®®ã€‚"
                    ))
                
                # å¦‚æœæ­¥éª¤è¶…è¿‡3æ­¥ï¼Œæ·»åŠ æ›´å¤šç»†åŒ–åˆ†æ
                for i in range(4, steps + 1):
                    analysis_steps.append(create_analysis_step(
                        i, f"è¿›ä¸€æ­¥ç»†åŒ–åˆ†æç¬¬{i-3}ä¸ªæ–¹é¢",
                        "ç»§ç»­æ·±åŒ–åˆ†æï¼š\n{previous_analysis}\n\nè¯·è¿›ä¸€æ­¥ç»†åŒ–å’Œè¡¥å……åˆ†æï¼Œæä¾›æ›´è¯¦ç»†çš„è§è§£ã€‚"
                    ))

                # åˆ›å»ºçº¿æ€§é“¾æ¡
                first_step = chain_client.create_linear_chain(analysis_steps, "analysis_chain")
                
                # æ‰§è¡Œé“¾æ¡
                context = chain_client.create_context({"query": query})
                result_context = await chain_client.execute_chain(
                    first_step, context, show_step_details=show_details
                )

                # æ˜¾ç¤ºç»“æœ
                if result_context.history:
                    safe_print(f"\n[bold blue]ğŸ¯ Chain of Thought åˆ†æç»“æœ[/bold blue]")
                    safe_print(f"[dim]{'=' * 60}[/dim]")
                    
                    for i, step_result in enumerate(result_context.history):
                        step_title = f"æ­¥éª¤ {i+1}"
                        if i == 0:
                            step_title += " - é—®é¢˜ç†è§£"
                        elif i == 1:
                            step_title += " - æ·±å…¥åˆ†æ"
                        elif i == 2:
                            step_title += " - ç»¼åˆç»“è®º"
                        else:
                            step_title += f" - ç»†åŒ–åˆ†æ {i-2}"
                            
                        safe_print(f"\n[bold cyan]{step_title}[/bold cyan]")
                        safe_print(f"[green]{step_result.response}[/green]")
                    
                    # æ‰§è¡Œæ‘˜è¦
                    summary = result_context.get_execution_summary()
                    safe_print(f"\n[dim]ğŸ“Š æ‰§è¡Œç»Ÿè®¡: {summary['total_steps']} ä¸ªæ­¥éª¤, "
                              f"è€—æ—¶ {summary['total_execution_time']:.2f}ç§’, "
                              f"æˆåŠŸç‡ {summary['success_rate']*100:.1f}%[/dim]")
                else:
                    safe_print("[red]âŒ åˆ†ææ‰§è¡Œå¤±è´¥ï¼Œæ²¡æœ‰ç”Ÿæˆç»“æœ[/red]")

            except Exception as e:
                safe_print(f"[red]âŒ Chain of Thoughtåˆ†ææ‰§è¡Œå¤±è´¥: {e}[/red]")
                safe_print("[yellow]ğŸ’¡ è¯·æ£€æŸ¥æ¨¡å‹é…ç½®å’Œç½‘ç»œè¿æ¥[/yellow]")

        return asyncio.run(run_chain_analysis())

    def chain_reasoning(
        self,
        query: str,
        model: str = None,
        base_url: str = None,
        api_key: str = None,
        temperature: float = 0.1,
        max_tokens: int = 2000,
        show_details: bool = False,
        **kwargs,
    ):
        """ä½¿ç”¨Chain of Thoughtè¿›è¡Œé€»è¾‘æ¨ç†
        
        Args:
            query: éœ€è¦æ¨ç†çš„é—®é¢˜æˆ–æƒ…å¢ƒ
            model: ä½¿ç”¨çš„æ¨¡å‹
            base_url: APIæœåŠ¡åœ°å€
            api_key: APIå¯†é’¥
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§tokenæ•°
            show_details: æ˜¯å¦æ˜¾ç¤ºæ¯ä¸ªæ­¥éª¤çš„è¯¦ç»†ä¿¡æ¯
        """
        import asyncio
        from flexllm.chain_of_thought_client import ChainOfThoughtClient, LinearStep, ExecutionConfig
        from flexllm.openaiclient import OpenAIClient

        # ä»é…ç½®è·å–é»˜è®¤å€¼
        mllm_config = self.cli.maque_config.get("mllm", {})
        model = model or mllm_config.get("model", "gemma3:latest")
        base_url = base_url or mllm_config.get("base_url", "http://localhost:11434/v1")
        api_key = api_key or mllm_config.get("api_key", "EMPTY")

        async def run_chain_reasoning():
            try:
                safe_print(f"[bold green]ğŸ§  å¼€å§‹Chain of Thoughté€»è¾‘æ¨ç†[/bold green]")
                safe_print(f"[cyan]ğŸ’­ æ¨ç†é—®é¢˜: {query}[/cyan]")
                safe_print(f"[dim]ğŸ”§ æ¨¡å‹: {model}[/dim]\n")

                # åˆå§‹åŒ–å®¢æˆ·ç«¯
                openai_client = OpenAIClient(model=model, base_url=base_url, api_key=api_key)
                
                config = ExecutionConfig(
                    enable_monitoring=True,
                    enable_progress=show_details,
                    log_level="INFO" if show_details else "WARNING"
                )
                
                chain_client = ChainOfThoughtClient(openai_client, config)

                # å®šä¹‰æ¨ç†æ­¥éª¤
                def create_reasoning_step(step_name: str, prompt_template: str):
                    def prepare_messages(context):
                        previous_reasoning = ""
                        if context.history:
                            previous_reasoning = "\n\n".join([
                                f"[{step.step_name}]: {step.response}" 
                                for step in context.history
                            ])
                        
                        return [
                            {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªé€»è¾‘æ¨ç†ä¸“å®¶ã€‚è¯·ä½¿ç”¨ä¸¥è°¨çš„é€»è¾‘æ€ç»´ï¼Œä¸€æ­¥ä¸€æ­¥åœ°åˆ†æå’Œæ¨ç†ã€‚æ¯ä¸€æ­¥éƒ½è¦æœ‰æ˜ç¡®çš„é€»è¾‘ä¾æ®ã€‚"},
                            {"role": "user", "content": prompt_template.format(
                                query=context.query,
                                previous_reasoning=previous_reasoning
                            )}
                        ]
                    
                    return LinearStep(
                        name=step_name,
                        prepare_messages_fn=prepare_messages,
                        model_params={
                            "temperature": temperature,
                            "max_tokens": max_tokens,
                            **kwargs
                        }
                    )

                # åˆ›å»ºæ¨ç†é“¾æ¡
                reasoning_steps = [
                    create_reasoning_step(
                        "observation",
                        "é¦–å…ˆï¼Œè®©æˆ‘è§‚å¯Ÿå’Œç†è§£è¿™ä¸ªé—®é¢˜ï¼š\n{query}\n\nè¯·ä»”ç»†è§‚å¯Ÿé—®é¢˜ä¸­çš„å…³é”®ä¿¡æ¯ã€å·²çŸ¥æ¡ä»¶å’Œè¦æ±‚è§£ç­”çš„å†…å®¹ã€‚åˆ—å‡ºæ‰€æœ‰é‡è¦çš„äº‹å®å’Œå‡è®¾ã€‚"
                    ),
                    create_reasoning_step(
                        "hypothesis",
                        "åŸºäºè§‚å¯Ÿåˆ°çš„ä¿¡æ¯ï¼š\n{previous_reasoning}\n\nç°åœ¨è¯·æå‡ºå¯èƒ½çš„å‡è®¾æˆ–è§£å†³æ–¹æ¡ˆã€‚è€ƒè™‘å¤šç§å¯èƒ½æ€§ï¼Œå¹¶è¯´æ˜æ¯ç§å‡è®¾çš„ä¾æ®ã€‚"
                    ),
                    create_reasoning_step(
                        "deduction",
                        "åŸºäºå‰é¢çš„è§‚å¯Ÿå’Œå‡è®¾ï¼š\n{previous_reasoning}\n\nç°åœ¨è¿›è¡Œé€»è¾‘æ¨å¯¼ã€‚ä½¿ç”¨æ¼”ç»æ¨ç†ï¼Œä»å·²çŸ¥æ¡ä»¶æ¨å¯¼å‡ºç»“è®ºã€‚ç¡®ä¿æ¯ä¸€æ­¥æ¨ç†éƒ½æœ‰æ˜ç¡®çš„é€»è¾‘å…³ç³»ã€‚"
                    ),
                    create_reasoning_step(
                        "verification",
                        "åŸºäºæ¨ç†è¿‡ç¨‹ï¼š\n{previous_reasoning}\n\nç°åœ¨éªŒè¯æ¨ç†ç»“æœã€‚æ£€æŸ¥é€»è¾‘æ˜¯å¦ä¸€è‡´ï¼Œç»“è®ºæ˜¯å¦åˆç†ï¼Œæ˜¯å¦é—æ¼äº†é‡è¦å› ç´ ã€‚å¦‚æœå‘ç°é—®é¢˜ï¼Œè¯·æŒ‡å‡ºå¹¶ä¿®æ­£ã€‚"
                    ),
                    create_reasoning_step(
                        "conclusion",
                        "ç»¼åˆæ•´ä¸ªæ¨ç†è¿‡ç¨‹ï¼š\n{previous_reasoning}\n\nè¯·ç»™å‡ºæœ€ç»ˆç»“è®ºã€‚æ€»ç»“æ¨ç†çš„å…³é”®æ­¥éª¤ï¼Œæ˜ç¡®å›ç­”åŸå§‹é—®é¢˜ï¼Œå¹¶è¯´æ˜ç»“è®ºçš„å¯ä¿¡åº¦ã€‚"
                    )
                ]

                # åˆ›å»ºå’Œæ‰§è¡Œé“¾æ¡
                first_step = chain_client.create_linear_chain(reasoning_steps, "reasoning_chain")
                context = chain_client.create_context({"query": query})
                result_context = await chain_client.execute_chain(
                    first_step, context, show_step_details=show_details
                )

                # æ˜¾ç¤ºæ¨ç†ç»“æœ
                if result_context.history:
                    safe_print(f"\n[bold blue]ğŸ¯ Chain of Thought æ¨ç†ç»“æœ[/bold blue]")
                    safe_print(f"[dim]{'=' * 60}[/dim]")
                    
                    step_names = {
                        "observation": "ğŸ” è§‚å¯Ÿåˆ†æ",
                        "hypothesis": "ğŸ’¡ å‡è®¾æå‡º", 
                        "deduction": "ğŸ”— é€»è¾‘æ¨å¯¼",
                        "verification": "âœ… éªŒè¯æ£€æŸ¥",
                        "conclusion": "ğŸ¯ æœ€ç»ˆç»“è®º"
                    }
                    
                    for step_result in result_context.history:
                        step_display = step_names.get(step_result.step_name, step_result.step_name)
                        safe_print(f"\n[bold cyan]{step_display}[/bold cyan]")
                        safe_print(f"[green]{step_result.response}[/green]")
                    
                    # æ‰§è¡Œæ‘˜è¦
                    summary = result_context.get_execution_summary()
                    safe_print(f"\n[dim]ğŸ“Š æ¨ç†ç»Ÿè®¡: {summary['total_steps']} ä¸ªæ­¥éª¤, "
                              f"è€—æ—¶ {summary['total_execution_time']:.2f}ç§’, "
                              f"æˆåŠŸç‡ {summary['success_rate']*100:.1f}%[/dim]")
                else:
                    safe_print("[red]âŒ æ¨ç†æ‰§è¡Œå¤±è´¥ï¼Œæ²¡æœ‰ç”Ÿæˆç»“æœ[/red]")

            except Exception as e:
                safe_print(f"[red]âŒ Chain of Thoughtæ¨ç†æ‰§è¡Œå¤±è´¥: {e}[/red]")
                safe_print("[yellow]ğŸ’¡ è¯·æ£€æŸ¥æ¨¡å‹é…ç½®å’Œç½‘ç»œè¿æ¥[/yellow]")

        return asyncio.run(run_chain_reasoning())

    def chain_run(
        self,
        config_file: str,
        input_data: str = None,
        model: str = None,
        base_url: str = None,
        api_key: str = None,
        show_details: bool = False,
        **kwargs,
    ):
        """è¿è¡Œè‡ªå®šä¹‰çš„Chain of Thoughté…ç½®æ–‡ä»¶
        
        Args:
            config_file: YAMLæ ¼å¼çš„é“¾æ¡é…ç½®æ–‡ä»¶è·¯å¾„
            input_data: è¾“å…¥æ•°æ®ï¼Œä¼šä½œä¸ºqueryä¼ å…¥
            model: ä½¿ç”¨çš„æ¨¡å‹ï¼ˆè¦†ç›–é…ç½®æ–‡ä»¶ä¸­çš„è®¾ç½®ï¼‰
            base_url: APIæœåŠ¡åœ°å€
            api_key: APIå¯†é’¥
            show_details: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†æ‰§è¡Œä¿¡æ¯
        """
        import asyncio
        import yaml
        import os
        from pathlib import Path
        from flexllm.chain_of_thought_client import ChainOfThoughtClient, LinearStep, ExecutionConfig
        from flexllm.openaiclient import OpenAIClient

        async def run_chain_config():
            try:
                # è¯»å–é…ç½®æ–‡ä»¶
                config_path = Path(config_file)
                if not config_path.exists():
                    safe_print(f"[red]âŒ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_file}[/red]")
                    return

                safe_print(f"[bold green]ğŸ“‹ è¿è¡ŒChain of Thoughté…ç½®[/bold green]")
                safe_print(f"[cyan]ğŸ“ é…ç½®æ–‡ä»¶: {config_file}[/cyan]")

                with open(config_path, 'r', encoding='utf-8') as f:
                    config = yaml.safe_load(f)

                # ä»é…ç½®æ–‡ä»¶å’Œå‘½ä»¤è¡Œå‚æ•°åˆå¹¶è®¾ç½®
                mllm_config = self.cli.maque_config.get("mllm", {})
                
                # æ¨¡å‹é…ç½®ä¼˜å…ˆçº§: å‘½ä»¤è¡Œ > é…ç½®æ–‡ä»¶ > å…¨å±€é…ç½®
                final_model = model or config.get('model') or mllm_config.get("model", "gemma3:latest")
                final_base_url = base_url or config.get('base_url') or mllm_config.get("base_url", "http://localhost:11434/v1")
                final_api_key = api_key or config.get('api_key') or mllm_config.get("api_key", "EMPTY")

                # è·å–è¾“å…¥æ•°æ®
                query = input_data or config.get('query', '')
                if not query:
                    safe_print("[red]âŒ ç¼ºå°‘è¾“å…¥æ•°æ®ï¼Œè¯·é€šè¿‡ --input-data å‚æ•°æˆ–åœ¨é…ç½®æ–‡ä»¶ä¸­çš„ 'query' å­—æ®µæŒ‡å®š[/red]")
                    return

                safe_print(f"[cyan]ğŸ“ è¾“å…¥: {query}[/cyan]")
                safe_print(f"[dim]ğŸ”§ æ¨¡å‹: {final_model}[/dim]\n")

                # åˆå§‹åŒ–å®¢æˆ·ç«¯
                openai_client = OpenAIClient(model=final_model, base_url=final_base_url, api_key=final_api_key)
                
                # æ‰§è¡Œé…ç½®
                exec_config = ExecutionConfig(
                    enable_monitoring=config.get('enable_monitoring', True),
                    enable_progress=show_details,
                    log_level="INFO" if show_details else "WARNING",
                    step_timeout=config.get('step_timeout'),
                    chain_timeout=config.get('chain_timeout'),
                    max_retries=config.get('max_retries', 0),
                    retry_delay=config.get('retry_delay', 1.0)
                )
                
                chain_client = ChainOfThoughtClient(openai_client, exec_config)

                # æ„å»ºæ­¥éª¤
                steps = config.get('steps', [])
                if not steps:
                    safe_print("[red]âŒ é…ç½®æ–‡ä»¶ä¸­æ²¡æœ‰å®šä¹‰æ­¥éª¤[/red]")
                    return

                def create_config_step(step_config):
                    step_name = step_config['name']
                    system_prompt = step_config.get('system_prompt', '')
                    user_prompt = step_config.get('user_prompt', '')
                    
                    def prepare_messages(context):
                        # å¤„ç†æ¨¡æ¿å˜é‡
                        template_vars = {
                            'query': context.query,
                            'previous_responses': '\n\n'.join([f"[{s.step_name}]: {s.response}" for s in context.history])
                        }
                        
                        # æ·»åŠ è‡ªå®šä¹‰å˜é‡
                        custom_vars = context.get_custom_data('template_vars', {})
                        template_vars.update(custom_vars)
                        
                        messages = []
                        if system_prompt:
                            messages.append({
                                "role": "system", 
                                "content": system_prompt.format(**template_vars)
                            })
                        
                        messages.append({
                            "role": "user",
                            "content": user_prompt.format(**template_vars)
                        })
                        
                        return messages
                    
                    # è·å–æ¨¡å‹å‚æ•°
                    model_params = step_config.get('model_params', {})
                    model_params.update(kwargs)  # å‘½ä»¤è¡Œå‚æ•°è¦†ç›–
                    
                    return LinearStep(
                        name=step_name,
                        prepare_messages_fn=prepare_messages,
                        model_params=model_params
                    )

                # åˆ›å»ºæ‰€æœ‰æ­¥éª¤
                chain_steps = [create_config_step(step_config) for step_config in steps]
                
                # åˆ›å»ºå’Œæ‰§è¡Œé“¾æ¡
                chain_name = config.get('name', 'custom_chain')
                first_step = chain_client.create_linear_chain(chain_steps, chain_name)
                
                # æ·»åŠ è‡ªå®šä¹‰æ¨¡æ¿å˜é‡åˆ°ä¸Šä¸‹æ–‡
                context = chain_client.create_context({"query": query})
                if config.get('template_vars'):
                    context.add_custom_data('template_vars', config['template_vars'])
                
                result_context = await chain_client.execute_chain(
                    first_step, context, show_step_details=show_details
                )

                # æ˜¾ç¤ºç»“æœ
                if result_context.history:
                    safe_print(f"\n[bold blue]ğŸ¯ {config.get('name', 'Chain')} æ‰§è¡Œç»“æœ[/bold blue]")
                    safe_print(f"[dim]{'=' * 60}[/dim]")
                    
                    for step_result in result_context.history:
                        step_display = step_result.step_name.replace('_', ' ').title()
                        safe_print(f"\n[bold cyan]ğŸ“ {step_display}[/bold cyan]")
                        safe_print(f"[green]{step_result.response}[/green]")
                    
                    # æ‰§è¡Œæ‘˜è¦
                    summary = result_context.get_execution_summary()
                    safe_print(f"\n[dim]ğŸ“Š æ‰§è¡Œç»Ÿè®¡: {summary['total_steps']} ä¸ªæ­¥éª¤, "
                              f"è€—æ—¶ {summary['total_execution_time']:.2f}ç§’, "
                              f"æˆåŠŸç‡ {summary['success_rate']*100:.1f}%[/dim]")
                else:
                    safe_print("[red]âŒ é“¾æ¡æ‰§è¡Œå¤±è´¥ï¼Œæ²¡æœ‰ç”Ÿæˆç»“æœ[/red]")

            except yaml.YAMLError as e:
                safe_print(f"[red]âŒ YAMLé…ç½®æ–‡ä»¶è§£æé”™è¯¯: {e}[/red]")
            except FileNotFoundError as e:
                safe_print(f"[red]âŒ é…ç½®æ–‡ä»¶æœªæ‰¾åˆ°: {e}[/red]")
            except Exception as e:
                safe_print(f"[red]âŒ Chainæ‰§è¡Œå¤±è´¥: {e}[/red]")
                safe_print("[yellow]ğŸ’¡ è¯·æ£€æŸ¥é…ç½®æ–‡ä»¶æ ¼å¼å’Œæ¨¡å‹è¿æ¥[/yellow]")

        return asyncio.run(run_chain_config())
