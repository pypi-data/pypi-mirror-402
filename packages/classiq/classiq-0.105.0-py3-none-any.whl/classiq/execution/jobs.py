import warnings
import webbrowser
from dataclasses import asdict, dataclass
from datetime import datetime
from typing import TYPE_CHECKING, Any

if TYPE_CHECKING:
    import pandas as pd
from urllib.parse import urljoin

import httpx

from classiq.interface.exceptions import (
    ClassiqAPIError,
    ClassiqError,
)
from classiq.interface.executor.execution_request import ExecutionJobDetails, JobCost
from classiq.interface.executor.execution_result import (
    ResultsCollection,
    TaggedMinimizeResult,
)
from classiq.interface.executor.result import (
    EstimationResult,
    EstimationResults,
    ExecutionDetails,
    MultipleExecutionDetails,
)
from classiq.interface.jobs import JobStatus, JSONObject
from classiq.interface.server.routes import EXECUTION_JOBS_FULL_PATH

from classiq._internals.api_wrapper import ApiWrapper
from classiq._internals.async_utils import syncify_function
from classiq._internals.client import client
from classiq._internals.jobs import JobID, JobPoller

TOTAL_COST = "total_cost"
CURRENCY_CODE = "currency_code"
COST = "cost"


class ClassiqExecutionResultError(ClassiqError):
    def __init__(self, primitive: str) -> None:
        super().__init__(
            f"Execution job does not contain a single {primitive!r} result, make sure you use the 'get_*_result' method matching the primitive you executed. You can use the 'result' method to see the general result."
        )


@dataclass
class ExecutionJobFilters:
    """
    Filter parameters for querying execution jobs.

    All filters are combined using AND logic: only jobs matching all specified filters are returned.
    Range filters (with _min/_max suffixes) are inclusive.
    Datetime filters are compared against the job's timestamps.
    """

    id: str | None = None  # Exact match on job ID
    session_id: str | None = None  # Exact match on session ID
    status: JobStatus | None = (
        None  # Exact match on job status (e.g., "COMPLETED", "FAILED")
    )
    name: str | None = None  # Exact match on job name
    provider: str | None = None  # Exact match on provider name (e.g., "ibm", "aws")
    backend: str | None = None  # Exact match on backend name
    program_id: str | None = None  # Exact match on program ID
    total_cost_min: float | None = None  # Minimum total cost (inclusive)
    total_cost_max: float | None = None  # Maximum total cost (inclusive)
    start_time_min: datetime | None = None  # Earliest job start time (inclusive)
    start_time_max: datetime | None = None  # Latest job start time (inclusive)
    end_time_min: datetime | None = None  # Earliest job end time (inclusive)
    end_time_max: datetime | None = None  # Latest job end time (inclusive)

    def format_filters(self) -> dict[str, Any]:
        """Convert filter fields to API kwargs, excluding None values and converting datetimes."""
        filter_dict = asdict(self)
        return {
            k: (v.isoformat() if isinstance(v, datetime) else v)
            for k, v in filter_dict.items()
            if v is not None
        }


class ExecutionJob:
    _details: ExecutionJobDetails
    _result: ResultsCollection | None

    def __init__(self, details: ExecutionJobDetails) -> None:
        self._details = details
        self._result = None

    @property
    def id(self) -> str:
        return self._details.id

    @property
    def name(self) -> str | None:
        return self._details.name

    @property
    def start_time(self) -> datetime:
        return self._details.start_time

    @property
    def end_time(self) -> datetime | None:
        return self._details.end_time

    @property
    def provider(self) -> str | None:
        return self._details.provider

    @property
    def backend_name(self) -> str | None:
        return self._details.backend_name

    @property
    def status(self) -> JobStatus:
        return self._details.status

    @property
    def num_shots(self) -> int | None:
        return self._details.num_shots

    @property
    def program_id(self) -> str | None:
        return self._details.program_id

    @property
    def error(self) -> str | None:
        return self._details.error

    def __repr__(self) -> str:
        class_name = self.__class__.__name__
        if self.name is None:
            return f"{class_name}(id={self.id!r})"
        else:
            return f"{class_name}(name={self.name!r}, id={self.id!r})"

    def cost(self, *, verbose: bool = False) -> str | JobCost:
        if self._details.cost is None:
            self._details.cost = JobCost()
        if verbose:
            return self._details.cost
        return f"{self._details.cost.total_cost} {self._details.cost.currency_code}"

    @classmethod
    async def from_id_async(
        cls,
        id: str,
        _http_client: httpx.AsyncClient | None = None,
    ) -> "ExecutionJob":
        details = await ApiWrapper.call_get_execution_job_details(
            JobID(job_id=id), http_client=_http_client
        )
        return cls(details)

    # `syncify_function` doesn't work well for class methods, so I wrote `from_id`
    # explicitly
    @classmethod
    def from_id(
        cls,
        id: str,
        _http_client: httpx.AsyncClient | None = None,
    ) -> "ExecutionJob":
        return syncify_function(cls.from_id_async)(id, _http_client=_http_client)

    @property
    def _job_id(self) -> JobID:
        return JobID(job_id=self.id)

    async def result_async(
        self,
        timeout_sec: float | None = None,
        _http_client: httpx.AsyncClient | None = None,
    ) -> ResultsCollection:
        await self.poll_async(timeout_sec=timeout_sec, _http_client=_http_client)

        if self.status == JobStatus.FAILED:
            raise ClassiqAPIError(self.error or "")
        if self.status == JobStatus.CANCELLED:
            raise ClassiqAPIError("Job has been cancelled.")

        if self._result is None:
            self._result = (
                await ApiWrapper.call_get_execution_job_result(
                    job_id=self._job_id,
                    http_client=_http_client,
                )
            ).results
        return self._result

    result = syncify_function(result_async)

    def result_value(self, *args: Any, **kwargs: Any) -> Any:
        return self.result(*args, **kwargs)[0].value

    def get_sample_result(
        self, _http_client: httpx.AsyncClient | None = None
    ) -> ExecutionDetails:
        """
        Returns the job's result as a single sample result after validation. If the result is not yet available, waits for it.

        Returns:
            The sample result of the execution job.

        Raises:
            ClassiqExecutionResultError: In case the result does not contain a single sample result.
            ClassiqAPIError: In case the job has failed.
        """
        results = self.result(_http_client=_http_client)
        if len(results) != 1:
            raise ClassiqExecutionResultError("sample")

        result = results[0].value
        if isinstance(result, MultipleExecutionDetails) and len(result.details) == 1:
            result = result.details[0]
        if not isinstance(result, ExecutionDetails):
            raise ClassiqExecutionResultError("sample")
        for warning_str in result.warnings:
            warnings.warn(warning_str, stacklevel=2)
        return result

    def get_batch_sample_result(
        self, _http_client: httpx.AsyncClient | None = None
    ) -> list[ExecutionDetails]:
        """
        Returns the job's result as a single batch_sample result after validation. If the result is not yet available, waits for it.

        Returns:
            The batch_sample result of the execution job.

        Raises:
            ClassiqExecutionResultError: In case the result does not contain a single batch_sample result.
            ClassiqAPIError: In case the job has failed.
        """
        results = self.result(_http_client=_http_client)
        if len(results) != 1:
            raise ClassiqExecutionResultError("batch_sample")

        result = results[0].value
        if isinstance(result, ExecutionDetails):
            result_list = [result]
        elif isinstance(result, MultipleExecutionDetails):
            result_list = result.details
        else:
            raise ClassiqExecutionResultError("batch_sample")

        warning_strs = [
            warning for result in result_list for warning in result.warnings
        ]
        for warning_str in warning_strs:
            warnings.warn(warning_str, stacklevel=2)
        return result_list

    def get_estimate_result(
        self, _http_client: httpx.AsyncClient | None = None
    ) -> EstimationResult:
        """
        Returns the job's result as a single estimate result after validation. If the result is not yet available, waits for it.

        Returns:
            The estimate result of the execution job.

        Raises:
            ClassiqExecutionResultError: In case the result does not contain a single estimate result.
            ClassiqAPIError: In case the job has failed.
        """
        results = self.result(_http_client=_http_client)
        if len(results) != 1:
            raise ClassiqExecutionResultError("estimate")

        result = results[0].value
        if isinstance(result, EstimationResult):
            return result
        if isinstance(result, EstimationResults) and len(result.results) == 1:
            return result.results[0]
        raise ClassiqExecutionResultError("estimate")

    def get_batch_estimate_result(
        self, _http_client: httpx.AsyncClient | None = None
    ) -> list[EstimationResult]:
        """
        Returns the job's result as a single batch_estimate result after validation. If the result is not yet available, waits for it.

        Returns:
            The batch_estimate result of the execution job.

        Raises:
            ClassiqExecutionResultError: In case the result does not contain a single batch_estimate result.
            ClassiqAPIError: In case the job has failed.
        """
        results = self.result(_http_client=_http_client)
        if len(results) != 1:
            raise ClassiqExecutionResultError("batch_estimate")

        result = results[0].value
        if isinstance(result, EstimationResult):
            return [result]
        if isinstance(result, EstimationResults):
            return result.results

        raise ClassiqExecutionResultError("batch_estimate")

    def get_minimization_result(
        self, _http_client: httpx.AsyncClient | None = None
    ) -> TaggedMinimizeResult:
        """
        Returns the job's result as a single minimization result after validation. If the result is not yet available, waits for it.

        Returns:
            The minimization result of the execution job.

        Raises:
            ClassiqExecutionResultError: In case the result does not contain a single minimization result.
            ClassiqAPIError: In case the job has failed.
        """
        results = self.result(_http_client=_http_client)
        if len(results) != 1:
            raise ClassiqExecutionResultError("minimization")

        result = results[0]
        if isinstance(result, TaggedMinimizeResult):
            return result
        raise ClassiqExecutionResultError("minimization")

    async def poll_async(
        self,
        timeout_sec: float | None = None,
        _http_client: httpx.AsyncClient | None = None,
    ) -> None:
        if not self.status.is_final():
            await self._poll_job(timeout_sec=timeout_sec, _http_client=_http_client)

    poll = syncify_function(poll_async)

    async def _poll_job(
        self,
        timeout_sec: float | None = None,
        _http_client: httpx.AsyncClient | None = None,
    ) -> None:
        def response_parser(json_response: JSONObject) -> bool | None:
            self._details = ExecutionJobDetails.model_validate(json_response)
            if self.status.is_final():
                return True
            return None

        poller = JobPoller(
            base_url=EXECUTION_JOBS_FULL_PATH,
        )
        await poller.poll(
            job_id=self._job_id,
            response_parser=response_parser,
            timeout_sec=timeout_sec,
            http_client=_http_client,
        )

    async def rename_async(
        self,
        name: str,
        _http_client: httpx.AsyncClient | None = None,
    ) -> None:
        self._details = await ApiWrapper.call_patch_execution_job(
            self._job_id,
            name,
            http_client=_http_client,
        )

    rename = syncify_function(rename_async)

    async def cancel_async(
        self,
        _http_client: httpx.AsyncClient | None = None,
    ) -> None:
        """
        Cancels the execution job. This implies the cancellation of any ongoing jobs
        sent to the provider during this execution job.

        The function returns without waiting to the actual cancellation. It is possible
        to continue polling the job in order to ensure its cancellation, which might
        not be immediate.
        """
        await ApiWrapper.call_cancel_execution_job(
            self._job_id,
            http_client=_http_client,
        )

    cancel = syncify_function(cancel_async)

    @property
    def ide_url(self) -> str:
        base_url = client().config.ide.unicode_string()
        return urljoin(base_url, f"jobs/{self.id}")

    def open_in_ide(self) -> None:
        webbrowser.open_new_tab(self.ide_url)


async def get_execution_jobs_async(
    offset: int = 0,
    limit: int = 50,
) -> list[ExecutionJob]:
    execution_user_jobs = await ApiWrapper.call_query_execution_jobs(
        offset, limit, http_client=None
    )
    return [ExecutionJob(details) for details in execution_user_jobs.results]


def get_execution_jobs(
    offset: int = 0,
    limit: int = 50,
) -> list[ExecutionJob]:
    """Query execution jobs.

    Args:
        offset: Number of results to skip (default: 0)
        limit: Maximum number of results to return (default: 50)

    Returns:
        List of ExecutionJob objects.

    Examples:
        # Query all jobs:
        jobs = get_execution_jobs(limit=10)
    """
    return syncify_function(get_execution_jobs_async)(offset, limit)


def _flatten_cost_field(action_dict: dict[str, Any]) -> dict[str, Any]:
    cost_obj = action_dict.pop(COST, {}) or {}
    action_dict[TOTAL_COST] = cost_obj.get(TOTAL_COST, 0)
    action_dict[CURRENCY_CODE] = cost_obj.get(CURRENCY_CODE)
    return action_dict


async def get_execution_actions_async(
    offset: int = 0,
    limit: int = 50,
    filters: ExecutionJobFilters | None = None,
) -> "pd.DataFrame":
    import pandas as pd

    api_kwargs = filters.format_filters() if filters is not None else {}
    execution_user_actions = await ApiWrapper.call_query_execution_jobs(
        offset, limit, http_client=None, **api_kwargs
    )
    execution_actions = execution_user_actions.results

    if not execution_actions:
        return pd.DataFrame()

    data = [_flatten_cost_field(action.model_dump()) for action in execution_actions]
    return pd.DataFrame(data)


def get_execution_actions(
    offset: int = 0, limit: int = 50, filters: ExecutionJobFilters | None = None
) -> "pd.DataFrame":
    """Query execution jobs with optional filters.

    Args:
        offset: Number of results to skip (default: 0)
        limit: Maximum number of results to return (default: 50)
        filters: Optional ExecutionJobFilters object containing filter parameters.

    Returns:
        pandas.DataFrame containing execution job information with columns:
        id, name, start_time, end_time, provider, backend_name, status,
        num_shots, program_id, error, cost.

    Examples:
        # Query all jobs:
        df = get_execution_actions(limit=10)

        # Query with filters:
        filters = ExecutionJobFilters(status="COMPLETED", provider="ibm")
        df = get_execution_actions(filters=filters, limit=10)
    """
    return syncify_function(get_execution_actions_async)(offset, limit, filters)
