"""
Test suite for AI Patch Python CLI
"""

import sys
import os
from pathlib import Path

# Import from installed package
from ai_patch.config import Config
from ai_patch.report import ReportGenerator


def test_config_auto_detect():
    """Test configuration auto-detection"""
    print("Testing config auto-detection...")
    
    # Test OpenAI-compatible
    os.environ['OPENAI_API_KEY'] = 'test-key'
    os.environ['OPENAI_BASE_URL'] = 'https://api.openai.com'
    
    config = Config.auto_detect('openai-compatible')
    
    assert config.base_url == 'https://api.openai.com', "Base URL should match"
    assert config.api_key == 'test-key', "API key should match"
    assert config.provider == 'openai-compatible', "Provider should match"
    
    print("✅ Config auto-detection test passed")


def test_report_generation():
    """Test report generation"""
    print("Testing report generation...")
    
    report_gen = ReportGenerator()
    
    # Create sample check results
    checks = {
        'streaming': {
            'status': 'warn',
            'findings': [
                {'severity': 'warning', 'message': 'TTFB is high'}
            ],
            'metrics': {'ttfb_ms': 245.5, 'total_time_s': 3.2}
        },
        'retries': {
            'status': 'pass',
            'findings': [
                {'severity': 'info', 'message': 'Recommended: Use exponential backoff'}
            ],
            'metrics': {}
        }
    }
    
    report = report_gen.create_report(
        target='all',
        provider='openai-compatible',
        base_url='https://api.openai.com',
        checks=checks,
        duration=2.5
    )
    
    assert report['version'] == '1.0.0', "Version should be 1.0.0"
    assert report['target'] == 'all', "Target should match"
    assert report['summary']['status'] == 'warning', "Status should be warning"
    assert 'streaming' in report['checks'], "Should have streaming check"
    
    # Test markdown generation
    md = report_gen.generate_markdown(report)
    assert '# AI Patch Report' in md, "Should have title"
    assert '[streaming]' in md, "Should have streaming check mention"
    assert 'Generated by AI Patch' in md, "Should have footer"
    
    print("✅ Report generation test passed")


def test_check_modules_import():
    """Test that all check modules can be imported"""
    print("Testing check module imports...")
    
    try:
        from ai_patch.checks import streaming, retries, cost, trace
        
        assert hasattr(streaming, 'check'), "streaming should have check function"
        assert hasattr(retries, 'check'), "retries should have check function"
        assert hasattr(cost, 'check'), "cost should have check function"
        assert hasattr(trace, 'check'), "trace should have check function"
        
        print("✅ Check module imports test passed")
    except ImportError as e:
        # If httpx or other dependencies are missing, just check files exist
        print(f"⚠️  Import error (missing dependencies): {e}")
        print("   Checking if check files exist instead...")
        
        checks_dir = Path(__file__).parent.parent / 'src' / 'ai_patch' / 'checks'
        check_files = ['streaming.py', 'retries.py', 'cost.py', 'trace.py']
        
        for check_file in check_files:
            assert (checks_dir / check_file).exists(), f"{check_file} should exist"
        
        print("✅ Check module files exist (dependencies not installed)")


def test_config_validation():
    """Test configuration validation"""
    print("Testing config validation...")
    
    # Valid config
    config = Config(
        base_url='https://api.openai.com',
        api_key='test-key',
        provider='openai-compatible',
        model='gpt-3.5-turbo'
    )
    
    assert config.is_valid(), "Valid config should pass validation"
    
    # Invalid config (missing API key)
    config_invalid = Config(
        base_url='https://api.openai.com',
        api_key='',
        provider='openai-compatible'
    )
    
    assert not config_invalid.is_valid(), "Invalid config should fail validation"
    assert 'OPENAI_API_KEY' in config_invalid.get_missing_vars(), "Should identify missing var"
    
    print("✅ Config validation test passed")


def test_no_next_step_in_report():
    """Test that next_step field is not in report (removed per RULE 1)"""
    print("Testing that next_step is removed from reports...")
    
    report_gen = ReportGenerator()
    
    # Create sample check results with failure
    checks = {
        'streaming': {
            'status': 'fail',
            'findings': [
                {'severity': 'error', 'message': 'Stream timeout'}
            ],
            'metrics': {}
        }
    }
    
    report = report_gen.create_report(
        target='streaming',
        provider='openai-compatible',
        base_url='https://api.openai.com',
        checks=checks,
        duration=1.5
    )
    
    # next_step should not be in summary
    assert 'next_step' not in report['summary'], "next_step should not be in report summary"
    
    # Verify summary only has expected fields
    assert 'status' in report['summary'], "Should have status"
    assert 'duration_seconds' in report['summary'], "Should have duration"
    
    print("✅ No next_step in report test passed")


def run_all_tests():
    """Run all tests"""
    print("=" * 80)
    print("AI Patch Python CLI - Test Suite")
    print("=" * 80)
    print()
    
    tests = [
        test_config_auto_detect,
        test_report_generation,
        test_check_modules_import,
        test_config_validation,
        test_no_next_step_in_report
    ]
    
    passed = 0
    failed = 0
    
    for test in tests:
        try:
            test()
            passed += 1
            print()
        except AssertionError as e:
            print(f"❌ Test failed: {e}")
            failed += 1
            print()
        except Exception as e:
            print(f"❌ Test error: {e}")
            failed += 1
            print()
    
    print("=" * 80)
    print(f"Results: {passed} passed, {failed} failed out of {len(tests)} tests")
    print("=" * 80)
    
    return failed == 0


if __name__ == '__main__':
    success = run_all_tests()
    sys.exit(0 if success else 1)
