"""
Generador de datasets masivos para entrenamiento federado de EmpoorioLM.
Crea datasets realistas de alta calidad para pruebas de escala real.
"""

import json
import random
import hashlib
from pathlib import Path
from typing import List, Dict, Any, Iterator
import asyncio
from concurrent.futures import ThreadPoolExecutor
import time
from dataclasses import dataclass
from enum import Enum


class DatasetType(Enum):
    """Tipos de datasets disponibles."""
    WIKIPEDIA_ARTICLES = "wikipedia"
    TECHNICAL_DOCS = "technical"
    CODE_REPOSITORIES = "code"
    BOOKS_TEXTS = "books"
    NEWS_ARTICLES = "news"
    SOCIAL_MEDIA = "social"


@dataclass
class DatasetConfig:
    """ConfiguraciÃ³n para generaciÃ³n de datasets."""
    dataset_type: DatasetType
    num_samples: int = 10000
    min_length: int = 100
    max_length: int = 2000
    language: str = "es"
    quality_level: str = "high"
    include_metadata: bool = True


class MassiveDatasetGenerator:
    """
    Generador de datasets masivos para entrenamiento federado.
    Crea contenido realista de alta calidad para pruebas de escala.
    """

    def __init__(self, output_dir: str = "./data/massive_datasets"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)

        # Templates de contenido por tipo
        self.templates = self._load_templates()

        # EstadÃ­sticas de generaciÃ³n
        self.stats = {
            "total_samples": 0,
            "total_tokens": 0,
            "generation_time": 0,
            "datasets_created": []
        }

    def _load_templates(self) -> Dict[str, List[str]]:
        """Carga templates de contenido por categorÃ­a."""
        return {
            "wikipedia": [
                "La inteligencia artificial (IA) es una rama de la informÃ¡tica que busca crear mÃ¡quinas capaces de realizar tareas que requieren inteligencia humana. El tÃ©rmino fue acuÃ±ado en 1956 por John McCarthy en la Conferencia de Darmouth. Desde entonces, ha evolucionado significativamente, incorporando tÃ©cnicas como el aprendizaje automÃ¡tico, el procesamiento de lenguaje natural y la visiÃ³n por computadora.",
                "El aprendizaje federado es un enfoque de aprendizaje automÃ¡tico que permite entrenar modelos de IA de manera distribuida, manteniendo la privacidad de los datos. Desarrollado por Google en 2016, permite que mÃºltiples dispositivos colaboren en el entrenamiento de un modelo sin compartir sus datos locales.",
                "La computaciÃ³n en la nube se refiere a la entrega de servicios de computaciÃ³n a travÃ©s de internet. Incluye servidores, almacenamiento, bases de datos, redes, software y anÃ¡lisis. Los proveedores principales son Amazon Web Services (AWS), Microsoft Azure y Google Cloud Platform.",
                "El procesamiento de lenguaje natural (NLP) es una rama de la IA que se enfoca en la interacciÃ³n entre computadoras y lenguaje humano. Incluye tareas como traducciÃ³n automÃ¡tica, anÃ¡lisis de sentimientos, resumen de textos y generaciÃ³n de lenguaje natural.",
                "La blockchain es una tecnologÃ­a de registro distribuido que mantiene una lista creciente de registros, llamados bloques, que estÃ¡n vinculados mediante criptografÃ­a. Cada bloque contiene un hash criptogrÃ¡fico del bloque anterior, una marca de tiempo y datos de transacciÃ³n."
            ],
            "technical": [
                "La arquitectura de microservicios permite construir aplicaciones como una colecciÃ³n de servicios pequeÃ±os e independientes. Cada servicio implementa una funcionalidad especÃ­fica y se comunica con otros servicios a travÃ©s de APIs bien definidas. Esta arquitectura facilita el escalado, el mantenimiento y el despliegue independiente de componentes.",
                "Los contenedores Docker proporcionan un entorno de ejecuciÃ³n ligero y portable para aplicaciones. Utilizan el kernel del sistema operativo host pero aÃ­slan completamente el sistema de archivos, la red y otros recursos. Esto permite ejecutar aplicaciones de manera consistente en diferentes entornos.",
                "El aprendizaje profundo utiliza redes neuronales con mÃºltiples capas para resolver problemas complejos. Las redes convolucionales (CNN) son especialmente efectivas en visiÃ³n por computadora, mientras que las redes recurrentes (RNN) y transformers son ideales para procesamiento de secuencias.",
                "La ingenierÃ­a de datos implica el diseÃ±o y construcciÃ³n de sistemas para recopilar, almacenar y analizar grandes volÃºmenes de datos. Incluye ETL (Extract, Transform, Load), modelado de datos, optimizaciÃ³n de consultas y garantÃ­a de calidad de datos.",
                "La seguridad en la nube requiere mÃºltiples capas de protecciÃ³n: autenticaciÃ³n multifactor, encriptaciÃ³n de datos en trÃ¡nsito y en reposo, control de acceso basado en roles, monitoreo continuo y cumplimiento de estÃ¡ndares regulatorios."
            ],
            "code": [
                "def train_federated_model(self, global_weights, local_epochs=3):\n    '''\n    Entrena el modelo localmente con pesos globales iniciales.\n    \n    Args:\n        global_weights: Pesos del modelo global\n        local_epochs: NÃºmero de epochs locales\n    \n    Returns:\n        dict: Pesos actualizados y mÃ©tricas\n    '''\n    self.model.load_state_dict(global_weights)\n    self.model.train()\n    \n    for epoch in range(local_epochs):\n        for batch in self.train_loader:\n            self.optimizer.zero_grad()\n            outputs = self.model(batch['input_ids'])\n            loss = self.criterion(outputs, batch['labels'])\n            loss.backward()\n            self.optimizer.step()\n    \n    return {\n        'weights': self.model.state_dict(),\n        'accuracy': self.evaluate_accuracy(),\n        'loss': loss.item()\n    }",
                "class FederatedCoordinator:\n    '''\n    Coordinador central para entrenamiento federado.\n    Gestiona sesiones, nodos y agregaciÃ³n de pesos.\n    '''\n    \n    def __init__(self, config):\n        self.config = config\n        self.active_sessions = {}\n        self.node_registry = {}\n        \n    async def create_session(self, model_config):\n        session_id = str(uuid.uuid4())\n        self.active_sessions[session_id] = {\n            'model_config': model_config,\n            'nodes': [],\n            'start_time': time.time(),\n            'status': 'waiting_for_nodes'\n        }\n        return session_id\n        \n    def aggregate_weights(self, node_weights):\n        '''Agrega pesos usando algoritmo FedAvg.'''\n        aggregated = {}\n        for key in node_weights[0].keys():\n            tensors = [w[key] for w in node_weights]\n            aggregated[key] = torch.stack(tensors).mean(dim=0)\n        return aggregated",
                "import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\n\nclass EmpoorioLM(nn.Module):\n    '''\n    Modelo de lenguaje transformer optimizado para federated learning.\n    '''\n    \n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n        \n        # Embeddings\n        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size)\n        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n        \n        # Transformer blocks\n        self.blocks = nn.ModuleList([\n            TransformerBlock(config) for _ in range(config.num_hidden_layers)\n        ])\n        \n        # Language modeling head\n        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size)\n        \n    def forward(self, input_ids, attention_mask=None):\n        # Embedding + positional encoding\n        embeddings = self.word_embeddings(input_ids) + self.position_embeddings(torch.arange(input_ids.size(1)))\n        \n        # Apply transformer blocks\n        hidden_states = embeddings\n        for block in self.blocks:\n            hidden_states = block(hidden_states, attention_mask)\n            \n        # Language modeling\n        logits = self.lm_head(hidden_states)\n        return {'logits': logits, 'hidden_states': hidden_states}"
            ],
            "books": [
                "En el vasto universo de la literatura, cada pÃ¡gina representa una puerta hacia mundos desconocidos. Los libros no son meros objetos inanimados, sino compaÃ±eros de viaje que nos transportan a travÃ©s del tiempo y el espacio. Desde las antiguas epopeyas sumerias hasta las novelas contemporÃ¡neas, la escritura ha sido el vehÃ­culo principal para preservar el conocimiento humano, las emociones y las experiencias colectivas.",
                "La filosofÃ­a de la ciencia moderna comenzÃ³ con la revoluciÃ³n copernicana, cuando NicolÃ¡s CopÃ©rnico propuso que la Tierra no era el centro del universo. Esta idea revolucionaria, desarrollada posteriormente por Galileo Galilei y Johannes Kepler, sentÃ³ las bases para el mÃ©todo cientÃ­fico experimental. Francis Bacon y RenÃ© Descartes contribuyeron con sus respectivos mÃ©todos inductivo y deductivo, estableciendo las bases epistemolÃ³gicas de la investigaciÃ³n cientÃ­fica.",
                "La historia de la humanidad puede verse como una progresiÃ³n desde la ignorancia hacia el conocimiento, desde la supersticiÃ³n hacia la razÃ³n. Cada generaciÃ³n construye sobre los hombros de la anterior, incorporando nuevas tecnologÃ­as, nuevos conocimientos y nuevas formas de entender el mundo. Sin embargo, este progreso no ha sido lineal ni uniforme; ha estado marcado por retrocesos, conflictos y momentos de iluminaciÃ³n colectiva.",
                "El arte contemporÃ¡neo desafÃ­a las convenciones tradicionales, explorando nuevos medios y formas de expresiÃ³n. Desde las instalaciones multimedia hasta el arte digital, los artistas modernos buscan cuestionar nuestras percepciones del mundo, desafiar las normas sociales y explorar los lÃ­mites de la creatividad humana. El arte ya no se limita a los museos y galerÃ­as; se ha convertido en una experiencia interactiva y participativa.",
                "La psicologÃ­a cognitiva ha revolucionado nuestra comprensiÃ³n de cÃ³mo funciona la mente humana. Desde los experimentos de memoria de Hermann Ebbinghaus hasta las teorÃ­as de procesamiento de informaciÃ³n de Ulric Neisser, hemos aprendido que el pensamiento no es un proceso pasivo, sino una actividad constructiva que implica la manipulaciÃ³n activa de informaciÃ³n simbÃ³lica."
            ],
            "news": [
                "La UniÃ³n Europea ha anunciado un paquete de medidas sin precedentes para combatir el cambio climÃ¡tico, con inversiones de 500 mil millones de euros destinadas a energÃ­as renovables y eficiencia energÃ©tica. El plan incluye objetivos ambiciosos de reducciÃ³n de emisiones para 2030 y 2050, con el objetivo de convertir a Europa en el primer continente climÃ¡ticamente neutro del mundo.",
                "Los avances en inteligencia artificial estÃ¡n transformando la industria automotriz, con vehÃ­culos autÃ³nomos que prometen revolucionar el transporte urbano. Empresas como Tesla, Waymo y Baidu estÃ¡n invirtiendo miles de millones en el desarrollo de sistemas de conducciÃ³n automatizada, que combinan sensores avanzados, aprendizaje profundo y algoritmos de toma de decisiones en tiempo real.",
                "El sector de la salud digital estÃ¡ experimentando un crecimiento exponencial, impulsado por la pandemia de COVID-19. Las teleconsultas, las aplicaciones de monitoreo remoto y los sistemas de diagnÃ³stico asistido por IA estÃ¡n mejorando el acceso a la atenciÃ³n mÃ©dica, especialmente en Ã¡reas rurales y paÃ­ses en desarrollo. Sin embargo, estos avances tambiÃ©n plantean desafÃ­os Ã©ticos y de privacidad.",
                "La exploraciÃ³n espacial comercial estÃ¡ entrando en una nueva era, con empresas privadas compitiendo con agencias gubernamentales. SpaceX ha demostrado la viabilidad de reutilizar cohetes, reduciendo drÃ¡sticamente los costos de lanzamiento. Mientras tanto, Blue Origin y Virgin Galactic se centran en el turismo espacial, abriendo nuevas oportunidades econÃ³micas.",
                "La transformaciÃ³n digital de las empresas tradicionales estÃ¡ acelerÃ¡ndose, impulsada por la necesidad de adaptarse a un entorno econÃ³mico volÃ¡til. Las tecnologÃ­as emergentes como blockchain, IoT y realidad aumentada estÃ¡n creando nuevas oportunidades de negocio, pero tambiÃ©n requieren una reevaluaciÃ³n fundamental de los modelos operativos y las estrategias de recursos humanos."
            ],
            "social": [
                "Â¡IncreÃ­ble cÃ³mo la tecnologÃ­a estÃ¡ cambiando nuestras vidas! Hoy mismo pude hablar con mi familia al otro lado del mundo sin costo alguno gracias a internet. Â¿QuÃ© opinan ustedes sobre cÃ³mo el acceso a la informaciÃ³n global estÃ¡ democratizando el conocimiento? #TecnologÃ­a #Conectividad",
                "Reflexionando sobre el futuro del trabajo: con la automatizaciÃ³n y la IA avanzando tan rÃ¡pido, Â¿quÃ© habilidades serÃ¡n mÃ¡s valiosas en los prÃ³ximos aÃ±os? Creo que la creatividad, el pensamiento crÃ­tico y la capacidad de adaptaciÃ³n serÃ¡n clave. Â¿EstÃ¡n preparados para este cambio? #FuturoDelTrabajo #IA",
                "Â¡QuÃ© maravilla de dÃ­a! SalÃ­ a caminar por el parque y pude observar cÃ³mo la naturaleza sigue su ciclo perfecto. En medio del caos urbano, estos momentos de conexiÃ³n con lo natural nos recuerdan lo importante que es preservar nuestro planeta. Â¿CuÃ¡l es su lugar favorito para reconectar con la naturaleza? #Naturaleza #Bienestar",
                "Compartiendo mi experiencia con el aprendizaje online durante la pandemia. Las plataformas educativas han hecho posible que miles de personas continÃºen formÃ¡ndose desde casa. Sin embargo, echo de menos la interacciÃ³n personal en el aula. Â¿CÃ³mo creen que evolucionarÃ¡ la educaciÃ³n en el futuro? #EducaciÃ³n #AprendizajeOnline",
                "Celebrando los pequeÃ±os logros diarios: hoy terminÃ© un proyecto que habÃ­a estado postergando por semanas. Me recuerda que la consistencia y la perseverancia son mÃ¡s importantes que el talento innato. Â¿CuÃ¡l ha sido su mayor logro reciente? Â¡Felicitaciones por adelantado! #MotivaciÃ³n #Logros"
            ]
        }

    def generate_sample(self, dataset_type: DatasetType, config: DatasetConfig) -> Dict[str, Any]:
        """Genera una muestra individual de dataset."""
        templates = self.templates[dataset_type.value]

        # Seleccionar template base
        base_text = random.choice(templates)

        # Generar variaciones
        if dataset_type == DatasetType.WIKIPEDIA_ARTICLES:
            sample = self._generate_wikipedia_article(base_text, config)
        elif dataset_type == DatasetType.TECHNICAL_DOCS:
            sample = self._generate_technical_doc(base_text, config)
        elif dataset_type == DatasetType.CODE_REPOSITORIES:
            sample = self._generate_code_sample(base_text, config)
        elif dataset_type == DatasetType.BOOKS_TEXTS:
            sample = self._generate_book_text(base_text, config)
        elif dataset_type == DatasetType.NEWS_ARTICLES:
            sample = self._generate_news_article(base_text, config)
        elif dataset_type == DatasetType.SOCIAL_MEDIA:
            sample = self._generate_social_post(base_text, config)
        else:
            sample = self._generate_generic_text(base_text, config)

        return sample

    def _generate_wikipedia_article(self, base_text: str, config: DatasetConfig) -> Dict[str, Any]:
        """Genera artÃ­culo estilo Wikipedia."""
        titles = [
            "Inteligencia Artificial", "Aprendizaje Federado", "ComputaciÃ³n en la Nube",
            "Procesamiento de Lenguaje Natural", "Blockchain", "CriptografÃ­a",
            "Aprendizaje Profundo", "Redes Neuronales", "Big Data", "Machine Learning"
        ]

        title = random.choice(titles)
        content = base_text

        # AÃ±adir secciones
        sections = ["IntroducciÃ³n", "Historia", "Aplicaciones", "DesafÃ­os", "Futuro"]
        for section in random.sample(sections, random.randint(2, 4)):
            content += f"\n\n{section}\n" + ".".join(base_text.split(".")[:2]) + "."

        return {
            "title": title,
            "content": content,
            "language": config.language,
            "category": "encyclopedia",
            "word_count": len(content.split()),
            "source": "wikipedia_synthetic",
            "quality_score": random.uniform(0.85, 0.95)
        }

    def _generate_technical_doc(self, base_text: str, config: DatasetConfig) -> Dict[str, Any]:
        """Genera documentaciÃ³n tÃ©cnica."""
        topics = [
            "Arquitectura de Microservicios", "Contenedores Docker", "Aprendizaje Profundo",
            "IngenierÃ­a de Datos", "Seguridad en la Nube", "DevOps", "CI/CD",
            "APIs RESTful", "Bases de Datos Distribuidas", "Monitoreo y Logging"
        ]

        topic = random.choice(topics)
        content = base_text

        # AÃ±adir elementos tÃ©cnicos
        technical_terms = ["API", "microservicios", "contenedores", "orquestaciÃ³n", "escalabilidad"]
        for term in random.sample(technical_terms, random.randint(2, 4)):
            content += f" La implementaciÃ³n de {term} requiere consideraciones especÃ­ficas de diseÃ±o y arquitectura."

        return {
            "title": f"GuÃ­a de {topic}",
            "content": content,
            "language": config.language,
            "category": "technical",
            "difficulty": random.choice(["beginner", "intermediate", "advanced"]),
            "word_count": len(content.split()),
            "source": "technical_docs_synthetic",
            "quality_score": random.uniform(0.90, 0.98)
        }

    def _generate_code_sample(self, base_text: str, config: DatasetConfig) -> Dict[str, Any]:
        """Genera muestra de cÃ³digo con documentaciÃ³n."""
        languages = ["python", "javascript", "typescript", "java", "go", "rust"]
        frameworks = ["tensorflow", "pytorch", "react", "django", "spring", "fastapi"]

        language = random.choice(languages)
        framework = random.choice(frameworks)

        # El base_text ya contiene cÃ³digo, aÃ±adir documentaciÃ³n
        documentation = f"Este cÃ³digo demuestra el uso de {framework} en {language} para implementar funcionalidades avanzadas. Incluye manejo de errores, logging y optimizaciones de rendimiento."

        return {
            "title": f"Ejemplo de {framework} en {language}",
            "code": base_text,
            "documentation": documentation,
            "language": language,
            "framework": framework,
            "category": "code",
            "complexity": random.choice(["simple", "medium", "complex"]),
            "word_count": len((base_text + documentation).split()),
            "source": "code_repository_synthetic",
            "quality_score": random.uniform(0.88, 0.96)
        }

    def _generate_book_text(self, base_text: str, config: DatasetConfig) -> Dict[str, Any]:
        """Genera texto estilo libro."""
        genres = ["filosofÃ­a", "ciencia", "historia", "literatura", "psicologÃ­a"]
        authors = ["AristÃ³teles", "PlatÃ³n", "Descartes", "Kant", "Nietzsche", "Sartre"]

        genre = random.choice(genres)
        author = random.choice(authors)

        return {
            "title": f"Reflexiones sobre {genre.title()}",
            "content": base_text,
            "author": author,
            "genre": genre,
            "language": config.language,
            "category": "literature",
            "word_count": len(base_text.split()),
            "source": "book_text_synthetic",
            "quality_score": random.uniform(0.82, 0.94)
        }

    def _generate_news_article(self, base_text: str, config: DatasetConfig) -> Dict[str, Any]:
        """Genera artÃ­culo de noticias."""
        topics = ["tecnologÃ­a", "ciencia", "economÃ­a", "medio ambiente", "salud", "polÃ­tica"]
        sources = ["El PaÃ­s", "BBC News", "Reuters", "CNN", "The Guardian", "New York Times"]

        topic = random.choice(topics)
        source = random.choice(sources)

        return {
            "title": f"Avances en {topic.title()}: Nuevos descubrimientos transforman el sector",
            "content": base_text,
            "topic": topic,
            "source": source,
            "language": config.language,
            "category": "news",
            "word_count": len(base_text.split()),
            "publication_date": "2024-11-06",
            "quality_score": random.uniform(0.87, 0.93)
        }

    def _generate_social_post(self, base_text: str, config: DatasetConfig) -> Dict[str, Any]:
        """Genera post de redes sociales."""
        platforms = ["twitter", "facebook", "instagram", "linkedin", "tiktok"]
        sentiments = ["positive", "neutral", "negative", "enthusiastic", "reflective"]

        platform = random.choice(platforms)
        sentiment = random.choice(sentiments)

        return {
            "content": base_text,
            "platform": platform,
            "sentiment": sentiment,
            "language": config.language,
            "category": "social_media",
            "hashtags": ["#TecnologÃ­a", "#IA", "#InnovaciÃ³n", "#Futuro"],
            "word_count": len(base_text.split()),
            "source": "social_media_synthetic",
            "quality_score": random.uniform(0.75, 0.89)
        }

    def _generate_generic_text(self, base_text: str, config: DatasetConfig) -> Dict[str, Any]:
        """Genera texto genÃ©rico."""
        return {
            "content": base_text,
            "language": config.language,
            "category": "general",
            "word_count": len(base_text.split()),
            "source": "generic_synthetic",
            "quality_score": random.uniform(0.80, 0.90)
        }

    def generate_dataset(
        self,
        config: DatasetConfig,
        output_file: str = None,
        batch_size: int = 1000
    ) -> str:
        """
        Genera un dataset completo.

        Args:
            config: ConfiguraciÃ³n del dataset
            output_file: Archivo de salida (opcional)
            batch_size: TamaÃ±o de batch para escritura

        Returns:
            Ruta del archivo generado
        """
        if output_file is None:
            timestamp = int(time.time())
            output_file = f"{config.dataset_type.value}_{config.num_samples}_{timestamp}.jsonl"

        output_path = self.output_dir / output_file

        start_time = time.time()
        samples_generated = 0

        print(f"ğŸš€ Generando dataset {config.dataset_type.value} con {config.num_samples} muestras...")

        with open(output_path, 'w', encoding='utf-8') as f:
            batch = []

            for i in range(config.num_samples):
                # Generar muestra
                sample = self.generate_sample(config.dataset_type, config)
                batch.append(sample)
                samples_generated += 1

                # Escribir en batches
                if len(batch) >= batch_size:
                    for item in batch:
                        json.dump(item, f, ensure_ascii=False)
                        f.write('\n')
                    batch = []

                    # Progreso
                    progress = (samples_generated / config.num_samples) * 100
                    print(f"   ğŸ“Š Progreso: {progress:.1f}%")
                # Ãšltimo batch
                for item in batch:
                    json.dump(item, f, ensure_ascii=False)
                    f.write('\n')

        generation_time = time.time() - start_time

        # EstadÃ­sticas
        total_tokens = sum(len(sample.get('content', '').split()) for sample in batch)

        dataset_info = {
            "dataset_type": config.dataset_type.value,
            "num_samples": config.num_samples,
            "output_file": str(output_path),
            "generation_time": generation_time,
            "samples_per_second": config.num_samples / generation_time,
            "total_tokens": total_tokens,
            "config": {
                "language": config.language,
                "quality_level": config.quality_level,
                "min_length": config.min_length,
                "max_length": config.max_length
            }
        }

        # Guardar metadatos
        metadata_file = output_path.with_suffix('.metadata.json')
        with open(metadata_file, 'w', encoding='utf-8') as f:
            json.dump(dataset_info, f, indent=2, ensure_ascii=False)

        # Actualizar estadÃ­sticas globales
        self.stats["total_samples"] += config.num_samples
        self.stats["total_tokens"] += total_tokens
        self.stats["generation_time"] += generation_time
        self.stats["datasets_created"].append(dataset_info)

        print("âœ… Dataset generado exitosamente:")
        print(f"   ğŸ“ Archivo: {output_path}")
        print(f"   ğŸ“Š Muestras: {config.num_samples}")
        print(f"   â±ï¸ Tiempo: {generation_time:.2f}s")
        print(f"   ğŸ“ˆ Velocidad: {config.num_samples/generation_time:.1f} muestras/s")

        return str(output_path)

    def generate_massive_dataset_suite(self, target_size_gb: float = 1.0) -> List[str]:
        """
        Genera una suite completa de datasets masivos.

        Args:
            target_size_gb: TamaÃ±o objetivo total en GB

        Returns:
            Lista de archivos generados
        """
        print(f"ğŸ¯ Generando suite de datasets masivos ({target_size_gb}GB objetivo)...")

        # Calcular distribuciÃ³n por tipo
        dataset_configs = [
            DatasetConfig(DatasetType.WIKIPEDIA_ARTICLES, num_samples=15000),
            DatasetConfig(DatasetType.TECHNICAL_DOCS, num_samples=12000),
            DatasetConfig(DatasetType.CODE_REPOSITORIES, num_samples=8000),
            DatasetConfig(DatasetType.BOOKS_TEXTS, num_samples=10000),
            DatasetConfig(DatasetType.NEWS_ARTICLES, num_samples=13000),
            DatasetConfig(DatasetType.SOCIAL_MEDIA, num_samples=20000)
        ]

        generated_files = []

        for config in dataset_configs:
            file_path = self.generate_dataset(config)
            generated_files.append(file_path)

        # Verificar tamaÃ±o total
        total_size = sum(Path(f).stat().st_size for f in generated_files) / (1024**3)  # GB

        print("\nğŸ“Š SUITE COMPLETA GENERADA:")
        print(f"   ğŸ“ Datasets: {len(generated_files)}")
        print(f"   ğŸ’¾ TamaÃ±o total: {total_size:.2f}GB")
        print(f"   ğŸ“ˆ Eficiencia: {total_size/target_size_gb*100:.1f}% del objetivo")
        return generated_files

    def save_stats(self):
        """Guarda estadÃ­sticas de generaciÃ³n."""
        stats_file = self.output_dir / "generation_stats.json"
        with open(stats_file, 'w', encoding='utf-8') as f:
            json.dump(self.stats, f, indent=2, default=str)


# Funciones de conveniencia
def generate_wikipedia_dataset(num_samples: int = 10000) -> str:
    """Genera dataset de artÃ­culos Wikipedia."""
    generator = MassiveDatasetGenerator()
    config = DatasetConfig(DatasetType.WIKIPEDIA_ARTICLES, num_samples=num_samples)
    return generator.generate_dataset(config)


def generate_technical_dataset(num_samples: int = 8000) -> str:
    """Genera dataset de documentaciÃ³n tÃ©cnica."""
    generator = MassiveDatasetGenerator()
    config = DatasetConfig(DatasetType.TECHNICAL_DOCS, num_samples=num_samples)
    return generator.generate_dataset(config)


def generate_massive_suite(target_gb: float = 1.0) -> List[str]:
    """Genera suite completa de datasets masivos."""
    generator = MassiveDatasetGenerator()
    return generator.generate_massive_dataset_suite(target_gb)


if __name__ == "__main__":
    # Demo de generaciÃ³n
    print("ğŸ§ª DEMO: Generador de Datasets Masivos para AILOOS")
    print("=" * 60)

    generator = MassiveDatasetGenerator()

    # Generar dataset pequeÃ±o para demo
    print("\n1ï¸âƒ£ Generando dataset de prueba (Wikipedia)...")
    config = DatasetConfig(DatasetType.WIKIPEDIA_ARTICLES, num_samples=100)
    wiki_file = generator.generate_dataset(config)

    print("\n2ï¸âƒ£ Generando dataset tÃ©cnico...")
    config = DatasetConfig(DatasetType.TECHNICAL_DOCS, num_samples=50)
    tech_file = generator.generate_dataset(config)

    print("\n3ï¸âƒ£ Generando muestra de cÃ³digo...")
    config = DatasetConfig(DatasetType.CODE_REPOSITORIES, num_samples=25)
    code_file = generator.generate_dataset(config)

    # Mostrar estadÃ­sticas
    print("\nğŸ“Š ESTADÃSTICAS FINALES:")
    print(f"   ğŸ“ Datasets generados: {len(generator.stats['datasets_created'])}")
    print(f"   ğŸ“Š Muestras totales: {generator.stats['total_samples']}")
    print(f"   ğŸ“ Tokens totales: {generator.stats['total_tokens']}")
    print(f"   â±ï¸ Tiempo total: {generator.stats['generation_time']:.2f}s")
    print("\nâœ… Â¡GeneraciÃ³n completada! Los datasets estÃ¡n listos para entrenamiento federado.")
    print(f"ğŸ“‚ UbicaciÃ³n: {generator.output_dir}")

    # Guardar estadÃ­sticas
    generator.save_stats()