#!/usr/bin/env python3
"""
LocalTrainer - Entrenador local con fine-tuning LoRA para federated learning
Implementa fine-tuning real con LoRA en datasets locales para EmpoorioLM y modelos pre-entrenados.
"""

import asyncio
import json
import os
import time
import torch
import torch.nn as nn
from typing import Dict, List, Any, Optional, Tuple, Union, Callable
from dataclasses import dataclass, field
from datetime import datetime
from pathlib import Path
import numpy as np
import psutil
import GPUtil
from contextlib import nullcontext

try:
    from peft import LoraConfig, get_peft_model, PeftModel
    PEFT_AVAILABLE = True
except ImportError:
    PEFT_AVAILABLE = False
    print("‚ö†Ô∏è PEFT not available, LoRA functionality will be limited")

from ..core.logging import get_logger
from ..core.config import get_config
from ..data.dataset_manager import get_dataset_manager
from ..models.empoorio_lm import EmpoorioLM, EmpoorioLMConfig, get_config_for_model_size
from transformers import AutoTokenizer, AutoModelForCausalLM, DataCollatorForLanguageModeling
from torch.utils.data import DataLoader, Dataset
from accelerate import Accelerator

logger = get_logger(__name__)


@dataclass
class LocalTrainerConfig:
    """Configuraci√≥n para LocalTrainer."""
    # Modelo
    model_name_or_path: str = "EmpoorioLM-base"
    model_size: str = "base"  # tiny, small, base, large
    use_empoorio_lm: bool = True

    # LoRA
    lora_rank: int = 8
    lora_alpha: float = 16
    lora_dropout: float = 0.05
    target_modules: List[str] = field(default_factory=lambda: ["q_proj", "k_proj", "v_proj", "o_proj"])
    modules_to_save: List[str] = field(default_factory=list)

    # Entrenamiento
    learning_rate: float = 2e-5
    batch_size: int = 4
    gradient_accumulation_steps: int = 4
    max_epochs: int = 3
    max_steps: int = -1
    warmup_steps: int = 100
    weight_decay: float = 0.01
    gradient_clip_norm: float = 1.0

    # Optimizaci√≥n de memoria
    use_gradient_checkpointing: bool = True
    use_fp16: bool = True
    use_bf16: bool = False
    dataloader_num_workers: int = 2
    dataloader_pin_memory: bool = True

    # Dataset
    dataset_name: str = ""
    max_seq_length: int = 512
    preprocessing_num_workers: int = 4

    # Monitoreo
    logging_steps: int = 10
    save_steps: int = 500
    eval_steps: int = 500
    save_total_limit: int = 3

    # Federated learning
    extract_gradients: bool = True
    compute_model_diff: bool = True

    # Directorios
    output_dir: str = "./local_training_output"
    logging_dir: str = "./logs"


@dataclass
class TrainingMetrics:
    """M√©tricas de entrenamiento."""
    step: int = 0
    epoch: int = 0
    loss: float = 0.0
    learning_rate: float = 0.0
    grad_norm: float = 0.0
    train_time: float = 0.0
    gpu_memory_used: float = 0.0
    cpu_memory_used: float = 0.0
    throughput: float = 0.0  # samples/second
    timestamp: datetime = field(default_factory=datetime.now)


@dataclass
class ModelDiff:
    """Diferencias del modelo para federated learning."""
    lora_weights_diff: Dict[str, Any] = field(default_factory=dict)
    gradients: Dict[str, Any] = field(default_factory=dict)
    num_samples: int = 0
    training_steps: int = 0
    metadata: Dict[str, Any] = field(default_factory=dict)


class TextDataset(Dataset):
    """Dataset simple para texto."""

    def __init__(self, texts: List[str], tokenizer, max_length: int = 512):
        self.texts = texts
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = self.texts[idx]
        encodings = self.tokenizer(
            text,
            truncation=True,
            padding="max_length",
            max_length=self.max_length,
            return_tensors="pt"
        )

        return {
            "input_ids": encodings["input_ids"].flatten(),
            "attention_mask": encodings["attention_mask"].flatten(),
            "labels": encodings["input_ids"].flatten()  # Para causal LM
        }


class LocalTrainer:
    """
    Entrenador local con fine-tuning LoRA para federated learning.

    Implementa fine-tuning real con LoRA en datasets locales, optimizaci√≥n de memoria,
    monitoreo de m√©tricas y extracci√≥n de diferencias para federated learning.
    """

    def __init__(self, config: LocalTrainerConfig = None):
        self.config = config or LocalTrainerConfig()

        # Componentes principales
        self.model = None
        self.tokenizer = None
        self.lora_config = None
        self.peft_model = None

        # Entrenamiento
        self.optimizer = None
        self.scheduler = None
        self.accelerator = None

        # Dataset y dataloader
        self.train_dataset = None
        self.eval_dataset = None
        self.train_dataloader = None
        self.eval_dataloader = None

        # Estado
        self.is_initialized = False
        self.is_training = False
        self.global_step = 0
        self.epoch = 0

        # M√©tricas y monitoreo
        self.metrics_history: List[TrainingMetrics] = []
        self.start_time = None

        # Modelo base para calcular diferencias
        self.base_model_state = None

        # Dataset manager
        self.dataset_manager = get_dataset_manager()

        # Callbacks
        self.training_callbacks: List[Callable] = []

        logger.info("üöÄ LocalTrainer initialized with LoRA fine-tuning capabilities")

    async def initialize(self) -> bool:
        """
        Inicializar el entrenador local.

        Returns:
            True si la inicializaci√≥n fue exitosa
        """
        try:
            logger.info("üîß Initializing LocalTrainer...")

            # 1. Inicializar accelerator para optimizaci√≥n de memoria
            self.accelerator = Accelerator(
                gradient_accumulation_steps=self.config.gradient_accumulation_steps,
                mixed_precision="fp16" if self.config.use_fp16 else "bf16" if self.config.use_bf16 else "no"
            )

            # 2. Cargar modelo y tokenizer
            await self._load_model_and_tokenizer()

            # 3. Configurar LoRA
            self._setup_lora()

            # 4. Preparar dataset
            await self._prepare_dataset()

            # 5. Configurar optimizer y scheduler
            self._setup_optimizer_and_scheduler()

            # 6. Preparar para accelerator
            self._prepare_accelerator()

            # 7. Crear directorios de salida
            self._create_output_dirs()

            # 8. Guardar estado base del modelo
            self._save_base_model_state()

            self.is_initialized = True
            logger.info("‚úÖ LocalTrainer initialized successfully")
            logger.info(f"   üìä Model: {self.config.model_name_or_path}")
            logger.info(f"   üéØ LoRA rank: {self.config.lora_rank}")
            logger.info(f"   üìà Batch size: {self.config.batch_size}")
            logger.info(f"   üîÑ Gradient accumulation: {self.config.gradient_accumulation_steps}")

            return True

        except Exception as e:
            logger.error(f"‚ùå Error initializing LocalTrainer: {e}")
            return False

    async def _load_model_and_tokenizer(self):
        """Cargar modelo y tokenizer."""
        logger.info(f"üì• Loading model: {self.config.model_name_or_path}")

        if self.config.use_empoorio_lm:
            # Cargar EmpoorioLM
            if "EmpoorioLM" in self.config.model_name_or_path:
                model_config = get_config_for_model_size(self.config.model_size)
                self.model = EmpoorioLM(model_config)
                # Para EmpoorioLM, crear tokenizer b√°sico
                from ..models.empoorio_lm import load_trained_tokenizer
                self.tokenizer = load_trained_tokenizer()
            else:
                # Cargar modelo transformers pero usar EmpoorioLM wrapper
                self.tokenizer = AutoTokenizer.from_pretrained(self.config.model_name_or_path)
                base_model = AutoModelForCausalLM.from_pretrained(
                    self.config.model_name_or_path,
                    torch_dtype=torch.float16 if self.config.use_fp16 else torch.float32
                )
                model_config = get_config_for_model_size(self.config.model_size)
                self.model = EmpoorioLM(model_config)  # Wrapper
                # Copiar pesos del modelo transformers al EmpoorioLM
                self._transfer_weights(base_model)
        else:
            # Cargar modelo transformers est√°ndar
            self.tokenizer = AutoTokenizer.from_pretrained(self.config.model_name_or_path)
            self.model = AutoModelForCausalLM.from_pretrained(
                self.config.model_name_or_path,
                torch_dtype=torch.float16 if self.config.use_fp16 else torch.float32
            )

        # Configurar tokenizer
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token

        # Mover modelo a dispositivo
        self.model = self.model.to(self.accelerator.device)

        # Habilitar gradient checkpointing si est√° configurado
        if self.config.use_gradient_checkpointing and hasattr(self.model, 'gradient_checkpointing_enable'):
            self.model.gradient_checkpointing_enable()

        logger.info(f"‚úÖ Model loaded: {self.model.__class__.__name__}")
        logger.info(f"   üìä Parameters: {sum(p.numel() for p in self.model.parameters()):,}")

    def _transfer_weights(self, source_model):
        """Transferir pesos de modelo transformers a EmpoorioLM."""
        # Esta es una implementaci√≥n simplificada
        # En la pr√°ctica, necesitar√≠as mapear las capas correctamente
        logger.info("üîÑ Transferring weights from transformers model to EmpoorioLM")

    def _setup_lora(self):
        """Configurar LoRA con PEFT."""
        if not PEFT_AVAILABLE:
            logger.warning("‚ö†Ô∏è PEFT not available, skipping LoRA setup")
            self.peft_model = self.model
            return

        logger.info("üéØ Setting up LoRA configuration")

        self.lora_config = LoraConfig(
            r=self.config.lora_rank,
            lora_alpha=self.config.lora_alpha,
            target_modules=self.config.target_modules,
            lora_dropout=self.config.lora_dropout,
            bias="none",
            task_type="CAUSAL_LM",
            modules_to_save=self.config.modules_to_save
        )

        # Aplicar LoRA al modelo
        self.peft_model = get_peft_model(self.model, self.lora_config)

        # Mostrar par√°metros entrenables
        self.peft_model.print_trainable_parameters()

        logger.info("‚úÖ LoRA configured successfully")

    async def _prepare_dataset(self):
        """Preparar dataset para entrenamiento."""
        logger.info(f"üìö Preparing dataset: {self.config.dataset_name}")

        if not self.config.dataset_name:
            logger.warning("‚ö†Ô∏è No dataset specified, using dummy dataset")
            self._create_dummy_dataset()
            return

        # Intentar cargar dataset desde el dataset manager
        try:
            # Aqu√≠ integrar√≠as con el dataset manager existente
            # Por simplicidad, creamos un dataset dummy
            self._create_dummy_dataset()

        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Error loading dataset {self.config.dataset_name}: {e}")
            self._create_dummy_dataset()

        # Crear dataloaders
        data_collator = DataCollatorForLanguageModeling(
            tokenizer=self.tokenizer,
            mlm=False  # Causal LM
        )

        self.train_dataloader = DataLoader(
            self.train_dataset,
            batch_size=self.config.batch_size,
            shuffle=True,
            collate_fn=data_collator,
            num_workers=self.config.dataloader_num_workers,
            pin_memory=self.config.dataloader_pin_memory
        )

        if self.eval_dataset:
            self.eval_dataloader = DataLoader(
                self.eval_dataset,
                batch_size=self.config.batch_size,
                shuffle=False,
                collate_fn=data_collator,
                num_workers=self.config.dataloader_num_workers,
                pin_memory=self.config.dataloader_pin_memory
            )

        logger.info(f"‚úÖ Dataset prepared: {len(self.train_dataset)} training samples")

    def _create_dummy_dataset(self):
        """Crear dataset dummy para pruebas."""
        # Crear textos de ejemplo
        texts = [
            "El aprendizaje autom√°tico es una rama de la inteligencia artificial.",
            "Los transformers son modelos muy poderosos para procesamiento de lenguaje natural.",
            "El fine-tuning permite adaptar modelos pre-entrenados a tareas espec√≠ficas.",
            "LoRA es una t√©cnica eficiente para fine-tuning de grandes modelos de lenguaje.",
            "El aprendizaje federado permite entrenar modelos sin compartir datos privados."
        ] * 100  # Multiplicar para tener m√°s datos

        self.train_dataset = TextDataset(texts, self.tokenizer, self.config.max_seq_length)
        self.eval_dataset = TextDataset(texts[:20], self.tokenizer, self.config.max_seq_length)

    def _setup_optimizer_and_scheduler(self):
        """Configurar optimizer y scheduler."""
        logger.info("‚öôÔ∏è Setting up optimizer and scheduler")

        # Optimizer
        self.optimizer = torch.optim.AdamW(
            self.peft_model.parameters(),
            lr=self.config.learning_rate,
            weight_decay=self.config.weight_decay
        )

        # Scheduler
        num_training_steps = len(self.train_dataloader) * self.config.max_epochs
        if self.config.max_steps > 0:
            num_training_steps = min(num_training_steps, self.config.max_steps)

        self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
            self.optimizer,
            T_max=num_training_steps
        )

        logger.info(f"‚úÖ Optimizer configured: AdamW with lr={self.config.learning_rate}")
        logger.info(f"   üìÖ Scheduler: CosineAnnealingLR with {num_training_steps} steps")

    def _prepare_accelerator(self):
        """Preparar componentes para accelerator."""
        logger.info("üöÄ Preparing accelerator")

        # Preparar modelo, optimizer, dataloaders
        (
            self.peft_model,
            self.optimizer,
            self.train_dataloader,
            self.eval_dataloader
        ) = self.accelerator.prepare(
            self.peft_model,
            self.optimizer,
            self.train_dataloader,
            self.eval_dataloader if self.eval_dataloader else self.train_dataloader
        )

        logger.info("‚úÖ Accelerator prepared")

    def _create_output_dirs(self):
        """Crear directorios de salida."""
        Path(self.config.output_dir).mkdir(parents=True, exist_ok=True)
        Path(self.config.logging_dir).mkdir(parents=True, exist_ok=True)

    def _save_base_model_state(self):
        """Guardar estado base del modelo para calcular diferencias."""
        logger.info("üíæ Saving base model state")

        self.base_model_state = {}
        for name, param in self.peft_model.named_parameters():
            if param.requires_grad:
                self.base_model_state[name] = param.data.clone().detach()

        logger.info(f"‚úÖ Base model state saved: {len(self.base_model_state)} parameters")

    async def train(self) -> Dict[str, Any]:
        """
        Ejecutar entrenamiento local con LoRA.

        Returns:
            Resultados del entrenamiento
        """
        if not self.is_initialized:
            raise RuntimeError("LocalTrainer not initialized. Call initialize() first.")

        self.is_training = True
        self.start_time = time.time()

        logger.info("üèÉ Starting local training with LoRA fine-tuning")
        logger.info(f"   üéØ Epochs: {self.config.max_epochs}")
        logger.info(f"   üìä Batch size: {self.config.batch_size}")
        logger.info(f"   üîÑ Gradient accumulation: {self.config.gradient_accumulation_steps}")

        try:
            # Bucle de entrenamiento
            for epoch in range(self.config.max_epochs):
                self.epoch = epoch
                logger.info(f"üìà Starting epoch {epoch + 1}/{self.config.max_epochs}")

                # Entrenamiento
                await self._train_epoch()

                # Evaluaci√≥n
                if self.eval_dataloader and (epoch + 1) % 1 == 0:  # Evaluar cada epoch
                    await self._evaluate()

                # Guardar checkpoint
                if (epoch + 1) % 1 == 0:  # Guardar cada epoch
                    await self._save_checkpoint(epoch + 1)

                # Verificar l√≠mite de pasos
                if self.config.max_steps > 0 and self.global_step >= self.config.max_steps:
                    logger.info(f"üõë Reached max_steps: {self.config.max_steps}")
                    break

            # Entrenamiento completado
            training_time = time.time() - self.start_time
            logger.info(f"‚úÖ Training completed in {training_time:.2f}s")

            # Calcular diferencias del modelo
            model_diff = await self._compute_model_diff()

            # Resultados finales
            results = {
                "training_completed": True,
                "total_epochs": self.epoch + 1,
                "total_steps": self.global_step,
                "training_time": training_time,
                "final_metrics": self._get_current_metrics(),
                "model_diff": model_diff,
                "config": self.config.__dict__
            }

            return results

        except Exception as e:
            logger.error(f"‚ùå Error during training: {e}")
            raise
        finally:
            self.is_training = False

    async def _train_epoch(self):
        """Entrenar una epoch."""
        self.peft_model.train()
        epoch_loss = 0.0
        num_steps = 0

        progress_bar = tqdm(self.train_dataloader, desc=f"Epoch {self.epoch + 1}")

        for step, batch in enumerate(progress_bar):
            # Forward pass
            with self.accelerator.accumulate(self.peft_model):
                outputs = self.peft_model(**batch)
                loss = outputs.loss

                # Backward pass
                self.accelerator.backward(loss)

                # Gradient clipping
                if self.accelerator.sync_gradients:
                    self.accelerator.clip_grad_norm_(
                        self.peft_model.parameters(),
                        self.config.gradient_clip_norm
                    )

                # Optimizer step
                self.optimizer.step()
                self.scheduler.step()
                self.optimizer.zero_grad()

            # Actualizar m√©tricas
            epoch_loss += loss.item()
            num_steps += 1
            self.global_step += 1

            # Logging
            if self.global_step % self.config.logging_steps == 0:
                await self._log_metrics(loss.item(), step)

            # Actualizar barra de progreso
            progress_bar.set_postfix({
                "loss": f"{loss.item():.4f}",
                "step": self.global_step
            })

            # Verificar l√≠mite de pasos
            if self.config.max_steps > 0 and self.global_step >= self.config.max_steps:
                break

        # Promedio de loss de la epoch
        avg_epoch_loss = epoch_loss / num_steps
        logger.info(f"üìä Epoch {self.epoch + 1} completed - Avg loss: {avg_epoch_loss:.4f}")

    async def _evaluate(self):
        """Evaluar el modelo."""
        if not self.eval_dataloader:
            return

        self.peft_model.eval()
        eval_loss = 0.0
        num_eval_steps = 0

        logger.info("üß™ Evaluating model...")

        with torch.no_grad():
            for batch in self.eval_dataloader:
                outputs = self.peft_model(**batch)
                eval_loss += outputs.loss.item()
                num_eval_steps += 1

        avg_eval_loss = eval_loss / num_eval_steps
        logger.info(f"üìä Evaluation completed - Avg loss: {avg_eval_loss:.4f}")

        return {"eval_loss": avg_eval_loss}

    async def _log_metrics(self, loss: float, step: int):
        """Registrar m√©tricas de entrenamiento."""
        current_lr = self.scheduler.get_last_lr()[0] if self.scheduler else self.config.learning_rate

        # Calcular grad norm
        grad_norm = 0.0
        for param in self.peft_model.parameters():
            if param.grad is not None:
                grad_norm += param.grad.data.norm(2).item() ** 2
        grad_norm = grad_norm ** 0.5

        # Memoria GPU
        gpu_memory = 0.0
        try:
            gpus = GPUtil.getGPUs()
            if gpus:
                gpu_memory = gpus[0].memoryUsed
        except:
            pass

        # Memoria CPU
        cpu_memory = psutil.virtual_memory().percent

        # Throughput
        elapsed_time = time.time() - self.start_time
        throughput = self.global_step * self.config.batch_size * self.config.gradient_accumulation_steps / elapsed_time

        # Crear m√©tricas
        metrics = TrainingMetrics(
            step=self.global_step,
            epoch=self.epoch,
            loss=loss,
            learning_rate=current_lr,
            grad_norm=grad_norm,
            train_time=elapsed_time,
            gpu_memory_used=gpu_memory,
            cpu_memory_used=cpu_memory,
            throughput=throughput
        )

        self.metrics_history.append(metrics)

        # Log to console
        logger.info(
            f"Step {self.global_step}: loss={loss:.4f}, "
            f"lr={current_lr:.6f}, grad_norm={grad_norm:.4f}, "
            f"gpu_mem={gpu_memory:.1f}MB, throughput={throughput:.2f} samples/s"
        )

        # Trigger callbacks
        for callback in self.training_callbacks:
            try:
                await callback(metrics)
            except Exception as e:
                logger.warning(f"Error in training callback: {e}")

    async def _save_checkpoint(self, epoch: int):
        """Guardar checkpoint del modelo."""
        checkpoint_dir = Path(self.config.output_dir) / f"checkpoint-{epoch}"
        checkpoint_dir.mkdir(parents=True, exist_ok=True)

        logger.info(f"üíæ Saving checkpoint to {checkpoint_dir}")

        # Guardar modelo LoRA
        self.peft_model.save_pretrained(checkpoint_dir)

        # Guardar tokenizer
        self.tokenizer.save_pretrained(checkpoint_dir)

        # Guardar configuraci√≥n y estado
        checkpoint_data = {
            "epoch": epoch,
            "global_step": self.global_step,
            "config": self.config.__dict__,
            "metrics": [m.__dict__ for m in self.metrics_history[-10:]],  # √öltimas 10 m√©tricas
            "optimizer_state": self.optimizer.state_dict(),
            "scheduler_state": self.scheduler.state_dict() if self.scheduler else None
        }

        with open(checkpoint_dir / "trainer_state.json", "w") as f:
            json.dump(checkpoint_data, f, indent=2, default=str)

        logger.info("‚úÖ Checkpoint saved")

    async def _compute_model_diff(self) -> ModelDiff:
        """
        Calcular diferencias del modelo para federated learning.

        Returns:
            Diferencias del modelo
        """
        logger.info("üîÑ Computing model differences for federated learning")

        model_diff = ModelDiff()
        model_diff.num_samples = len(self.train_dataset) if self.train_dataset else 0
        model_diff.training_steps = self.global_step

        # Calcular diferencias en pesos LoRA
        if self.base_model_state:
            lora_weights_diff = {}
            for name, param in self.peft_model.named_parameters():
                if param.requires_grad and name in self.base_model_state:
                    diff = param.data - self.base_model_state[name]
                    lora_weights_diff[name] = diff.cpu().numpy().tolist()

            model_diff.lora_weights_diff = lora_weights_diff
            logger.info(f"‚úÖ Computed LoRA weight differences: {len(lora_weights_diff)} parameters")

        # Extraer gradientes si est√° habilitado
        if self.config.extract_gradients:
            gradients = {}
            for name, param in self.peft_model.named_parameters():
                if param.grad is not None:
                    gradients[name] = param.grad.cpu().numpy().tolist()

            model_diff.gradients = gradients
            logger.info(f"‚úÖ Extracted gradients: {len(gradients)} parameters")

        # Metadata
        model_diff.metadata = {
            "model_name": self.config.model_name_or_path,
            "lora_rank": self.config.lora_rank,
            "training_time": time.time() - self.start_time if self.start_time else 0,
            "final_metrics": self._get_current_metrics(),
            "config": self.config.__dict__
        }

        return model_diff

    def _get_current_metrics(self) -> Dict[str, Any]:
        """Obtener m√©tricas actuales."""
        if not self.metrics_history:
            return {}

        latest = self.metrics_history[-1]
        return {
            "loss": latest.loss,
            "learning_rate": latest.learning_rate,
            "grad_norm": latest.grad_norm,
            "gpu_memory_used": latest.gpu_memory_used,
            "cpu_memory_used": latest.cpu_memory_used,
            "throughput": latest.throughput
        }

    async def save_lora_adapter(self, output_path: str):
        """
        Guardar adaptador LoRA.

        Args:
            output_path: Ruta donde guardar el adaptador
        """
        if not self.peft_model:
            raise ValueError("No LoRA model available")

        logger.info(f"üíæ Saving LoRA adapter to {output_path}")

        Path(output_path).parent.mkdir(parents=True, exist_ok=True)
        self.peft_model.save_pretrained(output_path)

        # Guardar configuraci√≥n adicional
        adapter_config = {
            "model_name": self.config.model_name_or_path,
            "lora_config": self.lora_config.to_dict() if self.lora_config else {},
            "training_config": self.config.__dict__,
            "final_metrics": self._get_current_metrics(),
            "training_steps": self.global_step,
            "created_at": datetime.now().isoformat()
        }

        with open(Path(output_path) / "adapter_info.json", "w") as f:
            json.dump(adapter_config, f, indent=2, default=str)

        logger.info("‚úÖ LoRA adapter saved")

    def add_training_callback(self, callback: Callable):
        """
        Agregar callback de entrenamiento.

        Args:
            callback: Funci√≥n callback que recibe TrainingMetrics
        """
        self.training_callbacks.append(callback)

    def get_training_stats(self) -> Dict[str, Any]:
        """
        Obtener estad√≠sticas de entrenamiento.

        Returns:
            Estad√≠sticas completas
        """
        return {
            "is_initialized": self.is_initialized,
            "is_training": self.is_training,
            "global_step": self.global_step,
            "current_epoch": self.epoch,
            "config": self.config.__dict__,
            "metrics_history": [m.__dict__ for m in self.metrics_history],
            "current_metrics": self._get_current_metrics(),
            "gpu_available": torch.cuda.is_available(),
            "gpu_count": torch.cuda.device_count() if torch.cuda.is_available() else 0,
            "accelerator_device": str(self.accelerator.device) if self.accelerator else None
        }

    async def cleanup(self):
        """Limpiar recursos."""
        logger.info("üßπ Cleaning up LocalTrainer resources")

        # Limpiar modelo
        if self.model:
            del self.model
        if self.peft_model:
            del self.peft_model

        # Limpiar optimizer y scheduler
        if self.optimizer:
            del self.optimizer
        if self.scheduler:
            del self.scheduler

        # Limpiar accelerator
        if self.accelerator:
            self.accelerator.free_memory()

        # Limpiar datasets
        if self.train_dataset:
            del self.train_dataset
        if self.eval_dataset:
            del self.eval_dataset

        # Forzar garbage collection
        import gc
        gc.collect()

        if torch.cuda.is_available():
            torch.cuda.empty_cache()

        logger.info("‚úÖ Resources cleaned up")


# Funciones de conveniencia

async def create_local_trainer(
    model_name: str = "EmpoorioLM-base",
    dataset_name: str = "",
    lora_rank: int = 8,
    batch_size: int = 4,
    max_epochs: int = 3,
    output_dir: str = "./local_training_output"
) -> LocalTrainer:
    """
    Crear e inicializar un LocalTrainer con configuraci√≥n predeterminada.

    Args:
        model_name: Nombre del modelo base
        dataset_name: Nombre del dataset
        lora_rank: Rank de LoRA
        batch_size: Tama√±o del batch
        max_epochs: N√∫mero m√°ximo de epochs
        output_dir: Directorio de salida

    Returns:
        LocalTrainer inicializado
    """
    config = LocalTrainerConfig(
        model_name_or_path=model_name,
        dataset_name=dataset_name,
        lora_rank=lora_rank,
        batch_size=batch_size,
        max_epochs=max_epochs,
        output_dir=output_dir
    )

    trainer = LocalTrainer(config)
    success = await trainer.initialize()

    if not success:
        raise RuntimeError("Failed to initialize LocalTrainer")

    return trainer


async def train_with_lora(
    model_name: str,
    dataset_name: str,
    **kwargs
) -> Dict[str, Any]:
    """
    Funci√≥n de conveniencia para entrenamiento con LoRA.

    Args:
        model_name: Nombre del modelo
        dataset_name: Nombre del dataset
        **kwargs: Argumentos adicionales para configuraci√≥n

    Returns:
        Resultados del entrenamiento
    """
    trainer = await create_local_trainer(model_name, dataset_name, **kwargs)

    try:
        results = await trainer.train()
        return results
    finally:
        await trainer.cleanup()


# Import tqdm for progress bars
try:
    from tqdm import tqdm
except ImportError:
    # Fallback progress bar
    class tqdm:
        def __init__(self, iterable, desc=""):
            self.iterable = iterable
            self.desc = desc

        def __iter__(self):
            return iter(self.iterable)

        def set_postfix(self, data):
            pass