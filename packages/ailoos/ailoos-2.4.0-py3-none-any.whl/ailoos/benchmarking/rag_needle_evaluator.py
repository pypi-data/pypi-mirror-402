"""
Evaluador RAG Needle-in-Haystack para medir capacidad de recuperaci√≥n de informaci√≥n
en contextos largos. Compara EmpoorioLM vs GPT-4/Claude/Gemini.
"""

import os
import time
import json
import logging
import random
import hashlib
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, field
from pathlib import Path
import statistics
from concurrent.futures import ThreadPoolExecutor, as_completed

# Imports de datasets
try:
    from datasets import load_dataset, Dataset
    DATASETS_AVAILABLE = True
except ImportError:
    DATASETS_AVAILABLE = False
    print("‚ö†Ô∏è Datasets no disponible, usando datos mock")

# Imports de modelos (reutilizando de benchmark_vs_giants)
try:
    from ailoos.api.empoorio_api import EmpoorioLMApi, GenerationConfig
    EMPOORIO_AVAILABLE = True
except ImportError:
    EMPOORIO_AVAILABLE = False

try:
    import openai
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False

try:
    import anthropic
    ANTHROPIC_AVAILABLE = True
except ImportError:
    ANTHROPIC_AVAILABLE = False

try:
    import google.generativeai as genai
    GOOGLE_AVAILABLE = True
except ImportError:
    GOOGLE_AVAILABLE = False

# Para gr√°ficos
try:
    import matplotlib.pyplot as plt
    import pandas as pd
    PLOTTING_AVAILABLE = True
except ImportError:
    PLOTTING_AVAILABLE = False

# Configuraci√≥n de logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


@dataclass
class NeedleInHaystackTask:
    """Tarea needle-in-haystack individual."""
    context: str
    needle: str
    needle_position: int  # Posici√≥n en tokens donde se inserta la aguja
    context_length_tokens: int
    question: str
    expected_answer: str
    task_id: str


@dataclass
class RagNeedleConfig:
    """Configuraci√≥n del evaluador RAG Needle-in-Haystack."""
    context_sizes: List[int] = field(default_factory=lambda: [1024, 4096, 8192, 16384, 32768])  # tokens
    num_tasks_per_size: int = 10
    needle_types: List[str] = field(default_factory=lambda: ['fact', 'definition', 'quote', 'code'])
    datasets_for_context: List[str] = field(default_factory=lambda: ['wikipedia', 'books', 'articles'])
    output_dir: str = './rag_needle_results'
    api_keys: Dict[str, str] = field(default_factory=dict)
    models_to_test: List[str] = field(default_factory=lambda: ['empoorio', 'gpt4', 'claude', 'gemini'])
    max_retries: int = 3
    timeout_seconds: int = 120
    enable_energy_tracking: bool = True
    generate_plots: bool = True


@dataclass
class RagNeedleResult:
    """Resultado de una evaluaci√≥n needle-in-haystack."""
    model_name: str
    context_size: int
    task_id: str
    accuracy: float  # 1.0 si recupera correctamente, 0.0 si no
    latency: float
    tokens_processed: int
    energy_joules: float = 0.0
    success: bool = True
    error: Optional[str] = None


class ContextGenerator:
    """Generador de contextos largos para needle-in-haystack."""

    def __init__(self, config: RagNeedleConfig):
        self.config = config
        self.context_cache = {}

    def generate_context(self, target_length_tokens: int, needle_type: str) -> str:
        """Genera un contexto largo con informaci√≥n irrelevante."""
        cache_key = f"{target_length_tokens}_{needle_type}"

        if cache_key in self.context_cache:
            return self.context_cache[cache_key]

        context_parts = []

        # Cargar datos de diferentes fuentes
        for dataset_name in self.config.datasets_for_context:
            try:
                data = self._load_dataset_data(dataset_name)
                if data:
                    context_parts.extend(data)
            except Exception as e:
                logger.warning(f"Error cargando datos de {dataset_name}: {e}")

        # Si no hay datos reales, usar datos sint√©ticos
        if not context_parts:
            context_parts = self._generate_synthetic_data()

        # Mezclar y seleccionar contenido hasta alcanzar la longitud deseada
        random.shuffle(context_parts)
        context = ""
        current_tokens = 0

        for part in context_parts:
            part_tokens = self._estimate_tokens(part)
            if current_tokens + part_tokens > target_length_tokens:
                # Truncar si es necesario
                remaining_tokens = target_length_tokens - current_tokens
                if remaining_tokens > 100:  # Solo agregar si hay espacio significativo
                    truncated_part = self._truncate_to_tokens(part, remaining_tokens)
                    context += truncated_part + "\n\n"
                    current_tokens += self._estimate_tokens(truncated_part)
                break
            else:
                context += part + "\n\n"
                current_tokens += part_tokens

        # Rellenar si es necesario con datos sint√©ticos
        while current_tokens < target_length_tokens * 0.9:
            synthetic_data = self._generate_synthetic_paragraph()
            synthetic_tokens = self._estimate_tokens(synthetic_data)
            if current_tokens + synthetic_tokens <= target_length_tokens:
                context += synthetic_data + "\n\n"
                current_tokens += synthetic_tokens
            else:
                break

        self.context_cache[cache_key] = context.strip()
        return self.context_cache[cache_key]

    def _load_dataset_data(self, dataset_name: str) -> List[str]:
        """Carga datos reales de datasets."""
        if not DATASETS_AVAILABLE:
            return []

        try:
            if dataset_name == 'wikipedia':
                # Cargar art√≠culos de Wikipedia
                dataset = load_dataset('wikipedia', '20220301.en', split='train', streaming=True)
                articles = []
                for item in dataset.take(100):  # Tomar primeros 100 art√≠culos
                    text = item.get('text', '')
                    if len(text) > 500:  # Solo art√≠culos sustanciales
                        articles.append(text[:5000])  # Limitar longitud
                return articles

            elif dataset_name == 'books':
                # Cargar libros
                dataset = load_dataset('bookcorpus', split='train', streaming=True)
                books = []
                for item in dataset.take(50):
                    text = item.get('text', '')
                    if len(text) > 1000:
                        books.append(text[:3000])
                return books

            elif dataset_name == 'articles':
                # Cargar art√≠culos cient√≠ficos
                dataset = load_dataset('scientific_papers', 'arxiv', split='train', streaming=True)
                articles = []
                for item in dataset.take(50):
                    abstract = item.get('abstract', '')
                    if len(abstract) > 200:
                        articles.append(abstract[:2000])
                return articles

        except Exception as e:
            logger.warning(f"Error cargando dataset {dataset_name}: {e}")
            return []

        return []

    def _generate_synthetic_data(self) -> List[str]:
        """Genera datos sint√©ticos cuando no hay datasets reales."""
        synthetic_data = []

        topics = [
            "historia antigua", "ciencia moderna", "literatura cl√°sica", "tecnolog√≠a actual",
            "filosof√≠a", "arte contempor√°neo", "econom√≠a global", "medicina", "astronom√≠a",
            "biolog√≠a molecular", "psicolog√≠a", "sociolog√≠a", "pol√≠tica internacional"
        ]

        for topic in topics:
            paragraphs = []
            for i in range(3):
                paragraph = f"En el campo de la {topic}, se han desarrollado numerosos conceptos importantes. Los investigadores han dedicado d√©cadas al estudio de diversos aspectos relacionados con este tema. Las teor√≠as propuestas han evolucionado significativamente con el tiempo, incorporando nuevos descubrimientos y metodolog√≠as avanzadas. Los expertos en la materia contin√∫an explorando las implicaciones pr√°cticas de estos conocimientos te√≥ricos."
                paragraphs.append(paragraph)
            synthetic_data.extend(paragraphs)

        return synthetic_data

    def _generate_synthetic_paragraph(self) -> str:
        """Genera un p√°rrafo sint√©tico."""
        templates = [
            "En el √°mbito de la investigaci√≥n cient√≠fica, los avances tecnol√≥gicos han permitido desarrollar nuevas metodolog√≠as para el an√°lisis de datos complejos. Los cient√≠ficos utilizan algoritmos sofisticados para procesar grandes vol√∫menes de informaci√≥n de manera eficiente.",
            "La historia de la humanidad est√° marcada por importantes descubrimientos que han transformado nuestra comprensi√≥n del mundo. Desde la antig√ºedad hasta la era moderna, el conocimiento acumulado ha sido fundamental para el progreso de la sociedad.",
            "En el campo de la medicina, los tratamientos innovadores han mejorado significativamente la calidad de vida de los pacientes. Los m√©dicos y investigadores colaboran para desarrollar terapias m√°s efectivas y menos invasivas.",
            "La tecnolog√≠a digital ha revolucionado la forma en que nos comunicamos e interactuamos. Las redes sociales y las plataformas en l√≠nea han creado nuevas oportunidades para el intercambio de ideas y conocimientos.",
            "En el estudio de la naturaleza, los bi√≥logos han identificado patrones fascinantes en el comportamiento de los organismos vivos. Estos descubrimientos contribuyen a nuestra comprensi√≥n de la biodiversidad y la evoluci√≥n."
        ]

        return random.choice(templates)

    def _estimate_tokens(self, text: str) -> int:
        """Estima el n√∫mero de tokens en un texto (aproximaci√≥n simple)."""
        # Aproximaci√≥n: ~4 caracteres por token en ingl√©s
        return len(text) // 4

    def _truncate_to_tokens(self, text: str, max_tokens: int) -> str:
        """Trunca un texto a un n√∫mero m√°ximo de tokens."""
        max_chars = max_tokens * 4
        if len(text) <= max_chars:
            return text
        return text[:max_chars].rsplit(' ', 1)[0]  # Cortar en l√≠mite de palabra


class NeedleGenerator:
    """Generador de 'agujas' (informaci√≥n espec√≠fica) para esconder en el contexto."""

    def __init__(self):
        self.needle_templates = {
            'fact': [
                "La capital de Francia es {fact}.",
                "El s√≠mbolo qu√≠mico del oro es {fact}.",
                "La velocidad de la luz en el vac√≠o es de {fact} metros por segundo.",
                "El planeta m√°s grande del sistema solar es {fact}.",
                "El a√±o de la independencia de Estados Unidos fue {fact}."
            ],
            'definition': [
                "Una {fact} es un mam√≠fero marino inteligente con capacidad para usar herramientas.",
                "La {fact} es el proceso por el cual las plantas convierten la luz solar en energ√≠a qu√≠mica.",
                "Una {fact} es una secuencia de instrucciones que una computadora puede ejecutar.",
                "La {fact} es la rama de la matem√°tica que estudia las relaciones entre √°ngulos y lados de los tri√°ngulos.",
                "Una {fact} es una reacci√≥n qu√≠mica en la que una sustancia se descompone en dos o m√°s productos."
            ],
            'quote': [
                '"{fact}" - Albert Einstein',
                '"{fact}" - William Shakespeare',
                '"{fact}" - Mahatma Gandhi',
                '"{fact}" - Steve Jobs',
                '"{fact}" - Nelson Mandela'
            ],
            'code': [
                "La funci√≥n para calcular el factorial de un n√∫mero en Python es: {fact}",
                "El c√≥digo HTML b√°sico para una p√°gina web es: {fact}",
                "La consulta SQL para seleccionar todos los usuarios es: {fact}",
                "La expresi√≥n regular para validar emails es: {fact}",
                "El comando Git para clonar un repositorio es: {fact}"
            ]
        }

        self.facts = {
            'fact': [
                "Par√≠s", "Au", "299792458", "J√∫piter", "1776",
                "Roma", "H", "149597870.7", "Saturno", "1492",
                "Tokio", "O", "31557600", "Urano", "1969"
            ],
            'definition': [
                "delf√≠n", "fotos√≠ntesis", "algoritmo", "trigonometr√≠a", "descomposici√≥n",
                "ballena", "respiraci√≥n", "base de datos", "geometr√≠a", "oxidaci√≥n",
                "chimpanc√©", "evaporaci√≥n", "red neuronal", "estad√≠stica", "hidr√≥lisis"
            ],
            'quote': [
                "La imaginaci√≥n es m√°s importante que el conocimiento",
                "Ser o no ser, esa es la cuesti√≥n",
                "La violencia es el √∫ltimo refugio del incompetente",
                "Mant√©n el hambre, mant√©n la locura",
                "Nuestra vida cotidiana es la mayor fuente de inspiraci√≥n"
            ],
            'code': [
                "def factorial(n): return 1 if n <= 1 else n * factorial(n-1)",
                "<html><head><title>T√≠tulo</title></head><body><h1>Hola Mundo</h1></body></html>",
                "SELECT * FROM usuarios WHERE activo = 1",
                r"^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$",
                "git clone https://github.com/usuario/repositorio.git"
            ]
        }

    def generate_needle(self, needle_type: str) -> Tuple[str, str, str]:
        """
        Genera una aguja con su pregunta y respuesta esperada.

        Returns:
            Tuple[needle_text, question, expected_answer]
        """
        if needle_type not in self.needle_templates:
            needle_type = random.choice(list(self.needle_templates.keys()))

        template = random.choice(self.needle_templates[needle_type])
        fact = random.choice(self.facts[needle_type])

        needle_text = template.format(fact=fact)

        # Generar pregunta basada en el tipo de aguja
        if needle_type == 'fact':
            if 'capital' in needle_text.lower():
                question = "¬øCu√°l es la capital de Francia?"
                expected_answer = fact
            elif 's√≠mbolo' in needle_text.lower():
                question = "¬øCu√°l es el s√≠mbolo qu√≠mico del oro?"
                expected_answer = fact
            elif 'velocidad' in needle_text.lower():
                question = "¬øCu√°l es la velocidad de la luz en el vac√≠o?"
                expected_answer = fact
            elif 'planeta' in needle_text.lower():
                question = "¬øCu√°l es el planeta m√°s grande del sistema solar?"
                expected_answer = fact
            else:
                question = "¬øEn qu√© a√±o se independiz√≥ Estados Unidos?"
                expected_answer = fact

        elif needle_type == 'definition':
            if 'delf√≠n' in needle_text:
                question = "¬øQu√© es un delf√≠n?"
                expected_answer = "un mam√≠fero marino inteligente con capacidad para usar herramientas"
            elif 'fotos√≠ntesis' in needle_text:
                question = "¬øQu√© es la fotos√≠ntesis?"
                expected_answer = "el proceso por el cual las plantas convierten la luz solar en energ√≠a qu√≠mica"
            elif 'algoritmo' in needle_text:
                question = "¬øQu√© es un algoritmo?"
                expected_answer = "una secuencia de instrucciones que una computadora puede ejecutar"
            else:
                question = "¬øQu√© es la trigonometr√≠a?"
                expected_answer = "la rama de la matem√°tica que estudia las relaciones entre √°ngulos y lados de los tri√°ngulos"

        elif needle_type == 'quote':
            question = f"¬øQui√©n dijo: '{fact}'?"
            if 'Einstein' in needle_text:
                expected_answer = "Albert Einstein"
            elif 'Shakespeare' in needle_text:
                expected_answer = "William Shakespeare"
            elif 'Gandhi' in needle_text:
                expected_answer = "Mahatma Gandhi"
            elif 'Jobs' in needle_text:
                expected_answer = "Steve Jobs"
            else:
                expected_answer = "Nelson Mandela"

        elif needle_type == 'code':
            if 'factorial' in needle_text:
                question = "¬øC√≥mo se calcula el factorial de un n√∫mero en Python?"
                expected_answer = fact
            elif 'html' in needle_text.lower():
                question = "¬øCu√°l es el c√≥digo HTML b√°sico para una p√°gina web?"
                expected_answer = fact
            elif 'sql' in needle_text.upper():
                question = "¬øCu√°l es la consulta SQL para seleccionar todos los usuarios?"
                expected_answer = fact
            else:
                question = "¬øCu√°l es el comando Git para clonar un repositorio?"
                expected_answer = fact

        return needle_text, question, expected_answer


class RagNeedleEvaluator:
    """Evaluador principal RAG Needle-in-Haystack."""

    def __init__(self, config: RagNeedleConfig):
        self.config = config
        self.context_generator = ContextGenerator(config)
        self.needle_generator = NeedleGenerator()
        self.results = []

        # Crear directorio de salida
        os.makedirs(config.output_dir, exist_ok=True)

        # Inicializar modelos
        self.models = {}
        self._init_models()

    def _init_models(self):
        """Inicializa los modelos a evaluar."""
        from scripts.benchmark_vs_giants import EmpoorioWrapper, GPT4Wrapper, ClaudeWrapper, GeminiWrapper

        model_classes = {
            'empoorio': EmpoorioWrapper,
            'gpt4': GPT4Wrapper,
            'claude': ClaudeWrapper,
            'gemini': GeminiWrapper
        }

        for model_name in self.config.models_to_test:
            if model_name in model_classes:
                try:
                    # Crear configuraci√≥n b√°sica para el modelo
                    from scripts.benchmark_vs_giants import BenchmarkConfig
                    benchmark_config = BenchmarkConfig()
                    benchmark_config.api_keys = self.config.api_keys

                    self.models[model_name] = model_classes[model_name](benchmark_config)
                    logger.info(f"‚úÖ Modelo {model_name} inicializado")
                except Exception as e:
                    logger.error(f"‚ùå Error inicializando {model_name}: {e}")
            else:
                logger.warning(f"‚ö†Ô∏è Modelo {model_name} no reconocido")

    def generate_tasks(self) -> List[NeedleInHaystackTask]:
        """Genera todas las tareas needle-in-haystack."""
        tasks = []

        for context_size in self.config.context_sizes:
            for task_idx in range(self.config.num_tasks_per_size):
                for needle_type in self.config.needle_types:
                    # Generar contexto
                    context = self.context_generator.generate_context(context_size, needle_type)

                    # Generar aguja
                    needle_text, question, expected_answer = self.needle_generator.generate_needle(needle_type)

                    # Insertar aguja en posici√≥n aleatoria
                    context_tokens = self.context_generator._estimate_tokens(context)
                    needle_position = random.randint(
                        context_tokens // 4,  # No al inicio
                        3 * context_tokens // 4  # No al final
                    )

                    # Convertir posici√≥n a caracteres aproximada
                    needle_char_pos = needle_position * 4
                    if needle_char_pos >= len(context):
                        needle_char_pos = len(context) // 2

                    # Insertar aguja
                    context_with_needle = (
                        context[:needle_char_pos] +
                        f"\n\n{needle_text}\n\n" +
                        context[needle_char_pos:]
                    )

                    # Crear tarea
                    task_id = f"{context_size}_{needle_type}_{task_idx}_{hashlib.md5(context_with_needle.encode()).hexdigest()[:8]}"

                    task = NeedleInHaystackTask(
                        context=context_with_needle,
                        needle=needle_text,
                        needle_position=needle_position,
                        context_length_tokens=context_tokens,
                        question=question,
                        expected_answer=expected_answer,
                        task_id=task_id
                    )

                    tasks.append(task)

        logger.info(f"‚úÖ Generadas {len(tasks)} tareas needle-in-haystack")
        return tasks

    def evaluate_model_on_task(self, model_name: str, model, task: NeedleInHaystackTask) -> RagNeedleResult:
        """Eval√∫a un modelo en una tarea espec√≠fica."""
        try:
            # Preparar prompt
            prompt = f"""Bas√°ndote √∫nicamente en la informaci√≥n proporcionada en el contexto a continuaci√≥n, responde a la pregunta.

Contexto:
{task.context}

Pregunta: {task.question}

Responde de manera concisa y directa. Si la informaci√≥n no est√° en el contexto, di "No puedo encontrar esa informaci√≥n en el contexto proporcionado"."""

            # Medir tiempo y energ√≠a
            start_time = time.time()

            # Generar respuesta
            response, metrics = model.generate(prompt, max_tokens=200, temperature=0.1)

            end_time = time.time()
            latency = end_time - start_time

            # Evaluar precisi√≥n
            accuracy = self._evaluate_answer(response, task.expected_answer)

            # Crear resultado
            result = RagNeedleResult(
                model_name=model_name,
                context_size=task.context_length_tokens,
                task_id=task.task_id,
                accuracy=accuracy,
                latency=latency,
                tokens_processed=task.context_length_tokens,
                energy_joules=metrics.get('energy_joules', 0.0),
                success=True
            )

            return result

        except Exception as e:
            logger.error(f"Error evaluando {model_name} en tarea {task.task_id}: {e}")
            return RagNeedleResult(
                model_name=model_name,
                context_size=task.context_length_tokens,
                task_id=task.task_id,
                accuracy=0.0,
                latency=self.config.timeout_seconds,
                tokens_processed=task.context_length_tokens,
                success=False,
                error=str(e)
            )

    def _evaluate_answer(self, response: str, expected_answer: str) -> float:
        """Eval√∫a si la respuesta es correcta."""
        if not response or not expected_answer:
            return 0.0

        # Normalizar respuestas
        response_norm = response.lower().strip()
        expected_norm = expected_answer.lower().strip()

        # Remover puntuaci√≥n
        import re
        response_norm = re.sub(r'[^\w\s]', '', response_norm)
        expected_norm = re.sub(r'[^\w\s]', '', expected_norm)

        # Verificar si la respuesta esperada est√° contenida en la respuesta
        if expected_norm in response_norm:
            return 1.0

        # Verificar similitud de palabras clave
        expected_words = set(expected_norm.split())
        response_words = set(response_norm.split())

        if expected_words.issubset(response_words):
            return 1.0

        # Verificar si menciona que no puede encontrar la informaci√≥n
        if "no puedo encontrar" in response_norm or "no est√° en el contexto" in response_norm:
            return 0.0

        # Verificar respuestas parciales (al menos 70% de overlap)
        overlap = len(expected_words.intersection(response_words))
        if overlap / len(expected_words) >= 0.7:
            return 1.0

        return 0.0

    def run_evaluation(self) -> List[RagNeedleResult]:
        """Ejecuta la evaluaci√≥n completa."""
        logger.info("üöÄ Iniciando evaluaci√≥n RAG Needle-in-Haystack")

        # Generar tareas
        tasks = self.generate_tasks()

        # Ejecutar evaluaci√≥n
        all_results = []

        for model_name, model in self.models.items():
            logger.info(f"ü§ñ Evaluando modelo: {model_name}")

            model_results = []

            # Procesar tareas en paralelo
            with ThreadPoolExecutor(max_workers=4) as executor:
                futures = []

                for task in tasks:
                    future = executor.submit(self.evaluate_model_on_task, model_name, model, task)
                    futures.append(future)

                for future in as_completed(futures):
                    try:
                        result = future.result()
                        model_results.append(result)
                        all_results.append(result)

                        # Log progreso
                        accuracy = result.accuracy
                        logger.info(f"  ‚úÖ Tarea {result.task_id}: Precisi√≥n {accuracy:.1f}, Latencia {result.latency:.2f}s")

                    except Exception as e:
                        logger.error(f"Error procesando resultado: {e}")

            logger.info(f"‚úÖ Modelo {model_name} evaluado en {len(model_results)} tareas")

        self.results = all_results
        logger.info("‚úÖ Evaluaci√≥n RAG Needle-in-Haystack completada")
        return all_results

    def generate_reports(self):
        """Genera reportes con m√©tricas y gr√°ficos."""
        if not self.results:
            logger.warning("No hay resultados para generar reportes")
            return

        timestamp = time.strftime('%Y%m%d_%H%M%S')

        # Convertir resultados a DataFrame
        df = pd.DataFrame([{
            'model_name': r.model_name,
            'context_size': r.context_size,
            'task_id': r.task_id,
            'accuracy': r.accuracy,
            'latency': r.latency,
            'tokens_processed': r.tokens_processed,
            'energy_joules': r.energy_joules,
            'success': r.success,
            'error': r.error
        } for r in self.results])

        # Reporte JSON
        json_file = os.path.join(self.config.output_dir, f'rag_needle_results_{timestamp}.json')
        df.to_json(json_file, orient='records', indent=2)

        # Reporte CSV
        csv_file = os.path.join(self.config.output_dir, f'rag_needle_results_{timestamp}.csv')
        df.to_csv(csv_file, index=False)

        # Generar gr√°ficos si est√° disponible y configurado
        if PLOTTING_AVAILABLE and self.config.generate_plots:
            self._generate_plots(df, timestamp)

        # Generar reporte de resumen
        self._generate_summary_report(df, timestamp)

        logger.info(f"üìä Reportes guardados en {self.config.output_dir}")

    def _generate_plots(self, df: pd.DataFrame, timestamp: str):
        """Genera gr√°ficos de precisi√≥n vs contexto."""
        # Agrupar por modelo y tama√±o de contexto
        summary = df.groupby(['model_name', 'context_size']).agg({
            'accuracy': ['mean', 'std'],
            'latency': ['mean', 'std'],
            'energy_joules': 'mean'
        }).round(3)

        # Gr√°fico de precisi√≥n vs tama√±o de contexto
        plt.figure(figsize=(12, 8))

        plt.subplot(2, 2, 1)
        for model in df['model_name'].unique():
            model_data = df[df['model_name'] == model]
            avg_accuracy = model_data.groupby('context_size')['accuracy'].mean()
            plt.plot(avg_accuracy.index, avg_accuracy.values, marker='o', label=model, linewidth=2)

        plt.xlabel('Tama√±o del Contexto (tokens)')
        plt.ylabel('Precisi√≥n Promedio')
        plt.title('Precisi√≥n vs Tama√±o del Contexto')
        plt.legend()
        plt.grid(True, alpha=0.3)
        plt.xscale('log')

        # Gr√°fico de latencia vs tama√±o de contexto
        plt.subplot(2, 2, 2)
        for model in df['model_name'].unique():
            model_data = df[df['model_name'] == model]
            avg_latency = model_data.groupby('context_size')['latency'].mean()
            plt.plot(avg_latency.index, avg_latency.values, marker='s', label=model, linewidth=2)

        plt.xlabel('Tama√±o del Contexto (tokens)')
        plt.ylabel('Latencia Promedio (s)')
        plt.title('Latencia vs Tama√±o del Contexto')
        plt.legend()
        plt.grid(True, alpha=0.3)
        plt.xscale('log')

        # Gr√°fico de energ√≠a vs tama√±o de contexto
        plt.subplot(2, 2, 3)
        for model in df['model_name'].unique():
            model_data = df[df['model_name'] == model]
            avg_energy = model_data.groupby('context_size')['energy_joules'].mean()
            plt.plot(avg_energy.index, avg_energy.values, marker='^', label=model, linewidth=2)

        plt.xlabel('Tama√±o del Contexto (tokens)')
        plt.ylabel('Energ√≠a Promedio (J)')
        plt.title('Consumo Energ√©tico vs Tama√±o del Contexto')
        plt.legend()
        plt.grid(True, alpha=0.3)
        plt.xscale('log')

        # Gr√°fico de distribuci√≥n de precisi√≥n por modelo
        plt.subplot(2, 2, 4)
        accuracy_by_model = df.groupby('model_name')['accuracy'].mean()
        accuracy_by_model.plot(kind='bar', color=['skyblue', 'lightgreen', 'orange', 'pink'])
        plt.xlabel('Modelo')
        plt.ylabel('Precisi√≥n Promedio')
        plt.title('Precisi√≥n Promedio por Modelo')
        plt.xticks(rotation=45)
        plt.grid(True, alpha=0.3, axis='y')

        try:
            plt.tight_layout()
        except UserWarning:
            pass  # Layout already tight

        # Guardar gr√°fico
        plot_file = os.path.join(self.config.output_dir, f'rag_needle_analysis_{timestamp}.png')
        plt.savefig(plot_file, dpi=300, bbox_inches='tight')
        plt.close()

        logger.info(f"üìà Gr√°fico guardado: {plot_file}")

    def _generate_summary_report(self, df: pd.DataFrame, timestamp: str):
        """Genera reporte de resumen."""
        summary_file = os.path.join(self.config.output_dir, f'rag_needle_summary_{timestamp}.txt')

        with open(summary_file, 'w', encoding='utf-8') as f:
            f.write("üöÄ Evaluaci√≥n RAG Needle-in-Haystack - Resumen\n")
            f.write("=" * 60 + "\n\n")
            f.write(f"Fecha: {time.strftime('%Y-%m-%d %H:%M:%S')}\n\n")

            # Estad√≠sticas generales
            f.write("üìä ESTAD√çSTICAS GENERALES\n")
            f.write("-" * 30 + "\n")
            f.write(f"Total de evaluaciones: {len(df)}\n")
            f.write(f"Tama√±os de contexto probados: {sorted(df['context_size'].unique())}\n")
            f.write(f"Modelos evaluados: {list(df['model_name'].unique())}\n\n")

            # Rendimiento por modelo
            f.write("ü§ñ RENDIMIENTO POR MODELO\n")
            f.write("-" * 30 + "\n")

            for model in df['model_name'].unique():
                model_data = df[df['model_name'] == model]
                avg_accuracy = model_data['accuracy'].mean()
                avg_latency = model_data['latency'].mean()
                avg_energy = model_data['energy_joules'].mean()

                f.write(f"\n{model.upper()}:\n")
                f.write(f"  Precisi√≥n promedio: {avg_accuracy:.3f}\n")
                f.write(f"  Latencia promedio: {avg_latency:.2f}s\n")
                f.write(f"  Energ√≠a promedio: {avg_energy:.2f}J\n")

                # Rendimiento por tama√±o de contexto
                f.write("  Por tama√±o de contexto:\n")
                for context_size in sorted(model_data['context_size'].unique()):
                    context_data = model_data[model_data['context_size'] == context_size]
                    context_accuracy = context_data['accuracy'].mean()
                    f.write(f"    {context_size} tokens: {context_accuracy:.3f}\n")

            # An√°lisis de degradaci√≥n
            f.write("\nüìâ AN√ÅLISIS DE DEGRADACI√ìN\n")
            f.write("-" * 30 + "\n")

            for model in df['model_name'].unique():
                model_data = df[df['model_name'] == model]
                degradation_data = []

                for context_size in sorted(model_data['context_size'].unique()):
                    context_accuracy = model_data[model_data['context_size'] == context_size]['accuracy'].mean()
                    degradation_data.append((context_size, context_accuracy))

                if len(degradation_data) > 1:
                    f.write(f"\n{model.upper()} - Degradaci√≥n de precisi√≥n:\n")
                    baseline_accuracy = degradation_data[0][1]
                    for size, accuracy in degradation_data:
                        degradation = (baseline_accuracy - accuracy) / baseline_accuracy * 100
                        f.write(f"  {size} tokens: {degradation:+.1f}% vs baseline\n")

            f.write("\nüí° CONCLUSIONES\n")
            f.write("-" * 30 + "\n")
            f.write("Esta evaluaci√≥n mide la capacidad de los modelos para recuperar\n")
            f.write("informaci√≥n espec√≠fica ('needle') de contextos largos ('haystack').\n")
            f.write("Un rendimiento perfecto ser√≠a mantener precisi√≥n alta incluso en\n")
            f.write("contextos muy largos.\n")

        logger.info(f"üìã Reporte de resumen guardado: {summary_file}")


def main():
    """Funci√≥n principal."""
    import argparse

    parser = argparse.ArgumentParser(description='Evaluador RAG Needle-in-Haystack')
    parser.add_argument('--config', type=str, help='Archivo de configuraci√≥n JSON')
    parser.add_argument('--models', nargs='+', help='Modelos a testear')
    parser.add_argument('--context-sizes', nargs='+', type=int, help='Tama√±os de contexto en tokens')
    parser.add_argument('--num-tasks', type=int, default=10, help='N√∫mero de tareas por configuraci√≥n')
    parser.add_argument('--output', type=str, default='./rag_needle_results', help='Directorio de salida')
    parser.add_argument('--openai-key', type=str, help='API key de OpenAI')
    parser.add_argument('--anthropic-key', type=str, help='API key de Anthropic')
    parser.add_argument('--google-key', type=str, help='API key de Google')

    args = parser.parse_args()

    # Configuraci√≥n por defecto
    config = RagNeedleConfig()

    # Sobrescribir con argumentos
    if args.models:
        config.models_to_test = args.models
    if args.context_sizes:
        config.context_sizes = args.context_sizes
    if args.num_tasks:
        config.num_tasks_per_size = args.num_tasks
    if args.output:
        config.output_dir = args.output

    # API keys
    if args.openai_key:
        config.api_keys['openai'] = args.openai_key
    if args.anthropic_key:
        config.api_keys['anthropic'] = args.anthropic_key
    if args.google_key:
        config.api_keys['google'] = args.google_key

    # Cargar desde archivo si especificado
    if args.config:
        with open(args.config, 'r') as f:
            config_data = json.load(f)
        for key, value in config_data.items():
            if hasattr(config, key):
                setattr(config, key, value)

    # Ejecutar evaluaci√≥n
    print("üöÄ Iniciando evaluaci√≥n RAG Needle-in-Haystack...")
    evaluator = RagNeedleEvaluator(config)
    results = evaluator.run_evaluation()

    # Generar reportes
    evaluator.generate_reports()

    print("\nüéâ Evaluaci√≥n completada!")
    print(f"üìÅ Resultados guardados en: {config.output_dir}")

    # Mostrar resumen
    if results:
        print("\nüìä Resumen de resultados:")
        for model in config.models_to_test:
            model_results = [r for r in results if r.model_name == model]
            if model_results:
                avg_accuracy = sum(r.accuracy for r in model_results) / len(model_results)
                avg_latency = sum(r.latency for r in model_results) / len(model_results)
                print(f"ü§ñ {model}: Precisi√≥n {avg_accuracy:.3f}, Latencia {avg_latency:.2f}s")


if __name__ == "__main__":
    main()