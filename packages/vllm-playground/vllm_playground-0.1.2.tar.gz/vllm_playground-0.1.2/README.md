# vLLM Playground

A modern web interface for managing and interacting with vLLM servers (www.github.com/vllm-project/vllm). Supports GPU and CPU modes, with special optimizations for macOS Apple Silicon and enterprise deployment on OpenShift/Kubernetes.

### âœ¨ Agentic-Ready with MCP Support
![vLLM Playground MCP Integration](https://raw.githubusercontent.com/micytao/vllm-playground/main/assets/vllm-playground-mcp-client.png)

*MCP (Model Context Protocol) integration enables models to use external tools with human-in-the-loop approval.*

### âœ¨ Tool Calling Support
![vLLM Playground Interface](https://raw.githubusercontent.com/micytao/vllm-playground/main/assets/vllm-playground-newUI.png)

### âœ¨ Structured Outputs Support
![vLLM Playground with Structured Outputs](https://raw.githubusercontent.com/micytao/vllm-playground/main/assets/vllm-playground-structured-outputs.png)

### ğŸ†• What's New in v0.1.2

- ğŸŒ **ModelScope Support** - Alternative model source for China region users
- ğŸŒ **i18n Chinese** - Comprehensive Chinese language translations
- ğŸ’¬ **Chat Export** - Save conversations with export functionality
- ğŸ› **Bug Fixes** - Windows Unicode fix, sidebar UI improvements

See **[Changelog](CHANGELOG.md)** for full details.

---

## ğŸš€ Quick Start

```bash
# Install from PyPI
pip install vllm-playground

# Pre-download container image (~10GB for GPU)
vllm-playground pull

# Start the playground
vllm-playground
```

Open http://localhost:7860 and click "Start Server" - that's it! ğŸ‰

### CLI Options

```bash
vllm-playground pull                # Pre-download GPU image
vllm-playground pull --cpu          # Pre-download CPU image
vllm-playground --port 8080         # Custom port
vllm-playground stop                # Stop running instance
vllm-playground status              # Check status
```

---

## âœ¨ Key Features

| Feature | Description |
|---------|-------------|
| ğŸ’¬ **Modern Chat UI** | Streamlined ChatGPT-style interface with streaming responses |
| ğŸ”§ **Tool Calling** | Function calling with Llama, Mistral, Qwen, and more |
| ğŸ”— **MCP Integration** | Connect to MCP servers for agentic capabilities |
| ğŸ—ï¸ **Structured Outputs** | Constrain responses to JSON Schema, Regex, or Grammar |
| ğŸ³ **Container Mode** | Zero-setup vLLM via automatic container management |
| â˜¸ï¸ **OpenShift/K8s** | Enterprise deployment with dynamic pod creation |
| ğŸ“Š **Benchmarking** | GuideLLM integration for load testing |
| ğŸ“š **Recipes** | One-click configs from vLLM community recipes |

---

## ğŸ“¦ Installation Options

| Method | Command | Best For |
|--------|---------|----------|
| **PyPI** | `pip install vllm-playground` | Most users |
| **With Benchmarking** | `pip install vllm-playground[benchmark]` | Load testing |
| **From Source** | `git clone` + `python run.py` | Development |
| **OpenShift/K8s** | `./openshift/deploy.sh` | Enterprise |

**ğŸ“– See [Installation Guide](docs/INSTALLATION.md)** for detailed instructions.

---

## ğŸ”§ Configuration

### Tool Calling

Enable in **Server Configuration** before starting:

1. Check "Enable Tool Calling"
2. Select parser (or "Auto-detect")
3. Start server
4. Define tools in the ğŸ”§ toolbar panel

**Supported Models:**
- Llama 3.x (`llama3_json`)
- Mistral (`mistral`)
- Qwen (`hermes`)
- Hermes (`hermes`)

### MCP Servers

Connect to external tools via Model Context Protocol:

1. Go to **MCP Servers** in the sidebar
2. Add a server (presets available: Filesystem, Git, Fetch, Time)
3. Connect and enable in chat panel

**âš ï¸ MCP requires Python 3.10+**

### CPU Mode (macOS)

Edit `config/vllm_cpu.env`:
```bash
export VLLM_CPU_KVCACHE_SPACE=40
export VLLM_CPU_OMP_THREADS_BIND=auto
```

---

## ğŸ“– Documentation

### Getting Started
- **[Installation Guide](docs/INSTALLATION.md)** - All installation methods
- **[Quick Start](docs/QUICKSTART.md)** - Get running in minutes
- **[macOS CPU Guide](docs/MACOS_CPU_GUIDE.md)** - Apple Silicon setup

### Features
- **[Features Overview](docs/FEATURES.md)** - Complete feature list
- **[Gated Models Guide](docs/GATED_MODELS_GUIDE.md)** - Access Llama, Gemma, etc.

### Deployment
- **[OpenShift/K8s Deployment](openshift/README.md)** - Enterprise deployment
- **[Architecture Overview](docs/ARCHITECTURE.md)** - System design
- **[Container Variants](containers/README.md)** - Container options

### Reference
- **[Troubleshooting](docs/TROUBLESHOOTING.md)** - Common issues
- **[Performance Metrics](docs/PERFORMANCE_METRICS.md)** - Benchmarking
- **[Command Reference](docs/QUICK_REFERENCE.md)** - CLI cheat sheet

### Releases
- **[Changelog](CHANGELOG.md)** - Version history and changes
- **[v0.1.2](releases/v0.1.2.md)** - ModelScope integration, i18n improvements
- **[v0.1.1](releases/v0.1.1.md)** - MCP integration, runtime detection
- **[v0.1.0](releases/v0.1.0.md)** - First release, modern UI, tool calling

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   User Browser   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ http://localhost:7860
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Web UI (Host)  â”‚  â† FastAPI + JavaScript
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”
    â†“         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€-â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ vLLM    â”‚ â”‚  MCP   â”‚  â† Containers / External Servers
â”‚Containerâ”‚ â”‚Servers â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€-â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**ğŸ“– See [Architecture Overview](docs/ARCHITECTURE.md)** for details.

---

## ğŸ†˜ Quick Troubleshooting

| Issue | Solution |
|-------|----------|
| Port in use | `vllm-playground stop` |
| Container won't start | `podman logs vllm-service` |
| Tool calling fails | Restart with "Enable Tool Calling" checked |
| Image pull errors | `vllm-playground pull --all` |

**ğŸ“– See [Troubleshooting Guide](docs/TROUBLESHOOTING.md)** for more.

---

## ğŸ”— Related Projects

- **[vLLM](https://github.com/vllm-project/vllm)** - High-throughput LLM serving
- **[LLMCompressor Playground](https://github.com/micytao/llmcompressor-playground)** - Model compression & quantization
- **[GuideLLM](https://github.com/neuralmagic/guidellm)** - Performance benchmarking
- **[MCP Servers](https://github.com/modelcontextprotocol/servers)** - Official MCP servers

---

## ğŸ“ License

Apache 2.0 License - See [LICENSE](LICENSE) file for details.

## ğŸ¤ Contributing

Contributions welcome! Please feel free to submit issues and pull requests.

---

Made with â¤ï¸ for the vLLM community
