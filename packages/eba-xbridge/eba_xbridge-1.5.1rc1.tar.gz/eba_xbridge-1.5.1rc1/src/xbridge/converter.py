"""Module holding the Converter class, which converts from XBRL-XML to XBRL-CSV
taking as input the taxonomy object and the instance object
"""

from __future__ import annotations

import csv
import json
import warnings
from pathlib import Path
from shutil import rmtree
from tempfile import TemporaryDirectory
from typing import Any, Dict, Union
from zipfile import ZipFile

import pandas as pd

from xbridge.exceptions import (
    DecimalValueError,
    FilingIndicatorValueError,
    FilingIndicatorWarning,
)
from xbridge.instance import CsvInstance, Instance, XmlInstance
from xbridge.modules import Module, Table

INDEX_FILE = Path(__file__).parent / "modules" / "index.json"
MAPPING_PATH = Path(__file__).parent / "modules"

if not INDEX_FILE.exists():
    raise ValueError(
        "Cannot find the index file for the modules. "
        "Please make sure that the index file and the "
        "JSON files with the mappings exist in the modules folder."
    )

with open(INDEX_FILE, "r", encoding="utf-8") as fl:
    index: Dict[str, str] = json.load(fl)


class Converter:
    """Converter different types of files into others, using the EBA
    :obj:`taxonomy <xbridge.taxonomy.Taxonomy>` and XBRL-instance. Each file is extracted and saved
    in a temporary directory.
    Then, these files are converted into JSON and again saved in a
    compressed folder such as ZIP or 7z.

    :obj:`Variables <xbridge.taxonomy.Variable>` are generated by combining the open keys
    column with the attributes found within the taxonomy
    :obj:`modules <xbridge.taxonomy.Module>`.
    Then, variable's dimension is extracted
    from it with the purpose to be used with the context
    coming from the ``XML_instance``,
    to create the :obj:`scenario <xbridge.xml_instance.Scenario>`.

    Finally, an inner join is done between the variables
    created and the values from the facts
    of the ``XML_instance`` :obj:`context <xbridge.xml_instance.Context>`.

    """

    def __init__(self, instance_path: Union[str, Path]) -> None:
        path_str = str(instance_path)
        inst = Instance.from_path(path_str)
        module_ref = getattr(inst, "module_ref", None)
        if not isinstance(module_ref, str):
            inst = Instance(path_str)
            module_ref = getattr(inst, "module_ref", None)
        self.instance = inst

        if not module_ref:
            raise ValueError("No module_ref found in the instance.")
        if module_ref not in index:
            raise ValueError(f"Module {module_ref} not found in the taxonomy index")

        module_path = Path(__file__).parent / "modules" / index[module_ref]
        self.module = Module.from_serialized(module_path)
        self._reported_tables: list[str] = []
        self._decimals_parameters: dict[str, Union[int, str]] = {}

    def convert(
        self,
        output_path: Union[str, Path],
        headers_as_datapoints: bool = False,
        validate_filing_indicators: bool = True,
        strict_validation: bool = False,
    ) -> Path:
        """Convert the ``XML Instance`` to a CSV file or between CSV formats"""
        if not output_path:
            raise ValueError("Output path not provided")

        if isinstance(output_path, str):
            output_path = Path(output_path)
        if self.instance is None:
            raise ValueError("Instance not provided")

        if self.module is None:
            raise ValueError("Module of the instance file not found in the taxonomy")

        if isinstance(self.instance, XmlInstance):
            return self.convert_xml(
                output_path, headers_as_datapoints, validate_filing_indicators, strict_validation
            )
        elif isinstance(self.instance, CsvInstance):
            if self.module.architecture != "headers":
                raise ValueError("Cannot convert CSV instance with non-headers architecture")
            return self.convert_csv(output_path)
        else:
            raise ValueError("Invalid instance type")

    def convert_xml(
        self,
        output_path: Path,
        headers_as_datapoints: bool = False,
        validate_filing_indicators: bool = True,
        strict_validation: bool = True,
    ) -> Path:
        module_filind_codes = [table.filing_indicator_code for table in self.module.tables]

        filing_indicator_codes = (
            self.instance.filing_indicators if self.instance.filing_indicators else []
        )

        for filing_indicator in filing_indicator_codes:
            if filing_indicator.table not in module_filind_codes:
                raise ValueError(
                    f"Filing indicator {filing_indicator.table} not found in the module tables."
                )

        instance_path_stem = Path(str(self.instance.path)).stem

        temp_dir = TemporaryDirectory()
        temp_dir_path = Path(temp_dir.name)

        meta_inf_dir = temp_dir_path / instance_path_stem / "META-INF"
        report_dir = temp_dir_path / instance_path_stem / "reports"

        meta_inf_dir.mkdir(parents=True)
        report_dir.mkdir(parents=True)

        with open(meta_inf_dir / "reportPackage.json", "w", encoding="UTF-8") as fl:
            json.dump(
                {"documentInfo": {"documentType": "http://xbrl.org/PWD/2020-12-09/report-package"}},
                fl,
            )

        with open(report_dir / "report.json", "w", encoding="UTF-8") as fl:
            json.dump(
                {
                    "documentInfo": {
                        "documentType": "https://xbrl.org/CR/2021-02-03/xbrl-csv",
                        "extends": [self.module.url],
                    }
                },
                fl,
            )

        self._convert_filing_indicator(report_dir)

        if validate_filing_indicators:
            self._validate_filing_indicators(strict_validation=strict_validation)

        with open(MAPPING_PATH / self.module.dim_dom_file_name, "r", encoding="utf-8") as fl:
            mapping_dict: Dict[str, str] = json.load(fl)
        self._convert_tables(report_dir, mapping_dict, headers_as_datapoints)
        self._convert_parameters(report_dir)

        file_name = instance_path_stem + ".zip"

        zip_file_path = output_path / file_name

        with ZipFile(zip_file_path, "w") as zip_fl:
            for file in meta_inf_dir.iterdir():
                zip_fl.write(file, arcname=f"{instance_path_stem}/META-INF/{file.name}")
            for file in report_dir.iterdir():
                zip_fl.write(file, arcname=f"{instance_path_stem}/reports/{file.name}")

        temp_dir.cleanup()

        return zip_file_path

    def convert_csv(self, output_path: Path) -> Path:
        for table_file in self.instance.table_files:
            table_url = table_file.name
            for table in self.module.tables:
                if table.url == table_url:
                    table_columns = table.columns
                    open_keys_mapping = table._open_keys_mapping
                    break
            else:
                raise ValueError(f"Table {table_url} not found in the module")

            table_df = pd.read_csv(table_file)
            # For type 't' datapoints is only accepted 'true' as value
            # but pandas convert it directly to 'True'.
            # For the rest of boolean datapoints 'true' is also accepted
            table_df = table_df.replace(True, "true")

            columns_rename = {
                f"c{column_code}": property_code
                for property_code, column_code in open_keys_mapping.items()
            }
            table_df.rename(columns=columns_rename, inplace=True)
            open_keys_properties = list(columns_rename.values())
            measure_columns = [
                column["code"] for column in table_columns if column["code"] not in columns_rename
            ]
            measure_columns = [column for column in measure_columns if column in table_df.columns]

            table_df = table_df.melt(id_vars=open_keys_properties, value_vars=measure_columns)

            mapping_dict = {
                column["code"]: f"dp{column['variable_id']}" for column in table_columns
            }
            mapping_df = pd.DataFrame(mapping_dict.items(), columns=["variable", "datapoint"])
            table_df = pd.merge(mapping_df, table_df, on="variable", how="inner")

            table_df.drop(columns=["variable"], inplace=True)
            table_df.rename(columns={"value": "factValue"}, inplace=True)

            table_df.to_csv(table_file, index=False)

        file_name = self.instance.path.name
        zip_file_path = output_path / file_name

        root = self.instance._root_folder

        temp_dir = self.instance.temp_dir_path
        if temp_dir is None:
            raise ValueError("CSV instance has no temp dir path")

        meta_inf_dir = temp_dir / "META-INF"
        report_dir = temp_dir / "reports"

        with ZipFile(zip_file_path, "w") as zip_fl:
            for file in meta_inf_dir.iterdir():
                zip_fl.write(file, arcname=f"{root}/META-INF/{file.name}")
            for file in report_dir.iterdir():
                zip_fl.write(file, arcname=f"{root}/reports/{file.name}")

        rmtree(temp_dir)

        return zip_file_path

    def _get_instance_df(self, table: Table) -> pd.DataFrame:
        """Returns the dataframe with the subset of instace facts applicable to the table"""
        if self.instance.instance_df is None:
            return pd.DataFrame(columns=["datapoint", "value"])
        instance_columns = set(self.instance.instance_df.columns)
        variable_columns = set(table.variable_columns or [])
        open_keys = set(table.open_keys)
        attributes = set(table.attributes)

        # If any open key is not in the instance, then the table cannot have
        # any datapoint
        if not open_keys.issubset(instance_columns):
            return pd.DataFrame(columns=["datapoint", "value"] + list(open_keys))

        # Determine the not relevant dims
        not_relevant_dims = (
            instance_columns
            - variable_columns
            - open_keys
            - attributes
            - {"value", "unit", "decimals"}
        )

        # Convert to list so Pandas won't complain
        needed_columns = list(
            variable_columns | open_keys | attributes | {"value", "decimals"} | not_relevant_dims
        )

        # Intersect with instance_columns as a list
        needed_columns = list(set(needed_columns).intersection(instance_columns))

        instance_df = self.instance.instance_df[needed_columns].copy()

        cols_to_drop = [
            col for col in ["unit"] if col not in attributes and col in instance_df.columns
        ]
        if cols_to_drop:
            instance_df.drop(columns=cols_to_drop, inplace=True)

        # Drop datapoints that have non-null values in not relevant dimensions
        # And drop the not relevant columns
        if not_relevant_dims:
            # Convert to list
            nrd_list = list(not_relevant_dims)
            # Create mask to filter out rows where not relevant dimensions have non-null values
            mask = instance_df[nrd_list].isnull().all(axis=1)
            instance_df = instance_df.loc[mask]
            instance_df.drop(columns=nrd_list, inplace=True)

        # Rows missing values for required open keys do not belong to the table
        if open_keys:
            instance_df.dropna(subset=list(open_keys), inplace=True)

        return instance_df

    def _matching_fact_indices(self, table: Table) -> set[int]:
        """Return indices of instance facts that actually match the table definition."""
        if self.instance.instance_df is None:
            return set()

        instance_df = self._get_instance_df(table)
        if instance_df.empty or table.variable_df is None:
            return set()

        variable_columns = set(table.variable_columns or [])
        open_keys = set(table.open_keys)

        # self.instance.instance_df is guaranteed to not be None due to check at line 296
        instance_columns = set(self.instance.instance_df.columns)

        datapoint_df = table.variable_df.copy()

        # Handle missing columns by filtering datapoint_df (same as _variable_generator)
        # This prevents requiring dimensions that don't exist in the instance
        missing_cols = list(variable_columns - instance_columns)
        if "data_type" in missing_cols:
            missing_cols.remove("data_type")
        if missing_cols:
            mask = datapoint_df[missing_cols].isnull().all(axis=1)
            datapoint_df = datapoint_df.loc[mask]
            datapoint_df = datapoint_df.drop(columns=missing_cols)

        # Match on all variable columns (dimensions) to avoid Cartesian product
        # explosion. Consistent with _variable_generator() to prevent OOM.
        merge_cols = list(variable_columns & instance_columns)

        instance_df = instance_df.copy()
        instance_df["_idx"] = instance_df.index

        merged_df = pd.merge(datapoint_df, instance_df, on=merge_cols, how="inner")

        if open_keys:
            valid_open_keys = [key for key in open_keys if key in merged_df.columns]
            if valid_open_keys:
                merged_df.dropna(subset=valid_open_keys, inplace=True)

        return set(merged_df["_idx"].tolist())

    def _variable_generator(self, table: Table) -> pd.DataFrame:
        """Returns the dataframe with the CSV file for the table

        :param table: The table we use.

        """
        instance_df = self._get_instance_df(table)
        if instance_df.empty or table.variable_df is None:
            return instance_df

        variable_columns = set(table.variable_columns or [])

        open_keys = set(table.open_keys)
        instance_columns = (
            set(self.instance.instance_df.columns)
            if self.instance.instance_df is not None
            else set()
        )

        # Do the intersection and drop from datapoints the columns and records
        datapoint_df = table.variable_df.copy()
        missing_cols = list(variable_columns - instance_columns)
        if "data_type" in missing_cols:
            missing_cols.remove("data_type")
        if missing_cols:
            mask = datapoint_df[missing_cols].isnull().all(axis=1)
            datapoint_df = datapoint_df.loc[mask]
            datapoint_df = datapoint_df.drop(columns=missing_cols)

        # Join the dataframes on the datapoint_columns
        merge_cols = list(variable_columns & instance_columns)

        table_df = pd.merge(datapoint_df, instance_df, on=merge_cols, how="inner")

        if "data_type" in table_df.columns and "decimals" in table_df.columns:
            decimals_table = table_df[["decimals", "data_type"]].drop_duplicates()
            for _, row in decimals_table.iterrows():
                if not row["data_type"] or not row["decimals"]:
                    continue

                data_type = row["data_type"][1:]
                decimals = row["decimals"]
                normalized_decimals = self._normalize_decimals_value(decimals)

                if data_type not in self._decimals_parameters:
                    self._decimals_parameters[data_type] = normalized_decimals
                else:
                    # Skip special values when we already have an entry,
                    # as numeric values take precedence.
                    if normalized_decimals in {"INF", "#none"}:
                        continue

                    existing_value = self._decimals_parameters[data_type]
                    if existing_value in {"INF", "#none"} or (
                        isinstance(existing_value, int)
                        and isinstance(normalized_decimals, int)
                        and normalized_decimals < existing_value
                    ):
                        self._decimals_parameters[data_type] = normalized_decimals

            drop_columns = merge_cols + ["data_type", "decimals"]
        else:
            drop_columns = merge_cols

        # Also drop allowed_values if present (it's metadata, not data)
        if "allowed_values" in table_df.columns:
            drop_columns.append("allowed_values")

        table_df.drop(columns=drop_columns, inplace=True)

        # Drop the datapoints that have null values in the open keys
        valid_open_keys = [key for key in open_keys if key in table_df.columns]
        if valid_open_keys:
            table_df.dropna(subset=valid_open_keys, inplace=True)

        if "unit" in table.attributes and "unit" in table_df.columns and self.instance.units:
            table_df["unit"] = table_df["unit"].map(self.instance.units, na_action="ignore")

        return table_df

    def _normalize_decimals_value(self, decimals: Any) -> Union[int, str]:
        """Return a validated decimals value or raise a DecimalValueError."""
        candidate = decimals
        if isinstance(candidate, str):
            candidate = candidate.strip()

        if candidate in {"INF", "#none"}:
            return candidate

        try:
            return int(candidate)
        except (TypeError, ValueError) as exc:
            raise DecimalValueError(
                f"Invalid decimals value: {decimals}, should be integer, 'INF' or '#none'",
                offending_value=decimals,
            ) from exc

    def _convert_tables(
        self,
        temp_dir_path: Path,
        mapping_dict: Dict[str, str],
        headers_as_datapoints: bool,
    ) -> None:
        for table in self.module.tables:
            ##Workaround:
            # To calculate the table code for abstract tables, we look whether the name
            # ends with a letter, and if so, we remove the last part of the code
            # Possible alternative: add metadata mapping abstract and concrete tables to
            # avoid doing this kind of corrections
            # Defining the output path and check if the table is reported

            if table.filing_indicator_code not in self._reported_tables:
                continue

            datapoints = self._variable_generator(table)

            if datapoints.empty:
                continue

            # if table.architecture == 'datapoints':

            # Cleaning up the dataframe and sorting it
            datapoints = datapoints.rename(columns={"value": "factValue"})
            # Workaround
            # The enumerated key dimensions need to have a prefix like the one
            # Defined by the EBA in the JSON files. We take them from the taxonomy
            # Because EBA is using exactly those for the JSON files.

            datapoints.sort_values(by=["datapoint"], ascending=True, inplace=True)
            output_path_table = temp_dir_path / (table.url or "table.csv")

            export_index = False

            if table.architecture == "headers" and not headers_as_datapoints:
                datapoint_column_df = pd.DataFrame(table.columns, columns=["code", "variable_id"])
                datapoint_column_df.rename(
                    columns={"variable_id": "datapoint", "code": "column_code"},
                    inplace=True,
                )
                open_keys_mapping = {k: f"c{v}" for k, v in table._open_keys_mapping.items()}
                datapoints.rename(columns=open_keys_mapping, inplace=True)
                datapoints = pd.merge(datapoint_column_df, datapoints, on="datapoint", how="inner")
                if not table.open_keys:
                    datapoints["index"] = 0
                    index = "index"
                else:
                    index = list(open_keys_mapping.values())  # type: ignore[assignment]
                    export_index = True
                datapoints = datapoints.pivot(
                    index=index, columns="column_code", values="factValue"
                )

            elif table.architecture == "headers" and headers_as_datapoints:
                datapoints["datapoint"] = "dp" + datapoints["datapoint"].astype(str)

            datapoints.to_csv(output_path_table, index=export_index)

    def _convert_filing_indicator(self, temp_dir_path: Path) -> None:
        # Workaround;
        # Developed for the EBA structure
        output_path_fi = temp_dir_path / "FilingIndicators.csv"
        if self.instance.filing_indicators is None:
            return
        filing_indicators = self.instance.filing_indicators

        with output_path_fi.open("w", newline="", encoding="utf-8") as fl:
            csv_writer = csv.writer(fl)
            csv_writer.writerow(["templateID", "reported"])
            for fil_ind in filing_indicators:
                value = "true" if fil_ind.value else "false"
                csv_writer.writerow([fil_ind.table, value])
                if fil_ind.value and fil_ind.table:
                    self._reported_tables.append(fil_ind.table)

    def _validate_filing_indicators(self, strict_validation: bool = True) -> None:
        """Validate that no facts are orphaned (belong only to non-reported tables).

        Raises:
            FilingIndicatorValueError: If facts exist that belong only to tables with filed=false
        """
        if self.instance.instance_df is None or self.instance.instance_df.empty:
            return

        # Step 1: Track which facts belong to ANY reported table without materializing a huge set
        reported_mask = pd.Series(False, index=self.instance.instance_df.index)
        for table in self.module.tables:
            if table.filing_indicator_code in self._reported_tables:
                reported_indices = self._matching_fact_indices(table)
                if reported_indices:
                    reported_mask.loc[list(reported_indices)] = True

        # Step 2: Find facts that belong ONLY to non-reported tables
        orphaned_mask = pd.Series(False, index=self.instance.instance_df.index)
        orphaned_per_table = {}

        for table in self.module.tables:
            if table.filing_indicator_code not in self._reported_tables:
                orphaned_indices = self._matching_fact_indices(table)
                if orphaned_indices:
                    # Facts in this table that never appear in a reported table
                    orphaned_in_this_table = [
                        idx for idx in orphaned_indices if not reported_mask.loc[idx]
                    ]
                    if orphaned_in_this_table:
                        orphaned_mask.loc[orphaned_in_this_table] = True
                        orphaned_per_table[table.filing_indicator_code] = len(
                            orphaned_in_this_table
                        )

        total_orphaned = int(orphaned_mask.sum())

        if total_orphaned:
            error_msg = (
                f"Filing indicator inconsistency detected:\n"
                f"Found {total_orphaned} fact(s) that belong ONLY"
                f" to non-reported tables:\n"
            )
            for table_code, count in orphaned_per_table.items():
                error_msg += f"  - {table_code}: {count} fact(s)\n"

            if strict_validation:
                error_msg += (
                    "\nThe conversion process will not continue due to strict validation mode. "
                    "Either set filed=true for the relevant tables "
                    "or remove these facts from the XML."
                )
                raise FilingIndicatorValueError(error_msg, orphaned_per_table)
            error_msg += (
                "\nThese facts will be excluded from the output. "
                "Consider setting filed=true for the relevant tables "
                "or removing these facts from the XML."
            )
            warnings.warn(
                error_msg,
                category=FilingIndicatorWarning,
                stacklevel=2,
            )

    def _convert_parameters(self, temp_dir_path: Path) -> None:
        # Workaround;
        # Developed for the EBA structure
        output_path_parameters = temp_dir_path / "parameters.csv"
        parameters: Dict[str, Any] = {
            "entityID": self.instance.entity,
            "refPeriod": self.instance.period,
            "baseCurrency": self.instance.base_currency,
        }

        for data_type, decimals in self._decimals_parameters.items():
            parameters[data_type] = decimals

        with open(output_path_parameters, "w", newline="", encoding="utf-8") as fl:
            csv_writer = csv.writer(fl)
            csv_writer.writerow(["name", "value"])
            for k, v in parameters.items():
                csv_writer.writerow([k, v])
