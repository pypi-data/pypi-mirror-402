"""
Recursive Language Model (RLM) Environment.

Implements the RLM inference strategy where language models can decompose and
recursively interact with input data of unbounded length through REPL environments.

Based on: https://www.alexzhang.dev/blog/recursive-language-models

Architecture:
- REPL loop runs in the framework (MultiTurnEnv pattern)
- Code execution backend can be sandboxed or local (persistent Python worker)
- Sub-LLM calls from worker code are intercepted via HTTP proxy

Key features:
- Works with any dataset that has a normal prompt
- Optional large input data can be provided in info["context"]
- Root model only sees query, not full input data (unless it peeks via code)
- Model can make recursive sub-LLM calls via llm_batch() function
- Final answer returned via answer variable
"""

import asyncio
import base64
import json
import logging
import os
import shlex
import shutil
import subprocess
import sys
import tempfile
import textwrap
import time
import uuid
from dataclasses import dataclass
from pathlib import Path
from time import perf_counter
from typing import Any, Callable, Literal, cast

if sys.version_info < (3, 12):
    from typing_extensions import TypedDict
else:
    from typing import TypedDict

from aiohttp import web
from prime_sandboxes import CommandTimeoutError

import verifiers as vf
from verifiers.envs.sandbox_env import (
    SandboxCreationError,
    SandboxEnv,
    SandboxNotReadyError,
)
from verifiers.rubrics.rubric import Rubric
from verifiers.types import Messages, ModelResponse, State, TrajectoryStep
from verifiers.utils.async_utils import maybe_await
from verifiers.utils.data_utils import extract_boxed_answer
from verifiers.utils.message_utils import concat_messages
from verifiers.utils.response_utils import (
    parse_is_truncated,
    parse_response_messages,
    parse_response_tokens,
)
from verifiers.utils.rlm_data_serialization_utils import (
    DataSerializer,
    PreparedContextData,
    SerializerRegistry,
    build_default_serializer_registry,
    prepare_context_data,
)
from verifiers.utils.tool_utils import convert_func_to_oai_tool
from verifiers.utils.tunnel_utils import TunnelPool

logger = logging.getLogger(__name__)


class RLMCodeExecutionTimeout(Exception):
    """Raised when code execution exceeds the configured timeout."""


@dataclass(frozen=True)
class RLMWorkerPaths:
    base_dir: str
    command_fifo: str
    response_fifo: str
    ready_flag: str
    install_done_flag: str
    worker_path: str
    worker_pid_file: str
    context_file: str
    answer_file: str
    log_file: str

    def to_dict(self) -> dict[str, str]:
        return {
            "base_dir": self.base_dir,
            "command_fifo": self.command_fifo,
            "response_fifo": self.response_fifo,
            "ready_flag": self.ready_flag,
            "install_done_flag": self.install_done_flag,
            "worker_path": self.worker_path,
            "worker_pid_file": self.worker_pid_file,
            "context_file": self.context_file,
            "answer_file": self.answer_file,
            "log_file": self.log_file,
        }


@dataclass
class LocalRLMReplSession:
    rollout_id: str
    temp_dir: tempfile.TemporaryDirectory
    paths: RLMWorkerPaths
    worker_process: subprocess.Popen | None = None
    venv_path: str | None = None


@dataclass(frozen=True)
class RLMExecResult:
    stdout: str
    stderr: str | None = None
    exit_code: int | None = None


def _extract_tokens_from_response(response: Any) -> tuple[int, int]:
    usage = getattr(response, "usage", None)
    if not usage and isinstance(response, dict):
        usage = response.get("usage")
    if not usage:
        return 0, 0
    if isinstance(usage, dict):
        return (
            int(usage.get("prompt_tokens", 0) or 0),
            int(usage.get("completion_tokens", 0) or 0),
        )
    return (
        int(getattr(usage, "prompt_tokens", 0) or 0),
        int(getattr(usage, "completion_tokens", 0) or 0),
    )


def _ensure_rlm_metric_state(state: State) -> None:
    state.setdefault("sub_llm_call_count", 0)
    state.setdefault("sub_llm_total_turns", 0)
    state.setdefault("sub_llm_prompt_tokens", 0)
    state.setdefault("sub_llm_completion_tokens", 0)
    state.setdefault("sub_llm_total_tool_calls", 0)
    state.setdefault("sub_llm_batch_count", 0)
    state.setdefault("sub_llm_max_batch_size", 0)
    state.setdefault("sub_llm_mean_batch_size", 0.0)

    state.setdefault("main_rlm_turns", 0)
    state.setdefault("main_rlm_prompt_tokens", 0)
    state.setdefault("main_rlm_completion_tokens", 0)

    state.setdefault("repl_total_time_seconds", 0.0)
    state.setdefault("repl_call_count", 0)
    state.setdefault("repl_mean_time_seconds", 0.0)

    state.setdefault("_rlm_sub_llm_call_ids", {})
    state.setdefault("_rlm_sub_llm_batch_counts", {})


def _update_rlm_repl_metrics(state: State, execution_seconds: float) -> None:
    _ensure_rlm_metric_state(state)
    state["repl_total_time_seconds"] += execution_seconds
    state["repl_call_count"] += 1
    if state["repl_call_count"] > 0:
        state["repl_mean_time_seconds"] = (
            state["repl_total_time_seconds"] / state["repl_call_count"]
        )


def update_rlm_metrics_from_step(state: State, step: TrajectoryStep) -> None:
    _ensure_rlm_metric_state(state)
    extras = step.get("extras", {}) or {}
    is_sub_llm = bool(extras.get("is_sub_llm_call"))

    prompt_tokens, completion_tokens = _extract_tokens_from_response(
        step.get("response")
    )

    if is_sub_llm:
        state["sub_llm_total_turns"] += 1
        state["sub_llm_prompt_tokens"] += prompt_tokens
        state["sub_llm_completion_tokens"] += completion_tokens
        state["sub_llm_total_tool_calls"] += int(extras.get("tool_call_count", 0) or 0)

        batch_id = extras.get("batch_id")
        request_id = extras.get("request_id")
        call_ids: dict[str, bool] = state.get("_rlm_sub_llm_call_ids", {})
        batch_counts: dict[str, int] = state.get("_rlm_sub_llm_batch_counts", {})

        if batch_id:
            request_id_norm = request_id if request_id not in (None, "") else "_missing"
            key = f"{batch_id}:{request_id_norm}"
            if key not in call_ids:
                call_ids[key] = True
                state["sub_llm_call_count"] += 1
                batch_counts[batch_id] = batch_counts.get(batch_id, 0) + 1
        else:
            # Fallback: treat each turn as its own call if identifiers are missing.
            state["sub_llm_call_count"] += 1

        state["_rlm_sub_llm_call_ids"] = call_ids
        state["_rlm_sub_llm_batch_counts"] = batch_counts

        if batch_counts:
            batch_sizes = list(batch_counts.values())
            state["sub_llm_batch_count"] = len(batch_sizes)
            state["sub_llm_max_batch_size"] = max(batch_sizes)
            state["sub_llm_mean_batch_size"] = sum(batch_sizes) / len(batch_sizes)
        else:
            state["sub_llm_batch_count"] = 0
            state["sub_llm_max_batch_size"] = 0
            state["sub_llm_mean_batch_size"] = 0.0
    else:
        state["main_rlm_turns"] += 1
        state["main_rlm_prompt_tokens"] += prompt_tokens
        state["main_rlm_completion_tokens"] += completion_tokens


class RLMMonitorRubric(vf.Rubric):
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self.add_metric(self.sub_llm_call_count)
        self.add_metric(self.sub_llm_total_turns)
        self.add_metric(self.sub_llm_prompt_tokens)
        self.add_metric(self.sub_llm_completion_tokens)
        self.add_metric(self.sub_llm_total_tool_calls)
        self.add_metric(self.sub_llm_batch_count)
        self.add_metric(self.sub_llm_max_batch_size)
        self.add_metric(self.sub_llm_mean_batch_size)
        self.add_metric(self.main_rlm_turns)
        self.add_metric(self.main_rlm_prompt_tokens)
        self.add_metric(self.main_rlm_completion_tokens)
        self.add_metric(self.repl_total_time_seconds)
        self.add_metric(self.repl_call_count)
        self.add_metric(self.repl_mean_time_seconds)

    async def sub_llm_call_count(self, state: State) -> int:
        return state["sub_llm_call_count"]

    async def sub_llm_total_turns(self, state: State) -> int:
        return state["sub_llm_total_turns"]

    async def sub_llm_prompt_tokens(self, state: State) -> int:
        return state["sub_llm_prompt_tokens"]

    async def sub_llm_completion_tokens(self, state: State) -> int:
        return state["sub_llm_completion_tokens"]

    async def sub_llm_total_tool_calls(self, state: State) -> int:
        return state["sub_llm_total_tool_calls"]

    async def sub_llm_batch_count(self, state: State) -> int:
        return state["sub_llm_batch_count"]

    async def sub_llm_max_batch_size(self, state: State) -> int:
        return state["sub_llm_max_batch_size"]

    async def sub_llm_mean_batch_size(self, state: State) -> float:
        return state["sub_llm_mean_batch_size"]

    async def main_rlm_turns(self, state: State) -> int:
        return state["main_rlm_turns"]

    async def main_rlm_prompt_tokens(self, state: State) -> int:
        return state["main_rlm_prompt_tokens"]

    async def main_rlm_completion_tokens(self, state: State) -> int:
        return state["main_rlm_completion_tokens"]

    async def repl_total_time_seconds(self, state: State) -> float:
        return state["repl_total_time_seconds"]

    async def repl_call_count(self, state: State) -> int:
        return state["repl_call_count"]

    async def repl_mean_time_seconds(self, state: State) -> float:
        return state["repl_mean_time_seconds"]


class SubLLMTurn(TypedDict):
    """A single turn in a sub-LLM call (used by RLMEnv)."""

    prompt_messages: list[dict]  # Messages before this LLM call
    response: ModelResponse  # Full response object (with token_ids, logprobs)
    tool_call_count: int  # Number of tool calls made in this turn


class SubLLMResult(TypedDict):
    """Result of a sub-LLM call, possibly with multiple turns (used by RLMEnv)."""

    final_content: str
    turns: list[SubLLMTurn]
    total_prompt_tokens: int
    total_completion_tokens: int
    tool_call_count: int
    num_turns: int
    max_turns_reached: bool


# Worker script that runs inside the sandbox - handles code execution only
# The REPL loop is managed by the framework, not this script
_RLM_WORKER_SCRIPT = textwrap.dedent(
    '''
    import ast
    import contextlib
    import io
    import json
    import os
    import random
    import sys
    import time
    import traceback
    from pathlib import Path
    from concurrent.futures import ThreadPoolExecutor
    import requests

    COMMAND_FIFO = "{command_fifo}"
    RESPONSE_FIFO = "{response_fifo}"
    READY_FLAG = "{ready_flag}"
    CONTEXT_FILE = "{context_file}"
    ANSWER_FILE = "{answer_file}"

    # Sub-LLM configuration from environment
    INTERCEPTION_URL = os.environ.get("RLM_INTERCEPTION_URL", "")
    SUB_MODEL = os.environ.get("RLM_SUB_MODEL", "")
    MAX_SUB_LLM_PARALLELISM = int(os.environ.get("RLM_MAX_SUB_LLM_PARALLELISM", "5"))
    SUB_LLM_TIMEOUT = int(os.environ.get("RLM_SUB_LLM_TIMEOUT", "300"))
    SANDBOX_TIMEOUT = int(os.environ.get("RLM_SANDBOX_TIMEOUT", "120"))
    SUB_LLM_STAGGER_MS = int(os.environ.get("RLM_SUB_LLM_STAGGER_MS", "0"))
    SUB_LLM_STAGGER_JITTER_MS = int(
        os.environ.get("RLM_SUB_LLM_STAGGER_JITTER_MS", "0")
    )
    if SANDBOX_TIMEOUT > 0:
        SUB_LLM_TIMEOUT = min(SUB_LLM_TIMEOUT, SANDBOX_TIMEOUT)

    # Guardrails for user code execution (best-effort, not a sandbox)
    def _parse_disallowed(raw: str) -> list[str]:
        if not raw:
            return []
        raw = raw.replace(",", " ")
        return [item.strip() for item in raw.split() if item.strip()]

    DISALLOWED_MODULES = set(
        _parse_disallowed(os.environ.get("RLM_DISALLOWED_MODULES", ""))
    )
    DISALLOWED_BUILTINS = set(
        _parse_disallowed(os.environ.get("RLM_DISALLOWED_BUILTINS", ""))
    )

    def _is_disallowed_module(name: str) -> bool:
        for blocked in DISALLOWED_MODULES:
            if name == blocked or name.startswith(blocked + "."):
                return True
        return False

    def _build_restricted_builtins() -> dict:
        builtins_obj = __builtins__
        if not isinstance(builtins_obj, dict):
            builtins_obj = builtins_obj.__dict__
        restricted = dict(builtins_obj)

        if DISALLOWED_MODULES:
            original_import = restricted.get("__import__")

            def _restricted_import(
                name, globals=None, locals=None, fromlist=(), level=0
            ):
                if _is_disallowed_module(name):
                    raise ImportError(
                        f"Import of '{{name}}' is blocked by RLM policy"
                    )
                if original_import is None:
                    raise ImportError("Import mechanism unavailable")
                return original_import(name, globals, locals, fromlist, level)

            restricted["__import__"] = _restricted_import

        for builtin_name in DISALLOWED_BUILTINS:
            restricted.pop(builtin_name, None)

        return restricted

    def ensure_fifo(path: str) -> None:
        if os.path.exists(path):
            os.remove(path)
        os.mkfifo(path)

    for fifo_path in (COMMAND_FIFO, RESPONSE_FIFO):
        ensure_fifo(fifo_path)

    # Load extra_data from file (written by setup_state)
    extra_data = None
    if Path(CONTEXT_FILE).exists():
        with open(CONTEXT_FILE, "r", encoding="utf-8") as f:
            _full_context = json.load(f)
            data_spec = _full_context.get("input_data_spec") or {{}}
            if data_spec:
                payload_path = data_spec.get("payload_path")
                payload_encoding = data_spec.get("payload_encoding")
                payload = None
                if not payload_path:
                    raise ValueError("input_data_spec is missing payload_path")
                if payload_encoding:
                    with open(payload_path, "r", encoding=payload_encoding) as pf:
                        payload = pf.read()
                else:
                    with open(payload_path, "rb") as pf:
                        payload = pf.read()

                dtype = data_spec.get("dtype", "")
                deserializer_code = data_spec.get("deserializer_code")
                deserializer_function = data_spec.get("deserializer_function")

                if deserializer_code and deserializer_function:
                    namespace = {{}}
                    exec(deserializer_code, namespace)
                    if deserializer_function not in namespace:
                        raise ValueError(
                            "Deserializer function '"
                            + str(deserializer_function)
                            + "' not found"
                        )
                    extra_data = namespace[deserializer_function](payload, data_spec)
                elif dtype == "text":
                    extra_data = payload
                elif dtype == "json":
                    if isinstance(payload, bytes):
                        payload = payload.decode("utf-8")
                    if isinstance(payload, str):
                        extra_data = json.loads(payload)
                    else:
                        extra_data = payload
                else:
                    raise ValueError(
                        "No deserializer provided for dtype '" + str(dtype) + "'."
                    )
            else:
                extra_data = _full_context.get("input_data")

    # Initialize answer structure
    answer = {{"ready": False, "content": ""}}
    if Path(ANSWER_FILE).exists():
        with open(ANSWER_FILE, "r", encoding="utf-8") as f:
            answer = json.load(f)

    def _single_llm_call(prompt: str, batch_id: str, **kwargs) -> dict:
        """Make a single sub-LLM call via interception server.
        
        Returns a dict with 'content' and 'metadata' keys (including 'elapsed_seconds').
        """
        from time import perf_counter
        import uuid
        start_time = perf_counter()
        
        if not INTERCEPTION_URL:
            return {{
                "content": "Error: Sub-LLM interception URL not configured",
                "metadata": {{"error": True, "elapsed_seconds": 0.0}},
            }}
        
        try:
            request_id = uuid.uuid4().hex[:8]
            payload = {{
                "model": SUB_MODEL or "default",
                "messages": [{{"role": "user", "content": prompt}}],
                "_batch_id": batch_id,
                "_request_id": request_id,
            }}
            # Add any extra kwargs
            for k, v in kwargs.items():
                if k not in ("model", "messages", "_batch_id", "_request_id"):
                    payload[k] = v
            
            resp = requests.post(
                INTERCEPTION_URL,
                json=payload,
                timeout=SUB_LLM_TIMEOUT,
            )
            resp.raise_for_status()
            data = resp.json()
            content = data.get("choices", [{{}}])[0].get("message", {{}}).get("content", "")
            metadata = data.get("_rlm_metadata", {{}})
            elapsed = perf_counter() - start_time
            metadata["elapsed_seconds"] = elapsed
            return {{"content": content, "metadata": metadata}}
        except Exception as e:
            elapsed = perf_counter() - start_time
            return {{
                "content": f"Error in sub-LLM call: {{e}}",
                "metadata": {{"error": True, "elapsed_seconds": elapsed}},
            }}

    def llm_batch(prompts: list, **kwargs) -> list:
        """
        Make multiple sub-LLM calls in parallel.
        
        Prints a summary of each call's metadata (including timing), then returns the list of responses.
        
        Parallelism is controlled by RLM_MAX_SUB_LLM_PARALLELISM.
        Sandbox timeout is available via SANDBOX_TIMEOUT env var.
        
        Args:
            prompts: List of prompts for the sub-LLMs
            **kwargs: Additional arguments applied to all calls
        
        Returns:
            List of response contents in the same order as the input prompts
        """
        from time import perf_counter
        import uuid
        batch_start = perf_counter()
        batch_id = uuid.uuid4().hex[:8]
        with ThreadPoolExecutor(max_workers=MAX_SUB_LLM_PARALLELISM) as executor:
            futures = []
            for i, prompt in enumerate(prompts):
                jitter_ms = (
                    random.random() * SUB_LLM_STAGGER_JITTER_MS
                    if SUB_LLM_STAGGER_JITTER_MS > 0
                    else 0.0
                )
                delay_s = max(0.0, (i * SUB_LLM_STAGGER_MS + jitter_ms) / 1000.0)

                def _call_with_delay(
                    p=prompt, d=delay_s, b=batch_id, kw=kwargs
                ):
                    if d:
                        time.sleep(d)
                    return _single_llm_call(p, b, **kw)

                futures.append(executor.submit(_call_with_delay))
            results = [f.result() for f in futures]
        batch_elapsed = perf_counter() - batch_start
        
        # Print metadata summary with timing
        print(f"llm_batch: {{len(results)}} call(s) in {{batch_elapsed:.2f}}s")
        for i, r in enumerate(results):
            meta = r.get("metadata", {{}})
            elapsed = meta.get("elapsed_seconds", 0.0)
            if meta.get("error"):
                print(f"  [{{i}}]: error ({{elapsed:.2f}}s)")
            else:
                tokens = meta.get("prompt_tokens", 0) + meta.get("completion_tokens", 0)
                tool_calls = meta.get("tool_call_count", 0)
                max_turns = meta.get("max_turns_reached", False)
                status = "⚠ max turns" if max_turns else "✓"
                print(f"  [{{i}}]: {{tokens}} tokens, {{tool_calls}} tool calls, {{elapsed:.2f}}s {{status}}")
        
        # Return just the content
        return [r.get("content", "") for r in results]

    restricted_builtins = _build_restricted_builtins()

    # Persistent execution namespace
    namespace: dict[str, object] = {{
        "__name__": "__main__",
        "__builtins__": restricted_builtins,
        "extra_data": extra_data,
        "answer": answer,
        "llm_batch": llm_batch,
    }}

    # Signal ready
    Path(READY_FLAG).write_text("ready", encoding="utf-8")

    execution_count = 0

    while True:
        with open(COMMAND_FIFO, "r", encoding="utf-8") as command_file:
            payload = command_file.read()
        if not payload:
            continue
        request = json.loads(payload)
        if request.get("shutdown"):
            break
        
        code = request.get("code", "")
        seq = request.get("seq", 0)  # Sequence number for request/response matching
        execution_count += 1
        
        result = {{
            "status": "ok",
            "stdout": "",
            "stderr": "",
            "result": None,
            "execution_count": execution_count,
            "seq": seq,  # Echo back sequence number for framework to verify
            "answer": namespace.get("answer", {{"ready": False, "content": ""}}),
        }}
        
        stdout_buffer = io.StringIO()
        stderr_buffer = io.StringIO()
        
        try:
            with contextlib.redirect_stdout(stdout_buffer), contextlib.redirect_stderr(stderr_buffer):
                module_ast = ast.parse(code, mode="exec")
                body = list(module_ast.body)
                trailing_expr = None
                if body and isinstance(body[-1], ast.Expr):
                    trailing_expr = body.pop()
                if body:
                    exec_module = ast.Module(body=body, type_ignores=[])
                    exec(compile(exec_module, "<cell>", "exec"), namespace, namespace)
                if trailing_expr is not None:
                    value = eval(
                        compile(ast.Expression(trailing_expr.value), "<cell>", "eval"),
                        namespace,
                        namespace,
                    )
                    if value is not None:
                        result["result"] = repr(value)
        except Exception:
            result["status"] = "error"
            result["result"] = traceback.format_exc()
        
        result["stdout"] = stdout_buffer.getvalue()
        result["stderr"] = stderr_buffer.getvalue()
        result["answer"] = namespace.get("answer", {{"ready": False, "content": ""}})
        
        # Save answer to file for persistence
        with open(ANSWER_FILE, "w", encoding="utf-8") as f:
            json.dump(result["answer"], f)
        
        with open(RESPONSE_FIFO, "w", encoding="utf-8") as response_file:
            response_file.write(json.dumps(result))
    '''
)


_RLM_START_COMMAND_TEMPLATE = textwrap.dedent(
    """
    bash -lc '
    set -euo pipefail

    command_fifo="{command_fifo}"
    response_fifo="{response_fifo}"
    ready_flag="{ready_flag}"
    install_done_flag="{install_done_flag}"
    worker_path="{worker_path}"
    worker_pid_file="{worker_pid_file}"

    rm -f "$command_fifo" "$response_fifo" "$ready_flag" "$install_done_flag" "$worker_pid_file"

    # Write worker script but do NOT start it yet
    # Worker will be started by setup_state after context/env vars are set
    python - <<'PY'
import base64
from pathlib import Path

Path("{worker_path}").write_bytes(base64.b64decode("{worker_b64}"))
PY

    tail -f /dev/null
    '
    """
)


def _make_ready_wait_script(
    ready_flag: str,
    max_wait_seconds: int,
    error_message: str = "RLM worker failed to start",
) -> str:
    """Generate a ready wait script with configurable timeout."""
    # Each iteration sleeps 0.05 seconds, so calculate iterations needed
    iterations = max(1, int(max_wait_seconds / 0.05))
    return textwrap.dedent(
        f"""
        bash -lc '
        for i in $(seq 1 {iterations}); do
          if [ -f "{ready_flag}" ]; then
            exit 0
          fi
          sleep 0.05
        done
        echo "{error_message}" >&2
        exit 1
        '
        """
    )


def _make_worker_ready_wait_script(
    ready_flag: str,
    pid_file: str,
    log_file: str,
    max_wait_seconds: int,
) -> str:
    """Wait for worker ready flag or fail fast if the worker process exits."""
    iterations = max(1, int(max_wait_seconds / 0.1))
    return textwrap.dedent(
        f"""
        bash -lc '
        for i in $(seq 1 {iterations}); do
          if [ -f "{ready_flag}" ]; then
            exit 0
          fi
          if [ -f "{pid_file}" ]; then
            pid=$(cat "{pid_file}" 2>/dev/null || true)
            if [ -n "$pid" ] && ! kill -0 "$pid" 2>/dev/null; then
              echo "RLM worker exited" >&2
              if [ -f "{log_file}" ]; then
                echo "---LOG---" >&2
                tail -n 200 "{log_file}" >&2
              fi
              exit 1
            fi
          fi
          sleep 0.1
        done
        echo "RLM worker failed to start" >&2
        if [ -f "{log_file}" ]; then
          echo "---LOG---" >&2
          tail -n 200 "{log_file}" >&2
        fi
        exit 1
        '
        """
    )


def _build_worker_paths(base_dir: str) -> RLMWorkerPaths:
    base_dir = base_dir.rstrip("/") or base_dir
    return RLMWorkerPaths(
        base_dir=base_dir,
        command_fifo=os.path.join(base_dir, "rlm_cmd"),
        response_fifo=os.path.join(base_dir, "rlm_res"),
        ready_flag=os.path.join(base_dir, "rlm_ready"),
        install_done_flag=os.path.join(base_dir, "rlm_install_done"),
        worker_path=os.path.join(base_dir, "rlm_worker.py"),
        worker_pid_file=os.path.join(base_dir, "rlm_worker.pid"),
        context_file=os.path.join(base_dir, "rlm_context.json"),
        answer_file=os.path.join(base_dir, "rlm_answer.json"),
        log_file=os.path.join(base_dir, "rlm_worker.log"),
    )


def _render_worker_script(paths: RLMWorkerPaths) -> str:
    return _RLM_WORKER_SCRIPT.format(
        command_fifo=paths.command_fifo,
        response_fifo=paths.response_fifo,
        ready_flag=paths.ready_flag,
        context_file=paths.context_file,
        answer_file=paths.answer_file,
    )


# System prompt for sub-LLMs (called via llm_batch)
_SUB_LLM_SYSTEM_PROMPT = """You are a sub-agent being called by a parent model to help with a specific task.
Answer the query directly and concisely. Put your final answer inside \\boxed{}.

Example: If asked "What is 2+2?", respond with reasoning then \\boxed{4}."""


# System prompt for RLM
_RLM_SYSTEM_PROMPT = """You are operating in a Recursive Language Model (RLM) environment - an iterative Python REPL where you explore data step by step.

## Critical: This is an ITERATIVE environment

You will write code, see its output, then write more code based on what you learned. **Do NOT try to solve everything in one tool call.** Each tool call executes and returns output before you continue.

Use the `call_python_repl` tool to execute Python code. The REPL maintains state across calls. See the tool description for available variables and functions.

## Input Data Metadata
{metadata_summary}

## Workflow

**Step 1: Explore the data**
```python
print(type(extra_data))
print(extra_data[:500] if isinstance(extra_data, str) else extra_data[:3])
```
Wait for output. Now you know the actual format.

**Step 2: Process and build your answer**
```python
answer["content"] = "your current best answer"
```

**Step 3: Verify and finalize (only after reviewing output)**
```python
print(f"My answer: {answer['content']}")
answer["ready"] = True
```

## Important Rules

1. **NEVER set `answer["ready"] = True` until you have seen execution output** - you need feedback first
2. **One step at a time** - make small tool calls, see output, then continue
3. **Use `llm_batch()` for semantic tasks** - summarization, understanding text, classification, etc.
"""


class BaseRLMExecutor:
    def __init__(self, env: "RLMEnv") -> None:
        self.env = env

    async def get_payload_dir(self, state: State) -> str | None:
        return None

    async def setup(self, state: State, prepared: "PreparedContextData") -> None:
        raise NotImplementedError

    async def execute(self, payload: dict[str, Any], state: State) -> RLMExecResult:
        raise NotImplementedError

    async def read_answer(self, state: State) -> str:
        return ""

    async def recover_from_timeout(self, state: State) -> bool:
        return False

    async def cleanup(self, state: State) -> None:
        return None

    async def teardown(self) -> None:
        return None


class SandboxRLMExecutor(BaseRLMExecutor):
    def __init__(self, env: "RLMEnv") -> None:
        super().__init__(env)
        self._paths = env._sandbox_paths

    async def get_payload_dir(self, state: State) -> str | None:
        return self._paths.base_dir

    async def setup(self, state: State, prepared: "PreparedContextData") -> None:
        state["rlm_paths"] = self._paths.to_dict()
        max_sandbox_retries = 5
        for attempt in range(max_sandbox_retries):
            try:
                await self._prepare_sandbox_and_start_worker(state, prepared)
                return
            except vf.SandboxError as e:
                cause_text = str(e.__cause__ or e)
                lower_cause = cause_text.lower()
                retryable = (
                    isinstance(e, SandboxNotReadyError)
                    or "worker failed to start" in lower_cause
                    or "sandbox_not_ready" in lower_cause
                    or "timeout during sandbox creation" in lower_cause
                )
                if retryable and attempt < max_sandbox_retries - 1:
                    logger.warning(
                        "Sandbox startup failed (attempt %s/%s): %s. Recreating sandbox...",
                        attempt + 1,
                        max_sandbox_retries,
                        cause_text,
                    )
                    state = await self._recreate_sandbox(state)
                else:
                    raise

    async def execute(self, payload: dict[str, Any], state: State) -> RLMExecResult:
        sandbox_id = state.get("sandbox_id")
        if not sandbox_id:
            raise vf.SandboxError() from Exception("Sandbox ID not set")
        payload_json = json.dumps(payload)
        payload_b64 = base64.b64encode(payload_json.encode("utf-8")).decode("utf-8")
        command = textwrap.dedent(
            f"""
            python3 - <<'PY'
import base64
import json
import sys

data = base64.b64decode('{payload_b64}').decode('utf-8')
with open('{self._paths.command_fifo}', 'w', encoding='utf-8') as command_file:
    command_file.write(data)
with open('{self._paths.response_fifo}', 'r', encoding='utf-8') as response_file:
    sys.stdout.write(response_file.read())
PY
            """
        )
        try:
            result = await self.env.sandbox_client.execute_command(
                sandbox_id, command, timeout=self.env.code_execution_timeout
            )
        except CommandTimeoutError as e:
            logger.warning(
                "Code execution timed out after %ss", self.env.code_execution_timeout
            )
            raise RLMCodeExecutionTimeout from e
        except Exception as e:
            logger.error(f"Sandbox error during code execution: {e}")
            raise vf.SandboxError() from e
        return RLMExecResult(
            stdout=result.stdout or "",
            stderr=result.stderr or "",
            exit_code=getattr(result, "exit_code", None),
        )

    async def read_answer(self, state: State) -> str:
        sandbox_id = state.get("sandbox_id")
        if not sandbox_id:
            return ""
        try:
            result = await self._execute_command_with_retry(
                sandbox_id,
                f'cat {self._paths.answer_file} 2>/dev/null || echo \'{{"content": ""}}\'',
            )
            return json.loads(result.stdout.strip()).get("content", "")
        except Exception:
            return ""

    async def recover_from_timeout(self, state: State) -> bool:
        context_dict = state.get("rlm_context")
        if not context_dict:
            logger.error("Cannot recover from timeout: missing rlm_context in state")
            return False
        try:
            state = await self._recreate_sandbox(state)
            prepared = PreparedContextData(
                context_dict=context_dict,
                payload_bytes=state.get("rlm_payload_bytes"),
                payload_path=state.get("rlm_payload_path"),
                payload_name=state.get("rlm_payload_name"),
            )
            await self._prepare_sandbox_and_start_worker(state, prepared)
        except Exception as e:
            logger.error(f"Failed to recover from code timeout: {e}")
            return False
        state["rlm_worker_ready"] = True
        state["_exec_seq"] = 0
        return True

    async def cleanup(self, state: State) -> None:
        return None

    async def _execute_command_with_retry(
        self, sandbox_id: str, command: str, timeout: int | None = None
    ):
        """Execute command with retry logic for transient sandbox errors."""
        effective_timeout = timeout or self.env.timeout_per_command_seconds
        start = perf_counter()
        logger.debug(f"Executing command in sandbox {sandbox_id}: {command[:100]}...")
        try:
            result = await self.env.with_retry(self.env.sandbox_client.execute_command)(
                sandbox_id, command, timeout=effective_timeout
            )
        except CommandTimeoutError as e:
            logger.debug(
                f"Command timed out after {effective_timeout}s in sandbox {sandbox_id}"
            )
            raise vf.SandboxError() from e
        except Exception as e:
            raise vf.SandboxError() from e
        elapsed = perf_counter() - start
        logger.debug(f"Command completed in {elapsed:.1f}s")
        return result

    async def _write_json_to_sandbox(
        self, sandbox_id: str, data: dict, file_path: str, filename: str
    ) -> None:
        data_bytes = json.dumps(data).encode("utf-8")
        await self.env.with_retry(self.env.sandbox_client.upload_bytes)(
            sandbox_id, file_path=file_path, file_bytes=data_bytes, filename=filename
        )

    async def upload_file_to_sandbox(
        self, sandbox_id: str, data: bytes, file_path: str, filename: str | None
    ) -> None:
        import tempfile

        tmp_path = None
        try:
            suffix = f"-{filename}" if filename else ""
            with tempfile.NamedTemporaryFile(delete=False, suffix=suffix) as tmp_file:
                tmp_path = Path(tmp_file.name)
                tmp_file.write(data)
            await self.env.with_retry(self.env.sandbox_client.upload_file)(
                sandbox_id, file_path, str(tmp_path)
            )
        finally:
            if tmp_path is not None:
                tmp_path.unlink(missing_ok=True)

    async def _wait_for_worker_ready(self, sandbox_id: str) -> None:
        wait_script = _make_worker_ready_wait_script(
            self._paths.ready_flag,
            self._paths.worker_pid_file,
            self._paths.log_file,
            self.env.max_startup_wait_seconds,
        )
        timeout = self.env.max_startup_wait_seconds + 10
        result = await self._execute_command_with_retry(
            sandbox_id, wait_script, timeout=timeout
        )
        stderr = result.stderr or ""
        stdout = result.stdout or ""
        if (
            "RLM worker failed to start" in stdout
            or "RLM worker failed to start" in stderr
            or "RLM worker exited" in stdout
            or "RLM worker exited" in stderr
        ):
            debug_result = await self._execute_command_with_retry(
                sandbox_id,
                (
                    f"ls -la {self._paths.base_dir}/rlm* 2>&1; echo '---PID---'; "
                    f"cat {self._paths.worker_pid_file} 2>&1 || echo 'no pid'; "
                    "echo '---LOG---'; "
                    f"cat {self._paths.log_file} 2>&1 || echo 'no log'; "
                    "echo '---PS---'; ps aux 2>&1"
                ),
            )
            logger.error(
                f"RLM worker failed to start. Debug info:\n{debug_result.stdout}"
            )
            raise vf.SandboxError() from Exception(
                f"RLM worker failed to start: {debug_result.stdout[:500]}"
            )

    async def _wait_for_install_done(self, sandbox_id: str) -> None:
        install_wait_seconds = self.env._compute_install_wait_seconds()
        packages = ["requests"]
        extra_packages = [
            p.strip() for p in self.env.pip_install_packages.split() if p.strip()
        ]
        packages.extend(extra_packages)
        if not packages:
            return
        install_cmd = " ".join(packages)
        timeout = install_wait_seconds + 10
        install_script = textwrap.dedent(
            f"""
            bash -lc '
            set -euo pipefail
            rm -f "{self._paths.install_done_flag}"
            pip install -q {install_cmd} 2>&1 | tee /tmp/rlm_pip.log
            touch "{self._paths.install_done_flag}"
            '
            """
        )
        result = await self._execute_command_with_retry(
            sandbox_id, install_script, timeout=timeout
        )
        exit_code = getattr(result, "exit_code", 0)
        if (
            isinstance(exit_code, int)
            and not isinstance(exit_code, bool)
            and exit_code != 0
        ):
            debug_result = await self._execute_command_with_retry(
                sandbox_id,
                (
                    "echo '---PIP LOG---'; tail -n 200 /tmp/rlm_pip.log 2>&1 || "
                    "echo 'no pip log'"
                ),
            )
            logger.error(
                "RLM pip install failed (exit_code=%s). Log tail:\n%s",
                exit_code,
                debug_result.stdout,
            )
            raise vf.SandboxError() from Exception("RLM pip install failed")

    async def _start_worker(self, state: State) -> None:
        sandbox_id = state["sandbox_id"]
        interception_url = state["interception_url"]

        sub_llm_timeout = self.env.sub_llm_timeout
        disallowed_modules = shlex.quote(self.env.disallowed_modules)
        disallowed_builtins = shlex.quote(self.env.disallowed_builtins)
        script_wait_iterations = max(1, int(self.env.max_startup_wait_seconds / 0.1))
        await self._wait_for_install_done(sandbox_id)
        start_worker_cmd = f"""
export RLM_INTERCEPTION_URL="{interception_url}"
export RLM_SUB_MODEL="{self.env.sub_model or state.get("model", "")}"
export RLM_MAX_SUB_LLM_PARALLELISM="{self.env.max_sub_llm_parallelism}"
export RLM_SUB_LLM_STAGGER_MS="{self.env.sub_llm_stagger_ms}"
export RLM_SUB_LLM_STAGGER_JITTER_MS="{self.env.sub_llm_stagger_jitter_ms}"
export RLM_SUB_LLM_TIMEOUT="{sub_llm_timeout}"
export RLM_SANDBOX_TIMEOUT="{self.env.code_execution_timeout}"
export RLM_DISALLOWED_MODULES={disallowed_modules}
export RLM_DISALLOWED_BUILTINS={disallowed_builtins}

sync 2>/dev/null || true
for i in $(seq 1 {script_wait_iterations}); do
    if [ -f "{self._paths.worker_path}" ]; then
        break
    fi
    sleep 0.1
done

        if [ ! -f "{self._paths.worker_path}" ]; then
            echo "Worker script not found - pip install may have failed or timed out" >&2
            exit 1
        fi

        sleep 0.2

        nohup python -u {self._paths.worker_path} >> {self._paths.log_file} 2>&1 &
        echo $! > {self._paths.worker_pid_file}
"""
        start_worker_timeout = self.env.max_startup_wait_seconds + 30
        await self._execute_command_with_retry(
            sandbox_id, start_worker_cmd, timeout=start_worker_timeout
        )
        await self._wait_for_worker_ready(sandbox_id)

    async def _prepare_sandbox_and_start_worker(
        self, state: State, prepared: "PreparedContextData"
    ) -> None:
        sandbox_id = state["sandbox_id"]
        try:
            await self.env.sandbox_client.wait_for_creation(sandbox_id)
        except Exception as e:
            raise SandboxNotReadyError(e)
        if prepared.payload_bytes is not None and prepared.payload_path:
            await self.upload_file_to_sandbox(
                sandbox_id,
                prepared.payload_bytes,
                prepared.payload_path,
                prepared.payload_name,
            )
        await self._write_json_to_sandbox(
            sandbox_id,
            prepared.context_dict,
            self._paths.context_file,
            "rlm_context.json",
        )
        await self._write_json_to_sandbox(
            sandbox_id,
            {"ready": False, "content": ""},
            self._paths.answer_file,
            "rlm_answer.json",
        )
        await self._start_worker(state)

    async def _recreate_sandbox(self, state: State) -> State:
        old_sandbox_id = state.get("sandbox_id")
        if old_sandbox_id:
            self.env.active_sandboxes.discard(old_sandbox_id)
            try:
                await self.env.sandbox_client.delete(old_sandbox_id)
            except Exception as e:
                logger.warning(f"Failed to delete broken sandbox {old_sandbox_id}: {e}")

        await asyncio.sleep(1)

        request = self.env.get_sandbox_request(state)
        try:
            sandbox = await self.env.with_retry(self.env.sandbox_client.create)(request)
        except Exception as e:
            raise SandboxCreationError(e)
        self.env.active_sandboxes.add(sandbox.id)
        logger.debug(f"Created replacement sandbox {sandbox.id}")
        state["sandbox_id"] = sandbox.id
        return state


class LocalRLMExecutor(BaseRLMExecutor):
    def __init__(self, env: "RLMEnv") -> None:
        super().__init__(env)
        self._sessions: dict[str, LocalRLMReplSession] = {}
        self._venv_lock = asyncio.Lock()
        self._instance_venv_path: str | None = None
        self._instance_venv_ready = False

    async def get_payload_dir(self, state: State) -> str | None:
        session = self._get_or_create_session(state)
        return session.paths.base_dir

    async def setup(self, state: State, prepared: "PreparedContextData") -> None:
        session = self._get_or_create_session(state)
        venv_path = await self._ensure_venv(session)
        session.venv_path = venv_path

        await self._write_local_files(session, prepared)
        await self._start_worker(state, session)

    async def execute(self, payload: dict[str, Any], state: State) -> RLMExecResult:
        session = self._get_session(state)
        if session.worker_process is None:
            raise vf.SandboxError() from Exception("RLM worker process not running")
        if session.worker_process.poll() is not None:
            raise vf.SandboxError() from Exception("RLM worker process not running")

        def _do_io() -> str:
            payload_json = json.dumps(payload)
            with open(
                session.paths.command_fifo, "w", encoding="utf-8"
            ) as command_file:
                command_file.write(payload_json)
            with open(
                session.paths.response_fifo, "r", encoding="utf-8"
            ) as response_file:
                return response_file.read()

        try:
            raw = await asyncio.wait_for(
                asyncio.to_thread(_do_io),
                timeout=self.env.code_execution_timeout,
            )
        except asyncio.TimeoutError as e:
            logger.warning(
                "Code execution timed out after %ss", self.env.code_execution_timeout
            )
            raise RLMCodeExecutionTimeout from e
        except Exception as e:
            raise vf.SandboxError() from e

        return RLMExecResult(stdout=raw, stderr="")

    async def read_answer(self, state: State) -> str:
        session = self._sessions.get(state.get("rollout_id", ""))
        if not session:
            return ""
        try:
            content = Path(session.paths.answer_file).read_text(encoding="utf-8")
            return json.loads(content).get("content", "")
        except Exception:
            return ""

    async def recover_from_timeout(self, state: State) -> bool:
        session = self._sessions.get(state.get("rollout_id", ""))
        if not session:
            logger.error("Cannot recover from timeout: missing local session")
            return False
        try:
            self._stop_worker(session)
            prepared = PreparedContextData(
                context_dict=state.get("rlm_context", {}),
                payload_bytes=state.get("rlm_payload_bytes"),
                payload_path=state.get("rlm_payload_path"),
                payload_name=state.get("rlm_payload_name"),
            )
            await self._write_local_files(session, prepared)
            await self._start_worker(state, session)
        except Exception as e:
            logger.error(f"Failed to recover from code timeout: {e}")
            return False
        state["rlm_worker_ready"] = True
        state["_exec_seq"] = 0
        return True

    async def cleanup(self, state: State) -> None:
        rollout_id = state.get("rollout_id")
        if not rollout_id:
            return
        session = self._sessions.pop(rollout_id, None)
        if not session:
            return
        self._stop_worker(session)
        session.temp_dir.cleanup()

    async def teardown(self) -> None:
        if self.env.execution_backend != "local":
            return
        if self._sessions:
            sessions = list(self._sessions.values())
            self._sessions.clear()
            for session in sessions:
                try:
                    self._stop_worker(session)
                finally:
                    session.temp_dir.cleanup()
        if self.env.local_venv_scope != "instance":
            return
        if self._instance_venv_path:
            if sys.is_finalizing():
                shutil.rmtree(self._instance_venv_path, True)
            else:
                try:
                    await asyncio.to_thread(
                        shutil.rmtree, self._instance_venv_path, True
                    )
                except RuntimeError as e:
                    if "cannot schedule new futures" in str(e).lower():
                        shutil.rmtree(self._instance_venv_path, True)
                    else:
                        raise
            self._instance_venv_path = None
            self._instance_venv_ready = False

    def _get_or_create_session(self, state: State) -> LocalRLMReplSession:
        rollout_id = state.get("rollout_id")
        if not rollout_id:
            raise ValueError("rollout_id must be set before creating local session")
        session = self._sessions.get(rollout_id)
        if session:
            return session
        temp_dir = tempfile.TemporaryDirectory(prefix=f"rlm_{rollout_id}_")
        paths = _build_worker_paths(temp_dir.name)
        session = LocalRLMReplSession(
            rollout_id=rollout_id,
            temp_dir=temp_dir,
            paths=paths,
        )
        self._sessions[rollout_id] = session
        state["rlm_paths"] = paths.to_dict()
        return session

    def _get_session(self, state: State) -> LocalRLMReplSession:
        rollout_id = state.get("rollout_id")
        if not rollout_id or rollout_id not in self._sessions:
            raise vf.SandboxError() from Exception("Local session not initialized")
        return self._sessions[rollout_id]

    async def _ensure_venv(self, session: LocalRLMReplSession) -> str:
        if self.env.local_venv_scope == "rollout":
            venv_path = os.path.join(session.paths.base_dir, ".venv")
            await self._create_venv(venv_path, force=True)
            await self._install_packages(venv_path)
            return venv_path

        async with self._venv_lock:
            if self._instance_venv_ready and self._instance_venv_path:
                return self._instance_venv_path
            venv_path = self._instance_venv_path or tempfile.mkdtemp(prefix="rlm_venv_")
            await self._create_venv(venv_path, force=not self._instance_venv_ready)
            await self._install_packages(venv_path)
            self._instance_venv_path = venv_path
            self._instance_venv_ready = True
            return venv_path

    async def _create_venv(self, venv_path: str, force: bool) -> None:
        if force and os.path.exists(venv_path):
            await asyncio.to_thread(shutil.rmtree, venv_path, True)
        args = ["uv", "venv", venv_path]
        await self._run_uv_command(args, self.env._compute_install_wait_seconds())

    async def _install_packages(self, venv_path: str) -> None:
        packages = ["requests"]
        extras = [p.strip() for p in self.env.pip_install_packages.split() if p.strip()]
        packages.extend(extras)
        if not packages:
            return
        python_path = self._venv_python(venv_path)
        args = ["uv", "pip", "install", "-q", "--python", python_path]
        args.extend(packages)
        await self._run_uv_command(args, self.env._compute_install_wait_seconds())

    async def _run_uv_command(self, args: list[str], timeout: int) -> None:
        def _run() -> subprocess.CompletedProcess:
            return subprocess.run(
                args,
                check=False,
                capture_output=True,
                text=True,
                timeout=timeout,
            )

        try:
            result = await asyncio.to_thread(_run)
        except FileNotFoundError:
            raise vf.SandboxError() from RuntimeError(
                "uv not found on PATH; local execution requires uv installed"
            )
        except subprocess.TimeoutExpired:
            raise vf.SandboxError() from RuntimeError(
                f"uv command timed out after {timeout} seconds"
            )
        if result.returncode != 0:
            stderr = (result.stderr or "").strip()
            stdout = (result.stdout or "").strip()
            raise vf.SandboxError() from Exception(
                f"uv command failed: {' '.join(args)}\nstdout: {stdout}\nstderr: {stderr}"
            )

    def _venv_python(self, venv_path: str) -> str:
        if os.name == "nt":
            return os.path.join(venv_path, "Scripts", "python.exe")
        return os.path.join(venv_path, "bin", "python")

    async def _write_local_files(
        self, session: LocalRLMReplSession, prepared: "PreparedContextData"
    ) -> None:
        Path(session.paths.base_dir).mkdir(parents=True, exist_ok=True)
        if prepared.payload_bytes is not None and prepared.payload_path:
            payload_path = Path(prepared.payload_path)
            payload_path.parent.mkdir(parents=True, exist_ok=True)
            payload_path.write_bytes(prepared.payload_bytes)
        Path(session.paths.context_file).write_text(
            json.dumps(prepared.context_dict), encoding="utf-8"
        )
        Path(session.paths.answer_file).write_text(
            json.dumps({"ready": False, "content": ""}), encoding="utf-8"
        )

    async def _start_worker(self, state: State, session: LocalRLMReplSession) -> None:
        if not session.venv_path:
            raise vf.SandboxError() from Exception("Local venv not initialized")
        worker_script = _render_worker_script(session.paths)
        Path(session.paths.worker_path).write_text(worker_script, encoding="utf-8")

        env_vars = os.environ.copy()
        env_vars.update(
            {
                "RLM_INTERCEPTION_URL": state["interception_url"],
                "RLM_SUB_MODEL": self.env.sub_model or state.get("model", ""),
                "RLM_MAX_SUB_LLM_PARALLELISM": str(self.env.max_sub_llm_parallelism),
                "RLM_SUB_LLM_STAGGER_MS": str(self.env.sub_llm_stagger_ms),
                "RLM_SUB_LLM_STAGGER_JITTER_MS": str(
                    self.env.sub_llm_stagger_jitter_ms
                ),
                "RLM_SUB_LLM_TIMEOUT": str(self.env.sub_llm_timeout),
                "RLM_SANDBOX_TIMEOUT": str(self.env.code_execution_timeout),
                "RLM_DISALLOWED_MODULES": self.env.disallowed_modules,
                "RLM_DISALLOWED_BUILTINS": self.env.disallowed_builtins,
            }
        )

        python_path = self._venv_python(session.venv_path)
        with open(session.paths.log_file, "a", encoding="utf-8") as log_file:
            process = subprocess.Popen(
                [python_path, "-u", session.paths.worker_path],
                stdout=log_file,
                stderr=subprocess.STDOUT,
                env=env_vars,
            )
        session.worker_process = process

        await self._wait_for_ready(session)

    async def _wait_for_ready(self, session: LocalRLMReplSession) -> None:
        max_wait_seconds = self.env.max_startup_wait_seconds
        start = perf_counter()
        while True:
            if Path(session.paths.ready_flag).exists():
                return
            if session.worker_process and session.worker_process.poll() is not None:
                log_tail = ""
                try:
                    log_tail = Path(session.paths.log_file).read_text(encoding="utf-8")[
                        -2000:
                    ]
                except Exception:
                    pass
                raise vf.SandboxError() from Exception(
                    f"RLM worker exited before ready. Log tail:\n{log_tail}"
                )
            if perf_counter() - start > max_wait_seconds:
                raise vf.SandboxError() from Exception("RLM worker failed to start")
            await asyncio.sleep(0.1)

    def _stop_worker(self, session: LocalRLMReplSession) -> None:
        if not session.worker_process:
            return
        try:
            session.worker_process.terminate()
            session.worker_process.wait(timeout=5)
        except Exception:
            try:
                session.worker_process.kill()
            except Exception:
                pass
        session.worker_process = None


class RLMEnv(SandboxEnv):
    """
    Recursive Language Model Environment.

    Extends SandboxEnv to provide a Python REPL environment where the model can:
    - Interact with large input data stored as a variable (`extra_data`)
    - Make recursive sub-LLM calls via `llm_batch()`
    - Return final answers via an `answer` variable

    Architecture:
    - REPL loop runs in the framework (standard MultiTurnEnv pattern)
    - Code execution runs in a sandbox or locally (persistent Python worker)
    - Sub-LLM calls from worker code are intercepted via HTTP proxy

    Works with any dataset that has a normal prompt. Input data can optionally
    be provided in info[context_key] for large data that shouldn't be in the prompt.

    Args:
        sub_model: Model to use for sub-LLM calls (defaults to same as root model)
        sub_tools: List of Python functions that sub-LLMs can use as tools.
                   These tools are NOT available to the root model.
        sub_tool_max_turns: Maximum tool-calling turns for sub-LLM calls (default: 5)
        max_iterations: Maximum REPL iterations before stopping (maps to max_turns)
        max_output_length: Maximum length of code execution output
        max_sub_llm_parallelism: Maximum number of concurrent sub-LLM calls
        sub_llm_stagger_ms: Optional fixed per-call stagger delay (ms) within llm_batch.
        sub_llm_stagger_jitter_ms: Optional random jitter (ms) added to stagger delay.
        context_key: Key in info containing optional input data (default: "context")
        context_dtype: Optional dtype override for input data serialization.
                   If set, must match a supported serializer dtype.
        data_serializers: Optional list of custom serializers provided by the designer.
                   These are registered on top of the default registry (text/json),
                   overriding by dtype if there are conflicts.
        serializer_registry: Optional explicit serializer registry. If provided,
                   data_serializers must be None and this registry is used as-is.
        system_prompt: Custom system prompt (default: RLM standard prompt)
        interception_host: Optional hostname/IP for interception server (auto-tunneled if not set)
        interception_port: Port for interception server (default: 8766)
        pip_install_packages: Space-separated packages to install in addition to requests
                   (default: "")
        max_startup_wait_seconds: Maximum seconds to wait for worker startup (default: 120)
        include_sub_llm_in_trajectory: Whether to include sub-LLM calls as trajectory steps.
                   When True (default), sub-LLM turns are prepended to the trajectory as
                   TrajectoryStep objects with tokens, enabling training on sub-LLM calls.
                   When False, sub-LLM calls happen but are not stored.
        context_warning_threshold: Fraction of max_seq_len at which to warn the model
                   to finish (default: 0.80). Only active if max_seq_len is set.
        code_execution_timeout: Timeout in seconds for code execution (default: 120).
                   This is longer than the default command timeout to allow for
                   llm_batch calls which can take several minutes.
        abort_on_code_timeout: If True, abort the rollout when code execution times out.
                   If False (default), return an error message to the model so it can
                   try a more efficient approach.
        execution_backend: Where code execution runs. "sandbox" (default) preserves
                   existing behavior; "local" runs on the host without tunnels.
        local_venv_scope: For local execution, whether to create one uv venv per
                   env instance ("instance", default) or per rollout ("rollout").
        disallowed_modules: Space-separated module names that user code may not import
                   (best-effort guardrail). Defaults to blocking common filesystem modules.
        disallowed_builtins: Space-separated builtin names removed from user code
                   execution (best-effort guardrail). Defaults to "open".
        **kwargs: Additional arguments passed to SandboxEnv
    """

    # Worker file paths
    _WORKER_PATH = "/tmp/rlm_worker.py"
    _COMMAND_FIFO = "/tmp/rlm_cmd"
    _RESPONSE_FIFO = "/tmp/rlm_res"
    _READY_FLAG = "/tmp/rlm_ready"
    _INSTALL_DONE_FLAG = "/tmp/rlm_install_done"
    _WORKER_PID_FILE = "/tmp/rlm_worker.pid"
    _CONTEXT_FILE = "/tmp/rlm_context.json"
    _ANSWER_FILE = "/tmp/rlm_answer.json"

    def __init__(
        self,
        sub_model: str | None = None,
        sub_tools: list[Callable] | None = None,
        sub_tool_max_turns: int = 5,
        max_iterations: int = 50,
        max_output_length: int = 8192,
        max_sub_llm_parallelism: int = 5,
        sub_llm_stagger_ms: int = 200,
        sub_llm_stagger_jitter_ms: int = 50,
        context_key: str = "context",
        context_dtype: str | None = None,
        data_serializers: list[DataSerializer] | None = None,
        serializer_registry: SerializerRegistry | None = None,
        system_prompt: str | None = None,
        interception_host: str | None = None,
        interception_port: int = 8766,
        pip_install_packages: str = "",
        max_startup_wait_seconds: int = 120,
        include_sub_llm_in_trajectory: bool = True,
        context_warning_threshold: float = 0.80,
        code_execution_timeout: int = 120,
        abort_on_code_timeout: bool = False,
        execution_backend: Literal["sandbox", "local"] = "sandbox",
        local_venv_scope: Literal["instance", "rollout"] = "instance",
        disallowed_modules: str = ("os sys pathlib shutil glob tempfile io builtins"),
        disallowed_builtins: str = "open",
        rubric: Rubric | None = None,
        **kwargs,
    ):
        self.sub_model = sub_model
        self.sub_tools = sub_tools or []
        self.sub_tool_max_turns = sub_tool_max_turns
        self.max_iterations = max_iterations
        self.max_output_length = max_output_length
        self.max_sub_llm_parallelism = max_sub_llm_parallelism
        self.sub_llm_stagger_ms = sub_llm_stagger_ms
        self.sub_llm_stagger_jitter_ms = sub_llm_stagger_jitter_ms
        self.context_key = context_key
        self.context_dtype = context_dtype
        if serializer_registry is not None and data_serializers is not None:
            raise ValueError(
                "Provide either serializer_registry or data_serializers, not both."
            )
        if serializer_registry is not None:
            self.serializer_registry = serializer_registry
        else:
            registry = build_default_serializer_registry()
            for serializer in data_serializers or []:
                registry.register(serializer, allow_override=True)
            self.serializer_registry = registry
        self.data_serializers = self.serializer_registry.all()
        self.custom_system_prompt = system_prompt
        if execution_backend == "local" and interception_host is None:
            interception_host = "127.0.0.1"
        self.interception_host = interception_host
        self.interception_port = interception_port
        self.pip_install_packages = pip_install_packages
        self.max_startup_wait_seconds = max_startup_wait_seconds
        self.include_sub_llm_in_trajectory = include_sub_llm_in_trajectory
        self.context_warning_threshold = context_warning_threshold
        self.code_execution_timeout = code_execution_timeout
        self.abort_on_code_timeout = abort_on_code_timeout
        self.execution_backend = execution_backend
        self.local_venv_scope = local_venv_scope
        self.disallowed_modules = disallowed_modules
        self.disallowed_builtins = disallowed_builtins
        if self.execution_backend not in ("sandbox", "local"):
            raise ValueError("execution_backend must be 'sandbox' or 'local'.")
        if self.local_venv_scope not in ("instance", "rollout"):
            raise ValueError("local_venv_scope must be 'instance' or 'rollout'.")
        self._interception_bind_host = (
            "127.0.0.1" if execution_backend == "local" else "0.0.0.0"
        )
        # Server-side timeout for LLM API calls (shorter than sandbox HTTP timeout)
        # This ensures server responds before sandbox worker's HTTP request times out
        (
            self.sub_llm_api_timeout,
            self.sub_llm_timeout,
        ) = self._compute_sub_llm_timeouts()

        # Convert sub_tools to OAI format (reusing existing infrastructure)
        self.sub_oai_tools = [convert_func_to_oai_tool(tool) for tool in self.sub_tools]
        self.sub_tool_map = {
            getattr(tool, "__name__", tool.__class__.__name__): tool
            for tool in self.sub_tools
        }

        self._sandbox_paths = _build_worker_paths("/tmp")
        worker_script = _render_worker_script(self._sandbox_paths)
        worker_b64 = base64.b64encode(worker_script.encode("utf-8")).decode("utf-8")

        start_command = _RLM_START_COMMAND_TEMPLATE.format(
            command_fifo=self._sandbox_paths.command_fifo,
            response_fifo=self._sandbox_paths.response_fifo,
            ready_flag=self._sandbox_paths.ready_flag,
            install_done_flag=self._sandbox_paths.install_done_flag,
            worker_path=self._sandbox_paths.worker_path,
            worker_pid_file=self._sandbox_paths.worker_pid_file,
            worker_b64=worker_b64,
            pip_install_packages=pip_install_packages,
        )

        # Interception server state (shared across rollouts)
        self._interception_server: Any = None
        self._server_lock = asyncio.Lock()
        self._server_runner: Any = None
        self._server_site: Any = None

        # Tunnel pool for exposing interception server to sandboxes
        self._tunnel_pool: TunnelPool | None = (
            TunnelPool(port=interception_port)
            if execution_backend == "sandbox" and interception_host is None
            else None
        )

        # Active rollout tracking for sub-LLM request routing
        self.active_rollouts: dict[str, dict[str, Any]] = {}

        # Logprobs support detection (None = unknown, True/False = known)
        self._sub_llm_supports_logprobs: bool | None = None

        super().__init__(
            sandbox_name="rlm-env",
            start_command=start_command,
            max_turns=max_iterations,
            rubric=rubric,
            **kwargs,
        )
        self.add_rubric(RLMMonitorRubric())
        self._executor = (
            LocalRLMExecutor(self)
            if self.execution_backend == "local"
            else SandboxRLMExecutor(self)
        )

        # Remove bash tool from parent - we use our own REPL tool
        if hasattr(self, "tool_map") and "bash" in self.tool_map:
            self.remove_tool(self.bash)

        # Add the Python REPL tool (sandbox_id and state are injected via update_tool_args)
        self.add_tool(self.call_python_repl, args_to_skip=["sandbox_id", "state"])

    # =========================================================================
    # Sub-Agent Tool Infrastructure
    # =========================================================================

    def _compute_sub_llm_timeouts(self) -> tuple[int, int]:
        """Compute sub-LLM timeouts based on the overall code execution timeout."""
        code_timeout = max(1, int(self.code_execution_timeout))
        min_timeout = min(10, max(1, code_timeout - 1))

        api_timeout = max(min_timeout, int(code_timeout * 0.8))
        worker_timeout = max(min_timeout, int(code_timeout * 0.9))

        if code_timeout > 1:
            api_timeout = min(api_timeout, code_timeout - 1)
            worker_timeout = min(worker_timeout, code_timeout - 1)

        api_timeout = min(api_timeout, worker_timeout)

        if code_timeout < 10:
            logger.warning(
                "code_execution_timeout=%s is low; sub-LLM calls may be unreliable",
                code_timeout,
            )

        return api_timeout, worker_timeout

    def _compute_install_wait_seconds(self) -> int:
        """Estimate how long to wait for pip installs based on package count."""
        packages = [p.strip() for p in self.pip_install_packages.split() if p.strip()]
        package_count = len(packages) + 1  # Always includes requests
        estimated_seconds = 30 * package_count
        return max(self.max_startup_wait_seconds, estimated_seconds)

    def _generate_packages_documentation(self) -> str:
        """Generate documentation for installed packages to include in system prompt."""
        if not self.pip_install_packages:
            return ""

        # Parse package names from pip_install_packages string
        packages = [p.strip() for p in self.pip_install_packages.split() if p.strip()]
        if not packages:
            return ""

        lines = ["\n## Installed Packages\n"]
        lines.append(
            "The following Python packages are pre-installed in the REPL environment:\n"
        )
        for pkg in packages:
            lines.append(f"- `{pkg}`")
        lines.append("")
        lines.append("You can import and use these packages directly in your code.\n")

        return "\n".join(lines)

    def _generate_sub_tools_documentation(self) -> str:
        """Generate documentation for sub-agent tools to include in system prompt."""
        if not self.sub_tools:
            return ""

        lines = ["\n## Sub-Agent Tools\n"]
        lines.append(
            "The sub-LLMs called via `llm_batch()` have access to the following tools:\n"
        )

        for oai_tool in self.sub_oai_tools:
            func_def = oai_tool["function"]
            name = func_def["name"]
            desc = func_def.get("description", "No description")
            params = cast(
                dict[str, Any], func_def.get("parameters", {}).get("properties", {})
            )

            lines.append(f"### `{name}`")
            lines.append(f"{desc}\n")

            if params:
                lines.append("**Parameters:**")
                for param_name, param_info in params.items():
                    param_type = param_info.get("type", "any")
                    param_desc = param_info.get("description", "")
                    lines.append(f"- `{param_name}` ({param_type}): {param_desc}")
                lines.append("")

        lines.append(
            "When delegating tasks to sub-LLMs via `llm_batch()`, they can use these "
            "tools autonomously."
        )
        lines.append(
            "You do NOT need to manage tool calls yourself - just describe the task "
            "in your prompt.\n"
        )

        return "\n".join(lines)

    def _generate_metadata_documentation(self, metadata: dict[str, Any]) -> str:
        """Generate a concise summary of input data metadata for the system prompt."""
        if not metadata:
            return "No input data metadata available."

        lines = ["The environment contains the following input data in `extra_data`:"]
        for key, value in metadata.items():
            # Format key for better readability
            display_key = key.replace("_", " ").title()
            lines.append(f"- **{display_key}**: `{value}`")
        return "\n".join(lines)

    @staticmethod
    def _extract_tokens(response: Any) -> tuple[int, int]:
        """Extract prompt and completion tokens from response usage."""
        usage = getattr(response, "usage", None)
        if not usage:
            return 0, 0
        return (
            getattr(usage, "prompt_tokens", 0) or 0,
            getattr(usage, "completion_tokens", 0) or 0,
        )

    @staticmethod
    def _is_logprobs_param_error(error: Exception) -> bool:
        """Return True if the error indicates logprobs is an unsupported/invalid param."""
        error_text = str(error).lower()
        if "logprob" not in error_text:
            return False
        param_markers = (
            "not supported",
            "unsupported",
            "not allowed",
            "not permitted",
            "forbidden",
            "invalid",
            "unknown",
            "unexpected",
            "additional properties",
            "not a valid",
            "unrecognized",
            "extra fields",
            "parameter",
            "params",
            "schema",
            "403",
        )
        return any(marker in error_text for marker in param_markers)

    async def _call_sub_tool(
        self, tool_name: str, tool_args: dict, tool_call_id: str
    ) -> dict:
        """Execute a sub-agent tool call. Returns tool message dict."""
        try:
            tool_func = self.sub_tool_map[tool_name]
            result = await maybe_await(tool_func, **tool_args)
            return {
                "role": "tool",
                "content": str(result),
                "tool_call_id": tool_call_id,
            }
        except Exception as e:
            return {
                "role": "tool",
                "content": f"Error: {e}",
                "tool_call_id": tool_call_id,
            }

    def _normalize_message_content(self, messages: list[dict]) -> list[dict]:
        """Normalize message content fields to formats the API accepts.

        The API expects content to be: string, array of objects, or None.
        Handles several malformed cases:
        1. Content is a nested message dict (has 'role' and 'content' keys) - extract inner content
        2. Content is a content part object (has 'type' key) - wrap in array
        """
        normalized = []
        for msg in messages:
            msg_copy = dict(msg)
            content = msg_copy.get("content")

            if content is not None and isinstance(content, dict):
                # Check if content is a nested message dict (has 'role' and 'content' keys)
                # This happens when model passes message dicts to llm_batch instead of strings
                if "role" in content and "content" in content:
                    msg_copy["content"] = content["content"]
                elif "type" in content:
                    # Content part object (e.g. {"type": "text", "text": "..."}) - wrap in array
                    msg_copy["content"] = [content]
                else:
                    # Unknown dict structure - try wrapping in array as fallback
                    msg_copy["content"] = [content]
            normalized.append(msg_copy)
        return normalized

    async def _call_sub_llm_api(
        self, client: Any, model: str, messages: list[dict], tools: list | None = None
    ) -> Any | None:
        """Make a single sub-LLM API call with timeout. Returns None on timeout."""
        normalized_messages = self._normalize_message_content(messages)
        logprobs_support = self._sub_llm_supports_logprobs

        async def _create_call(logprobs: bool | None) -> Any:
            return await asyncio.wait_for(
                client.chat.completions.create(
                    model=model,
                    messages=normalized_messages,
                    tools=tools,
                    logprobs=logprobs,
                ),
                timeout=self.sub_llm_api_timeout,
            )

        try:
            if logprobs_support is False:
                return await _create_call(None)
            if logprobs_support is True:
                return await _create_call(True)

            # Unknown support: try logprobs=True once, then fallback on param errors.
            response = await _create_call(True)
            self._sub_llm_supports_logprobs = True
            return response
        except asyncio.TimeoutError:
            logger.warning(
                f"Sub-LLM API call timed out after {self.sub_llm_api_timeout}s"
            )
            return None
        except Exception as e:
            if logprobs_support is None and self._is_logprobs_param_error(e):
                if self._sub_llm_supports_logprobs is None:
                    self._sub_llm_supports_logprobs = False
                try:
                    return await _create_call(None)
                except asyncio.TimeoutError:
                    logger.warning(
                        f"Sub-LLM API call timed out after {self.sub_llm_api_timeout}s"
                    )
                    return None
            raise

    def _make_timeout_result(
        self,
        turns: list[SubLLMTurn],
        total_prompt_tokens: int,
        total_completion_tokens: int,
        tool_call_count: int,
        num_turns: int,
    ) -> SubLLMResult:
        """Create a SubLLMResult for timeout cases."""
        return SubLLMResult(
            final_content=f"Error: Sub-LLM API call timed out after {self.sub_llm_api_timeout}s",
            turns=turns,
            total_prompt_tokens=total_prompt_tokens,
            total_completion_tokens=total_completion_tokens,
            tool_call_count=tool_call_count,
            num_turns=num_turns,
            max_turns_reached=True,
        )

    async def _run_sub_llm(
        self, client: Any, model: str, messages: list[dict]
    ) -> SubLLMResult:
        """Run a sub-LLM call, with optional tool-calling loop."""
        # Fast path: no tools configured - single LLM call
        if not self.sub_tools:
            response = await self._call_sub_llm_api(client, model, messages)
            if response is None:
                return self._make_timeout_result([], 0, 0, 0, 0)

            prompt_tokens, completion_tokens = self._extract_tokens(response)
            return SubLLMResult(
                final_content=response.choices[0].message.content or "",
                turns=[
                    SubLLMTurn(
                        prompt_messages=[dict(m) for m in messages],
                        response=response,
                        tool_call_count=0,
                    )
                ],
                total_prompt_tokens=prompt_tokens,
                total_completion_tokens=completion_tokens,
                tool_call_count=0,
                num_turns=1,
                max_turns_reached=False,
            )

        # Tool-calling loop path
        current_messages = list(messages)
        total_prompt_tokens = 0
        total_completion_tokens = 0
        tool_call_count = 0
        num_turns = 0
        turns: list[SubLLMTurn] = []
        tools = self.sub_oai_tools if self.sub_oai_tools else None

        for _ in range(self.sub_tool_max_turns):
            num_turns += 1
            prompt_snapshot = [dict(m) for m in current_messages]

            response = await self._call_sub_llm_api(
                client, model, current_messages, tools
            )
            if response is None:
                return self._make_timeout_result(
                    turns,
                    total_prompt_tokens,
                    total_completion_tokens,
                    tool_call_count,
                    num_turns,
                )

            prompt_tokens, completion_tokens = self._extract_tokens(response)
            total_prompt_tokens += prompt_tokens
            total_completion_tokens += completion_tokens

            assistant_message = response.choices[0].message
            tool_calls = getattr(assistant_message, "tool_calls", None)
            turn_tool_count = len(tool_calls) if tool_calls else 0
            tool_call_count += turn_tool_count

            turns.append(
                SubLLMTurn(
                    prompt_messages=prompt_snapshot,
                    response=response,
                    tool_call_count=turn_tool_count,
                )
            )

            if not tool_calls:
                return SubLLMResult(
                    final_content=assistant_message.content or "",
                    turns=turns,
                    total_prompt_tokens=total_prompt_tokens,
                    total_completion_tokens=total_completion_tokens,
                    tool_call_count=tool_call_count,
                    num_turns=num_turns,
                    max_turns_reached=False,
                )

            current_messages.append(assistant_message.model_dump())

            for tool_call in tool_calls:
                tool_name = tool_call.function.name
                try:
                    tool_args = json.loads(tool_call.function.arguments)
                except json.JSONDecodeError:
                    tool_args = {}
                tool_result = await self._call_sub_tool(
                    tool_name, tool_args, tool_call.id
                )
                current_messages.append(tool_result)

        # Max turns reached - add prompt for final answer and make call without tools
        num_turns += 1
        current_messages.append(
            {
                "role": "user",
                "content": "You've reached the maximum number of tool calls. "
                "Based on the information gathered, provide your final answer inside \\boxed{}.",
            }
        )

        prompt_snapshot = [dict(m) for m in current_messages]
        response = await self._call_sub_llm_api(client, model, current_messages)
        if response is None:
            return self._make_timeout_result(
                turns,
                total_prompt_tokens,
                total_completion_tokens,
                tool_call_count,
                num_turns,
            )

        turns.append(
            SubLLMTurn(
                prompt_messages=prompt_snapshot, response=response, tool_call_count=0
            )
        )
        prompt_tokens, completion_tokens = self._extract_tokens(response)

        return SubLLMResult(
            final_content=response.choices[0].message.content or "",
            turns=turns,
            total_prompt_tokens=total_prompt_tokens + prompt_tokens,
            total_completion_tokens=total_completion_tokens + completion_tokens,
            tool_call_count=tool_call_count,
            num_turns=num_turns,
            max_turns_reached=True,
        )

    # =========================================================================
    # Interception Server (for sub-LLM calls from sandbox code)
    # =========================================================================

    async def _ensure_interception_server(self):
        """Start shared HTTP server for sub-LLM interception if needed."""
        async with self._server_lock:
            if self._interception_server is not None:
                return

            app = web.Application()
            app.router.add_post(
                "/rollout/{rollout_id}/v1/chat/completions",
                self._handle_sub_llm_request,
            )

            runner = web.AppRunner(app)
            await runner.setup()
            site = web.TCPSite(
                runner, self._interception_bind_host, self.interception_port
            )
            await site.start()

            self._interception_server = app
            self._server_runner = runner
            self._server_site = site

            if self.interception_port == 0:
                server = getattr(site, "_server", None)
                sockets = getattr(server, "sockets", None) if server else None
                if sockets:
                    self.interception_port = sockets[0].getsockname()[1]

            logger.debug(
                f"Started RLM interception server on port {self.interception_port}"
            )

    async def _handle_sub_llm_request(self, request: Any) -> Any:
        """Handle sub-LLM requests from sandbox code."""
        rollout_id = request.match_info["rollout_id"]
        context = self.active_rollouts.get(rollout_id)
        if not context:
            return web.json_response({"error": "Rollout not found"}, status=404)

        try:
            request_body = await request.json()
        except Exception as e:
            return web.json_response({"error": f"Invalid JSON: {e}"}, status=400)

        # Get client and model from rollout context
        client = context.get("client")
        sub_model = context.get("sub_model") or context.get("model")

        if not client:
            return web.json_response({"error": "Client not available"}, status=500)
        if not sub_model:
            return web.json_response({"error": "Model not available"}, status=500)

        messages = request_body.get("messages", [])
        batch_id = request_body.get("_batch_id", "")
        request_id = request_body.get("_request_id", "")

        # Prepend system message with \boxed{} instruction
        messages_with_system = [
            {"role": "system", "content": _SUB_LLM_SYSTEM_PROMPT},
            *messages,
        ]

        state_ref = context.get("state") if context else None

        try:
            # Run sub-LLM call (handles both with-tools and no-tools cases)
            result = await self._run_sub_llm(client, sub_model, messages_with_system)
            final_content = result["final_content"]
            prompt_tokens = result["total_prompt_tokens"]
            completion_tokens = result["total_completion_tokens"]
            tool_call_count = result["tool_call_count"]
            num_turns = result["num_turns"]
            max_turns_reached = result["max_turns_reached"]
            turns = result["turns"]

            # Extract boxed answer for response to sandbox
            boxed_content = extract_boxed_answer(final_content)

            parent_turn = context.get("current_turn", 0)
            timestamp = time.time()

            total_sub_turns = len(turns)
            for sub_turn_index, turn in enumerate(turns):
                extras = {
                    "is_sub_llm_call": True,
                    "parent_turn": parent_turn,
                    "batch_id": batch_id,
                    "request_id": request_id,
                    "sub_turn_index": sub_turn_index,
                    "total_sub_turns": total_sub_turns,
                    "timestamp": timestamp,
                    "tool_call_count": turn["tool_call_count"],
                }

                if self.include_sub_llm_in_trajectory:
                    # Parse tokens from response
                    tokens = await parse_response_tokens(
                        turn["response"], "chat", self.max_seq_len
                    )
                    # Parse completion messages
                    completion_messages = await parse_response_messages(
                        turn["response"], "chat"
                    )
                    # Check if response was truncated
                    response_is_truncated = await parse_is_truncated(
                        turn["response"], "chat"
                    )
                    is_truncated = response_is_truncated or (
                        tokens is not None and bool(tokens.get("is_truncated"))
                    )

                    trajectory_step = TrajectoryStep(
                        prompt=cast(Messages, turn["prompt_messages"]),
                        completion=completion_messages,
                        response=turn["response"],
                        tokens=tokens,
                        reward=None,
                        advantage=None,
                        is_truncated=is_truncated,
                        trajectory_id=f"{batch_id}_{request_id}",
                        extras=extras,
                    )
                    if state_ref is not None:
                        await self.add_trajectory_step(state_ref, trajectory_step)
                else:
                    if state_ref is None:
                        continue
                    trajectory_step = TrajectoryStep(
                        prompt=cast(Messages, turn["prompt_messages"]),
                        completion=[],
                        response=turn["response"],
                        tokens=None,
                        reward=None,
                        advantage=None,
                        is_truncated=False,
                        trajectory_id=f"{batch_id}_{request_id}",
                        extras=extras,
                    )
                    update_rlm_metrics_from_step(state_ref, trajectory_step)

            # Build response dict for sandbox
            response_dict = {
                "choices": [{"message": {"content": boxed_content}}],
                "_rlm_metadata": {
                    "prompt_tokens": prompt_tokens,
                    "completion_tokens": completion_tokens,
                    "tool_call_count": tool_call_count,
                    "num_turns": num_turns,
                    "max_turns_reached": max_turns_reached,
                },
            }

            return web.json_response(response_dict)
        except Exception as e:
            logger.error(f"Sub-LLM call failed: {e}")
            return web.json_response({"error": str(e)}, status=500)

    @vf.teardown
    async def teardown_tunnels(self):
        """Stop all cloudflared tunnel processes."""
        if self._tunnel_pool:
            self._tunnel_pool.teardown()

    async def _teardown_interception_server(self):
        """Stop the interception server if it was started."""
        async with self._server_lock:
            if self._server_site is not None:
                try:
                    await self._server_site.stop()
                finally:
                    self._server_site = None
            if self._server_runner is not None:
                try:
                    await self._server_runner.cleanup()
                finally:
                    self._server_runner = None
                    self._interception_server = None

    @vf.teardown
    async def teardown_interception_server(self):
        """Stop the interception server if it was started."""
        await self._teardown_interception_server()

    @vf.teardown
    async def teardown_executor(self):
        """Cleanup executor-level resources (e.g., local venv)."""
        await self._executor.teardown()

    # =========================================================================
    # State Management
    # =========================================================================

    def update_tool_args(
        self,
        tool_name: str,
        tool_args: dict[str, Any],
        messages: Messages,
        state: State,
        **kwargs,
    ) -> dict[str, Any]:
        """Inject sandbox_id and state into call_python_repl tool args."""
        if tool_name == "call_python_repl":
            updated_args = dict(tool_args)
            updated_args["sandbox_id"] = state.get("sandbox_id")
            updated_args["state"] = state
            return updated_args
        else:
            return super().update_tool_args(
                tool_name, tool_args, messages, state, **kwargs
            )

    async def _setup_interception_and_register(
        self, state: State, rollout_id: str
    ) -> State:
        """Start interception server, configure tunnel, and register rollout."""
        await self._ensure_interception_server()

        if self._tunnel_pool:
            tunnel_url = await self._tunnel_pool.get_tunnel_url(
                len(self.active_rollouts)
            )
            interception_url = f"{tunnel_url}/rollout/{rollout_id}/v1/chat/completions"
        else:
            tunnel_url = None
            interception_url = f"http://{self.interception_host}:{self.interception_port}/rollout/{rollout_id}/v1/chat/completions"

        state["interception_url"] = interception_url
        state["tunnel_url"] = tunnel_url

        self.active_rollouts[rollout_id] = {
            "client": state.get("client"),
            "model": state.get("model"),
            "sub_model": self.sub_model or state.get("model"),
            "state": state,
        }
        return state

    async def setup_state(self, state: State, **kwargs) -> State:
        """Setup sandbox with context and worker, plus interception for sub-LLM calls."""
        # 1. Create sandbox via parent (sandbox backend only)
        if self.execution_backend == "sandbox":
            state = await super().setup_state(state, **kwargs)
            sandbox_id = state.get("sandbox_id")
            if not sandbox_id:
                raise vf.SandboxError() from Exception("Sandbox ID not set")
        else:
            state = await vf.StatefulToolEnv.setup_state(self, state, **kwargs)
            state["sandbox_state"] = {
                "ready": False,
                "ready_wait_time": 0.0,
                "command_execution_times": [],
            }

        rollout_id = f"rlm_{uuid.uuid4().hex[:8]}"
        state["rollout_id"] = rollout_id

        # 2. Setup interception, tunnels, and register rollout
        state = await self._setup_interception_and_register(state, rollout_id)

        # 3. Build context
        info = state.get("info", {})
        context_data = info.get(self.context_key, None)
        disk_size_gb = getattr(self.sandbox_request, "disk_size_gb", None)
        max_payload_bytes = None
        if isinstance(disk_size_gb, (int, float)) and disk_size_gb > 0:
            max_payload_bytes = int(disk_size_gb * 1024**3)

        payload_dir = await self._executor.get_payload_dir(state)
        prepared_context = prepare_context_data(
            context_data,
            self.context_dtype,
            self.serializer_registry,
            max_payload_bytes,
            payload_dir=payload_dir,
        )
        context_dict = prepared_context.context_dict
        state["rlm_context"] = context_dict
        state["rlm_payload_bytes"] = prepared_context.payload_bytes
        state["rlm_payload_path"] = prepared_context.payload_path
        state["rlm_payload_name"] = prepared_context.payload_name

        metadata = context_dict.get("input_data_metadata", {})
        metadata_summary = self._generate_metadata_documentation(metadata)
        base_system_prompt = self.custom_system_prompt or _RLM_SYSTEM_PROMPT
        if "{metadata_summary}" in base_system_prompt:
            # Use replace instead of format to avoid conflict with curly braces from Python code
            base_system_prompt = base_system_prompt.replace(
                "{metadata_summary}", metadata_summary
            )
        else:
            # If custom prompt doesn't have placeholder, prepend it
            base_system_prompt = f"{metadata_summary}\n\n{base_system_prompt}"

        packages_docs = self._generate_packages_documentation()
        sub_tools_docs = self._generate_sub_tools_documentation()
        state["rlm_system_prompt"] = base_system_prompt + packages_docs + sub_tools_docs
        state["rlm_packages_docs"] = packages_docs
        state["rlm_sub_tools_docs"] = sub_tools_docs

        # 4. Prepare backend and start worker
        await self._executor.setup(state, prepared_context)

        state["rlm_worker_ready"] = True

        # Initialize context warning flag (feature enabled if max_seq_len is set)
        state["context_warning_sent"] = False

        # Initialize FIFO sequence counter for detecting stale responses
        state["_exec_seq"] = 0

        _ensure_rlm_metric_state(state)

        return state

    # =========================================================================
    # Code Execution
    # =========================================================================

    async def _recover_from_code_timeout(self, state: State) -> bool:
        """Attempt to recover from a code execution timeout via the active backend."""
        return await self._executor.recover_from_timeout(state)

    async def _execute_code(
        self, sandbox_id: str | None, code: str, state: State
    ) -> dict[str, Any]:
        """Execute code in worker and return result."""
        # Increment and track sequence number for this execution
        seq = state.get("_exec_seq", 0) + 1
        state["_exec_seq"] = seq

        payload = {"code": code, "seq": seq}
        try:
            result = await self._executor.execute(payload, state)
        except RLMCodeExecutionTimeout as e:
            logger.warning(
                "Code execution timed out after %ss", self.code_execution_timeout
            )
            if self.abort_on_code_timeout:
                # Abort rollout immediately on timeout
                raise vf.SandboxError() from e
            recovered = await self._recover_from_code_timeout(state)
            recovery_note = (
                " The sandbox was restarted and the REPL state was reset."
                if recovered
                else " Failed to restart the sandbox; the REPL may be unusable."
            )
            # Return error to model so it can try more efficient code
            return {
                "status": "error",
                "stdout": "",
                "stderr": "",
                "result": (
                    f"Code execution timed out after {self.code_execution_timeout} seconds."
                    f"{recovery_note} Your code may be too slow - consider a more "
                    "efficient algorithm or breaking the computation into smaller steps."
                ),
                "answer": {"ready": False, "content": ""},
            }

        if not result.stdout:
            return {
                "status": "error",
                "stdout": "",
                "stderr": result.stderr or "",
                "result": "Worker returned no output",
                "answer": {"ready": False, "content": ""},
            }

        try:
            parsed_result = json.loads(result.stdout)
        except json.JSONDecodeError as e:
            return {
                "status": "error",
                "stdout": result.stdout,
                "stderr": result.stderr or "",
                "result": f"Failed to parse worker response: {e}",
                "answer": {"ready": False, "content": ""},
            }

        # Check sequence number to detect stale responses (FIFO desync)
        response_seq = parsed_result.get("seq", -1)
        if response_seq != seq:
            logger.warning(
                f"FIFO sequence mismatch: expected seq={seq}, got seq={response_seq}. "
                "This indicates a desync - likely from a previous timeout."
            )
            return {
                "status": "error",
                "stdout": "",
                "stderr": "",
                "result": (
                    f"Communication desync detected: received stale response "
                    f"(expected seq={seq}, got seq={response_seq}). "
                    "This may happen after a timeout. Please retry your command."
                ),
                "answer": {"ready": False, "content": ""},
            }

        return parsed_result

    def _format_execution_output(self, result: dict[str, Any]) -> str:
        """Format execution result for display to model."""
        parts: list[str] = []

        stdout = (result.get("stdout") or "").rstrip()
        if stdout:
            parts.append(stdout)

        stderr = (result.get("stderr") or "").rstrip()
        if stderr:
            parts.append(f"stderr:\n{stderr}")

        status = result.get("status")
        result_text = result.get("result")
        execution_count = result.get("execution_count", 0)

        if status == "error" and result_text:
            parts.append(result_text.rstrip())
        elif status == "ok" and result_text is not None:
            parts.append(f"Out[{execution_count}]: {result_text}")

        output = "\n".join(parts) if parts else "(no output)"

        # Truncate if too long
        if len(output) > self.max_output_length:
            output = output[: self.max_output_length] + "\n... [output truncated]"

        return output

    # =========================================================================
    # REPL Tool
    # =========================================================================

    async def call_python_repl(
        self, code: str, sandbox_id: str | None, state: Any
    ) -> str:
        """
        Execute Python code in a persistent REPL environment.

        The REPL maintains state across calls and provides access to:

        - `extra_data`: The actual input data you need to process.

        - `answer`: A dictionary for your final answer:
          - `answer["content"]`: Your answer (string) - update this as you work
          - `answer["ready"]`: Set to `True` to finish (terminates execution immediately)

        - `llm_batch(prompts, **kwargs)`: Make sub-LLM calls for help with subtasks
          - Takes a list of prompts, returns a list of answers (same order)
          - Useful for semantic understanding, summarization, complex reasoning
          - Prints metadata summary showing tokens and tool calls per sub-LLM

        Args:
            code: Python code to execute in the persistent REPL

        Returns:
            Execution output including stdout, stderr, and expression results
        """
        # Update current turn in rollout context for sub-LLM call tracking
        rollout_id = state.get("rollout_id")
        if rollout_id and rollout_id in self.active_rollouts:
            self.active_rollouts[rollout_id]["current_turn"] = state.get("turn", 0)

        # Time the full tool call execution
        execution_start = perf_counter()
        result = await self._execute_code(sandbox_id, code, state)
        execution_time = perf_counter() - execution_start
        output = self._format_execution_output(result)

        # Track timing in state for metrics
        state.setdefault("tool_call_timings", []).append(
            {
                "turn": state.get("turn", 0),
                "execution_seconds": execution_time,
            }
        )
        _update_rlm_repl_metrics(state, execution_time)

        # Append execution time to output
        output += f"\n[Execution time: {execution_time:.2f}s]"

        # Check if answer is ready
        answer = result.get("answer", {})
        if answer.get("ready", False):
            state["final_answer"] = answer.get("content", "")
            logger.debug(f"Answer ready: {state['final_answer'][:100]}...")

        # Inject context limit warning if approaching limit
        if self.max_seq_len and not state.get("context_warning_sent"):
            # Get prompt token count from latest main-model trajectory response
            trajectory = state.get("trajectory", [])
            last_main = next(
                (
                    step
                    for step in reversed(trajectory)
                    if not step.get("extras", {}).get("is_sub_llm_call")
                ),
                None,
            )
            response = last_main.get("response") if last_main else None
            usage = getattr(response, "usage", None) if response else None
            prompt_tokens = getattr(usage, "prompt_tokens", 0) or 0 if usage else 0
            warning_threshold = int(self.max_seq_len * self.context_warning_threshold)

            if prompt_tokens >= warning_threshold:
                state["context_warning_sent"] = True
                pct = prompt_tokens / self.max_seq_len
                output += (
                    f"\n\n[CONTEXT LIMIT WARNING] You have used {prompt_tokens:,} of "
                    f"{self.max_seq_len:,} tokens ({pct:.0%}). Please finalize your answer "
                    "soon by setting answer['ready'] = True."
                )

        return output

    async def add_trajectory_step(self, state: State, trajectory_step: TrajectoryStep):
        update_rlm_metrics_from_step(state, trajectory_step)
        await super().add_trajectory_step(state, trajectory_step)

    # =========================================================================
    # MultiTurnEnv Interface
    # =========================================================================

    async def get_prompt_messages(self, state: State) -> Messages:
        """Build prompt messages, adding system prompt with tool docs on first turn."""
        if len(state["trajectory"]) == 0:
            # First turn: add system prompt
            prompt = state.get("prompt", [])
            if isinstance(prompt, str):
                prompt = [{"role": "user", "content": prompt}]

            system_prompt = state.get("rlm_system_prompt")
            packages_docs = state.get("rlm_packages_docs")
            sub_tools_docs = state.get("rlm_sub_tools_docs")
            if system_prompt is None or packages_docs is None or sub_tools_docs is None:
                raise ValueError("RLM setup_state must run before get_prompt_messages")

            messages = list(prompt)
            if not messages or messages[0].get("role") != "system":
                messages.insert(0, {"role": "system", "content": system_prompt})
            else:
                # Append packages and tool docs to existing system prompt
                messages[0] = {
                    "role": "system",
                    "content": messages[0]["content"] + packages_docs + sub_tools_docs,
                }
            return cast(Messages, messages)
        else:
            # Subsequent turns: use parent implementation
            return await super().get_prompt_messages(state)

    # =========================================================================
    # Stop Conditions
    # =========================================================================

    async def _ensure_final_answer(self, state: State) -> None:
        """Read final answer from sandbox if not already set."""
        if "final_answer" in state:
            return
        state["final_answer"] = await self._executor.read_answer(state)

    @vf.stop
    async def answer_ready(self, state: State) -> bool:
        """Stop when model sets answer['ready'] = True."""
        return "final_answer" in state

    @vf.stop
    async def prompt_too_long(self, state: State) -> bool:
        """Stop when API returns overlong prompt error."""
        if not state.get("prompt_too_long", False):
            return False

        await self._ensure_final_answer(state)
        return True

    # =========================================================================
    # Cleanup
    # =========================================================================

    @vf.cleanup
    async def cleanup_rlm_state(self, state: State):
        """Cleanup RLM-specific state and prepend sub-LLM trajectory steps."""
        rollout_id = state.get("rollout_id")

        if rollout_id and rollout_id in self.active_rollouts:
            del self.active_rollouts[rollout_id]

        # Release tunnel
        if (tunnel_url := state.get("tunnel_url")) and self._tunnel_pool:
            await self._tunnel_pool.release_tunnel(tunnel_url)

        try:
            await self._executor.cleanup(state)
        finally:
            if not self.active_rollouts:
                await self._teardown_interception_server()
                if self._tunnel_pool:
                    self._tunnel_pool.teardown()

    async def render_completion(self, state: State):
        """Render completion from main model steps only, ignoring sub-LLM steps."""

        if len(state["trajectory"]) == 0:
            state["completion"] = []
            return

        # Find the last trajectory step from the main model (matching trajectory_id)
        main_trajectory_id = state["trajectory_id"]
        last_main_step = None
        for step in reversed(state["trajectory"]):
            if step.get("trajectory_id") == main_trajectory_id:
                last_main_step = step
                break

        if last_main_step is None:
            state["completion"] = []
            return

        last_prompt = last_main_step["prompt"]
        last_completion = last_main_step["completion"]
        full_conversation = concat_messages([last_prompt, last_completion])
        if state.get("final_env_response"):
            full_conversation = concat_messages(
                [full_conversation, state["final_env_response"]]
            )
        state["completion"] = full_conversation[len(state["prompt"]) :]

    async def post_rollout(self, state: State):
        """Read final answer from sandbox if not already set."""
        await self._ensure_final_answer(state)
