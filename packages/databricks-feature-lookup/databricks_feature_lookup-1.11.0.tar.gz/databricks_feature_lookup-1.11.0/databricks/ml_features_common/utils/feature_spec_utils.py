import logging
import os
from dataclasses import dataclass
from functools import reduce
from typing import Dict, List, Set, Tuple, Type, Union

import mlflow
from mlflow.utils.file_utils import TempDir

from databricks.ml_features_common.entities.column_info import ColumnInfo
from databricks.ml_features_common.entities.feature_column_info import FeatureColumnInfo
from databricks.ml_features_common.entities.feature_spec import (
    FeatureSpec,
    _FeatureSpecBase,
)
from databricks.ml_features_common.entities.feature_spec_constants import (
    SERIALIZATION_VERSION,
)
from databricks.ml_features_common.entities.on_demand_column_info import (
    OnDemandColumnInfo,
)
from databricks.ml_features_common.entities.source_data_column_info import (
    SourceDataColumnInfo,
)
from databricks.ml_features_common.utils.topological_sort import topological_sort
from databricks.ml_features_common.utils.utils_common import is_artifact_uri
from databricks.ml_features_common.utils.yaml_utils import SAFE_DUMPER, read_yaml
from databricks.sdk import WorkspaceClient

# The backend enforces the actual limit (controlled by a flag).
# The client also has a leftover limit check from old code, which is a bug.
# Although the client code is not used for DAG generation, it incorrectly throws an error
# if the DAG exceeds the client-side max width. This blocks customers who need larger DAGs.
# As a temporary fix, we raise the client-side limit to a high value until the code is cleaned up.
# TODO[ML-57036]: clean up client side yaml generation.
DEFAULT_GRAPH_DEPTH_LIMIT = 1000

COLUMN_INFO_TYPE_SOURCE = "SOURCE"
COLUMN_INFO_TYPE_ON_DEMAND = "ON_DEMAND"
COLUMN_INFO_TYPE_FEATURE = "FEATURE"

_logger = logging.getLogger(__name__)


@dataclass
class FeatureExecutionGroup:
    type: str  # could be FEATURE, ON_DEMAND, SOURCE
    features: Union[
        List[FeatureColumnInfo], List[OnDemandColumnInfo], List[SourceDataColumnInfo]
    ]


# Small number has high priority. Besides SOURCE, preferring FEATURE over ON_DEMAND in topological
# sorting to make sure ON_DEMAND columns after FEATURE in simple cases to align with previous
# assumption before implementing TLT.
# NOTE: changing this priority may cause performance regression, proceed with caution.
COLUMN_TYPE_PRIORITY = {
    COLUMN_INFO_TYPE_SOURCE: 0,
    COLUMN_INFO_TYPE_ON_DEMAND: 1,
    COLUMN_INFO_TYPE_FEATURE: 2,
}


class _GraphNode:
    def __init__(self, column_info: ColumnInfo):
        info = column_info.info
        self.column_info = column_info
        self.output_name = info.output_name

        if isinstance(column_info.info, SourceDataColumnInfo):
            self.input_names = set()
            self.type = COLUMN_INFO_TYPE_SOURCE
        elif isinstance(column_info.info, FeatureColumnInfo):
            self.input_names = set(info.lookup_key)
            self.type = COLUMN_INFO_TYPE_FEATURE
        elif isinstance(column_info.info, OnDemandColumnInfo):
            self.input_names = set(info.input_bindings.values())
            self.type = COLUMN_INFO_TYPE_ON_DEMAND
        else:
            raise ValueError("unknown column info type")

    def __str__(self):
        return "node<" + self.output_name + ">"

    def __repr__(self):
        return str(self)


def _column_info_sort_key(node: _GraphNode) -> Tuple[int, str]:
    """
    Returns a tuple of an int and a str as the sorting key for _GraphNode. Priority is determined by
    the first element and then use the second element to break ties.
    """
    return COLUMN_TYPE_PRIORITY[node.type], node.output_name


def _should_be_grouped(node: _GraphNode) -> bool:
    """
    Returns True if the given node is of type that should be grouped together as much as possible.
    """
    return node.type == COLUMN_INFO_TYPE_FEATURE


def _validate_graph_depth(nodes: List[_GraphNode], depth_limit: int):
    name_to_node = {node.output_name: node for node in nodes}
    visited_depth = {}

    def dfs(node: _GraphNode, depth: int):
        if depth > depth_limit:
            raise ValueError(
                f"The given graph contains a dependency path longer than the limit {depth_limit}"
            )
        if (
            node.output_name in visited_depth
            and depth <= visited_depth[node.output_name]
        ):
            return
        visited_depth[node.output_name] = depth
        for column_name in node.input_names:
            dependency = name_to_node[column_name]
            dfs(dependency, depth + 1)

    for node in nodes:
        dfs(node, 1)


def get_encoded_graph_map(column_infos: List[ColumnInfo]) -> Dict[str, List[str]]:
    """
    Creates a dictionary of columns with their dependency columns for metric use. Columns are
    encoded with a string representing the type and index. For example:
      {
        "f3": ["s1", "s2"],
        "o4": ["f3"],
        "o5": []
      }
    "s1" and "s2" are SourceColumnInfos, "f3" is FeatureColumnInfo and "o4", "o5" are
    OnDemandColumnInfos. "f3" depends on "s1" and "s2", "o5" doesn't depend on any column, etc.
    :param column_infos: A list of ColumnInfos.
    """
    nodes = {info.output_name: _GraphNode(info) for info in column_infos}
    next_node_index = 0
    # A map from column info's output_name to its label.
    node_label = {}

    def get_node_label(node):
        nonlocal next_node_index
        output_name = node.output_name
        if output_name not in node_label:
            if node.type == COLUMN_INFO_TYPE_SOURCE:
                type_simple_str = "s"
            if node.type == COLUMN_INFO_TYPE_FEATURE:
                type_simple_str = "f"
            if node.type == COLUMN_INFO_TYPE_ON_DEMAND:
                type_simple_str = "o"
            new_label = type_simple_str + str(next_node_index)
            next_node_index += 1
            node_label[output_name] = new_label
        return node_label[output_name]

    graph_map = {}
    for node in nodes.values():
        label = get_node_label(node)
        dependencies = []
        for dep_name in sorted(node.input_names):
            if dep_name not in nodes:
                # skip the column if it's not in the feature spec.
                continue
            dep = get_node_label(nodes[dep_name])
            dependencies.append(dep)
        graph_map[label] = dependencies
    return graph_map


def assign_topological_ordering(
    column_infos: List[ColumnInfo],
    allow_missing_source_columns=False,
    graph_depth_limit=DEFAULT_GRAPH_DEPTH_LIMIT,
) -> List[ColumnInfo]:
    """
    Assigns the topological ordering for each ColumnInfo of the input. Returns a list of new
    ColumnInfo objects with topological_ordering set to an integer.

    :param column_infos: a list of ColumnInfos.
    :param allow_missing_source_columns: ONLY USED BY FSE TEMPORARILY. Allow lookup key or
    function input be missing from source columns. If true, this method will assign
    topological_ordering to columns as if the missing sources are added in the column_infos.
    :param graph_depth_limit raises if the given graph exceed the limit.
    :raises ValueError if there is a cycle in the graph.
    """
    nodes = list(map(lambda c: _GraphNode(c), column_infos))
    # allow_missing_source_columns is used when feature_serving_endpoint_client creates training
    # sets. It doesn't include source columns in the dataframe.
    # TODO[ML-33809]: clean up allow_missing_source_columns.
    all_output_names = set([n.output_name for n in nodes])
    all_input_names = reduce(lambda a, b: a | b, [n.input_names for n in nodes])
    missing_inputs = all_input_names - all_output_names
    if allow_missing_source_columns:
        for input_name in missing_inputs:
            if input_name not in all_output_names:
                nodes.append(
                    _GraphNode(ColumnInfo(SourceDataColumnInfo(input_name), False))
                )
    elif len(missing_inputs) > 0:
        missing_input_names_str = ", ".join(
            [f"'{name}'" for name in sorted(missing_inputs)]
        )
        raise ValueError(
            f"Input columns {missing_input_names_str} required by FeatureLookups or "
            "FeatureFunctions are not provided by input DataFrame or other FeatureFunctions and "
            "FeatureLookups"
        )
    output_name_to_node = {node.output_name: node for node in nodes}
    graph = {
        node: [output_name_to_node[input_name] for input_name in node.input_names]
        for node in nodes
    }
    sorted_nodes = topological_sort(graph, _column_info_sort_key, _should_be_grouped)
    # validate depth after sorting the graph because cycle is detected during sorting.
    _validate_graph_depth(nodes, graph_depth_limit)
    name_to_ordering = {node.output_name: i for i, node in enumerate(sorted_nodes)}
    return [
        column.with_topological_ordering(name_to_ordering[column.output_name])
        for column in column_infos
    ]


def get_feature_execution_groups(
    feature_spec: FeatureSpec, df_columns: List[str] = []
) -> List[FeatureExecutionGroup]:
    """
    Splits the list of column_infos in feature_spec into groups based on the topological_ordering of
    the column_infos such that each group contains only one type of feature columns and columns
    don't depend on other columns in the same group. The type of feature column is equivalent to the
    class type of column_info.info field.
    Example:
        Given FeatureSpec with some columns, after sorting the columns by topological_ordering,
        assuming the sorted list:
            [source_1, feature_2, feature_3, on_demand_4, on_demand_5]
        where feature_2 depends on feature_3. The resulting groups will be:
            [
                group(SOURCE, [source_1]),
                group(FEATURE, [feature_2]),
                group(FEATURE, [feature_3]),
                group(ON_DEMAND, [on_demand_4, on_demand_5]),
            ]

    :param feature_spec: A FeatureSpec with topologically sorted column_infos.
    :param df_columns: the columns from the DF used to create_training_set or score_batch.
    """
    # convert column infos into _GraphNode
    nodes = list(map(lambda c: _GraphNode(c), feature_spec.column_infos))
    if any(info.topological_ordering is None for info in feature_spec.column_infos):
        # The old version of feature_spec may not have topological_ordering, we can safely assume
        # they are already sorted because of validations during the feature_spec creation.
        _logger.warning(
            "Processing a feature spec that at least one of the column_infos has no "
            "topological_ordering"
        )
    else:
        # sort nodes by topological_ordering
        nodes = sorted(nodes, key=lambda n: n.column_info.topological_ordering)
    # A buffer holding the columns in a group.
    buffer = []
    # output names of columns in the current buffer.
    buffered_output_names = set()
    # Used to validate the topological sorting.
    # df_columns is used to be backward compatible. In old FeatureSpecs, source columns might not
    # exist. So we need to consider the df as initial resolved columns.
    resolved_columns = set(df_columns)
    result_list = []
    last_type = None
    for node in nodes:
        if not node.input_names.issubset(resolved_columns):
            raise ValueError(
                "The column_infos in the FeatureSpec is not topologically sorted"
            )
        if node.type != last_type or buffered_output_names.intersection(
            node.input_names
        ):
            # split group if the current node has a different type from the previous node OR
            # any of the inputs are from the nodes in the current group.
            if buffer:
                result_list.append(FeatureExecutionGroup(last_type, buffer))
                buffer = []
            buffered_output_names.clear()
            last_type = node.type
        buffer.append(node.column_info.info)
        resolved_columns.add(node.output_name)
        buffered_output_names.add(node.output_name)
    if buffer:
        result_list.append(FeatureExecutionGroup(last_type, buffer))
    return result_list


# TODO[ML-59822]: move Feature into feature-store-common and properly import the classes here.
def inflate_feature_spec_declarative(
    serializable_spec: "_SerializableFeatureSpecDeclarative",
    workspace_client: WorkspaceClient,
) -> "FeatureSpecDeclarative":
    """
    Inflates a serializable feature spec declarative by fetching full feature definitions from the workspace.

    :param serializable_spec: A _SerializableFeatureSpecDeclarative object containing the spec info.
    :param workspace_client: WorkspaceClient to use for fetching feature definitions.
    :return: A FeatureSpecDeclarative with full feature definitions.
    """
    # Importing here to avoid circular dependencies and lookup-client compatibility issues
    from databricks.ml_features.entities.feature import Feature
    from databricks.ml_features_common.entities.feature_spec_declarative import (
        FeatureSpecDeclarative,
    )

    features = []
    for feature_name in serializable_spec._feature_names:
        feature = workspace_client.feature_engineering.get_feature(feature_name)
        features.append(Feature._from_sdk_feature(feature))

    return FeatureSpecDeclarative(
        features=features,
        column_infos=serializable_spec._input_columns,
        workspace_id=serializable_spec._workspace_id,
        feature_store_client_version=serializable_spec._feature_store_client_version,
        serialization_version=serializable_spec.serialization_version,
    )


def load_feature_spec(
    path: str, workspace_client: WorkspaceClient
) -> Union[FeatureSpec, "FeatureSpecDeclarative"]:
    """
    Loads a FeatureSpec from a YAML file.
    """
    # Importing FeatureSpecDeclarative here instead of the top of because FeatureSpecDeclarative
    # cannot be imported in the lookup-client.
    from databricks.ml_features_common.entities.feature_spec_declarative import (
        FeatureSpecDeclarative,
        _SerializableFeatureSpecDeclarative,
    )

    # Create the full file path to the FeatureSpec.
    path = os.path.join(path, _FeatureSpecBase.FEATURE_ARTIFACT_FILE)

    if is_artifact_uri(path):
        with TempDir() as tmp_location:
            # Returns a file and not directory since the artifact_uri is a single file.
            local_path = mlflow.artifacts.download_artifacts(
                artifact_uri=path, dst_path=tmp_location.path()
            )
            parent_dir, file = os.path.split(path)
            yaml_dict = read_yaml(parent_dir, file)
    else:
        parent_dir, file = os.path.split(path)
        yaml_dict = read_yaml(path)

    # Default to version 1 if the serialization_version is not present.
    serialization_version = yaml_dict.get(SERIALIZATION_VERSION, 1)

    # Parse the yaml_dict based on the version number
    if serialization_version >= 11 and "features" in yaml_dict:
        # Version 11 contains "features" as a list of full qualified feature names which shold be
        # parsed as a declarative features and inflated.
        # If "features" is not present, it's a basic FeatureSpec and should fallback to the legacy logic.
        serializable_feature_spec_declarative = (
            _SerializableFeatureSpecDeclarative._from_dict(yaml_dict)
        )
        return inflate_feature_spec_declarative(
            serializable_spec=serializable_feature_spec_declarative,
            workspace_client=workspace_client,
        )
    elif (
        serialization_version >= 10
        and "input_columns" in yaml_dict
        and "features" in yaml_dict
    ):
        # Version 10 contains input_columns and features fields and "features" field
        # contains the whole feature definition so it doesn't need to be inflated.
        # This is a version only used in Declarative Features PrPr and should be remove soon.
        _logger.warning(
            "The model version was logged with a legacy version of FeatureEngineeringClient. "
            "Upgrade it by logging a new version of the model with the latest "
            "FeatureEngineeringClient to avoid unexpected behavior."
        )
        return FeatureSpecDeclarative._from_dict(yaml_dict)
    else:
        # Fallback to the basic FeatureSpec
        return FeatureSpec._from_dict(yaml_dict)
