import json
import re
from sys import exit as sys_exit
import xlsxwriter
from dateutil import parser
import time

from gitlab_evaluate.lib import utils
from gitlab_evaluate.migration_readiness.ado.evaluate import AdoEvaluateClient
from gitlab_evaluate.migration_readiness.ado.feed_worker import process_feed_packages
from gitlab_ps_utils.processes import MultiProcessing


class AdoReportGenerator:
    def __init__(self, host, token, filename, output_to_screen=False, project=None, processes=None, api_version=None, skip_details=False, verify=True, max_commits=None):
        self.skip_details = skip_details
        self.max_commits = max_commits
        self.total_repositories = 0
        self.total_disabled_repositories = 0
        self.total_uninitialized_repositories = 0
        self.total_tfvc_repositories = 0
        self.total_projects = 0
        self.total_users = 0
        self.total_agent_pools = 0
        self.total_feeds = 0
        self.total_variables = 0
        self.total_wikis = 0
        self.total_packages = 0
        self.total_pull_requests = 0
        self.total_work_items = 0
        self.total_pipelines = 0
        self.packages_by_type = {}
        self.total_packages_to_migrate = 0  # Packages not cached from upstream (npmjs, maven central, etc.)
        self.packages_to_migrate_by_type = {}
        self.host = host
        # Store connection params for multiprocessing workers
        self._token = token
        self._api_version = api_version
        self._verify = verify
        self.ado_client = AdoEvaluateClient(host, token, api_version=api_version, verify=verify)
        self.validate_token()
        self.workbook = xlsxwriter.Workbook(filename)
        self.app_stats = self.workbook.add_worksheet('Organization Insights')
        self.align_left = self.workbook.add_format({'align': 'left'})
        self.header_format = self.workbook.add_format({'bg_color': 'black', 'font_color': 'white', 'bold': True, 'font_size': 10})
        self.users = self.workbook.add_worksheet('Users')
        self.agent_pools = self.workbook.add_worksheet('Agent Pools')
        self.variable_groups = self.workbook.add_worksheet('Variable Groups')
        self.projects = self.workbook.add_worksheet('Raw Project Data')
        self.pipelines = self.workbook.add_worksheet('Pipelines')
        self.raw_output = self.workbook.add_worksheet('Raw Repository Data')
        self.feeds = self.workbook.add_worksheet('Feeds')
        self.output_to_screen = output_to_screen
        self.processes = processes
        self.project = project
        self.columns = [
            'Project Name',
            'Project ID',
            'Repository Name',
            'Repository ID',
            'Default Branch',
            'Git URL',
            'Last activity',
            'Branches',
            'Commits',
            'Pull Requests',
            'Repository Size in MB',
            'Repository Disabled'
        ]
        self.user_headers = [
            'Object ID',
            'Descriptor',
            'Display Name',
            'Principal Name',
            'Email'
        ]
        self.agent_pool_headers = [
            'Pool ID',
            'Name',
            'Is Hosted',
            'Pool Size',
            'Legacy',
            'Owner'
        ]
        self.variables_groups_headers = [
            'Project ID',
            'Variable Group ID',
            'Variable Group Name',
            'Variable Group Description',
            'Type',
            'Variable Name',
            'Enabled',
            'Is Secret',
            'Created By',
            'Created on',
            'Modified on'
        ]
        self.project_headers = [
            'Project ID',
            'URL',
            'Name',
            'Total Repositories',
            'Total Build Definitions (Classic)',
            'Total Build Definitions (Multi-stage YAML)',
            'Total Release Definitions',
            'Total Work Items',
            'Administrators',
            'Project Users',
            'TFVC',
            'Wiki'
        ]
        
        self.pipeline_headers = [
            'Project ID',
            'Repository ID',
            'Pipeline ID',
            'Pipeline Name',
            'Pipeline Type',
            'Pipeline URL',
        ]
        
        self.feeds_headers = [
            'Scope',
            'Project ID',
            'Project Name',
            'Feed ID',
            'Feed Name',
            'Package ID',
            'Package Name',
            'Package Type',
            'Version',
            'isLatest',
            'isListed',
            'Total Downloads',
            'Publish date',
            'Last downloaded',
            'Upstream Source ID',
            'Upstream Source URL',
            'Upstream Source Type'
        ]
        
        utils.write_headers(0, self.raw_output, self.columns, self.header_format)
        utils.write_headers(0, self.users, self.user_headers, self.header_format)
        utils.write_headers(0, self.agent_pools, self.agent_pool_headers, self.header_format)
        utils.write_headers(0, self.variable_groups, self.variables_groups_headers, self.header_format)
        utils.write_headers(0, self.projects, self.project_headers, self.header_format)
        utils.write_headers(0, self.pipelines, self.pipeline_headers, self.header_format)
        utils.write_headers(0, self.feeds, self.feeds_headers, self.header_format)
        self.multi = MultiProcessing()

    def write_workbook(self):
        self.app_stats.autofit()
        self.raw_output.autofit()
        self.users.autofit()
        self.projects.autofit()
        self.feeds.autofit()
        self.pipelines.autofit()
        self.agent_pools.autofit()
        self.variable_groups.autofit()
        self.workbook.close()

    def get_app_stats(self):
        '''
            Gets Azure DevOps instance stats
        '''
        # Section headers that should be formatted with black background and bold
        section_headers = {
            'General Information',
            'Projects & Repositories',
            'Users & Resources',
            'Azure Artifacts (Feeds)',
            'All Packages by Type'
        }
        
        # General Information
        report_stats = [
            ('General Information', ''),
            ('Organization URL', f"{self.host}/{self.project}" if self.project else self.host),
            ('Evaluate Version', utils.get_package_version()),
            ('Customer', '<CUSTOMERNAME>'),
            ('Date Run', utils.get_date_run()),
            ('Source', self.scrape_source_details()),
            ('Total Users', self.total_users),
            ('Total Projects', self.total_projects),
            ('Total Repositories', self.total_repositories + self.total_tfvc_repositories),
            ('Total Pull Requests', self.total_pull_requests),
            ('Total Work Items', self.total_work_items),
            ('Total Pipelines', self.total_pipelines),
            ('Total Packages', self.total_packages),
            ('Total Wikis', self.total_wikis),
            ('Total Variables', self.total_variables),
            ('Total Agent Pools', self.total_agent_pools),
            ('', ''),
            ('Projects & Repositories', ''),
            ('Total Projects', self.total_projects),
            ('Total Repositories', self.total_repositories),
            ('Total Disabled Repositories', self.total_disabled_repositories),
            ('Total Uninitialized Repositories', self.total_uninitialized_repositories),
            ('Total TFVC Repositories', self.total_tfvc_repositories),
            ('', ''),
            ('Users & Resources', ''),
            ('Total Users', self.total_users),
            ('Total Agent Pools', self.total_agent_pools),
            ('Total Variables', self.total_variables),
            ('Total Wikis', self.total_wikis),
            ('', ''),
            ('Azure Artifacts (Feeds)', ''),
            ('Total Feeds', self.total_feeds),
            ('Total Packages', self.total_packages),
            ('Total private packages', self.total_packages_to_migrate),
        ]
        
        # Add packages to migrate counts by type
        for package_type, count in sorted(self.packages_to_migrate_by_type.items()):
            report_stats.append((f'{package_type} private packages', count))
        
        report_stats.append(('', ''))
        report_stats.append(('All Packages by Type', ''))
        
        # Add package counts by type
        for package_type, count in sorted(self.packages_by_type.items()):
            report_stats.append((f'{package_type} Packages', count))
        
        for row, stat in enumerate(report_stats):
            if stat[0] in section_headers:
                self.app_stats.write(row, 0, stat[0], self.header_format)
                self.app_stats.write(row, 1, stat[1], self.header_format)
            else:
                self.app_stats.write(row, 0, stat[0])
                self.app_stats.write(row, 1, stat[1])
        return report_stats

    def handle_getting_data(self, skip_details):
        params = {"$top": "100", "$skip": "0"}

        try:
            if self.project:
                print(f"Fetching data for single project: {self.project}")
                project_id = self.project
                response = self.ado_client.retry_request(self.ado_client.get_project, params, project_id)
                project = response.json()
                self.total_projects = 1

                self._process_project(project_id)
                result = self.ado_client.handle_getting_project_data(project)
                self.total_work_items += result.get('Total Work Items', 0) if isinstance(result.get('Total Work Items'), int) else 0
                utils.append_to_workbook(self.projects, [result], self.project_headers)
                self._process_repos([project], skip_details)
            else:
                print("Fetching projects data...")
                while True:
                    response = self.ado_client.retry_request(self.ado_client.get_projects, params)
                    projects = response.json()
                    projects_in_page = len(projects.get('value', []))
                    self.total_projects += projects_in_page
                    print(f"Retrieved {self.total_projects} projects so far...")

                    for project in projects['value']:
                        self._process_project(project)

                    for result in self.multi.start_multi_process(self.ado_client.handle_getting_project_data, projects['value'], processes=self.processes):
                        self.total_work_items += result.get('Total Work Items', 0) if isinstance(result.get('Total Work Items'), int) else 0
                        utils.append_to_workbook(self.projects, [result], self.project_headers)

                    self._process_repos(projects['value'], skip_details)

                    # check if rate limit has been hit
                    self.ado_client.wait_timer(response.headers, "Projects List")

                    # Check for next page - if we got fewer items than requested, we've reached the end
                    if projects_in_page < 100:
                        break
                    # Move to next page using $skip
                    params["$skip"] = str(self.total_projects)


        except Exception as e:
            print(f"An error occurred while fetching project data: {e}")

    def _process_project(self, project):
        project_id = project['id'] if isinstance(project, dict) else project
        self._update_tfvc_count(project_id)
        self._update_wiki_count(project_id, project.get('name') if isinstance(project, dict) else None)

    def _process_repos(self, projects, skip_details):
        for repo_list in self.multi.start_multi_process(self.ado_client.handle_getting_repo_data, projects, processes=self.processes):
            for repo in repo_list:
                self.write_output_to_files(repo, skip_details)
                if repo.get('isDisabled'):
                    self.total_disabled_repositories += 1
                else:
                    self.total_repositories += 1
                if not repo.get('size'):
                    self.total_uninitialized_repositories += 1


    def _update_tfvc_count(self, project_id):
        try:
            properties_response = self.ado_client.get_project_properties(project_id)
            properties = properties_response.json().get("value", [])
            for prop in properties:
                if prop.get("name") == "System.SourceControlTfvcEnabled" and prop.get("value") == "True":
                    self.total_tfvc_repositories += 1
                    break
        except Exception as e:
            print(f"Failed to get project properties for TFVC check: {e}")

    def _update_wiki_count(self, project_id, project_name=None):
        try:
            wikis_response = self.ado_client.get_wikis(project_id)
            if wikis_response and wikis_response.status_code == 200:
                wiki_count = len(wikis_response.json().get('value', []))
                self.total_wikis += wiki_count
            else:
                print(f"Failed to get wiki count for project {project_name or project_id}")
        except Exception as e:
            print(f"Failed to get wikis for project {project_name or project_id}: {e}")

    def handle_getting_project_data(self, project):
        params = {}
        print("Fetching project data...")
        project_id = project["id"]
        project_name = project["name"]
        if "dev.azure.com" in self.host:
            print(f"Retriving project administrators in {project_name}...")
            project_admins = self.ado_client.get_project_administrators(project_id)
            if project_admins and isinstance(project_admins, list):
                project_admins_str = ', '.join(project_admins)
            else:
                project_admins_str = str(project_admins) if project_admins is not None else "No administrators found"
        else:
            print("Project administrators retrieval is not supported for this URL. Skipping ... ")
            project_admins_str = "N/A"

        print(f"Retriving total repositories, yaml definitions, classic releases and work items in {project_name}...")

        get_repos_response = self.ado_client.get_repos(project_id, params=params)
        try:
            total_repos = len(get_repos_response.json().get("value", []))
        except get_repos_response.exceptions.JSONDecodeError:
            print(f"Failed to decode JSON for repositories. Raw response: {get_repos_response.text}")
            total_repos = 0

        get_build_definitions_response = self.ado_client.get_build_definitions(project_id, params=params)
        try:
            total_build_definitions = len(get_build_definitions_response.json().get("value", []))
        except get_build_definitions_response.exceptions.JSONDecodeError:
            print(f"Failed to decode JSON for repositories. Raw response: {get_build_definitions_response.text}")
            total_build_definitions = 0

        get_release_definitions_response = self.ado_client.get_release_definitions(project_id, params=params)
        try:
            total_release_definitions = len(get_release_definitions_response.json().get("value", []))
        except get_release_definitions_response.exceptions.JSONDecodeError:
            print(f"Failed to decode JSON for repositories. Raw response: {get_release_definitions_response.text}")
            total_release_definitions = 0

        get_work_items_response = self.ado_client.get_work_items(project_id, project_name, params=params)
        try:
            total_work_items = len(get_work_items_response.json().get("workItems", []))
        except get_work_items_response.exceptions.JSONDecodeError:
            print(f"Failed to decode JSON for work items. Raw response: {get_work_items_response.text}")
            total_work_items = 0

        project_data = {
            'Project ID': project.get('id', 'N/A'),
            'URL': project.get('url', 'N/A'),
            'Name': project.get('name', 'N/A'),
            'Total Repositories': total_repos,
            'Total Build Definitions': total_build_definitions,
            'Total Release Definitions': total_release_definitions,
            'Total Work Items': total_work_items,
            'Administrators': project_admins_str
        }

        return project_data

    def handle_getting_user_data(self):
        # TODO: implement check if `--project` flag is passed and fetch users for that project only
        params = {
            "subjectTypes": "aad,msa"
        }
        print("Fetching user data...")
        while True:
            response = self.ado_client.retry_request(self.ado_client.get_users, params)
            users = response.json()
            for user in users["value"]:
                user_data = {
                    'Object ID': user.get('originId', 'N/A'),
                    'Descriptor': user.get('descriptor', 'N/A'),
                    'Display Name': user.get('displayName', 'N/A'),
                    'Principal Name': user.get('principalName', 'N/A'),
                    'Email': user.get('mailAddress', 'N/A')
                }
                utils.append_to_workbook(self.users, [user_data], self.user_headers)
            self.total_users += len(users['value'] if 'value' in users else [])
            print(f"Retrieved {self.total_users} users so far...")

            # check if rate limit has been hit
            self.ado_client.wait_timer(response.headers, "Users List")

            # Check if there's a next page
            if not any(key.lower() == "x-ms-continuationtoken" for key in response.headers):
                print(f"Retrieved a total of {self.total_users} users.")
                break  # No more pages
            # There is page, so get the continuation token for the next page
            params["continuationToken"] = response.headers["X-MS-ContinuationToken"]
            # print(response.request.url)

    def handle_getting_agent_pool_data(self):
        params = {
            "$top": "100"
        }
        print("Fetching agent pool data...")
        while True:
            response = self.ado_client.retry_request(self.ado_client.get_agent_pools, params)
            agent_pools = response.json()
            for pool in agent_pools["value"]:
                agent_pool_data = {
                    'Pool ID': pool.get('id', 'N/A'),
                    'Name': pool.get('name', 'N/A'),
                    'Is Hosted': pool.get('isHosted', 'N/A'),
                    'Pool Size': pool.get('size', 'N/A'),
                    'Legacy': pool.get('isLegacy', 'N/A'),
                    'Owner': (pool.get('owner') or {}).get('displayName', 'N/A')
                }
                utils.append_to_workbook(self.agent_pools, [agent_pool_data], self.agent_pool_headers)
            self.total_agent_pools += len(agent_pools['value'])
            print(f"Retrieved {len(agent_pools['value'])} agent pools so far...")

            # check if rate limit has been hit
            self.ado_client.wait_timer(response.headers, "Agent Pools List")

            # Check if there's a next page
            if not any(key.lower() == "x-ms-continuationtoken" for key in response.headers):
                break  # No more pages
            # There is page, so get the continuation token for the next page
            params["continuationToken"] = response.headers["X-MS-ContinuationToken"]

    def get_total_feeds(self):
        """Returns the total number of feeds counted during data collection"""
        return self.total_feeds

    def handle_getting_feeds_data(self):
        params = {
            "$top": "100"
        }
        print("Fetching Azure Artifact feeds data...")
        
        # Collect all feeds first
        all_feeds = []
        while True:
            response = self.ado_client.retry_request(self.ado_client.get_feeds, params)
            if not response or response.status_code != 200:
                print(f"Failed to fetch feeds: {response.text if response else 'No response'}")
                break
            feeds = response.json()
            self.total_feeds += len(feeds.get('value', []))
            all_feeds.extend(feeds.get('value', []))
            
            # check if rate limit has been hit
            self.ado_client.wait_timer(response.headers, "Artifacts Feeds")

            # Check if there's a next page
            if not any(key.lower() == "x-ms-continuationtoken" for key in response.headers):
                break  # No more pages
            # There is page, so get the continuation token for the next page
            params["continuationToken"] = response.headers["X-MS-ContinuationToken"]
        
        print(f"Found {len(all_feeds)} feeds. Processing packages in parallel...")
        
        # Process feeds in parallel and collect all package data
        all_package_data = []
        for result in self.multi.start_multi_process_stream_with_args(
            process_feed_packages,
            iter(all_feeds),
            self.host, self._token, self._api_version, self._verify,
            processes=self.processes
        ):
            if result:
                all_package_data.extend(result)
        
        # Aggregate results and write to workbook
        self._aggregate_and_write_feeds_data(all_package_data)

    def _process_feed_packages(self, feed, project_id):
        """Process all packages in a feed and get their metrics (non-multiprocessing version)"""
        packages_params = {}
        
        packages_response = self.ado_client.retry_request(
            self.ado_client.get_packages, packages_params, feed['id'], project_id
        )
        if not packages_response or packages_response.status_code != 200:
            print(f"Failed to fetch packages for feed {feed['name']}: {packages_response.text if packages_response else 'No response'}")
            return
            
        packages = packages_response.json()
        
        all_package_data = []
        for package in packages.get("value", []):
            print(f"Processing package {package.get('name', 'Unknown')} in feed {feed['name']}...")
            
            # Get package metrics
            package_metrics = self._get_package_metrics(feed['id'], package, project_id)
            
            # Build comprehensive package data
            package_data = self._build_package_data(feed, package, package_metrics)
            all_package_data.append(package_data)

        # Aggregate and write to workbook (handles counting)
        self._aggregate_and_write_feeds_data(all_package_data)

        # check if rate limit has been hit
        self.ado_client.wait_timer(packages_response.headers, "Packages List")

    def _get_package_metrics(self, feed_id, package, project_id):
        """Get package metrics using packagemetricsbatch endpoint"""
        try:
            # Prepare metrics request payload
            metrics_payload = json.dumps({
                "packageIds": [package.get('id')]
            })
            
            metrics_response = self.ado_client.get_package_metrics_batch(
                payload=metrics_payload, 
                feed_id=feed_id, 
                project_id=project_id
            )
            
            if metrics_response and metrics_response.status_code == 200:
                metrics_data = metrics_response.json()
                print(f"Retrieved metrics for package {package.get('name', 'Unknown')}")
                return metrics_data
            else:
                print(f"Failed to get metrics for package {package.get('name', 'Unknown')}: {metrics_response.text if metrics_response else 'No response'}")
                return None
                
        except Exception as e:
            print(f"Error getting package metrics: {e}")
            return None

    def _aggregate_and_write_feeds_data(self, all_package_data):
        """Aggregate package data and write to workbook after multiprocessing completes."""
        print(f"Aggregating {len(all_package_data)} packages...")
        
        for package_data in all_package_data:
            package_type = package_data.get('Package Type', 'Unknown')
            is_from_upstream = package_data.pop('_is_from_upstream', False)
            
            # Update counters
            self.total_packages += 1
            if package_type in self.packages_by_type:
                self.packages_by_type[package_type] += 1
            else:
                self.packages_by_type[package_type] = 1
            
            # Track packages to migrate (not cached from upstream sources)
            if not is_from_upstream:
                self.total_packages_to_migrate += 1
                if package_type in self.packages_to_migrate_by_type:
                    self.packages_to_migrate_by_type[package_type] += 1
                else:
                    self.packages_to_migrate_by_type[package_type] = 1
            
            # Write to workbook
            utils.append_to_workbook(self.feeds, [package_data], self.feeds_headers)
        
        print(f"Finished processing {self.total_packages} packages from {self.total_feeds} feeds.")

    def _build_package_data(self, feed, package, package_metrics):
        """Build comprehensive package data including metrics"""
        
        total_downloads = "N/A"
        last_download_date = "N/A"
        publish_date = "N/A"
        total_versions = "N/A"
        upstream_source_id = "N/A"
        upstream_source_url = "N/A"
        upstream_source_type = "N/A"
        
        if package_metrics and package_metrics.get('value'):
            for metric in package_metrics['value']:
                if metric.get('packageId') == package.get('id'):
                    total_downloads = metric.get('downloadCount', 0)
                    last_download_date = self.convert_utc_to_local(metric.get('lastDownloaded', 'N/A'))
                    break
        
        latest_version = None
        if 'versions' in package:
            total_versions = len(package['versions'])
            for version in package.get('versions', []):
                if version.get('isLatest', False):
                    latest_version = version
                    break

            if latest_version:
                publish_date = self.convert_utc_to_local(latest_version.get('publishDate'))
                upstream_source_id = latest_version.get('directUpstreamSourceId', 'N/A')
        
        if upstream_source_id != "N/A" and feed.get('upstreamSources'):
            for upstream_source in feed['upstreamSources']:
                if upstream_source.get('id') == upstream_source_id:
                    upstream_source_url = upstream_source.get('location', 'N/A')
                    upstream_source_type = upstream_source.get('upstreamSourceType', 'N/A')
                    break
        
        return {
            'Scope': 'Project' if feed.get('project') else 'Organization',
            'Project ID': feed.get('project', {}).get('id', 'N/A') if feed.get('project') else 'N/A',
            'Project Name': feed.get('project', {}).get('name', 'N/A') if feed.get('project') else 'N/A',
            'Feed ID': feed.get('id', 'N/A'),
            'Feed Name': feed.get('name', 'N/A'),
            'Package ID': package.get('id', 'N/A'),
            'Package Name': package.get('name', 'N/A'),
            'Package Type': package.get('protocolType', 'N/A'),
            'Total versions': total_versions,
            'Total Downloads': total_downloads,
            'Publish date': publish_date,
            'Last downloaded': last_download_date,
            'Upstream Source ID': upstream_source_id,
            'Upstream Source URL': upstream_source_url,
            'Upstream Source Type': upstream_source_type,
            '_is_from_upstream': upstream_source_url != "N/A"
        }

    def handle_getting_variable_groups_data(self):

        if self.project:
            print(f"Fetching variable groups for project {self.project}...")
            self._process_variable_groups(self.project, self.project)
        else:
            projects = self.ado_client.get_projects(params={}).json().get('value', [])
            for project in projects:
                self._process_variable_groups(project.get('id'), project.get('name'))

    def _process_variable_groups(self, project_id, project_name):
        variable_groups = self.ado_client.get_variable_groups(project_id, params={}).json()
        for group in variable_groups.get("value", []):
            print(f"Fetching variable groups for project {project_name}...")
            variables = group.get('variables', {})
            for variable_name, variable_info in variables.items():
                variable_data = {
                    'Project ID': project_id,
                    'Variable Group ID': group['id'],
                    'Variable Group Name': group['name'],
                    'Variable Group Description': group.get('description', 'N/A'),
                    'Type': group.get('type'),
                    'Variable Name': variable_name,
                    'Enabled': variable_info.get('enabled'),
                    'Is Secret': variable_info.get('isSecret'),
                    'Created By': group.get('createdBy', {}).get('uniqueName', 'N/A'),
                    'Created on': self.convert_utc_to_local(group.get('createdOn', 'N/A')),
                    'Modified on': self.convert_utc_to_local(group.get('modifiedOn', 'N/A'))
                }
                utils.append_to_workbook(self.variable_groups, [variable_data], self.variables_groups_headers)
            self.total_variables += len(variables)

    def handle_getting_pipelines_data(self):
        for project in self.ado_client.get_projects(params={}).json()['value']:
            project_id = project.get('id')
            pipelines = self.ado_client.get_build_definitions(project_id, params={}).json()
            self.total_pipelines += len(pipelines.get('value', []))
            for pipeline in pipelines.get('value', []):
                print(f"Fetching pipelines for project {project['name']}...")
                pipeline_data = {
                    'Project ID': project_id,
                    'Repository ID': pipeline.get('repository', {}).get('id', 'N/A') if pipeline.get('repository') else 'N/A',
                    'Pipeline ID': pipeline.get('id'),
                    'Pipeline Name': pipeline.get('name'),
                    'Pipeline URL': pipeline.get('url'),
                    'Pipeline Type': self.get_pipeline_type(pipeline),
                }
                utils.append_to_workbook(self.pipelines, [pipeline_data], self.pipeline_headers)

    def get_pipeline_type(self, pipeline):
        if (process_type := pipeline.get("process", {}).get("type")) == 2:
            pipeline_type = "Multi-stage YAML"
        elif process_type == 1:
            pipeline_type = "Classic"
        return pipeline_type

    def write_output_to_files(self, repo, skip_details):
        project_id = repo['project']['id']
        repository_id = repo['id']
        last_activity = "N/A"
        default_branch = repo['defaultBranch'] if 'defaultBranch' in repo else 'N/A'

        if repo.get('isDisabled', False) is False:

            if skip_details is False:
                branches = []
                branch_params = {'$top': 1000}
                branch_page = 0
                repo_name = repo.get('name', repository_id)
                print(f"[{repo_name}] Fetching branches...", flush=True)
                while True:
                    branch_page += 1
                    print(f"[{repo_name}] Fetching branches page {branch_page}...", flush=True)
                    branches_response = self.ado_client.retry_request(self.ado_client.get_branches, branch_params, project_id, repository_id)
                    if branches_response and branches_response.status_code == 200:
                        new_branches = branches_response.json()['value']
                        branches.extend(new_branches)
                        print(f"[{repo_name}] Page {branch_page}: Got {len(new_branches)} branches (total: {len(branches)})", flush=True)

                        # check if rate limit has been hit
                        self.ado_client.wait_timer(branches_response.headers, "Branches List")

                        # Check if there's a next page
                        if not any(key.lower() == "x-ms-continuationtoken" for key in branches_response.headers):
                            break  # No more pages
                        # There is page, so get the continuation token for the next page
                        branch_params["continuationToken"] = branches_response.headers["X-MS-ContinuationToken"]
                    else:
                        print(f"[{repo_name}] Branches request failed or returned non-200 status", flush=True)
                        break
                print(f"[{repo_name}] Finished fetching branches: {len(branches)} total", flush=True)
            else:
                print("Skipping branch details retrieval since `--skip-details` flag was passed", flush=True)
                branches = "N/A"

            if skip_details is False:
                pull_requests = []
                prs_params = {'$top': 1000}
                pr_page = 0
                repo_name = repo.get('name', repository_id)
                print(f"[{repo_name}] Fetching pull requests...", flush=True)
                while True:
                    pr_page += 1
                    print(f"[{repo_name}] Fetching pull requests page {pr_page}...", flush=True)
                    prs_response = self.ado_client.retry_request(self.ado_client.get_prs, prs_params, project_id, repository_id)
                    if prs_response and prs_response.status_code == 200:
                        new_prs = prs_response.json()['value']
                        if len(new_prs) > 0:
                            pull_requests.extend(new_prs)
                            print(f"[{repo_name}] Page {pr_page}: Got {len(new_prs)} PRs (total: {len(pull_requests)})", flush=True)

                            if "dev.azure.com" in self.host:
                                # Check if there's a next page
                                if not any(key.lower() == "x-ms-continuationtoken" for key in prs_response.headers):
                                    break  # No more pages
                                # There is page, so get the continuation token for the next page
                                prs_params["continuationToken"] = prs_response.headers["X-MS-ContinuationToken"]
                            else:
                                prs_params['$skip'] = len(pull_requests)

                        else:
                            print(f"[{repo_name}] No more pull requests on page {pr_page}", flush=True)
                            break

                        # check if rate limit has been hit
                        self.ado_client.wait_timer(prs_response.headers, "Repository Pull Requests")
                    else:
                        print(f"[{repo_name}] Pull requests request failed or returned non-200 status", flush=True)
                        break
                print(f"[{repo_name}] Finished fetching pull requests: {len(pull_requests)} total", flush=True)
            else:
                print("Skipping PR details retrieval since `--skip-details` flag was passed", flush=True)
                pull_requests = "N/A"

            # Count pull requests for total
            if isinstance(pull_requests, list):
                self.total_pull_requests += len(pull_requests)

            if skip_details is False:
                commits = []
                commit_params = {'$top': 1000}
                commit_page = 0
                repo_name = repo.get('name', repository_id)
                max_commits_reached = False
                print(f"[{repo_name}] Fetching commits...", flush=True)
                while True:
                    commit_page += 1
                    print(f"[{repo_name}] Fetching commits page {commit_page}...", flush=True)
                    commits_response = self.ado_client.retry_request(self.ado_client.get_commits, commit_params, project_id, repository_id)
                    if commits_response and commits_response.status_code == 200:
                        new_commits = commits_response.json()['value']
                        if len(new_commits) > 0:
                            commits.extend(new_commits)
                            print(f"[{repo_name}] Page {commit_page}: Got {len(new_commits)} commits (total: {len(commits)})", flush=True)
                            
                            # Check if max_commits limit is reached
                            if self.max_commits and len(commits) >= self.max_commits:
                                print(f"[{repo_name}] Max commits limit ({self.max_commits}) reached. Stopping commit fetch.", flush=True)
                                max_commits_reached = True
                                break
                            
                            commit_params['$skip'] = len(commits)
                        else:
                            print(f"[{repo_name}] No more commits on page {commit_page}", flush=True)
                            break
                    
                        # check if rate limit has been hit
                        self.ado_client.wait_timer(commits_response.headers, "Repository Commits")
                    else:
                        print(f"[{repo_name}] Commits request failed or returned non-200 status", flush=True)
                        break
                print(f"[{repo_name}] Finished fetching commits: {len(commits)} total{' (limit reached)' if max_commits_reached else ''}", flush=True)
                last_activity = commits[0]['committer']['date'] if commits else 'N/A'
                # If max_commits was reached, display as ">X" to indicate there are more
                if max_commits_reached:
                    commit_count = f">{self.max_commits}"
                else:
                    commit_count = len(commits)
            else:
                print("Skipping commits details retrieval since `--skip-details` flag was passed", flush=True)
                commit_count = "N/A"

        repo_size_mb = "N/A"
        if repo.get('size') is not None and repo.get("isDisabled", False) is False:
            repo_size_mb = round(repo.get('size') / 1024 / 1024, 2)

        repo_data = {
            'Project Name': repo.get('project', {}).get('name', 'N/A'),
            'Project ID': repo.get('project', {}).get('id', 'N/A'),
            'Repository Name': repo.get('name', 'N/A'),
            'Repository ID': repo.get('id', 'N/A'),
            'Default Branch': default_branch,
            'Git URL': repo.get('remoteUrl', 'N/A'),
            'Last activity': last_activity,
            'Branches': len(branches) if repo.get("isDisabled", False) is False and skip_details is False else "N/A",
            'Commits': commit_count if repo.get("isDisabled", False) is False and skip_details is False else "N/A",
            'Pull Requests': len(pull_requests) if repo.get("isDisabled", False) is False and skip_details is False else "N/A",
            'Repository Size in MB': repo_size_mb,
            'Repository Disabled': repo.get('isDisabled', False)
        }
        utils.append_to_workbook(self.raw_output, [repo_data], self.columns)
        if self.output_to_screen:
            print(f"Repository Data: {repo_data}")

    def validate_token(self):
        params = {}
        response = self.ado_client.test_connection(params=params)
        if response.status_code != 200:
            print("Invalid URL or PAT. Exiting...")
            print(f"Response: {response.url} - {response.text}")
            sys_exit(1)

    def convert_utc_to_local(self, date_str, fmt="%Y-%m-%d %H:%M:%S"):
        if not date_str or date_str == "N/A":
            return "N/A"
        try:
            utc_dt = parser.isoparse(date_str)
            local_dt = utc_dt.astimezone()
            return local_dt.strftime(fmt)
        except Exception:
            return date_str

    def scrape_source_details(self):
        request = self.ado_client.get_source_details()
        if request.status_code == 200:
            source = self.extract_ado_version(request.text)
        else:
            source = "Unknown server version"
        return source

    def extract_ado_version(self, text):
        match = re.search(r'"serviceVersion"\s*:\s*"([^"]+)"', text)
        if match:
            return match.group(1)
        else:
            if "tfs" in self.host:
                return "Team Foundation Server"
            else:
                return "Unknown server version"