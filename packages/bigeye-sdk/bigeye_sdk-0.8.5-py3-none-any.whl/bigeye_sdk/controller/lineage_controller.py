import logging
from typing import List, Dict, Tuple, Union, Optional, Set, Any

from bigeye_sdk.model.enums import MatchType
from bigeye_sdk.functions.search_and_match_functions import wildcard_search, fuzzy_match
from bigeye_sdk.functions.table_functions import fully_qualified_table_to_elements
from bigeye_sdk.generated.com.bigeye.models.generated import (
    Table,
    Integration,
    TableauWorkbook,
    Source,
    Schema,
    Delta,
    TableColumn,
    DataNodeType,
    DataNode,
    LineageRelationship,
    LineageNavigationNodeV2Response, RelationshipType, CreateLineageEdgeV2Request, LineageNodeV2
)
from bigeye_sdk.log import get_logger
from bigeye_sdk.client.datawatch_client import DatawatchClient
from bigeye_sdk.model.lineage_facade import SimpleLineageConfigurationFile, SimpleLineageEdgeRequest, \
    LineageColumnOverride, SimpleCustomNode, LineageConfiguration, SimpleLineageNodeRequest
from bigeye_sdk.model.lineage_graph import ContainmentNode
from bigeye_sdk.model.protobuf_enum_facade import SimpleDataNodeType, SimpleCatalogEntityType

log = get_logger(__name__)


class LineageController:
    def __init__(self, client: DatawatchClient, sources_ix: Optional[dict] = None):
        self.client = client
        self.sources_by_name_ix: Dict[str, Source] = self.set_source_ix(sources_ix=sources_ix)
        self.edge_requests: List[SimpleLineageEdgeRequest] = []
        self.node_requests: List[SimpleLineageNodeRequest] = []
        self.existing_relations: Dict[int, List[int]] = {}
        self.custom_nodes_ix_by_id: Dict[int, LineageNodeV2] = {}
        self.custom_node_search_cache: Dict[str, List[LineageNodeV2]] = {}
        self.cache_populated_node_ids: Set[int] = set()  # Track which node IDs have been scanned for cache population
        self.processed_etl_downstream_pairs: Set[Tuple[int, int]] = set()  # Track (etl_task_id, downstream_table_id) pairs
        self.lineage_node_ix_by_id: Dict[int, ContainmentNode] = {}
        self.catalog_attributes_request_cache: Dict[int, dict] = {}

    def set_source_ix(self, sources_ix: Optional[Dict[Any, Source]] = None):
        return {s.name: s for s in sources_ix.values()} if sources_ix else self.client.get_sources_by_name()


    def get_table_by_name(self, entity_name: str) -> Table:
        warehouse, schema, entity_name = fully_qualified_table_to_elements(entity_name)
        table: Table = self.client.get_tables(
            schema=[schema], table_name=[entity_name]
        ).tables[0]
        return table

    def get_tableau_workbook_by_name(
            self, entity_name: str, integration_name: str
    ) -> TableauWorkbook:
        integration: Integration = [
            i for i in self.client.get_integrations() if i.name == integration_name
        ][0]
        workbook = [
            w
            for w in self.client.get_integration_entities(integration_id=integration.id)
            if w.name == entity_name
        ][0]
        return workbook

    def create_node_by_name(self, entity_name: str, integration_name: str) -> DataNode:
        """Create a lineage node for an entity"""
        if not integration_name:
            table = self.get_table_by_name(entity_name=entity_name)
            log.info(f"Creating lineage node for table: {entity_name}")
            entity_id = table.id
            node_type = SimpleDataNodeType.TABLE.to_datawatch_object()

        else:
            workbook = self.get_tableau_workbook_by_name(
                entity_name=entity_name, integration_name=integration_name
            )
            log.info(f"Creating lineage node for entity: {workbook.name}")
            entity_id = workbook.id
            node_type = SimpleDataNodeType.TABLEAU.to_datawatch_object()

        return self.client.create_data_node(
            node_type=node_type, node_entity_id=entity_id
        )

    def delete_node_by_name(self, entity_name: str, integration_name: str):
        """Delete a lineage node for an entity"""
        if not integration_name:
            table = self.get_table_by_name(entity_name=entity_name)
            node_id = table.data_node_id
            log.info(f"Deleting lineage node for table: {table.name}")
        else:
            workbook = self.get_tableau_workbook_by_name(
                entity_name=entity_name, integration_name=integration_name
            )
            node_id = workbook.data_node_id
            log.info(f"Deleting lineage node for table: {workbook.name}")

        self.client.delete_data_node(data_node_id=node_id)

    def create_relation_from_name(
            self, upstream_table_name: str, downstream_table_name: str
    ) -> LineageRelationship:
        """Create a lineage relationship for 2 entities"""
        warehouse, u_schema, u_table_name = fully_qualified_table_to_elements(
            upstream_table_name
        )
        warehouse, d_schema, d_table_name = fully_qualified_table_to_elements(
            downstream_table_name
        )

        upstream: Table = self.client.get_tables(
            schema=[u_schema], table_name=[u_table_name]
        ).tables[0]
        downstream: Table = self.client.get_tables(
            schema=[d_schema], table_name=[d_table_name]
        ).tables[0]

        log.info(
            f"Creating relationship from {upstream_table_name} to {downstream_table_name}"
        )

        return self.client.create_table_lineage_relationship(
            upstream_data_node_id=upstream.data_node_id,
            downstream_data_node_id=downstream.data_node_id,
        )

    def delete_relationships_by_name(self, entity_name: str, integration_name: str):
        """Deletes all relationships for a node by name."""
        if integration_name:
            workbook = self.get_tableau_workbook_by_name(
                entity_name=entity_name, integration_name=integration_name
            )
            node_id = workbook.data_node_id
            log.info(
                f"Deleting all lineage relationships for workbook: {workbook.name}"
            )
        else:
            table = self.get_table_by_name(entity_name=entity_name)
            node_id = table.data_node_id
            log.info(f"Deleting all lineage relationships for table: {table.name}")

        self.client.delete_lineage_relationship_for_node(data_node_id=node_id)

    def get_schemas_from_selector(self, selector: str) -> List[Schema]:
        # Split selectors into patterns
        source_pattern, schema_pattern, table_pattern = fully_qualified_table_to_elements(selector.lower())

        # Only take source ids that match pattern
        source_ids = [
            source.id
            for source_name, source in self.sources_by_name_ix.items()
            if source_name.lower()
               in wildcard_search(search_string=source_pattern, content=[source_name.lower()])
        ]

        # Only take schemas from those sources that match pattern
        schemas_by_name_ix: Dict[str, Schema] = {
            s.name.lower(): s for s in self.client.get_schemas(warehouse_id=source_ids).schemas
        }
        schemas = [
            schema
            for schema_name, schema in schemas_by_name_ix.items()
            if schema_name
               in wildcard_search(search_string=schema_pattern, content=[schema_name])
        ]

        return schemas

    def get_tables_from_selector(self, selector: str) -> List[Table]:
        # Split selectors into patterns
        source_pattern, schema_pattern, table_pattern = fully_qualified_table_to_elements(selector.lower())
        # Get schemas
        schema_ids = [schema.id for schema in self.get_schemas_from_selector(selector.lower())]

        # Only take tables from those schemas that match pattern
        if not schema_ids:
            log.warning(f"No schemas found for given selector {selector}.")
            return []

        tables_by_id_ix: Dict[int, Table] = {
            t.id: t for t in self.client.get_tables_post(
                schema_ids=schema_ids,
                ignore_fields=False,
                include_data_node_ids=True
            ).tables
        }

        tables = [
            table
            for table_id, table in tables_by_id_ix.items()
            if table.name.lower()
               in wildcard_search(search_string=table_pattern, content=[table.name.lower()])
        ]

        return tables

    @staticmethod
    def infer_relationships_from_lists(
            upstream,
            downstream,
            task: Optional[SimpleCustomNode] = None,
            match_type: MatchType = MatchType.STRICT
    ):
        matching = []
        if match_type == MatchType.STRICT:
            for u in upstream:
                matching_downstream = [d for d in downstream if d.name.lower() == u.name.lower()]
                if matching_downstream:
                    for md in matching_downstream:
                        matching.append((u, md, task))
        elif match_type == MatchType.FUZZY:
            for u in upstream:
                matching_downstream = fuzzy_match(
                    search_string=u.name.lower(),
                    contents=[d.name.lower() for d in downstream],
                    min_match_score=95,
                )
                if matching_downstream:
                    for match in matching_downstream:
                        md_table = [md for md in downstream if md.name.lower() == match[1]]
                        for mdt in md_table:
                            matching.append((u, mdt, task))
        return matching

    def create_edges(self,
                     upstream: Union[Schema, Table, TableColumn, SimpleCustomNode],
                     downstream: Union[Schema, Table, TableColumn, SimpleCustomNode],
                     node_type: DataNodeType):
        if upstream.data_node_id and downstream.data_node_id:
            self.client.create_lineage_edge(upstream_data_node_id=upstream.data_node_id,
                                            downstream_data_node_id=downstream.data_node_id)
        elif upstream.data_node_id and not downstream.data_node_id:
            d_node = self.client.create_data_node(node_type=node_type, node_entity_id=downstream.id)
            self.client.create_lineage_edge(upstream_data_node_id=upstream.data_node_id,
                                            downstream_data_node_id=d_node.id)
        elif not upstream.data_node_id and downstream.data_node_id:
            u_node = self.client.create_data_node(node_type=node_type, node_entity_id=upstream.id)
            self.client.create_lineage_edge(upstream_data_node_id=u_node.id,
                                            downstream_data_node_id=downstream.data_node_id)
        else:
            u_node = self.client.create_data_node(node_type=node_type, node_entity_id=upstream.id)
            d_node = self.client.create_data_node(node_type=node_type, node_entity_id=downstream.id)
            self.client.create_lineage_edge(upstream_data_node_id=u_node.id,
                                            downstream_data_node_id=d_node.id)

    def create_relations_from_deltas(self, deltas: List[Delta]):
        for d in deltas:
            target_ids = [dc.target_table_id for dc in d.comparison_table_configurations]

            if len(target_ids) > 1:
                log.warning(f'We are unable to determine the proper lineage for deltas with more than 1 target. '
                            f'Please review the `bigeye lineage infer-relations` command for an alternative option.')
            else:
                source_table = self.client.get_tables(ids=[d.source_table.id]).tables[0]
                target_table = self.client.get_tables(ids=target_ids).tables[0]
                try:
                    self.infer_column_level_lineage_from_tables(tables=[(source_table, target_table, None)])
                except Exception as e:
                    log.warning(f'Failed to create lineage relationship between upstream table: {source_table.name} '
                                f'and downstream table: {target_table.name}. Exception: {e}')

    def get_matching_tables_from_selectors(
            self,
            upstream_selector: str,
            downstream_selector: str,
            match_type: MatchType = MatchType.STRICT
    ) -> List[Tuple[Table, Table, SimpleCustomNode]]:
        upstream_tables = self.get_tables_from_selector(upstream_selector)
        downstream_tables = self.get_tables_from_selector(downstream_selector)
        matching_tables: List[Tuple[Table, Table, SimpleCustomNode]] = self.infer_relationships_from_lists(
            upstream=upstream_tables,
            downstream=downstream_tables,
            match_type=match_type
        )
        return matching_tables

    def _search_custom_nodes(self, custom_node: SimpleCustomNode, search_for_container: Optional[bool] = False) -> Optional[LineageNodeV2]:
        if search_for_container:
            search_results = self.client.search_lineage(
                search=custom_node.container_name,
                search_type=DataNodeType.DATA_NODE_TYPE_CUSTOM
            ).results
            lineage_search_results = [sr.lineage_node for sr in search_results]
            matching_result = next(
                (r for r in lineage_search_results if r.node_type == custom_node.node_type
                 and r.node_name.lower() == custom_node.container_name.lower() and custom_node.container_name in r.catalog_path.path_parts),
                None
            )
            if matching_result:
                custom_node.container_node_id = matching_result.id
        else:
            search_results = self.client.search_lineage(
                search=custom_node.name,
                search_type=custom_node.node_type
            ).results
            lineage_search_results = [sr.lineage_node for sr in search_results]
            matching_result = next(
                (r for r in lineage_search_results if r.node_type == custom_node.node_type
                 and r.node_name.lower() == custom_node.name.lower()
                 and custom_node.container_name.lower() in [part.lower() for part in r.catalog_path.path_parts]),
                None
            )
            if matching_result:
                custom_node.data_node_id = matching_result.id

        return matching_result

    def _get_or_set_custom_node(self, custom_node: SimpleCustomNode) -> SimpleCustomNode:
        """Get the data node id of a custom node, if not available then create one and
        return custom task including node id."""

        # Custom Container Node
        custom_container_node = self.custom_nodes_ix_by_id.get(custom_node.container_node_id, None)
        if custom_container_node is None:
            container_search_cache_results = self.custom_node_search_cache.get(custom_node.container_name, None)
            if container_search_cache_results:
                custom_container_node = next(
                    (c for c in container_search_cache_results
                     if c.node_name == custom_node.container_name and c.node_type == DataNodeType.DATA_NODE_TYPE_CUSTOM),
                    None
                )

        if custom_container_node:
            custom_node.container_node_id = custom_container_node.id
            custom_node.container_entity_id = custom_container_node.node_entity_id
        else:
            existing_container = self._search_custom_nodes(custom_node=custom_node, search_for_container=True)
            if existing_container:
                custom_node.container_node_id = existing_container.id
                custom_node.container_entity_id = existing_container.node_entity_id
                if custom_node.container_name not in self.custom_node_search_cache:
                    self.custom_node_search_cache[custom_node.container_name] = []
                self.custom_node_search_cache[custom_node.container_name].append(existing_container)
                self.custom_nodes_ix_by_id[custom_node.container_node_id] = existing_container
            # if task does not exist yet, then create new custom node and assign the data node id
            else:
                # Use custom lineage API if custom IDs are present
                if custom_node.custom_repository_id and custom_node.custom_node_type_id:
                    new_container = self.client.create_custom_lineage_node(
                        node_name=custom_node.container_name,
                        node_type=DataNodeType.DATA_NODE_TYPE_CUSTOM,
                        node_container_name=custom_node.container_name,
                        custom_repository_id=custom_node.custom_repository_id,
                        custom_node_type_id=custom_node.custom_node_type_id,
                        icon_url=custom_node.node_icon_url
                    )
                else:
                    new_container = self.client.create_lineage_node(
                        node_name=custom_node.container_name,
                        node_container_name=custom_node.container_name,
                        node_type=DataNodeType.DATA_NODE_TYPE_CUSTOM,
                        icon_url=custom_node.node_icon_url
                    )
                custom_node.container_node_id = new_container.id
                custom_node.container_entity_id = new_container.node_entity_id
                self.custom_nodes_ix_by_id[new_container.id] = new_container

        # Set container metadata if provided
        if custom_node.container_metadata is not None and custom_node.container_entity_id:
            posted_metadata = self.catalog_attributes_request_cache.get(custom_node.container_entity_id, None)
            if posted_metadata is None or custom_node.container_metadata != posted_metadata:
                self.client.set_attributes(
                    entity_type=SimpleCatalogEntityType.DATA_NODE_ENTITY,
                    entity_id=custom_node.container_entity_id,
                    attributes=custom_node.container_metadata
                )
                self.catalog_attributes_request_cache[custom_node.container_entity_id] = custom_node.container_metadata

        # Custom Node
        existing_custom_node = self.custom_nodes_ix_by_id.get(custom_node.data_node_id, None)
        if existing_custom_node is None:
            search_cache_results = self.custom_node_search_cache.get(custom_node.name, None)
            if search_cache_results:
                existing_custom_node = next(
                    (c for c in search_cache_results
                     if c.node_name == custom_node.name and c.node_type == custom_node.node_type
                     and custom_node.container_name.lower() in [part.lower() for part in c.catalog_path.path_parts]),
                    None
                )

        if existing_custom_node:
            custom_node.data_node_id = existing_custom_node.id
            custom_node.entity_id = existing_custom_node.node_entity_id
            entity_id = existing_custom_node.node_entity_id
        else:
            log.info(f"Searching for {custom_node.name} in {custom_node.container_name}...")
            existing_custom_node = self._search_custom_nodes(custom_node=custom_node)
            if existing_custom_node:
                custom_node.data_node_id = existing_custom_node.id
                custom_node.entity_id = existing_custom_node.node_entity_id
                if custom_node.name not in self.custom_node_search_cache:
                    self.custom_node_search_cache[custom_node.name] = []
                self.custom_node_search_cache[custom_node.name].append(existing_custom_node)
                self.custom_nodes_ix_by_id[custom_node.data_node_id] = existing_custom_node
                entity_id = existing_custom_node.node_entity_id
            # if task does not exist yet, then create new custom node and assign the data node id
            else:
                # Use custom lineage API if custom IDs are present
                if custom_node.custom_repository_id and custom_node.custom_node_type_id:
                    new_task = self.client.create_custom_lineage_node(
                        node_name=custom_node.name,
                        node_type=custom_node.node_type,
                        node_container_name=custom_node.container_name,
                        custom_repository_id=custom_node.custom_repository_id,
                        custom_node_type_id=custom_node.custom_node_type_id,
                        node_container_entity_id=custom_node.container_entity_id,
                        icon_url=custom_node.node_icon_url
                    )
                else:
                    new_task = self.client.create_lineage_node(
                        node_name=custom_node.name,
                        node_container_name=custom_node.container_name,
                        node_type=custom_node.node_type,
                        icon_url=custom_node.node_icon_url
                    )
                custom_node.data_node_id = new_task.id
                custom_node.entity_id = new_task.node_entity_id
                self.custom_nodes_ix_by_id[custom_node.data_node_id] = new_task
                entity_id = new_task.node_entity_id
                # Add containment association to container and task only for non-custom lineage nodes
                # (custom lineage API handles containment automatically)
                if not (custom_node.custom_repository_id and custom_node.custom_node_type_id):
                    self.client.create_lineage_edge(
                        upstream_data_node_id=custom_node.container_node_id,
                        downstream_data_node_id=custom_node.data_node_id,
                        relationship_type=RelationshipType.RELATIONSHIP_TYPE_CONTAINMENT
                    )

        if custom_node.metadata is not None:
            posted_metadata = self.catalog_attributes_request_cache.get(entity_id, None)
            if posted_metadata is None or custom_node.metadata != posted_metadata:
                self.client.set_attributes(
                    entity_type=SimpleCatalogEntityType.DATA_NODE_ENTITY,
                    entity_id=entity_id,
                    attributes=custom_node.metadata
                )
                self.catalog_attributes_request_cache[entity_id] = custom_node.metadata

        return custom_node

    def process_all_edge_requests(self, purge_lineage: bool = False):
        count_successful_relations = 0
        count_skipped_relations = 0
        count_failed_relations = 0
        count_deleted_relations = 0

        final_edge_requests: List[CreateLineageEdgeV2Request] = []
        column_to_table_cache: Dict[int, Table] = {}  # Cache mapping column_id -> Table to avoid repeated fetches

        # Pre-fetch all columns and tables needed for caching to avoid N individual API calls
        if not purge_lineage:
            self._prefetch_columns_and_tables(column_to_table_cache)

        for r in self.edge_requests:
            try:
                # Validate that upstream and downstream have valid data_node_ids
                if not r.upstream.data_node_id or r.upstream.data_node_id == 0:
                    log.warning(f"Skipping edge request: upstream {r.upstream.name} has invalid data_node_id: {r.upstream.data_node_id}")
                    count_skipped_relations += 1
                    continue
                if not r.downstream.data_node_id or r.downstream.data_node_id == 0:
                    log.warning(f"Skipping edge request: downstream {r.downstream.name} has invalid data_node_id: {r.downstream.data_node_id}")
                    count_skipped_relations += 1
                    continue

                if purge_lineage:
                    "Purging lineage"
                    # TODO update once this is implemented, this current method will deletes containment relationships
                    # https://linear.app/torodata/issue/ONE-2510/[feature-request]-delete-all-relationships-for-a-node-id
                    self.client.delete_lineage_relationship_for_node(data_node_id=r.upstream.data_node_id)
                    # If etl task and purging, then delete all custom objects nested under the container
                    if r.etl_task:
                        custom_node_ids = self._get_custom_node_ids_for_task(r.etl_task)
                        for ln in custom_node_ids.values():
                            self.client.delete_lineage_node(node_id=ln.id)
                    count_deleted_relations += 1
                elif not r.etl_task:
                    existing_relations = self.existing_relations.get(r.upstream.data_node_id, None)
                    if existing_relations is None or r.downstream.data_node_id not in existing_relations:
                        final_edge_requests.append(CreateLineageEdgeV2Request(
                            upstream_data_node_id=r.upstream.data_node_id,
                            downstream_data_node_id=r.downstream.data_node_id,
                            relationship_type=RelationshipType.RELATIONSHIP_TYPE_LINEAGE
                        ))
                        count_successful_relations += 1
                    else:
                        "Skipping request because the relationship already exists."
                        count_skipped_relations += 1
                elif r.etl_task:
                    # If etl_task exists, then create a single custom node for etl_task container name
                    # then for every subproject create a custom node
                    # then for all output columns create a custom node entry
                    task = self._get_or_set_custom_node(custom_node=r.etl_task)
                    self._get_existing_relations_for_nodes([task.data_node_id, r.upstream.data_node_id])

                    # Pre-populate cache with all children of this task to avoid N individual searches
                    self._update_search_cache_for_node(task.data_node_id, scan_upstream=True)

                    source_to_etl_deps = self.existing_relations.get(r.upstream.data_node_id, None)
                    etl_downstream_deps = self.existing_relations.get(task.data_node_id, None)

                    if isinstance(r.upstream, TableColumn) or isinstance(r.upstream, SimpleCustomNode):
                        # new column node - use downstream column name to match ETL task output columns
                        etl_custom_column_node = SimpleCustomNode(
                            name=r.downstream.name,
                            container_name=r.etl_task.name,
                            container_node_id=task.data_node_id,
                            node_type=DataNodeType.DATA_NODE_TYPE_CUSTOM_ENTRY
                        )
                        etl_column_node = self._get_or_set_custom_node(etl_custom_column_node)
                        # containment relationship of column node with task
                        if etl_column_node.data_node_id not in etl_downstream_deps:
                            final_edge_requests.append(CreateLineageEdgeV2Request(
                                upstream_data_node_id=task.data_node_id,
                                downstream_data_node_id=etl_column_node.data_node_id,
                                relationship_type=RelationshipType.RELATIONSHIP_TYPE_CONTAINMENT
                            ))

                        if source_to_etl_deps is None or etl_column_node.data_node_id not in source_to_etl_deps:
                            log.info(f"Adding request for {r.upstream.name} to {etl_column_node.container_name} {etl_column_node.name}")
                            # lineage relationship of column node to upstream dep
                            final_edge_requests.append(CreateLineageEdgeV2Request(
                                upstream_data_node_id=r.upstream.data_node_id,
                                downstream_data_node_id=etl_column_node.data_node_id,
                                relationship_type=RelationshipType.RELATIONSHIP_TYPE_LINEAGE
                            ))
                            count_successful_relations += 1
                        else:
                            "Skipping request because the relationship already exists."
                            count_skipped_relations += 1

                        etl_column_downstream_deps = self.existing_relations.get(etl_column_node.data_node_id, None)
                        if etl_column_downstream_deps is None or r.downstream.data_node_id not in etl_column_downstream_deps:
                            log.info(
                                f"Adding request for {etl_column_node.container_name} {etl_column_node.name} to {r.downstream.name}")
                            # lineage relationship of column node to downstream dep
                            final_edge_requests.append(CreateLineageEdgeV2Request(
                                upstream_data_node_id=etl_column_node.data_node_id,
                                downstream_data_node_id=r.downstream.data_node_id,
                                relationship_type=RelationshipType.RELATIONSHIP_TYPE_LINEAGE
                            ))
                            count_successful_relations += 1
                        else:
                            "Skipping request because the relationship already exists."
                            count_skipped_relations += 1

                    # Ensure all downstream table columns have corresponding ETL task columns
                    # Only process once per (etl_task, downstream_table) pair to avoid duplicates
                    if isinstance(r.downstream, TableColumn):
                        # Check cache first to avoid repeated API calls for the same table
                        if r.downstream.id not in column_to_table_cache:
                            # Get the table associated with this column
                            column_with_table = self.client.get_columns(column_ids=[r.downstream.id]).columns[0]
                            downstream_table = self.client.get_tables_post(
                                table_ids=[column_with_table.table.id],
                                include_data_node_ids=True,
                                ignore_fields=False
                            ).tables[0]
                            column_to_table_cache[r.downstream.id] = downstream_table
                        else:
                            downstream_table = column_to_table_cache[r.downstream.id]

                        etl_downstream_pair = (task.data_node_id, downstream_table.id)
                        if etl_downstream_pair not in self.processed_etl_downstream_pairs:
                            self.processed_etl_downstream_pairs.add(etl_downstream_pair)
                            log.info(f"Ensuring all {len(downstream_table.columns)} columns in {downstream_table.name} have ETL task columns")

                            # Get existing ETL children to avoid unnecessary searches
                            etl_children_nodes = self.client.get_catalog_entity_children(
                                node_id=task.data_node_id,
                                node_type=DataNodeType.DATA_NODE_TYPE_CUSTOM
                            ).entities

                            for column in downstream_table.columns:
                                etl_col = next(
                                    (c for c in etl_children_nodes if c.node_name.lower() == column.name.lower()),
                                    None
                                )
                                if etl_col is None:
                                    etl_col_node = SimpleCustomNode(
                                        name=column.name,
                                        container_name=r.etl_task.name,
                                        container_node_id=task.data_node_id,
                                        node_type=DataNodeType.DATA_NODE_TYPE_CUSTOM_ENTRY
                                    )
                                    etl_col = self._get_or_set_custom_node(etl_col_node)
                                    final_edge_requests.append(CreateLineageEdgeV2Request(
                                        upstream_data_node_id=task.data_node_id,
                                        downstream_data_node_id=etl_col.data_node_id,
                                        relationship_type=RelationshipType.RELATIONSHIP_TYPE_CONTAINMENT
                                    ))

                                final_edge_requests.append(CreateLineageEdgeV2Request(
                                    upstream_data_node_id=etl_col.data_node_id,
                                    downstream_data_node_id=column.data_node_id,
                                    relationship_type=RelationshipType.RELATIONSHIP_TYPE_LINEAGE
                                ))

                    else:
                        if source_to_etl_deps is None or task.data_node_id not in source_to_etl_deps:
                            self.create_edges(
                                upstream=r.upstream,
                                downstream=task,
                                node_type=r.node_type
                            )
                            count_successful_relations += 1
                        else:
                            "Skipping request because the relationship already exists."
                            count_skipped_relations += 1

                        if etl_downstream_deps is None or r.downstream.data_node_id not in etl_downstream_deps:
                            self.create_edges(
                                upstream=task,
                                downstream=r.downstream,
                                node_type=r.node_type
                            )
                            count_successful_relations += 1
                        else:
                            "Skipping request because the relationship already exists."
                            count_skipped_relations += 1
                else:
                    "Skipping request because the relationship already exists."
                    count_skipped_relations += 1
            except Exception as e:
                log.error(
                    f"Failed to create relationship between upstream {r.node_type.name}: {r.upstream.name} and "
                    f"downstream {r.node_type.name}: {r.downstream.name}. Exception {e}"
                )
                count_failed_relations += 1

        # Make the lineage request
        cleansed_requests = []
        for r in final_edge_requests:
            if r.upstream_data_node_id != r.downstream_data_node_id:
                cleansed_requests.append(r)

        # Submit edges in chunks of 100 to avoid overloading the API
        chunk_size = 100
        total_requests = len(cleansed_requests)
        if total_requests > 0:
            log.info(f"Submitting {total_requests} lineage edges in chunks of {chunk_size}")
            for i in range(0, total_requests, chunk_size):
                chunk = cleansed_requests[i:i + chunk_size]
                log.info(f"Submitting chunk {i // chunk_size + 1}/{(total_requests + chunk_size - 1) // chunk_size} ({len(chunk)} edges)")
                self.client.bulk_create_lineage_edges(chunk)
            log.info(f"Successfully submitted all {total_requests} lineage edges")
        else:
            log.info("No lineage edges to submit")

        # Delete any custom nodes
        if purge_lineage:
            for node_id, node in self.custom_nodes_ix_by_id.items():
                self.client.delete_lineage_node(node_id=node_id)

        logging.disable(level=logging.NOTSET)

        log.info(
            f"\n\n------------LINEAGE REPORT--------------"
            f"\nCreated {count_successful_relations} edges."
            f"\nSkipped {count_skipped_relations} edges. "
            f"\nDeleted {count_deleted_relations} edges. "
            f"\nFailed {count_failed_relations} edges. \n"
        )

    def _prefetch_columns_and_tables(self, column_to_table_cache: Dict[int, Table]) -> None:
        """
        Pre-fetch all columns and tables needed for processing to avoid N individual API calls.
        Populates the column_to_table_cache with column_id -> Table mappings.
        """
        # Collect all unique column IDs from edge requests where downstream is a TableColumn
        column_ids = set()
        for r in self.edge_requests:
            if isinstance(r.downstream, TableColumn):
                column_ids.add(r.downstream.id)

        if not column_ids:
            log.info("No table columns found in edge requests, skipping pre-fetch")
            return

        log.info(f"Pre-fetching {len(column_ids)} columns and their parent tables to optimize API calls")

        # Batch fetch all columns
        column_ids_list = list(column_ids)
        try:
            columns_response = self.client.get_columns(column_ids=column_ids_list)
            columns = columns_response.columns
            log.info(f"Fetched {len(columns)} columns in batch")

            # Extract unique table IDs from the fetched columns
            table_ids = list(set(col.table.id for col in columns if col.table))

            if not table_ids:
                log.warning("No table IDs found from fetched columns")
                return

            # Batch fetch all tables with full details
            tables_response = self.client.get_tables_post(
                table_ids=table_ids,
                include_data_node_ids=True,
                ignore_fields=False
            )
            tables = tables_response.tables
            log.info(f"Fetched {len(tables)} unique tables in batch")

            # Build table lookup by table_id
            tables_by_id = {table.id: table for table in tables}

            # Populate the cache: column_id -> Table
            for col in columns:
                if col.table and col.table.id in tables_by_id:
                    column_to_table_cache[col.column.id] = tables_by_id[col.table.id]

            log.info(f"Populated cache with {len(column_to_table_cache)} column-to-table mappings")

        except Exception as e:
            log.error(f"Failed to pre-fetch columns and tables: {str(e)}")
            # Continue processing - the main loop will handle individual fetches as fallback

    def _get_existing_relations_for_nodes(self, node_ids: List[int]) -> None:
        """Get the existing relations for a schema. This is done at the top level to limit the number of requests
        that we have to make."""
        for nid in node_ids:
            # Skip invalid node IDs
            if not nid or nid == 0:
                continue
            if self.existing_relations.get(nid, None) is None:
                try:
                    downstream_nodes = self.client.get_downstream_nodes(node_id=nid)
                except Exception as e:
                    log.error(f"Failed to get downstream nodes for node_id {nid}: {str(e)}")
                    continue

                for node in downstream_nodes.nodes.values():
                    # Not sure why but nodes in TableLineageV2Response is a Dict[int, dict].
                    ln_node = LineageNavigationNodeV2Response().from_dict(node)
                    self.existing_relations[ln_node.lineage_node.id] = [d.downstream_id for d in
                                                                        ln_node.downstream_edges]
                    if ln_node.lineage_node.node_name not in self.custom_node_search_cache:
                        self.custom_node_search_cache[ln_node.lineage_node.node_name] = []
                    self.custom_node_search_cache[ln_node.lineage_node.node_name].append(ln_node.lineage_node)

    def _update_search_cache_for_node(self, node_id: int, node_name: Optional[str] = None, scan_upstream: bool = False):
        # Skip if we've already scanned this node_id
        if scan_upstream and node_id in self.cache_populated_node_ids:
            return
        try:
            if node_name is None or self.custom_node_search_cache.get(node_name, None) is None:
                if scan_upstream:
                    custom_nodes = self.client.get_downstream_nodes(node_id=node_id, depth=1).nodes
                    self.cache_populated_node_ids.add(node_id)  # Mark this node as scanned
                    for node_id, node in custom_nodes.items():
                        ln_node: LineageNavigationNodeV2Response = LineageNavigationNodeV2Response().from_dict(node)
                        for up_edge in ln_node.upstream_edges:
                            if (up_edge.relationship_type == RelationshipType.RELATIONSHIP_TYPE_CONTAINMENT
                                    and ln_node.lineage_node.node_type in [DataNodeType.DATA_NODE_TYPE_CUSTOM,
                                                                           DataNodeType.DATA_NODE_TYPE_CUSTOM_ENTRY]):
                                self.custom_nodes_ix_by_id[up_edge.downstream_id] = ln_node.lineage_node
                                if self.custom_node_search_cache.get(ln_node.lineage_node.node_name, None) is None:
                                    self.custom_node_search_cache[ln_node.lineage_node.node_name] = [ln_node.lineage_node]
                                else:
                                    self.custom_node_search_cache[ln_node.lineage_node.node_name].append(ln_node.lineage_node)
                else:
                    custom_nodes = self.client.get_downstream_nodes(node_id=node_id, depth=1).nodes
                    for node_id, node in custom_nodes.items():
                        ln_node: LineageNavigationNodeV2Response = LineageNavigationNodeV2Response().from_dict(node)
                        for dn_edge in ln_node.downstream_edges:
                            if (dn_edge.relationship_type == RelationshipType.RELATIONSHIP_TYPE_CONTAINMENT
                                    and ln_node.lineage_node.node_type in [DataNodeType.DATA_NODE_TYPE_CUSTOM,
                                                                           DataNodeType.DATA_NODE_TYPE_CUSTOM_ENTRY]):
                                self.custom_nodes_ix_by_id[dn_edge.upstream_id] = ln_node.lineage_node
                                if self.custom_node_search_cache.get(ln_node.lineage_node.node_name, None) is None:
                                    self.custom_node_search_cache[ln_node.lineage_node.node_name] = [ln_node.lineage_node]
                                else:
                                    self.custom_node_search_cache[ln_node.lineage_node.node_name].append(ln_node.lineage_node)
        except Exception as e:
            log.error(f"Failed to update search cache for node {node_id}: {str(e)}")

    def _execute_lineage_workflow_from_selectors(self, selectors: List[str]):
        source_ids = []
        for s in selectors:
            # Split selectors into patterns
            source_pattern, schema_pattern, table_pattern = fully_qualified_table_to_elements(s)

            # Only take source ids that match pattern
            selector_source_ids = [
                source.id
                for source_name, source in self.sources_by_name_ix.items()
                if source_name
                   in wildcard_search(search_string=source_pattern, content=[source_name])
            ]
            source_ids.extend([ssi for ssi in selector_source_ids if ssi not in source_ids])

        for sid in source_ids:
            self.client.rebuild_source(source_id=sid)

    def _get_custom_node_ids_for_task(self, etl_task: SimpleCustomNode) -> Dict[str, LineageNodeV2]:
        """Get all data node IDs for an etl_task."""

        # First search for the task container, i.e. Python of Airflow
        task = self._search_custom_nodes(custom_node=etl_task)

        # Get all downstream nodes from that task container
        try:
            nodes = self.client.get_downstream_nodes(node_id=task.id).nodes
        except Exception as e:
            log.error(f"Failed to get downstream nodes for node_id {task.id}: {str(e)}")
            return {}

        # Loop through all downstream nodes
        node_response = {}
        task_node_ids = []
        for node in nodes.values():
            ln_node: LineageNavigationNodeV2Response = LineageNavigationNodeV2Response().from_dict(node)
            if ln_node.lineage_node.id == task.id:
                for edge in ln_node.downstream_edges:
                    if edge.relationship_type == RelationshipType.RELATIONSHIP_TYPE_CONTAINMENT:
                        task_node_ids.append(edge.downstream_id)
                        node_response[ln_node.lineage_node.node_name] = ln_node.lineage_node

        for node in nodes.values():
            ln_node: LineageNavigationNodeV2Response = LineageNavigationNodeV2Response().from_dict(node)
            if ln_node.lineage_node.id in task_node_ids:
                node_response[ln_node.lineage_node.node_name] = ln_node.lineage_node

        return node_response

    def _delete_unused_custom_nodes(self, custom_node: SimpleCustomNode, table_to_compare: Table):
        # Check if there are any pre-existing custom column nodes that are no longer present
        # on the table and delete them
        existing_custom_nodes_for_task = self._get_custom_node_ids_for_task(custom_node)
        downstream_column_names = [d_col.name for d_col in table_to_compare.columns]
        nodes_to_delete = [n for node_name, n in existing_custom_nodes_for_task.items()
                           if n.node_name not in downstream_column_names]
        for node in nodes_to_delete:
            self.client.delete_lineage_node(node_id=node.id)

    def create_edges_from_table_names(
            self,
            upstream_table_name: str,
            downstream_table_name: str,
            etl_task_name: Optional[str] = None,
            etl_task_container: Optional[str] = "Python",
            column_overrides: Optional[List[LineageColumnOverride]] = None,
            infer_lineage: bool = True,
            purge_lineage: bool = False
    ):
        """Create a lineage edge for 2 entities"""
        upstream_table: Table = self.get_tables_from_selector(upstream_table_name)[0]
        downstream_table: Table = self.get_tables_from_selector(downstream_table_name)[0]

        etl_task = None
        if etl_task_name:
            etl_task = SimpleCustomNode(name=etl_task_name, container_name=etl_task_container)

        if column_overrides:
            u_columns_ix_by_name: Dict[str, TableColumn] = {c.name: c for c in upstream_table.columns}
            d_columns_ix_by_name: Dict[str, TableColumn] = {c.name: c for c in downstream_table.columns}

            for c_override in column_overrides:
                try:
                    up_column = u_columns_ix_by_name[c_override.upstream_column_name]
                    down_column = d_columns_ix_by_name[c_override.downstream_column_name]
                except KeyError as e:
                    log.warning(
                        f"No column found for provided column override. Please check spelling and try again."
                        f" Exception: {e}")
                    continue

                if up_column.data_node_id != down_column.data_node_id:
                    self.edge_requests.append(
                        SimpleLineageEdgeRequest(
                            upstream=u_columns_ix_by_name[c_override.upstream_column_name],
                            downstream=d_columns_ix_by_name[c_override.downstream_column_name],
                            node_type=DataNodeType.DATA_NODE_TYPE_COLUMN,
                            etl_task=etl_task
                        )
                    )
            if infer_lineage:
                self.infer_column_level_lineage_from_tables(
                    tables=[(upstream_table, downstream_table, etl_task)]
                )

        elif not column_overrides and not infer_lineage:
            if upstream_table.data_node_id != downstream_table.data_node_id:
                self.edge_requests.append(
                    SimpleLineageEdgeRequest(
                        upstream=upstream_table,
                        downstream=downstream_table,
                        node_type=DataNodeType.DATA_NODE_TYPE_TABLE,
                        etl_task=etl_task
                    )
                )
                self.process_all_edge_requests(purge_lineage=purge_lineage)

        elif infer_lineage and not column_overrides:
            self.infer_column_level_lineage_from_tables(
                tables=[(upstream_table, downstream_table, etl_task)]
            )

    def infer_relations_from_database_tables(self,
                                             r: LineageConfiguration,
                                             process_requests: Optional[bool] = False,
                                             match_by_name: Optional[bool] = True):
        matching_tables: List[Tuple[Table, Table, SimpleCustomNode]] = []

        upstream_tables: List[Table] = self.get_tables_from_selector(f'{r.upstream_schema_name}.*')
        downstream_tables: List[Table] = self.get_tables_from_selector(f'{r.downstream_schema_name}.*')

        if match_by_name:
            matching_tables_by_name = self.infer_relationships_from_lists(
                upstream=upstream_tables,
                downstream=downstream_tables,
                task=r.etl_task
            )
            matching_tables.extend(matching_tables_by_name)

        # index tables by name for reference later
        upstream_tables_ix_by_name: Dict[str, Table] = {t.name: t for t in upstream_tables}
        downstream_tables_ix_by_name: Dict[str, Table] = {t.name: t for t in downstream_tables}

        if r.table_overrides is not None:
            for t_override in r.table_overrides:
                try:
                    u_table = upstream_tables_ix_by_name[t_override.upstream_table_name]
                    d_table = downstream_tables_ix_by_name[t_override.downstream_table_name]
                    self._update_search_cache_for_node(u_table.data_node_id, scan_upstream=True)
                    self._update_search_cache_for_node(d_table.data_node_id, scan_upstream=True)
                    self._get_existing_relations_for_nodes(node_ids=[u_table.data_node_id, d_table.data_node_id])
                except KeyError as e:
                    log.warning(f"No table found for provided table override. Please check spelling and try again."
                                f" Exception: {e}")
                    continue

                u_columns_ix_by_name: Dict[str, TableColumn] = {c.name: c for c in u_table.columns}
                d_columns_ix_by_name: Dict[str, TableColumn] = {c.name: c for c in d_table.columns}

                # Loop through column exclusions and remove columns from tables if names match
                if t_override.column_name_exclusions is not None:
                    for col_excl in t_override.column_name_exclusions:
                        u_columns_ix_by_name.pop(col_excl, None)
                        d_columns_ix_by_name.pop(col_excl, None)

                    u_table.columns = [col for name, col in u_columns_ix_by_name.items()]
                    d_table.columns = [col for name, col in d_columns_ix_by_name.items()]

                # append to matching tables after removing columns
                # and remove any existing entries if table names are the same
                matched_by_name = (u_table, d_table, r.etl_task)
                if matched_by_name in matching_tables:
                    matching_tables.remove(matched_by_name)
                matching_tables.append((u_table, d_table, t_override.etl_task))

                if t_override.column_overrides is not None:
                    for c_override in t_override.column_overrides:
                        try:
                            up_column = u_columns_ix_by_name[c_override.upstream_column_name]
                            down_column = d_columns_ix_by_name[c_override.downstream_column_name]
                        except KeyError as e:
                            log.warning(
                                f"No column found for provided column override. Please check spelling and try again."
                                f" Exception: {e}")
                            continue

                        if up_column.data_node_id != down_column.data_node_id:
                            self.edge_requests.append(
                                SimpleLineageEdgeRequest(
                                    upstream=u_columns_ix_by_name[c_override.upstream_column_name],
                                    downstream=d_columns_ix_by_name[c_override.downstream_column_name],
                                    node_type=DataNodeType.DATA_NODE_TYPE_COLUMN,
                                    etl_task=t_override.etl_task,
                                )
                            )

        self.infer_column_level_lineage_from_tables(tables=matching_tables, process_requests=process_requests, match_by_name=match_by_name)

    def infer_column_level_lineage_from_file(
            self,
            lineage_configuration_file: SimpleLineageConfigurationFile,
            purge_lineage: bool = False,
            match_by_name: bool = True
    ):
        for r in lineage_configuration_file.relations:
            if not r.has_custom:
                self.infer_relations_from_database_tables(r=r, match_by_name=match_by_name)
            else:
                if r.etl_task is not None:
                    task = self._get_or_set_custom_node(r.etl_task)
                    self._get_existing_relations_for_nodes([task.data_node_id])
                    self._update_search_cache_for_node(task.data_node_id)

                # If upstream is custom, find or create the custom upstream schema node
                if r.has_custom_upstream:
                    up_database, up_schema = r.upstream_schema_name.split(".", maxsplit=1)
                    upstream_task = SimpleCustomNode(name=up_schema, container_name=up_database, node_icon=r.upstream_icon_url)
                    upstream_schema = self._get_or_set_custom_node(upstream_task)
                    self._update_search_cache_for_node(upstream_schema.data_node_id, scan_upstream=True)

                else:
                    upstream_schema = self.get_schemas_from_selector(f'{r.upstream_schema_name}.*')[0]
                    up_schema = upstream_schema.name
                    up_database = upstream_schema.warehouse_name
                    # Database schemas are too large to request lineage for
                    # self._update_search_cache_for_node(upstream_schema.data_node_id, scan_upstream=True)

                # If downstream is custom, find or create the custom downstream schema node
                if r.has_custom_downstream:
                    dn_database, dn_schema = r.downstream_schema_name.split(".", maxsplit=1)
                    downstream_task = SimpleCustomNode(name=dn_schema, container_name=dn_database, node_icon=r.downstream_icon_url)
                    downstream_schema = self._get_or_set_custom_node(downstream_task)
                    self._update_search_cache_for_node(downstream_schema.data_node_id, scan_upstream=True)

                else:
                    downstream_schema = self.get_schemas_from_selector(f'{r.downstream_schema_name}.*')[0]
                    dn_schema = downstream_schema.name
                    dn_database = downstream_schema.warehouse_name
                    # Database schemas are too large to request lineage for
                    # self._update_search_cache_for_node(downstream_schema.data_node_id, scan_upstream=True)

                # For every table override:
                table_node_ids = []
                for t_override in r.table_overrides:
                    # If upstream is custom, find or create the custom upstream table node
                    if r.has_custom_upstream:
                        upstream_table_custom_node = SimpleCustomNode(
                            name=t_override.upstream_table_name,
                            container_name=up_schema,
                            container_node_id=upstream_schema.data_node_id,
                        )
                        upstream_table = self._get_or_set_custom_node(upstream_table_custom_node)
                    else:
                        upstream_table = next(
                            (
                                t for t in self.get_tables_from_selector(f'{r.upstream_schema_name}.{t_override.upstream_table_name}')
                            ),
                            None
                        )
                        if upstream_table is None:
                            log.warning(
                                f"No table found for provided table override. Please check spelling and try again."
                                f" Upstream table name: {t_override.upstream_table_name}"
                            )
                            continue

                    table_node_ids.append(upstream_table.data_node_id)

                    # If downstream is custom, find or create the custom downstream table node
                    if r.has_custom_downstream:
                        downstream_table_custom_node = SimpleCustomNode(
                            name=t_override.downstream_table_name,
                            container_name=dn_schema,
                            container_node_id=downstream_schema.data_node_id
                        )
                        downstream_table = self._get_or_set_custom_node(downstream_table_custom_node)
                    else:
                        downstream_table = next(
                            (
                                t for t in self.get_tables_from_selector(f'{r.downstream_schema_name}.{t_override.downstream_table_name}')
                            ),
                            None
                        )
                        if downstream_table is None:
                            log.warning(
                                f"No table found for provided table override. Please check spelling and try again."
                                f" Downstream table name: {t_override.downstream_table_name}"
                            )
                            continue

                    table_node_ids.append(downstream_table.data_node_id)

                    if not t_override.column_overrides:
                        # if the upstream or downstream tables are custom, then we need to manually create the custom columns
                        # to match the others column structure
                        # if both are custom, then we will ignore columns for now
                        if r.has_custom_upstream and not r.has_custom_downstream:
                            for d_col in downstream_table.columns:
                                upstream_column_custom_node = SimpleCustomNode(
                                    name=d_col.name,
                                    container_name=upstream_table.name,
                                    container_node_id=upstream_table.data_node_id,
                                    node_type=DataNodeType.DATA_NODE_TYPE_CUSTOM_ENTRY
                                )
                                upstream_column = self._get_or_set_custom_node(upstream_column_custom_node)
                                self.edge_requests.append(
                                    SimpleLineageEdgeRequest(
                                        upstream=upstream_column,
                                        downstream=d_col,
                                        node_type=DataNodeType.DATA_NODE_TYPE_CUSTOM_ENTRY,
                                        etl_task=t_override.etl_task
                                    )
                                )
                            # self._delete_unused_custom_nodes(custom_node=upstream_table,
                            #                                  table_to_compare=downstream_table)

                        elif r.has_custom_downstream and not r.has_custom_upstream:
                            for u_col in upstream_table.columns:
                                downstream_column_custom_node = SimpleCustomNode(
                                    name=u_col.name,
                                    container_name=downstream_table.name,
                                    container_node_id=downstream_table.data_node_id,
                                    node_type=DataNodeType.DATA_NODE_TYPE_CUSTOM_ENTRY
                                )
                                downstream_column = self._get_or_set_custom_node(downstream_column_custom_node)
                                self.edge_requests.append(
                                    SimpleLineageEdgeRequest(
                                        upstream=u_col,
                                        downstream=downstream_column,
                                        node_type=DataNodeType.DATA_NODE_TYPE_CUSTOM_ENTRY,
                                        etl_task=t_override.etl_task
                                    )
                                )
                            # self._delete_unused_custom_nodes(custom_node=downstream_table,
                            #                                  table_to_compare=upstream_table)
                        else:
                            self.edge_requests.append(
                                SimpleLineageEdgeRequest(
                                    upstream=upstream_table,
                                    downstream=downstream_table,
                                    node_type=DataNodeType.DATA_NODE_TYPE_CUSTOM,
                                    etl_task=t_override.etl_task
                                )
                            )

                    # For every column override
                    for col_override in t_override.column_overrides:
                        # If upstream is custom, find or create the custom upstream column node (convert to custom_entry)
                        if r.has_custom_upstream:
                            upstream_column_custom_node = SimpleCustomNode(
                                name=col_override.upstream_column_name,
                                container_name=upstream_table.name,
                                container_node_id=upstream_table.data_node_id,
                                node_type=DataNodeType.DATA_NODE_TYPE_CUSTOM_ENTRY
                            )
                            upstream_column = self._get_or_set_custom_node(upstream_column_custom_node)
                        else:
                            upstream_column = next((c for c in upstream_table.columns if c.name == col_override.upstream_column_name), None)
                            if upstream_column is None:
                                log.warning(
                                    f"Skipping column lineage: Column '{col_override.upstream_column_name}' not found in upstream table '{upstream_table.name}'. "
                                    f"Available columns: {[c.name for c in upstream_table.columns]}"
                                )
                                continue

                        # If downstream is custom, find or create the custom downstream column node (convert to custom_entry)
                        if r.has_custom_downstream:
                            downstream_column_custom_node = SimpleCustomNode(
                                name=col_override.downstream_column_name,
                                container_name=downstream_table.name,
                                container_node_id=downstream_table.data_node_id,
                                node_type=DataNodeType.DATA_NODE_TYPE_CUSTOM_ENTRY
                            )
                            downstream_column = self._get_or_set_custom_node(downstream_column_custom_node)
                        else:
                            downstream_column = next((c for c in downstream_table.columns if c.name == col_override.downstream_column_name), None)
                            if downstream_column is None:
                                log.warning(
                                    f"Skipping column lineage: Column '{col_override.downstream_column_name}' not found in downstream table '{downstream_table.name}'. "
                                    f"Available columns: {[c.name for c in downstream_table.columns]}"
                                )
                                continue

                        # Create a lineage edge between the upstream and downstream column nodes
                        self.edge_requests.append(
                            SimpleLineageEdgeRequest(
                                upstream=upstream_column,
                                downstream=downstream_column,
                                node_type=DataNodeType.DATA_NODE_TYPE_CUSTOM_ENTRY,
                            )
                        )
                # Get all the relationships for tables and store in index
                self._get_existing_relations_for_nodes(table_node_ids)

        self.process_all_edge_requests(purge_lineage=purge_lineage)

        # TODO remove once this is implemented
        # https://linear.app/torodata/issue/ONE-2510/[feature-request]-delete-all-relationships-for-a-node-id
        if purge_lineage:
            self._execute_lineage_workflow_from_selectors(
                selectors=[f'{r.upstream_schema_name}.*' for r in lineage_configuration_file.relations]
            )
            log.warning(f"Purging lineage currently requires the sources to be re-indexed. This may take a few minutes,"
                        f" do not try to rebuild the lineage until re-indexing process has completed.")

    def infer_column_level_lineage_from_tables(
            self,
            tables: List[Tuple[Union[Table, SimpleCustomNode], Union[Table, SimpleCustomNode], Optional[SimpleCustomNode]]],
            purge_lineage: Optional[bool] = False,
            process_requests: Optional[bool] = True,
            match_by_name: Optional[bool] = True
    ):
        for upstream, downstream, etl_task in tables:
            matching_columns = []
            if match_by_name:
                matching_columns: List[
                    Tuple[TableColumn, TableColumn, SimpleCustomNode]] = self.infer_relationships_from_lists(
                    upstream=upstream.columns, downstream=downstream.columns, task=etl_task
                )

            for up_column, down_column, task in matching_columns:
                if up_column.data_node_id != down_column.data_node_id:
                    self.edge_requests.append(
                        SimpleLineageEdgeRequest(
                            upstream=up_column,
                            downstream=down_column,
                            node_type=DataNodeType.DATA_NODE_TYPE_COLUMN,
                            etl_task=task
                        )
                    )

        if process_requests:
            self.process_all_edge_requests(purge_lineage=purge_lineage)

    def search_nodes_for_table(self, table_id: int):
        table = self.client.search_tables(ids=[table_id], ignore_fields=True, include_data_node_ids=True).tables[0]
        self.continuous_search_from_data_node(data_node_id=table.data_node_id)

    def continuous_search_from_data_node(self, data_node_id: int):
        to_search = {data_node_id}
        searched_nodes = set()
        while to_search:
            data_node_id = to_search.pop()
            if data_node_id in searched_nodes:
                continue
                
            searched_nodes.add(data_node_id)
            current_node = self.lineage_node_ix_by_id.get(data_node_id)
            if not current_node:
                graph = self.client.get_lineage_graph_from_data_node(data_node_id=data_node_id, timeout=120)
                current_node = ContainmentNode.build(node_id=data_node_id, graph=graph)
                self.lineage_node_ix_by_id[current_node.id] = current_node

            to_search.update({up_id for up_id in current_node.upstream_objects if up_id not in searched_nodes})
            to_search.update({down_id for down_id in current_node.downstream_objects if down_id not in searched_nodes})
