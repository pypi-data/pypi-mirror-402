# VectorVein Python SDK

[![PyPI version](https://badge.fury.io/py/vectorvein.svg)](https://badge.fury.io/py/vectorvein)
[![Python versions](https://img.shields.io/pypi/pyversions/vectorvein.svg)](https://pypi.org/project/vectorvein/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

VectorVein Python SDK æ˜¯ä¸€ä¸ªåŠŸèƒ½å¼ºå¤§çš„ Python åº“ï¼Œæä¾›äº†å¯¹å‘é‡è„‰ç»œ(VectorVein)å¹³å°çš„å®Œæ•´è®¿é—®èƒ½åŠ›ã€‚å®ƒåŒ…å«ä¸¤å¤§æ ¸å¿ƒåŠŸèƒ½ï¼š
1. **å‘é‡è„‰ç»œ API å®¢æˆ·ç«¯** - ç”¨äºè°ƒç”¨å‘é‡è„‰ç»œçš„å·¥ä½œæµå’ŒVAppåŠŸèƒ½
2. **å¤šæ¨¡å‹èŠå¤©å®¢æˆ·ç«¯** - ç»Ÿä¸€çš„æ¥å£æ”¯æŒå¤šç§å¤§è¯­è¨€æ¨¡å‹ï¼ˆClaudeã€OpenAIã€é€šä¹‰åƒé—®ã€æ™ºè°±AIç­‰ï¼‰
3. **å·¥ä½œæµè®¾è®¡æ¡†æ¶** - ç”¨äºæ„å»ºå’Œè®¾è®¡å¤æ‚çš„AIå·¥ä½œæµ

## ğŸš€ å¿«é€Ÿå¼€å§‹

### å®‰è£…

```bash
pip install vectorvein
```

### åŸºæœ¬ä½¿ç”¨

#### 1. VectorVein API å®¢æˆ·ç«¯

```python
from vectorvein.api import VectorVeinClient, WorkflowInputField

# åˆ›å»ºå®¢æˆ·ç«¯å®ä¾‹
client = VectorVeinClient(api_key="YOUR_API_KEY")

# å‡†å¤‡å·¥ä½œæµè¾“å…¥å­—æ®µ
input_fields = [
    WorkflowInputField(
        node_id="8fc6eceb-8599-46a7-87fe-58bf7c0b633e",
        field_name="å•†å“åç§°",
        value="æµ‹è¯•å•†å“"
    )
]

# å¼‚æ­¥è¿è¡Œå·¥ä½œæµ
rid = client.run_workflow(
    wid="abcde0985736457aa72cc667f17bfc89",
    input_fields=input_fields,
    wait_for_completion=False
)
print(f"å·¥ä½œæµè¿è¡ŒID: {rid}")

# åŒæ­¥è¿è¡Œå·¥ä½œæµ
result = client.run_workflow(
    wid="abcde0985736457aa72cc667f17bfc89",
    input_fields=input_fields,
    wait_for_completion=True
)
print(f"å·¥ä½œæµè¿è¡Œç»“æœ: {result}")
```

#### 2. èŠå¤©å®¢æˆ·ç«¯

```python
from vectorvein.chat_clients import create_chat_client, BackendType
from vectorvein.settings import settings

# åŠ è½½è®¾ç½®ï¼ˆåŒ…å«APIå¯†é’¥ç­‰é…ç½®ï¼‰
settings.load({
    "rate_limit": {
        "enabled": True,
        "backend": "redis",  # æˆ– "diskcache"
        "redis": {
            "host": "127.0.0.1",
            "port": 6379,
            "db": 0,
        },
        "default_rpm": 60,
        "default_tpm": 1000000,
    },
    "endpoints": [
        {
            "id": "anthropic-default",
            "api_base": "https://api.anthropic.com",
            "api_key": "your_claude_api_key",
            "rpm": 60,
            "tpm": 1000000
        },
        {
            "id": "openai-default", 
            "api_base": "https://api.openai.com/v1",
            "api_key": "your_openai_api_key",
            "rpm": 3500,
            "tpm": 90000
        }
    ],
    "anthropic": {
        "models": {
            "claude-3-7-sonnet-20250219": {
                "id": "claude-3-7-sonnet-20250219",
                "endpoints": ["anthropic-default"],
                "context_length": 200000,
                "max_output_tokens": 8192,
                "function_call_available": True,
                "native_multimodal": True
            }
        }
    },
    "openai": {
        "models": {
            "gpt-4o": {
                "id": "gpt-4o", 
                "endpoints": ["openai-default"],
                "context_length": 128000,
                "max_output_tokens": 16384,
                "function_call_available": True,
                "response_format_available": True,
                "native_multimodal": True
            }
        }
    }
})

# åˆ›å»º Claude å®¢æˆ·ç«¯
client = create_chat_client(BackendType.Anthropic, model="claude-3-7-sonnet-20250219")

# å‘é€æ¶ˆæ¯
response = client.create_completion([
    {"role": "user", "content": "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹äººå·¥æ™ºèƒ½çš„å‘å±•å†ç¨‹"}
])
print(response.content)

# åˆ›å»º OpenAI å®¢æˆ·ç«¯
openai_client = create_chat_client(BackendType.OpenAI, model="gpt-4o")

# æµå¼å“åº”
for chunk in openai_client.create_stream([
    {"role": "user", "content": "å†™ä¸€é¦–å…³äºæ˜¥å¤©çš„è¯—"}
]):
    print(chunk.content, end="", flush=True)
```

#### 3. å·¥ä½œæµè®¾è®¡

```python
from vectorvein.workflow.graph.workflow import Workflow
from vectorvein.workflow.nodes.llms import Claude
from vectorvein.workflow.nodes.text_processing import TemplateCompose
from vectorvein.workflow.nodes.output import Text

# åˆ›å»ºå·¥ä½œæµ
workflow = Workflow()

# åˆ›å»ºèŠ‚ç‚¹
template = TemplateCompose()
template.add_port(name="ç”¨æˆ·è¾“å…¥", port_type="textarea", show=True)
template.ports["template"].value = "è¯·å›ç­”ä»¥ä¸‹é—®é¢˜ï¼š{{ç”¨æˆ·è¾“å…¥}}"

claude = Claude()
claude.ports["llm_model"].value = "claude-3-7-sonnet-20250219"
claude.ports["temperature"].value = 0.7

output = Text()
output.ports["output_title"].value = "AIå›ç­”"

# æ·»åŠ èŠ‚ç‚¹åˆ°å·¥ä½œæµ
workflow.add_nodes([template, claude, output])

# è¿æ¥èŠ‚ç‚¹
workflow.connect(template, "output", claude, "prompt")
workflow.connect(claude, "output", output, "text")

# å¸ƒå±€å’Œå¯¼å‡º
workflow.layout()
print(workflow.to_json())
```

## ğŸ“š åŠŸèƒ½ç‰¹æ€§

### VectorVein API å®¢æˆ·ç«¯

- **å·¥ä½œæµç®¡ç†**: è¿è¡Œå·¥ä½œæµã€æ£€æŸ¥çŠ¶æ€ã€ç®¡ç†æ‰§è¡Œ
- **è®¿é—®å¯†é’¥ç®¡ç†**: åˆ›å»ºã€è·å–ã€åˆ—è¡¨ã€æ›´æ–°ã€åˆ é™¤è®¿é—®å¯†é’¥
- **VApp é›†æˆ**: ç”ŸæˆVAppè®¿é—®é“¾æ¥
- **å¼‚æ­¥æ”¯æŒ**: å®Œæ•´çš„å¼‚æ­¥APIæ”¯æŒ
- **é”™è¯¯å¤„ç†**: è¯¦ç»†çš„å¼‚å¸¸ç±»å‹å’Œé”™è¯¯ä¿¡æ¯

### èŠå¤©å®¢æˆ·ç«¯

#### æ”¯æŒçš„æ¨¡å‹æä¾›å•†

- **Anthropic**: Claude-3, Claude-3.5, Claude-4, Claude Opus ç­‰
- **OpenAI**: GPT-3.5, GPT-4, GPT-4o, o1, o3 ç³»åˆ—
- **é˜¿é‡Œäº‘**: é€šä¹‰åƒé—® Qwen2.5, Qwen3, QVQ ç­‰
- **æ™ºè°±AI**: GLM-4, GLM-4.5, GLM-Z1 ç­‰
- **DeepSeek**: DeepSeek-Chat, DeepSeek-Reasoner
- **æœˆä¹‹æš—é¢**: Kimi, Moonshot ç³»åˆ—
- **Google**: Gemini 1.5, Gemini 2.0, Gemini 2.5
- **ç™¾å·æ™ºèƒ½**: Baichuan3, Baichuan4
- **é›¶ä¸€ä¸‡ç‰©**: Yi-Lightning, Yi-Vision
- **MiniMax**: MiniMax-Text, MiniMax-M1
- **Mistral**: Mistral Large, Codestral
- **Groq**: Llama3, Mixtral ç­‰
- **XAI**: Grok-2, Grok-3, Grok-4
- **ç™¾åº¦æ–‡å¿ƒ**: ERNIE ç³»åˆ—
- **é˜¶è·ƒæ˜Ÿè¾°**: Step-1, Step-2 ç³»åˆ—
- **æœ¬åœ°æ¨¡å‹**: æ”¯æŒæœ¬åœ°éƒ¨ç½²çš„æ¨¡å‹

#### æ ¸å¿ƒåŠŸèƒ½

- **ç»Ÿä¸€æ¥å£**: æ‰€æœ‰æ¨¡å‹ä½¿ç”¨ç›¸åŒçš„APIæ¥å£
- **æµå¼å“åº”**: æ”¯æŒå®æ—¶æµå¼è¾“å‡º
- **å¤šæ¨¡æ€**: æ”¯æŒå›¾åƒã€éŸ³é¢‘è¾“å…¥çš„æ¨¡å‹
- **å·¥å…·è°ƒç”¨**: æ”¯æŒFunction Callingçš„æ¨¡å‹
- **ä¸Šä¸‹æ–‡ç®¡ç†**: è‡ªåŠ¨å¤„ç†ä¸Šä¸‹æ–‡é•¿åº¦é™åˆ¶
- **ä»¤ç‰Œç»Ÿè®¡**: ç²¾ç¡®çš„ä»¤ç‰Œè®¡æ•°å’Œä½¿ç”¨ç»Ÿè®¡
- **é€Ÿç‡é™åˆ¶**: å†…ç½®é€Ÿç‡é™åˆ¶å’Œé‡è¯•æœºåˆ¶
- **å“åº”æ ¼å¼**: æ”¯æŒJSONæ¨¡å¼ç­‰ç»“æ„åŒ–è¾“å‡º

### å·¥ä½œæµè®¾è®¡æ¡†æ¶

- **å¯è§†åŒ–èŠ‚ç‚¹**: ä¸°å¯Œçš„é¢„ç½®èŠ‚ç‚¹åº“
- **çµæ´»è¿æ¥**: èŠ‚ç‚¹é—´çš„æ•°æ®æµè¿æ¥
- **æ‰¹é‡å¤„ç†**: æ”¯æŒåˆ—è¡¨è¾“å…¥çš„æ‰¹é‡å¤„ç†
- **ä»£ç æ‰§è¡Œ**: å†…ç½®Pythonä»£ç æ‰§è¡ŒèŠ‚ç‚¹
- **æ–‡ä»¶å¤„ç†**: æ–‡æ¡£è¯»å–ã€å›¾åƒå¤„ç†ã€éŸ³é¢‘å¤„ç†
- **æ•°æ®è¾“å‡º**: è¡¨æ ¼ã€æ–‡æ¡£ã€å›¾è¡¨ç­‰å¤šç§è¾“å‡ºæ ¼å¼

## ğŸ”§ å®‰è£…å’Œé…ç½®

### ä¾èµ–è¦æ±‚

- Python 3.10+
- å„æ¨¡å‹APIå¯†é’¥ï¼ˆæŒ‰éœ€é…ç½®ï¼‰

### å¯é€‰ä¾èµ–

```bash
# æœåŠ¡å™¨åŠŸèƒ½
pip install vectorvein[server]

# Redisç¼“å­˜
pip install vectorvein[redis]

# ç£ç›˜ç¼“å­˜
pip install vectorvein[diskcache]

# Google Vertex AI
pip install vectorvein[vertex]

# AWS Bedrock
pip install vectorvein[bedrock]
```

### è®¾ç½®é…ç½®

```python
from vectorvein.settings import settings

# é€šè¿‡å­—å…¸é…ç½®ï¼ˆv2 ç‰ˆæœ¬ï¼‰
settings_dict = {
    "rate_limit": {
        "enabled": True,
        "backend": "redis",  # æˆ– "diskcache"
        "redis": {
            "host": "127.0.0.1",
            "port": 6379,
            "db": 0,
        },
        "diskcache": {
            "cache_dir": ".rate_limit_cache",
        },
        "default_rpm": 60,
        "default_tpm": 1000000,
    },
    "endpoints": [
        {
            "id": "anthropic-default",
            "api_base": "https://api.anthropic.com",
            "api_key": "sk-ant-...",
            "rpm": 60,
            "tpm": 1000000,
            "concurrent_requests": 5
        },
        {
            "id": "openai-default",
            "api_base": "https://api.openai.com/v1", 
            "api_key": "sk-...",
            "rpm": 3500,
            "tpm": 90000,
            "concurrent_requests": 10
        },
        {
            "id": "qwen-default",
            "api_base": "https://dashscope.aliyuncs.com/compatible-mode/v1",
            "api_key": "sk-...",
            "rpm": 100,
            "tpm": 1000000
        },
        {
            "id": "azure-openai",
            "region": "East US",
            "api_base": "https://your-resource.openai.azure.com",
            "api_key": "your-azure-key",
            "rpm": 900,
            "tpm": 150000,
            "is_azure": True
        },
        {
            "id": "vertex-anthropic",
            "region": "europe-west1",
            "api_base": "https://europe-west1-aiplatform.googleapis.com",
            "credentials": {
                "token": "...",
                "refresh_token": "...",
                "client_id": "...",
                "client_secret": "...",
                "quota_project_id": "your-project-id"
            },
            "is_vertex": True
        }
    ],
    "anthropic": {
        "models": {
            "claude-3-7-sonnet-20250219": {
                "id": "claude-3-7-sonnet-20250219",
                "endpoints": ["anthropic-default"],
                "context_length": 200000,
                "max_output_tokens": 8192,
                "function_call_available": True,
                "native_multimodal": True
            },
            "claude-3-5-sonnet-20240620": {
                "id": "claude-3-5-sonnet@20240620",
                "endpoints": ["vertex-anthropic"],
                "context_length": 200000,
                "max_output_tokens": 8192,
                "function_call_available": True,
                "native_multimodal": True
            }
        }
    },
    "openai": {
        "models": {
            "gpt-4o": {
                "id": "gpt-4o",
                "endpoints": ["openai-default", "azure-openai"],
                "context_length": 128000,
                "max_output_tokens": 16384,
                "function_call_available": True,
                "response_format_available": True,
                "native_multimodal": True
            },
            "gpt-4o-mini": {
                "id": "gpt-4o-mini",
                "endpoints": ["openai-default"],
                "context_length": 128000,
                "max_output_tokens": 16384,
                "function_call_available": True,
                "response_format_available": True,
                "native_multimodal": True
            }
        }
    },
    "qwen": {
        "models": {
            "qwen3-32b": {
                "id": "qwen3-32b", 
                "endpoints": ["qwen-default"],
                "context_length": 32768,
                "max_output_tokens": 8192,
                "function_call_available": True,
                "response_format_available": True,
                "native_multimodal": False
            },
            "qwen2.5-72b-instruct": {
                "id": "qwen2.5-72b-instruct",
                "endpoints": ["qwen-default"],
                "context_length": 131072,
                "max_output_tokens": 8192,
                "function_call_available": True,
                "response_format_available": True,
                "native_multimodal": False
            }
        }
    }
}
settings.load(settings_dict)

# æˆ–é€šè¿‡æ–‡ä»¶é…ç½®
settings.load_from_file("config.json")
```

## ğŸ“– è¯¦ç»†æ–‡æ¡£

### API å®¢æˆ·ç«¯è¯¦ç»†ä½¿ç”¨

#### è®¿é—®å¯†é’¥ç®¡ç†

```python
from vectorvein.api import VectorVeinClient

client = VectorVeinClient(api_key="YOUR_API_KEY")

# åˆ›å»ºè®¿é—®å¯†é’¥
keys = client.create_access_keys(
    access_key_type="L",  # L: é•¿æœŸ, M: å¤šæ¬¡, O: ä¸€æ¬¡æ€§
    app_id="YOUR_APP_ID",
    count=1,
    max_credits=500,
    description="æµ‹è¯•å¯†é’¥"
)

# è·å–è®¿é—®å¯†é’¥ä¿¡æ¯
keys_info = client.get_access_keys(["ACCESS_KEY_1", "ACCESS_KEY_2"])

# åˆ—å‡ºè®¿é—®å¯†é’¥
response = client.list_access_keys(
    page=1,
    page_size=10,
    sort_field="create_time",
    sort_order="descend"
)

# æ›´æ–°è®¿é—®å¯†é’¥
client.update_access_keys(
    access_key="ACCESS_KEY",
    description="æ›´æ–°çš„æè¿°"
)

# åˆ é™¤è®¿é—®å¯†é’¥
client.delete_access_keys(
    app_id="YOUR_APP_ID",
    access_keys=["ACCESS_KEY_1", "ACCESS_KEY_2"]
)
```

#### ç”ŸæˆVAppè®¿é—®é“¾æ¥

```python
url = client.generate_vapp_url(
    app_id="YOUR_APP_ID",
    access_key="YOUR_ACCESS_KEY",
    key_id="YOUR_KEY_ID"
)
print(f"VAppè®¿é—®é“¾æ¥: {url}")
```

### èŠå¤©å®¢æˆ·ç«¯é«˜çº§ç”¨æ³•

#### å·¥å…·è°ƒç”¨ï¼ˆFunction Callingï¼‰

```python
from vectorvein.chat_clients import create_chat_client, BackendType

client = create_chat_client(BackendType.OpenAI, model="gpt-4o")

tools = [{
    "type": "function",
    "function": {
        "name": "get_weather",
        "description": "è·å–æŒ‡å®šåŸå¸‚çš„å¤©æ°”ä¿¡æ¯",
        "parameters": {
            "type": "object",
            "properties": {
                "city": {"type": "string", "description": "åŸå¸‚åç§°"}
            },
            "required": ["city"]
        }
    }
}]

response = client.create_completion(
    messages=[{"role": "user", "content": "åŒ—äº¬ä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ"}],
    tools=tools
)

if response.tool_calls:
    for tool_call in response.tool_calls:
        print(f"è°ƒç”¨å·¥å…·: {tool_call.function.name}")
        print(f"å‚æ•°: {tool_call.function.arguments}")
```

#### å¤šæ¨¡æ€è¾“å…¥

```python
client = create_chat_client(BackendType.Anthropic, model="claude-3-7-sonnet-20250219")

messages = [{
    "role": "user",
    "content": [
        {"type": "text", "text": "è¿™å¼ å›¾ç‰‡é‡Œæœ‰ä»€ä¹ˆï¼Ÿ"},
        {
            "type": "image",
            "source": {
                "type": "base64",
                "media_type": "image/jpeg",
                "data": "base64_encoded_image_data"
            }
        }
    ]
}]

response = client.create_completion(messages)
```

#### ç»“æ„åŒ–è¾“å‡º

```python
client = create_chat_client(BackendType.OpenAI, model="gpt-4o")

response = client.create_completion(
    messages=[{"role": "user", "content": "åˆ†æä»¥ä¸‹æ•°æ®å¹¶è¿”å›JSONæ ¼å¼"}],
    response_format={"type": "json_object"}
)
```

### å·¥ä½œæµèŠ‚ç‚¹å‚è€ƒ

#### LLM èŠ‚ç‚¹

```python
from vectorvein.workflow.nodes.llms import Claude, OpenAI, AliyunQwen

# Claude èŠ‚ç‚¹
claude = Claude()
claude.ports["llm_model"].value = "claude-3-7-sonnet-20250219"
claude.ports["temperature"].value = 0.7
claude.ports["prompt"].show = True

# OpenAI èŠ‚ç‚¹
openai = OpenAI()
openai.ports["llm_model"].value = "gpt-4o"
openai.ports["response_format"].value = "json_object"

# é€šä¹‰åƒé—®èŠ‚ç‚¹
qwen = AliyunQwen()
qwen.ports["llm_model"].value = "qwen3-32b"
```

#### æ–‡æœ¬å¤„ç†èŠ‚ç‚¹

```python
from vectorvein.workflow.nodes.text_processing import (
    TemplateCompose, TextSplitters, TextReplace
)

# æ–‡æœ¬åˆæˆ
template = TemplateCompose()
template.add_port(name="æ ‡é¢˜", port_type="text", show=True)
template.add_port(name="å†…å®¹", port_type="textarea", show=True)
template.ports["template"].value = "# {{æ ‡é¢˜}}\n\n{{å†…å®¹}}"

# æ–‡æœ¬åˆ†å‰²
splitter = TextSplitters()
splitter.ports["split_method"].value = "delimiter"
splitter.ports["delimiter"].value = "\n"
splitter.ports["text"].show = True

# æ–‡æœ¬æ›¿æ¢
replacer = TextReplace()
replacer.ports["old_text"].value = "æ—§æ–‡æœ¬"
replacer.ports["new_text"].value = "æ–°æ–‡æœ¬"
replacer.ports["text"].show = True
```

#### æ–‡ä»¶å¤„ç†èŠ‚ç‚¹

```python
from vectorvein.workflow.nodes.file_processing import FileLoader

loader = FileLoader()
loader.ports["parse_quality"].value = "high"  # é«˜è´¨é‡è§£æ
loader.ports["files"].show = True  # æ˜¾ç¤ºæ–‡ä»¶ä¸Šä¼ ç•Œé¢
```

#### è¾“å‡ºèŠ‚ç‚¹

```python
from vectorvein.workflow.nodes.output import Text, Table, Document

# æ–‡æœ¬è¾“å‡º
text_output = Text()
text_output.ports["output_title"].value = "ç»“æœ"

# è¡¨æ ¼è¾“å‡º
table_output = Table()

# æ–‡æ¡£è¾“å‡º
doc_output = Document()
doc_output.ports["file_name"].value = "æŠ¥å‘Š"
doc_output.ports["export_type"].value = ".xlsx"
```

## ğŸ” å¼‚å¸¸å¤„ç†

```python
from vectorvein.api import (
    VectorVeinAPIError, APIKeyError, WorkflowError, 
    AccessKeyError, RequestError, TimeoutError
)

try:
    result = client.run_workflow(wid="invalid", input_fields=[])
except APIKeyError as e:
    print(f"APIå¯†é’¥é”™è¯¯: {e}")
except WorkflowError as e:
    print(f"å·¥ä½œæµé”™è¯¯: {e}")
except TimeoutError as e:
    print(f"è¯·æ±‚è¶…æ—¶: {e}")
except VectorVeinAPIError as e:
    print(f"APIé”™è¯¯: {e.message}, çŠ¶æ€ç : {e.status_code}")
```

## ğŸ§ª æµ‹è¯•

```bash
# å®‰è£…å¼€å‘ä¾èµ–
pip install -e .[dev]

# è¿è¡Œæµ‹è¯•
pytest tests/

# è¿è¡Œç‰¹å®šæµ‹è¯•
pytest tests/test_simple.py -v

# ç”Ÿæˆè¦†ç›–ç‡æŠ¥å‘Š
pytest --cov=vectorvein tests/
```

## ğŸ“ å¼€å‘æŒ‡å—

### é¡¹ç›®ç»“æ„

```
src/vectorvein/
â”œâ”€â”€ api/                 # VectorVein APIå®¢æˆ·ç«¯
â”‚   â”œâ”€â”€ client.py       # ä¸»è¦å®¢æˆ·ç«¯ç±»
â”‚   â”œâ”€â”€ models.py       # æ•°æ®æ¨¡å‹
â”‚   â””â”€â”€ exceptions.py   # å¼‚å¸¸å®šä¹‰
â”œâ”€â”€ chat_clients/        # èŠå¤©å®¢æˆ·ç«¯
â”‚   â”œâ”€â”€ __init__.py     # å®¢æˆ·ç«¯å·¥å‚å‡½æ•°
â”‚   â”œâ”€â”€ base_client.py  # åŸºç¡€å®¢æˆ·ç«¯ç±»
â”‚   â”œâ”€â”€ anthropic_client.py  # Claudeå®¢æˆ·ç«¯
â”‚   â”œâ”€â”€ openai_client.py     # OpenAIå®¢æˆ·ç«¯
â”‚   â””â”€â”€ ...             # å…¶ä»–æ¨¡å‹å®¢æˆ·ç«¯
â”œâ”€â”€ workflow/           # å·¥ä½œæµè®¾è®¡æ¡†æ¶
â”‚   â”œâ”€â”€ graph/          # å›¾ç»“æ„å®šä¹‰
â”‚   â”œâ”€â”€ nodes/          # èŠ‚ç‚¹å®šä¹‰
â”‚   â””â”€â”€ utils/          # å·¥å…·å‡½æ•°
â”œâ”€â”€ settings/           # é…ç½®ç®¡ç†
â”œâ”€â”€ types/              # ç±»å‹å®šä¹‰
â””â”€â”€ utilities/          # å®ç”¨å·¥å…·
```

### è´¡çŒ®ä»£ç 

1. Fork é¡¹ç›®
2. åˆ›å»ºåŠŸèƒ½åˆ†æ”¯ (`git checkout -b feature/amazing-feature`)
3. æäº¤æ›´æ”¹ (`git commit -m 'Add amazing feature'`)
4. æ¨é€åˆ†æ”¯ (`git push origin feature/amazing-feature`)
5. åˆ›å»º Pull Request

### ä»£ç è§„èŒƒ

- ä½¿ç”¨ `ruff` è¿›è¡Œä»£ç æ ¼å¼åŒ–å’Œæ£€æŸ¥
- éµå¾ª Python ç±»å‹æç¤º
- ç¼–å†™æµ‹è¯•ç”¨ä¾‹è¦†ç›–æ–°åŠŸèƒ½
- æ›´æ–°ç›¸å…³æ–‡æ¡£

## ğŸ¤ ç¤¾åŒºå’Œæ”¯æŒ

- **æ–‡æ¡£**: [å®˜æ–¹æ–‡æ¡£](https://docs.vectorvein.com)
- **é—®é¢˜åé¦ˆ**: [GitHub Issues](https://github.com/vectorvein/python-vectorvein/issues)
- **è®¨è®º**: [GitHub Discussions](https://github.com/vectorvein/python-vectorvein/discussions)
- **æ›´æ–°æ—¥å¿—**: [CHANGELOG.md](CHANGELOG.md)

## ğŸ“œ è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨ MIT è®¸å¯è¯ - æŸ¥çœ‹ [LICENSE](LICENSE) æ–‡ä»¶äº†è§£è¯¦æƒ…ã€‚

## ğŸ™ è‡´è°¢

æ„Ÿè°¢æ‰€æœ‰ä¸º VectorVein Python SDK åšå‡ºè´¡çŒ®çš„å¼€å‘è€…å’Œç”¨æˆ·ã€‚

---

**æ³¨æ„äº‹é¡¹**ï¼š
1. è¯·å¦¥å–„ä¿ç®¡æ‚¨çš„APIå¯†é’¥ï¼Œä¸è¦å°†å…¶æ³„éœ²ç»™ä»–äºº
2. APIè°ƒç”¨æœ‰é€Ÿç‡é™åˆ¶ï¼Œè¯·åˆç†ä½¿ç”¨
3. å»ºè®®åœ¨ç”Ÿäº§ç¯å¢ƒä¸­ä½¿ç”¨å¼‚æ­¥æ–¹å¼è¿è¡Œå·¥ä½œæµ
4. ä¸åŒæ¨¡å‹æ”¯æŒçš„åŠŸèƒ½å¯èƒ½æœ‰æ‰€å·®å¼‚ï¼Œè¯·å‚è€ƒå…·ä½“æ¨¡å‹æ–‡æ¡£