import asyncio
import csv
import logging
import os
import time
from typing import Any, List

import pandas as pd
import typer

from utils.logger import get_logger
from utils.path import gen_fp_with_suffix
from utils.pd import dt_to_timestamp
from utils.pyredis import get_redis_client

app = typer.Typer()

r = get_redis_client()
logger = get_logger(__name__, level=logging.INFO)


async def csv_to_redis(
    key_prefix: str = "bitget_grid",
    fp: str = "d:/github/meme2046/data/bitget_0.csv",
    id1="order_id",
    id2="client_order_id",
):
    pipe = r.pipeline()  # å¯ç”¨ pipeline
    count = 0

    with open(fp, newline="", encoding="utf-8") as f:
        for row in csv.DictReader(f):
            idx1 = row[id1]
            idx2 = row[id2]
            if not idx1 or not idx2:
                raise ValueError("æ— æ•ˆçš„è¡Œ")
            id = f"{idx1}_{idx2}"

            key = f"{key_prefix}:{id}"
            # HSET è‡ªåŠ¨è¦†ç›–å·²æœ‰å­—æ®µ
            pipe.hset(key, mapping=row)

            pipe.zadd(f"by_time:{key_prefix}", {id: time.time()})

            count += 1

        await pipe.execute()
        logger.info(f"å†™å…¥ã€{count}ã€æ¡")


async def csv_pd_redis(
    id1="order_id",
    id2="client_order_id",
    key_prefix: str = "bitget_sf",
    fp: str = "d:/github/meme2046/data/bitget_sf_0.csv",
):
    if os.path.exists(fp):
        df: pd.DataFrame = pd.read_csv(
            fp,
            encoding="utf-8",
            dtype={
                "order_id": str,
                "fx_order_id": str,
                "spot_order_id": str,
                "futures_order_id": str,
                "spot_tracking_no": str,
                "futures_tracking_no": str,
                "open_at": str,
                "close_at": str,
                "spot_close_at": str,
                "futures_close_at": str,
            },
        )

        # å¦‚æœopen_at ä¸ºç©ºï¼Œåˆ™è®¾ç½®ä¸º created_at
        # df["open_at"] = df["open_at"].fillna(df["created_at"])
        del_column_names = ["created_at", "updated_at", "deleted_at"]
        # åªåˆ é™¤å­˜åœ¨çš„åˆ—
        columns_to_drop = [col for col in del_column_names if col in df.columns]
        df = df.drop(columns=columns_to_drop)

        # datetime_cols = ["open_at", "close_at", "spot_close_at", "futures_close_at"]
        # for col in datetime_cols:
        #     if col in df.columns:
        #         # å¤„ç†ä¸åŒçš„æ—¥æœŸæ—¶é—´æ ¼å¼ï¼Œä½¿ç”¨ format='mixed' è®© pandas è‡ªåŠ¨æ¨æ–­æ ¼å¼
        #         df[col] = pd.to_datetime(df[col], format="mixed")
        #         df[col] = dt_to_timestamp(df[col])

        r = get_redis_client()
        pipe = r.pipeline()  # å¯ç”¨ pipeline
        count = 0

        for _, row in df.iterrows():
            idx1 = row[id1]
            idx2 = row[id2]
            if not idx1 or not idx2:
                raise ValueError("ERR:idè¡Œæ— æ•ˆ")
            id = f"{idx1}_{idx2}"
            key = f"{key_prefix}:{id}"

            # è½¬æ¢è¡Œæ•°æ®ä¸ºå­—å…¸ï¼ˆå¤„ç† NaN ä¸º None æˆ–ç©ºå­—ç¬¦ä¸²ï¼‰
            row_dict = row.where(pd.notna(row), "").to_dict()

            # 1. å†™å…¥å®Œæ•´æ•°æ®åˆ° Hashï¼ˆè‡ªåŠ¨è¦†ç›–ï¼‰
            pipe.hset(key, mapping=row_dict)
            # 2. å†™å…¥ ZSet ç´¢å¼•ï¼šscore = Unix æ—¶é—´æˆ³
            pipe.zadd(f"by_time:{key_prefix}", {id: time.time()})
            count += 1

        await pipe.execute()
        logger.info(f"ğŸ§± to redis:ã€{count}ã€")


async def get_latest_n(key_prefix="bitget_grid", n=5000) -> List[Any]:
    ids = await r.zrevrange(f"by_time:{key_prefix}", 0, n - 1)
    if not ids:
        return []
    pipe = r.pipeline()
    for id in ids:
        pipe.hgetall(f"{key_prefix}:{id}")
    return await pipe.execute()


async def count_async(key_prefix="bitget_sf"):
    ids = await r.zrevrange(f"by_time:{key_prefix}", 0, -1)
    logger.info(f"è®°å½•æ•°:ã€{len(ids)}ã€")
    # âŒâŒ ã€ã€


@app.command()
def count(
    key_prefix: str = typer.Argument(
        "bitget_sf",
        help="redis zset keyå‰ç¼€",
    ),
):
    """
    æŸ¥è¯¢by_time:{key_prefix} redis zsetä¸­çš„è®°å½•æ•°

    :param key_prefix: ä¼ å…¥ä¼šè‡ªåŠ¨ç»„åˆä¸º by_time:{key_prefix}
    :type key_prefix: str

    """
    asyncio.run(count_async(key_prefix))


@app.command()
def csv2redis(
    id1: str,
    id2: str,
    kp: str = typer.Option(
        "bitget_sf",
        "--key-prefix",
        "-kp",
        help="å­˜å…¥redisçš„keyå‰ç¼€",
    ),
    fp: str = typer.Option(
        "d:/github/meme2046/data/bitget_sf_0.csv",
        "--file-path",
        "-fp",
        help="csvæ–‡ä»¶è·¯å¾„",
    ),
):
    """
    csv -> redis

    :param id1: id1çš„åˆ—å,ä¼šæŒ‰ç…§å½“å‰è®°å½•çš„id1ã€id2ç”Ÿæˆredis.key
    :type id1: str
    :param id2: id2çš„åˆ—å,ä¼šæŒ‰ç…§å½“å‰è®°å½•çš„id1ã€id2ç”Ÿæˆredis.key
    :type id2: str
    :param kp: è¯´æ˜
    :type kp: å†™å…¥redisçš„key_prefix
    :param fp: è¯´æ˜
    :type fp: csvæ–‡ä»¶è·¯å¾„
    """
    asyncio.run(
        csv_pd_redis(
            id1,
            id2,
            kp,
            fp,
        )
    )


@app.command()
def convert(
    fp: str = typer.Argument(
        "d:/github/meme2046/data/deprecated/bitget_sf_0.csv",
        help="csvæ–‡ä»¶è·¯å¾„",
    ),
):
    """
    csv -> csv
    ä¸»è¦æ˜¯å°†æ—¶é—´å­—ç¬¦ä¸²è½¬ä¸ºæ—¶é—´æˆ³,åˆ é™¤ä¸€äº›åˆ—

    :param fp: csvæ–‡ä»¶è·¯å¾„
    :type fp: str
    """
    if not os.path.exists(fp):
        logger.error(f"æ–‡ä»¶ä¸å­˜åœ¨:ã€{fp}ã€")
        return

    df: pd.DataFrame = pd.read_csv(
        fp,
        encoding="utf-8",
        dtype={
            "order_id": str,
            "fx_order_id": str,
            "spot_order_id": str,
            "futures_order_id": str,
            "spot_tracking_no": str,
            "futures_tracking_no": str,
        },
    )

    df["open_at"] = df["open_at"].fillna(df["created_at"])
    del_column_names = ["created_at", "updated_at", "deleted_at"]
    columns_to_drop = [col for col in del_column_names if col in df.columns]
    df = df.drop(columns=columns_to_drop)

    datetime_cols = [
        "created_at",
        "open_at",
        "close_at",
        "spot_close_at",
        "futures_close_at",
    ]
    for col in datetime_cols:
        if col in df.columns:
            # å¤„ç†ä¸åŒçš„æ—¥æœŸæ—¶é—´æ ¼å¼ï¼Œä½¿ç”¨ format='mixed' è®© pandas è‡ªåŠ¨æ¨æ–­æ ¼å¼
            df[col] = pd.to_datetime(df[col], format="mixed")
            df[col] = dt_to_timestamp(df[col])

    out_fp = gen_fp_with_suffix(fp, "tmp")

    df.to_csv(
        out_fp,
        mode="a",
        header=not os.path.exists(out_fp),
        index=False,
        encoding="utf-8",
    )
