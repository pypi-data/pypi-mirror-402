import os
import time

import pandas as pd
from dotenv import load_dotenv
from sqlalchemy import Engine, create_engine, text

from utils import logger
from utils.pd import deduplicated, dt_to_timestamp
from utils.pyredis import get_redis_client


def get_database_engine(env_path: str) -> Engine:
    """åˆ›å»ºæ•°æ®åº“å¼•æ“"""
    load_dotenv(env_path)
    host = os.getenv("MYSQL_HOST")
    port = os.getenv("MYSQL_PORT")
    database = os.getenv("MYSQL_DATABASE")
    user = os.getenv("MYSQL_USER")
    password = os.getenv("MYSQL_PASSWORD")

    engine = create_engine(
        f"mysql+pymysql://{user}:{password}@{host}:{port}/{database}",
        connect_args={
            # å¯¹äºMySQLï¼Œå¯ä»¥åœ¨è¿æ¥å‚æ•°ä¸­æŒ‡å®šæ—¶åŒº
            # "init_command": "SET time_zone='+00:00'",  # UTCæ—¶é—´
            # æˆ–è€…ä½¿ç”¨æœ¬åœ°æ—¶åŒºï¼Œä¾‹å¦‚ï¼š
            # "init_command": "SET time_zone='+08:00'",  # ä¸­å›½æ ‡å‡†æ—¶é—´
        },
    )

    try:
        with engine.connect() as connection:
            connection.execute(text("SELECT 1"))
    except Exception as e:
        logger.error(f"æ•°æ®åº“è¿æ¥å¤±è´¥: {str(e)}")
        raise

    return engine


def mysql_to_csv(
    engine: Engine,
    csv_path: str,
    table: str,
    query: str,
    update_status: int,
    d_column_names: list[str],
    pd_dtype: dict | None = None,
    del_column_names: list[str] = ["id"],
) -> int:
    # æŸ¥è¯¢æ•°æ®
    data_frame = pd.read_sql(query, engine, dtype=pd_dtype)
    # æå– 'id' åˆ—
    ids = data_frame["id"].tolist()
    # åˆ é™¤ä¸éœ€è¦çš„åˆ—
    data_frame = data_frame.drop(columns=del_column_names)

    # æ ¹æ® 'open_at' åˆ—é™åºæ’åº
    # data_frame = data_frame.sort_values(by="open_at", ascending=False)

    # å°†æ•°æ®è¿½åŠ å†™å…¥ CSV æ–‡ä»¶
    data_frame.to_csv(
        csv_path,
        mode="a",
        header=not os.path.exists(csv_path),
        index=False,
        encoding="utf-8",
    )
    # csvå»é‡,ä¿ç•™æœ€ååŠ å…¥çš„æ•°æ®
    deduplicated(csv_path, d_column_names, "last", pd_dtype)

    # æ ¹æ®æå–çš„ 'id' åˆ—æ›´æ–°æ•°æ®åº“ä¸­ up_status å­—æ®µ
    if ids:
        # ä½¿ç”¨ text() æ„å»ºæŸ¥è¯¢æ—¶ï¼Œç¡®ä¿ :ids æ˜¯ä¸€ä¸ªåˆ—è¡¨
        update_query = text(
            f"UPDATE {table} SET up_status = :status WHERE id IN ({','.join(map(str, ids))});"
        )
        with engine.connect() as connection:
            with connection.begin():
                result = connection.execute(
                    update_query,
                    {"status": update_status},
                )

                return result.rowcount

    return 0


async def mysql_to_redis_and_csv(
    engine: Engine,
    key_prefix: str,
    csv_fp: str,
    table: str,
    query: str,
    update_status: int,
    d_column_names: list[str],
    pd_dtype: dict | None = None,
    del_column_names: list[str] = ["id", "created_at", "updated_at", "deleted_at"],
) -> int:
    # æŸ¥è¯¢æ•°æ®
    df = pd.read_sql(query, engine, dtype=pd_dtype)
    df["open_at"] = df["open_at"].fillna(df["created_at"])
    # æå– 'id' åˆ—
    ids = df["id"].tolist()
    # åˆ é™¤ä¸éœ€è¦çš„åˆ—
    columns_to_drop = [col for col in del_column_names if col in df.columns]
    df = df.drop(columns=columns_to_drop)

    datetime_cols = ["open_at", "close_at", "spot_close_at", "futures_close_at"]

    logger.debug(df.head())
    logger.debug(df.dtypes)

    for col in datetime_cols:
        if col in df.columns:
            df[col] = dt_to_timestamp(df[col])
            # df[col] = dt_to_timestamp(pd.to_datetime(df[col], errors="coerce"))

    # æ•°æ®å†™å…¥redis
    r = get_redis_client()
    pipe = r.pipeline()  # å¯ç”¨ pipeline
    count = 0
    n1, n2 = d_column_names

    for _, row in df.iterrows():
        idx1 = row[n1]
        idx2 = row[n2]
        if not idx1 or not idx2:
            raise ValueError("ERR:idè¡Œæ— æ•ˆ")
        id = f"{idx1}_{idx2}"
        key = f"{key_prefix}:{id}"

        # è½¬æ¢è¡Œæ•°æ®ä¸ºå­—å…¸ï¼ˆå¤„ç† NaN ä¸º None æˆ–ç©ºå­—ç¬¦ä¸²ï¼‰
        row_dict = row.where(pd.notna(row), "").to_dict()

        # 1. å†™å…¥å®Œæ•´æ•°æ®åˆ° Hashï¼ˆè‡ªåŠ¨è¦†ç›–ï¼‰
        pipe.hset(key, mapping=row_dict)
        # 2. å†™å…¥ ZSet ç´¢å¼•ï¼šscore = Unix æ—¶é—´æˆ³
        pipe.zadd(f"by_time:{key_prefix}", {id: time.time()})
        count += 1

    await pipe.execute()
    logger.debug(f"ğŸ§± to redis: {count}")

    df.to_csv(
        csv_fp,
        mode="a",
        header=not os.path.exists(csv_fp),
        index=False,
        encoding="utf-8",
    )

    deduplicated(
        csv_fp,
        d_column_names,
        "last",
        pd_dtype={
            "order_id": str,
            "fx_order_id": str,
            "spot_order_id": str,
            "futures_order_id": str,
            "spot_tracking_no": str,
            "futures_tracking_no": str,
            "open_at": str,
            "close_at": str,
            "spot_close_at": str,
            "futures_close_at": str,
        },
    )

    logger.debug(f"ğ„œ to csv: {count}")

    # æ ¹æ®æå–çš„ 'id' åˆ—æ›´æ–°æ•°æ®åº“ä¸­ up_status å­—æ®µ
    if ids:
        # ä½¿ç”¨ text() æ„å»ºæŸ¥è¯¢æ—¶ï¼Œç¡®ä¿ :ids æ˜¯ä¸€ä¸ªåˆ—è¡¨
        update_query = text(
            f"UPDATE {table} SET up_status = :status WHERE id IN ({','.join(map(str, ids))});"
        )
        with engine.connect() as connection:
            with connection.begin():
                result = connection.execute(
                    update_query,
                    {"status": update_status},
                )

                return result.rowcount

    return 0
