# mypy: ignore-errors

import glob
import json
import ntpath
import tarfile
import tempfile
from importlib.util import find_spec
from pathlib import Path

import tensorflow as tf  # type: ignore
from code_loader.contract.mapping import NodeConnection, NodeMapping  # type: ignore
from keras import Model  # type: ignore
from keras_data_format_converter import convert_channels_first_to_last  # type: ignore
from leap_model_rebuilder import rebuild_model  # type: ignore
from onnx2kerastl import onnx_to_keras  # type: ignore
from onnx2kerastl.converter import ConvertedResponse  # type: ignore
from onnx2kerastl.customonnxlayer import onnx_custom_layers  # type: ignore

from onnx2kerastl.exceptions import OnnxUnsupported  # type: ignore
from tensorflow.keras.models import load_model  # type: ignore

from leap_model_parser.contract.graph import Node, InputInfo
from leap_model_parser.contract.importmodelresponse import ImportModelTypeEnum
from leap_model_parser.keras_json_model_import import KerasJsonModelImport
from leap_model_parser.leap_graph_editor import LeapGraphEditor  # type: ignore

from typing import Callable, Optional, List, Dict, Tuple, Type

onnx_imported = False
package_name = 'onnx'
spec = find_spec(package_name)
if spec is not None:
    import onnx  # type: ignore

    onnx_imported = True
#Deal with broken models
GIT_LFS_CONTENT = b"version https://git-lfs.github.com/spec/v1"

class InvalidModelFile(Exception):
    pass

def _is_git_lfs_pointer(path: str) -> bool:
    try:
        with open(path, "rb") as f:
            return f.read(128).startswith(GIT_LFS_CONTENT)
    except OSError:
        return False


def _text_snippet(path: str, n: int = 200) -> Optional[str]:
    try:
        with open(path, "rb") as f:
            b = f.read(n)
        if b"\x00" in b:
            return None
        s = b.decode("utf-8", errors="replace").strip()
        return s or None
    except OSError:
        return None


def _raise_model_file_error(path: str, kind: str, original_exc: Exception) -> None:
    suffix = Path(path).suffix
    if _is_git_lfs_pointer(path):
        raise InvalidModelFile(
            f"This {suffix} file is a Git LFS pointer, not the actual model binary. "
            "Run `git lfs pull` (or re-download the artifact) to fetch the real file."
        ) from original_exc
    snippet = _text_snippet(path)
    if snippet is not None:
        raise InvalidModelFile(
            f"The provided model is not a real {kind} model binary although its suffix is {suffix}. "
            f"File's content starts with:\n{snippet}."
        ) from original_exc
    raise InvalidModelFile(f"Failed to open {kind} model file: {original_exc}") from original_exc

class ModelParser:
    def __init__(self, should_transform_inputs_and_outputs=False,
                 custom_layers=None,
                 mapping_connections: Optional[List[NodeConnection]] = None):
        self._should_transform_inputs_and_outputs = should_transform_inputs_and_outputs
        self.custom_layers = custom_layers
        if custom_layers is None:
            self.custom_layers = {}

        self.custom_layers = {**self.custom_layers, **onnx_custom_layers}

        self.mapping_connections = mapping_connections

        self._model_types_converter = {
            ImportModelTypeEnum.JSON_TF2.value: self.convert_json_model,
            ImportModelTypeEnum.H5_TF2.value: self.convert_h5_model,
            ImportModelTypeEnum.ONNX.value: self.convert_onnx_model,
            ImportModelTypeEnum.PB_TF2.value: self.convert_pb_model,
        }

    @staticmethod
    def _add_output_node_shape_to_model_schema(model_schema: Dict, keras_model: Model):
        for i, layer in enumerate(keras_model.layers):
            model_schema['config']['layers'][i]['output_shape'] = list(layer.output_shape)

    def get_keras_model_and_model_graph(
            self, model_path: Path, model_type: ImportModelTypeEnum) -> Tuple[
        Dict[str, Node], List[InputInfo], Optional[Model], Optional[str]]:
        model_to_keras_converter: Optional[Callable[[str], Tuple[Dict[str, Node], Model, Optional[str]]]] = \
            self._model_types_converter.get(model_type.value)
        if model_to_keras_converter is None:
            raise Exception(
                f"Unable to import external version, {str(model_path)} file format isn't supported")

        file_path = str(model_path)
        model_schema, keras_model_with_weights, error_info = model_to_keras_converter(file_path)

        self._add_output_node_shape_to_model_schema(model_schema, keras_model_with_weights)

        model_generator = KerasJsonModelImport(self.custom_layers)

        keras_model = keras_model_with_weights
        if keras_model is None:
            keras_model = tf.keras.models.model_from_json(json.dumps(model_schema))
        layer_name_to_inbound_nodes = {
            layer.name: layer.inbound_nodes
            for layer in keras_model.layers
        }

        graph, connected_inputs = model_generator.generate_graph(
            model_schema, layer_name_to_inbound_nodes)
        # make sure input order is kept with original model
        input_list = []
        for inp in keras_model.inputs:
            name = inp.name
            name = name.replace(".", "_")
            for inp_graph in connected_inputs:
                if inp_graph.name == name:
                    input_list.append(inp_graph)
        if self.mapping_connections is not None:
            leap_graph_editor = LeapGraphEditor(graph, keras_model_with_weights)
            leap_graph_editor.add_connections_to_graph(self.mapping_connections)

        return graph, input_list, keras_model_with_weights, error_info

    def _get_k_model_from_pb_path(self, file_path: str):
        tar_file = tarfile.open(file_path)
        with tempfile.TemporaryDirectory() as temp_dir:
            tar_file.extractall(temp_dir)
            pb_files = glob.glob(temp_dir + "/**/*.pb", recursive=True)
            if len(pb_files) == 0:
                raise Exception('no pb files were found')

            pb_file_path = next(iter(pb_files))
            pb_folder_path = next(iter(ntpath.split(pb_file_path)))

            k_model = self._load_keras_model_with_custom_layers(pb_folder_path)
        return k_model

    def generate_model_graph(self, model_path: Path, model_type: ImportModelTypeEnum) -> Tuple[
        Dict[str, Node], List[InputInfo]]:
        model_graph, connected_inputs, _, error_info = self.get_keras_model_and_model_graph(
            model_path, model_type)
        return model_graph, connected_inputs

    @classmethod
    def convert_json_model(cls, file_path: str) -> Tuple[Dict[str, Node], None, Optional[str]]:
        with open(file_path, 'r') as f:
            model_schema = json.load(f)
        return model_schema, None, None

    def convert_pb_model(self, file_path: str) -> Tuple[Dict[str, Node], Model, Optional[str]]:
        k_model = self._get_k_model_from_pb_path(file_path)
        return self.convert_to_keras_model(k_model)

    def convert_onnx_model(self, file_path: str) -> Tuple[Dict[str, Node], Model, Optional[str]]:
        if not onnx_imported:
            raise OnnxUnsupported()

        try:
            onnx_model = onnx.load_model(file_path)
        except Exception as e:
            print("yam's message onnx")
            _raise_model_file_error(file_path, "onnx", e)
        input_all = [_input.name for _input in onnx_model.graph.input]
        input_initializer = [
            node.name for node in onnx_model.graph.initializer]
        input_names = [name for name in input_all if name not in input_initializer]
        converted_response: ConvertedResponse = onnx_to_keras(onnx_model, input_names=input_names,
                                                              name_policy='attach_weights_name',
                                                              allow_partial_compilation=False)
        return self.convert_to_keras_model(converted_response.converted_model, converted_response.error_info)

    def _load_keras_model_with_custom_layers(self, file_path: str):
        custom_objects = {}
        if self.custom_layers is not None:
            custom_objects = self.custom_layers

        try:
            return load_model(file_path, custom_objects=custom_objects, compile=False)
        except OSError as e:
            if "signature" in str(e):
                print("yam's message h5")
                _raise_model_file_error(file_path, "keras", e)
            raise

    def convert_h5_model(self, file_path: str) -> Tuple[Dict[str, Node], Model, Optional[str]]:
        imported_model = self._load_keras_model_with_custom_layers(file_path)
        return self.convert_to_keras_model(imported_model)

    def convert_to_keras_model(self, k_model, error_info: Optional[str] = None) -> Tuple[
        Dict[str, Node], Model, Optional[str]]:
        converted_k_model = convert_channels_first_to_last(
            k_model, self._should_transform_inputs_and_outputs, self.custom_layers)

        from keras.saving.legacy.saved_model import json_utils  # type: ignore
        import numpy as np

        _orig_get_json_type = json_utils.get_json_type

        def _patched_get_json_type(obj):  # type: ignore
            # Handle numpy dtype explicitly
            if isinstance(obj, np.dtype):
                return obj.name  # e.g. "int64"
            # Make sure common numpy scalars/containers are handled robustly
            if isinstance(obj, np.integer):
                return int(obj)
            if isinstance(obj, np.floating):
                return float(obj)
            if isinstance(obj, np.bool_):
                return bool(obj)
            if isinstance(obj, np.ndarray):
                return obj.tolist()
            return _orig_get_json_type(obj)

        json_utils.get_json_type = _patched_get_json_type

        model_schema = json.loads(converted_k_model.to_json())

        return model_schema, converted_k_model, error_info
