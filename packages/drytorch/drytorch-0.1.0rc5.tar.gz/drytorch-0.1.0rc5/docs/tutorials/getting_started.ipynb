{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Getting Started with DRYTorch\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/nverchev/drytorch/blob/main/docs/tutorials/getting_started.ipynb)\n",
    "\n",
    "This notebook demonstrates how to use the DRYTorch library for organizing machine learning experiments.\n",
    "\n",
    "## Deep Prior Upsampling\n",
    "\n",
    "We will perform image super-resolution (super-resolution) using a technique called Deep Image Priors (DIP).\n",
    "\n",
    "The approach is as follows: We train the network to minimize the difference between its downsampled output and the low-resolution target image. The network should be able to generalize this training objective and reconstruct the high-resolution detail. We then evaluate its output against the original image, using simple bilinear interpolation of the low-resolution version as a non-learning baseline for comparison.\n",
    "\n",
    "\n",
    "### Requirements\n",
    "DRYTorch’s only mandatory dependencies are `numpy` and `torch`. For enhanced functionality and a smoother experience, it is recommended to install `PyYAML` and `tqdm`. The library requires Python 3.11 or newer. This tutorial does not rely on any additional optional dependencies.\n",
    "\n",
    "The cell below uses [`uv`](https://docs.astral.sh/uv/) as package manager."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "! uv pip install drytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "### Download the data\n",
    "\n",
    "Download the image data from the GitHub repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "from io import BytesIO\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "url = 'https://raw.githubusercontent.com/nverchev/drytorch/main/docs/tutorials/data/flower.npy'\n",
    "\n",
    "with urllib.request.urlopen(url) as response:  # noqa: S310\n",
    "    data_bytes = response.read()\n",
    "\n",
    "flower_np = np.load(BytesIO(data_bytes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "### Preprocess the image\n",
    "The target image has a resolution of 128 x 128. We downsample it to 64 x 64 using bilinear interpolation and use it again to upsample back to the original resolution to create a baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "TARGET_SIZE = 128\n",
    "DOWNSAMPLED_SIZE = 64\n",
    "\n",
    "\n",
    "def interpolate(image_torch: torch.Tensor, target_size: int) -> torch.Tensor:\n",
    "    \"\"\"Performs an interpolation on an image array.\n",
    "\n",
    "    Args:\n",
    "        image_torch: a NumPy array of an image.\n",
    "        target_size: resulting height and width\n",
    "\n",
    "    Returns:\n",
    "        A torch tensor of shape (batch, target_size, target_size).\n",
    "    \"\"\"\n",
    "    return torch.nn.functional.interpolate(\n",
    "        image_torch,\n",
    "        size=(target_size, target_size),\n",
    "        mode='bilinear',\n",
    "    )\n",
    "\n",
    "\n",
    "flower_torch = torch.from_numpy(flower_np).permute(2, 0, 1).float()\n",
    "flower_torch /= 255\n",
    "flower_torch = interpolate(flower_torch.unsqueeze(0), TARGET_SIZE).squeeze()\n",
    "flower_torch_downsampled = interpolate(\n",
    "    flower_torch.unsqueeze(0), DOWNSAMPLED_SIZE\n",
    ").squeeze()\n",
    "baseline_reconstruction = interpolate(\n",
    "    flower_torch_downsampled.unsqueeze(0), TARGET_SIZE\n",
    ").squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "Let us visualize the image we want to reproduce and the baseline\n",
    "reconstruction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "from IPython.display import display as ip_display\n",
    "from PIL import Image as Image\n",
    "\n",
    "\n",
    "def display_tensor_images(named_images: dict[str, torch.Tensor]) -> None:\n",
    "    \"\"\"Display a PyTorch image tensor.\n",
    "\n",
    "    Args:\n",
    "        named_images: list of tensors with dimensions (channels, height, width).\n",
    "    \"\"\"\n",
    "    title_html = ''\n",
    "\n",
    "    for image_name in named_images:\n",
    "        title_html += f\"\"\"\n",
    "        <div style=\"text-align: center; width: 256px; font-size: 2em;\">\n",
    "        {image_name}\n",
    "        </div>\"\"\"\n",
    "\n",
    "    ip_display(HTML(f\"\"\"<div style=\" display: flex;\">{title_html}</div>\"\"\"))\n",
    "    image = torch.cat(list(named_images.values()), dim=2)\n",
    "    image = torch.nn.functional.interpolate(\n",
    "        image.unsqueeze(0),\n",
    "        size=(256, 256 * len(named_images)),\n",
    "        mode='bilinear',\n",
    "    ).squeeze()\n",
    "\n",
    "    np_array = image.detach().cpu().permute(1, 2, 0).numpy().copy()\n",
    "    np_array_int = (np_array * 255).astype(np.uint8)\n",
    "    ip_display(Image.fromarray(np_array_int))\n",
    "    return\n",
    "\n",
    "\n",
    "display_tensor_images(\n",
    "    {'Original': flower_torch, 'Baseline': baseline_reconstruction}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## Set up the experiment\n",
    "\n",
    "In DRYTorch, an experiment is a reproducible execution entirely defined by its configuration file. A run is a session that you can later resume. Multiple runs of the same experiment are intended for reproducibility and debugging.\n",
    "\n",
    "### Define the Specification\n",
    "\n",
    "DRYTorch supports scope-dependent configuration, meaning the experiment configuration is globally available during the experimental run. This design avoids the need to explicitly pass the configuration as a parameter and ensures it is reliably aligned with the current experiment.\n",
    "\n",
    "For improved type checking and auto-completion, it is recommended to subclass the base experiment class and explicitly integrate your specific configuration structure.\n",
    "\n",
    "\n",
    "*Tip: use Pydantic dataclasses to validate your settings.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataclasses as dataclasses\n",
    "import os\n",
    "\n",
    "from drytorch import Experiment\n",
    "\n",
    "\n",
    "@dataclasses.dataclass(frozen=True)\n",
    "class ArchitecturalSettings:\n",
    "    \"\"\"Architectural settings for a Resnet with three stages.\n",
    "\n",
    "    Attributes:\n",
    "        norm_flag: whether to use instance normalization.\n",
    "        n_blocks_per_stage: number of convolutional blocks in the three stages.\n",
    "        max_width: number of channels of the middle stage (half in the others).\n",
    "    \"\"\"\n",
    "\n",
    "    norm_flag: bool = False\n",
    "    n_blocks_per_stage: tuple[int, int, int] = (2, 2, 2)\n",
    "    max_width: int = 32\n",
    "\n",
    "\n",
    "@dataclasses.dataclass(frozen=True)\n",
    "class TrainingSettings:\n",
    "    \"\"\"Training settings for the current experiment.\n",
    "\n",
    "    Attributes:\n",
    "        lr: learning rate.\n",
    "        n_epochs: number of training epochs.\n",
    "    \"\"\"\n",
    "\n",
    "    lr: float = 0.001\n",
    "    n_epochs: int = int(os.getenv('EPOCHS', '30'))\n",
    "\n",
    "\n",
    "@dataclasses.dataclass(frozen=True)\n",
    "class AllSettings:\n",
    "    \"\"\"General settings for the current experiment.\n",
    "\n",
    "    Attributes:\n",
    "        architecture: number of training epochs.\n",
    "        train: how many samples in each mini-batch\n",
    "    \"\"\"\n",
    "\n",
    "    architecture = ArchitecturalSettings()\n",
    "    train = TrainingSettings()\n",
    "\n",
    "\n",
    "class UpsamplingExperiment(Experiment[AllSettings]):\n",
    "    \"\"\"Class for upsampling experiments.\"\"\"\n",
    "\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "### Create an experiment\n",
    "\n",
    "Together with the configuration, you can assign a name and a directory to the experiment, which will determine where the checkpoints, the metadata, and the logs will be located."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "\n",
    "settings = AllSettings()\n",
    "experiment = UpsamplingExperiment(\n",
    "    settings, name='Getting Started', par_dir=pathlib.Path('experiments')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "### Select the trackers\n",
    "\n",
    "Trackers are responsible for logging and plotting but have no impact on the experiment's computation. By default, DRYTorch provides trackers for logging, displaying a progress bar, and storing metadata.\n",
    "\n",
    "Here, we want to save the training and test results in `.csv` files, so we add the `CSVDumper` to the default ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from drytorch.trackers.csv import CSVDumper\n",
    "\n",
    "\n",
    "experiment.trackers.subscribe(CSVDumper())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "### Create the run and start the experiment\n",
    "It is recommended to create a new run or resume a previous one using the `create_run` method.\n",
    "\n",
    "The Run instance works best as a context manager:\n",
    "\n",
    "```python\n",
    "with experiment.create_run():\n",
    "    execute_ml_op()\n",
    "\n",
    "```\n",
    "\n",
    "However, in notebooks, starting the run procedurally is more practical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "run = experiment.create_run()\n",
    "run.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "## Custom classes\n",
    "The DRYTorch philosophy prioritizes maximum flexibility, allowing you to write custom classes and structured data for your task while taking full advantage of type annotations. The classes should have as few arguments as possible and rely on the global configuration for their instantiation.\n",
    "\n",
    "\n",
    "### Define Data Structures\n",
    "\n",
    "Following its philosophy, DRYTorch supports structured inputs and outputs for\n",
    " your model, as well as targets for your loss. This is particularly helpful\n",
    " for unstructured data of different inputs (e.g., modalities) or targets.\n",
    "\n",
    "The structure for inputs and targets must be compatible with the default PyTorch `collate_fn`. This means it can be a `torch.Tensor`, a list of tensors, or a namedtuple of tensors. Outputs can have an arbitrary type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "class Inputs(NamedTuple):\n",
    "    \"\"\"Structured input.\n",
    "\n",
    "    Deep priors do not use any input, so this class is empty\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "class TrainingTargets(NamedTuple):\n",
    "    \"\"\"Structured target for training.\n",
    "\n",
    "    Attributes:\n",
    "        flower_downsampled: flower downsampled 2 to 1.\n",
    "    \"\"\"\n",
    "\n",
    "    flower_downsampled: torch.Tensor\n",
    "\n",
    "\n",
    "class TestTargets(NamedTuple):\n",
    "    \"\"\"Structured target for testing.\n",
    "\n",
    "    Attributes:\n",
    "        flower_original: the original flower array.\n",
    "    \"\"\"\n",
    "\n",
    "    flower_original: torch.Tensor\n",
    "\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class Outputs:\n",
    "    \"\"\"Structured target.\n",
    "\n",
    "    Attributes:\n",
    "        flower_reconstructed: reconstructed array of the flower image.\n",
    "    \"\"\"\n",
    "\n",
    "    flower_reconstructed: torch.Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "### Create the Datasets\n",
    "The test and training datasets consist of a single image. In the test dataset, the image is the one we want to reconstruct, while in the training dataset, the image is its downsampled version.\n",
    "To better illustrate training results and the functioning of DRYTorch, we define a training epoch as a fixed number of iterations over the given image.\n",
    "\n",
    "\n",
    "DRYTorch expects an (annotated) instance of `torch.utils.data.Dataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from torch.utils import data\n",
    "\n",
    "\n",
    "class TestDataset(data.Dataset[tuple[Inputs, TestTargets]]):\n",
    "    \"\"\"This dataset evaluates the reconstruction of the image.\n",
    "\n",
    "    Attributes:\n",
    "        target_image: the image to reconstruct\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, target_image: torch.Tensor) -> None:\n",
    "        \"\"\"Constructor.\n",
    "\n",
    "        Args:\n",
    "            target_image: the image to reconstruct\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.target_image = target_image\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Number of samples to test.\"\"\"\n",
    "        return 1\n",
    "\n",
    "    def __getitem__(self, _) -> tuple[Inputs, TestTargets]:\n",
    "        \"\"\"Method to overwrite to define the batch.\n",
    "\n",
    "        Returns:\n",
    "            Tuple with Inputs and Targets instances.\n",
    "        \"\"\"\n",
    "        inputs = Inputs()\n",
    "        targets = TestTargets(flower_original=self.target_image)\n",
    "        return inputs, targets\n",
    "\n",
    "\n",
    "class TrainingDataset(data.Dataset[tuple[Inputs, TrainingTargets]]):\n",
    "    \"\"\"This dataset trains the network to reconstruct the image.\n",
    "\n",
    "    Attributes:\n",
    "        downsampled_image: the image to upsample.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, downsampled_image: torch.Tensor) -> None:\n",
    "        \"\"\"Constructor.\n",
    "\n",
    "        Args:\n",
    "            downsampled_image: the image to upsample.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.downsampled_image = downsampled_image\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Number of iterations per epoch.\"\"\"\n",
    "        return 32\n",
    "\n",
    "    def __getitem__(self, _) -> tuple[Inputs, TrainingTargets]:\n",
    "        \"\"\"Method to overwrite to define the batch.\n",
    "\n",
    "        Returns:\n",
    "            tuple with Inputs and Targets instances.\n",
    "        \"\"\"\n",
    "        inputs = Inputs()\n",
    "        targets: TrainingTargets = TrainingTargets(\n",
    "            flower_downsampled=self.downsampled_image\n",
    "        )\n",
    "        return inputs, targets\n",
    "\n",
    "\n",
    "training_dataset = TrainingDataset(flower_torch_downsampled)\n",
    "test_dataset = TestDataset(flower_torch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "## Create the Network\n",
    "\n",
    "This implementation of a Deep Prior network uses an internal fixed parameter as the input to a residual convolutional network.\n",
    "\n",
    "You define the PyTorch Module for your model as usual, defining (and annotating) the forward method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "class ResNetBlock(nn.Module):\n",
    "    \"\"\"The fundamental residual block for ResNet.\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels: int, out_channels: int) -> None:\n",
    "        \"\"\"Initiates two convolution and the skip connection.\"\"\"\n",
    "        super().__init__()\n",
    "        cfg_arc = UpsamplingExperiment.get_config().architecture\n",
    "        norm_flag = cfg_arc.norm_flag\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels, out_channels, kernel_size=3, padding=1, bias=False\n",
    "            ),\n",
    "            nn.LayerNorm(out_channels) if norm_flag else nn.Identity(),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(\n",
    "                out_channels,\n",
    "                out_channels,\n",
    "                kernel_size=3,\n",
    "                padding=1,\n",
    "                bias=False,\n",
    "            ),\n",
    "            nn.InstanceNorm2d(out_channels) if norm_flag else nn.Identity(),\n",
    "        )\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(\n",
    "                    in_channels,\n",
    "                    out_channels,\n",
    "                    kernel_size=1,\n",
    "                    bias=False,\n",
    "                ),\n",
    "                nn.LayerNorm(out_channels) if norm_flag else nn.Identity(),\n",
    "            )\n",
    "        self.final_activation = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Passes the input through the net, adding the residual connection.\"\"\"\n",
    "        out = self.net(x) + self.shortcut(x)\n",
    "        out = self.final_activation(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNetStage(nn.Sequential):\n",
    "    \"\"\"A single stage of the ResNet architecture.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, in_channels: int, out_channels: int, num_blocks: int\n",
    "    ) -> None:\n",
    "        \"\"\"Initializes a stage with the required number of blocks.\"\"\"\n",
    "        layers: list[nn.Module] = []\n",
    "        current_channels = in_channels\n",
    "        for _ in range(num_blocks):\n",
    "            layers.append(ResNetBlock(current_channels, out_channels))\n",
    "            current_channels = out_channels\n",
    "        super().__init__(*layers)\n",
    "\n",
    "\n",
    "class ResNet(nn.Sequential):\n",
    "    \"\"\"A ResNet model for image reconstruction.\"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        \"\"\"Initializes the entire ResNet structure.\"\"\"\n",
    "        cfg_arc = UpsamplingExperiment.get_config().architecture\n",
    "        norm_flag = cfg_arc.norm_flag\n",
    "        max_width = cfg_arc.max_width\n",
    "        num_blocks = cfg_arc.n_blocks_per_stage\n",
    "\n",
    "        layers: list[nn.Module] = [\n",
    "            nn.Conv2d(16, max_width // 2, kernel_size=3, padding=1, bias=False),\n",
    "            nn.LayerNorm(max_width // 2) if norm_flag else nn.Identity(),\n",
    "            nn.ReLU(inplace=True),\n",
    "            ResNetStage(max_width // 2, max_width, num_blocks[0]),\n",
    "            ResNetStage(max_width, max_width, num_blocks[1]),\n",
    "            ResNetStage(max_width, max_width // 2, num_blocks[2]),\n",
    "            nn.Conv2d(max_width // 2, 3, kernel_size=3, padding=1),\n",
    "            nn.Sigmoid(),\n",
    "        ]\n",
    "        super().__init__(*layers)\n",
    "\n",
    "\n",
    "class DeepPriorNet(nn.Module):\n",
    "    \"\"\"A net with a fixed input parameter.\"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        \"\"\"Instantiate the net and the fixed input.\"\"\"\n",
    "        super().__init__()\n",
    "        self.fixed_input = torch.nn.Parameter(\n",
    "            torch.randn(1, 16, TARGET_SIZE, TARGET_SIZE)\n",
    "        )\n",
    "        self.net = ResNet()\n",
    "\n",
    "    def forward(self, _: Inputs) -> Outputs:\n",
    "        \"\"\"Call the net on the fixed input.\"\"\"\n",
    "        return Outputs(self.net(self.fixed_input))\n",
    "\n",
    "\n",
    "network = DeepPriorNet()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "### Define the loss and metric functions\n",
    "\n",
    "In this experiment, we will use the Mean Squared Error (MSE) for training and the Peak Signal-to-Noise Ratio (PSNR) for evaluating the super-resolution image. The bilinear interpolation baseline has a PSNR of 25.90.\n",
    "\n",
    "DRYTorch expects loss and metric functions that take outputs and targets as arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def psnr(recon: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Compute Peak Signal-to-Noise Ratio between two images ([0, 1] range).\"\"\"\n",
    "    mse = nn.functional.mse_loss(recon, target)\n",
    "\n",
    "    if mse == 0:\n",
    "        return torch.tensor(torch.inf)\n",
    "\n",
    "    return -10 * torch.log10(mse)\n",
    "\n",
    "\n",
    "def target_psnr(outputs: Outputs, targets: TestTargets) -> torch.Tensor:\n",
    "    \"\"\"Mean Square Error in the target space.\"\"\"\n",
    "    return psnr(outputs.flower_reconstructed, targets.flower_original)\n",
    "\n",
    "\n",
    "def downsampled_mse(outputs: Outputs, targets: TrainingTargets) -> torch.Tensor:\n",
    "    \"\"\"Mean Square Error in the downsampled space.\"\"\"\n",
    "    reconstruction_downsampled = interpolate(\n",
    "        outputs.flower_reconstructed, DOWNSAMPLED_SIZE\n",
    "    )\n",
    "    return nn.functional.mse_loss(\n",
    "        reconstruction_downsampled, targets.flower_downsampled\n",
    "    )\n",
    "\n",
    "\n",
    "baseline_psnr = psnr(baseline_reconstruction, flower_torch).item()\n",
    "\n",
    "if not np.isclose(baseline_psnr, 25.90, atol=1e-2):\n",
    "    raise AssertionError('Baseline should value should be about 25.90.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "## Library implementations\n",
    "\n",
    "\n",
    "DRYTorch provides default implementations for the core components of a\n",
    "typical ML workflow. Below, we briefly summarize the main functionalities:\n",
    "\n",
    "- **DataLoader:**\n",
    "  A data loader with “smart loading” behavior that adapts automatically when\n",
    "  PyTorch switches to evaluation mode.\n",
    "\n",
    "- **Model:**\n",
    "  A wrapper around `torch.nn.Module` that stores additional metadata and allows\n",
    "  saving and loading model state from disk.\n",
    "\n",
    "- **LearningSchema:**\n",
    "  A dataclass that encapsulates advanced optimizer settings, schedulers, and\n",
    "  gradient operations. It includes convenient methods for standard\n",
    "  configurations.\n",
    "\n",
    "- **Loss** and **Metric:**\n",
    "  Classes that wrap loss and metric functions. They support operator\n",
    "  overloading for combining multiple losses/metrics and provide automatic\n",
    "  self-documentation.\n",
    "\n",
    "- **Test:**\n",
    "  A class for evaluating model performance on a test dataset. It reports\n",
    "  metric values to registered trackers and can optionally store outputs for\n",
    "  qualitative analysis.\n",
    "\n",
    "- **Trainer:**\n",
    "  Trains a model according to a given learning schema and optionally validates\n",
    "  it on a validation dataset. It supports callbacks executed before and after\n",
    "  each epoch and can save/load checkpoints (including module and optimizer\n",
    "  states).\n",
    "\n",
    "**Note:** If the type annotations of the DataLoader, Model, and Loss/Metric are\n",
    "incompatible, the type checker will raise an error when instantiating the\n",
    "`Trainer` or `Test` classes.\n",
    "\n",
    "### Train the model\n",
    "We perform a basic training cycle using the library implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from drytorch import DataLoader, LearningSchema, Loss, Model, Trainer\n",
    "\n",
    "\n",
    "cfg_train = UpsamplingExperiment.get_config().train\n",
    "train_loader = DataLoader(training_dataset, batch_size=1)\n",
    "\n",
    "model = Model(network, name='DeepPriorNet')\n",
    "learning_schema = LearningSchema.adam(cfg_train.lr)\n",
    "loss = Loss(downsampled_mse, 'MSE')\n",
    "trainer = Trainer(\n",
    "    model, loader=train_loader, loss=loss, learning_schema=learning_schema\n",
    ")\n",
    "trainer.train(cfg_train.n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from drytorch import Metric, Test\n",
    "\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=1)\n",
    "metric = Metric(target_psnr, 'PSNR Target', False)\n",
    "test = Test(model, loader=test_loader, metric=metric)\n",
    "test(store_outputs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "### Qualitative results\n",
    "We get the stored output with the deep prior reconstruction from the previous test and visualize it side by side with the original image and the baseline reconstruction.\n",
    "\n",
    "Visually, the deep prior image should be able to recover some of the high-frequency details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructed = test.outputs_list[0].flower_reconstructed.squeeze()\n",
    "display_tensor_images(\n",
    "    {\n",
    "        'Original': flower_torch,\n",
    "        'Baseline': baseline_reconstruction,\n",
    "        'Deep Prior': reconstructed,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "## End the experiment\n",
    "### Documentation\n",
    "Documentation is stored both on disk and through the selected trackers.\n",
    "\n",
    "- Basic run information is saved in the `.drytorch` folder.\n",
    "- The `metadata` folder contains a readable representation extracted from the\n",
    "  Model, Trainer, and Test classes, documenting their attributes recursively (only if you have PyAML installed).\n",
    "- The `csv` folder stores a dump of all metrics produced by the Trainer and\n",
    "  Test classes.\n",
    "\n",
    "### Stopping the run\n",
    "To rerun this notebook a second time and start a new run, you must first stop the\n",
    "current one (this happens automatically if you use run as a context manager).\n",
    "Stopping the run also takes care of cleaning up resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "run.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "# Data Distributed Training\n",
    "\n",
    "DRYTorch supports data-distributed processing. DRYTorch does not provide a\n",
    "default setting for data-distributed training but lets the user define the\n",
    "data-parallelism strategy.\n",
    "\n",
    "The user needs to set up the distributed training environment and indicate\n",
    "explicitly set the device for each process. DRYTorch library implementation\n",
    "will do the rest (distributed loading, consistent checkpointing,\n",
    "parallelized model, synchronized metrics, and deduplicated logging).\n",
    "\n",
    "For more advanced distributed training strategies (model sharding,\n",
    "data parallelism + data-distributed parallelism, etc.), it is recommended\n",
    "to subclass DRYTorch's `Model` and `LocalCheckpoint` classes.\n",
    "\n",
    "## Define train worker:\n",
    "The pipeline for distributed training is the same as before.\n",
    "\n",
    "To set up and clean up the distributed environment, see the [official tutorial]\n",
    "(https://pytorch.org/tutorials/intermediate/ddp_tutorial.html).\n",
    "\n",
    " To see how to extend distributed support to metrics from `torchmetrics` and\n",
    " `torcheval`, see the `metrics_and_losses` tutorial.\n",
    "\n",
    "```python\n",
    "\n",
    "def train_and_eval() -> None:\n",
    "    \"\"\"Train the model in distributed mode.\"\"\"\n",
    "    ddp_cfg_train = UpsamplingExperiment.get_config().train\n",
    "    ddp_training_dataset = TrainingDataset(flower_torch_downsampled)\n",
    "    ddp_train_loader = DataLoader(ddp_training_dataset, batch_size=1)\n",
    "    ddp_test_dataset = TestDataset(flower_torch)\n",
    "    ddp_network = DeepPriorNet()\n",
    "    ddp_model = Model(ddp_network, name='DeepPriorNet')\n",
    "    ddp_learning_schema = LearningSchema.adam(ddp_cfg_train.lr)\n",
    "    ddp_loss = Loss(downsampled_mse, 'MSE downsampled')\n",
    "    ddp_trainer = Trainer(\n",
    "        ddp_model,\n",
    "        loader=ddp_train_loader,\n",
    "        loss=ddp_loss,\n",
    "        learning_schema=ddp_learning_schema,\n",
    "    )\n",
    "    ddp_trainer.train(ddp_cfg_train.n_epochs)\n",
    "    ddp_test_loader = DataLoader(ddp_test_dataset, batch_size=1)\n",
    "    ddp_metric = Metric(target_psnr, 'PSNR Target', False)\n",
    "    ddp_test = Test(ddp_model, loader=ddp_test_loader, metric=ddp_metric)\n",
    "    ddp_test(store_outputs=True)\n",
    "    return\n",
    "\n",
    "\n",
    "def worker(rank: int, world_size: int) -> None:\n",
    "    \"\"\"Wrapper for distributed training.\n",
    "\n",
    "    Args:\n",
    "        rank: rank of the current process.\n",
    "        world_size: total number of processes.\n",
    "    \"\"\"\n",
    "    setup(rank=rank, world_size=world_size)\n",
    "    ddp_experiment = UpsamplingExperiment(\n",
    "        settings,\n",
    "        name='Data Distributed Training',\n",
    "        par_dir=pathlib.Path('experiments'),\n",
    "    )\n",
    "    ddp_experiment.trackers.subscribe(CSVDumper())\n",
    "    try:\n",
    "        with ddp_experiment.create_run():\n",
    "            train_and_eval()\n",
    "    finally:\n",
    "        cleanup()\n",
    "```\n",
    "\n",
    "Note that the previous code will not work in a notebook environment."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "default_lexer": "ipython3",
   "formats": "ipynb,md:myst"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
