# General
AGENTMAKE_USERNAME=
AGENTMAKE_ASSISTANT_NAME=
DEVELOPER_MODE=FALSE
DEFAULT_AI_BACKEND=
# DEFAULT_SYSTEM_MESSAGE could be set to `auto`, `reasoning`, a pre-defined tag or a custom string
DEFAULT_SYSTEM_MESSAGE=
DEFAULT_FOLLOW_UP_PROMPT="Ask a follow-up question and answer it."
DEFAULT_OPEN_COMMAND=
DEFAULT_TEXT_EDITOR=etextedit
DEFAULT_REFINE_INSTRUCTION=
DEFAULT_CUSTOM_LABEL=
# e.g. search/searxng is a better alternative, but extra setup is needed
DEFAULT_ONLINE_SEARCH_TOOL=search/google
DEFAULT_MAXIMUM_ONLINE_SEARCHES=5
DEFAULT_FABRIC_PATTERNS_PATH=
# Markdown theme option['abap', 'algol', 'algol_nu', 'arduino', 'autumn', 'bw', 'borland', 'coffee', 'colorful', 'default', 'dracula', 'emacs', 'friendly_grayscale', 'friendly', 'fruity', 'github-dark', 'gruvbox-dark', 'gruvbox-light', 'igor', 'inkpot', 'lightbulb', 'lilypond', 'lovelace', 'manni', 'material', 'monokai', 'murphy', 'native', 'nord-darker', 'nord', 'one-dark', 'paraiso-dark', 'paraiso-light', 'pastie', 'perldoc', 'rainbow_dash', 'rrt', 'sas', 'solarized-dark', 'solarized-light', 'staroffice', 'stata-dark', 'stata-light', 'tango', 'trac', 'vim', 'vs', 'xcode', 'zenburn']
DEFAULT_MARKDOWN_THEME="github-dark"

# Backend: anthropic
ANTHROPIC_API_KEY=
ANTHROPIC_MODEL=claude-sonnet-4-5
ANTHROPIC_TEMPERATURE=0.3
ANTHROPIC_MAX_TOKENS=64000

# Backend: azure_openai
# Deploy AI models with Azure service: https://ai.azure.com/github
# use Azure OpenAI Service endpoint for running OpenAI models; the endpoint should look like https://{resource-id}.openai.azure.com/
AZURE_OPENAI_API_VERSION=2024-10-21
AZURE_OPENAI_API_KEY=
AZURE_OPENAI_API_ENDPOINT=
AZURE_OPENAI_MODEL=gpt-5.2
AZURE_OPENAI_TEMPERATURE=0.3
AZURE_OPENAI_MAX_TOKENS=128000

# Backend: azure_anthropic
# use Azure Service endpoint for running Claude models; check https://ai.azure.com/github
# API endpoint should look like https://{resource-id}.services.ai.azure.com/anthropic/
AZURE_ANTHROPIC_API_VERSION=2024-10-21
AZURE_ANTHROPIC_API_KEY=
AZURE_ANTHROPIC_API_ENDPOINT=
AZURE_ANTHROPIC_MODEL=claude-sonnet-4-5
AZURE_ANTHROPIC_TEMPERATURE=0.3
AZURE_ANTHROPIC_MAX_TOKENS=64000
# Backend: azure_deepseek
# use Azure Service endpoint for running Deekseek models; check https://ai.azure.com/github
# API endpoint should look like https://{resource-id}.services.ai.azure.com/openai/v1/
AZURE_DEEPSEEK_API_KEY=
AZURE_DEEPSEEK_API_ENDPOINT=
AZURE_DEEPSEEK_MODEL=DeepSeek-V3.2
AZURE_DEEPSEEK_TEMPERATURE=0.3
AZURE_DEEPSEEK_MAX_TOKENS=128000
# Backend: azure_mistral
# use Azure Service endpoint for running Mistral models; check https://ai.azure.com/github
# API endpoint should look like https://{resource-id}.services.ai.azure.com/openai/v1/
AZURE_MISTRAL_API_KEY=
AZURE_MISTRAL_API_ENDPOINT=
AZURE_MISTRAL_MODEL=Mistral-Large-3
AZURE_MISTRAL_TEMPERATURE=0.3
AZURE_MISTRAL_MAX_TOKENS=256000
# Backend: azure_xai
# use Azure Service endpoint for running Grok models; check https://ai.azure.com/github
# API endpoint should look like https://{resource-id}.services.ai.azure.com/openai/v1/
AZURE_XAI_API_KEY=
AZURE_XAI_API_ENDPOINT=
AZURE_XAI_MODEL=grok-4-fast-non-reasoning
AZURE_XAI_TEMPERATURE=0.3
AZURE_XAI_MAX_TOKENS=128000
# Backend: azure_cohere
# use Azure Service endpoint for running Cohere models; check https://ai.azure.com/github
# API endpoint should look like https://{resource-id}.services.ai.azure.com/openai/v1/
AZURE_COHERE_API_KEY=
AZURE_COHERE_API_ENDPOINT=
AZURE_COHERE_MODEL=Cohere-command-r-plus-08-2024
AZURE_COHERE_TEMPERATURE=0.3
AZURE_COHERE_MAX_TOKENS=128000

# Backend: azure_sdk
# use Azure AI inference endpoint for running DeepSeek-R1 and Phi-4; the endpoint should look like https://resource_name.services.ai.azure.com/models
AZURE_SDK_API_KEY=
AZURE_SDK_API_ENDPOINT=
AZURE_SDK_MODEL=grok-3
AZURE_SDK_TEMPERATURE=0.3
AZURE_SDK_MAX_TOKENS=16384
AZURE_PHI_API_KEY=
AZURE_PHI_API_ENDPOINT=
AZURE_PHI_MODEL=Phi-4
AZURE_PHI_TEMPERATURE=0.3
AZURE_PHI_MAX_TOKENS=8000
AZURE_EMBEDDING_MODEL=azure-text-embedding-3-large
AZURE_VISUAL_MODEL=gpt-4o
AZURE_VISUAL_TEMPERATURE=0.3
AZURE_VISUAL_MAX_TOKENS=4096
AZURE_DALLE_API_KEY=
AZURE_DALLE_API_ENDPOINT=
AZURE_DALLE_MODEL=dall-e-3
AZURE_WHISPER_API_KEY=
AZURE_WHISPER_API_ENDPOINT=
AZURE_WHISPER_MODEL=whisper

# Backend: cohere
# support multiple Cohere API keys, use comma ',' as separator
COHERE_API_KEY=
COHERE_MODEL=command-r-plus
COHERE_TEMPERATURE=0.3
COHERE_MAX_TOKENS=4000

# Backend: custom
# e.g. agentmake
CUSTOM_API_KEY=
# e.g. "http://localhost:11434/v1" for Ollama
CUSTOM_API_ENDPOINT=
CUSTOM_MODEL=
CUSTOM_TEMPERATURE=0.3
CUSTOM_MAX_TOKENS=4000

# Backend: custom1
# e.g. agentmake
CUSTOM1_API_KEY=
# e.g. "http://localhost:11434/v1" for Ollama
CUSTOM1_API_ENDPOINT=
CUSTOM1_MODEL=
CUSTOM1_TEMPERATURE=0.3
CUSTOM1_MAX_TOKENS=4000

# Backend: custom2
# e.g. agentmake
CUSTOM2_API_KEY=
# e.g. "http://localhost:11434/v1" for Ollama
CUSTOM2_API_ENDPOINT=
CUSTOM2_MODEL=
CUSTOM2_TEMPERATURE=0.3
CUSTOM2_MAX_TOKENS=4000

# Backend: deepseek
DEEPSEEK_API_KEY=
DEEPSEEK_MODEL=deepseek-chat
DEEPSEEK_TEMPERATURE = 0.3
DEEPSEEK_MAX_TOKENS = 8000

# Backend: github
# support multiple Github API keys, use comma ',' as separator
GITHUB_API_KEY=
GITHUB_MODEL=gpt-4o
GITHUB_TEMPERATURE=0.3
GITHUB_MAX_TOKENS=4000
GITHUB_ANY_API_KEY=
GITHUB_ANY_MODEL=DeepSeek-V3
GITHUB_ANY_TEMPERATURE=0.3
GITHUB_ANY_MAX_TOKENS=4000
GITHUB_VISUAL_MODEL=gpt-4o
GITHUB_VISUAL_TEMPERATURE=0.3
GITHUB_VISUAL_MAX_TOKENS=4000

# Backend: googleai
GOOGLEAI_API_KEY=${GEMINI_API_KEY}
GOOGLEAI_MODEL=gemini-2.5-flash
GOOGLEAI_TEMPERATURE=0.3
GOOGLEAI_MAX_TOKENS=65536
GOOGLEAI_VISUAL_MODEL=gemini-2.5-flash
GOOGLEAI_VISUAL_TEMPERATURE=0.3
GOOGLEAI_VISUAL_MAX_TOKENS=4096

# Backend: vertexai
# Enter the path of your google application credentials json file as the API key
# Default JSON credentials path: ~/agentmake/google_application_credentials.json
VERTEXAI_API_KEY=${GOOGLEAI_API_KEY}
VERTEXAI_API_PROJECT_ID=
VERTEXAI_API_SERVICE_LOCATION=us-central1
VERTEXAI_MODEL=gemini-2.5-flash
VERTEXAI_TEMPERATURE=0.3
VERTEXAI_MAX_TOKENS=65536
VERTEXAI_VISUAL_MODEL=gemini-2.5-flash
VERTEXAI_VISUAL_TEMPERATURE=0.3
VERTEXAI_VISUAL_MAX_TOKENS=8192
VERTEXAI_IMAGEN_MODEL=imagen-3.0-generate-002

# Backend: groq
# support multiple Groq API keys, use comma ',' as separator
GROQ_API_KEY=
GROQ_MODEL=openai/gpt-oss-120b
GROQ_MAX_TOKENS=65536
#GROQ_MODEL=llama-3.3-70b-versatile
#GROQ_MAX_TOKENS=32768
GROQ_TEMPERATURE=0.3
GROQ_VISUAL_MODEL=llama-3.2-90b-vision-preview
GROQ_VISUAL_TEMPERATURE=0.3
GROQ_VISUAL_MAX_TOKENS=8192

# Backend: llamacpp
LLAMACPP_API_ENDPOINT=http://127.0.0.1:8080/v1
LLAMACPP_TEMPERATURE=0.3
LLAMACPP_MAX_TOKENS=2048

# Backend: mistral
# support multiple Mistral API keys, use comma ',' as separator
MISTRAL_API_KEY=
MISTRAL_MODEL=mistral-large-latest
MISTRAL_TEMPERATURE=0.3
MISTRAL_MAX_TOKENS=8000
MISTRAL_VISUAL_MODEL=pixtral-large-latest
MISTRAL_VISUAL_TEMPERATURE=0.3
MISTRAL_VISUAL_MAX_TOKENS=8000

# Backend: ollama
# Empty the value of OLLAMA_ENDPOINT if you want to use dynamic local ip
OLLAMA_API_KEY=
OLLAMA_ENDPOINT="http://127.0.0.1:11434"
OLLAMA_MODEL=llama3.2
OLLAMA_TEMPERATURE=0.3
OLLAMA_MAX_TOKENS=-1
OLLAMA_CONTEXT_WINDOW=2048
OLLAMA_BATCH_SIZE=512
OLLAMA_KEEP_ALIVE=5m
OLLAMA_VISUAL_MODEL=granite3.2-vision
OLLAMA_VISUAL_TEMPERATURE=0.3
OLLAMA_VISUAL_MAX_TOKENS=-1

# Backend: ollamacloud
# support multiple OllamaCloud API keys, use comma ',' as separator
OLLAMACLOUD_API_KEY=
OLLAMACLOUD_ENDPOINT="https://ollama.com"
OLLAMACLOUD_MODEL=gpt-oss:120b
OLLAMACLOUD_TEMPERATURE=0.3
# Ollama cloud does not support -1 for max tokens
OLLAMACLOUD_MAX_TOKENS=65536
OLLAMACLOUD_CONTEXT_WINDOW=131072
OLLAMACLOUD_BATCH_SIZE=2048
OLLAMACLOUD_KEEP_ALIVE=5m
OLLAMACLOUD_VISUAL_MODEL=qwen3-vl:235b
OLLAMACLOUD_VISUAL_TEMPERATURE=0.3
OLLAMACLOUD_VISUAL_MAX_TOKENS=32768

# Backend: openai
OPENAI_API_KEY=
OPENAI_MODEL=gpt-5
OPENAI_TEMPERATURE=1
OPENAI_MAX_TOKENS=128000
OPENAI_VISUAL_MODEL=gpt-4o
OPENAI_VISUAL_TEMPERATURE=0.3
OPENAI_VISUAL_MAX_TOKENS=4096

# Backend: xai
XAI_API_KEY=
XAI_MODEL=grok-4
XAI_TEMPERATURE=0.3
# maximum context window size: 131072 tokens, including both input and output tokens
XAI_MAX_TOKENS=8192
XAI_VISUAL_MODEL=grok-2-vision-latest
XAI_VISUAL_TEMPERATURE=0.3
XAI_VISUAL_MAX_TOKENS=4096

# Other APIs
# support multiple Tavily API keys, use comma ',' as separator
TAVILY_API_KEY=
# support multiple OpenWeatherMap API keys, use comma ',' as separator
OPENWEATHERMAP_API_KEY=

# Tool: RAG
# openai models: "text-embedding-3-small", "text-embedding-3-large", "text-embedding-ada-002"
# azure openai models: "azure-text-embedding-3-small", "azure-text-embedding-3-large", "azure-text-embedding-ada-002"
# cohere models: "embed-english-v3.0", "embed-english-light-v3.0", "embed-multilingual-v3.0", "embed-multilingual-light-v3.0"
# mistral models: "mistral-embed"
# genai/vertexai/googleai models: "text-embedding-004"
# ollama models: https://ollama.com/search?c=embedding
RAG_EMBEDDING_MODEL="paraphrase-multilingual"
RAG_CHUNK_SIZE=1200
RAG_CHUNK_OVERLAP_SIZE=200
RAG_QUERY_TOP_K=5

# Tool: Perplexica
PERPLEXICA_HOST="http://127.0.0.1"
PERPLEXICA_PORT=3000
# local embedding options "xenova-bge-small-en-v1.5", "xenova-gte-small", "xenova-bert-base-multilingual-uncased"
PERPLEXICA_LOCAL_EMBEDDING="xenova-bge-small-en-v1.5"
# optimizationMode: 'speed' | 'balanced'
PERPLEXICA_OPTIMIZATION_MODE=speed
# focusMode: `webSearch`, `academicSearch`, `writingAssistant`, `wolframAlphaSearch`, `youtubeSearch`, `redditSearch`
PERPLEXICA_FOCUS_MODE=webSearch

# Tool: SearXNG
SEARXNG_HOST="http://127.0.0.1"
SEARXNG_PORT=4000

# Tool: Stable Diffusion CPP
FLUX_IMAGE_MODEL="flux1-dev-q4_k.gguf"
FLUX_IMAGE_WIDTH=1920
FLUX_IMAGE_HEIGHT=1088
FLUX_IMAGE_SAMPLE_STEPS=20

# Tool: whisper
# https://github.com/openai/whisper/tree/main#available-models-and-languages
# "tiny", "base", "small", "medium", "large"
WHISPER_MODEL=base

# Tool: install python package
# specify the path of `pip` command located in the virtual environment, if any
PIP_PATH=

# Tool: Memory
MEMORY_TYPES="general,instruction,fact,event,concept"

# Tool: UBA API
UBA_API_LOCAL_PORT=8080
UBA_API_ENDPOINT="https://bible.gospelchurchuk.org/plain"
UBA_API_TIMEOUT=10
UBA_API_PRIVATE_KEY=

# Tool: BibleMate AI Web API
BM_API_ENDPOINT="https://biblemate.gospelchurch.uk/api/data"
#BM_API_ENDPOINT="http://localhost:33355/api/data"
BM_API_TIMEOUT=10
BM_API_CUSTOM_KEY=

# BibleMate AI
BIBLEMATE_STATIC_TOKEN=
BIBLEMATE_MCP_PUBLIC_KEY=
BIBLEMATE_MCP_PRIVATE_KEY=
BIBLEMATE_MCP_ISSUER=
BIBLEMATE_MCP_AUDIENCE=

# Tool: Text-to-speech
TTS_EDGE_VOICE="en-GB-SoniaNeural"
TTS_EDGE_RATE=1.0
TTS_USE_VLC=
TTS_VLC_RATE=1.0
TTS_USE_MPV=
# https://cloud.google.com/text-to-speech/docs/voices
TTS_TERMUX_LANGUAGE="en-US"
TTS_TERMUX_RATE=1.0
TTS_PIPER_VOICE="en_GB-aru-medium"
TTS_PIPER_OPTIONS=

# Agents
MAXIMUM_ACTION_ROUND=20
MAXIMUM_AUTO_HEALING=3
# DEFAULT_TOOL_CHOICES="@chat @search/google @files/extract_text @install_python_package @magic"
DEFAULT_TOOL_CHOICES=

# Custom instruction for running interactive mode
CUSTOM_INSTRUCTION_1=
CUSTOM_INSTRUCTION_2=
CUSTOM_INSTRUCTION_3=
CUSTOM_INSTRUCTION_4=
CUSTOM_INSTRUCTION_5=
CUSTOM_INSTRUCTION_6=
CUSTOM_INSTRUCTION_7=
CUSTOM_INSTRUCTION_8=
CUSTOM_INSTRUCTION_9=
CUSTOM_INSTRUCTION_10=