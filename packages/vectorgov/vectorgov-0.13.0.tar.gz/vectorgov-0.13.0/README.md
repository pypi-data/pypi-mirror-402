# VectorGov SDK

SDK Python para acessar bases de conhecimento jur√≠dico VectorGov.

Acesse informa√ß√µes de leis, decretos e instru√ß√µes normativas brasileiras com 3 linhas de c√≥digo.

[![PyPI version](https://badge.fury.io/py/vectorgov.svg)](https://badge.fury.io/py/vectorgov)
[![Python 3.9+](https://img.shields.io/badge/python-3.9+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

---

## SDKs Dispon√≠veis

| Linguagem | Pacote | Reposit√≥rio |
|-----------|--------|-------------|
| **Python** | [`pip install vectorgov`](https://pypi.org/project/vectorgov/) | Este reposit√≥rio |
| **TypeScript/JavaScript** | [`npm install vectorgov`](https://www.npmjs.com/package/vectorgov) | [vectorgov-sdk-ts](https://github.com/euteajudo/vectorgov-sdk-ts) |

> **Usando TypeScript/JavaScript?** Veja a documenta√ß√£o completa do SDK TypeScript em [github.com/euteajudo/vectorgov-sdk-ts](https://github.com/euteajudo/vectorgov-sdk-ts)

---

## √çndice

- [Instala√ß√£o](#instala√ß√£o)
  - [Instala√ß√£o com Extras](#instala√ß√£o-com-extras-opcionais)
- [In√≠cio R√°pido](#in√≠cio-r√°pido)
- **Modelos Comerciais (APIs Pagas)**
  - [OpenAI (GPT-4)](#openai)
  - [Google Gemini](#google-gemini)
  - [Anthropic Claude](#anthropic-claude)
- **Modelos Open-Source (Gratuitos)**
  - [Ollama (Recomendado)](#integra√ß√£o-com-ollama)
  - [HuggingFace Transformers](#integra√ß√£o-com-huggingface-transformers)
- **Frameworks de Agentes**
  - [Function Calling](#function-calling-agentes)
  - [LangChain](#integra√ß√£o-com-langchain)
  - [LangGraph](#integra√ß√£o-com-langgraph)
  - [Google ADK](#integra√ß√£o-com-google-adk)
- **Integra√ß√µes**
  - [Servidor MCP](#servidor-mcp-claude-desktop-cursor-etc)
- **Configura√ß√£o**
  - [Modos de Busca](#modos-de-busca)
  - [Filtros](#filtros)
  - [Formata√ß√£o de Resultados](#formata√ß√£o-de-resultados)
  - [System Prompts](#system-prompts-customizados)
  - [Feedback](#feedback)
  - [Tratamento de Erros](#tratamento-de-erros)
- **Gerenciamento de Documentos**
  - [Permissoes](#permiss√µes)
  - [Listar e Consultar](#listar-e-consultar-documentos)
  - [Upload e Ingestao (Admin)](#upload-e-ingest√£o-admin)
  - [Enriquecimento (Admin)](#enriquecimento-admin)
  - [Exclusao (Admin)](#exclus√£o-admin)
- **Documenta√ß√£o para LLMs**
  - [llms.txt](#llmstxt)
  - [CLAUDE.md](#claudemd)
- [Obter sua API Key](#obter-sua-api-key)
- **Do B√°sico ao Avan√ßado**
  - [N√≠vel 1: O M√≠nimo Necess√°rio](#n√≠vel-1-o-m√≠nimo-necess√°rio)
  - [N√≠vel 2: Passando para seu LLM](#n√≠vel-2-passando-para-seu-llm)
  - [N√≠vel 3: Feedback](#n√≠vel-3-melhorando-o-sistema-com-feedback)
  - [N√≠vel 4: Filtros](#n√≠vel-4-refinando-com-filtros)
  - [N√≠vel 5: Modos](#n√≠vel-5-controlando-performance-com-modos)
  - [N√≠vel 6: Prompts](#n√≠vel-6-controlando-custos-com-prompts)
  - [N√≠vel 7: Auditoria](#n√≠vel-7-rastreabilidade-e-auditoria)
  - [N√≠vel 8: Integra√ß√µes](#n√≠vel-8-integra√ß√µes-avan√ßadas)
  - [Exemplo Completo](#-exemplo-completo-tudo-junto)

---

## Instala√ß√£o

```bash
pip install vectorgov
```

### Instala√ß√£o com Extras (Opcionais)

Algumas integra√ß√µes requerem depend√™ncias adicionais. Instale conforme sua necessidade:

| Extra | Comando | Descri√ß√£o |
|-------|---------|-----------|
| **LangChain** | `pip install 'vectorgov[langchain]'` | Retriever e Tool para LangChain |
| **LangGraph** | `pip install 'vectorgov[langgraph]'` | Ferramenta para agentes ReAct |
| **Google ADK** | `pip install 'vectorgov[google-adk]'` | Toolset para Google Agent Dev Kit |
| **Transformers** | `pip install 'vectorgov[transformers]'` | RAG com modelos HuggingFace locais |
| **MCP Server** | `pip install 'vectorgov[mcp]'` | Servidor MCP para Claude Desktop |
| **Tudo** | `pip install 'vectorgov[all]'` | Todas as depend√™ncias acima |

> **Nota:** A integra√ß√£o com **Ollama** n√£o requer extras - usa apenas a biblioteca padr√£o do Python.

> **Nota:** Para usar **OpenAI**, **Gemini** ou **Claude**, instale as bibliotecas separadamente:
> ```bash
> pip install openai          # Para OpenAI GPT
> pip install google-generativeai  # Para Google Gemini
> pip install anthropic       # Para Anthropic Claude
> ```

## In√≠cio R√°pido

```python
from vectorgov import VectorGov

# Conectar √† API
vg = VectorGov(api_key="vg_sua_chave_aqui")

# Buscar informa√ß√µes
results = vg.search("Quando o ETP pode ser dispensado?")

# Ver resultados
for hit in results:
    print(f"{hit.source}: {hit.text}")
```

> **Nota:** O SDK retorna o **texto completo** de cada chunk em `hit.text`. N√£o h√° limite de caracteres - voc√™ recebe todo o conte√∫do do artigo/par√°grafo/inciso recuperado.

---

# üí∞ Modelos Comerciais (APIs Pagas)

Use LLMs de provedores comerciais para gera√ß√£o de respostas. Requer API key do provedor.

## OpenAI

```bash
pip install openai
```

```python
from vectorgov import VectorGov
from openai import OpenAI

vg = VectorGov(api_key="vg_xxx")
openai_client = OpenAI(api_key="sk-xxx")

# Buscar contexto
query = "Quais os crit√©rios de julgamento na licita√ß√£o?"
results = vg.search(query)

# Gerar resposta
response = openai_client.chat.completions.create(
    model="gpt-4o-mini",
    messages=results.to_messages(query)
)

print(response.choices[0].message.content)
```

## Google Gemini

```bash
pip install google-generativeai
```

```python
from vectorgov import VectorGov
import google.generativeai as genai

vg = VectorGov(api_key="vg_xxx")
genai.configure(api_key="sua_google_key")

query = "O que √© ETP?"
results = vg.search(query)

# Monta o prompt
messages = results.to_messages(query)
system_prompt = messages[0]["content"]
user_prompt = messages[1]["content"]

# Cria o modelo com system instruction
model = genai.GenerativeModel(
    model_name="gemini-2.0-flash",
    system_instruction=system_prompt
)

response = model.generate_content(user_prompt)
print(response.text)
```

## Anthropic Claude

```bash
pip install anthropic
```

```python
from vectorgov import VectorGov
from anthropic import Anthropic

vg = VectorGov(api_key="vg_xxx")
client = Anthropic(api_key="sk-ant-xxx")

query = "O que √© ETP?"
results = vg.search(query)

# Monta o prompt
messages = results.to_messages(query)

response = client.messages.create(
    model="claude-sonnet-4-20250514",
    max_tokens=1024,
    system=messages[0]["content"],  # System prompt separado
    messages=[{"role": "user", "content": messages[1]["content"]}]
)

print(response.content[0].text)
```

---

# üÜì Modelos Open-Source (Gratuitos)

Use LLMs locais gratuitos para RAG sem custos de API. Ideal para desenvolvimento, prototipagem ou produ√ß√£o com controle total.

## Integra√ß√£o com Ollama

**Recomendado** - Forma mais simples de rodar LLMs localmente.

### Instala√ß√£o

```bash
# 1. Instale o Ollama: https://ollama.ai/
# 2. Baixe um modelo
ollama pull qwen3:8b
```

N√£o precisa de depend√™ncias extras do Python!

### Pipeline RAG Simples

```python
from vectorgov import VectorGov
from vectorgov.integrations.ollama import create_rag_pipeline

vg = VectorGov(api_key="vg_xxx")

# Cria pipeline RAG com Ollama
rag = create_rag_pipeline(vg, model="qwen3:8b")

# Usa como fun√ß√£o
resposta = rag("Quais os crit√©rios de julgamento na licita√ß√£o?")
print(resposta)
```

### Classe VectorGovOllama

```python
from vectorgov import VectorGov
from vectorgov.integrations.ollama import VectorGovOllama

vg = VectorGov(api_key="vg_xxx")
rag = VectorGovOllama(vg, model="qwen3:8b", top_k=5)

response = rag.ask("O que √© ETP?")

print(response.answer)
print(response.sources)      # Lista de fontes
print(response.latency_ms)   # Lat√™ncia total
print(response.model)        # Modelo usado
```

### Modelos Recomendados (Ollama)

| Modelo | RAM | Qualidade | Portugu√™s | Comando |
|--------|-----|-----------|-----------|---------|
| `qwen2.5:0.5b` | 1GB | B√°sica | Bom | `ollama pull qwen2.5:0.5b` |
| `qwen2.5:3b` | 4GB | Boa | Muito Bom | `ollama pull qwen2.5:3b` |
| `qwen2.5:7b` | 8GB | Muito Boa | **Excelente** | `ollama pull qwen2.5:7b` |
| `qwen3:8b` | 8GB | **Excelente** | **Excelente** | `ollama pull qwen3:8b` |
| `llama3.2:3b` | 4GB | Boa | Bom | `ollama pull llama3.2:3b` |

```python
from vectorgov.integrations.ollama import list_models, get_recommended_models

# Lista modelos instalados
print(list_models())

# Lista modelos recomendados
for name, info in get_recommended_models().items():
    print(f"{name}: {info['description']}")
```

### Chat com Hist√≥rico

```python
from vectorgov.integrations.ollama import VectorGovOllama

rag = VectorGovOllama(vg, model="qwen3:8b")

messages = [
    {"role": "user", "content": "O que √© ETP?"}
]

response = rag.chat(messages, use_rag=True)
print(response)

# Continua a conversa
messages.append({"role": "assistant", "content": response})
messages.append({"role": "user", "content": "E quando pode ser dispensado?"})

response2 = rag.chat(messages, use_rag=True)
print(response2)
```

---

## Integra√ß√£o com HuggingFace Transformers

Use modelos do HuggingFace Hub diretamente no Python.

### Instala√ß√£o

```bash
pip install 'vectorgov[transformers]'
# ou
pip install vectorgov transformers torch accelerate
```

### Pipeline RAG Simples

```python
from vectorgov import VectorGov
from vectorgov.integrations.transformers import create_rag_pipeline
from transformers import pipeline

# Inicializa
vg = VectorGov(api_key="vg_xxx")
llm = pipeline("text-generation", model="Qwen/Qwen2.5-3B-Instruct", device_map="auto")

# Cria pipeline RAG
rag = create_rag_pipeline(vg, llm, top_k=5, max_new_tokens=512)

# Usa como fun√ß√£o
resposta = rag("Quais os crit√©rios de julgamento na licita√ß√£o?")
print(resposta)
```

### Classe VectorGovRAG

```python
from vectorgov import VectorGov
from vectorgov.integrations.transformers import VectorGovRAG
from transformers import pipeline

vg = VectorGov(api_key="vg_xxx")
llm = pipeline("text-generation", model="meta-llama/Llama-3.2-3B-Instruct", device_map="auto")

rag = VectorGovRAG(vg, llm, top_k=5, temperature=0.1)

response = rag.ask("O que √© ETP?")

print(response.answer)
print(response.sources)      # Lista de fontes usadas
print(response.latency_ms)   # Tempo de busca
```

### Modelos Recomendados (HuggingFace)

| Modelo | VRAM | Qualidade | Portugu√™s |
|--------|------|-----------|-----------|
| `meta-llama/Llama-3.2-1B-Instruct` | 2GB | B√°sica | Bom |
| `Qwen/Qwen2.5-3B-Instruct` | 6GB | Boa | **Excelente** |
| `meta-llama/Llama-3.2-3B-Instruct` | 6GB | Boa | Bom |
| `Qwen/Qwen2.5-7B-Instruct` | 14GB | Muito Boa | **Excelente** |
| `microsoft/Phi-3-mini-4k-instruct` | 4GB | Boa | Razo√°vel |

```python
from vectorgov.integrations.transformers import get_recommended_models

# Lista modelos com detalhes
for name, info in get_recommended_models().items():
    print(f"{name}: {info['vram_gb']}GB, {info['portuguese']}")
```

### Rodando sem GPU (CPU)

```python
from transformers import pipeline
import torch

# For√ßa CPU com modelo leve
llm = pipeline(
    "text-generation",
    model="meta-llama/Llama-3.2-1B-Instruct",
    device="cpu",
    torch_dtype=torch.float32,
)
```

### Modelo Quantizado (4-bit)

```python
from transformers import pipeline, BitsAndBytesConfig
import torch

# Quantiza√ß√£o 4-bit (usa ~4GB VRAM para modelo 7B)
quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float16,
)

llm = pipeline(
    "text-generation",
    model="Qwen/Qwen2.5-7B-Instruct",
    model_kwargs={"quantization_config": quantization_config},
    device_map="auto",
)
```

---

# ü§ñ Frameworks de Agentes

## Function Calling (Agentes)

O VectorGov pode ser usado como ferramenta em agentes de IA. O LLM decide automaticamente quando consultar a legisla√ß√£o.

### OpenAI Function Calling

```python
from vectorgov import VectorGov
from openai import OpenAI

vg = VectorGov(api_key="vg_xxx")
client = OpenAI()

# Primeira chamada - GPT decide se precisa consultar legisla√ß√£o
response = client.chat.completions.create(
    model="gpt-4o",
    messages=[{"role": "user", "content": "Quais os crit√©rios de julgamento?"}],
    tools=[vg.to_openai_tool()],  # Registra VectorGov como ferramenta
)

# Se GPT quiser usar a ferramenta
if response.choices[0].message.tool_calls:
    tool_call = response.choices[0].message.tool_calls[0]
    result = vg.execute_tool_call(tool_call)  # Executa busca

    # Segunda chamada com o resultado
    final = client.chat.completions.create(
        model="gpt-4o",
        messages=[
            {"role": "user", "content": "Quais os crit√©rios de julgamento?"},
            response.choices[0].message,
            {"role": "tool", "tool_call_id": tool_call.id, "content": result},
        ],
    )
    print(final.choices[0].message.content)
```

### Anthropic Claude Tools

```python
from vectorgov import VectorGov
from anthropic import Anthropic

vg = VectorGov(api_key="vg_xxx")
client = Anthropic()

response = client.messages.create(
    model="claude-sonnet-4-20250514",
    messages=[{"role": "user", "content": "O que √© ETP?"}],
    tools=[vg.to_anthropic_tool()],
)

# Processar tool_use se houver
for block in response.content:
    if block.type == "tool_use":
        result = vg.execute_tool_call(block)
```

### Google Gemini Function Calling

```python
from vectorgov import VectorGov
import google.generativeai as genai

vg = VectorGov(api_key="vg_xxx")
genai.configure(api_key="sua_key")

model = genai.GenerativeModel(
    model_name="gemini-2.0-flash",
    tools=[vg.to_google_tool()],
)

response = model.generate_content("O que √© ETP?")
```

---

## Integra√ß√£o com LangChain

```bash
pip install 'vectorgov[langchain]'
```

### VectorGovRetriever

```python
from vectorgov.integrations.langchain import VectorGovRetriever
from langchain.chains import RetrievalQA
from langchain_openai import ChatOpenAI

# Criar retriever
retriever = VectorGovRetriever(api_key="vg_xxx", top_k=5)

# Usar com RetrievalQA
qa = RetrievalQA.from_chain_type(
    llm=ChatOpenAI(model="gpt-4o-mini"),
    retriever=retriever,
)

answer = qa.invoke("Quando o ETP pode ser dispensado?")
print(answer["result"])
```

### Com LCEL (LangChain Expression Language)

```python
from vectorgov.integrations.langchain import VectorGovRetriever
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain_openai import ChatOpenAI

retriever = VectorGovRetriever(api_key="vg_xxx")
llm = ChatOpenAI(model="gpt-4o-mini")

prompt = ChatPromptTemplate.from_template("""
Contexto: {context}

Pergunta: {question}
""")

def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)

chain = (
    {"context": retriever | format_docs, "question": RunnablePassthrough()}
    | prompt
    | llm
    | StrOutputParser()
)

answer = chain.invoke("O que √© ETP?")
```

### VectorGovTool para Agentes

```python
from vectorgov.integrations.langchain import VectorGovTool
from langchain.agents import AgentExecutor, create_openai_tools_agent
from langchain_openai import ChatOpenAI

tool = VectorGovTool(api_key="vg_xxx")
llm = ChatOpenAI(model="gpt-4o")

# Criar agente com a ferramenta
agent = create_openai_tools_agent(llm, [tool], prompt)
executor = AgentExecutor(agent=agent, tools=[tool])

result = executor.invoke({"input": "O que diz a lei sobre ETP?"})
```

---

## Integra√ß√£o com LangGraph

```bash
pip install 'vectorgov[langgraph]'
```

### ReAct Agent

```python
from vectorgov.integrations.langgraph import create_vectorgov_tool
from langgraph.prebuilt import create_react_agent
from langchain_openai import ChatOpenAI

# Criar ferramenta VectorGov
tool = create_vectorgov_tool(api_key="vg_xxx", top_k=5)

# Criar agente ReAct
llm = ChatOpenAI(model="gpt-4o-mini")
agent = create_react_agent(llm, tools=[tool])

# Executar
result = agent.invoke({"messages": [("user", "O que √© ETP?")]})
print(result["messages"][-1].content)
```

### Grafo RAG Customizado

```python
from vectorgov.integrations.langgraph import create_retrieval_node, VectorGovState
from langgraph.graph import StateGraph, START, END
from langchain_openai import ChatOpenAI

# N√≥ de retrieval VectorGov
retrieval_node = create_retrieval_node(api_key="vg_xxx", top_k=5)

# N√≥ de gera√ß√£o
def generate(state: VectorGovState) -> dict:
    llm = ChatOpenAI(model="gpt-4o-mini")
    context = state.get("context", "")
    query = state.get("query", "")
    response = llm.invoke(f"Contexto: {context}\n\nPergunta: {query}")
    return {"response": response.content}

# Construir grafo
builder = StateGraph(dict)
builder.add_node("retrieve", retrieval_node)
builder.add_node("generate", generate)
builder.add_edge(START, "retrieve")
builder.add_edge("retrieve", "generate")
builder.add_edge("generate", END)

graph = builder.compile()

# Executar
result = graph.invoke({"query": "Quando o ETP pode ser dispensado?"})
print(result["response"])
```

### Grafo RAG Pr√©-configurado

```python
from vectorgov.integrations.langgraph import create_legal_rag_graph
from langchain_openai import ChatOpenAI

llm = ChatOpenAI(model="gpt-4o-mini")
graph = create_legal_rag_graph(llm=llm, api_key="vg_xxx")

result = graph.invoke({"query": "Quais os crit√©rios de julgamento?"})
print(result["response"])
```

---

## Integra√ß√£o com Google ADK

```bash
pip install 'vectorgov[google-adk]'
```

### Ferramenta de Busca

```python
from vectorgov.integrations.google_adk import create_search_tool

# Criar ferramenta
search = create_search_tool(api_key="vg_xxx", top_k=5)

# Testar diretamente (sem agente)
result = search("O que √© ETP?")
print(result)
```

### Toolset Completo

```python
from vectorgov.integrations.google_adk import VectorGovToolset

toolset = VectorGovToolset(api_key="vg_xxx")

# Lista ferramentas dispon√≠veis
for tool in toolset.get_tools():
    print(f"- {tool.__name__}")
# - search_brazilian_legislation
# - list_legal_documents
# - get_article_text

# Usar com agente ADK
from google.adk.agents import Agent

agent = Agent(
    name="legal_assistant",
    model="gemini-2.0-flash",
    tools=toolset.get_tools(),
)
```

### Agente ADK Pr√©-configurado

```python
from vectorgov.integrations.google_adk import create_legal_agent

agent = create_legal_agent(api_key="vg_xxx")

response = agent.run("Quais os crit√©rios de julgamento na licita√ß√£o?")
print(response)
```

---

# üîå Integra√ß√µes

## Servidor MCP (Claude Desktop, Cursor, etc.)

O VectorGov pode funcionar como servidor MCP (Model Context Protocol), permitindo integra√ß√£o direta com Claude Desktop, Cursor, Windsurf e outras ferramentas compat√≠veis.

### Instala√ß√£o

```bash
pip install 'vectorgov[mcp]'
```

### Configura√ß√£o no Claude Desktop

Adicione ao arquivo `claude_desktop_config.json`:

**Windows**: `%APPDATA%\Claude\claude_desktop_config.json`
**macOS**: `~/Library/Application Support/Claude/claude_desktop_config.json`

```json
{
    "mcpServers": {
        "vectorgov": {
            "command": "uvx",
            "args": ["vectorgov-mcp"],
            "env": {
                "VECTORGOV_API_KEY": "vg_sua_chave_aqui"
            }
        }
    }
}
```

Ou se instalou via pip:

```json
{
    "mcpServers": {
        "vectorgov": {
            "command": "vectorgov-mcp",
            "env": {
                "VECTORGOV_API_KEY": "vg_sua_chave_aqui"
            }
        }
    }
}
```

### Executar Manualmente

```bash
# Via uvx (sem instalar)
uvx vectorgov-mcp

# Via pip (ap√≥s instalar)
vectorgov-mcp

# Via Python
python -m vectorgov.mcp
```

### Ferramentas Dispon√≠veis

O servidor MCP exp√µe tr√™s ferramentas para Claude:

| Ferramenta | Descri√ß√£o |
|------------|-----------|
| `search_legislation` | Busca sem√¢ntica em legisla√ß√£o brasileira |
| `list_available_documents` | Lista documentos dispon√≠veis na base |
| `get_article_text` | Obt√©m texto completo de um artigo espec√≠fico |

---

# ‚öôÔ∏è Configura√ß√£o

## Modos de Busca

| Modo | Descri√ß√£o | Lat√™ncia | Cache Padr√£o | Uso Recomendado |
|------|-----------|----------|--------------|-----------------|
| `fast` | Busca r√°pida, sem reranking | ~2s | ‚ùå Desligado | Chatbots, alta escala |
| `balanced` | Busca com reranking | ~5s | ‚ùå Desligado | **Uso geral (default)** |
| `precise` | Busca com HyDE + reranking | ~15s | ‚ùå Desligado | An√°lises cr√≠ticas |

> **Nota:** O cache est√° desabilitado por padr√£o em todos os modos para proteger sua privacidade.
> Veja a se√ß√£o [Aviso de Privacidade](#Ô∏è-aviso-de-privacidade---cache-compartilhado) para mais detalhes.

```python
# Busca r√°pida (chatbots)
results = vg.search("query", mode="fast")

# Busca balanceada (default)
results = vg.search("query", mode="balanced")

# Busca precisa (an√°lises)
results = vg.search("query", mode="precise")

# Qualquer modo COM cache (trade-off: privacidade vs lat√™ncia)
results = vg.search("query", mode="fast", use_cache=True)
```

## Filtros

```python
# Filtrar por tipo de documento
results = vg.search("licita√ß√£o", filters={"tipo": "lei"})

# Filtrar por ano
results = vg.search("preg√£o", filters={"ano": 2021})

# M√∫ltiplos filtros
results = vg.search("contrata√ß√£o direta", filters={
    "tipo": "in",
    "ano": 2022,
    "orgao": "seges"
})
```

## Formata√ß√£o de Resultados

```python
results = vg.search("O que √© ETP?")

# String simples para contexto
context = results.to_context()
print(context)
# [1] Lei 14.133/2021, Art. 3
# O Estudo T√©cnico Preliminar - ETP √© documento...
#
# [2] IN 58/2022, Art. 6
# O ETP deve conter...

# Mensagens para chat (OpenAI, Anthropic)
messages = results.to_messages("O que √© ETP?")
# [{"role": "system", "content": "..."}, {"role": "user", "content": "..."}]

# Prompt √∫nico (Gemini)
prompt = results.to_prompt("O que √© ETP?")
```

## System Prompts Customizados

O SDK inclui 4 prompts pr√©-definidos otimizados para diferentes casos de uso. Voc√™ tamb√©m pode criar prompts personalizados para ter **controle total sobre tokens e custos**.

### Prompts Dispon√≠veis

| Prompt | Tokens | Uso Recomendado |
|--------|--------|-----------------|
| `concise` | ~40 | Chatbots, alto volume, economia m√°xima |
| `chatbot` | ~60 | Atendimento ao p√∫blico, linguagem acess√≠vel |
| `default` | ~95 | Uso geral, equil√≠brio entre qualidade e custo |
| `detailed` | ~120 | Pareceres jur√≠dicos, an√°lises detalhadas |

### Conte√∫do dos Prompts

<details>
<summary><b>default</b> (~95 tokens)</summary>

```text
Voc√™ √© um assistente especializado em legisla√ß√£o brasileira, especialmente em licita√ß√µes e contratos p√∫blicos.

Instru√ß√µes:
1. Use APENAS as informa√ß√µes do contexto fornecido para responder
2. Se a informa√ß√£o n√£o estiver no contexto, diga que n√£o encontrou
3. Sempre cite as fontes usando o formato [Fonte: Lei X, Art. Y]
4. Seja objetivo e direto nas respostas
5. Use linguagem formal adequada ao contexto jur√≠dico
```
</details>

<details>
<summary><b>concise</b> (~40 tokens) - Economia m√°xima</summary>

```text
Voc√™ √© um assistente jur√≠dico. Responda de forma concisa e direta usando apenas o contexto fornecido. Cite as fontes.
```
</details>

<details>
<summary><b>detailed</b> (~120 tokens) - An√°lises completas</summary>

```text
Voc√™ √© um especialista em direito administrativo brasileiro.

Ao responder:
1. Analise cuidadosamente todo o contexto fornecido
2. Estruture a resposta em t√≥picos quando apropriado
3. Cite TODAS as fontes relevantes no formato [Lei X/Ano, Art. Y, ¬ßZ]
4. Explique termos t√©cnicos quando necess√°rio
5. Se houver diverg√™ncias ou exce√ß√µes, mencione-as
6. Conclua com um resumo pr√°tico quando aplic√°vel

Use SOMENTE informa√ß√µes do contexto. N√£o invente ou extrapole.
```
</details>

<details>
<summary><b>chatbot</b> (~60 tokens) - Linguagem acess√≠vel</summary>

```text
Voc√™ √© um assistente virtual amig√°vel especializado em licita√ß√µes p√∫blicas.
Responda de forma clara e acess√≠vel, evitando jarg√£o excessivo.
Baseie suas respostas apenas no contexto fornecido e cite as fontes.
```
</details>

### Impacto no Custo por LLM

Custo estimado **por requisi√ß√£o** (prompt + contexto ~1000 tokens + resposta ~500 tokens):

| LLM | `concise` | `default` | `detailed` |
|-----|-----------|-----------|------------|
| **GPT-4o** | ~$0.0077 | ~$0.0078 | ~$0.0079 |
| **GPT-4o-mini** | ~$0.00046 | ~$0.00047 | ~$0.00048 |
| **Claude Sonnet** | ~$0.0107 | ~$0.0108 | ~$0.0109 |
| **Gemini 1.5 Flash** | ~$0.00023 | ~$0.00023 | ~$0.00024 |

> **Nota:** O system prompt representa ~5-10% do custo total. O maior impacto vem do **contexto** (chunks) e da **resposta gerada**.

### Uso B√°sico

```python
# Usar prompt pr√©-definido
results = vg.search("query")
messages = results.to_messages(
    query="O que √© ETP?",
    system_prompt=vg.get_system_prompt("detailed")
)

# Ver prompts dispon√≠veis
print(vg.available_prompts)
# ['default', 'concise', 'detailed', 'chatbot']

# Ver conte√∫do de um prompt
print(vg.get_system_prompt("concise"))
```

### Prompt Personalizado (Controle Total)

Crie seu pr√≥prio prompt para ter controle total sobre tokens e comportamento:

```python
# Prompt ultra-curto para economia m√°xima (~15 tokens)
meu_prompt = "Responda usando apenas o contexto. Cite fontes."

messages = results.to_messages(
    query="O que √© ETP?",
    system_prompt=meu_prompt
)

# Prompt especializado para seu dom√≠nio
prompt_pregao = """Voc√™ √© um pregoeiro experiente.
Responda apenas sobre preg√£o eletr√¥nico.
Cite artigos da Lei 14.133/2021."""

messages = results.to_messages(
    query="Qual o prazo para impugna√ß√£o?",
    system_prompt=prompt_pregao
)

# Sem system prompt (s√≥ contexto + pergunta)
messages = results.to_messages(
    query="O que √© ETP?",
    system_prompt=""
)
```

### Dicas para Otimizar Custos

1. **Chatbots de alto volume**: Use `concise` ou prompt personalizado m√≠nimo
2. **Reduza o contexto**: `top_k=3` ao inv√©s de 5 reduz ~40% dos tokens
3. **Modelos mais baratos**: GPT-4o-mini √© 17x mais barato que GPT-4o
4. **Monitore tokens**: Use `tiktoken` para estimar custos antes de enviar

```python
import tiktoken

def estimar_tokens(messages, model="gpt-4o"):
    enc = tiktoken.encoding_for_model(model)
    return sum(len(enc.encode(m["content"])) for m in messages)

messages = results.to_messages("O que √© ETP?")
print(f"Esta requisi√ß√£o usar√° ~{estimar_tokens(messages)} tokens de input")
```

üìñ **[Guia Completo de System Prompts](docs/guides/system-prompts.md)** - Documenta√ß√£o detalhada com todos os cen√°rios de uso.

## Feedback

Ajude a melhorar o sistema enviando feedback sobre a qualidade das respostas. O feedback √© usado para:
- Melhorar o ranking de resultados
- Treinar modelos futuros (fine-tuning)
- Monitorar a qualidade do sistema

### Feedback B√°sico (Busca VectorGov)

```python
results = vg.search("O que √© ETP?")

# Ap√≥s verificar que o resultado foi √∫til
vg.feedback(results.query_id, like=True)

# Se o resultado n√£o foi √∫til
vg.feedback(results.query_id, like=False)
```

### Feedback com LLM Externo (OpenAI, Gemini, Claude, etc.)

Quando voc√™ usa seu pr√≥prio LLM para gerar respostas, use `store_response()` para habilitar o feedback:

```python
from vectorgov import VectorGov
from openai import OpenAI

vg = VectorGov(api_key="vg_xxx")
openai_client = OpenAI()

# 1. Busca contexto no VectorGov
query = "O que √© ETP?"
results = vg.search(query)

# 2. Gera resposta com seu LLM
response = openai_client.chat.completions.create(
    model="gpt-4o",
    messages=results.to_messages(query)
)
answer = response.choices[0].message.content

# 3. Salva a resposta no VectorGov para habilitar feedback
stored = vg.store_response(
    query=query,
    answer=answer,
    provider="OpenAI",
    model="gpt-4o",
    chunks_used=len(results)
)

# 4. Agora o feedback funciona!
vg.feedback(stored.query_hash, like=True)
```

### Par√¢metros do store_response()

| Par√¢metro | Tipo | Obrigat√≥rio | Descri√ß√£o |
|-----------|------|-------------|-----------|
| `query` | str | ‚úÖ | A pergunta original |
| `answer` | str | ‚úÖ | A resposta gerada pelo LLM |
| `provider` | str | ‚úÖ | Nome do provedor (OpenAI, Google, Anthropic) |
| `model` | str | ‚úÖ | ID do modelo (gpt-4o, gemini-2.0-flash) |
| `chunks_used` | int | ‚ùå | Quantidade de chunks usados como contexto |
| `latency_ms` | float | ‚ùå | Lat√™ncia total em ms |
| `retrieval_ms` | float | ‚ùå | Tempo de busca em ms |
| `generation_ms` | float | ‚ùå | Tempo de gera√ß√£o do LLM em ms |

### Retorno do store_response()

```python
stored = vg.store_response(...)

stored.success     # bool - Se foi salvo com sucesso
stored.query_hash  # str - Hash para usar em feedback()
stored.message     # str - Mensagem de status
```

## Propriedades do Resultado

```python
results = vg.search("query")

# Informa√ß√µes gerais
results.query        # Query original
results.total        # Quantidade de resultados
results.latency_ms   # Tempo de resposta (ms)
results.cached       # Se veio do cache
results.query_id     # ID para feedback
results.mode         # Modo utilizado

# Iterar resultados
for hit in results:
    hit.text         # Texto do chunk
    hit.score        # Relev√¢ncia (0-1)
    hit.source       # Fonte formatada
    hit.metadata     # Metadados completos
```

## Tratamento de Erros

```python
from vectorgov import (
    VectorGov,
    VectorGovError,
    AuthError,
    RateLimitError,
    ValidationError,
)

try:
    results = vg.search("query")
except AuthError:
    print("API key inv√°lida ou expirada")
except RateLimitError as e:
    print(f"Rate limit. Tente em {e.retry_after}s")
except ValidationError as e:
    print(f"Erro no campo {e.field}: {e.message}")
except VectorGovError as e:
    print(f"Erro: {e.message}")
```

## Vari√°veis de Ambiente

```bash
# API key pode ser definida via ambiente
export VECTORGOV_API_KEY=vg_sua_chave_aqui
```

```python
# Usa automaticamente a vari√°vel de ambiente
vg = VectorGov()
```

## Configura√ß√£o Avan√ßada

```python
vg = VectorGov(
    api_key="vg_xxx",
    base_url="https://vectorgov.io/api/v1",  # URL customizada
    timeout=60,                               # Timeout em segundos
    default_top_k=10,                         # Resultados padr√£o
    default_mode="precise",                   # Modo padr√£o
)
```

---

# ‚ö†Ô∏è Aviso de Privacidade - Cache Compartilhado

## Entendendo o Cache Sem√¢ntico

O VectorGov utiliza um **cache sem√¢ntico compartilhado** entre todos os clientes da API. Isso significa:

| Aspecto | Comportamento |
|---------|---------------|
| **Suas perguntas** | Podem ser armazenadas no cache |
| **Suas respostas** | Podem ser servidas a outros clientes com perguntas similares |
| **Perguntas de outros** | Voc√™ pode receber respostas j√° geradas por outros clientes |

### Trade-off: Performance vs Privacidade

| Cache Habilitado | Cache Desabilitado |
|------------------|-------------------|
| ‚úÖ Lat√™ncia menor (~0.1s para cache hit) | ‚ùå Lat√™ncia maior (~5-15s) |
| ‚úÖ Resposta pode vir pr√©-validada | ‚ùå Sempre gera resposta nova |
| ‚ùå Perguntas vis√≠veis a outros clientes | ‚úÖ Total privacidade |
| ‚ùå Pode receber respostas de outros | ‚úÖ Respostas exclusivas |

### Controle de Cache

Por padr√£o, o cache est√° **DESABILITADO** para proteger sua privacidade:

```python
# Padr√£o: SEM cache (privado)
results = vg.search("O que √© ETP?")  # use_cache=False impl√≠cito

# Explicitamente habilitando cache (perda de privacidade)
results = vg.search("O que √© ETP?", use_cache=True)
```

### Quando Habilitar o Cache?

| Use Cache | N√£o Use Cache |
|-----------|---------------|
| Perguntas gen√©ricas sobre legisla√ß√£o | Perguntas com dados sens√≠veis |
| Alta escala de usu√°rios (chatbots p√∫blicos) | An√°lises confidenciais |
| Demos e testes | Ambientes corporativos restritos |
| Quando lat√™ncia √© cr√≠tica | Quando privacidade √© prioridade |

### Exemplo de Uso Consciente

```python
from vectorgov import VectorGov

vg = VectorGov(api_key="vg_xxx")

# Pergunta gen√©rica - pode usar cache
results = vg.search("Quais os crit√©rios de julgamento?", use_cache=True)

# Pergunta espec√≠fica com dados sens√≠veis - N√ÉO usar cache
results = vg.search("Contrato da empresa XYZ foi regular?", use_cache=False)
```

> **Nota:** O cache desabilitado n√£o afeta a qualidade da resposta, apenas a lat√™ncia.
> O sistema de duas fases garante alta precis√£o independente do cache.

---

## Documenta√ß√£o para LLMs

O VectorGov fornece documenta√ß√£o estruturada para facilitar a integra√ß√£o com assistentes de IA e LLMs.

### llms.txt

Seguindo o padr√£o [llmstxt.org](https://llmstxt.org/), disponibilizamos documenta√ß√£o otimizada para consumo por LLMs:

**URL:** [https://vectorgov.io/llms.txt](https://vectorgov.io/llms.txt)

Este arquivo cont√©m:
- Vis√£o geral do SDK e API
- Exemplos de c√≥digo prontos para uso
- Documenta√ß√£o de todos os m√©todos (`search`, `ask`, `feedback`, `store_response`)
- Integra√ß√µes com OpenAI, Gemini e Claude
- Modos de busca e par√¢metros dispon√≠veis
- Tratamento de erros

Assistentes de IA podem acessar este arquivo para aprender a usar o VectorGov automaticamente.

### CLAUDE.md

Instru√ß√µes espec√≠ficas para o Claude Code (CLI):

**URL:** [https://vectorgov.io/CLAUDE.md](https://vectorgov.io/CLAUDE.md)

Cont√©m:
- Padr√µes de c√≥digo recomendados
- Exemplos de integra√ß√£o com diferentes LLMs
- Boas pr√°ticas para uso do SDK
- Estrutura de resposta e tratamento de erros

### robots.txt

O arquivo `robots.txt` em [https://vectorgov.io/robots.txt](https://vectorgov.io/robots.txt) permite acesso de crawlers de IA:

```
User-agent: GPTBot
User-agent: ChatGPT-User
User-agent: Claude-Web
User-agent: anthropic-ai
User-agent: Googlebot
Allow: /llms.txt
Allow: /CLAUDE.md
```

---

## Obter sua API Key

### 1) Criar uma API key (site)

1. Fa√ßa login no VectorGov.
2. Acesse **API Keys**: `https://vectorgov.io/api-keys`
3. Clique em **Nova API Key**, informe um nome (ex.: "Meu app dev") e confirme.
4. **Copie e salve a chave completa** (ela √© exibida uma √∫nica vez).

### 2) Testar no Playground (interface web)

1. Acesse o **Playground**: `https://vectorgov.io/playground`
2. Fa√ßa uma pergunta e ajuste as configura√ß√µes (modo, top_k, cache).
3. Use a se√ß√£o **C√≥digo equivalente** para copiar um exemplo (Python/TypeScript/cURL)
   e substitua `vg_sua_chave` pela sua API key.

### 3) Ver limite e acompanhar uso da API key

- Em `https://vectorgov.io/api-keys`, cada chave mostra:
  - **Status** (ativa/revogada)
  - **Rate limit** (requisi√ß√µes por minuto)
  - **Total de requests** (contador acumulado)
- Para detalhes do minuto atual, abra a configura√ß√£o da chave e veja:
  - **Uso no minuto atual**
  - **Restantes no minuto**
- Para logs detalhados de chamadas, use **Uso da API** (quando dispon√≠vel no seu menu).

## Documenta√ß√£o

- [Documenta√ß√£o](https://vectorgov.io/documentacao)
- [API Keys](https://vectorgov.io/api-keys)
- [Playground](https://vectorgov.io/playground)

## Suporte

- [GitHub Issues](https://github.com/euteajudo/vectorgov-sdk/issues)
- Email: suporte@vectorgov.io

## Licen√ßa

MIT License - veja [LICENSE](LICENSE) para detalhes.

---

# üìÅ Gerenciamento de Documentos

O SDK permite gerenciar documentos na base de conhecimento. Algumas opera√ß√µes s√£o restritas a **administradores**.

## Permiss√µes

| Opera√ß√£o | Permiss√£o | M√©todo |
|----------|-----------|--------|
| Listar documentos | Todos | `list_documents()` |
| Ver detalhes | Todos | `get_document(id)` |
| Ver status ingest√£o | Todos | `get_ingest_status(task_id)` |
| Ver status enriquecimento | Todos | `get_enrichment_status(task_id)` |
| **Upload de PDF** | **Admin** | `upload_pdf()` |
| **Iniciar enriquecimento** | **Admin** | `start_enrichment()` |
| **Excluir documento** | **Admin** | `delete_document()` |

> **Nota:** Para obter permiss√µes de administrador, entre em contato com o suporte.

## Listar e Consultar Documentos

Qualquer usu√°rio autenticado pode listar e consultar documentos.

```python
from vectorgov import VectorGov

vg = VectorGov(api_key="vg_xxx")

# Listar todos os documentos
docs = vg.list_documents()
print(f"Total: {docs.total} documentos")

for doc in docs.documents:
    print(f"- {doc.document_id}: {doc.tipo_documento} {doc.numero}/{doc.ano}")
    print(f"  Chunks: {doc.chunks_count}, Enriquecidos: {doc.enriched_count}")
    print(f"  Progresso: {doc.enrichment_progress:.0%}")

# Pagina√ß√£o
docs = vg.list_documents(page=2, limit=10)

# Detalhes de um documento espec√≠fico
doc = vg.get_document("LEI-14133-2021")
print(f"Documento: {doc.titulo}")
print(f"Status: {'Enriquecido' if doc.is_enriched else 'Pendente'}")
```

## Upload e Ingest√£o (Admin)

> **Requer permiss√£o de administrador**

```python
from vectorgov import VectorGov

vg = VectorGov(api_key="vg_admin_xxx")  # API key com permiss√£o admin

# Upload de PDF
with open("lei_exemplo.pdf", "rb") as f:
    result = vg.upload_pdf(
        file=f,
        tipo_documento="LEI",
        numero="99999",
        ano=2024,
        titulo="Lei de Exemplo",
        descricao="Descri√ß√£o opcional"
    )

print(f"Upload: {result.message}")
print(f"Document ID: {result.document_id}")
print(f"Task ID: {result.task_id}")

# Acompanhar status da ingest√£o
status = vg.get_ingest_status(result.task_id)
print(f"Status: {status.status}")  # pending, processing, completed, failed
print(f"Progresso: {status.progress}%")
print(f"Chunks criados: {status.chunks_created}")
```

### Polling de Status

```python
import time

task_id = result.task_id

while True:
    status = vg.get_ingest_status(task_id)
    print(f"Status: {status.status} ({status.progress}%)")
    
    if status.status in ("completed", "failed"):
        break
    
    time.sleep(5)  # Aguarda 5 segundos

if status.status == "completed":
    print(f"Ingest√£o conclu√≠da! {status.chunks_created} chunks criados")
else:
    print(f"Erro: {status.message}")
```

## Enriquecimento (Admin)

> **Requer permiss√£o de administrador**

O enriquecimento adiciona contexto sem√¢ntico aos chunks (resumos, perguntas sint√©ticas, etc.), melhorando a qualidade da busca.

```python
from vectorgov import VectorGov

vg = VectorGov(api_key="vg_admin_xxx")

# Iniciar enriquecimento de um documento
result = vg.start_enrichment("LEI-14133-2021")
print(f"Task ID: {result.task_id}")

# Acompanhar progresso
status = vg.get_enrichment_status(result.task_id)
print(f"Status: {status.status}")
print(f"Progresso: {status.progress:.0%}")
print(f"Chunks enriquecidos: {status.chunks_enriched}")
print(f"Chunks pendentes: {status.chunks_pending}")
print(f"Erros: {status.chunks_failed}")

# Polling at√© concluir
import time

while status.status not in ("completed", "error"):
    time.sleep(10)
    status = vg.get_enrichment_status(result.task_id)
    print(f"Progresso: {status.progress:.0%} ({status.chunks_enriched}/{status.chunks_enriched + status.chunks_pending})")

print("Enriquecimento conclu√≠do!")
```

## Exclus√£o (Admin)

> **Requer permiss√£o de administrador**

```python
from vectorgov import VectorGov

vg = VectorGov(api_key="vg_admin_xxx")

# Excluir documento
result = vg.delete_document("LEI-99999-2024")

if result.success:
    print(f"Documento exclu√≠do: {result.message}")
else:
    print(f"Erro: {result.message}")
```

## Modelos de Resposta

### DocumentSummary

```python
@dataclass
class DocumentSummary:
    document_id: str      # Ex: "LEI-14133-2021"
    tipo_documento: str   # Ex: "LEI", "DECRETO", "IN"
    numero: str           # Ex: "14133"
    ano: int              # Ex: 2021
    titulo: str           # T√≠tulo do documento
    descricao: str        # Descri√ß√£o opcional
    chunks_count: int     # Total de chunks
    enriched_count: int   # Chunks enriquecidos
    
    # Propriedades calculadas
    is_enriched: bool           # True se todos chunks enriquecidos
    enrichment_progress: float  # 0.0 a 1.0
```

### IngestStatus

```python
@dataclass
class IngestStatus:
    task_id: str
    status: Literal["pending", "processing", "completed", "failed"]
    progress: int         # 0 a 100
    message: str
    document_id: str      # Dispon√≠vel ap√≥s conclus√£o
    chunks_created: int
```

### EnrichStatus

```python
@dataclass
class EnrichStatus:
    task_id: str
    status: Literal["pending", "processing", "completed", "error", "not_found"]
    progress: float       # 0.0 a 1.0
    chunks_enriched: int
    chunks_pending: int
    chunks_failed: int
    errors: list[str]     # Lista de erros, se houver
```

---

# Auditoria e Seguran√ßa

O VectorGov possui um sistema de guardrails que monitora e registra eventos de seguran√ßa. Usu√°rios da SDK podem acessar logs de auditoria filtrados por sua API Key.

## Por que Auditoria √© Importante?

| Caso de Uso | Descri√ß√£o |
|-------------|-----------|
| **Compliance** | Atenda requisitos de LGPD, auditoria interna e governan√ßa |
| **Seguran√ßa** | Detecte tentativas de inje√ß√£o, vazamento de PII e uso suspeito |
| **Debugging** | Investigue problemas de integra√ß√£o e erros de valida√ß√£o |
| **Monitoramento** | Acompanhe m√©tricas de uso, lat√™ncia e padr√µes de queries |
| **Billing** | Entenda o consumo da API para planejamento de custos |

## Privacidade: Seus Logs S√£o Isolados

O VectorGov √© uma plataforma **multi-tenant**. Isso significa que:

| Aspecto | Como Funciona |
|---------|---------------|
| **Isolamento** | Cada API Key s√≥ acessa seus pr√≥prios logs |
| **Filtro Autom√°tico** | O backend filtra por `api_key_id` automaticamente |
| **Sem Acesso Cruzado** | Imposs√≠vel ver logs de outras organiza√ß√µes |
| **Dados Sens√≠veis** | Queries podem conter informa√ß√µes confidenciais |

```python
# Empresa A s√≥ v√™ logs da Empresa A
vg_a = VectorGov(api_key="vg_empresa_a_xxx")
logs_a = vg_a.get_audit_logs()  # Apenas logs da Empresa A

# Empresa B s√≥ v√™ logs da Empresa B
vg_b = VectorGov(api_key="vg_empresa_b_yyy")
logs_b = vg_b.get_audit_logs()  # Apenas logs da Empresa B
```

## M√©todos Dispon√≠veis

O SDK oferece 3 m√©todos para acessar dados de auditoria:

| M√©todo | Fun√ß√£o | Retorno |
|--------|--------|---------|
| `get_audit_logs()` | Lista eventos de auditoria com filtros | `AuditLogsResponse` |
| `get_audit_stats()` | Estat√≠sticas agregadas de um per√≠odo | `AuditStats` |
| `get_audit_event_types()` | Lista tipos de eventos dispon√≠veis | `list[str]` |

---

## `get_audit_logs()` - Investiga√ß√£o e Compliance

### Por que √© Importante?

| Cen√°rio | Como o M√©todo Ajuda |
|---------|---------------------|
| **Investiga√ß√£o de Incidentes** | Veja exatamente o que aconteceu, quando e qual query causou o problema |
| **Compliance LGPD** | Prove que dados pessoais foram detectados e tratados adequadamente |
| **Debugging** | Identifique queries mal formadas ou que causam erros de valida√ß√£o |
| **Auditoria Interna** | Documente uso da API para relat√≥rios de governan√ßa |

### O que Cada Campo Retornado Significa

| Campo | Significado | A√ß√£o Recomendada |
|-------|-------------|------------------|
| `event_type` | Tipo do evento (ex: `pii_detected`) | Filtre por tipos cr√≠ticos |
| `severity` | Gravidade (`info`, `warning`, `critical`) | Monitore `critical` em tempo real |
| `risk_score` | Score de risco de 0.0 a 1.0 | Investigue scores > 0.7 |
| `action_taken` | O que o sistema fez (`logged`, `blocked`, `warned`) | Revise a√ß√µes `blocked` |
| `query_text` | Query que gerou o evento (truncada) | Use para reproduzir problemas |
| `detection_types` | O que foi detectado (ex: `["cpf", "email"]`) | Identifique padr√µes de PII |

### Exemplo de Uso

```python
from vectorgov import VectorGov

vg = VectorGov(api_key="vg_xxx")

# Listar logs da sua API Key
logs = vg.get_audit_logs(
    limit=50,
    severity="warning",         # Opcional: info, warning, critical
    event_type="pii_detected",  # Opcional: filtrar por tipo
    start_date="2025-01-01",    # Opcional: data in√≠cio
    end_date="2025-01-18"       # Opcional: data fim
)

for log in logs.logs:
    print(f"[{log.severity}] {log.event_type}: {log.query_text}")
    print(f"  A√ß√£o: {log.action_taken}")
    print(f"  Risk Score: {log.risk_score}")
    print(f"  Data: {log.created_at}")
```

---

## `get_audit_stats()` - Vis√£o Gerencial e Tend√™ncias

### Por que √© Importante?

| Cen√°rio | Como o M√©todo Ajuda |
|---------|---------------------|
| **Dashboard Executivo** | Mostre m√©tricas de seguran√ßa para stakeholders |
| **Identifica√ß√£o de Tend√™ncias** | Detecte aumento de tentativas de injection |
| **Planejamento de Capacidade** | Entenda volume de uso para sizing |
| **KPIs de Seguran√ßa** | Acompanhe taxa de bloqueios vs requisi√ß√µes totais |

### M√©tricas Retornadas

| Campo | Significado | Meta Ideal |
|-------|-------------|------------|
| `total_events` | Total de eventos no per√≠odo | Crescimento controlado |
| `blocked_count` | Requisi√ß√µes bloqueadas | Pr√≥ximo de 0 |
| `warning_count` | Avisos gerados | Monitorar tend√™ncia |
| `events_by_type` | Distribui√ß√£o por tipo | Maioria deve ser `search_completed` |
| `events_by_severity` | Distribui√ß√£o por gravidade | Maioria deve ser `info` |

### Exemplo de Uso

```python
# Obter estat√≠sticas dos √∫ltimos 30 dias
stats = vg.get_audit_stats(days=30)

print(f"Total de eventos: {stats.total_events}")
print(f"Bloqueados: {stats.blocked_count}")
print(f"Alertas: {stats.warning_count}")

# Por tipo de evento
print("\nPor tipo:")
for event_type, count in stats.events_by_type.items():
    print(f"  {event_type}: {count}")

# Por severidade
print("\nPor severidade:")
for severity, count in stats.events_by_severity.items():
    print(f"  {severity}: {count}")
```

---

## `get_audit_event_types()` - Descoberta e Integra√ß√£o

### Por que √© Importante?

| Cen√°rio | Como o M√©todo Ajuda |
|---------|---------------------|
| **Construir Interfaces** | Popular dropdowns de filtro dinamicamente |
| **Manter Compatibilidade** | Descobrir novos tipos de eventos adicionados |
| **Documenta√ß√£o** | Gerar docs autom√°ticos dos eventos poss√≠veis |
| **Valida√ß√£o** | Verificar se um tipo de evento existe antes de filtrar |

### Exemplo de Uso

```python
# Listar todos os tipos de eventos dispon√≠veis
event_types = vg.get_audit_event_types()

print("Tipos de eventos dispon√≠veis:")
for event_type in event_types:
    print(f"  - {event_type}")

# Usar para popular um dropdown de filtro
# event_types = ['pii_detected', 'injection_blocked', 'search_completed', ...]
```

---

## Eventos Monitorados

| Evento | Categoria | Descri√ß√£o |
|--------|-----------|-----------|
| `pii_detected` | security | Dados pessoais detectados na query |
| `injection_detected` | security | Tentativa de prompt injection detectada |
| `injection_blocked` | security | Prompt injection bloqueado |
| `low_relevance_query` | validation | Query com baixa relev√¢ncia para o contexto |
| `citation_invalid` | validation | Cita√ß√£o n√£o encontrada nos chunks |
| `circuit_breaker_open` | performance | Circuit breaker aberto (servi√ßo indispon√≠vel) |
| `circuit_breaker_close` | performance | Circuit breaker fechado (servi√ßo restaurado) |

## Modelos de Resposta

### AuditLog

```python
@dataclass
class AuditLog:
    id: str
    event_type: str           # pii_detected, injection_blocked, etc
    event_category: str       # security, performance, validation
    severity: str             # info, warning, critical
    query_text: str | None    # Query que gerou o evento
    detection_types: list[str]  # Tipos de detec√ß√£o (ex: ["cpf", "email"])
    risk_score: float | None  # Score de risco (0.0 a 1.0)
    action_taken: str | None  # A√ß√£o tomada (blocked, allowed, logged)
    endpoint: str | None      # Endpoint chamado
    created_at: str           # Timestamp ISO
    details: dict             # Detalhes adicionais
```

### AuditLogsResponse

```python
@dataclass
class AuditLogsResponse:
    logs: list[AuditLog]
    total: int
    page: int
    pages: int
    limit: int
```

### AuditStats

```python
@dataclass
class AuditStats:
    total_events: int
    events_by_type: dict[str, int]
    events_by_severity: dict[str, int]
    events_by_category: dict[str, int]
    blocked_count: int
    warning_count: int
    period_days: int
```

## Boas Pr√°ticas de Seguran√ßa

1. **Monitore regularmente**: Verifique logs de auditoria periodicamente
2. **Configure alertas**: Use `severity="critical"` para eventos importantes
3. **Evite PII nas queries**: N√£o inclua CPF, email ou dados pessoais nas perguntas
4. **Respeite rate limits**: Muitos bloqueios podem indicar uso inadequado
5. **Reporte falsos positivos**: Entre em contato se detectores estiverem incorretos

---

# üöÄ Do B√°sico ao Avan√ßado: Construindo sua Integra√ß√£o

Esta se√ß√£o mostra a **progress√£o natural** de uso do VectorGov SDK, come√ßando pelo m√≠nimo necess√°rio e adicionando features conforme sua necessidade cresce.

## N√≠vel 1: O M√≠nimo Necess√°rio

**Tudo que voc√™ precisa para come√ßar:** uma API key e o m√©todo `search()`.

```python
from vectorgov import VectorGov

vg = VectorGov(api_key="vg_sua_chave")
results = vg.search("O que √© ETP?")

for hit in results:
    print(hit.text)
```

‚úÖ **Isso j√° funciona!** Voc√™ recebe os chunks mais relevantes da legisla√ß√£o brasileira.

---

## N√≠vel 2: Passando para seu LLM

**Quer usar o contexto com seu pr√≥prio LLM?** Use `to_messages()`:

```python
from vectorgov import VectorGov
from openai import OpenAI

vg = VectorGov(api_key="vg_xxx")
openai = OpenAI()

results = vg.search("O que √© ETP?")

# Converte para formato de mensagens (funciona com OpenAI, Claude, Gemini)
response = openai.chat.completions.create(
    model="gpt-4o-mini",
    messages=results.to_messages("O que √© ETP?")
)

print(response.choices[0].message.content)
```

‚úÖ Agora voc√™ tem RAG funcionando com qualquer LLM de sua escolha.

---

## N√≠vel 3: Melhorando o Sistema com Feedback

**Quer ajudar a melhorar os resultados?** Envie feedback:

```python
results = vg.search("O que √© ETP?")

# ... usa os resultados ...

# Feedback positivo
vg.feedback(results.query_id, like=True)

# Ou negativo
vg.feedback(results.query_id, like=False)
```

Se estiver usando LLM externo, salve a resposta primeiro:

```python
# Gera resposta com seu LLM
answer = openai.chat.completions.create(...).choices[0].message.content

# Salva no VectorGov para habilitar feedback
stored = vg.store_response(
    query="O que √© ETP?",
    answer=answer,
    provider="OpenAI",
    model="gpt-4o"
)

# Agora pode enviar feedback
vg.feedback(stored.query_hash, like=True)
```

‚úÖ Seu feedback melhora o sistema para todos.

---

## N√≠vel 4: Refinando com Filtros

**Quer buscar em documentos espec√≠ficos?** Use filtros:

```python
# Apenas leis
results = vg.search("licita√ß√£o", filters={"tipo": "lei"})

# Apenas de 2021
results = vg.search("preg√£o", filters={"ano": 2021})

# M√∫ltiplos filtros
results = vg.search("contrata√ß√£o direta", filters={
    "tipo": "in",
    "ano": 2022,
    "orgao": "seges"
})
```

‚úÖ Resultados mais precisos para seu caso de uso.

---

## N√≠vel 5: Controlando Performance com Modos

**Precisa de mais velocidade ou precis√£o?** Escolha o modo:

```python
# R√°pido: chatbots, alta escala (~2s)
results = vg.search("query", mode="fast")

# Balanceado: uso geral (~5s) - DEFAULT
results = vg.search("query", mode="balanced")

# Preciso: an√°lises cr√≠ticas (~15s)
results = vg.search("query", mode="precise")

# Com cache para queries gen√©ricas (trade-off: privacidade)
results = vg.search("query", mode="fast", use_cache=True)
```

‚úÖ Otimize para seu caso: lat√™ncia vs precis√£o vs custo.

---

## N√≠vel 6: Controlando Custos com Prompts

**Quer economizar tokens no LLM?** Personalize o prompt:

```python
# Prompt m√≠nimo (~15 tokens) - economia m√°xima
results = vg.search("O que √© ETP?")
messages = results.to_messages(
    "O que √© ETP?",
    system_prompt="Responda usando o contexto. Cite fontes."
)

# Ou use prompts pr√©-definidos
messages = results.to_messages(
    "O que √© ETP?",
    system_prompt=vg.get_system_prompt("concise")  # ~40 tokens
)

# Ver op√ß√µes dispon√≠veis
print(vg.available_prompts)  # ['default', 'concise', 'detailed', 'chatbot']
```

‚úÖ Economia de at√© 80 tokens por requisi√ß√£o = ~$0.80/10.000 req no GPT-4o.

---

## N√≠vel 7: Rastreabilidade e Auditoria

**Precisa monitorar o uso?** Acesse os logs de auditoria:

```python
# Logs dos √∫ltimos 7 dias
logs = vg.get_audit_logs(days=7)

for log in logs.logs:
    print(f"[{log.severity}] {log.event_type}")

# Estat√≠sticas agregadas
stats = vg.get_audit_stats(days=30)
print(f"Total: {stats.total_events} eventos")
print(f"Bloqueados: {stats.blocked_count}")
```

‚úÖ Visibilidade completa sobre o uso e seguran√ßa.

---

## N√≠vel 8: Integra√ß√µes Avan√ßadas

**Quer usar com frameworks de agentes?** Escolha sua integra√ß√£o:

### LangChain
```python
from vectorgov.integrations.langchain import VectorGovRetriever
retriever = VectorGovRetriever(api_key="vg_xxx")
```

### LangGraph
```python
from vectorgov.integrations.langgraph import create_vectorgov_tool
tool = create_vectorgov_tool(api_key="vg_xxx")
```

### Function Calling
```python
# OpenAI
tools = [vg.to_openai_tool()]

# Anthropic
tools = [vg.to_anthropic_tool()]

# Google
tools = [vg.to_google_tool()]
```

### MCP (Claude Desktop, Cursor)
```json
{
    "mcpServers": {
        "vectorgov": {
            "command": "vectorgov-mcp",
            "env": {"VECTORGOV_API_KEY": "vg_xxx"}
        }
    }
}
```

‚úÖ VectorGov se integra com qualquer stack de IA.

---

## üéØ Exemplo Completo: Tudo Junto

Aqui est√° um exemplo de **produ√ß√£o real** que usa todas as features em um √∫nico fluxo:

```python
"""
Aplica√ß√£o RAG Completa com VectorGov
Inclui: filtros, modos, prompts, feedback, auditoria
"""

from vectorgov import VectorGov, VectorGovError, RateLimitError
from openai import OpenAI
import time

# =============================================================================
# CONFIGURA√á√ÉO
# =============================================================================

vg = VectorGov(
    api_key="vg_xxx",
    timeout=60,
    default_top_k=5,
)
openai_client = OpenAI()

# =============================================================================
# FUN√á√ÉO PRINCIPAL RAG
# =============================================================================

def responder_pergunta(
    query: str,
    filtros: dict = None,
    modo: str = "balanced",
    prompt_tipo: str = "default",
    usar_cache: bool = False,
) -> dict:
    """
    Fluxo RAG completo com todas as features.

    Args:
        query: Pergunta do usu√°rio
        filtros: Filtros de busca (tipo, ano, orgao)
        modo: fast, balanced ou precise
        prompt_tipo: default, concise, detailed, chatbot
        usar_cache: Se deve usar cache compartilhado

    Returns:
        dict com answer, sources, query_hash, latency
    """
    start_time = time.time()

    try:
        # -----------------------------------------------------------------
        # 1. BUSCA COM FILTROS E MODO
        # -----------------------------------------------------------------
        results = vg.search(
            query,
            mode=modo,
            filters=filtros,
            use_cache=usar_cache,
        )

        if not results.hits:
            return {
                "answer": "N√£o encontrei informa√ß√µes relevantes para sua pergunta.",
                "sources": [],
                "query_hash": None,
                "latency_ms": (time.time() - start_time) * 1000,
            }

        # -----------------------------------------------------------------
        # 2. MONTA PROMPT COM CONTROLE DE TOKENS
        # -----------------------------------------------------------------
        system_prompt = vg.get_system_prompt(prompt_tipo)
        messages = results.to_messages(query, system_prompt=system_prompt)

        # -----------------------------------------------------------------
        # 3. GERA RESPOSTA COM LLM
        # -----------------------------------------------------------------
        response = openai_client.chat.completions.create(
            model="gpt-4o-mini",  # Mais barato para alto volume
            messages=messages,
            temperature=0.1,  # Mais determin√≠stico para respostas jur√≠dicas
        )
        answer = response.choices[0].message.content

        # -----------------------------------------------------------------
        # 4. SALVA RESPOSTA PARA HABILITAR FEEDBACK
        # -----------------------------------------------------------------
        stored = vg.store_response(
            query=query,
            answer=answer,
            provider="OpenAI",
            model="gpt-4o-mini",
            chunks_used=len(results.hits),
        )

        # -----------------------------------------------------------------
        # 5. RETORNA RESULTADO ESTRUTURADO
        # -----------------------------------------------------------------
        return {
            "answer": answer,
            "sources": [hit.source for hit in results.hits],
            "query_hash": stored.query_hash,  # Para feedback posterior
            "latency_ms": (time.time() - start_time) * 1000,
            "cached": results.cached,
            "mode": modo,
        }

    except RateLimitError as e:
        return {
            "error": f"Rate limit. Tente novamente em {e.retry_after}s",
            "retry_after": e.retry_after,
        }

    except VectorGovError as e:
        return {
            "error": f"Erro VectorGov: {e.message}",
        }

# =============================================================================
# EXEMPLO DE USO
# =============================================================================

if __name__ == "__main__":
    # Pergunta simples
    resultado = responder_pergunta("O que √© ETP?")
    print(f"Resposta: {resultado['answer'][:200]}...")
    print(f"Fontes: {resultado['sources']}")
    print(f"Lat√™ncia: {resultado['latency_ms']:.0f}ms")

    # Pergunta com filtros e modo preciso
    resultado = responder_pergunta(
        query="Quando o ETP pode ser dispensado?",
        filtros={"tipo": "in", "ano": 2022},
        modo="precise",
        prompt_tipo="detailed",
    )

    # Enviar feedback (ap√≥s usu√°rio avaliar)
    if resultado.get("query_hash"):
        vg.feedback(resultado["query_hash"], like=True)
        print("Feedback enviado!")

    # Verificar logs de auditoria
    stats = vg.get_audit_stats(days=7)
    print(f"\nEstat√≠sticas da semana:")
    print(f"  Total de eventos: {stats.total_events}")
    print(f"  Bloqueados: {stats.blocked_count}")
```

### O que esse exemplo demonstra:

| Feature | Linha | Descri√ß√£o |
|---------|-------|-----------|
| **Busca b√°sica** | `vg.search()` | O m√≠nimo necess√°rio |
| **Modos** | `mode="balanced"` | Controle de lat√™ncia/precis√£o |
| **Filtros** | `filters={...}` | Refinamento de busca |
| **Cache** | `use_cache=False` | Trade-off privacidade/velocidade |
| **Prompts** | `vg.get_system_prompt()` | Controle de tokens/custos |
| **to_messages()** | `results.to_messages()` | Integra√ß√£o com qualquer LLM |
| **store_response()** | `vg.store_response()` | Habilita feedback para LLM externo |
| **Feedback** | `vg.feedback()` | Melhora o sistema |
| **Auditoria** | `vg.get_audit_stats()` | Rastreabilidade |
| **Tratamento de erros** | `try/except` | Robustez em produ√ß√£o |

---

## üìä Resumo: Qual Feature Usar Quando?

| Necessidade | Feature | Exemplo |
|-------------|---------|---------|
| Buscar legisla√ß√£o | `search()` | `vg.search("query")` |
| Usar com LLM | `to_messages()` | `results.to_messages(query)` |
| Melhorar resultados | `feedback()` | `vg.feedback(query_id, like=True)` |
| Busca espec√≠fica | `filters` | `filters={"tipo": "lei"}` |
| Mais velocidade | `mode="fast"` | Chatbots, alto volume |
| Mais precis√£o | `mode="precise"` | An√°lises cr√≠ticas |
| Economia de tokens | `system_prompt` | Prompt personalizado |
| LLM externo + feedback | `store_response()` | Salva resposta para feedback |
| Monitoramento | `get_audit_logs()` | Logs de seguran√ßa |
| Agentes IA | `to_openai_tool()` | Function calling |
| Claude Desktop | MCP Server | `vectorgov-mcp` |

---

> **Dica:** Comece simples com `search()` e v√° adicionando features conforme sua aplica√ß√£o evolui. N√£o precisa usar tudo desde o in√≠cio!
