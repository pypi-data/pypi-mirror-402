# VectorGov SDK

SDK Python para acessar bases de conhecimento jur√≠dico VectorGov.

Acesse informa√ß√µes de leis, decretos e instru√ß√µes normativas brasileiras com 3 linhas de c√≥digo.

[![PyPI version](https://badge.fury.io/py/vectorgov.svg)](https://badge.fury.io/py/vectorgov)
[![Python 3.9+](https://img.shields.io/badge/python-3.9+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

---

## √çndice

- [Instala√ß√£o](#instala√ß√£o)
  - [Instala√ß√£o com Extras](#instala√ß√£o-com-extras-opcionais)
- [In√≠cio R√°pido](#in√≠cio-r√°pido)
- [Streaming (Tempo Real)](#streaming-tempo-real)
- **Modelos Comerciais (APIs Pagas)**
  - [OpenAI (GPT-4)](#openai)
  - [Google Gemini](#google-gemini)
  - [Anthropic Claude](#anthropic-claude)
- **Modelos Open-Source (Gratuitos)**
  - [Ollama (Recomendado)](#integra√ß√£o-com-ollama)
  - [HuggingFace Transformers](#integra√ß√£o-com-huggingface-transformers)
- **Frameworks de Agentes**
  - [Function Calling](#function-calling-agentes)
  - [LangChain](#integra√ß√£o-com-langchain)
  - [LangGraph](#integra√ß√£o-com-langgraph)
  - [Google ADK](#integra√ß√£o-com-google-adk)
- **Integra√ß√µes**
  - [Servidor MCP](#servidor-mcp-claude-desktop-cursor-etc)
- **Configura√ß√£o**
  - [Modos de Busca](#modos-de-busca)
  - [Filtros](#filtros)
  - [Formata√ß√£o de Resultados](#formata√ß√£o-de-resultados)
  - [System Prompts](#system-prompts-customizados)
  - [Feedback](#feedback)
  - [Tratamento de Erros](#tratamento-de-erros)
- **Gerenciamento de Documentos**
  - [Permissoes](#permiss√µes)
  - [Listar e Consultar](#listar-e-consultar-documentos)
  - [Upload e Ingestao (Admin)](#upload-e-ingest√£o-admin)
  - [Enriquecimento (Admin)](#enriquecimento-admin)
  - [Exclusao (Admin)](#exclus√£o-admin)
- [Obter sua API Key](#obter-sua-api-key)

---

## Instala√ß√£o

```bash
pip install vectorgov
```

### Instala√ß√£o com Extras (Opcionais)

Algumas integra√ß√µes requerem depend√™ncias adicionais. Instale conforme sua necessidade:

| Extra | Comando | Descri√ß√£o |
|-------|---------|-----------|
| **LangChain** | `pip install 'vectorgov[langchain]'` | Retriever e Tool para LangChain |
| **LangGraph** | `pip install 'vectorgov[langgraph]'` | Ferramenta para agentes ReAct |
| **Google ADK** | `pip install 'vectorgov[google-adk]'` | Toolset para Google Agent Dev Kit |
| **Transformers** | `pip install 'vectorgov[transformers]'` | RAG com modelos HuggingFace locais |
| **MCP Server** | `pip install 'vectorgov[mcp]'` | Servidor MCP para Claude Desktop |
| **Tudo** | `pip install 'vectorgov[all]'` | Todas as depend√™ncias acima |

> **Nota:** A integra√ß√£o com **Ollama** n√£o requer extras - usa apenas a biblioteca padr√£o do Python.

> **Nota:** Para usar **OpenAI**, **Gemini** ou **Claude**, instale as bibliotecas separadamente:
> ```bash
> pip install openai          # Para OpenAI GPT
> pip install google-generativeai  # Para Google Gemini
> pip install anthropic       # Para Anthropic Claude
> ```

## In√≠cio R√°pido

```python
from vectorgov import VectorGov

# Conectar √† API
vg = VectorGov(api_key="vg_sua_chave_aqui")

# Buscar informa√ß√µes
results = vg.search("Quando o ETP pode ser dispensado?")

# Ver resultados
for hit in results:
    print(f"{hit.source}: {hit.text}")
```

> **Nota:** O SDK retorna o **texto completo** de cada chunk em `hit.text`. N√£o h√° limite de caracteres - voc√™ recebe todo o conte√∫do do artigo/par√°grafo/inciso recuperado.

---

## Streaming (Tempo Real)

Obtenha respostas em tempo real com o m√©todo `ask_stream()`. Ideal para interfaces de chat interativas.

```python
from vectorgov import VectorGov

vg = VectorGov(api_key="vg_xxx")

for chunk in vg.ask_stream("O que √© ETP?"):
    if chunk.type == "token":
        # Exibe cada token conforme √© gerado
        print(chunk.content, end="", flush=True)
    elif chunk.type == "retrieval":
        # Notifica√ß√£o de busca conclu√≠da
        print(f"[Recuperados {chunk.chunks} documentos em {chunk.time_ms}ms]")
    elif chunk.type == "complete":
        # Resposta completa com cita√ß√µes
        print(f"\n\nüìö Fontes: {len(chunk.citations)} cita√ß√µes")
```

### Tipos de Eventos

| Evento | Descri√ß√£o | Campos |
|--------|-----------|--------|
| `start` | In√≠cio do processamento | `query` |
| `retrieval` | Busca conclu√≠da | `chunks`, `time_ms` |
| `token` | Token da resposta | `content` |
| `complete` | Resposta finalizada | `citations`, `query_hash` |
| `error` | Erro no processamento | `message` |

### Exemplo com Interface

```python
import sys

for chunk in vg.ask_stream("Quando o ETP pode ser dispensado?"):
    if chunk.type == "start":
        print("üîç Buscando...", file=sys.stderr)
    elif chunk.type == "retrieval":
        print(f"üìÑ {chunk.chunks} documentos encontrados", file=sys.stderr)
    elif chunk.type == "token":
        print(chunk.content, end="", flush=True)
    elif chunk.type == "complete":
        print(f"\n\n---\nüìö {len(chunk.citations)} cita√ß√µes", file=sys.stderr)
    elif chunk.type == "error":
        print(f"‚ùå Erro: {chunk.message}", file=sys.stderr)
        break
```

---

# üí∞ Modelos Comerciais (APIs Pagas)

Use LLMs de provedores comerciais para gera√ß√£o de respostas. Requer API key do provedor.

## OpenAI

```bash
pip install openai
```

```python
from vectorgov import VectorGov
from openai import OpenAI

vg = VectorGov(api_key="vg_xxx")
openai_client = OpenAI(api_key="sk-xxx")

# Buscar contexto
query = "Quais os crit√©rios de julgamento na licita√ß√£o?"
results = vg.search(query)

# Gerar resposta
response = openai_client.chat.completions.create(
    model="gpt-4o-mini",
    messages=results.to_messages(query)
)

print(response.choices[0].message.content)
```

## Google Gemini

```bash
pip install google-generativeai
```

```python
from vectorgov import VectorGov
import google.generativeai as genai

vg = VectorGov(api_key="vg_xxx")
genai.configure(api_key="sua_google_key")

query = "O que √© ETP?"
results = vg.search(query)

# Monta o prompt
messages = results.to_messages(query)
system_prompt = messages[0]["content"]
user_prompt = messages[1]["content"]

# Cria o modelo com system instruction
model = genai.GenerativeModel(
    model_name="gemini-2.0-flash",
    system_instruction=system_prompt
)

response = model.generate_content(user_prompt)
print(response.text)
```

## Anthropic Claude

```bash
pip install anthropic
```

```python
from vectorgov import VectorGov
from anthropic import Anthropic

vg = VectorGov(api_key="vg_xxx")
client = Anthropic(api_key="sk-ant-xxx")

query = "O que √© ETP?"
results = vg.search(query)

# Monta o prompt
messages = results.to_messages(query)

response = client.messages.create(
    model="claude-sonnet-4-20250514",
    max_tokens=1024,
    system=messages[0]["content"],  # System prompt separado
    messages=[{"role": "user", "content": messages[1]["content"]}]
)

print(response.content[0].text)
```

---

# üÜì Modelos Open-Source (Gratuitos)

Use LLMs locais gratuitos para RAG sem custos de API. Ideal para desenvolvimento, prototipagem ou produ√ß√£o com controle total.

## Integra√ß√£o com Ollama

**Recomendado** - Forma mais simples de rodar LLMs localmente.

### Instala√ß√£o

```bash
# 1. Instale o Ollama: https://ollama.ai/
# 2. Baixe um modelo
ollama pull qwen3:8b
```

N√£o precisa de depend√™ncias extras do Python!

### Pipeline RAG Simples

```python
from vectorgov import VectorGov
from vectorgov.integrations.ollama import create_rag_pipeline

vg = VectorGov(api_key="vg_xxx")

# Cria pipeline RAG com Ollama
rag = create_rag_pipeline(vg, model="qwen3:8b")

# Usa como fun√ß√£o
resposta = rag("Quais os crit√©rios de julgamento na licita√ß√£o?")
print(resposta)
```

### Classe VectorGovOllama

```python
from vectorgov import VectorGov
from vectorgov.integrations.ollama import VectorGovOllama

vg = VectorGov(api_key="vg_xxx")
rag = VectorGovOllama(vg, model="qwen3:8b", top_k=5)

response = rag.ask("O que √© ETP?")

print(response.answer)
print(response.sources)      # Lista de fontes
print(response.latency_ms)   # Lat√™ncia total
print(response.model)        # Modelo usado
```

### Modelos Recomendados (Ollama)

| Modelo | RAM | Qualidade | Portugu√™s | Comando |
|--------|-----|-----------|-----------|---------|
| `qwen2.5:0.5b` | 1GB | B√°sica | Bom | `ollama pull qwen2.5:0.5b` |
| `qwen2.5:3b` | 4GB | Boa | Muito Bom | `ollama pull qwen2.5:3b` |
| `qwen2.5:7b` | 8GB | Muito Boa | **Excelente** | `ollama pull qwen2.5:7b` |
| `qwen3:8b` | 8GB | **Excelente** | **Excelente** | `ollama pull qwen3:8b` |
| `llama3.2:3b` | 4GB | Boa | Bom | `ollama pull llama3.2:3b` |

```python
from vectorgov.integrations.ollama import list_models, get_recommended_models

# Lista modelos instalados
print(list_models())

# Lista modelos recomendados
for name, info in get_recommended_models().items():
    print(f"{name}: {info['description']}")
```

### Chat com Hist√≥rico

```python
from vectorgov.integrations.ollama import VectorGovOllama

rag = VectorGovOllama(vg, model="qwen3:8b")

messages = [
    {"role": "user", "content": "O que √© ETP?"}
]

response = rag.chat(messages, use_rag=True)
print(response)

# Continua a conversa
messages.append({"role": "assistant", "content": response})
messages.append({"role": "user", "content": "E quando pode ser dispensado?"})

response2 = rag.chat(messages, use_rag=True)
print(response2)
```

---

## Integra√ß√£o com HuggingFace Transformers

Use modelos do HuggingFace Hub diretamente no Python.

### Instala√ß√£o

```bash
pip install 'vectorgov[transformers]'
# ou
pip install vectorgov transformers torch accelerate
```

### Pipeline RAG Simples

```python
from vectorgov import VectorGov
from vectorgov.integrations.transformers import create_rag_pipeline
from transformers import pipeline

# Inicializa
vg = VectorGov(api_key="vg_xxx")
llm = pipeline("text-generation", model="Qwen/Qwen2.5-3B-Instruct", device_map="auto")

# Cria pipeline RAG
rag = create_rag_pipeline(vg, llm, top_k=5, max_new_tokens=512)

# Usa como fun√ß√£o
resposta = rag("Quais os crit√©rios de julgamento na licita√ß√£o?")
print(resposta)
```

### Classe VectorGovRAG

```python
from vectorgov import VectorGov
from vectorgov.integrations.transformers import VectorGovRAG
from transformers import pipeline

vg = VectorGov(api_key="vg_xxx")
llm = pipeline("text-generation", model="meta-llama/Llama-3.2-3B-Instruct", device_map="auto")

rag = VectorGovRAG(vg, llm, top_k=5, temperature=0.1)

response = rag.ask("O que √© ETP?")

print(response.answer)
print(response.sources)      # Lista de fontes usadas
print(response.latency_ms)   # Tempo de busca
```

### Modelos Recomendados (HuggingFace)

| Modelo | VRAM | Qualidade | Portugu√™s |
|--------|------|-----------|-----------|
| `meta-llama/Llama-3.2-1B-Instruct` | 2GB | B√°sica | Bom |
| `Qwen/Qwen2.5-3B-Instruct` | 6GB | Boa | **Excelente** |
| `meta-llama/Llama-3.2-3B-Instruct` | 6GB | Boa | Bom |
| `Qwen/Qwen2.5-7B-Instruct` | 14GB | Muito Boa | **Excelente** |
| `microsoft/Phi-3-mini-4k-instruct` | 4GB | Boa | Razo√°vel |

```python
from vectorgov.integrations.transformers import get_recommended_models

# Lista modelos com detalhes
for name, info in get_recommended_models().items():
    print(f"{name}: {info['vram_gb']}GB, {info['portuguese']}")
```

### Rodando sem GPU (CPU)

```python
from transformers import pipeline
import torch

# For√ßa CPU com modelo leve
llm = pipeline(
    "text-generation",
    model="meta-llama/Llama-3.2-1B-Instruct",
    device="cpu",
    torch_dtype=torch.float32,
)
```

### Modelo Quantizado (4-bit)

```python
from transformers import pipeline, BitsAndBytesConfig
import torch

# Quantiza√ß√£o 4-bit (usa ~4GB VRAM para modelo 7B)
quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float16,
)

llm = pipeline(
    "text-generation",
    model="Qwen/Qwen2.5-7B-Instruct",
    model_kwargs={"quantization_config": quantization_config},
    device_map="auto",
)
```

---

# ü§ñ Frameworks de Agentes

## Function Calling (Agentes)

O VectorGov pode ser usado como ferramenta em agentes de IA. O LLM decide automaticamente quando consultar a legisla√ß√£o.

### OpenAI Function Calling

```python
from vectorgov import VectorGov
from openai import OpenAI

vg = VectorGov(api_key="vg_xxx")
client = OpenAI()

# Primeira chamada - GPT decide se precisa consultar legisla√ß√£o
response = client.chat.completions.create(
    model="gpt-4o",
    messages=[{"role": "user", "content": "Quais os crit√©rios de julgamento?"}],
    tools=[vg.to_openai_tool()],  # Registra VectorGov como ferramenta
)

# Se GPT quiser usar a ferramenta
if response.choices[0].message.tool_calls:
    tool_call = response.choices[0].message.tool_calls[0]
    result = vg.execute_tool_call(tool_call)  # Executa busca

    # Segunda chamada com o resultado
    final = client.chat.completions.create(
        model="gpt-4o",
        messages=[
            {"role": "user", "content": "Quais os crit√©rios de julgamento?"},
            response.choices[0].message,
            {"role": "tool", "tool_call_id": tool_call.id, "content": result},
        ],
    )
    print(final.choices[0].message.content)
```

### Anthropic Claude Tools

```python
from vectorgov import VectorGov
from anthropic import Anthropic

vg = VectorGov(api_key="vg_xxx")
client = Anthropic()

response = client.messages.create(
    model="claude-sonnet-4-20250514",
    messages=[{"role": "user", "content": "O que √© ETP?"}],
    tools=[vg.to_anthropic_tool()],
)

# Processar tool_use se houver
for block in response.content:
    if block.type == "tool_use":
        result = vg.execute_tool_call(block)
```

### Google Gemini Function Calling

```python
from vectorgov import VectorGov
import google.generativeai as genai

vg = VectorGov(api_key="vg_xxx")
genai.configure(api_key="sua_key")

model = genai.GenerativeModel(
    model_name="gemini-2.0-flash",
    tools=[vg.to_google_tool()],
)

response = model.generate_content("O que √© ETP?")
```

---

## Integra√ß√£o com LangChain

```bash
pip install 'vectorgov[langchain]'
```

### VectorGovRetriever

```python
from vectorgov.integrations.langchain import VectorGovRetriever
from langchain.chains import RetrievalQA
from langchain_openai import ChatOpenAI

# Criar retriever
retriever = VectorGovRetriever(api_key="vg_xxx", top_k=5)

# Usar com RetrievalQA
qa = RetrievalQA.from_chain_type(
    llm=ChatOpenAI(model="gpt-4o-mini"),
    retriever=retriever,
)

answer = qa.invoke("Quando o ETP pode ser dispensado?")
print(answer["result"])
```

### Com LCEL (LangChain Expression Language)

```python
from vectorgov.integrations.langchain import VectorGovRetriever
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain_openai import ChatOpenAI

retriever = VectorGovRetriever(api_key="vg_xxx")
llm = ChatOpenAI(model="gpt-4o-mini")

prompt = ChatPromptTemplate.from_template("""
Contexto: {context}

Pergunta: {question}
""")

def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)

chain = (
    {"context": retriever | format_docs, "question": RunnablePassthrough()}
    | prompt
    | llm
    | StrOutputParser()
)

answer = chain.invoke("O que √© ETP?")
```

### VectorGovTool para Agentes

```python
from vectorgov.integrations.langchain import VectorGovTool
from langchain.agents import AgentExecutor, create_openai_tools_agent
from langchain_openai import ChatOpenAI

tool = VectorGovTool(api_key="vg_xxx")
llm = ChatOpenAI(model="gpt-4o")

# Criar agente com a ferramenta
agent = create_openai_tools_agent(llm, [tool], prompt)
executor = AgentExecutor(agent=agent, tools=[tool])

result = executor.invoke({"input": "O que diz a lei sobre ETP?"})
```

---

## Integra√ß√£o com LangGraph

```bash
pip install 'vectorgov[langgraph]'
```

### ReAct Agent

```python
from vectorgov.integrations.langgraph import create_vectorgov_tool
from langgraph.prebuilt import create_react_agent
from langchain_openai import ChatOpenAI

# Criar ferramenta VectorGov
tool = create_vectorgov_tool(api_key="vg_xxx", top_k=5)

# Criar agente ReAct
llm = ChatOpenAI(model="gpt-4o-mini")
agent = create_react_agent(llm, tools=[tool])

# Executar
result = agent.invoke({"messages": [("user", "O que √© ETP?")]})
print(result["messages"][-1].content)
```

### Grafo RAG Customizado

```python
from vectorgov.integrations.langgraph import create_retrieval_node, VectorGovState
from langgraph.graph import StateGraph, START, END
from langchain_openai import ChatOpenAI

# N√≥ de retrieval VectorGov
retrieval_node = create_retrieval_node(api_key="vg_xxx", top_k=5)

# N√≥ de gera√ß√£o
def generate(state: VectorGovState) -> dict:
    llm = ChatOpenAI(model="gpt-4o-mini")
    context = state.get("context", "")
    query = state.get("query", "")
    response = llm.invoke(f"Contexto: {context}\n\nPergunta: {query}")
    return {"response": response.content}

# Construir grafo
builder = StateGraph(dict)
builder.add_node("retrieve", retrieval_node)
builder.add_node("generate", generate)
builder.add_edge(START, "retrieve")
builder.add_edge("retrieve", "generate")
builder.add_edge("generate", END)

graph = builder.compile()

# Executar
result = graph.invoke({"query": "Quando o ETP pode ser dispensado?"})
print(result["response"])
```

### Grafo RAG Pr√©-configurado

```python
from vectorgov.integrations.langgraph import create_legal_rag_graph
from langchain_openai import ChatOpenAI

llm = ChatOpenAI(model="gpt-4o-mini")
graph = create_legal_rag_graph(llm=llm, api_key="vg_xxx")

result = graph.invoke({"query": "Quais os crit√©rios de julgamento?"})
print(result["response"])
```

---

## Integra√ß√£o com Google ADK

```bash
pip install 'vectorgov[google-adk]'
```

### Ferramenta de Busca

```python
from vectorgov.integrations.google_adk import create_search_tool

# Criar ferramenta
search = create_search_tool(api_key="vg_xxx", top_k=5)

# Testar diretamente (sem agente)
result = search("O que √© ETP?")
print(result)
```

### Toolset Completo

```python
from vectorgov.integrations.google_adk import VectorGovToolset

toolset = VectorGovToolset(api_key="vg_xxx")

# Lista ferramentas dispon√≠veis
for tool in toolset.get_tools():
    print(f"- {tool.__name__}")
# - search_brazilian_legislation
# - list_legal_documents
# - get_article_text

# Usar com agente ADK
from google.adk.agents import Agent

agent = Agent(
    name="legal_assistant",
    model="gemini-2.0-flash",
    tools=toolset.get_tools(),
)
```

### Agente ADK Pr√©-configurado

```python
from vectorgov.integrations.google_adk import create_legal_agent

agent = create_legal_agent(api_key="vg_xxx")

response = agent.run("Quais os crit√©rios de julgamento na licita√ß√£o?")
print(response)
```

---

# üîå Integra√ß√µes

## Servidor MCP (Claude Desktop, Cursor, etc.)

O VectorGov pode funcionar como servidor MCP (Model Context Protocol), permitindo integra√ß√£o direta com Claude Desktop, Cursor, Windsurf e outras ferramentas compat√≠veis.

### Instala√ß√£o

```bash
pip install 'vectorgov[mcp]'
```

### Configura√ß√£o no Claude Desktop

Adicione ao arquivo `claude_desktop_config.json`:

**Windows**: `%APPDATA%\Claude\claude_desktop_config.json`
**macOS**: `~/Library/Application Support/Claude/claude_desktop_config.json`

```json
{
    "mcpServers": {
        "vectorgov": {
            "command": "uvx",
            "args": ["vectorgov-mcp"],
            "env": {
                "VECTORGOV_API_KEY": "vg_sua_chave_aqui"
            }
        }
    }
}
```

Ou se instalou via pip:

```json
{
    "mcpServers": {
        "vectorgov": {
            "command": "vectorgov-mcp",
            "env": {
                "VECTORGOV_API_KEY": "vg_sua_chave_aqui"
            }
        }
    }
}
```

### Executar Manualmente

```bash
# Via uvx (sem instalar)
uvx vectorgov-mcp

# Via pip (ap√≥s instalar)
vectorgov-mcp

# Via Python
python -m vectorgov.mcp
```

### Ferramentas Dispon√≠veis

O servidor MCP exp√µe tr√™s ferramentas para Claude:

| Ferramenta | Descri√ß√£o |
|------------|-----------|
| `search_legislation` | Busca sem√¢ntica em legisla√ß√£o brasileira |
| `list_available_documents` | Lista documentos dispon√≠veis na base |
| `get_article_text` | Obt√©m texto completo de um artigo espec√≠fico |

---

# ‚öôÔ∏è Configura√ß√£o

## Modos de Busca

| Modo | Descri√ß√£o | Lat√™ncia | Cache Padr√£o | Uso Recomendado |
|------|-----------|----------|--------------|-----------------|
| `fast` | Busca r√°pida, sem reranking | ~2s | ‚ùå Desligado | Chatbots, alta escala |
| `balanced` | Busca com reranking | ~5s | ‚ùå Desligado | **Uso geral (default)** |
| `precise` | Busca com HyDE + reranking | ~15s | ‚ùå Desligado | An√°lises cr√≠ticas |

> **Nota:** O cache est√° desabilitado por padr√£o em todos os modos para proteger sua privacidade.
> Veja a se√ß√£o [Aviso de Privacidade](#Ô∏è-aviso-de-privacidade---cache-compartilhado) para mais detalhes.

```python
# Busca r√°pida (chatbots)
results = vg.search("query", mode="fast")

# Busca balanceada (default)
results = vg.search("query", mode="balanced")

# Busca precisa (an√°lises)
results = vg.search("query", mode="precise")

# Qualquer modo COM cache (trade-off: privacidade vs lat√™ncia)
results = vg.search("query", mode="fast", use_cache=True)
```

## Filtros

```python
# Filtrar por tipo de documento
results = vg.search("licita√ß√£o", filters={"tipo": "lei"})

# Filtrar por ano
results = vg.search("preg√£o", filters={"ano": 2021})

# M√∫ltiplos filtros
results = vg.search("contrata√ß√£o direta", filters={
    "tipo": "in",
    "ano": 2022,
    "orgao": "seges"
})
```

## Formata√ß√£o de Resultados

```python
results = vg.search("O que √© ETP?")

# String simples para contexto
context = results.to_context()
print(context)
# [1] Lei 14.133/2021, Art. 3
# O Estudo T√©cnico Preliminar - ETP √© documento...
#
# [2] IN 58/2022, Art. 6
# O ETP deve conter...

# Mensagens para chat (OpenAI, Anthropic)
messages = results.to_messages("O que √© ETP?")
# [{"role": "system", "content": "..."}, {"role": "user", "content": "..."}]

# Prompt √∫nico (Gemini)
prompt = results.to_prompt("O que √© ETP?")
```

## System Prompts Customizados

```python
# Usar prompt pr√©-definido
results = vg.search("query")
messages = results.to_messages(
    system_prompt=vg.get_system_prompt("detailed")
)

# Prompts dispon√≠veis
print(vg.available_prompts)
# ['default', 'concise', 'detailed', 'chatbot']

# Prompt totalmente customizado
custom_prompt = """Voc√™ √© um advogado especialista em licita√ß√µes.
Responda de forma t√©cnica e cite artigos espec√≠ficos."""

messages = results.to_messages(system_prompt=custom_prompt)
```

## Feedback

Ajude a melhorar o sistema enviando feedback:

```python
results = vg.search("O que √© ETP?")

# Ap√≥s verificar que o resultado foi √∫til
vg.feedback(results.query_id, like=True)

# Se o resultado n√£o foi √∫til
vg.feedback(results.query_id, like=False)
```

## Propriedades do Resultado

```python
results = vg.search("query")

# Informa√ß√µes gerais
results.query        # Query original
results.total        # Quantidade de resultados
results.latency_ms   # Tempo de resposta (ms)
results.cached       # Se veio do cache
results.query_id     # ID para feedback
results.mode         # Modo utilizado

# Iterar resultados
for hit in results:
    hit.text         # Texto do chunk
    hit.score        # Relev√¢ncia (0-1)
    hit.source       # Fonte formatada
    hit.metadata     # Metadados completos
```

## Tratamento de Erros

```python
from vectorgov import (
    VectorGov,
    VectorGovError,
    AuthError,
    RateLimitError,
    ValidationError,
)

try:
    results = vg.search("query")
except AuthError:
    print("API key inv√°lida ou expirada")
except RateLimitError as e:
    print(f"Rate limit. Tente em {e.retry_after}s")
except ValidationError as e:
    print(f"Erro no campo {e.field}: {e.message}")
except VectorGovError as e:
    print(f"Erro: {e.message}")
```

## Vari√°veis de Ambiente

```bash
# API key pode ser definida via ambiente
export VECTORGOV_API_KEY=vg_sua_chave_aqui
```

```python
# Usa automaticamente a vari√°vel de ambiente
vg = VectorGov()
```

## Configura√ß√£o Avan√ßada

```python
vg = VectorGov(
    api_key="vg_xxx",
    base_url="https://vectorgov.io/api/v1",  # URL customizada
    timeout=60,                               # Timeout em segundos
    default_top_k=10,                         # Resultados padr√£o
    default_mode="precise",                   # Modo padr√£o
)
```

---

# ‚ö†Ô∏è Aviso de Privacidade - Cache Compartilhado

## Entendendo o Cache Sem√¢ntico

O VectorGov utiliza um **cache sem√¢ntico compartilhado** entre todos os clientes da API. Isso significa:

| Aspecto | Comportamento |
|---------|---------------|
| **Suas perguntas** | Podem ser armazenadas no cache |
| **Suas respostas** | Podem ser servidas a outros clientes com perguntas similares |
| **Perguntas de outros** | Voc√™ pode receber respostas j√° geradas por outros clientes |

### Trade-off: Performance vs Privacidade

| Cache Habilitado | Cache Desabilitado |
|------------------|-------------------|
| ‚úÖ Lat√™ncia menor (~0.1s para cache hit) | ‚ùå Lat√™ncia maior (~5-15s) |
| ‚úÖ Resposta pode vir pr√©-validada | ‚ùå Sempre gera resposta nova |
| ‚ùå Perguntas vis√≠veis a outros clientes | ‚úÖ Total privacidade |
| ‚ùå Pode receber respostas de outros | ‚úÖ Respostas exclusivas |

### Controle de Cache

Por padr√£o, o cache est√° **DESABILITADO** para proteger sua privacidade:

```python
# Padr√£o: SEM cache (privado)
results = vg.search("O que √© ETP?")  # use_cache=False impl√≠cito

# Explicitamente habilitando cache (perda de privacidade)
results = vg.search("O que √© ETP?", use_cache=True)
```

### Quando Habilitar o Cache?

| Use Cache | N√£o Use Cache |
|-----------|---------------|
| Perguntas gen√©ricas sobre legisla√ß√£o | Perguntas com dados sens√≠veis |
| Alta escala de usu√°rios (chatbots p√∫blicos) | An√°lises confidenciais |
| Demos e testes | Ambientes corporativos restritos |
| Quando lat√™ncia √© cr√≠tica | Quando privacidade √© prioridade |

### Exemplo de Uso Consciente

```python
from vectorgov import VectorGov

vg = VectorGov(api_key="vg_xxx")

# Pergunta gen√©rica - pode usar cache
results = vg.search("Quais os crit√©rios de julgamento?", use_cache=True)

# Pergunta espec√≠fica com dados sens√≠veis - N√ÉO usar cache
results = vg.search("Contrato da empresa XYZ foi regular?", use_cache=False)
```

> **Nota:** O cache desabilitado n√£o afeta a qualidade da resposta, apenas a lat√™ncia.
> O sistema de duas fases garante alta precis√£o independente do cache.

---

## Obter sua API Key

1. Acesse [vectorgov.io/playground](https://vectorgov.io/playground)
2. Crie uma conta ou fa√ßa login
3. Gere sua API key na se√ß√£o "Configura√ß√µes"

## Documenta√ß√£o

- [Documenta√ß√£o Completa](https://docs.vectorgov.io)
- [Exemplos](https://github.com/vectorgov/vectorgov-python/tree/main/examples)
- [Refer√™ncia da API](https://docs.vectorgov.io/api-reference)

## Suporte

- [GitHub Issues](https://github.com/vectorgov/vectorgov-python/issues)
- Email: suporte@vectorgov.io

## Licen√ßa

MIT License - veja [LICENSE](LICENSE) para detalhes.

---

# üìÅ Gerenciamento de Documentos

O SDK permite gerenciar documentos na base de conhecimento. Algumas opera√ß√µes s√£o restritas a **administradores**.

## Permiss√µes

| Opera√ß√£o | Permiss√£o | M√©todo |
|----------|-----------|--------|
| Listar documentos | Todos | `list_documents()` |
| Ver detalhes | Todos | `get_document(id)` |
| Ver status ingest√£o | Todos | `get_ingest_status(task_id)` |
| Ver status enriquecimento | Todos | `get_enrichment_status(task_id)` |
| **Upload de PDF** | **Admin** | `upload_pdf()` |
| **Iniciar enriquecimento** | **Admin** | `start_enrichment()` |
| **Excluir documento** | **Admin** | `delete_document()` |

> **Nota:** Para obter permiss√µes de administrador, entre em contato com o suporte.

## Listar e Consultar Documentos

Qualquer usu√°rio autenticado pode listar e consultar documentos.

```python
from vectorgov import VectorGov

vg = VectorGov(api_key="vg_xxx")

# Listar todos os documentos
docs = vg.list_documents()
print(f"Total: {docs.total} documentos")

for doc in docs.documents:
    print(f"- {doc.document_id}: {doc.tipo_documento} {doc.numero}/{doc.ano}")
    print(f"  Chunks: {doc.chunks_count}, Enriquecidos: {doc.enriched_count}")
    print(f"  Progresso: {doc.enrichment_progress:.0%}")

# Pagina√ß√£o
docs = vg.list_documents(page=2, limit=10)

# Detalhes de um documento espec√≠fico
doc = vg.get_document("LEI-14133-2021")
print(f"Documento: {doc.titulo}")
print(f"Status: {'Enriquecido' if doc.is_enriched else 'Pendente'}")
```

## Upload e Ingest√£o (Admin)

> **Requer permiss√£o de administrador**

```python
from vectorgov import VectorGov

vg = VectorGov(api_key="vg_admin_xxx")  # API key com permiss√£o admin

# Upload de PDF
with open("lei_exemplo.pdf", "rb") as f:
    result = vg.upload_pdf(
        file=f,
        tipo_documento="LEI",
        numero="99999",
        ano=2024,
        titulo="Lei de Exemplo",
        descricao="Descri√ß√£o opcional"
    )

print(f"Upload: {result.message}")
print(f"Document ID: {result.document_id}")
print(f"Task ID: {result.task_id}")

# Acompanhar status da ingest√£o
status = vg.get_ingest_status(result.task_id)
print(f"Status: {status.status}")  # pending, processing, completed, failed
print(f"Progresso: {status.progress}%")
print(f"Chunks criados: {status.chunks_created}")
```

### Polling de Status

```python
import time

task_id = result.task_id

while True:
    status = vg.get_ingest_status(task_id)
    print(f"Status: {status.status} ({status.progress}%)")
    
    if status.status in ("completed", "failed"):
        break
    
    time.sleep(5)  # Aguarda 5 segundos

if status.status == "completed":
    print(f"Ingest√£o conclu√≠da! {status.chunks_created} chunks criados")
else:
    print(f"Erro: {status.message}")
```

## Enriquecimento (Admin)

> **Requer permiss√£o de administrador**

O enriquecimento adiciona contexto sem√¢ntico aos chunks (resumos, perguntas sint√©ticas, etc.), melhorando a qualidade da busca.

```python
from vectorgov import VectorGov

vg = VectorGov(api_key="vg_admin_xxx")

# Iniciar enriquecimento de um documento
result = vg.start_enrichment("LEI-14133-2021")
print(f"Task ID: {result.task_id}")

# Acompanhar progresso
status = vg.get_enrichment_status(result.task_id)
print(f"Status: {status.status}")
print(f"Progresso: {status.progress:.0%}")
print(f"Chunks enriquecidos: {status.chunks_enriched}")
print(f"Chunks pendentes: {status.chunks_pending}")
print(f"Erros: {status.chunks_failed}")

# Polling at√© concluir
import time

while status.status not in ("completed", "error"):
    time.sleep(10)
    status = vg.get_enrichment_status(result.task_id)
    print(f"Progresso: {status.progress:.0%} ({status.chunks_enriched}/{status.chunks_enriched + status.chunks_pending})")

print("Enriquecimento conclu√≠do!")
```

## Exclus√£o (Admin)

> **Requer permiss√£o de administrador**

```python
from vectorgov import VectorGov

vg = VectorGov(api_key="vg_admin_xxx")

# Excluir documento
result = vg.delete_document("LEI-99999-2024")

if result.success:
    print(f"Documento exclu√≠do: {result.message}")
else:
    print(f"Erro: {result.message}")
```

## Modelos de Resposta

### DocumentSummary

```python
@dataclass
class DocumentSummary:
    document_id: str      # Ex: "LEI-14133-2021"
    tipo_documento: str   # Ex: "LEI", "DECRETO", "IN"
    numero: str           # Ex: "14133"
    ano: int              # Ex: 2021
    titulo: str           # T√≠tulo do documento
    descricao: str        # Descri√ß√£o opcional
    chunks_count: int     # Total de chunks
    enriched_count: int   # Chunks enriquecidos
    
    # Propriedades calculadas
    is_enriched: bool           # True se todos chunks enriquecidos
    enrichment_progress: float  # 0.0 a 1.0
```

### IngestStatus

```python
@dataclass
class IngestStatus:
    task_id: str
    status: Literal["pending", "processing", "completed", "failed"]
    progress: int         # 0 a 100
    message: str
    document_id: str      # Dispon√≠vel ap√≥s conclus√£o
    chunks_created: int
```

### EnrichStatus

```python
@dataclass
class EnrichStatus:
    task_id: str
    status: Literal["pending", "processing", "completed", "error", "not_found"]
    progress: float       # 0.0 a 1.0
    chunks_enriched: int
    chunks_pending: int
    chunks_failed: int
    errors: list[str]     # Lista de erros, se houver
```
