# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.

from typing import Dict, List, Union, Optional
from typing_extensions import Literal, Annotated, TypeAlias

from pydantic import Field as FieldInfo

from ..._utils import PropertyInfo
from ..._models import BaseModel
from .policy_enforcement_result_action_value import PolicyEnforcementResultActionValue

__all__ = [
    "PolicyEnforceResponse",
    "Evaluation",
    "EvaluationPromptInjectionEvaluationResponseSchema",
    "EvaluationPromptInjectionEvaluationResponseSchemaThread",
    "EvaluationContentSafetyEvaluationResponseSchema",
    "EvaluationContentSafetyEvaluationResponseSchemaThread",
    "EvaluationDataLeakageEvaluationResponseSchema",
    "EvaluationDataLeakageEvaluationResponseSchemaThread",
    "EvaluationContextSafetyEvaluationResponseSchema",
    "EvaluationContextSafetyEvaluationResponseSchemaThread",
]


class EvaluationPromptInjectionEvaluationResponseSchemaThread(BaseModel):
    """Thread details for prompt injection guardrail."""

    content_identifier: int = FieldInfo(alias="contentIdentifier")
    """Index of the chunk/part within the message."""

    message_identifier: int = FieldInfo(alias="messageIdentifier")
    """Index of the message containing the issue."""

    score: float
    """Confidence score for this detection."""


class EvaluationPromptInjectionEvaluationResponseSchema(BaseModel):
    """Evaluation payload for prompt injection guardrail."""

    action: PolicyEnforcementResultActionValue
    """Action suggested by this guardrail (allow, detect, redact, block)."""

    guardrail_key: Literal["prompt_injection_protection"] = FieldInfo(alias="guardrailKey")
    """Guardrail key that produced this evaluation."""

    score: float
    """Overall confidence score."""

    threads: List[EvaluationPromptInjectionEvaluationResponseSchemaThread]
    """Evidence or reasoning threads generated by the guardrail."""

    metadata: Optional[Dict[str, object]] = None
    """Additional guardrail-specific metadata."""


class EvaluationContentSafetyEvaluationResponseSchemaThread(BaseModel):
    """Thread details for content safety guardrail."""

    category: Literal["hate", "crime", "sexual", "violence"]
    """Content safety category hit by the message."""

    content_identifier: int = FieldInfo(alias="contentIdentifier")
    """Index of the chunk/part within the message."""

    message_identifier: int = FieldInfo(alias="messageIdentifier")
    """Index of the message containing the issue."""

    score: float
    """Confidence score for the category."""


class EvaluationContentSafetyEvaluationResponseSchema(BaseModel):
    """Evaluation payload for content safety guardrail."""

    action: PolicyEnforcementResultActionValue
    """Action suggested by this guardrail (allow, detect, redact, block)."""

    guardrail_key: Literal["content_safety_filtering"] = FieldInfo(alias="guardrailKey")
    """Guardrail key that produced this evaluation."""

    score: float
    """Overall confidence score."""

    threads: List[EvaluationContentSafetyEvaluationResponseSchemaThread]
    """Evidence or reasoning threads generated by the guardrail."""

    metadata: Optional[Dict[str, object]] = None
    """Additional guardrail-specific metadata."""


class EvaluationDataLeakageEvaluationResponseSchemaThread(BaseModel):
    """Thread details for data leakage guardrail."""

    category: Literal[
        "name",
        "email",
        "phone",
        "address",
        "location",
        "date",
        "ssn",
        "credit_card",
        "bank_account",
        "id_number",
        "ip_address",
        "url",
        "username",
        "password",
        "api_key",
        "organization",
        "other",
    ]
    """Detected PII/secret category (email, phone, etc)."""

    content_identifier: int = FieldInfo(alias="contentIdentifier")
    """Index of the chunk/part within the message."""

    message_identifier: int = FieldInfo(alias="messageIdentifier")
    """Index of the message containing the leak."""

    score: float
    """Confidence score for the detection."""

    span: str
    """Detected span of text that triggered the guardrail."""


class EvaluationDataLeakageEvaluationResponseSchema(BaseModel):
    """Evaluation payload for data leakage guardrail."""

    action: PolicyEnforcementResultActionValue
    """Action suggested by this guardrail (allow, detect, redact, block)."""

    guardrail_key: Literal["data_leakage_protection"] = FieldInfo(alias="guardrailKey")
    """Guardrail key that produced this evaluation."""

    score: float
    """Overall confidence score."""

    threads: List[EvaluationDataLeakageEvaluationResponseSchemaThread]
    """Evidence or reasoning threads generated by the guardrail."""

    metadata: Optional[Dict[str, object]] = None
    """Additional guardrail-specific metadata."""


class EvaluationContextSafetyEvaluationResponseSchemaThread(BaseModel):
    """Thread details for context safety guardrail."""

    content_identifier: int = FieldInfo(alias="contentIdentifier")
    """Index of the chunk/part within the message."""

    message_identifier: int = FieldInfo(alias="messageIdentifier")
    """Index of the message containing the issue."""

    score: float
    """Confidence score for the topic."""

    topic: str
    """Topic label that was flagged."""


class EvaluationContextSafetyEvaluationResponseSchema(BaseModel):
    """Evaluation payload for context safety guardrail."""

    action: PolicyEnforcementResultActionValue
    """Action suggested by this guardrail (allow, detect, redact, block)."""

    guardrail_key: Literal["context_safety_filtering"] = FieldInfo(alias="guardrailKey")
    """Guardrail key that produced this evaluation."""

    score: float
    """Overall confidence score."""

    threads: List[EvaluationContextSafetyEvaluationResponseSchemaThread]
    """Evidence or reasoning threads generated by the guardrail."""

    metadata: Optional[Dict[str, object]] = None
    """Additional guardrail-specific metadata."""


Evaluation: TypeAlias = Annotated[
    Union[
        EvaluationPromptInjectionEvaluationResponseSchema,
        EvaluationContentSafetyEvaluationResponseSchema,
        EvaluationDataLeakageEvaluationResponseSchema,
        EvaluationContextSafetyEvaluationResponseSchema,
    ],
    PropertyInfo(discriminator="guardrail_key"),
]


class PolicyEnforceResponse(BaseModel):
    """Policy enforcement response schema."""

    action: PolicyEnforcementResultActionValue
    """Overall action decided by the policy (allow, detect, redact, block)."""

    evaluations: List[Evaluation]
    """Per-guardrail evaluation results."""
